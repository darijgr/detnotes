\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{tabu}
\usepackage{hyperref}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{needspace}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Saturday, April 16, 2016 22:50:02}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\usetikzlibrary{arrows}
\newcounter{exer}
\newcounter{exera}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[section]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{condition}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{quest}[theo]{Question}
\newenvironment{algorithm}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{conclusion}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\newtheorem{exmp}[exer]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\newtheorem{exetwo}[exera]{Additional exercise}
\newenvironment{addexercise}[1][]
{\begin{exetwo}[#1]\begin{leftbar}}
{\end{leftbar}\end{exetwo}}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\newcommand{\id}{\operatorname{id}}
\ihead{Notes on the combinatorial fundamentals of algebra}
\ohead{page \thepage}
\cfoot{}
\begin{document}

\title{Notes on the combinatorial fundamentals of algebra\thanks{old title: PRIMES
2015 reading project: problems and solutions}}
\author{Darij Grinberg}
\date{
%TCIMACRO{\TeXButton{today}{\today}}%
%BeginExpansion
\today
%EndExpansion
}
\maketitle
\tableofcontents

\section{\label{chp.intro}Introduction}

These notes are meant as a detailed introduction to the basic combinatorics
that underlies the \textquotedblleft explicit\textquotedblright\ part of
abstract algebra (i.e., the theory of determinants, and concrete families of
polynomials). They cover permutations and determinants (from a combinatorial
viewpoint -- no linear algebra is presumed), as well as some basic material on
binomial coefficients and recurrent (Fibonacci-like) sequences. The reader is
assumed to be familiar with (low-level) \textquotedblleft contest
mathematics\textquotedblright\ (i.e., have a good proficiency with high-school
mathematics) and mature enough to read combinatorial proofs.

These notes were originally written for the PRIMES reading project I have
mentored in 2015. The goal of the project was to become familiar with some
fundamentals of algebra and combinatorics (particularly the ones needed to
understand the literature on cluster algebras).

The notes are unfinished and probably full of misprints. I thank Anya Zhang
and Karthik Karnik (the two students taking part in the project) for finding
some errors! Thanks also to the PRIMES project at MIT, which gave the impetus
for the writing of this notes; and to George Lusztig for the sponsorship of my
mentoring position in this project.

\subsection{Prerequisites}

These notes are still far from their final form, and the exact prerequisites
for a reader are subject to change. At the current moment, I assume that the reader

\begin{itemize}
\item has a good grasp on basic school-level mathematics (integers, rational
numbers, prime numbers, etc.);

\item has some experience with proofs (mathematical induction, strong
induction, proof by contradiction, the concept of \textquotedblleft
WLOG\textquotedblright, etc.) and mathematical notation (functions,
subscripts, cases, what it means for an object to be \textquotedblleft
well-defined\textquotedblright, etc.)\footnote{A great introduction into these
matters (and many others!) is the free book \cite{LeLeMe16} by Lehman,
Leighton and Meyer. (\textbf{Practical note:} As of 2016, this book is still
undergoing frequent revisions, so you may find a newer version of it on the
internet by the time you are reading this than what I am citing below.
Unfortunately, you will also find many older versions, often as the first
google hits. Try searching for the title of the book along with the current
year to find something new.)};

\item knows what a polynomial is (at least over $\mathbb{Z}$ and $\mathbb{Q}$)
and how polynomials differ from polynomial functions\footnote{See Section
\ref{sect.polynomials-emergency} below for a quick survey of what this means,
and which sources to consult for the precise definitions.};

\item knows the most basic properties of binomial coefficients (e.g., how
$\dbinom{n}{k}$ counts $k$-element subsets of an $n$-element set);

\item knows the basics of modular arithmetic (e.g., if $a\equiv
b\operatorname{mod}n$ and $c\equiv d\operatorname{mod}n$, then $ac\equiv
bd\operatorname{mod}n$);

\item is familiar with the summation sign ($\sum$) and the product sign
($\prod$) and knows how to transform them (e.g., interchanging summations, and
substituting the index)\footnote{See Section \ref{sect.sums-repetitorium}
below for a quick overview of the notations that we will need.};

\item has some familiarity with matrices (i.e., knows how to add and to
multiply them).
\end{itemize}

Probably a few more requirements creep in at certain points of the notes,
which I have overlooked. Some examples and remarks rely on additional
knowledge (such as analysis, graph theory, abstract algebra); however, these
can be skipped.

\begin{noncompile}
Basics on sums and products

In the notes below, we shall make much use of the summation sign ($\sum$) and
the product sign ($\prod$). In this short section, we shall introduce these
signs and

For instance, the reader should understand the following two arguments:

For any nonnegative integer $n$, we have%
\begin{align*}
2\sum_{i=0}^{n}i  &  =\sum_{i=0}^{n}i+\sum_{i=0}^{n}%
i\ \ \ \ \ \ \ \ \ \ \left(  \text{since }2q=q+q\text{ for every }%
q\in\mathbb{Q}\right) \\
&  =\sum_{i=0}^{n}i+\sum_{i=0}^{n}\left(  n-i\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n-i\text{ for
}i\text{ in the second sum}\right) \\
&  =\sum_{i=0}^{n}\underbrace{\left(  i+\left(  n-i\right)  \right)  }%
_{=n}=\sum_{i=0}^{n}n=\left(  n+1\right)  n=n\left(  n+1\right)
\end{align*}
and thus $\sum_{i=0}^{n}i=\dfrac{n\left(  n+1\right)  }{2}$. Since $\sum
_{i=0}^{n}i=0+\sum_{i=1}^{n}i=\sum_{i=1}^{n}i$, this rewrites as $\sum
_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }{2}$.

[to add:

-- $\sum_{s\in S}$

-- $\prod_{s\in S}$

-- def $\sum_{i=a}^{b}$

-- def $\prod_{i=a}^{b}$

-- explain empty sums \& products \& less-than-empty (do not \textquotedblleft
sum backwards\textquotedblright)

-- renaming the summation index

-- substituting index (with several examples)

-- splitting a sum/product on a predicate

-- splitting a sum (careful because of less-than-empty) by interval

-- splitting off addends

-- Fubini (and explain what it means)

-- telescope rule

-- $\sum_{i}\sum_{j}=\sum_{j}\sum_{i}$ in rectangles and triangles

-- little Gauss with 2 proofs (nice and ugly, both showing something)

-- search for ``famous formula'' and replace it by a correct reference

-- $\sum_{i=1}^{n}i^{2}$

-- infinite sums with $0$s and products with $1$s

-- product rule

-- $\sum1 = $ cardinality

-- cardinality of union of disjoint sets as sum

-- cardinality of cartesian product of sets as product

]
\end{noncompile}

\subsection{Notations}

\begin{itemize}
\item In the following, we use $\mathbb{N}$ to denote the set $\left\{
0,1,2,\ldots\right\}  $. (Be warned that some other authors use the letter
$\mathbb{N}$ for $\left\{  1,2,3,\ldots\right\}  $ instead.)

\item We let $\mathbb{Q}$ denote the set of all rational numbers; we let
$\mathbb{R}$ be the set of all real numbers; we let $\mathbb{C}$ be the set of
all complex numbers.

\item If $X$ and $Y$ are two sets, then we shall use the notation ``$X \to
Y,\ x \mapsto E$'' (where $x$ is some symbol which has no specific meaning in
the current context, and where $E$ is some expression which usually involves
$x$) for ``the map from $X$ to $Y$ which sends every $x \in X$ to $E$''. For
example, ``$\mathbb{N} \to\mathbb{N},\ x \mapsto x^{2}+x+6$'' means the map
from $\mathbb{N}$ to $\mathbb{N}$ which sends every $x \in\mathbb{N}$ to
$x^{2}+x+6$. For another example, ``$\mathbb{N} \to\mathbb{Q},\ x
\mapsto\dfrac{x}{1+x}$'' denotes the map from $\mathbb{N}$ to $\mathbb{Q}$
which sends every $x \in\mathbb{N}$ to $\dfrac{x}{1+x}$.\ \ \ \ \footnote{A
word of warning: Of course, the notation ``$X \to Y,\ x \mapsto E$'' does not
always make sense; indeed, the map that it stands for might sometimes not
exist. For instance, the notation ``$\mathbb{N} \to\mathbb{Q}, \ x
\mapsto\dfrac{x}{1-x}$'' does not actually define a map, because the map that
it is supposed to define (i.e., the map from $\mathbb{N}$ to $\mathbb{Q}$
which sends every $x \in\mathbb{N}$ to $\dfrac{x}{1-x}$) does not exist (since
$\dfrac{x}{1-x}$ is not defined for $x = 1$). For another example, the
notation ``$\mathbb{N} \to\mathbb{Z}, \ x \mapsto\dfrac{x}{1+x}$'' does not
define a map, because the map that it is supposed to define (i.e., the map
from $\mathbb{N}$ to $\mathbb{Z}$ which sends every $x \in\mathbb{N}$ to
$\dfrac{x}{1+x}$) does not exist (for $x = 2$, we have $\dfrac{x}{1+x} =
\dfrac{2}{1+2} \notin\mathbb{Z}$, which shows that a map from $\mathbb{N}$ to
$\mathbb{Z}$ cannot send this $x$ to this $\dfrac{x}{1+x}$). Thus, when
defining a map from $X$ to $Y$ (using whatever notation), do not forget to
check that it is well-defined (i.e., that your definition specifies precisely
one image for each $x \in X$, and that these images all lie in $Y$). In many
cases, this is obvious or very easy to check (I will usually not even mention
this check), but in some cases, this is a difficult task.}
\end{itemize}

Further notations will be defined whenever they arise for the first time.

\subsection{\label{sect.sums-repetitorium}Sums and products: a synopsis}

In this section, I will recall the definitions of the $\sum$ and $\prod$ signs
and collect some of their basic properties (without proofs). When I say
\textquotedblleft recall\textquotedblright, I am implying that the reader has
at least some prior acquaintance (and, ideally, experience) with these signs;
for a first introduction, this section is probably too brief and too abstract.
Ideally, you should use this section to familiarize yourself with my
(sometimes idiosyncratic) notations.

Throughout Section \ref{sect.sums-repetitorium}, we let $\mathbb{A}$ be one of
the sets $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and
$\mathbb{C}$.

\subsubsection{Definition of $\sum$}

Let us first define the $\sum$ sign. There are actually several (slightly
different, but still closely related) notations involving the $\sum$ sign; let
us define the most important of them:

\begin{itemize}
\item If $S$ is a finite set, and if $a_{s}$ is an element of $\mathbb{A}$ for
each $s\in S$, then $\sum_{s\in S}a_{s}$ denotes the sum of all of these
elements $a_{s}$. Formally, this sum is defined by recursion on $\left\vert
S\right\vert $, as follows:

\begin{itemize}
\item If $S=\varnothing$, then $\sum_{s\in S}a_{s}$ is defined to be $0$.

\item Let $n\in\mathbb{N}$. Assume that we have defined $\sum_{s\in S}a_{s}$
for every finite set $S$ with $\left\vert S\right\vert =n$ (and every choice
of elements $a_{s}$ of $\mathbb{A}$). Now, if $S$ is a finite set with
$\left\vert S\right\vert =n+1$ (and if $a_{s}\in\mathbb{A}$ are chosen for all
$s\in S$), then $\sum_{s\in S}a_{s}$ is defined by picking any $t\in
S$\ \ \ \ \footnote{This is possible, because $S$ is nonempty (in fact,
$\left\vert S\right\vert =n+1>n\geq0$).} and setting%
\begin{equation}
\sum_{s\in S}a_{s}=a_{t}+\sum_{s\in S\setminus\left\{  t\right\}  }a_{s}.
\label{eq.sum.def.1}%
\end{equation}
It is not immediately clear why this definition is legitimate: The right hand
side of (\ref{eq.sum.def.1}) is defined using a choice of $t$, but we want our
value of $\sum_{s\in S}a_{s}$ to depend only on $S$ and on the $a_{s}$ (not on
some arbitrarily chosen $t\in S$). However, it is possible to prove that the
right hand side of (\ref{eq.sum.def.1}) is actually independent of $t$ (that
is, any two choices of $t$ will lead to the same result).
\end{itemize}

\textbf{Examples:}

\begin{itemize}
\item If $S=\left\{  4,7,9\right\}  $ and $a_{s}=\dfrac{1}{s^{2}}$ for every
$s\in S$, then $\sum_{s\in S}a_{s}=a_{4}+a_{7}+a_{9}=\dfrac{1}{4^{2}}%
+\dfrac{1}{7^{2}}+\dfrac{1}{9^{2}}=\dfrac{6049}{63504}$.

\item If $S=\left\{  1,2,\ldots,n\right\}  $ (for some $n\in\mathbb{N}$) and
$a_{s}=s^{2}$ for every $s\in S$, then $\sum_{s\in S}a_{s}=\sum_{s\in S}%
s^{2}=1^{2}+2^{2}+\cdots+n^{2}$. (There is a formula saying that the right
hand side of this equality is $\dfrac{1}{6}n\left(  2n+1\right)  \left(
n+1\right)  $.)
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{s\in S}a_{s}$ is usually pronounced \textquotedblleft sum
of the $a_{s}$ over all $s\in S$\textquotedblright\ or \textquotedblleft sum
of the $a_{s}$ with $s$ ranging over $S$\textquotedblright\ or
\textquotedblleft sum of the $a_{s}$ with $s$ running through all elements of
$S$\textquotedblright. The letter \textquotedblleft$s$\textquotedblright\ in
the sum is called the \textquotedblleft summation index\textquotedblright%
\footnote{The plural of the word \textquotedblleft index\textquotedblright%
\ here is \textquotedblleft indices\textquotedblright, not \textquotedblleft
indexes\textquotedblright.}, and its exact choice is immaterial (for example,
you can rewrite $\sum_{s\in S}a_{s}$ as $\sum_{t\in S}a_{t}$ or as $\sum
_{\Phi\in S}a_{\Phi}$ or as $\sum_{\spadesuit\in S}a_{\spadesuit}$), as long
as it does not already have a different meaning outside of the sum\footnote{If
it already has a different meaning, then it must not be used as a summation
index! For example, you must not write \textquotedblleft every $n\in
\mathbb{N}$ satisfies $\sum_{n\in\left\{  0,1,\ldots,n\right\}  }%
n=\dfrac{n\left(  n+1\right)  }{2}$\textquotedblright, because here the
summation index $n$ clashes with a different meaning of the letter $n$.}.
(Ultimately, a summation index is the same kind of placeholder variable as the
\textquotedblleft$s$\textquotedblright\ in the statement \textquotedblleft for
all $s\in S$, we have $a_{s}+2a_{s}=3a_{s}$\textquotedblright, or as a loop
variable in a for-loop in programming.) The sign $\sum$ itself is called
\textquotedblleft the summation sign\textquotedblright\ or \textquotedblleft
the $\sum$ sign\textquotedblright. The numbers $a_{s}$ are called the
\textit{addends} (or \textit{summands}) of the sum $\sum_{s\in S}a_{s}$. More
precisely, for any given $t\in S$, we can refer to the number $a_{t}$ as the
\textquotedblleft addend corresponding to the index $t$\textquotedblright\ (or
as the \textquotedblleft addend for $s=t$\textquotedblright, or as the
\textquotedblleft addend for $t$\textquotedblright) of the sum $\sum_{s\in
S}a_{s}$.

\item When the set $S$ is empty, the sum $\sum_{s\in S}a_{s}$ is called an
\textit{empty sum}. Our definition implies that any empty sum is $0$. This
convention is used throughout mathematics, except in rare occasions where a
slightly subtler version of it is used\footnote{Do not worry about this
subtler version for the time being. If you really want to know what it is: Our
above definition is tailored to the cases when the $a_{s}$ are numbers (i.e.,
elements of one of the sets $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$,
$\mathbb{R}$ and $\mathbb{C}$). In more advanced settings, one tends to take
sums of the form $\sum_{s\in S}a_{s}$ where the $a_{s}$ are not numbers but
(for example) elements of a commutative ring $\mathbb{K}$. (See Definition
\ref{def.commring} for the definition of a commutative ring.) In such cases,
one wants the sum $\sum_{s\in S}a_{s}$ for an empty set $S$ to be not the
integer $0$, but the zero of the commutative ring $\mathbb{K}$ (which is
sometimes distinct from the integer $0$). This has the slightly confusing
consequence that the meaning of the sum $\sum_{s\in S}a_{s}$ for an empty set
$S$ depends on what ring $\mathbb{K}$ the $a_{s}$ belong to, even if (for an
empty set $S$) there are no $a_{s}$ to begin with! But in practice, the choice
of $\mathbb{K}$ is always clear from context, so this is not ambiguous.
\par
A similar caveat applies to the other versions of the $\sum$ sign, as well as
to the $\prod$ sign defined further below; I shall not elaborate on it
further.}. If someone tells you that empty sums are undefined, you should not
be listening!

\item The summation index does not always have to be a single letter. For
instance, if $S$ is a set of pairs, then we can write $\sum_{\left(
x,y\right)  \in S}a_{\left(  x,y\right)  }$ (meaning the same as $\sum_{s\in
S}a_{s}$). Here is an example of this notation:%
\[
\sum_{\left(  x,y\right)  \in\left\{  1,2,3\right\}  ^{2}}\dfrac{x}{y}%
=\dfrac{1}{1}+\dfrac{1}{2}+\dfrac{1}{3}+\dfrac{2}{1}+\dfrac{2}{2}+\dfrac{2}%
{3}+\dfrac{3}{1}+\dfrac{3}{2}+\dfrac{3}{3}%
\]
(here, we are using the notation $\sum_{\left(  x,y\right)  \in S}a_{\left(
x,y\right)  }$ with $S=\left\{  1,2,3\right\}  ^{2}$ and $a_{\left(
x,y\right)  }=\dfrac{x}{y}$). Note that we could not have rewritten this sum
in the form $\sum_{s\in S}a_{s}$ with a single-letter variable $s$ without
introducing an extra notation such as $a_{\left(  x,y\right)  }$ for the
quotients $\dfrac{x}{y}$.

\item Mathematicians don't seem to have reached an agreement on the operator
precedence of the $\sum$ sign. By this I mean the following question: Does
$\sum_{s\in S}a_{s}+b$ (where $b$ is some other element of $\mathbb{A}$) mean
$\sum_{s\in S}\left(  a_{s}+b\right)  $ or $\left(  \sum_{s\in S}a_{s}\right)
+b$ ? In my experience, the second interpretation (i.e., reading it as
$\left(  \sum_{s\in S}a_{s}\right)  +b$) is more widespread, and this is the
interpretation that I will follow. Nevertheless, be on the watch for possible
misunderstandings, as someone might be using the first interpretation when you
expect it the least!\footnote{This is similar to the notorious disagreement
about whether $a/bc$ means $\left(  a/b\right)  \cdot c$ or $a/\left(
bc\right)  $.}

However, the situation is different for products and nested sums. For
instance, the expression $\sum_{s\in S}ba_{s}c$ is understood to mean
$\sum_{s\in S}\left(  ba_{s}c\right)  $, and a nested sum like $\sum_{s\in
S}\sum_{t\in T}a_{s,t}$ (where $S$ and $T$ are two sets, and where $a_{s,t}$
is an element of $\mathbb{A}$ for each pair $\left(  s,t\right)  \in S\times
T$) is to be read as $\sum_{s\in S}\left(  \sum_{t\in T}a_{s,t}\right)  $.

\item Speaking of nested sums: they mean exactly what they seem to mean. For
instance, $\sum_{s\in S}\sum_{t\in T}a_{s,t}$ is what you get if you compute
the sum $\sum_{t\in T}a_{s,t}$ for each $s\in S$, and then sum up all of these
sums together. In a nested sum $\sum_{s\in S}\sum_{t\in T}a_{s,t}$, the first
summation sign ($\sum_{s\in S}$) is called the \textquotedblleft outer
summation\textquotedblright, and the second summation sign ($\sum_{t\in T}$)
is called the \textquotedblleft inner summation\textquotedblright.

\item We have required the set $S$ to be finite when defining $\sum_{s\in
S}a_{s}$. Of course, this requirement was necessary for our definition, and
there is no way to make sense of infinite sums such as $\sum_{s\in\mathbb{Z}%
}s^{2}$. However, \textbf{some} infinite sums can be made sense of. The
simplest case is when the set $S$ might be infinite, but only finitely many
among the $a_{s}$ are nonzero. In this case, we can define $\sum_{s\in S}%
a_{s}$ simply by discarding the zero addends and summing the finitely many
remaining addends. Other situations in which infinite sums make sense appear
in analysis and in topological algebra (e.g., power series).

\item The sum $\sum_{s\in S}a_{s}$ always belongs to $\mathbb{A}%
$.\ \ \ \ \footnote{Recall that we have assumed $\mathbb{A}$ to be one of the
sets $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$,
and that we have assumed the $a_{s}$ to belong to $\mathbb{A}$.} For instance,
a sum of elements of $\mathbb{N}$ belongs to $\mathbb{N}$; a sum of elements
of $\mathbb{R}$ belongs to $\mathbb{R}$, and so on.
\end{itemize}

\item A slightly more complicated version of the summation sign is the
following: Let $S$ be a finite set, and let $\mathcal{A}\left(  s\right)  $ be
a logical statement defined for every $s\in S$\ \ \ \ \footnote{Formally
speaking, this means that $\mathcal{A}$ is a map from $S$ to the set of all
logical statements. Such a map is called a \textit{predicate}.}. For example,
$S$ can be $\left\{  1,2,3,4\right\}  $, and $\mathcal{A}\left(  s\right)  $
can be the statement \textquotedblleft$s$ is even\textquotedblright. For each
$s\in S$ satisfying $\mathcal{A}\left(  s\right)  $, let $a_{s}$ be an element
of $\mathbb{A}$. Then, the sum $\sum_{\substack{s\in S;\\\mathcal{A}\left(
s\right)  }}a_{s}$ is defined by%
\[
\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\sum
_{s\in\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)  \right\}  }a_{s}.
\]
In other words, $\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }%
}a_{s}$ is the sum of the $a_{s}$ for all $s\in S$ which satisfy
$\mathcal{A}\left(  s\right)  $.

\textbf{Examples:}

\begin{itemize}
\item If $S=\left\{  1,2,3,4,5\right\}  $, then $\sum_{\substack{s\in
S;\\s\text{ is even}}}a_{s}=a_{2}+a_{4}$. (Of course, $\sum_{\substack{s\in
S;\\s\text{ is even}}}a_{s}$ is $\sum_{\substack{s\in S;\\\mathcal{A}\left(
s\right)  }}a_{s}$ when $\mathcal{A}\left(  s\right)  $ is defined to be the
statement \textquotedblleft$s$ is even\textquotedblright.)

\item If $S=\left\{  1,2,\ldots,n\right\}  $ (for some $n\in\mathbb{N}$) and
$a_{s}=s^{2}$ for every $s\in S$, then $\sum_{\substack{s\in S;\\s\text{ is
even}}}a_{s}=a_{2}+a_{4}+\cdots+a_{k}$, where $k$ is the largest even number
among $1,2,\ldots,n$ (that is, $k=n$ if $n$ is even, and $k=n-1$ otherwise).
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}$
is usually pronounced \textquotedblleft sum of the $a_{s}$ over all $s\in S$
satisfying $\mathcal{A}\left(  s\right)  $\textquotedblright. The semicolon
after \textquotedblleft$s\in S$\textquotedblright\ is often omitted or
replaced by a colon or a comma. Many authors often omit the \textquotedblleft%
$s\in S$\textquotedblright\ part (so they simply write $\sum_{\mathcal{A}%
\left(  s\right)  }a_{s}$) when it is clear enough what the $S$ is. (For
instance, they would write $\sum_{1\leq s\leq5}s^{2}$ instead of
$\sum_{\substack{s\in\mathbb{N};\\1\leq s\leq5}}s^{2}$.)

\item The set $S$ needs not be finite in order for $\sum_{\substack{s\in
S;\\\mathcal{A}\left(  s\right)  }}a_{s}$ to be defined; it suffices that the
set $\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)  \right\}  $ be finite
(i.e., that only finitely many $s\in S$ satisfy $\mathcal{A}\left(  s\right)
$).

\item The sum $\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}$
is said to be \textit{empty} whenever the set $\left\{  t\in S\ \mid
\ \mathcal{A}\left(  t\right)  \right\}  $ is empty (i.e., whenever no $s\in
S$ satisfies $\mathcal{A}\left(  s\right)  $).
\end{itemize}

\item Finally, here is the simplest version of the summation sign: Let $u$ and
$v$ be two integers. We agree to understand the set $\left\{  u,u+1,\ldots
,v\right\}  $ to be empty when $u>v$. Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in\left\{  u,u+1,\ldots,v\right\}  $. Then,
$\sum_{s=u}^{v}a_{s}$ is defined by%
\[
\sum_{s=u}^{v}a_{s}=\sum_{s\in\left\{  u,u+1,\ldots,v\right\}  }a_{s}.
\]


\textbf{Examples:}

\begin{itemize}
\item We have $\sum_{s=3}^{8}\dfrac{1}{s}=\sum_{s\in\left\{  3,4,\ldots
,8\right\}  }\dfrac{1}{s}=\dfrac{1}{3}+\dfrac{1}{4}+\dfrac{1}{5}+\dfrac{1}%
{6}+\dfrac{1}{7}+\dfrac{1}{8}=\dfrac{341}{280}$.

\item We have $\sum_{s=3}^{3}\dfrac{1}{s}=\sum_{s\in\left\{  3\right\}
}\dfrac{1}{s}=\dfrac{1}{3}$.

\item We have $\sum_{s=3}^{2}\dfrac{1}{s}=\sum_{s\in\varnothing}\dfrac{1}%
{s}=0$.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{s=u}^{v}a_{s}$ is usually pronounced \textquotedblleft
sum of the $a_{s}$ for all $s$ from $u$ to $v$ (inclusive)\textquotedblright.
It is often written $a_{u}+a_{u+1}+\cdots+a_{v}$, but this latter notation has
its drawbacks: In order to understand an expression like $a_{u}+a_{u+1}%
+\cdots+a_{v}$, one needs to correctly guess the pattern (which can be
unintuitive when the $a_{s}$ themselves are complicated: for example, it takes
a while to find the \textquotedblleft moving parts\textquotedblright\ in the
expression $\dfrac{2\cdot7}{3+2}+\dfrac{3\cdot7}{3+3}+\cdots+\dfrac{7\cdot
7}{3+7}$, whereas the notation $\sum_{s=2}^{7}\dfrac{s\cdot7}{3+s}$ for the
same sum is perfectly clear).

\item In the sum $\sum_{s=u}^{v}a_{s}$, the integer $u$ is called the
\textit{lower limit} (of the sum), whereas the integer $v$ is called the
\textit{upper limit} (of the sum). The sum is said to \textit{start} (or
\textit{begin}) at $u$ and \textit{end} at $v$.

\item The sum $\sum_{s=u}^{v}a_{s}$ is said to be \textit{empty} whenever
$u>v$. In other words, a sum of the form $\sum_{s=u}^{v}a_{s}$ is empty
whenever it \textquotedblleft ends before it has begun\textquotedblright.
However, a sum which \textquotedblleft ends right after it
begins\textquotedblright\ (i.e., a sum $\sum_{s=u}^{v}a_{s}$ with $u=v$) is
not empty; it just has one addend only. (This is unlike integrals, which are
$0$ whenever their lower and upper limit are equal.)

\item Let me stress once again that a sum $\sum_{s=u}^{v}a_{s}$ with $u>v$ is
empty and equals $0$. It does not matter how much greater $u$ is than $v$. So,
for example, $\sum_{s=1}^{-5}s=0$. The fact that the upper bound ($-5$) is
much smaller than the lower bound ($1$) does not mean that you have to
subtract rather than add.
\end{itemize}
\end{itemize}

Thus we have introduced the main three forms of the summation sign. Some mild
variations on them appear in the literature (e.g., there is a slightly awkward
notation $\sum_{\substack{s=u;\\\mathcal{A}\left(  s\right)  }}^{v}a_{s}$ for
$\sum_{\substack{s\in\left\{  u,u+1,\ldots,v\right\}  ;\\\mathcal{A}\left(
s\right)  }}a_{s}$).

\subsubsection{Properties of $\sum$}

Let me now show some basic properties of summation signs that are important in
making them useful:

\begin{itemize}
\item \underline{\textbf{Splitting-off:}} Let $S$ be a finite set. Let $t\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=a_{t}+\sum_{s\in S\setminus\left\{  t\right\}  }a_{s}.
\label{eq.sum.split-off}%
\end{equation}
(This is precisely the equality (\ref{eq.sum.def.1}).) This formula
(\ref{eq.sum.split-off}) allows us to \textquotedblleft split
off\textquotedblright\ an addend from a sum.

\textbf{Example:} If $n\in\mathbb{N}$, then
\[
\sum_{s\in\left\{  1,2,\ldots,n+1\right\}  }a_{s}=a_{n+1}+\sum_{s\in\left\{
1,2,\ldots,n\right\}  }a_{s}%
\]
(by (\ref{eq.sum.split-off}), applied to $S=\left\{  1,2,\ldots,n+1\right\}  $
and $t=n+1$), but also%
\[
\sum_{s\in\left\{  1,2,\ldots,n+1\right\}  }a_{s}=a_{1}+\sum_{s\in\left\{
2,3,\ldots,n+1\right\}  }a_{s}%
\]
(by (\ref{eq.sum.split-off}), applied to $S=\left\{  1,2,\ldots,n+1\right\}  $
and $t=1$).

\item \underline{\textbf{Splitting:}} Let $S$ be a finite set. Let $X$ and $Y$
be two subsets of $S$ such that $X\cap Y=\varnothing$ and $X\cup Y=S$.
(Equivalently, $X$ and $Y$ are two subsets of $S$ such that each element of
$S$ lies in \textbf{exactly} one of $X$ and $Y$.) Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{s\in X}a_{s}+\sum_{s\in Y}a_{s}. \label{eq.sum.split}%
\end{equation}
(Here, as we explained, $\sum_{s\in X}a_{s}+\sum_{s\in Y}a_{s}$ stands for
$\left(  \sum_{s\in X}a_{s}\right)  +\left(  \sum_{s\in Y}a_{s}\right)  $.)
The idea behind (\ref{eq.sum.split}) is that if we want to add a bunch of
numbers (the $a_{s}$ for $s\in S$), we can proceed by splitting it into two
\textquotedblleft sub-bunches\textquotedblright\ (one \textquotedblleft
sub-bunch\textquotedblright\ consisting of the $a_{s}$ for $s\in X$, and the
other consisting of the $a_{s}$ for $s\in Y$), then take the sum of each of
these two sub-bunches, and finally add together the two sums.

\textbf{Examples:}

\begin{itemize}
\item If $n\in\mathbb{N}$, then%
\[
\sum_{s\in\left\{  1,2,\ldots,2n\right\}  }a_{s}=\sum_{s\in\left\{
1,3,\ldots,2n-1\right\}  }a_{s}+\sum_{s\in\left\{  2,4,\ldots,2n\right\}
}a_{s}%
\]
(by (\ref{eq.sum.split}), applied to $S=\left\{  1,2,\ldots,2n\right\}  $,
$X=\left\{  1,3,\ldots,2n-1\right\}  $ and $Y=\left\{  2,4,\ldots,2n\right\}
$.)

\item If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, then%
\[
\sum_{s\in\left\{  -m,-m+1,\ldots,n\right\}  }a_{s}=\sum_{s\in\left\{
-m,-m+1,\ldots,0\right\}  }a_{s}+\sum_{s\in\left\{  1,2,\ldots,n\right\}
}a_{s}%
\]
(by (\ref{eq.sum.split}), applied to $S=\left\{  -m,-m+1,\ldots,n\right\}  $,
$X=\left\{  -m,-m+1,\ldots,0\right\}  $ and $Y=\left\{  1,2,\ldots,n\right\}
$.)

\item If $u$, $v$ and $w$ are three integers such that $u-1\leq v\leq w$, and
if $a_{s}$ is an element of $\mathbb{A}$ for each $s\in\left\{  u,u+1,\ldots
,w\right\}  $, then%
\begin{equation}
\sum_{s=u}^{w}a_{s}=\sum_{s=u}^{v}a_{s}+\sum_{s=v+1}^{w}a_{s}.
\label{eq.sum.split.uvw}%
\end{equation}
This follows from (\ref{eq.sum.split}), applied to $S=\left\{  u,u+1,\ldots
,w\right\}  $, $X=\left\{  u,u+1,\ldots,v\right\}  $ and $Y=\left\{
v+1,v+2,\ldots,w\right\}  $. Notice that the requirement $u-1\leq v\leq w$ is
important; otherwise, the $X\cap Y=\varnothing$ and $X\cup Y=S$ condition
would not hold!
\end{itemize}

\item \underline{\textbf{Splitting using a predicate:}} Let $S$ be a finite
set. Let $\mathcal{A}\left(  s\right)  $ be a logical statement for each $s\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)
}}a_{s}+\sum_{\substack{s\in S;\\\text{not }\mathcal{A}\left(  s\right)
}}a_{s} \label{eq.sum.split.pred}%
\end{equation}
(where \textquotedblleft not $\mathcal{A}\left(  s\right)  $\textquotedblright%
\ means the negation of $\mathcal{A}\left(  s\right)  $). This simply follows
from (\ref{eq.sum.split}), applied to $X=\left\{  s\in S\ \mid\ \mathcal{A}%
\left(  s\right)  \right\}  $ and $Y=\left\{  s\in S\ \mid\ \text{not
}\mathcal{A}\left(  s\right)  \right\}  $.

\textbf{Example:} If $S\subseteq\mathbb{Z}$, then%
\[
\sum_{s\in S}a_{s}=\sum_{\substack{s\in S;\\s\text{ is even}}}a_{s}%
+\sum_{\substack{s\in S;\\s\text{ is odd}}}a_{s}%
\]
(because \textquotedblleft$s$ is odd\textquotedblright\ is the negation of
\textquotedblleft$s$ is even\textquotedblright).

\item \underline{\textbf{Summing equal values:}} Let $S$ be a finite set. Let
$a$ be an element of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{s\in S}a=\left\vert S\right\vert \cdot a. \label{eq.sum.equal}%
\end{equation}
In other words, if all addends of a sum are equal to one and the same element
$a$, then the sum is just the number of its addends times $a$. In particular,%
\[
\sum_{s\in S}1=\left\vert S\right\vert \cdot1=\left\vert S\right\vert .
\]


\item \underline{\textbf{Splitting an addend:}} Let $S$ be a finite set. For
every $s\in S$, let $a_{s}$ and $b_{s}$ be elements of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{s\in S}\left(  a_{s}+b_{s}\right)  =\sum_{s\in S}a_{s}+\sum_{s\in
S}b_{s}. \label{eq.sum.linear1}%
\end{equation}


\textbf{Remark:} Of course, similar rules hold for other forms of summations:
If $\mathcal{A}\left(  s\right)  $ is a logical statement for each $s\in S$,
then%
\[
\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}\left(  a_{s}%
+b_{s}\right)  =\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }%
}a_{s}+\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}b_{s}.
\]
If $u$ and $v$ are two integers, then%
\begin{equation}
\sum_{s=u}^{v}\left(  a_{s}+b_{s}\right)  =\sum_{s=u}^{v}a_{s}+\sum_{s=u}%
^{v}b_{s}. \label{eq.sum.linear1.c}%
\end{equation}


\item \underline{\textbf{Factoring out:}} Let $S$ be a finite set. For every
$s\in S$, let $a_{s}$ be an element of $\mathbb{A}$. Also, let $\lambda$ be an
element of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{s\in S}\lambda a_{s}=\lambda\sum_{s\in S}a_{s}. \label{eq.sum.linear2}%
\end{equation}
Again, similar rules hold for the other types of summation sign.

\item \underline{\textbf{Zeros sum to zero:}} Let $S$ be a finite set. Then,%
\[
\sum_{s\in S}0=0.
\]
That is, any sum of zeroes is zero.

\textbf{Remark:} This applies even to infinite sums! Do not be fooled by the
infiniteness of a sum: There are no reasonable situations where an infinite
sum of zeroes is defined to be anything other than zero. The infinity does not
\textquotedblleft compensate\textquotedblright\ for the zero.

\item \underline{\textbf{Dropping zeroes:}} Let $S$ be a finite set. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Let $T$ be a subset
of $S$ such that every $s\in T$ satisfies $a_{s}=0$. Then,%
\[
\sum_{s\in S}a_{s}=\sum_{s\in S\setminus T}a_{s}.
\]
(That is, any addends which are zero can be removed from a sum without
changing the sum's value.)

\item \underline{\textbf{Renaming the index:}} Let $S$ be a finite set. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\sum_{s\in S}a_{s}=\sum_{t\in S}a_{t}.
\]
This is just saying that the summation index in a sum can be renamed at will,
as long as its name does not clash with other notation.

\item \underline{\textbf{Substituting the index I:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{t}$
be an element of $\mathbb{A}$ for each $t\in T$. Then,%
\begin{equation}
\sum_{t\in T}a_{t}=\sum_{s\in S}a_{f\left(  s\right)  }. \label{eq.sum.subs1}%
\end{equation}
(The idea here is that the sum $\sum_{s\in S}a_{f\left(  s\right)  }$ contains
the same addends as the sum $\sum_{t\in T}a_{t}$.)

\textbf{Examples:}

\begin{itemize}
\item For any $n\in\mathbb{N}$, we have%
\[
\sum_{t\in\left\{  1,2,\ldots,n\right\}  }t^{3}=\sum_{s\in\left\{
-n,-n+1,\ldots,-1\right\}  }\left(  -s\right)  ^{3}.
\]
(This follows from (\ref{eq.sum.subs1}), applied to $S=\left\{  -n,-n+1,\ldots
,-1\right\}  $, $T=\left\{  1,2,\ldots,n\right\}  $, $f\left(  s\right)  =-s$,
and $a_{t}=t^{3}$.)

\item The sets $S$ and $T$ in (\ref{eq.sum.subs1}) may well be the same. For
example, for any $n\in\mathbb{N}$, we have%
\[
\sum_{t\in\left\{  1,2,\ldots,n\right\}  }t^{3}=\sum_{s\in\left\{
1,2,\ldots,n\right\}  }\left(  n+1-s\right)  ^{3}.
\]
(This follows from (\ref{eq.sum.subs1}), applied to $S=\left\{  1,2,\ldots
,n\right\}  $, $T=\left\{  1,2,\ldots,n\right\}  $, $f\left(  s\right)
=n+1-s$ and $a_{t}=t^{3}$.)

\item More generally: Let $u$ and $v$ be two integers. Then, the map $\left\{
u,u+1,\ldots,v\right\}  \rightarrow\left\{  u,u+1,\ldots,v\right\}  $ sending
each $s\in\left\{  u,u+1,\ldots,v\right\}  $ to $u+v-s$ is a
bijection\footnote{Check this!}. Hence, we can substitute $u+v-s$ for $s$ in
the sum $\sum_{s=u}^{v}a_{s}$ whenever an element $a_{s}$ of $\mathbb{A}$ is
given for each $s\in\left\{  u,u+1,\ldots,v\right\}  $. We thus obtain the
formula%
\[
\sum_{s=u}^{v}a_{s}=\sum_{s=u}^{v}a_{u+v-s}.
\]

\end{itemize}

\textbf{Remark:}

\begin{itemize}
\item When I use (\ref{eq.sum.subs1}) to rewrite the sum $\sum_{t\in T}a_{t}$
as $\sum_{s\in S}a_{f\left(  s\right)  }$, I say that I have \textquotedblleft
substituted $f\left(  s\right)  $ for $t$ in the sum\textquotedblright.
Conversely, when I use (\ref{eq.sum.subs1}) to rewrite the sum $\sum_{s\in
S}a_{f\left(  s\right)  }$ as $\sum_{t\in T}a_{t}$, I say that I have
\textquotedblleft substituted $t$ for $f\left(  s\right)  $ in the
sum\textquotedblright.

\item For convenience, I have chosen $s$ and $t$ as summation indices in
(\ref{eq.sum.subs1}). But as before, they can be chosen to be any letters not
otherwise used. It is perfectly okay to use one and the same letter for both
of them, e.g., to write $\sum_{s\in T}a_{s}=\sum_{s\in S}a_{f\left(  s\right)
}$.

\item Here is the probably most famous example of substitution in a sum: Fix a
nonnegative integer $n$. Then, we can substitute $n-i$ for $i$ in the sum
$\sum_{i=0}^{n}i$ (since the map $\left\{  0,1,\ldots,n\right\}
\rightarrow\left\{  0,1,\ldots,n\right\}  ,\ i\mapsto n-i$ is a bijection).
Thus, we obtain%
\[
\sum_{i=0}^{n}i=\sum_{i=0}^{n}\left(  n-i\right)  .
\]
Now,%
\begin{align*}
2\sum_{i=0}^{n}i  &  =\sum_{i=0}^{n}i+\underbrace{\sum_{i=0}^{n}i}%
_{=\sum_{i=0}^{n}\left(  n-i\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}2q=q+q\text{ for every }q\in\mathbb{Q}\right) \\
&  =\sum_{i=0}^{n}i+\sum_{i=0}^{n}\left(  n-i\right) \\
&  =\sum_{i=0}^{n}\underbrace{\left(  i+\left(  n-i\right)  \right)  }%
_{=n}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have used
(\ref{eq.sum.linear1.c}) backwards}\right) \\
&  =\sum_{i=0}^{n}n=\left(  n+1\right)  n\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sum.equal})}\right) \\
&  =n\left(  n+1\right)  ,
\end{align*}
and therefore%
\begin{equation}
\sum_{i=0}^{n}i=\dfrac{n\left(  n+1\right)  }{2}. \label{eq.sum.littlegauss1}%
\end{equation}
Since $\sum_{i=0}^{n}i=0+\sum_{i=1}^{n}i=\sum_{i=1}^{n}i$, this rewrites as%
\begin{equation}
\sum_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }{2}. \label{eq.sum.littlegauss2}%
\end{equation}

\end{itemize}

\item \underline{\textbf{Substituting the index II:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{s}$
be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{t\in T}a_{f^{-1}\left(  t\right)  }.
\label{eq.sum.subs2}%
\end{equation}
This is, of course, just (\ref{eq.sum.subs1}) but applied to $T$, $S$ and
$f^{-1}$ instead of $S$, $T$ and $f$. (Nevertheless, I prefer to mention
(\ref{eq.sum.subs2}) separately because it often is used in this very form.)

\item \underline{\textbf{Telescoping sums:}} Let $u$ and $v$ be two integers
such that $u-1\leq v$. Let $a_{s}$ be an element of $\mathbb{A}$ for each
$s\in\left\{  u-1,u,\ldots,v\right\}  $. Then,%
\begin{equation}
\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  =a_{v}-a_{u-1}.
\label{eq.sum.telescope}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item Let us give a new proof of (\ref{eq.sum.littlegauss2}). Indeed, fix a
nonnegative integer $n$. An easy computation reveals that%
\begin{equation}
s=\dfrac{s\left(  s+1\right)  }{2}-\dfrac{\left(  s-1\right)  \left(  \left(
s-1\right)  +1\right)  }{2} \label{eq.sum.littlegauss2.pf2.1}%
\end{equation}
for each $s\in\mathbb{Z}$. Thus,%
\begin{align*}
\sum_{i=1}^{n}i  &  =\sum_{s=1}^{n}s=\sum_{s=1}^{n}\left(  \dfrac{s\left(
s+1\right)  }{2}-\dfrac{\left(  s-1\right)  \left(  \left(  s-1\right)
+1\right)  }{2}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sum.littlegauss2.pf2.1})}\right) \\
&  =\dfrac{n\left(  n+1\right)  }{2}-\underbrace{\dfrac{\left(  1-1\right)
\left(  \left(  1-1\right)  +1\right)  }{2}}_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.telescope}), applied to
}u=1\text{, }v=n\text{ and }a_{s}=\dfrac{s\left(  s+1\right)  }{2}\right) \\
&  =\dfrac{n\left(  n+1\right)  }{2}.
\end{align*}
Thus, (\ref{eq.sum.littlegauss2}) is proven again. This kind of proof works
often when we need to prove a formula like (\ref{eq.sum.littlegauss2}); the
only tricky part was to \textquotedblleft guess\textquotedblright\ the right
value of $a_{s}$, which is straightforward if you know what you are looking
for (you want $a_{n}-a_{1}$ to be $\dfrac{n\left(  n+1\right)  }{2}$), but
rather tricky if you don't.

\item Other examples for the use of (\ref{eq.sum.telescope}) can be found on
\href{https://en.wikipedia.org/wiki/Telescoping_series}{the Wikipedia page for
\textquotedblleft telescoping series\textquotedblright}. Let me add just one
more example: Given $n\in\mathbb{N}$, we want to compute $\sum_{i=1}^{n}%
\dfrac{1}{\sqrt{i}+\sqrt{i+1}}$. (Here, of course, we need to take
$\mathbb{A}=\mathbb{R}$ or $\mathbb{A}=\mathbb{C}$.) We proceed as follows:
For every positive integer $i$, we have%
\[
\dfrac{1}{\sqrt{i}+\sqrt{i+1}}=\dfrac{\left(  \sqrt{i+1}-\sqrt{i}\right)
}{\left(  \sqrt{i}+\sqrt{i+1}\right)  \left(  \sqrt{i+1}-\sqrt{i}\right)
}=\sqrt{i+1}-\sqrt{i}%
\]
(since $\left(  \sqrt{i}+\sqrt{i+1}\right)  \left(  \sqrt{i+1}-\sqrt
{i}\right)  =\left(  \sqrt{i+1}\right)  ^{2}-\left(  \sqrt{i}\right)
^{2}=\left(  i+1\right)  -i=1$). Thus,%
\begin{align*}
&  \sum_{i=1}^{n}\dfrac{1}{\sqrt{i}+\sqrt{i+1}}\\
&  =\sum_{i=1}^{n}\left(  \sqrt{i+1}-\sqrt{i}\right)  =\sum_{s=2}^{n+1}\left(
\sqrt{s}-\sqrt{s-1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }s-1\text{ for }i\text{ in the sum,}\\
\text{since the map }\left\{  2,3,\ldots,n+1\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  ,\ s\mapsto s-1\\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sqrt{n+1}-\underbrace{\sqrt{2-1}}_{=\sqrt{1}=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.telescope}), applied to
}u=2\text{, }v=n+1\text{ and }a_{s}=\sqrt{s}-\sqrt{s-1}\right) \\
&  =\sqrt{n+1}-1.
\end{align*}

\end{itemize}

\textbf{Remark:} When we use the equality (\ref{eq.sum.telescope}) to rewrite
the sum $\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  $ as $a_{v}-a_{u-1}$, we
can say that the sum $\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  $
\textquotedblleft telescopes\textquotedblright\ to $a_{v}-a_{u-1}$. A sum like
$\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  $ is said to be a
\textquotedblleft telescoping sum\textquotedblright. This terminology
references the idea that the sum $\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)
$ \textquotedblleft shrink\textquotedblright\ to the simple difference
$a_{v}-a_{u-1}$ like a telescope does when it is collapsed.

\item \underline{\textbf{Splitting a sum by a value of a function:}} Let $S$
be a finite set. Let $W$ be a set. Let $f:S\rightarrow W$ be a map. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{w\in W}\sum_{\substack{s\in S;\\f\left(  s\right)
=w}}a_{s}. \label{eq.sum.sheph}%
\end{equation}
The idea behind this formula is the following: The left hand side is the sum
of all $a_{s}$ for $s\in S$. The right hand side is the same sum, but split in
a particular way: First, for each $w\in W$, we sum the $a_{s}$ for all $s\in
S$ satisfying $f\left(  s\right)  =w$, and then we take the sum of all these
\textquotedblleft partial sums\textquotedblright.

\textbf{Examples:}

\begin{itemize}
\item Let $n\in\mathbb{N}$. Then,%
\begin{equation}
\sum_{s\in\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  }s^{3}%
=\sum_{w\in\left\{  0,1,\ldots,n\right\}  }\sum_{\substack{s\in\left\{
-n,-\left(  n-1\right)  ,\ldots,n\right\}  ;\\\left\vert s\right\vert
=w}}s^{3}. \label{eq.sum.sheph.exam1}%
\end{equation}
(This follows from (\ref{eq.sum.sheph}), applied to $S=\left\{  -n,-\left(
n-1\right)  ,\ldots,n\right\}  $, $W=\left\{  0,1,\ldots,n\right\}  $ and
$f\left(  s\right)  =\left\vert s\right\vert $.) You might wonder what you
gain by this observation. But actually, it allows you to compute the sum: For
any $w\in\left\{  0,1,\ldots,n\right\}  $, the sum $\sum_{\substack{s\in
\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  ;\\\left\vert s\right\vert
=w}}s^{3}$ is $0$\ \ \ \ \footnote{\textit{Proof.} If $w=0$, then this sum
$\sum_{\substack{s\in\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}
;\\\left\vert s\right\vert =w}}s^{3}$ consists of one addend only, and this
addend is $0^{3}$. If $w>0$, then this sum has two addends, namely $\left(
-w\right)  ^{3}$ and $w^{3}$. In either case, the sum is $0$ (because
$0^{3}=0$ and $\left(  -w\right)  ^{3}+w^{3}=-w^{3}+w^{3}=0$).}, and therefore
(\ref{eq.sum.sheph.exam1}) becomes%
\[
\sum_{s\in\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  }s^{3}%
=\sum_{w\in\left\{  0,1,\ldots,n\right\}  }\underbrace{\sum_{\substack{s\in
\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  ;\\\left\vert s\right\vert
=w}}s^{3}}_{=0}=\sum_{w\in\left\{  0,1,\ldots,n\right\}  }0=0.
\]
Thus, a strategic application of (\ref{eq.sum.sheph}) can help in evaluating a sum.

\item Let $S$ be a finite set. Let $W$ be a set. Let $f:S\rightarrow W$ be a
map. If we apply (\ref{eq.sum.sheph}) to $a_{s}=1$, then we obtain%
\[
\sum_{s\in S}1=\sum_{w\in W}\underbrace{\sum_{\substack{s\in S;\\f\left(
s\right)  =w}}1}_{\substack{=\left\vert \left\{  s\in S\ \mid\ f\left(
s\right)  =w\right\}  \right\vert \cdot1\\=\left\vert \left\{  s\in
S\ \mid\ f\left(  s\right)  =w\right\}  \right\vert }}=\sum_{w\in W}\left\vert
\left\{  s\in S\ \mid\ f\left(  s\right)  =w\right\}  \right\vert .
\]
Since $\sum_{s\in S}1=\left\vert S\right\vert \cdot1=\left\vert S\right\vert
$, this rewrites as follows:%
\begin{equation}
\left\vert S\right\vert =\sum_{w\in W}\left\vert \left\{  s\in S\ \mid
\ f\left(  s\right)  =w\right\}  \right\vert . \label{eq.sum.sheph.exam2}%
\end{equation}
This equality is often called the \textit{shepherd's principle}, because it is
connected to the joke that \textquotedblleft in order to count a flock of
sheep, just count the legs and divide by $4$\textquotedblright. The connection
is somewhat weak, actually; the equality (\ref{eq.sum.sheph.exam2}) is better
regarded as a formalization of the (less funny) idea that in order to count
all legs of a flock of sheep, you can count the legs of every single sheep,
and then sum the resulting numbers over all sheep in the flock. Think of the
$S$ in (\ref{eq.sum.sheph.exam2}) as the set of all legs of all sheep in the
flock; think of $W$ as the set of all sheep in the flock; and think of $f$ as
the function which sends every leg to the (hopefully uniquely determined)
sheep it belongs to.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item If $f:S\rightarrow W$ is a map between two sets $S$ and $W$, and if $w$
is an element of $W$, then it is common to denote the set $\left\{  s\in
S\ \mid\ f\left(  s\right)  =w\right\}  $ by $f^{-1}\left(  w\right)  $.
(Formally speaking, this notation might clash with the notation $f^{-1}\left(
w\right)  $ for the actual preimage of $w$ when $f$ happens to be bijective;
but in practice, this causes far less confusion than it might seem to.) Using
this notation, we can rewrite (\ref{eq.sum.sheph}) as follows:%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{w\in W}\underbrace{\sum_{\substack{s\in S;\\f\left(
s\right)  =w}}}_{=\sum_{s\in f^{-1}\left(  w\right)  }}a_{s}=\sum_{w\in W}%
\sum_{s\in f^{-1}\left(  w\right)  }a_{s}. \label{eq.sum.sheph.preimg}%
\end{equation}


\item When I rewrite a sum $\sum_{s\in S}a_{s}$ as $\sum_{w\in W}%
\sum_{\substack{s\in S;\\f\left(  s\right)  =w}}a_{s}$ (or as $\sum_{w\in
W}\sum_{s\in f^{-1}\left(  w\right)  }a_{s}$), I say that I am
\textquotedblleft splitting the sum according to the value of $f\left(
s\right)  $\textquotedblright. (Though, most of the time, I shall be doing
such manipulations without explicit mention.)
\end{itemize}

\item \underline{\textbf{Splitting a sum into subsums:}} Let $S$ be a finite
set. Let $S_{1},S_{2},\ldots,S_{n}$ be finitely many subsets of $S$. Assume
that these subsets $S_{1},S_{2},\ldots,S_{n}$ are pairwise disjoint (i.e., we
have $S_{i}\cap S_{j}=\varnothing$ for any two distinct elements $i$ and $j$
of $\left\{  1,2,\ldots,n\right\}  $) and their union is $S$. (Thus, every
element of $S$ lies in precisely one of the subsets $S_{1},S_{2},\ldots,S_{n}%
$.) Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{w=1}^{n}\sum_{s\in S_{w}}a_{s}.
\label{eq.sum.split-n}%
\end{equation}
This is a generalization of (\ref{eq.sum.split}) (indeed, (\ref{eq.sum.split})
is obtained from (\ref{eq.sum.split-n}) by setting $n=2$, $S_{1}=X$ and
$S_{2}=Y$). It is also a consequence of (\ref{eq.sum.sheph}): Indeed, set
$W=\left\{  1,2,\ldots,n\right\}  $, and define a map $f:S\rightarrow W$ to
send each $s\in S$ to the unique $w\in\left\{  1,2,\ldots,n\right\}  $ for
which $s\in S_{w}$. Then, every $w\in W$ satisfies $\sum_{\substack{s\in
S;\\f\left(  s\right)  =w}}a_{s}=\sum_{s\in S_{w}}a_{s}$; therefore,
(\ref{eq.sum.sheph}) becomes (\ref{eq.sum.split-n}).

\textbf{Example:} If we set $a_{s}=1$ for each $s\in S$, then
(\ref{eq.sum.split-n}) becomes%
\[
\sum_{s\in S}1=\sum_{w=1}^{n}\underbrace{\sum_{s\in S_{w}}1}_{=\left\vert
S_{w}\right\vert }=\sum_{w=1}^{n}\left\vert S_{w}\right\vert .
\]
Hence,
\[
\sum_{w=1}^{n}\left\vert S_{w}\right\vert =\sum_{s\in S}1=\left\vert
S\right\vert .
\]


\item \underline{\textbf{Fubini's theorem (interchanging the order of
summation):}} Let $X$ and $Y$ be two finite sets. Let $a_{\left(  x,y\right)
}$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in X\times Y$.
Then,%
\begin{equation}
\sum_{x\in X}\sum_{y\in Y}a_{\left(  x,y\right)  }=\sum_{\left(  x,y\right)
\in X\times Y}a_{\left(  x,y\right)  }=\sum_{y\in Y}\sum_{x\in X}a_{\left(
x,y\right)  }. \label{eq.sum.fubini}%
\end{equation}
This is called \textit{Fubini's theorem for finite sums}, and is a lot easier
to prove than what analysts tend to call Fubini's theorem. I shall sketch a
proof shortly (in the Remark below); but first, let me give some intuition for
the statement. Imagine that you have a rectangular table filled with numbers.
If you want to sum the numbers in the table, you can proceed in several ways.
One way is to sum the numbers in each row, and then sum all the sums you have
obtained. Another way is to sum the numbers in each column, and then sum all
the obtained sums. Either way, you get the same result -- namely, the sum of
all numbers in the table. This is essentially what (\ref{eq.sum.fubini}) says,
at least when $X=\left\{  1,2,\ldots,n\right\}  $ and $Y=\left\{
1,2,\ldots,m\right\}  $ for some integers $n$ and $m$. In this case, the
numbers $a_{\left(  x,y\right)  }$ can be viewed as forming a table, where
$a_{\left(  x,y\right)  }$ is placed in the cell at the intersection of row
$x$ with column $y$. When $X$ and $Y$ are arbitrary finite sets (not
necessarily $\left\{  1,2,\ldots,n\right\}  $ and $\left\{  1,2,\ldots
,m\right\}  $), then you need to slightly stretch your imagination in order to
see the $a_{\left(  x,y\right)  }$ as \textquotedblleft forming a
table\textquotedblright; in fact, there is no obvious order in which the
numbers appear in a row or column, but there is still a notion of rows and columns.

\textbf{Examples:}

\begin{itemize}
\item Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $a_{\left(  x,y\right)
}$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $. Then,
\begin{equation}
\sum_{x=1}^{n}\sum_{y=1}^{m}a_{\left(  x,y\right)  }=\sum_{\left(  x,y\right)
\in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}
}a_{\left(  x,y\right)  }=\sum_{y=1}^{m}\sum_{x=1}^{n}a_{\left(  x,y\right)
}. \label{eq.sum.fubini.nm}%
\end{equation}
(This follows from (\ref{eq.sum.fubini}), applied to $X=\left\{
1,2,\ldots,n\right\}  $ and $Y=\left\{  1,2,\ldots,m\right\}  $.) We can
rewrite the equality (\ref{eq.sum.fubini.nm}) without using $\sum$ signs; it
then takes the following form:%
\begin{align*}
&  \left(  a_{\left(  1,1\right)  }+a_{\left(  1,2\right)  }+\cdots+a_{\left(
1,m\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  2,1\right)  }+a_{\left(
2,2\right)  }+\cdots+a_{\left(  2,m\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\cdots\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  n,1\right)  }+a_{\left(
n,2\right)  }+\cdots+a_{\left(  n,m\right)  }\right) \\
&  =a_{\left(  1,1\right)  }+a_{\left(  1,2\right)  }+\cdots+a_{\left(
n,m\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{this is the sum of all
}nm\text{ numbers }a_{\left(  x,y\right)  }\right) \\
&  =\left(  a_{\left(  1,1\right)  }+a_{\left(  2,1\right)  }+\cdots
+a_{\left(  n,1\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  1,2\right)  }+a_{\left(
2,2\right)  }+\cdots+a_{\left(  n,2\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\cdots\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  1,m\right)  }+a_{\left(
2,m\right)  }+\cdots+a_{\left(  n,m\right)  }\right)  .
\end{align*}


\item Here is a concrete application of (\ref{eq.sum.fubini.nm}): Let
$n\in\mathbb{N}$ and $m\in\mathbb{N}$. We want to compute $\sum_{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  }xy$. (This is the sum of all entries of the $n\times m$
multiplication table.) Applying (\ref{eq.sum.fubini.nm}) to $a_{\left(
x,y\right)  }=xy$, we obtain%
\[
\sum_{x=1}^{n}\sum_{y=1}^{m}xy=\sum_{\left(  x,y\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  }xy=\sum_{y=1}%
^{m}\sum_{x=1}^{n}xy.
\]
Hence,%
\begin{align*}
&  \sum_{\left(  x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  }xy\\
&  =\sum_{x=1}^{n}\underbrace{\sum_{y=1}^{m}xy}_{\substack{=\sum_{s=1}%
^{m}xs=x\sum_{s=1}^{m}s\\\text{(by (\ref{eq.sum.linear2}), applied to
}S=\left\{  1,2,\ldots,m\right\}  \text{,}\\a_{s}=s\text{ and }\lambda
=x\text{)}}}=\sum_{x=1}^{n}x\underbrace{\sum_{s=1}^{m}s}_{\substack{=\sum
_{i=1}^{m}i=\dfrac{m\left(  m+1\right)  }{2}\\\text{(by
(\ref{eq.sum.littlegauss2}), applied to }m\\\text{instead of }n\text{)}}}\\
&  =\sum_{x=1}^{n}x\dfrac{m\left(  m+1\right)  }{2}=\sum_{x=1}^{n}%
\dfrac{m\left(  m+1\right)  }{2}x=\sum_{s=1}^{n}\dfrac{m\left(  m+1\right)
}{2}s\\
&  =\dfrac{m\left(  m+1\right)  }{2}\underbrace{\sum_{s=1}^{n}s}%
_{\substack{=\sum_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }{2}\\\text{(by
(\ref{eq.sum.littlegauss2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.linear2}), applied to
}S=\left\{  1,2,\ldots,n\right\}  \text{, }a_{s}=s\text{ and }\lambda
=\dfrac{m\left(  m+1\right)  }{2}\right) \\
&  =\dfrac{m\left(  m+1\right)  }{2}\cdot\dfrac{n\left(  n+1\right)  }{2}.
\end{align*}

\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item I have promised to outline a proof of (\ref{eq.sum.fubini}). Here it
comes: Let $S=X\times Y$ and $W=Y$, and let $f:S\rightarrow W$ be the map
which sends every pair $\left(  x,y\right)  $ to its second entry $y$. Then,
(\ref{eq.sum.sheph.preimg}) shows that%
\begin{equation}
\sum_{s\in X\times Y}a_{s}=\sum_{w\in Y}\sum_{s\in f^{-1}\left(  w\right)
}a_{s}. \label{eq.sum.fubini.pf.1}%
\end{equation}
But for every given $w\in Y$, the set $f^{-1}\left(  w\right)  $ is simply the
set of all pairs $\left(  x,w\right)  $ with $x\in X$. Thus, for every given
$w\in Y$, there is a bijection $g_{w}:X\rightarrow f^{-1}\left(  w\right)  $
given by%
\[
g_{w}\left(  x\right)  =\left(  x,w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}x\in X.
\]
Hence, for every given $w\in Y$, we can substitute $g_{w}\left(  x\right)  $
for $s$ in the sum $\sum_{s\in f^{-1}\left(  w\right)  }a_{s}$, and thus
obtain%
\[
\sum_{s\in f^{-1}\left(  w\right)  }a_{s}=\sum_{x\in X}\underbrace{a_{g_{w}%
\left(  x\right)  }}_{\substack{=a_{\left(  x,w\right)  }\\\text{(since }%
g_{w}\left(  x\right)  =\left(  x,w\right)  \text{)}}}=\sum_{x\in X}a_{\left(
x,w\right)  }.
\]
Hence, (\ref{eq.sum.fubini.pf.1}) becomes%
\[
\sum_{s\in X\times Y}a_{s}=\sum_{w\in Y}\underbrace{\sum_{s\in f^{-1}\left(
w\right)  }a_{s}}_{=\sum_{x\in X}a_{\left(  x,w\right)  }}=\sum_{w\in Y}%
\sum_{x\in X}a_{\left(  x,w\right)  }=\sum_{y\in Y}\sum_{x\in X}a_{\left(
x,y\right)  }%
\]
(here, we have renamed the summation index $w$ as $y$ in the outer sum).
Therefore,%
\[
\sum_{y\in Y}\sum_{x\in X}a_{\left(  x,y\right)  }=\sum_{s\in X\times Y}%
a_{s}=\sum_{\left(  x,y\right)  \in X\times Y}a_{\left(  x,y\right)  }%
\]
(here, we have renamed the summation index $s$ as $\left(  x,y\right)  $).
Thus, we have proven the second part of the equality (\ref{eq.sum.fubini}).
The first part can be proven similarly.

\item I like to abbreviate the equality (\ref{eq.sum.fubini.nm}) as follows:%
\begin{equation}
\sum_{x=1}^{n}\sum_{y=1}^{m}=\sum_{\left(  x,y\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  }=\sum_{y=1}%
^{m}\sum_{x=1}^{n}. \label{eq.sum.fubini.eq-sums}%
\end{equation}
This is an \textquotedblleft equality between summation
signs\textquotedblright; it should be understood as follows: Every time you
see an \textquotedblleft$\sum_{x=1}^{n}\sum_{y=1}^{m}$\textquotedblright\ in
an expression, you can replace it by a \textquotedblleft$\sum_{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  }$\textquotedblright\ or by a \textquotedblleft$\sum_{y=1}^{m}%
\sum_{x=1}^{n}$\textquotedblright, and similarly the other ways round.
\end{itemize}

\item \underline{\textbf{Triangular Fubini's theorem I:}} The equality
(\ref{eq.sum.fubini.nm}) formalizes the idea that we can sum the entries of a
rectangular table by first tallying each row and then adding together, or
first tallying each column and adding together. The same holds for triangular
tables. More precisely: Let $n\in\mathbb{N}$. Let $T_{n}$ be the set $\left\{
\left(  x,y\right)  \in\left\{  1,2,3,\ldots\right\}  ^{2}\ \mid\ x+y\leq
n\right\}  $. (For instance, if $n=3$, then $T_{n}=T_{3}=\left\{  \left(
1,1\right)  ,\left(  1,2\right)  ,\left(  2,1\right)  \right\}  $.) Let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(
x,y\right)  \in T_{n}$. Then,%
\begin{equation}
\sum_{x=1}^{n}\sum_{y=1}^{n-x}a_{\left(  x,y\right)  }=\sum_{\left(
x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }=\sum_{y=1}^{n}\sum_{x=1}%
^{n-y}a_{\left(  x,y\right)  }. \label{eq.sum.fubini.triangle}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item In the case when $n=4$, the formula (\ref{eq.sum.fubini.triangle})
(rewritten without the use of $\sum$ signs) looks as follows:%
\begin{align*}
&  \left(  a_{\left(  1,1\right)  }+a_{\left(  1,2\right)  }+a_{\left(
1,3\right)  }\right)  +\left(  a_{\left(  2,1\right)  }+a_{\left(  2,2\right)
}\right)  +a_{\left(  3,1\right)  }\\
&  =\left(  \text{the sum of the }a_{\left(  x,y\right)  }\text{ for all
}\left(  x,y\right)  \in T_{4}\right) \\
&  =\left(  a_{\left(  1,1\right)  }+a_{\left(  2,1\right)  }+a_{\left(
3,1\right)  }\right)  +\left(  a_{\left(  1,2\right)  }+a_{\left(  2,2\right)
}\right)  +a_{\left(  1,3\right)  }.
\end{align*}


\item Let us use (\ref{eq.sum.fubini.triangle}) to compute $\left\vert
T_{n}\right\vert $. Indeed, we can apply (\ref{eq.sum.fubini.triangle}) to
$a_{\left(  x,y\right)  }=1$. Thus, we obtain%
\[
\sum_{x=1}^{n}\sum_{y=1}^{n-x}1=\sum_{\left(  x,y\right)  \in T_{n}}%
1=\sum_{y=1}^{n}\sum_{x=1}^{n-y}1.
\]
Hence,%
\[
\sum_{x=1}^{n}\sum_{y=1}^{n-x}1=\sum_{\left(  x,y\right)  \in T_{n}%
}1=\left\vert T_{n}\right\vert ,
\]
so that%
\begin{align*}
\left\vert T_{n}\right\vert  &  =\sum_{x=1}^{n}\underbrace{\sum_{y=1}^{n-x}%
1}_{=n-x}=\sum_{x=1}^{n}\left(  n-x\right)  =\sum_{i=0}^{n-1}i\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }i\text{ for }n-x\text{ in the sum,}\\
\text{since the map }\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
0,1,\ldots,n-1\right\}  ,\ x\mapsto n-x\\
\text{is a bijection}%
\end{array}
\right) \\
&  =\dfrac{\left(  n-1\right)  \left(  \left(  n-1\right)  +1\right)  }%
{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.littlegauss1}), applied
to }n-1\text{ instead of }n\right) \\
&  =\dfrac{\left(  n-1\right)  n}{2}.
\end{align*}

\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{\left(  x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }$
in (\ref{eq.sum.fubini.triangle}) can also be rewritten as $\sum
_{\substack{\left(  x,y\right)  \in\left\{  1,2,3,\ldots\right\}
^{2};\\x+y\leq n}}a_{\left(  x,y\right)  }$.

\item Let us prove (\ref{eq.sum.fubini.triangle}). Indeed, the proof will be
very similar to our proof of (\ref{eq.sum.fubini}) above. Let $S=T_{n}$ and
$W=\left\{  1,2,\ldots,n\right\}  $, and let $f:S\rightarrow W$ be the map
which sends every pair $\left(  x,y\right)  $ to its second entry $y$. Then,
(\ref{eq.sum.sheph.preimg}) shows that%
\begin{equation}
\sum_{s\in T_{n}}a_{s}=\sum_{w\in W}\sum_{s\in f^{-1}\left(  w\right)  }a_{s}.
\label{eq.sum.fubini.triangle.pf.1}%
\end{equation}
But for every given $w\in W$, the set $f^{-1}\left(  w\right)  $ is simply the
set of all pairs $\left(  x,w\right)  $ with $x\in\left\{  1,2,\ldots
,n-w\right\}  $. Thus, for every given $w\in W$, there is a bijection
$g_{w}:\left\{  1,2,\ldots,n-w\right\}  \rightarrow f^{-1}\left(  w\right)  $
given by%
\[
g_{w}\left(  x\right)  =\left(  x,w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}x\in\left\{  1,2,\ldots,n-w\right\}  .
\]
Hence, for every given $w\in W$, we can substitute $g_{w}\left(  x\right)  $
for $s$ in the sum $\sum_{s\in f^{-1}\left(  w\right)  }a_{s}$, and thus
obtain%
\[
\sum_{s\in f^{-1}\left(  w\right)  }a_{s}=\underbrace{\sum_{x\in\left\{
1,2,\ldots,n-w\right\}  }}_{=\sum_{x=1}^{n-w}}\underbrace{a_{g_{w}\left(
x\right)  }}_{\substack{=a_{\left(  x,w\right)  }\\\text{(since }g_{w}\left(
x\right)  =\left(  x,w\right)  \text{)}}}=\sum_{x=1}^{n-w}a_{\left(
x,w\right)  }.
\]
Hence, (\ref{eq.sum.fubini.triangle.pf.1}) becomes%
\[
\sum_{s\in T_{n}}a_{s}=\underbrace{\sum_{w\in W}}_{\substack{=\sum_{w=1}%
^{n}\\\text{(since }W=\left\{  1,2,\ldots,n\right\}  \text{)}}%
}\underbrace{\sum_{s\in f^{-1}\left(  w\right)  }a_{s}}_{=\sum_{x=1}%
^{n-w}a_{\left(  x,w\right)  }}=\sum_{w=1}^{n}\sum_{x=1}^{n-w}a_{\left(
x,w\right)  }=\sum_{y=1}^{n}\sum_{x=1}^{n-y}a_{\left(  x,y\right)  }%
\]
(here, we have renamed the summation index $w$ as $y$ in the outer sum).
Therefore,%
\[
\sum_{y=1}^{n}\sum_{x=1}^{n-y}a_{\left(  x,y\right)  }=\sum_{s\in T_{n}}%
a_{s}=\sum_{\left(  x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }.
\]
Thus, we have proven the second part of the equality
(\ref{eq.sum.fubini.triangle}). The first part can be proven similarly.
\end{itemize}

\item \underline{\textbf{Triangular Fubini's theorem II:}} Here is another
equality similar to (\ref{eq.sum.fubini.triangle}). Let $n\in\mathbb{N}$. Let
$Q_{n}$ be the set $\left\{  \left(  x,y\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}\ \mid\ x\leq y\right\}  $. (For instance, if $n=3$, then
$Q_{n}=Q_{3}=\left\{  \left(  1,1\right)  ,\left(  1,2\right)  ,\left(
1,3\right)  ,\left(  2,2\right)  ,\left(  2,3\right)  ,\left(  3,3\right)
\right\}  $.) Let $a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$ for
each $\left(  x,y\right)  \in Q_{n}$. Then,%
\begin{equation}
\sum_{x=1}^{n}\sum_{y=x}^{n}a_{\left(  x,y\right)  }=\sum_{\left(  x,y\right)
\in Q_{n}}a_{\left(  x,y\right)  }=\sum_{y=1}^{n}\sum_{x=1}^{y}a_{\left(
x,y\right)  }. \label{eq.sum.fubini.triangle2}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item Let us use (\ref{eq.sum.fubini.triangle2}) to compute $\left\vert
Q_{n}\right\vert $. Indeed, we can apply (\ref{eq.sum.fubini.triangle2}) to
$a_{\left(  x,y\right)  }=1$. Thus, we obtain%
\[
\sum_{x=1}^{n}\sum_{y=x}^{n}1=\sum_{\left(  x,y\right)  \in Q_{n}}1=\sum
_{y=1}^{n}\sum_{x=1}^{y}1.
\]
Hence,%
\[
\sum_{y=1}^{n}\sum_{x=1}^{y}1=\sum_{\left(  x,y\right)  \in Q_{n}}1=\left\vert
Q_{n}\right\vert ,
\]
so that%
\[
\left\vert Q_{n}\right\vert =\sum_{y=1}^{n}\underbrace{\sum_{x=1}^{y}1}%
_{=y}=\sum_{y=1}^{n}y=\sum_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }%
{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.littlegauss2})}\right)
.
\]

\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{\left(  x,y\right)  \in Q_{n}}a_{\left(  x,y\right)  }$
in (\ref{eq.sum.fubini.triangle2}) can also be rewritten as $\sum
_{\substack{\left(  x,y\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\x\leq
y}}a_{\left(  x,y\right)  }$. It is also often written as $\sum_{1\leq x\leq
y\leq n}a_{\left(  x,y\right)  }$.

\item The proof of (\ref{eq.sum.fubini.triangle2}) is similar to that of
(\ref{eq.sum.fubini.triangle}).
\end{itemize}

\item \underline{\textbf{Fubini's theorem with a predicate:}} Let $X$ and $Y$
be two finite sets. For every pair $\left(  x,y\right)  \in X\times Y$, let
$\mathcal{A}\left(  x,y\right)  $ be a logical statement. For each $\left(
x,y\right)  \in X\times Y$ satisfying $\mathcal{A}\left(  x,y\right)  $, let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{x\in X}\sum_{\substack{y\in Y;\\\mathcal{A}\left(  x,y\right)
}}a_{\left(  x,y\right)  }=\sum_{\substack{\left(  x,y\right)  \in X\times
Y;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(  x,y\right)  }=\sum_{y\in
Y}\sum_{\substack{x\in X;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(
x,y\right)  }. \label{eq.sum.fubini.predicate}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item For any $n\in\mathbb{N}$ and $m\in\mathbb{N}$, we have%
\[
\sum_{x\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{y\in\left\{
1,2,\ldots,m\right\}  ;\\x+y\text{ is even}}}xy=\sum_{\substack{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  ;\\x+y\text{ is even}}}xy=\sum_{y\in\left\{  1,2,\ldots,m\right\}
}\sum_{\substack{x\in\left\{  1,2,\ldots,n\right\}  ;\\x+y\text{ is even}%
}}xy.
\]
(This follows from (\ref{eq.sum.fubini.predicate}), applied to $X=\left\{
1,2,\ldots,n\right\}  $, $Y=\left\{  1,2,\ldots,m\right\}  $ and
$\mathcal{A}\left(  x,y\right)  =\left(  \text{\textquotedblleft}x+y\text{ is
even\textquotedblright}\right)  $.)
\end{itemize}

\item \underline{\textbf{Interchange of predicates:}} Let $S$ be a finite set.
For every $s\in S$, let $\mathcal{A}\left(  s\right)  $ and $\mathcal{B}%
\left(  s\right)  $ be two equivalent logical statements. (\textquotedblleft
Equivalent\textquotedblright\ means that $\mathcal{A}\left(  s\right)  $ holds
if and only if $\mathcal{B}\left(  s\right)  $ holds.) Let $a_{s}$ be an
element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\sum
_{\substack{s\in S;\\\mathcal{B}\left(  s\right)  }}a_{s}.
\]
(If you regard equivalent logical statements as identical, then you will see
this as a tautology. If not, it is still completely obvious, since the
equivalence of $\mathcal{A}\left(  s\right)  $ with $\mathcal{B}\left(
s\right)  $ shows that $\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)
\right\}  =\left\{  t\in S\ \mid\ \mathcal{B}\left(  t\right)  \right\}  $.)
\end{itemize}

\subsubsection{Definition of $\prod$}

We shall now define the $\prod$ sign. Since the $\prod$ sign is (in many
aspects) analogous to the $\sum$ sign, we shall be brief and confine ourselves
to the bare necessities; we trust the reader to transfer most of what we said
about $\sum$ to the case of $\prod$. In particular, we shall give very few
examples and no proofs.

\begin{itemize}
\item If $S$ is a finite set, and if $a_{s}$ is an element of $\mathbb{A}$ for
each $s\in S$, then $\prod\nolimits_{s\in S}a_{s}$ denotes the product of all
of these elements $a_{s}$. Formally, this product is defined by recursion on
$\left\vert S\right\vert $, as follows:

\begin{itemize}
\item If $S=\varnothing$, then $\prod_{s\in S}a_{s}$ is defined to be $1$.

\item Let $n\in\mathbb{N}$. Assume that we have defined $\prod_{s\in S}a_{s}$
for every finite set $S$ with $\left\vert S\right\vert =n$ (and every choice
of elements $a_{s}$ of $\mathbb{A}$). Now, if $S$ is a finite set with
$\left\vert S\right\vert =n+1$ (and if $a_{s}\in\mathbb{A}$ are chosen for all
$s\in S$), then $\prod_{s\in S}a_{s}$ is defined by picking any $t\in S$ and
setting%
\begin{equation}
\prod_{s\in S}a_{s}=a_{t}\cdot\prod_{s\in S\setminus\left\{  t\right\}  }%
a_{s}. \label{eq.prod.def.1}%
\end{equation}
As for $\sum_{s\in S}a_{s}$, this definition is not obviously legitimate, but
it can be proven to be legitimate nevertheless.
\end{itemize}

\textbf{Examples:}

\begin{itemize}
\item If $S=\left\{  1,2,\ldots,n\right\}  $ (for some $n\in\mathbb{N}$) and
$a_{s}=s$ for every $s\in S$, then $\prod_{s\in S}a_{s}=\prod_{s\in S}%
s=1\cdot2\cdot\cdots\cdot n$. This number $1\cdot2\cdot\cdots\cdot n$ is
denoted by $n!$ and called the \textit{factorial of }$n$.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The product $\prod_{s\in S}a_{s}$ is usually pronounced
\textquotedblleft product of the $a_{s}$ over all $s\in S$\textquotedblright%
\ or \textquotedblleft product of the $a_{s}$ with $s$ ranging over
$S$\textquotedblright\ or \textquotedblleft product of the $a_{s}$ with $s$
running through all elements of $S$\textquotedblright. The letter
\textquotedblleft$s$\textquotedblright\ in the product is called the
\textquotedblleft product index\textquotedblright, and its exact choice is
immaterial, as long as it does not already have a different meaning outside of
the product. The sign $\prod$ itself is called \textquotedblleft the product
sign\textquotedblright\ or \textquotedblleft the $\prod$
sign\textquotedblright. The numbers $a_{s}$ are called the \textit{factors} of
the product $\prod_{s\in S}a_{s}$. More precisely, for any given $t\in S$, we
can refer to the number $a_{t}$ as the \textquotedblleft factor corresponding
to the index $t$\textquotedblright\ (or as the \textquotedblleft factor for
$s=t$\textquotedblright, or as the \textquotedblleft factor for $t$%
\textquotedblright) of the product $\prod_{s\in S}a_{s}$.

\item When the set $S$ is empty, the product $\prod_{s\in S}a_{s}$ is called
an \textit{empty product}. Our definition implies that any empty product is
$1$. This convention is used throughout mathematics, except in rare occasions
where a slightly subtler version of it is used\footnote{Just as with sums, the
subtlety lies in the fact that mathematicians sometimes want an empty product
to be not the integer $1$ but the unity of some ring. As before, this does not
matter for us right now.}.

\item If $a\in\mathbb{A}$ and $n\in\mathbb{N}$, then the $n$-th power of $a$
(written $a^{n}$) is defined by%
\[
a^{n}=\underbrace{a\cdot a\cdot\cdots\cdot a}_{n\text{ times}}=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }a.
\]
Thus, $a^{0}$ is an empty product, and therefore equal to $1$. This holds for
any $a\in\mathbb{A}$, including $0$; thus, $0^{0}=1$. \textbf{There is nothing
controversial about the equality }$0^{0}=1$; it is a consequence of the only
reasonable definition of the $n$-th power of a number. If anyone tells you
that $0^{0}$ is \textquotedblleft undefined\textquotedblright\ or
\textquotedblleft indeterminate\textquotedblright\ or \textquotedblleft can be
$0$ or $1$ or anything, depending on the context\textquotedblright, do not
listen to them.\footnote{I am talking about the \textbf{number} $0^{0}$ here.
There is also something called \textquotedblleft the
\href{https://en.wikipedia.org/wiki/Indeterminate form}{indeterminate form}
$0^{0}$\textquotedblright, which is a much different story.}

\item The product index (just like a summation index) needs not be a single
letter; it can be a pair or a triple, for example.

\item Mathematicians don't seem to have reached an agreement on the operator
precedence of the $\prod$ sign. My convention is that the product sign has
higher precedence than the plus sign (so an expression like $\prod_{s\in
S}a_{s}+b$ must be read as $\left(  \prod_{s\in S}a_{s}\right)  +b$, and not
as $\prod_{s\in S}\left(  a_{s}+b\right)  $); this is, of course, in line with
the standard convention that multiplication-like operations have higher
precedence than addition-like operations (\textquotedblleft
PEMDAS\textquotedblright). Be warned that some authors disagree even with this
convention. I strongly advise against writing things like $\prod_{s\in S}%
a_{s}b$, since it might mean both $\left(  \prod_{s\in S}a_{s}\right)  b$ and
$\prod_{s\in S}\left(  a_{s}b\right)  $ depending on the weather. In
particular, I advise against writing things like $\prod_{s\in S}a_{s}%
\cdot\prod_{s\in S}b_{s}$ without parentheses (although I do use a similar
convention for sums, namely $\sum_{s\in S}a_{s}+\sum_{s\in S}b_{s}$, and I
find it to be fairly harmless). These rules are not carved in stone, and you
should use whatever conventions make \textbf{you} safe from ambiguity; either
way, you should keep in mind that other authors make different choices.

\item We have required the set $S$ to be finite when defining $\prod_{s\in
S}a_{s}$. Such products are not generally defined when $S$ is infinite.
However, \textbf{some} infinite products can be made sense of. The simplest
case is when the set $S$ might be infinite, but only finitely many among the
$a_{s}$ are distinct from $1$. In this case, we can define $\prod_{s\in
S}a_{s}$ simply by discarding the factors which equal $1$ and multiplying the
finitely many remaining factors. Other situations in which infinite products
make sense appear in analysis and in topological algebra.

\item The product $\prod_{s\in S}a_{s}$ always belongs to $\mathbb{A}$.
\end{itemize}

\item A slightly more complicated version of the product sign is the
following: Let $S$ be a finite set, and let $\mathcal{A}\left(  s\right)  $ be
a logical statement defined for every $s\in S$. For each $s\in S$ satisfying
$\mathcal{A}\left(  s\right)  $, let $a_{s}$ be an element of $\mathbb{A}$.
Then, the product $\prod_{\substack{s\in S;\\\mathcal{A}\left(  s\right)
}}a_{s}$ is defined by%
\[
\prod_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\prod
_{s\in\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)  \right\}  }a_{s}.
\]


\item Finally, here is the simplest version of the product sign: Let $u$ and
$v$ be two integers. As before, we understand the set $\left\{  u,u+1,\ldots
,v\right\}  $ to be empty when $u>v$. Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in\left\{  u,u+1,\ldots,v\right\}  $. Then,
$\prod_{s=u}^{v}a_{s}$ is defined by%
\[
\prod_{s=u}^{v}a_{s}=\prod_{s\in\left\{  u,u+1,\ldots,v\right\}  }a_{s}.
\]


\textbf{Examples:}

\begin{itemize}
\item We have $\prod_{s=1}^{n}s=1\cdot2\cdot\cdots\cdot n=n!$ for each
$n\in\mathbb{N}$.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The product $\prod_{s=u}^{v}a_{s}$ is usually pronounced
\textquotedblleft product of the $a_{s}$ for all $s$ from $u$ to $v$
(inclusive)\textquotedblright. It is often written $a_{u}\cdot a_{u+1}%
\cdot\cdots\cdot a_{v}$ (or just $a_{u}a_{u+1}\cdots a_{v}$), but this latter
notation has the same drawbacks as the similar notation $a_{u}+a_{u+1}%
+\cdots+a_{v}$ for $\sum_{s=u}^{v}a_{s}$.

\item The product $\prod_{s=u}^{v}a_{s}$ is said to be \textit{empty} whenever
$u>v$. As with sums, it does not matter how much smaller $v$ is than $u$; as
long as $v$ is smaller than $u$, the product is empty and equals $1$.
\end{itemize}
\end{itemize}

Thus we have introduced the main three forms of the product sign.

\subsubsection{Properties of $\prod$}

Now, let me summarize the most important properties of the $\prod$ sign. These
properties mirror the properties of $\sum$ discussed before; thus, I will
again be brief.

\begin{itemize}
\item \underline{\textbf{Splitting-off:}} Let $S$ be a finite set. Let $t\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=a_{t}\cdot\prod_{s\in S\setminus\left\{  t\right\}  }%
a_{s}.
\]


\item \underline{\textbf{Splitting:}} Let $S$ be a finite set. Let $X$ and $Y
$ be two subsets of $S$ such that $X\cap Y=\varnothing$ and $X\cup Y=S$.
(Equivalently, $X$ and $Y$ are two subsets of $S$ such that each element of
$S$ lies in \textbf{exactly} one of $X$ and $Y$.) Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\left(  \prod_{s\in X}a_{s}\right)  \cdot\left(
\prod_{s\in Y}a_{s}\right)  .
\]


\item \underline{\textbf{Splitting using a predicate:}} Let $S$ be a finite
set. Let $\mathcal{A}\left(  s\right)  $ be a logical statement for each $s\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\left(  \prod_{\substack{s\in S; \\\mathcal{A}\left(
s\right)  }}a_{s}\right)  \cdot\left(  \prod_{\substack{s\in S; \\\text{not
}\mathcal{A}\left(  s\right)  }}a_{s}\right)  .
\]


\item \underline{\textbf{Multiplying equal values:}} Let $S$ be a finite set.
Let $a$ be an element of $\mathbb{A}$. Then,%
\[
\prod_{s\in S}a=a^{\left\vert S\right\vert }.
\]


\item \underline{\textbf{Splitting a factor:}} Let $S$ be a finite set. For
every $s\in S$, let $a_{s}$ and $b_{s}$ be elements of $\mathbb{A}$. Then,%
\begin{equation}
\prod_{s\in S}\left(  a_{s}b_{s}\right)  =\left(  \prod_{s\in S}a_{s}\right)
\cdot\left(  \prod_{s\in S}b_{s}\right)  . \label{eq.prod.linear1}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item Here is a frequently used particular case of (\ref{eq.prod.linear1}):
Let $S$ be a finite set. For every $s\in S$, let $b_{s}$ be an element of
$\mathbb{A}$. Let $a$ be an element of $\mathbb{A}$. Then,
(\ref{eq.prod.linear1}) (applied to $a_{s}=a$) yields%
\begin{equation}
\prod_{s\in S}\left(  ab_{s}\right)  =\underbrace{\left(  \prod_{s\in
S}a\right)  }_{=a^{\left\vert S\right\vert }}\cdot\left(  \prod_{s\in S}%
b_{s}\right)  =a^{\left\vert S\right\vert }\cdot\left(  \prod_{s\in S}%
b_{s}\right)  . \label{eq.prod.linear1.ex1}%
\end{equation}


\item Here is an even further particular case: Let $S$ be a finite set. For
every $s\in S$, let $b_{s}$ be an element of $\mathbb{A}$. Then,%
\[
\prod_{s\in S}\underbrace{\left(  -b_{s}\right)  }_{=\left(  -1\right)  b_{s}%
}=\prod_{s\in S}\left(  \left(  -1\right)  b_{s}\right)  =\left(  -1\right)
^{\left\vert S\right\vert }\cdot\left(  \prod_{s\in S}b_{s}\right)
\]
(by (\ref{eq.prod.linear1.ex1}), applied to $a=-1$).
\end{itemize}

\item \underline{\textbf{Factoring out an exponent:}} Let $S$ be a finite set.
For every $s\in S$, let $a_{s}$ be an element of $\mathbb{A}$. Also, let
$\lambda\in\mathbb{N}$. Then,%
\[
\prod_{s\in S}a_{s}^{\lambda}=\left(  \prod_{s\in S}a_{s}\right)  ^{\lambda}.
\]


\item \underline{\textbf{Ones multiply to one:}} Let $S$ be a finite set.
Then,%
\[
\prod_{s\in S}1=1.
\]


\item \underline{\textbf{Dropping ones:}} Let $S$ be a finite set. Let $a_{s}$
be an element of $\mathbb{A}$ for each $s\in S$. Let $T$ be a subset of $S$
such that every $s\in T$ satisfies $a_{s}=1$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{s\in S\setminus T}a_{s}.
\]


\item \underline{\textbf{Renaming the index:}} Let $S$ be a finite set. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{t\in S}a_{t}.
\]


\item \underline{\textbf{Substituting the index I:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{t}$
be an element of $\mathbb{A}$ for each $t\in T$. Then,%
\[
\prod_{t\in T}a_{t}=\prod_{s\in S}a_{f\left(  s\right)  }.
\]


\item \underline{\textbf{Substituting the index II:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{s}$
be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{t\in T}a_{f^{-1}\left(  t\right)  }.
\]


\item \underline{\textbf{Telescoping products:}} Let $u$ and $v$ be two
integers such that $u-1\leq v$. Let $a_{s}$ be an element of $\mathbb{A}$ for
each $s\in\left\{  u-1,u,\ldots,v\right\}  $. Then,%
\begin{equation}
\prod_{s=u}^{v}\dfrac{a_{s}}{a_{s-1}}=\dfrac{a_{v}}{a_{u-1}}
\label{eq.prod.telescope}%
\end{equation}
(provided that $a_{s-1}\neq0$ for all $s\in\left\{  u,u+1,\ldots,v\right\}  $).

\textbf{Examples:}

\begin{itemize}
\item Let $n$ be a positive integer. Then,%
\begin{align*}
\prod_{s=2}^{n}\underbrace{\left(  1-\dfrac{1}{s}\right)  }_{=\dfrac{s-1}%
{s}=\dfrac{1/s}{1/\left(  s-1\right)  }}  &  =\prod_{s=2}^{n}\dfrac
{1/s}{1/\left(  s-1\right)  }=\dfrac{1/n}{1/\left(  2-1\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.prod.telescope}), applied to
}u=2\text{, }v=n\text{ and }a_{s}=1/s\right) \\
&  =\dfrac{1}{n}.
\end{align*}

\end{itemize}

\item \underline{\textbf{Splitting a product by a value of a function:}} Let
$S$ be a finite set. Let $W$ be a set. Let $f:S\rightarrow W$ be a map. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{w\in W}\prod_{\substack{s\in S;\\f\left(  s\right)
=w}}a_{s}.
\]
(The right hand side is to be read as $\prod_{w\in W}\left(  \prod
_{\substack{s\in S;\\f\left(  s\right)  =w}}a_{s}\right)  $.)

\item \underline{\textbf{Splitting a product into subproducts:}} Let $S$ be a
finite set. Let $S_{1},S_{2},\ldots,S_{n}$ be finitely many subsets of $S$.
Assume that these subsets $S_{1},S_{2},\ldots,S_{n}$ are pairwise disjoint
(i.e., we have $S_{i}\cap S_{j}=\varnothing$ for any two distinct elements $i$
and $j$ of $\left\{  1,2,\ldots,n\right\}  $) and their union is $S$. (Thus,
every element of $S$ lies in precisely one of the subsets $S_{1},S_{2}%
,\ldots,S_{n}$.) Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$.
Then,%
\[
\prod_{s\in S}a_{s}=\prod_{w=1}^{n}\prod_{s\in S_{w}}a_{s}.
\]


\item \underline{\textbf{Fubini's theorem (interchanging the order of
multiplication):}} Let $X$ and $Y$ be two finite sets. Let $a_{\left(
x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in
X\times Y$. Then,%
\[
\prod_{x\in X}\prod_{y\in Y}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in X\times Y}a_{\left(  x,y\right)  }=\prod_{y\in Y}\prod_{x\in
X}a_{\left(  x,y\right)  }.
\]


In particular, if $n$ and $m$ are two elements of $\mathbb{N}$, and if
$a_{\left(  x,y\right)  }$ is an element of $\mathbb{A}$ for each $\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  $, then%
\[
\prod_{x=1}^{n}\prod_{y=1}^{m}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  }a_{\left(  x,y\right)  }=\prod_{y=1}^{m}\prod_{x=1}^{n}a_{\left(
x,y\right)  }.
\]


\item \underline{\textbf{Triangular Fubini's theorem I:}} Let $n\in\mathbb{N}%
$. Let $T_{n}$ be the set \newline$\left\{  \left(  x,y\right)  \in\left\{
1,2,3,\ldots\right\}  ^{2}\ \mid\ x+y\leq n\right\}  $. Let $a_{\left(
x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in
T_{n}$. Then,%
\[
\prod_{x=1}^{n}\prod_{y=1}^{n-x}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }=\prod_{y=1}^{n}\prod
_{x=1}^{n-y}a_{\left(  x,y\right)  }.
\]


\item \underline{\textbf{Triangular Fubini's theorem II:}} Let $n\in
\mathbb{N}$. Let $Q_{n}$ be the set \newline$\left\{  \left(  x,y\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}\ \mid\ x\leq y\right\}  $. Let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(
x,y\right)  \in Q_{n}$. Then,%
\[
\prod_{x=1}^{n}\prod_{y=x}^{n}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in Q_{n}}a_{\left(  x,y\right)  }=\prod_{y=1}^{n}\prod_{x=1}%
^{y}a_{\left(  x,y\right)  }.
\]


\item \underline{\textbf{Fubini's theorem with a predicate:}} Let $X$ and $Y$
be two finite sets. For every pair $\left(  x,y\right)  \in X\times Y$, let
$\mathcal{A}\left(  x,y\right)  $ be a logical statement. For each $\left(
x,y\right)  \in X\times Y$ satisfying $\mathcal{A}\left(  x,y\right)  $, let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$. Then,%
\[
\prod_{x\in X}\prod_{\substack{y\in Y;\\\mathcal{A}\left(  x,y\right)
}}a_{\left(  x,y\right)  }=\prod_{\substack{\left(  x,y\right)  \in X\times
Y;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(  x,y\right)  }=\prod_{y\in
Y}\prod_{\substack{x\in X;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(
x,y\right)  }.
\]


\item \underline{\textbf{Interchange of predicates:}} Let $S$ be a finite set.
For every $s\in S$, let $\mathcal{A}\left(  s\right)  $ and $\mathcal{B}%
\left(  s\right)  $ be two equivalent logical statements. (\textquotedblleft
Equivalent\textquotedblright\ means that $\mathcal{A}\left(  s\right)  $ holds
if and only if $\mathcal{B}\left(  s\right)  $ holds.) Let $a_{s}$ be an
element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\prod
_{\substack{s\in S;\\\mathcal{B}\left(  s\right)  }}a_{s}.
\]

\end{itemize}

\subsection{\label{sect.polynomials-emergency}Polynomials: a brief
introduction}

As I have already mentioned in the above list of prerequisites, the notion of
polynomials (in one and in several indeterminates) will be used in these
notes. Most likely, the reader already has at least a vague understanding of
this notion (e.g., from high school); this vague understanding is probably
sufficient for reading most of these notes. But polynomials are one of the
most important notions in algebra (if not to say in mathematics), and the
reader will likely encounter them over and over; sooner or later, it will
happen that the vague understanding is not sufficient and some subtleties do
matter. For that reason, anyone serious about doing abstract algebra should
know a complete and correct definition of polynomials and have some experience
working with it. I shall not give a complete definition of the most general
notion of polynomials in these notes, but I will comment on some of the
subtleties and define an important special case (that of polynomials in one
variable with rational coefficients) in the present section. A reader is
probably best advised to skip this section on their first read.

It is not easy to find a good (formal and sufficiently general) treatment of
polynomials in textbooks. Various authors tend to skimp on subtleties and
technical points such as the notion of an \textquotedblleft
indeterminate\textquotedblright, or the precise meaning of \textquotedblleft
formal expression\textquotedblright\ in the slogan \textquotedblleft a
polynomial is a formal expression\textquotedblright\ (the best texts do not
use this vague slogan at all), or the definition of the degree of the zero
polynomial, or the difference between regarding polynomials as sequences
(which is the classical viewpoint and particularly useful for polynomials in
one variable) and regarding polynomials as elements of a monoid ring (which is
important in the case of several variables, since it allows us to regard the
polynomial rings $\mathbb{Q}\left[  X\right]  $ and $\mathbb{Q}\left[
Y\right]  $ as two distinct subrings of $\mathbb{Q}\left[  X,Y\right]  $).
They also tend to take some questionable shortcuts, such as defining
polynomials in $n$ variables (by induction over $n$) as one-variable
polynomials over the ring of $\left(  n-1\right)  $-variable polynomials (this
shortcut has several shortcomings, such as making the symmetric role of the
$n$ variables opaque, and functioning only for finitely many variables).

More often than not, the polynomials we will be using will be polynomials in
one variable. These are usually handled well in good books on abstract algebra
-- e.g., in \cite[\S 4.5]{Walker87}, in \cite[Appendix G]{Hungerford}, in
\cite[Chapter III, \S 5]{Hungerford-03}, in \cite[\S 4.1, \S 4.2]%
{HoffmanKunze} (although in \cite[\S 4.1, \S 4.2]{HoffmanKunze}, only
polynomials over fields are studied, but the definition applies to commutative
rings mutatis mutandis), and in \cite[Chapter III, \S 6]{BirkMac}. Most of
these treatments rely on the notion of a \textit{commutative ring}, which is
not difficult but somewhat abstract (I shall introduce it below in Section
\ref{sect.commring}).

Let me give a brief survey of the notion of univariate polynomials (i.e.,
polynomials in one variable); I shall define them as sequences. For the sake
of simplicity, I shall only talk of polynomials with rational coefficients.
Similarly, one can obtain polynomials with integer coefficients, with real
coefficients, or with complex coefficients; of course, one then has to replace
each \textquotedblleft$\mathbb{Q}$\textquotedblright\ by a \textquotedblleft%
$\mathbb{Z}$\textquotedblright, a \textquotedblleft$\mathbb{R}$%
\textquotedblright\ or a \textquotedblleft$\mathbb{C}$\textquotedblright.

The rough idea behind the definition of a polynomial is that a polynomial with
rational coefficients should be a \textquotedblleft formal
expression\textquotedblright\ which is built out of rational numbers, an
\textquotedblleft indeterminate\textquotedblright\ $X$ as well as addition,
subtraction and multiplication signs, such as $X^{4}-27X+\dfrac{3}{2}$ or
$-X^{3}+2X+1$ or $\dfrac{1}{3}\left(  X-3\right)  \cdot X^{2}$ or
$X^{4}+7X^{3}\left(  X-2\right)  $ or $-15$. We have not explicitly allowed
powers, but we understand $X^{n}$ to mean the product $\underbrace{XX\cdots
X}_{n\text{ times}}$ (or $1$ when $n=0$). Notice that division is not allowed,
so we cannot get $\dfrac{X}{X+1}$ (but we can get $\dfrac{3}{2}X$, because
$\dfrac{3}{2}$ is a rational number). Notice also that a polynomial can be a
single rational number, since we never said that $X$ must necessarily be used;
for instance, $-15$ and $0$ are polynomials.

This is, of course, not a valid definition. One problem with it that it does
not explain what a \textquotedblleft formal expression\textquotedblright\ is.
For starters, we want an expression that is well-defined -- i.e., that we can
substitute a rational number for $X$ and obtain a valid term. For example,
$X-+\cdot5$ is not well-defined, so it does not fit our bill; neither is the
\textquotedblleft empty expression\textquotedblright. Furthermore, when do we
want two \textquotedblleft formal expressions\textquotedblright\ to be viewed
as one and the same polynomial? Do we want to equate $X\left(  X+2\right)  $
with $X^{2}+2X$ ? Do we want to equate $0X^{3}+2X+1$ with $2X+1$ ? The answer
is \textquotedblleft yes\textquotedblright\ both times, but a general rule is
not easy to give if we keep talking of \textquotedblleft formal
expressions\textquotedblright.

We \textit{could} define two polynomials $p\left(  X\right)  $ and $q\left(
X\right)  $ to be equal if and only if, for every number $\alpha\in\mathbb{Q}%
$, the values $p\left(  \alpha\right)  $ and $q\left(  \alpha\right)  $
(obtained by substituting $\alpha$ for $X$ in $p$ and in $q$, respectively)
are equal. This would be tantamount to treating polynomials as
\textit{functions}: it would mean that we identify a polynomial $p\left(
X\right)  $ with the function $\mathbb{Q}\rightarrow\mathbb{Q},\ \alpha\mapsto
p\left(  \alpha\right)  $. Such a definition would work well as long as we
would do only rather basic things with it\footnote{And some authors, such as
Axler in \cite[Chapter 4]{Axler}, do use this definition.}, but as soon as we
would try to go deeper, we would encounter technical issues which would make
it inadequate and painful\footnote{Here are the three most important among
these issues:
\par
\begin{itemize}
\item One of the strengths of polynomials is that we can evaluate them not
only at numbers, but also at many other things, e.g., at square matrices:
Evaluating the polynomial $X^{2}-3X$ at the square matrix $\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  $ gives $\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  ^{2}-3\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
-5 & 0\\
0 & -5
\end{array}
\right)  $. However, a function must have a well-defined domain, and does not
make sense outside of this domain. So, if the polynomial $X^{2}-3X$ is
regarded as the function $\mathbb{Q}\rightarrow\mathbb{Q},\ \alpha
\mapsto\alpha^{2}-3\alpha$, then it makes no sense to evaluate this polynomial
at the matrix $\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  $, just because this matrix does not lie in the domain $\mathbb{Q}$
of the function. We could, of course, extend the domain of the function to
(say) the set of square matrices over $\mathbb{Q}$, but then we would still
have the same problem with other things that we want to evaluate polynomials
at. At some point we want to be able to evaluate polynomials at functions and
at other polynomials, and if we would try to achieve this by extending the
domain, we would have to do this over and over, because each time we extend
the domain, we get even more polynomials to evaluate our polynomials at; thus,
the definition would be eternally \textquotedblleft hunting its own
tail\textquotedblright! (We could resolve this difficulty by defining
polynomials as \textit{natural transformations} in the sense of category
theory. I do not want to even go into this definition here, as it would take
several pages to properly introduce. At this point, it is not worth the
hassle.)
\par
\item Let $p\left(  X\right)  $ be a polynomial with real coefficients. Then,
it should be obvious that $p\left(  X\right)  $ can also be viewed as a
polynomial with complex coefficients: For instance, if $p\left(  X\right)  $
was defined as $3X+\dfrac{7}{2}X\left(  X-1\right)  $, then we can view the
numbers $3$, $\dfrac{7}{2}$ and $-1$ appearing in its definition as complex
numbers, and thus get a polynomial with complex coefficients. But wait! What
if two polynomials $p\left(  X\right)  $ and $q\left(  X\right)  $ are equal
when viewed as polynomials with real coefficients, but when viewed as
polynomials with complex coefficients become distinct (because when we view
them as polynomials with complex coefficients, their domains become extended,
and a new complex $\alpha$ might perhaps no longer satisfy $p\left(
\alpha\right)  =q\left(  \alpha\right)  $ )? This does not actually happen,
but ruling this out is not obvious if you regard polynomials as functions.
\par
\item (This requires some familiarity with finite fields:) Treating
polynomials as functions works halfway well for polynomials with integer,
rational, real and complex coefficients. But we will eventually want to
consider polynomials with coefficients in any arbitrary commutative ring
$\mathbb{K}$. An example for a commutative ring $\mathbb{K}$ is the finite
field $\mathbb{F}_{p}$ with $p$ elements, where $p$ is a prime. If we define
polynomials with coefficients in $\mathbb{F}_{p}$ as functions $\mathbb{F}%
_{p}\rightarrow\mathbb{F}_{p}$, then we really run into problems; for example,
the polynomials $X$ and $X^{p}$ over this field become identical as functions!
\end{itemize}
}. Also, if we equated polynomials with the functions they describe, then we
would waste the word \textquotedblleft polynomial\textquotedblright\ on a
concept (a function described by a polynomial) that already has a word for it
(namely, \textit{polynomial function}).

The preceding paragraphs should have convinced you that it is worth defining
\textquotedblleft polynomials\textquotedblright\ in a way that, on the one
hand, conveys the concept that they are more \textquotedblleft formal
expressions\textquotedblright\ than \textquotedblleft
functions\textquotedblright, but on the other hand, is less nebulous than
\textquotedblleft formal expression\textquotedblright. Here is one such definition:

\begin{definition}
\label{def.polynomial-univar}\textbf{(a)} A \textit{univariate polynomial with
rational coefficients} means a sequence $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  \in\mathbb{Q}^{\infty}$ of elements of $\mathbb{Q}$ such that%
\begin{equation}
\text{all but finitely many }k\in\mathbb{N}\text{ satisfy }p_{k}=0.
\label{eq.def.polynomial-univar.finite}%
\end{equation}
Here, the phrase \textquotedblleft all but finitely many $k\in\mathbb{N}$
satisfy $p_{k}=0$\textquotedblright\ means \textquotedblleft there exists some
finite subset $J$ of $\mathbb{N}$ such that every $k\in\mathbb{N}\setminus J$
satisfies $p_{k}=0$\textquotedblright. (See Definition \ref{def.allbutfin} for
the general definition of \textquotedblleft all but finitely
many\textquotedblright, and Section \ref{sect.infperm} for some practice with
this concept.)

For the remainder of this definition, \textquotedblleft univariate polynomial
with rational coefficients\textquotedblright\ will be abbreviated as
\textquotedblleft polynomial\textquotedblright.

For example, the sequences $\left(  0,0,0,\ldots\right)  $, $\left(
1,3,5,0,0,0,\ldots\right)  $, $\left(  4,0,-\dfrac{2}{3},5,0,0,0,\ldots
\right)  $, $\left(  0,-1,\dfrac{1}{2},0,0,0,\ldots\right)  $ (where the
\textquotedblleft$\ldots$\textquotedblright\ stand for infinitely many zeroes)
are polynomials, but the sequence $\left(  1,1,1,\ldots\right)  $ (where the
\textquotedblleft$\ldots$\textquotedblright\ stands for infinitely many $1$'s)
is not (since it does not satisfy (\ref{eq.def.polynomial-univar.finite})).

So we have defined a polynomial as an infinite sequence of rational numbers
with a certain property. So far, this does not seem to reflect any intuition
of polynomials as \textquotedblleft formal expressions\textquotedblright.
However, we shall soon (namely, in Definition \ref{def.polynomial-univar}
\textbf{(j)}) identify the polynomial $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  \in\mathbb{Q}^{\infty}$ with the \textquotedblleft formal
expression\textquotedblright\ $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ (this is an
infinite sum, but due to (\ref{eq.def.polynomial-univar.finite}) all but its
first few terms are $0$ and thus can be neglected). For instance, the
polynomial $\left(  1,3,5,0,0,0,\ldots\right)  $ will be identified with the
\textquotedblleft formal expression\textquotedblright\ $1+3X+5X^{2}%
+0X^{3}+0X^{4}+0X^{5}+\cdots=1+3X+5X^{2}$. Of course, we cannot do this
identification right now, since we do not have a reasonable definition of $X$.

\textbf{(b)} We let $\mathbb{Q}\left[  X\right]  $ denote the set of all
univariate polynomials with rational coefficients. Given a polynomial
$p=\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $,
we denote the numbers $p_{0},p_{1},p_{2},\ldots$ as the \textit{coefficients}
of $p$. More precisely, for every $i\in\mathbb{N}$, we shall refer to $p_{i}$
as the $i$\textit{-th coefficient} of $p$. (Do not forget that we are counting
from $0$ here: any polynomial \textquotedblleft begins\textquotedblright\ with
its $0$-th coefficient.) The $0$-th coefficient of $p$ is also known as the
\textit{constant term} of $p$.

Instead of \textquotedblleft the $i$-th coefficient of $p$\textquotedblright,
we often also say \textquotedblleft the \textit{coefficient before }$X^{i}%
$\textit{ of }$p$\textquotedblright\ or \textquotedblleft the
\textit{coefficient of }$X^{i}$ \textit{in }$p$\textquotedblright.

Thus, any polynomial $p\in\mathbb{Q}\left[  X\right]  $ is the sequence of its coefficients.

\textbf{(c)} We denote the polynomial $\left(  0,0,0,\ldots\right)
\in\mathbb{Q}\left[  X\right]  $ by $\mathbf{0}$. We will also write $0$ for
it when no confusion with the number $0$ is possible. The polynomial
$\mathbf{0}$ is called the \textit{zero polynomial}. A polynomial
$p\in\mathbb{Q}\left[  X\right]  $ is said to be \textit{nonzero} if
$p\neq\mathbf{0}$.

\textbf{(d)} We denote the polynomial $\left(  1,0,0,0,\ldots\right)
\in\mathbb{Q}\left[  X\right]  $ by $\mathbf{1}$. We will also write $1$ for
it when no confusion with the number $1$ is possible.

\textbf{(e)} For any $\lambda\in\mathbb{Q}$, we denote the polynomial $\left(
\lambda,0,0,0,\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ by
$\operatorname*{const}\lambda$. We call it the \textit{constant polynomial
with value }$\lambda$. It is often useful to identify $\lambda\in\mathbb{Q}$
with $\operatorname*{const}\lambda\in\mathbb{Q}\left[  X\right]  $. Notice
that $\mathbf{0}=\operatorname*{const}0$ and $\mathbf{1}=\operatorname*{const}%
1$.

\textbf{(f)} Now, let us define the sum, the difference and the product of two
polynomials. Indeed, let $a=\left(  a_{0},a_{1},a_{2},\ldots\right)
\in\mathbb{Q}\left[  X\right]  $ and $b=\left(  b_{0},b_{1},b_{2}%
,\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ be two polynomials. Then, we
define three polynomials $a+b$, $a-b$ and $a\cdot b$ in $\mathbb{Q}\left[
X\right]  $ by%
\begin{align*}
a+b  &  =\left(  a_{0}+b_{0},a_{1}+b_{1},a_{2}+b_{2},\ldots\right)  ;\\
a-b  &  =\left(  a_{0}-b_{0},a_{1}-b_{1},a_{2}-b_{2},\ldots\right)  ;\\
a\cdot b  &  =\left(  c_{0},c_{1},c_{2},\ldots\right)  ,
\end{align*}
where%
\[
c_{k}=\sum_{i=0}^{k}a_{i}b_{k-i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
k\in\mathbb{N}.
\]
We call $a+b$ the \textit{sum} of $a$ and $b$; we call $a-b$ the
\textit{difference} of $a$ and $b$; we call $a\cdot b$ the \textit{product} of
$a$ and $b$. We abbreviate $a\cdot b$ by $ab$.

For example,%
\begin{align*}
\left(  1,2,2,0,0,\ldots\right)  +\left(  3,0,-1,0,0,0,\ldots\right)   &
=\left(  4,2,1,0,0,0,\ldots\right)  ;\\
\left(  1,2,2,0,0,\ldots\right)  -\left(  3,0,-1,0,0,0,\ldots\right)   &
=\left(  -2,2,3,0,0,0,\ldots\right)  ;\\
\left(  1,2,2,0,0,\ldots\right)  \cdot\left(  3,0,-1,0,0,0,\ldots\right)   &
=\left(  3,6,5,-2,-2,0,0,0,\ldots\right)  .
\end{align*}


The definition of $a+b$ essentially says that \textquotedblleft polynomials
are added coefficientwise\textquotedblright\ (i.e., in order to obtain the sum
of two polynomials $a$ and $b$, it suffices to add each coefficient of $a$ to
the corresponding coefficient of $b$). Similarly, the definition of $a-b$ says
the same thing about subtraction. The definition of $a\cdot b$ is more
surprising. However, it loses its mystique when we identify the polynomials
$a$ and $b$ with the \textquotedblleft formal expressions\textquotedblright%
\ $a_{0}+a_{1}X+a_{2}X^{2}+\cdots$ and $b_{0}+b_{1}X+b_{2}X^{2}+\cdots$
(although, at this point, we do not know what these expressions really mean);
indeed, it simply says that
\[
\left(  a_{0}+a_{1}X+a_{2}X^{2}+\cdots\right)  \left(  b_{0}+b_{1}X+b_{2}%
X^{2}+\cdots\right)  =c_{0}+c_{1}X+c_{2}X^{2}+\cdots,
\]
where $c_{k}=\sum_{i=0}^{k}a_{i}b_{k-i}$ for every $k\in\mathbb{N}$. This is
precisely what one would expect, because if you expand $\left(  a_{0}%
+a_{1}X+a_{2}X^{2}+\cdots\right)  \left(  b_{0}+b_{1}X+b_{2}X^{2}%
+\cdots\right)  $ using the distributive law and collect equal powers of $X$,
then you get precisely $c_{0}+c_{1}X+c_{2}X^{2}+\cdots$. Thus, the definition
of $a\cdot b$ has been tailored to make the distributive law hold.

(By the way, why is $a\cdot b$ a polynomial? That is, why does it satisfy
(\ref{eq.def.polynomial-univar.finite}) ? The proof is easy, but we omit it.)

Addition, subtraction and multiplication of polynomials satisfy some of the
same rules as addition, subtraction and multiplication of numbers. For
example, the commutative laws $a+b=b+a$ and $ab=ba$ are valid for polynomials
just as they are for numbers; same holds for the associative laws $\left(
a+b\right)  +c=a+\left(  b+c\right)  $ and $\left(  ab\right)  c=a\left(
bc\right)  $ and the distributive laws $\left(  a+b\right)  c=ac+bc$ and
$a\left(  b+c\right)  =ab+ac$.

The set $\mathbb{Q}\left[  X\right]  $, endowed with the operations $+$ and
$\cdot$ just defined, and with the elements $\mathbf{0}$ and $\mathbf{1}$, is
a commutative ring (where we are using the notations of Definition
\ref{def.commring}). It is called the \textit{(univariate) polynomial ring
over }$\mathbb{Q}$.

\textbf{(g)} Let $a=\left(  a_{0},a_{1},a_{2},\ldots\right)  \in
\mathbb{Q}\left[  X\right]  $ and $\lambda\in\mathbb{Q}$. Then, $\lambda a$
denotes the polynomial $\left(  \lambda a_{0},\lambda a_{1},\lambda
a_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $. (This equals the
polynomial $\left(  \operatorname*{const}\lambda\right)  \cdot a$; thus,
identifying $\lambda$ with $\operatorname*{const}\lambda$ does not cause any
inconsistencies here.)

\textbf{(h)} If $p=\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}%
\left[  X\right]  $ is a nonzero polynomial, then the \textit{degree} of $p$
is defined to be the maximum $i\in\mathbb{N}$ satisfying $p_{i}\neq0$. If
$p\in\mathbb{Q}\left[  X\right]  $ is the zero polynomial, then the degree of
$p$ is defined to be $-\infty$. (Here, $-\infty$ is just a fancy symbol, not a
number.) For example, $\deg\left(  1,4,0,-1,0,0,0,\ldots\right)  =3$.

\textbf{(i)} If $a=\left(  a_{0},a_{1},a_{2},\ldots\right)  \in\mathbb{Q}%
\left[  X\right]  $ and $n\in\mathbb{N}$, then a polynomial $a^{n}%
\in\mathbb{Q}\left[  X\right]  $ is defined to be the product
$\underbrace{aa\cdots a}_{n\text{ times}}$. (This is understood to be
$\mathbf{1}$ when $n=0$. In general, an empty product of polynomials is always
understood to be $\mathbf{1}$.)

\textbf{(j)} We let $X$ denote the polynomial $\left(  0,1,0,0,0,\ldots
\right)  \in\mathbb{Q}\left[  X\right]  $. (This is the polynomial whose
$1$-st coefficient is $1$ and whose other coefficients are $0$.) This
polynomial is called the \textit{indeterminate} of $\mathbb{Q}\left[
X\right]  $. It is easy to see that, for any $n\in\mathbb{N}$, we have%
\[
X^{n}=\left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}},1,0,0,0,\ldots
\right)  .
\]


This polynomial $X$ finally provides an answer to the question
\textquotedblleft what is an indeterminate\textquotedblright\ and
\textquotedblleft what is a formal expression\textquotedblright. Namely, let
$\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ be
any polynomial. Then, the sum $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ is well-defined
(it is an infinite sum, but due to (\ref{eq.def.polynomial-univar.finite}) it
has only finitely many nonzero addends), and it is easy to see that this sum
equals $\left(  p_{0},p_{1},p_{2},\ldots\right)  $. Thus,
\[
\left(  p_{0},p_{1},p_{2},\ldots\right)  =p_{0}+p_{1}X+p_{2}X^{2}%
+\cdots\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  \in\mathbb{Q}\left[  X\right]  .
\]
This finally allows us to write a polynomial $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  $ as a sum $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ while remaining
honest; the sum $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ is no longer a
\textquotedblleft formal expression\textquotedblright\ of unclear meaning, nor
a function, but it is just an alternative way to write the sequence $\left(
p_{0},p_{1},p_{2},\ldots\right)  $. So, at last, our notion of a polynomial
resembles the intuitive notion of a polynomial!

Of course, we can write polynomials as finite sums as well. Indeed, if
$\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ is
a polynomial and $N$ is a nonnegative integer such that every $n>N$ satisfies
$p_{n}=0$, then%
\[
\left(  p_{0},p_{1},p_{2},\ldots\right)  =p_{0}+p_{1}X+p_{2}X^{2}+\cdots
=p_{0}+p_{1}X+\cdots+p_{N}X^{N}%
\]
(because addends can be discarded when they are $0$). For example, $\left(
4,1,0,0,0,\ldots\right)  =4+1X=4+X$ and $\left(  \dfrac{1}{2},0,\dfrac{1}%
{3},0,0,0,\ldots\right)  =\dfrac{1}{2}+0X+\dfrac{1}{3}X^{2}=\dfrac{1}%
{2}+\dfrac{1}{3}X^{2}$.

\textbf{(k)} For our definition of polynomials to be fully compatible with our
intuition, we are missing only one more thing: a way to evaluate a polynomial
at a number, or some other object (e.g., another polynomial or a function).
This is easy: Let $p=\left(  p_{0},p_{1},p_{2},\ldots\right)  \in
\mathbb{Q}\left[  X\right]  $ be a polynomial, and let $\alpha\in\mathbb{Q}$.
Then, $p\left(  \alpha\right)  $ means the number $p_{0}+p_{1}\alpha
+p_{2}\alpha^{2}+\cdots\in\mathbb{Q}$. (Again, the infinite sum $p_{0}%
+p_{1}\alpha+p_{2}\alpha^{2}+\cdots$ makes sense because of
(\ref{eq.def.polynomial-univar.finite}).) Similarly, we can define $p\left(
\alpha\right)  $ when $\alpha\in\mathbb{R}$ (but in this case, $p\left(
\alpha\right)  $ will be an element of $\mathbb{R}$) or when $\alpha
\in\mathbb{C}$ (in this case, $p\left(  \alpha\right)  \in\mathbb{C}$) or when
$\alpha$ is a square matrix with rational entries (in this case, $p\left(
\alpha\right)  $ will also be such a matrix) or when $\alpha$ is another
polynomial (in this case, $p\left(  \alpha\right)  $ is such a polynomial as well).

For example, if $p=\left(  1,-2,0,3,0,0,0,\ldots\right)  =1-2X+3X^{3}$, then
$p\left(  \alpha\right)  =1-2\alpha+3\alpha^{3}$ for every $\alpha$.

The map $\mathbb{Q}\rightarrow\mathbb{Q},\ \alpha\mapsto p\left(
\alpha\right)  $ is called the \textit{polynomial function described by }$p$.
As we said above, this function is not $p$, and it is not a good idea to
equate it with $p$.

If $\alpha$ is a number (or a square matrix, or another polynomial), then
$p\left(  \alpha\right)  $ is called the result of \textit{evaluating }$p$
\textit{at }$X=\alpha$ (or, simply, evaluating $p$ at $\alpha$), or the result
of \textit{substituting }$\alpha$\textit{ for }$X$\textit{ in }$p$. This
notation, of course, reminds of functions; nevertheless, (as we already said a
few times) $p$ is \textbf{not a function}.

Probably the simplest three cases of evaluation are the following ones:

\begin{itemize}
\item We have $p\left(  0\right)  =p_{0}+p_{1}0^{1}+p_{2}0^{2}+\cdots=p_{0}$.
In other words, evaluating $p$ at $X=0$ yields the constant term of $p$.

\item We have $p\left(  1\right)  =p_{0}+p_{1}1^{1}+p_{2}1^{2}+\cdots
=p_{0}+p_{1}+p_{2}+\cdots$. In other words, evaluating $p$ at $X=1$ yields the
sum of all coefficients of $p$.

\item We have $p\left(  X\right)  =p_{0}+p_{1}X^{1}+p_{2}X^{2}+\cdots
=p_{0}+p_{1}X+p_{2}X^{2}+\cdots=p$. In other words, evaluating $p$ at $X=X$
yields $p$ itself. This allows us to write $p\left(  X\right)  $ for $p$. Many
authors do so, just in order to stress that $p$ is a polynomial and that the
indeterminate is called $X$. It should be kept in mind that $X$ is \textbf{not
a variable} (just as $p$ is \textbf{not a function}); it is the (fixed!)
sequence $\left(  0,1,0,0,0,\ldots\right)  \in\mathbb{Q}\left[  X\right]  $
which serves as the indeterminate for polynomials in $\mathbb{Q}\left[
X\right]  $.
\end{itemize}

\textbf{(l)} Often, one wants (or is required) to give an indeterminate a name
other than $X$. (For instance, instead of polynomials with rational
coefficients, we could be considering polynomials whose coefficients
themselves are polynomials in $\mathbb{Q}\left[  X\right]  $; and then, we
would not be allowed to use the letter $X$ for the \textquotedblleft
new\textquotedblright\ indeterminate anymore, as it already means the
indeterminate of $\mathbb{Q}\left[  X\right]  $ !) This can be done, and the
rules are the following: Any letter (that does not already have a meaning) can
be used to denote the indeterminate; but then, the set of all polynomials has
to be renamed as $\mathbb{Q}\left[  \eta\right]  $, where $\eta$ is this
letter. For instance, if we want to denote the indeterminate as $x$, then we
have to denote the set by $\mathbb{Q}\left[  x\right]  $.

It is furthermore convenient to regard the sets $\mathbb{Q}\left[
\eta\right]  $ for different letters $\eta$ as distinct. Thus, for example,
the polynomial $3X^{2}+1$ is not the same as the polynomial $3Y^{2}+1$. (The
reason for doing so is that one sometimes wishes to view both of these
polynomials as polynomials in the two variables $X$ and $Y$.) Formally
speaking, this means that we should define a polynomial in $\mathbb{Q}\left[
\eta\right]  $ to be not just a sequence $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  $ of rational numbers, but actually a pair $\left(  \left(
p_{0},p_{1},p_{2},\ldots\right)  ,\text{\textquotedblleft}\eta
\text{\textquotedblright}\right)  $ of a sequence of rational numbers and the
letter $\eta$. (Here, \textquotedblleft$\eta$\textquotedblright\ really means
the letter $\eta$, not the sequence $\left(  0,1,0,0,0,\ldots\right)  $.) This
is, of course, a very technical point which is of little relevance to most of
mathematics; it becomes important when one tries to implement polynomials in a
programming language.

\textbf{(m)} As already explained, we can replace $\mathbb{Q}$ by $\mathbb{Z}%
$, $\mathbb{R}$, $\mathbb{C}$ or any other commutative ring $\mathbb{K}$ in
the above definition. (See Definition \ref{def.commring} for the definition of
a commutative ring.) When $\mathbb{Q}$ is replaced by a commutative ring
$\mathbb{K}$, the notion of \textquotedblleft univariate polynomials with
rational coefficients\textquotedblright\ becomes \textquotedblleft univariate
polynomials with coefficients in $\mathbb{K}$\textquotedblright\ (also known
as \textquotedblleft univariate polynomials over $\mathbb{K}$%
\textquotedblright), and the set of such polynomials is denoted by
$\mathbb{K}\left[  X\right]  $ rather than $\mathbb{Q}\left[  X\right]  $.
\end{definition}

So much for univariate polynomials.

Polynomials in multiple variables are (in my opinion) treated the best in
\cite[Chapter II, \S 3]{Lang02}, where they are introduced as elements of a
monoid ring. However, this treatment is rather abstract and uses a good deal
of algebraic language\footnote{Also, the book \cite{Lang02} is notorious for
its unpolished writing; it is best read with Bergman's companion
\cite{Bergman-Lang} at hand.}. The treatments in \cite[\S 4.5]{Walker87} and
\cite[Chapter IV, \S 4]{BirkMac} use the above-mentioned recursive shortcut
that makes them inferior (in my opinion). A neat (and rather elementary)
treatment of polynomials in $n$ variables (for finite $n$) can be found in
\cite[Chapter III, \S 5]{Hungerford-03}; it generalizes the viewpoint we used
in Definition \ref{def.polynomial-univar} for univariate polynomials above.

\section{\label{chp.quivers}On acyclic quivers and mutations}

\begin{remark}
Chapter \ref{chp.quivers} is rough and will probably be rewritten at some
point (as well as moved to a more appropriate place somewhere near the end of
the notes). Since the rest of these notes does not depend on this chapter, I
recommend to \textbf{skip it} when reading the notes.
\end{remark}

In this chapter, we will use the following notations (which come from
\cite[\S 2.1.1]{Lampe}):

\begin{itemize}
\item A \textit{quiver} means a tuple $Q=\left(  Q_{0},Q_{1},s,t\right)  $,
where $Q_{0}$ and $Q_{1}$ are two finite sets and where $s$ and $t$ are two
maps from $Q_{1}$ to $Q_{0}$. We call the elements of $Q_{0}$ the
\textit{vertices} of the quiver $Q$, and we call the elements of $Q_{1}$ the
\textit{arrows} of the quiver $Q$. For every $e\in Q_{1}$, we call $s\left(
e\right)  $ the \textit{starting point} of $e$ (and we say that $e$
\textit{starts at }$s\left(  e\right)  $), and we call $t\left(  e\right)  $
the \textit{terminal point} of $e$ (and we say that $e$ \textit{ends at}
$t\left(  e\right)  $). Furthermore, if $e\in Q_{1}$, then we say that $e$ is
an \textit{arrow from }$s\left(  e\right)  $ \textit{to }$t\left(  e\right)  $.

So the notion of a quiver is one of many different versions of the notion of a
finite directed graph. (Notice that it is a version which allows multiple
arrows, and which distinguishes between them -- i.e., the quiver stores not
just the information of how many arrows there are from a vertex to another,
but it actually has them all as distinguishable objects in $Q_{1}$. Lampe
himself seems to later tacitly switch to a different notion of quivers, where
edges from a given to vertex to another are indistinguishable and only exist
as a number. This does not matter for the next exercise, which works just as
well with either notion of a quiver; but I just wanted to have it mentioned.

\item The \textit{underlying undirected graph} of a quiver $Q=\left(
Q_{0},Q_{1},s,t\right)  $ is defined as the undirected multigraph with vertex
set $Q_{0}$ and edge multiset%
\[
\left\{  \left\{  s\left(  e\right)  ,t\left(  e\right)  \right\}
\ \mid\ e\in Q_{1}\right\}  _{\operatorname*{multiset}}.
\]
(\textquotedblleft Multigraph\textquotedblright\ means that multiple edges are
allowed, but we do not make them distinguishable.)

\item A quiver $Q=\left(  Q_{0},Q_{1},s,t\right)  $ is said to be
\textit{acyclic} if there is no sequence $\left(  a_{0},a_{1},\ldots
,a_{n}\right)  $ of elements of $Q_{0}$ such that $a_{0}=a_{n}$ and such that
$Q$ has an arrow from $a_{i}$ to $a_{i+1}$ for every $i\in\left\{
0,1,\ldots,n-1\right\}  $. (This is equivalent to \cite[Definition
2.1.7]{Lampe}.) Notice that this does not mean that the \textit{undirected}
version of $Q$ has no cycles.

\item Let $Q=\left(  Q_{0},Q_{1},s,t\right)  $. Then, a \textit{sink} of $Q$
means a vertex $v\in Q_{0}$ such that no $e\in Q_{1}$ starts at $v$ (in other
words, no arrow of $Q$ starts at $v$). A \textit{source} of $Q$ means a vertex
$v\in Q_{0}$ such that no $e\in Q_{1}$ ends at $v$ (in other words, no arrow
of $Q$ ends at $v$).

\item Let $Q=\left(  Q_{0},Q_{1},s,t\right)  $. If $i\in Q_{0}$ is a sink of
$Q$, then the \textit{mutation} $\mu_{i}\left(  Q\right)  $ of $Q$ at $i$ is
the quiver obtained from $Q$ simply by turning\footnote{To \textit{turn} an
arrow $e$ means to reverse its direction, i.e., to switch the values of
$s\left(  e\right)  $ and $t\left(  e\right)  $. We model this as a change to
the functions $s$ and $t$, not as a change to the arrow itself.} all arrows
ending at $i$. (To be really pedantic: We define $\mu_{i}\left(  Q\right)  $
as the quiver $\left(  Q_{0},Q_{1},s^{\prime},t^{\prime}\right)  $, where%
\[
s^{\prime}\left(  e\right)  =\left\{
\begin{array}
[c]{c}%
t\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  =i;\\
s\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  \neq i
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{and}\ \ \ \ \ \ \ \ \ \ t^{\prime}\left(
e\right)  =\left\{
\begin{array}
[c]{c}%
s\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  =i;\\
t\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  \neq i
\end{array}
\right.
\]
for all $i\in Q_{0}$.) If $i\in Q_{0}$ is a source of $Q$, then the
\textit{mutation} $\mu_{i}\left(  Q\right)  $ of $Q$ at $i$ is the quiver
obtained from $Q$ by turning all arrows starting at $i$. (Notice that if $i$
is both a source and a sink of $Q$, then these two definitions give the same
result; namely, $\mu_{i}\left(  Q\right)  =Q$ in this case.)

If $Q$ is an acyclic quiver, then $\mu_{i}\left(  Q\right)  $ is acyclic as
well (whenever $i\in Q_{0}$ is a sink or a source of $Q$).

We use the word \textquotedblleft mutation\textquotedblright\ not only for the
quiver $\mu_{i}\left(  Q\right)  $, but also for the operation that transforms
$Q$ into $\mu_{i}\left(  Q\right)  $. (We have defined this operation only if
$i$ is a sink or a source of $Q$. It can be viewed as a particular case of the
more general definition of mutation given in \cite[Definition 2.2.1]{Lampe},
at least if one gives up the ability to distinguish different arrows from one
vertex to another.)
\end{itemize}

\Needspace{15\baselineskip}

\begin{exercise}
\label{exe.ps1.1.1}Let $Q=\left(  Q_{0},Q_{1},s,t\right)  $ be an acyclic quiver.

\textbf{(a)} Let $A$ and $B$ be two subsets of $Q_{0}$ such that $A\cap
B=\varnothing$ and $A\cup B=Q_{0}$. Assume that there exists no arrow of $Q$
that starts at a vertex in $B$ and ends at a vertex in $A$. Then, by turning
all arrows of $Q$ which start at a vertex in $A$ and end at a vertex in $B$,
we obtain a new acyclic quiver $\operatorname*{mut}\nolimits_{A,B}Q$.

(When we say \textquotedblleft turning all arrows of $Q$ which start at a
vertex in $A$ and end at a vertex in $B$\textquotedblright, we mean
\textquotedblleft turning all arrows $e$ of $Q$ which satisfy $s\left(
e\right)  \in A$ and $t\left(  e\right)  \in B$\textquotedblright. We do
\textbf{not} mean that we fix a vertex $a$ in $A$ and a vertex $b$ in $B$, and
only turn the arrows from $a$ to $b$.)

For example, if $Q=%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%3 \ar[r] & 4 \\
%1 \ar[u] \ar[ru] \ar[r] & 2 \ar[u]
%}}}%
%BeginExpansion
\xymatrix{
3 \ar[r] & 4 \\
1 \ar[u] \ar[ru] \ar[r] & 2 \ar[u]
}%
%EndExpansion
$ and $A=\left\{  1,3\right\}  $ and $B=\left\{  2,4\right\}  $, then
\newline$\operatorname*{mut}\nolimits_{A,B}Q=%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%3 & 4 \ar[l] \ar[ld] \\
%1 \ar[u] & 2 \ar[u] \ar[l]
%}}}%
%BeginExpansion
\xymatrix{
3 & 4 \ar[l] \ar[ld] \\
1 \ar[u] & 2 \ar[u] \ar[l]
}%
%EndExpansion
$.

Prove that $\operatorname*{mut}\nolimits_{A,B}Q$ can be obtained from $Q$ by a
sequence of mutations at sinks. (More precisely, there exists a sequence
$\left(  Q^{\left(  0\right)  },Q^{\left(  1\right)  },\ldots,Q^{\left(
\ell\right)  }\right)  $ of acyclic quivers such that $Q^{\left(  0\right)
}=Q$, $Q^{\left(  \ell\right)  }=\operatorname*{mut}\nolimits_{A,B}Q$, and for
every $i\in\left\{  1,2,\ldots,\ell\right\}  $, the quiver $Q^{\left(
i\right)  }$ is obtained from $Q^{\left(  i-1\right)  }$ by mutation at a sink
of $Q^{\left(  i-1\right)  }$.)

[In our above example, we can mutate at $4$ first and then at $2$.]

\textbf{(b)} If $i\in Q_{0}$ is a \textbf{source} of $Q$, then show that the
mutation $\mu_{i}\left(  Q\right)  $ can be obtained from $Q$ by a sequence of
mutations at sinks.

\textbf{(c)} Assume now that the underlying \textbf{undirected} graph of $Q$
is a tree. (In particular, $Q$ cannot have more than one edge between two
vertices, as these would form a cycle in the underlying undirected graph!)
Show that any acyclic quiver which can be obtained from $Q$ by turning some of
its arrows can also be obtained from $Q$ by a sequence of mutations at sinks.
\end{exercise}

\begin{remark}
More general results than those of Exercise \ref{exe.ps1.1.1} are stated (for
directed graphs rather than quivers, but it is easy to translate from one
language into another) in \cite{Pretzel}.
\end{remark}

\section{\label{chp.binom}On binomial coefficients}

Let me now switch to a different subject. The present chapter is about
\textit{binomial coefficients} and some of their properties. This subject has
little to do with cluster algebras, but it proves two lemmas in the long
Lee--Schiffler paper \cite{LS} (specifically, our Exercise \ref{exe.ps1.1.3}
is \cite[Lemma 5.11]{LS}, and our Proposition \ref{exe.ps1.1.4} is \cite[Lemma
5.12]{LS}); besides, it is highly useful in many fields of mathematics and
provides good opportunities to practice the arts of mathematical induction and
of finding bijections.

Identities involving binomial coefficients are legion, and books have been
written about them (let me mention \cite[Chapter 5]{GKP} as a highly readable
introduction; but, e.g., \href{http://www.math.wvu.edu/~gould/}{Henry W.
Gould's website} goes far further down the rabbit hole). We shall only study a
few of these identities.

\subsection{Definitions and basic properties}

Recall that for every $n\in\mathbb{N}$, the binomial coefficient $\dbinom
{X}{n}$ is a polynomial in $X$ of degree $n$, with rational coefficients. It
is defined by%
\[
\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!}.
\]
\footnote{When $n=0$, then the numerator of this fraction (i.e., the product
$X\left(  X-1\right)  \cdots\left(  X-n+1\right)  $) is an empty product. By
convention, an empty product is always defined to be $1$.} This polynomial can
be evaluated at every integer or rational number or even complex number (i.e.,
we can substitute any such number for $X$), or at any other polynomial (for
example, we can substitute $X^{2}+2$ for $X$ to obtain $\dbinom{X^{2}+2}{n}$,
which is again a polynomial). Whenever $m$ is an integer (or rational number,
or complex number), we denote by $\dbinom{m}{n}$ the result of evaluating the
polynomial $\dbinom{X}{n}$ at $X=m$. These numbers $\dbinom{m}{n}$ are the
so-called \textit{binomial coefficients}, and form the so-called
\textit{\href{https://en.wikipedia.org/wiki/Pascal's_triangle}{\textit{Pascal's
triangle}}}\footnote{More precisely, the numbers $\dbinom{m}{n}$ for
$m\in\mathbb{N}$ and $n\in\left\{  0,1,\ldots,m\right\}  $ form Pascal's
triangle. Nevertheless, the \textquotedblleft other\textquotedblright%
\ binomial coefficients (particularly the ones where $m$ is a negative
integer) are highly useful.}. Let us state a few basic properties of these numbers:

\begin{itemize}
\item We have%
\begin{equation}
\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}
\label{eq.binom.mn}%
\end{equation}
for every $m\in\mathbb{Z}$ and $n\in\mathbb{N}$. (This follows by evaluating
both sides of the identity $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)
\cdots\left(  X-n+1\right)  }{n!}$ at $X=m$.)

The equality (\ref{eq.binom.mn}) is how the binomial coefficients $\dbinom
{m}{n}$ are usually defined in textbooks. Thus, our detour through polynomials
was not necessary. (But this detour will reveal to be useful soon, when we
will prove some properties of binomial coefficients.)

\item We have
\begin{equation}
\dbinom{X}{0}=1 \label{eq.binom.00X}%
\end{equation}
\footnote{\textit{Proof.} The definition of $\dbinom{X}{0}$ yields $\dbinom
{X}{0}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-0+1\right)  }{0!}$. Since
$X\left(  X-1\right)  \cdots\left(  X-0+1\right)  =\left(  \text{a product of
}0\text{ integers}\right)  =1$, this rewrites as $\dbinom{X}{0}=\dfrac{1}%
{0!}=1$ (since $0!=1$), qed.}. Thus,
\begin{equation}
\dbinom{m}{0}=1 \label{eq.binom.00}%
\end{equation}
for every $m\in\mathbb{Z}$. (This follows by substituting $0$ for $m$ in
(\ref{eq.binom.00X}).)

\item We have%
\begin{equation}
\dbinom{m}{n}=\dfrac{m!}{n!\left(  m-n\right)  !} \label{eq.binom.formula}%
\end{equation}
for any $m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m\geq n$%
\ \ \ \ \footnote{\textbf{Caution:} This formula holds only for $m\in
\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m\geq n$. Thus, neither
$\dbinom{-3}{2}$ nor $\dbinom{1/3}{3}$ nor $\dbinom{2}{5}$ nor the
\textbf{polynomial} $\dbinom{X}{3}$ can be evaluated using this formula! The
definition $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(
X-n+1\right)  }{n!}$ of binomial coefficients is a lot more general than
(\ref{eq.binom.formula}).
\par
For the sake of completeness, let us give a \textit{proof of
(\ref{eq.binom.formula}):} Let $m\in\mathbb{N}$ and $n\in\mathbb{N}$ be such
that $m\geq n$. Then, evaluating both sides of the identity $\dbinom{X}%
{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!}$ at $X=m$,
we obtain%
\[
\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!},
\]
so that $n!\cdot\dbinom{m}{n}=m\left(  m-1\right)  \cdots\left(  m-n+1\right)
$. But%
\begin{align*}
m!  &  =m\left(  m-1\right)  \cdots1=\left(  m\left(  m-1\right)
\cdots\left(  m-n+1\right)  \right)  \cdot\underbrace{\left(  \left(
m-n\right)  \left(  m-n-1\right)  \cdots1\right)  }_{=\left(  m-n\right)  !}\\
&  =\left(  m\left(  m-1\right)  \cdots\left(  m-n+1\right)  \right)
\cdot\left(  m-n\right)  !,
\end{align*}
so that $\dfrac{m!}{\left(  m-n\right)  !}=m\left(  m-1\right)  \cdots\left(
m-n+1\right)  $. Comparing this with $n!\cdot\dbinom{m}{n}=m\left(
m-1\right)  \cdots\left(  m-n+1\right)  $, we obtain $n!\cdot\dbinom{m}%
{n}=\dfrac{m!}{\left(  m-n\right)  !}$. Dividing this equality by $n!$, we
obtain $\dbinom{m}{n}=\dfrac{m!}{n!\left(  m-n\right)  !}$. Thus,
(\ref{eq.binom.formula}) is proven.}.

\item We have
\begin{equation}
\dbinom{m}{n}=0 \label{eq.binom.0}%
\end{equation}
for every $m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m<n$%
\ \ \ \ \footnote{\textbf{Caution:} This is not true if we drop the condition
$m\in\mathbb{N}$.
\par
Again for the sake of completeness, let us give a \textit{proof of
(\ref{eq.binom.0}):} Let $m\in\mathbb{N}$ and $n\in\mathbb{N}$ be such that
$m<n$. Evaluating both sides of the identity $\dbinom{X}{n}=\dfrac{X\left(
X-1\right)  \cdots\left(  X-n+1\right)  }{n!}$ at $X=m$, we obtain%
\[
\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}.
\]
\par
But $m\geq0$ (since $m\in\mathbb{N}$) and $m<n$. Hence, $m-m$ is one of the
$n$ integers $m,m-1,\ldots,m-n+1$. Thus, one of the $n$ factors of the product
$m\left(  m-1\right)  \cdots\left(  m-n+1\right)  $ is $m-m=0$. Therefore, the
whole product $m\left(  m-1\right)  \cdots\left(  m-n+1\right)  $ is $0$
(because if one of the factors of a product is $0$, then the whole product
must be $0$). Thus, $m\left(  m-1\right)  \cdots\left(  m-n+1\right)  =0$.
Hence,%
\begin{align*}
\dbinom{m}{n}  &  =\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)
}{n!}=\dfrac{0}{n!}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\left(
m-1\right)  \cdots\left(  m-n+1\right)  =0\right) \\
&  =0,
\end{align*}
qed.}.

\item We have%
\begin{equation}
\dbinom{m}{n}=\dbinom{m}{m-n} \label{eq.binom.symm}%
\end{equation}
for any $m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m\geq n$%
\ \ \ \ \footnote{\textit{Proof of (\ref{eq.binom.symm}):} Let $m\in
\mathbb{N}$ and $n\in\mathbb{N}$ be such that $m\geq n$. Then, $m-n\in
\mathbb{N}$ (since $m\geq n$) and $m\geq m-n$ (since $n\geq0$ (since
$n\in\mathbb{N}$)). Hence, (\ref{eq.binom.formula}) (applied to $m-n$ instead
of $n$) yields
\[
\dbinom{m}{m-n}=\dfrac{m!}{\left(  m-n\right)  !\left(  m-\left(  m-n\right)
\right)  !}=\dfrac{m!}{\left(  m-\left(  m-n\right)  \right)  !\left(
m-n\right)  !}=\dfrac{m!}{n!\left(  m-n\right)  !}%
\]
(since $m-\left(  m-n\right)  =n$). Compared with (\ref{eq.binom.formula}),
this yields $\dbinom{m}{n}=\dbinom{m}{m-n}$, qed.}.

\item We have%
\begin{equation}
\dbinom{m}{m}=1 \label{eq.binom.mm}%
\end{equation}
for every $m\in\mathbb{N}$\ \ \ \ \footnote{\textit{Proof of
(\ref{eq.binom.mm}):} Let $m\in\mathbb{N}$. Then, (\ref{eq.binom.symm})
(applied to $n=m$) yields $\dbinom{m}{m}=\dbinom{m}{m-m}=\dbinom{m}{0}=1$
(according to (\ref{eq.binom.00})). This proves (\ref{eq.binom.mm}).}.

\item If $m\in\mathbb{N}$ and $n\in\mathbb{N}$, and if $S$ is an $m$-element
set, then
\begin{equation}
\dbinom{m}{n}\text{ is the number of all }n\text{-element subsets of
}S\text{.} \label{eq.binom.subsets}%
\end{equation}
In less formal terms, this says that $\dbinom{m}{n}$ is the number of ways to
pick out $n$ among $m$ given objects, without replacement\footnote{That is,
one must not pick out the same object twice.} and without regard for the order
in which they are picked out. (Probabilists call this \textquotedblleft
unordered samples without replacement\textquotedblright.)

Notice that this does not hold for negative $m$ ! Indeed, when $m\in
\mathbb{Z}$ is negative, then $\dbinom{m}{n}$ is positive for $n$ even and
negative for $n$ odd (easy exercise), and so an interpretation of $\dbinom
{m}{n}$ as a number of ways to do something is rather unlikely. (On the other
hand, $\left(  -1\right)  ^{n}\dbinom{m}{n}$ does have such an interpretation.)

\item We have%
\begin{equation}
\dbinom{m}{n}=\left(  -1\right)  ^{n}\dbinom{n-m-1}{n}
\label{eq.binom.upper-neg}%
\end{equation}
for any $m\in\mathbb{Z}$ and $n\in\mathbb{N}$\ \ \ \ \footnote{\textit{Proof
of (\ref{eq.binom.upper-neg}):} Let $m\in\mathbb{Z}$ and $n\in\mathbb{N}$.
\par
Evaluating both sides of the identity $\dbinom{X}{n}=\dfrac{X\left(
X-1\right)  \cdots\left(  X-n+1\right)  }{n!}$ at $X=n-m-1$, we obtain%
\begin{align*}
\dbinom{n-m-1}{n}  &  =\dfrac{\left(  n-m-1\right)  \left(  \left(
n-m-1\right)  -1\right)  \cdots\left(  \left(  n-m-1\right)  -n+1\right)
}{n!}\\
&  =\dfrac{1}{n!}\left(  n-m-1\right)  \left(  \left(  n-m-1\right)
-1\right)  \cdots\underbrace{\left(  \left(  n-m-1\right)  -n+1\right)
}_{=-m}\\
&  =\dfrac{1}{n!}\underbrace{\left(  n-m-1\right)  \left(  \left(
n-m-1\right)  -1\right)  \cdots\left(  -m\right)  }_{\substack{=\left(
-m\right)  \left(  -m+1\right)  \cdots\left(  n-m-1\right)  \\\text{(here, we
have just reversed the order of the factors in the product)}}}\\
&  =\dfrac{1}{n!}\underbrace{\left(  -m\right)  }_{=\left(  -1\right)
m}\underbrace{\left(  -m+1\right)  }_{=\left(  -1\right)  \left(  m-1\right)
}\cdots\underbrace{\left(  n-m-1\right)  }_{=\left(  -1\right)  \left(
m-n+1\right)  }\\
&  =\dfrac{1}{n!}\left(  \left(  -1\right)  m\right)  \left(  \left(
-1\right)  \left(  m-1\right)  \right)  \cdots\left(  \left(  -1\right)
\left(  m-n+1\right)  \right) \\
&  =\dfrac{1}{n!}\left(  -1\right)  ^{n}\left(  m\left(  m-1\right)
\cdots\left(  m-n+1\right)  \right)  ,
\end{align*}
so that%
\begin{align*}
\left(  -1\right)  ^{n}\dbinom{n-m-1}{n}  &  =\left(  -1\right)  ^{n}%
\cdot\dfrac{1}{n!}\left(  -1\right)  ^{n}\left(  m\left(  m-1\right)
\cdots\left(  m-n+1\right)  \right) \\
&  =\underbrace{\left(  -1\right)  ^{n}\left(  -1\right)  ^{n}}%
_{\substack{=\left(  -1\right)  ^{2n}=1\\\text{(since }2n\text{ is even)}%
}}\cdot\dfrac{1}{n!}\left(  m\left(  m-1\right)  \cdots\left(  m-n+1\right)
\right) \\
&  =\dfrac{1}{n!}\left(  m\left(  m-1\right)  \cdots\left(  m-n+1\right)
\right)  =\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}.
\end{align*}
Compared with $\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(
m-n+1\right)  }{n!}$ (which is obtained by evaluating both sides of the
identity $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(
X-n+1\right)  }{n!}$ at $X=m$), this yields $\dbinom{m}{n}=\left(  -1\right)
^{n}\dbinom{n-m-1}{n}$, qed.}. This formula is known as the \textit{upper
negation formula} and holds more generally for any $m$ for which it makes
sense (in particular, $m$ can be a polynomial or a complex number). In
particular, we have%
\begin{equation}
\dbinom{X}{n}=\left(  -1\right)  ^{n}\dbinom{n-X-1}{n}
\label{eq.binom.upper-neg.X}%
\end{equation}
(an identity between polynomials in $X$) for every $n\in\mathbb{N}%
$\ \ \ \ \footnote{\textit{Proof of (\ref{eq.binom.upper-neg.X}):} To prove
(\ref{eq.binom.upper-neg.X}), just replace every appearance of
\textquotedblleft$m$\textquotedblright\ by \textquotedblleft$X$%
\textquotedblright\ in our proof of (\ref{eq.binom.upper-neg}).}.

\item We have%
\begin{equation}
\dbinom{X}{n}=\dbinom{X-1}{n}+\dbinom{X-1}{n-1} \label{eq.binom.rec}%
\end{equation}
for any $n\in\left\{  1,2,3,\ldots\right\}  $\ \ \ \ \footnote{\textit{Proof
of (\ref{eq.binom.rec}):} Let $n\in\left\{  1,2,3,\ldots\right\}  $. We have
$n!=n\cdot\left(  n-1\right)  !$, so that $\left(  n-1\right)  !=\dfrac{n!}%
{n}$ and thus $\dfrac{1}{\left(  n-1\right)  !}=1/\dfrac{n!}{n}=\dfrac{1}%
{n!}\cdot n$.
\par
The definition of $\dbinom{X}{n-1}$ yields
\[
\dbinom{X}{n-1}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-\left(
n-1\right)  +1\right)  }{\left(  n-1\right)  !}=\dfrac{1}{\left(  n-1\right)
!}\left(  X\left(  X-1\right)  \cdots\left(  X-\left(  n-1\right)  +1\right)
\right)  .
\]
Substituting $X-1$ for $X$ in this equality, we obtain%
\begin{align}
\dbinom{X-1}{n-1}  &  =\underbrace{\dfrac{1}{\left(  n-1\right)  !}}%
_{=\dfrac{1}{n!}\cdot n}\left(  \left(  X-1\right)  \underbrace{\left(
\left(  X-1\right)  -1\right)  }_{=X-2}\cdots\underbrace{\left(  \left(
X-1\right)  -\left(  n-1\right)  +1\right)  }_{=X-n+1}\right) \nonumber\\
&  =\dfrac{1}{n!}\cdot n\left(  \left(  X-1\right)  \left(  X-2\right)
\cdots\left(  X-n+1\right)  \right)  . \label{eq.binom.rec.pf.1}%
\end{align}
\par
On the other hand,
\[
\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }%
{n!}=\dfrac{1}{n!}\left(  X\left(  X-1\right)  \cdots\left(  X-n+1\right)
\right)  .
\]
Substituting $X-1$ for $X$ in this equality, we find%
\begin{align*}
\dbinom{X-1}{n}  &  =\dfrac{1}{n!}\left(  \left(  X-1\right)
\underbrace{\left(  \left(  X-1\right)  -1\right)  }_{=X-2}\cdots
\underbrace{\left(  \left(  X-1\right)  -n+1\right)  }_{=X-n}\right) \\
&  =\dfrac{1}{n!}\underbrace{\left(  \left(  X-1\right)  \left(  X-2\right)
\cdots\left(  X-n\right)  \right)  }_{=\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  \cdot\left(  X-n\right)  }\\
&  =\dfrac{1}{n!}\left(  \left(  X-1\right)  \left(  X-2\right)  \cdots\left(
X-n+1\right)  \right)  \cdot\left(  X-n\right) \\
&  =\dfrac{1}{n!}\left(  X-n\right)  \cdot\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  .
\end{align*}
Adding (\ref{eq.binom.rec.pf.1}) to this equality, we obtain%
\begin{align*}
&  \dbinom{X-1}{n}+\dbinom{X-1}{n-1}\\
&  =\dfrac{1}{n!}\left(  X-n\right)  \cdot\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  +\dfrac{1}{n!}\cdot n\left(
\left(  X-1\right)  \left(  X-2\right)  \cdots\left(  X-n+1\right)  \right) \\
&  =\dfrac{1}{n!}\underbrace{\left(  \left(  X-n\right)  +n\right)  }%
_{=X}\cdot\left(  \left(  X-1\right)  \left(  X-2\right)  \cdots\left(
X-n+1\right)  \right) \\
&  =\dfrac{1}{n!}\underbrace{X\cdot\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  }_{=X\left(  X-1\right)
\cdots\left(  X-n+1\right)  }=\dfrac{1}{n!}\left(  X\left(  X-1\right)
\cdots\left(  X-n+1\right)  \right)  =\dbinom{X}{n},
\end{align*}
qed.}. Thus,%
\begin{equation}
\dbinom{m}{n}=\dbinom{m-1}{n-1}+\dbinom{m-1}{n} \label{eq.binom.rec.m}%
\end{equation}
for any $m\in\mathbb{Z}$ and $n\in\left\{  1,2,3,\ldots\right\}  $. (This is
the result of substituting $m$ for $X$ in (\ref{eq.binom.rec}).) The formula
(\ref{eq.binom.rec.m}) is known as the \textit{recurrence relation of the
binomial coefficients}\footnote{Often it is extended to the case $n=0$ by
setting $\dbinom{X}{-1}=0$. It then follows from (\ref{eq.binom.00}) in this
case.
\par
The formula (\ref{eq.binom.rec.m}) is responsible for the fact that
\textquotedblleft every number in Pascal's triangle is the sum of the two
numbers above it\textquotedblright. (Of course, if you use this fact as a
\textit{definition} of Pascal's triangle, then (\ref{eq.binom.rec.m}) is
conversely responsible for the fact that the numbers in this triangle are the
binomial coefficients.)}.

\item We have%
\begin{equation}
\dbinom{m}{n}\in\mathbb{Z} \label{eq.binom.int}%
\end{equation}
for any $m\in\mathbb{Z}$ and $n\in\mathbb{N}$\ \ \ \ \footnote{\textit{Proof
of (\ref{eq.binom.int}):} Let $m\in\mathbb{Z}$ and $n\in\mathbb{N}$. We need
to show that $\dbinom{m}{n}\in\mathbb{Z}$. We are in one of the following two
cases:
\par
\textit{Case 1:} We have $m\geq0$.
\par
\textit{Case 2:} We have $m<0$.
\par
Let us first consider Case 1. In this case, we have $m\geq0$. Hence,
$m\in\mathbb{N}$. Thus, there exists an $m$-element set $S$ (for example,
$S=\left\{  1,2,\ldots,m\right\}  $). Consider such $S$. Then, $\dbinom{m}{n}$
is the number of all $n$-element subsets of $S$ (because of
(\ref{eq.binom.subsets})). Hence, $\dbinom{m}{n}$ is a nonnegative integer, so
that $\dbinom{m}{n}\in\mathbb{N}\subseteq\mathbb{Z}$. This proves
(\ref{eq.binom.int}) in Case 1.
\par
Let us now consider Case 2. In this case, we have $m<0$. Thus, $m+1\leq0$, so
that $n-m-1=n-\underbrace{\left(  m+1\right)  }_{\leq0}\geq n\geq0$. Hence,
$n-m-1\in\mathbb{N}$. Therefore, there exists an $\left(  n-m-1\right)
$-element set $S$ (for example, $S=\left\{  1,2,\ldots,n-m-1\right\}  $).
Consider such $S$. Then, $\dbinom{n-m-1}{n}$ is the number of all $n$-element
subsets of $S$ (because of (\ref{eq.binom.subsets}), applied to $n-m-1$
instead of $m$). Hence, $\dbinom{n-m-1}{n}\in\mathbb{N}\subseteq\mathbb{Z}$.
Now, (\ref{eq.binom.upper-neg}) shows that $\dbinom{m}{n}=\left(  -1\right)
^{n}\underbrace{\dbinom{n-m-1}{n}}_{\in\mathbb{Z}}\in\left(  -1\right)
^{n}\mathbb{Z}\subseteq\mathbb{Z}$ (where $\left(  -1\right)  ^{n}\mathbb{Z}$
means $\left\{  \left(  -1\right)  ^{n}z\ \mid\ z\in\mathbb{Z}\right\}  $).
This proves (\ref{eq.binom.int}) in Case 2.
\par
We thus have proven (\ref{eq.binom.int}) in each of the two Cases 1 and 2, and
can therefore conclude that (\ref{eq.binom.int}) always holds.
\par
This is the simplest proof of (\ref{eq.binom.int}) that I am aware of. There
is another which proceeds by induction on $m$ (using (\ref{eq.binom.rec})),
but this induction needs two induction steps ($m\rightarrow m+1$ and
$m\rightarrow m-1$) in order to reach all integers (positive and negative).
There is yet another proof using basic number theory (specifically, checking
how often a prime $p$ appears in the numerator and the denominator of
$\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}%
$), but this is not quite easy.}.

\item We have%
\begin{equation}
\left(  X+Y\right)  ^{n}=\sum_{k=0}^{n}\dbinom{n}{k}X^{k}Y^{n-k}
\label{eq.binom.binomial}%
\end{equation}
(as an equality between two polynomials in $X$ and $Y$) for every
$n\in\mathbb{N}$. This is the famous \textit{binomial formula} and has a
well-known standard proof by induction over $n$ (using (\ref{eq.binom.rec.m})
and (\ref{eq.binom.00})). Some versions of it hold for negative $n$ as well
(but not in the exact form (\ref{eq.binom.binomial}), and not without restrictions).

\item We have%
\begin{equation}
\dbinom{X}{n}=\dfrac{X}{n}\dbinom{X-1}{n-1} \label{eq.binom.X-1}%
\end{equation}
for any $n\in\left\{  1,2,3,\ldots\right\}  $\ \ \ \ \footnote{\textit{Proof
of (\ref{eq.binom.X-1}):} Let $n\in\left\{  1,2,3,\ldots\right\}  $. The
definition of $\dbinom{X}{n-1}$ yields $\dbinom{X}{n-1}=\dfrac{X\left(
X-1\right)  \cdots\left(  X-\left(  n-1\right)  +1\right)  }{\left(
n-1\right)  !}$. Substituting $X-1$ for $X$ in this equality, we obtain%
\begin{align*}
\dbinom{X-1}{n-1}  &  =\dfrac{\left(  X-1\right)  \left(  \left(  X-1\right)
-1\right)  \cdots\left(  \left(  X-1\right)  -\left(  n-1\right)  +1\right)
}{\left(  n-1\right)  !}\\
&  =\dfrac{\left(  X-1\right)  \left(  X-2\right)  \cdots\left(  X-n+1\right)
}{\left(  n-1\right)  !}%
\end{align*}
(since $\left(  X-1\right)  -1=X-2$ and $\left(  X-1\right)  -\left(
n-1\right)  +1=X-n+1$). Multiplying both sides of this equality by $\dfrac
{X}{n}$, we obtain%
\begin{align*}
\dfrac{X}{n}\cdot\dbinom{X-1}{n-1}  &  =\dfrac{X}{n}\cdot\dfrac{\left(
X-1\right)  \left(  X-2\right)  \cdots\left(  X-n+1\right)  }{\left(
n-1\right)  !}=\dfrac{X\left(  X-1\right)  \left(  X-2\right)  \cdots\left(
X-n+1\right)  }{n\left(  n-1\right)  !}\\
&  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!}%
\end{align*}
(since $X\left(  X-1\right)  \left(  X-2\right)  \cdots\left(  X-n+1\right)
=X\left(  X-1\right)  \cdots\left(  X-n+1\right)  $ and $n\left(  n-1\right)
!=n!$). Compared with%
\[
\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!},
\]
this yields $\dbinom{X}{n}=\dfrac{X}{n}\dbinom{X-1}{n-1}$. This proves
(\ref{eq.binom.X-1}).}.

\item If $a\in\mathbb{N}$ and $i\in\mathbb{N}$ are such that $i\geq a$, then%
\begin{equation}
\dbinom{X}{i}\dbinom{i}{a}=\dbinom{X}{a}\dbinom{X-a}{i-a}
\label{eq.binom.trinom-rev}%
\end{equation}
\footnote{\textit{Proof of (\ref{eq.binom.trinom-rev}):} Let $a\in\mathbb{N}$
and $i\in\mathbb{N}$ be such that $i\geq a$. We have%
\begin{align*}
&  \underbrace{\dbinom{X}{a}}_{\substack{=\dfrac{X\left(  X-1\right)
\cdots\left(  X-a+1\right)  }{a!}\\\text{(by the definition of }\dbinom{X}%
{a}\text{)}}}\underbrace{\dbinom{X-a}{i-a}}_{\substack{=\dfrac{\left(
X-a\right)  \left(  \left(  X-a\right)  -1\right)  \cdots\left(  \left(
X-a\right)  -\left(  i-a\right)  +1\right)  }{\left(  i-a\right)
!}\\\text{(by the definition of }\dbinom{X-a}{i-a}\text{)}}}\\
&  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-a+1\right)  }{a!}\cdot
\dfrac{\left(  X-a\right)  \left(  \left(  X-a\right)  -1\right)
\cdots\left(  \left(  X-a\right)  -\left(  i-a\right)  +1\right)  }{\left(
i-a\right)  !}\\
&  =\dfrac{1}{a!\cdot\left(  i-a\right)  !}\underbrace{\left(  X\left(
X-1\right)  \cdots\left(  X-a+1\right)  \right)  \cdot\left(  \left(
X-a\right)  \left(  \left(  X-a\right)  -1\right)  \cdots\left(  \left(
X-a\right)  -\left(  i-a\right)  +1\right)  \right)  }_{\substack{=X\left(
X-1\right)  \cdots\left(  \left(  X-a\right)  -\left(  i-a\right)  +1\right)
\\=X\left(  X-1\right)  \cdots\left(  X-i+1\right)  \\\text{(since }\left(
X-a\right)  -\left(  i-a\right)  =X-i\text{)}}}\\
&  =\dfrac{1}{a!\cdot\left(  i-a\right)  !}X\left(  X-1\right)  \cdots\left(
X-i+1\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\dbinom{X}{i}}_{\substack{=\dfrac{X\left(  X-1\right)
\cdots\left(  X-i+1\right)  }{i!}\\\text{(by the definition of }\dbinom{X}%
{i}\text{)}}}\underbrace{\dbinom{i}{a}}_{\substack{=\dfrac{i!}{a!\left(
i-a\right)  !}\\\text{(by (\ref{eq.binom.formula}), applied to }m=i\text{ and
}n=a\text{)}}}\\
&  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-i+1\right)  }{i!}\cdot
\dfrac{i!}{a!\left(  i-a\right)  !}=\dfrac{1}{a!\cdot\left(  i-a\right)
!}X\left(  X-1\right)  \cdots\left(  X-i+1\right)  ,
\end{align*}
this yields $\dbinom{X}{i}\dbinom{i}{a}=\dbinom{X}{a}\dbinom{X-a}{i-a}$. This
proves (\ref{eq.binom.trinom-rev}).
\par
Notice that we used (\ref{eq.binom.formula}) to simplify $\dbinom{i}{a}$ in
this proof. Do not be tempted to use (\ref{eq.binom.formula}) to simplify
$\dbinom{X}{i}$, $\dbinom{X}{a}$ and $\dbinom{X-a}{i-a}$: The $X$ in these
expressions is a polynomial indeterminate, and (\ref{eq.binom.formula}) cannot
be applied to it!}. In particular, if $m\in\mathbb{Z}$, $a\in\mathbb{N}$ and
$i\in\mathbb{N}$ are such that $i\geq a$, then%
\begin{equation}
\dbinom{m}{i}\dbinom{i}{a}=\dbinom{m}{a}\dbinom{m-a}{i-a}
\label{eq.binom.trinom-rev.m}%
\end{equation}
\footnote{This formula is obtained from (\ref{eq.binom.trinom-rev}) by
substituting $m$ for $X$.}. This is a simple and yet highly useful formula,
which Graham, Knuth and Patashnik call \textit{trinomial revision} in
\cite[Table 174]{GKP}.
\end{itemize}

\subsection{Binomial coefficients and polynomials}

Recall that any polynomial $P\in\mathbb{Q}\left[  X\right]  $ (that is, any
polynomial in the indeterminate $X$ with rational coefficients) can be
quasi-uniquely written in the form $P\left(  X\right)  =\sum_{i=0}^{d}%
c_{i}X^{i}$ with rational $c_{0},c_{1},\ldots,c_{d}$. The word
\textquotedblleft quasi-uniquely\textquotedblright\ here means that the
coefficients $c_{0},c_{1},\ldots,c_{d}$ are uniquely determined when
$d\in\mathbb{N}$ is specified; they are not literally unique because we can
always increase $d$ by adding new $0$ coefficients (for example, the
polynomial $\left(  1+X\right)  ^{2}$ can be written both as $1+2X+X^{2}$ and
as $1+2X+X^{2}+0X^{3}+0X^{4}$).

It is not hard to check that an analogue of this statement holds with the
$X^{i}$ replaced by the $\dbinom{X}{i}$:

\begin{proposition}
\label{prop.hartshorne}\textbf{(a)} Any polynomial $P\in\mathbb{Q}\left[
X\right]  $ can be quasi-uniquely written in the form $P\left(  X\right)
=\sum_{i=0}^{d}c_{i}\dbinom{X}{i}$ with rational $c_{0},c_{1},\ldots,c_{d}$.
(Again, \textquotedblleft quasi-uniquely\textquotedblright\ means that we can
always increase $d$ by adding new $0$ coefficients, but apart from this the
$c_{0},c_{1},\ldots,c_{d}$ are uniquely determined.)

\textbf{(b)} The polynomial $P$ is \textit{integer-valued} (i.e., its values
at integers are integers) if and only if these rationals $c_{0},c_{1}%
,\ldots,c_{d}$ are integers.
\end{proposition}

(We will not use this fact below, but it gives context to Theorem
\ref{thm.vandermonde} and Exercise \ref{exe.ps1.1.2} further below. The
\textquotedblleft if\textquotedblright\ part of Proposition
\ref{prop.hartshorne} \textbf{(b)} follows from (\ref{eq.binom.int}).)

We shall now prove some facts and give some exercises about binomial
coefficients; but let us first prove a fundamental property of polynomials:

\begin{lemma}
\label{lem.polyeq}\textbf{(a)} Let $P$ be a polynomial in the indeterminate
$X$ with rational coefficients. Assume that $P\left(  x\right)  =0$ for all
$x\in\mathbb{N}$. Then, $P=0$ as polynomials\footnote{Recall that two
polynomials are said to be equal if and only if their respective coefficients
are equal.}.

\textbf{(b)} Let $P$ and $Q$ be two polynomials in the indeterminate $X$ with
rational coefficients. Assume that $P\left(  x\right)  =Q\left(  x\right)  $
for all $x\in\mathbb{N}$. Then, $P=Q$ as polynomials.

\textbf{(c)} Let $P$ be a polynomial in the indeterminates $X$ and $Y$ with
rational coefficients. Assume that $P\left(  x,y\right)  =0$ for all
$x\in\mathbb{N}$ and $y\in\mathbb{N}$. Then, $P=0$ as polynomials.

\textbf{(d)} Let $P$ and $Q$ be two polynomials in the indeterminates $X$ and
$Y$ with rational coefficients. Assume that $P\left(  x,y\right)  =Q\left(
x,y\right)  $ for all $x\in\mathbb{N}$ and $y\in\mathbb{N}$. Then, $P=Q$ as polynomials.
\end{lemma}

Probably you have seen this lemma proven at least once in your life, but let
me still prove it for the sake of completeness.

\begin{proof}
[Proof of Lemma \ref{lem.polyeq}.]\textbf{(a)} The polynomial $P$ satisfies
$P\left(  x\right)  =0$ for every $x\in\mathbb{N}$. Hence, every
$x\in\mathbb{N}$ is a root of $P$. Thus, the polynomial $P$ has infinitely
many roots. But a nonzero polynomial in one variable (with rational
coefficients) can only have finitely many roots\footnote{In fact, a stronger
statement holds: A nonzero polynomial in one variable (with rational
coefficients) having degree $n\geq0$ has at most $n$ roots. See, for example,
\cite[Corollary 1.8.24]{Goodman} for a proof.}. If $P$ was nonzero, this would
force a contradiction with the sentence before. So $P$ must be zero. In other
words, $P=0$. Lemma \ref{lem.polyeq} \textbf{(a)} is proven.

\textbf{(b)} Every $x\in\mathbb{N}$ satisfies $\left(  P-Q\right)  \left(
x\right)  =P\left(  x\right)  -Q\left(  x\right)  =0$ (since $P\left(
x\right)  =Q\left(  x\right)  $). Hence, Lemma \ref{lem.polyeq} \textbf{(a)}
(applied to $P-Q$ instead of $P$) yields $P-Q=0$. Thus, $P=Q$. Lemma
\ref{lem.polyeq} \textbf{(b)} is thus proven.

\textbf{(c)} Every $x\in\mathbb{N}$ and $y\in\mathbb{N}$ satisfy%
\begin{equation}
P\left(  x,y\right)  =0. \label{pf.lem.polyeq.b.2}%
\end{equation}


We can write the polynomial $P$ in the form $P=\sum_{k=0}^{d}P_{k}\left(
X\right)  Y^{k}$, where $d$ is an integer and where each $P_{k}\left(
X\right)  $ (for $0\leq k\leq d$) is a polynomial in the single variable $X$.
Consider this $d$ and these $P_{k}\left(  X\right)  $.

Fix $\alpha\in\mathbb{N}$. Every $x\in\mathbb{N}$ satisfies%
\begin{align*}
P\left(  \alpha,x\right)   &  =\sum_{k=0}^{d}P_{k}\left(  \alpha\right)
x^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\alpha\text{ and
}x\text{ for }X\text{ and }Y\text{ in }P=\sum_{k=0}^{d}P_{k}\left(  X\right)
Y^{k}\right)  ,
\end{align*}
so that $\sum_{k=0}^{d}P_{k}\left(  \alpha\right)  x^{k}=P\left(
\alpha,x\right)  =0$ (by (\ref{pf.lem.polyeq.b.2}), applied to $\alpha$ and
$x$ instead of $x$ and $y$).

Therefore, Lemma \ref{lem.polyeq} \textbf{(a)} (applied to $\sum_{k=0}%
^{d}P_{k}\left(  \alpha\right)  X^{k}$ instead of $P$) yields that $\sum
_{k=0}^{d}P_{k}\left(  \alpha\right)  X^{k}=0$ as polynomials (in the
indeterminate $X$). In other words, all coefficients of the polynomial
$\sum_{k=0}^{d}P_{k}\left(  \alpha\right)  X^{k}$ are $0$. In other words,
$P_{k}\left(  \alpha\right)  =0$ for all $k\in\left\{  0,1,\ldots,d\right\}  $.

Now, let us forget that we fixed $\alpha$. We thus have shown that
$P_{k}\left(  \alpha\right)  =0$ for all $k\in\left\{  0,1,\ldots,d\right\}  $
and $\alpha\in\mathbb{N}$.

Let us now fix $k\in\left\{  0,1,\ldots,d\right\}  $. Then, $P_{k}\left(
\alpha\right)  =0$ for all $\alpha\in\mathbb{N}$. In other words,
$P_{k}\left(  x\right)  =0$ for all $x\in\mathbb{N}$. Hence, Lemma
\ref{lem.polyeq} \textbf{(a)} (applied to $P=P_{k}$) yields that $P_{k}=0$ as polynomials.

Let us forget that we fixed $k$. We thus have proven that $P_{k}=0$ as
polynomials for each $k\in\left\{  0,1,\ldots,d\right\}  $. Hence,
$P=\sum_{k=0}^{d}\underbrace{P_{k}\left(  X\right)  }_{=0}Y^{k}=0$. This
proves Lemma \ref{lem.polyeq} \textbf{(c)}.

\textbf{(d)} Every $x\in\mathbb{N}$ and $y\in\mathbb{N}$ satisfy%
\[
\left(  P-Q\right)  \left(  x,y\right)  =P\left(  x,y\right)  -Q\left(
x,y\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\left(  x,y\right)
=Q\left(  x,y\right)  \right)  .
\]
Hence, Lemma \ref{lem.polyeq} \textbf{(c)} (applied to $P-Q$ instead of $P$)
yields $P-Q=0$. Thus, $P=Q$. Lemma \ref{lem.polyeq} \textbf{(d)} is proven.
\end{proof}

Of course, Lemma \ref{lem.polyeq} can be generalized to polynomials in more
than two variables (the proof of Lemma \ref{lem.polyeq} \textbf{(c)}
essentially suggests how to prove this generalization by induction over the
number of variables).\footnote{If you know what a commutative ring is, you
might wonder whether Lemma \ref{lem.polyeq} can also be generalized to
polynomials with coefficients from other commutative rings (e.g., from
$\mathbb{R}$ or $\mathbb{C}$) instead of rational coefficients. In other
words, what happens if we replace \textquotedblleft rational
coefficients\textquotedblright\ by \textquotedblleft coefficients in
$R$\textquotedblright\ throughout Lemma \ref{lem.polyeq}, where $R$ is some
commutative ring? (Of course, we will then have to also replace $P\left(
x\right)  $ by $P\left(  x\cdot1_{R}\right)  $ and so on.)
\par
The answer is that Lemma \ref{lem.polyeq} becomes generally false if we don't
require anything more specific on $R$. However, there are certain conditions
on $R$ that make Lemma \ref{lem.polyeq} remain valid. For instance, Lemma
\ref{lem.polyeq} remains valid for $R=\mathbb{Z}$, for $R=\mathbb{R}$ and for
$R=\mathbb{C}$, as well as for $R$ being any polynomial ring over $\mathbb{Z}%
$, $\mathbb{Q}$, $\mathbb{R}$ or $\mathbb{C}$. More generally, Lemma
\ref{lem.polyeq} is valid if $R$ is any field of characteristic $0$ (i.e., any
field such that the elements $n\cdot1_{R}$ for $n$ ranging over $\mathbb{N}$
are pairwise distinct), or any subring of such a field.}

\subsection{The Chu-Vandermonde identity}

The following fact is known as the \textit{Chu-Vandermonde identity}%
\footnote{See
\href{https://en.wikipedia.org/wiki/Vandermonde\%27s_identity\#Chu-Vandermonde_identity}{the
Wikipedia page} for part of its history. Usually, the equality $\dbinom
{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}$ for two
\textbf{nonnegative integers} $x$ and $y$ is called the \textit{Vandermonde
identity}, whereas the name \textquotedblleft\textit{Chu-Vandermonde
identity}\textquotedblright\ is used for the identity $\dbinom{X+Y}{n}%
=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}$ in which $X$ and $Y$ are
\textbf{indeterminates}. However, this seems to be mostly a matter of
convention (which isn't even universally followed), and either way the two
identities are easily derived from one another as we will see in the first
proof of Theorem \ref{thm.vandermonde}.}:

\begin{theorem}
\label{thm.vandermonde}Let $n\in\mathbb{N}$. Then,%
\[
\dbinom{X+Y}{n}=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}%
\]
(an equality between polynomials in two variables $X$ and $Y$).
\end{theorem}

We will give two proofs of this theorem: one combinatorial, and one algebraic.

\begin{proof}
[First proof of Theorem \ref{thm.vandermonde}.]Let us first show that%
\begin{equation}
\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}
\label{pf.thm.vandermonde.eq.xy}%
\end{equation}
for any $x\in\mathbb{N}$ and $y\in\mathbb{N}$.

Keep in mind that (\ref{pf.thm.vandermonde.eq.xy}) and Theorem
\ref{thm.vandermonde} are different claims: The $x$ and $y$ in
(\ref{pf.thm.vandermonde.eq.xy}) are nonnegative integers, while the $X$ and
$Y$ in Theorem \ref{thm.vandermonde} are indeterminates!

\textit{Proof of (\ref{pf.thm.vandermonde.eq.xy}):} For every $N\in\mathbb{N}%
$, we let $\left[  N\right]  $ denote the $N$-element set $\left\{
1,2,\ldots,N\right\}  $.

Let $x\in\mathbb{N}$ and $y\in\mathbb{N}$. Recall that $\dbinom{x+y}{n}$ is
the number of $n$-element subsets of a given $\left(  x+y\right)  $-element
set\footnote{This follows from (\ref{eq.binom.subsets}).}. Since $\left[
x+y\right]  $ is an $\left(  x+y\right)  $-element set, we thus conclude that
$\dbinom{x+y}{n}$ is the number of $n$-element subsets of $\left[  x+y\right]
$.

But let us count the $n$-element subsets of $\left[  x+y\right]  $ in a
different way (i.e., find a different expression for the number of $n$-element
subsets of $\left[  x+y\right]  $). Namely, we can choose an $n$-element
subset $S$ of $\left[  x+y\right]  $ by means of the following process:

\begin{enumerate}
\item We decide how many elements of this subset $S$ will be among the numbers
$1,2,\ldots,x$. Let $k$ be the number of these elements. Clearly, $k$ must be
an integer between $0$ and $n$ (inclusive)\footnote{Because the subset $S$
will have $n$ elements in total, and thus at most $n$ of them can be among the
numbers $1,2,\ldots,x$.}.

\item Then, we choose these $k$ elements of $S$ among the numbers
$1,2,\ldots,x$. This can be done in $\dbinom{x}{k}$ different ways (because we
are choosing $k$ out of $x$ numbers, with no repetitions, and with no regard
for their order; in other words, we are choosing a $k$-element subset of
$\left\{  1,2,\ldots,x\right\}  $).

\item Then, we choose the remaining $n-k$ elements of $S$ (because $S$ should
have $n$ elements in total) among the remaining numbers $x+1,x+2,\ldots,x+y$.
This can be done in $\dbinom{y}{n-k}$ ways (because we are choosing $n-k$ out
of $y$ numbers, with no repetitions, and with no regard for their order).
\end{enumerate}

This process makes it clear that the total number of ways to choose an
$n$-element subset $S$ of $\left[  x+y\right]  $ is $\sum_{k=0}^{n}\dbinom
{x}{k}\dbinom{y}{n-k}$. In other words, the number of $n$-element subsets of
$\left[  x+y\right]  $ is $\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}$. But
earlier, we have shown that the same number is $\dbinom{x+y}{n}$. Comparing
these two results, we conclude that $\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom
{x}{k}\dbinom{y}{n-k}$. Thus, (\ref{pf.thm.vandermonde.eq.xy}) is proven.

Now, we need to prove Theorem \ref{thm.vandermonde} itself. We define two
polynomials $P$ and $Q$ in the indeterminates $X$ and $Y$ with rational
coefficients by setting%
\begin{align*}
P  &  =\dbinom{X+Y}{n};\\
Q  &  =\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}%
\end{align*}
\footnote{These are both polynomials since $\dbinom{X+Y}{n}$, $\dbinom{X}{k}$
and $\dbinom{Y}{n-k}$ are polynomials in $X$ and $Y$.}. The equality
(\ref{pf.thm.vandermonde.eq.xy}) (which we have proven) states that $P\left(
x,y\right)  =Q\left(  x,y\right)  $ for all $x\in\mathbb{N}$ and
$y\in\mathbb{N}$. Thus, Lemma \ref{lem.polyeq} \textbf{(d)} yields that $P=Q$.
Recalling how $P$ and $Q$ are defined, we can rewrite this as $\dbinom{X+Y}%
{n}=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}$. This proves Theorem
\ref{thm.vandermonde}.
\end{proof}

The argument that we used at the end of the above proof to derive Theorem
\ref{thm.vandermonde} from (\ref{pf.thm.vandermonde.eq.xy}) is a very common
argument that appears in proofs of equalities for binomial coefficients.
Binomial coefficients are polynomials that can be evaluated at many different
values\footnote{For example, terms like $\dbinom{-1/2}{3}$, $\dbinom
{2+\sqrt{3}}{5}$ and $\dbinom{-7}{0}$ make perfect sense. Generally,
$\dbinom{X}{n}$ is a polynomial in $X$ with rational coefficients, and thus we
can substitute any complex number for $X$. So $\dbinom{m}{n}$ is well-defined
for all $m\in\mathbb{C}$ and $n\in\mathbb{N}$. (But we cannot substitute
arbitrary complex numbers for $n$. So far we have only defined $\dbinom{X}{n}$
for $n\in\mathbb{N}$. It is usual to define $\dbinom{X}{n}$ to mean $0$ for
negative integers $n$, and using analysis (specifically, the $\Gamma$
function) it is possible to give a reasonable meaning to $\dbinom{m}{n}$ for
$m$ and $n$ being reals, but this will no longer be a polynomial in $m$.)},
but their combinatorial interpretation (via counting subsets) only makes sense
when they are evaluated at nonnegative integers. Thus, if we want to prove an
identity of the form $P=Q$ (where $P$ and $Q$ are two polynomials, say, in two
indeterminates $X$ and $Y$) using the combinatorial interpretation of binomial
coefficients, then a reasonable tactic is to first show that $P\left(
x,y\right)  =Q\left(  x,y\right)  $ for all $x\in\mathbb{N}$ and
$y\in\mathbb{N}$ (using combinatorics), and then to use something like Lemma
\ref{lem.polyeq} in order to conclude that $P$ and $Q$ are equal as
polynomials. We shall see this tactic used a few more times.

But we promised an algebraic proof of Theorem \ref{thm.vandermonde} as well;
let us show it now:

\begin{proof}
[Second proof of Theorem \ref{thm.vandermonde}.]We shall prove Theorem
\ref{thm.vandermonde} by induction over $n$.

\textit{Induction base:} We have $\dbinom{X}{0}=1$. Substituting $Y$ for $X$
in this equality, we obtain $\dbinom{Y}{0}=1$. Hence,
\begin{equation}
\sum_{k=0}^{0}\dbinom{X}{k}\dbinom{Y}{0-k}=\underbrace{\dbinom{X}{0}}%
_{=1}\underbrace{\dbinom{Y}{0-0}}_{=\dbinom{Y}{0}=1}=1.
\label{pf.thm.vandermonde.pf.2.1}%
\end{equation}


But substituting $X+Y$ for $X$ in $\dbinom{X}{0}=1$, we obtain $\dbinom
{X+Y}{0}=1$. Compared with (\ref{pf.thm.vandermonde.pf.2.1}), this yields
$\dbinom{X+Y}{0}=\sum_{k=0}^{0}\dbinom{X}{k}\dbinom{Y}{0-k}$. In other words,
Theorem \ref{thm.vandermonde} holds for $n=0$. This completes the induction base.

\textit{Induction step:} Let $N$ be a positive integer. Assume that Theorem
\ref{thm.vandermonde} holds for $n=N-1$. We need to prove that Theorem
\ref{thm.vandermonde} holds for $n=N$. In other words, we need to prove that%
\begin{equation}
\dbinom{X+Y}{N}=\sum_{k=0}^{N}\dbinom{X}{k}\dbinom{Y}{N-k}.
\label{pf.thm.vandermonde.pf.2.goal}%
\end{equation}
(Don't be fooled by the $N$ being uppercase! The $X$ and the $Y$ are
indeterminates, while the $N$ is a fixed positive integer.)

We have assumed that Theorem \ref{thm.vandermonde} holds for $n=N-1$. In other
words, we have%
\begin{equation}
\dbinom{X+Y}{N-1}=\sum_{k=0}^{N-1}\dbinom{X}{k}\dbinom{Y}{\left(  N-1\right)
-k}. \label{pf.thm.vandermonde.pf.2.4}%
\end{equation}


Substituting $X-1$ for $X$ in this equality, we obtain%
\begin{align*}
\dbinom{X-1+Y}{N-1}  &  =\sum_{k=0}^{N-1}\dbinom{X-1}{k}\dbinom{Y}{\left(
N-1\right)  -k}\\
&  =\sum_{k=1}^{N}\dbinom{X-1}{k-1}\underbrace{\dbinom{Y}{\left(  N-1\right)
-\left(  k-1\right)  }}_{=\dbinom{Y}{N-k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k-1\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=1}^{N}\dbinom{X-1}{k-1}\dbinom{Y}{N-k}.
\end{align*}
Since $X-1+Y=X+Y-1$, this rewrites as%
\begin{equation}
\dbinom{X+Y-1}{N-1}=\sum_{k=1}^{N}\dbinom{X-1}{k-1}\dbinom{Y}{N-k}.
\label{pf.thm.vandermonde.pf.2.5}%
\end{equation}
On the other hand, we can substitute $Y-1$ for $Y$ in the equality
(\ref{pf.thm.vandermonde.pf.2.4}). As a result, we obtain%
\begin{equation}
\dbinom{X+Y-1}{N-1}=\sum_{k=0}^{N-1}\dbinom{X}{k}\underbrace{\dbinom
{Y-1}{\left(  N-1\right)  -k}}_{=\dbinom{Y-1}{N-k-1}}=\sum_{k=0}^{N-1}%
\dbinom{X}{k}\dbinom{Y-1}{N-k-1}. \label{pf.thm.vandermonde.pf.2.6}%
\end{equation}


Next, we notice a simple consequence of (\ref{eq.binom.X-1}): We have%
\begin{equation}
\dfrac{X}{N}\dbinom{X-1}{a-1}=\dfrac{a}{N}\dbinom{X}{a}%
\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\left\{  1,2,3,\ldots\right\}
\label{pf.thm.vandermonde.pf.2.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.vandermonde.pf.2.7}):} Let
$a\in\left\{  1,2,3,\ldots\right\}  $. Then, (\ref{eq.binom.X-1}) (applied to
$n=a$) yields $\dbinom{X}{a}=\dfrac{X}{a}\dbinom{X-1}{a-1}$. Hence,%
\[
\dfrac{a}{N}\underbrace{\dbinom{X}{a}}_{=\dfrac{X}{a}\dbinom{X-1}{a-1}%
}=\underbrace{\dfrac{a}{N}\cdot\dfrac{X}{a}}_{=\dfrac{X}{N}}\dbinom{X-1}%
{a-1}=\dfrac{X}{N}\dbinom{X-1}{a-1}.
\]
This proves (\ref{pf.thm.vandermonde.pf.2.7}).}. Also,%
\begin{equation}
\dfrac{Y}{N}\dbinom{Y-1}{a-1}=\dfrac{a}{N}\dbinom{Y}{a}%
\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\left\{  1,2,3,\ldots\right\}  .
\label{pf.thm.vandermonde.pf.2.8}%
\end{equation}
(This follows by substituting $Y$ for $X$ in (\ref{pf.thm.vandermonde.pf.2.7}).)

We have%
\begin{align*}
\dfrac{X}{N}\underbrace{\dbinom{X+Y-1}{N-1}}_{\substack{=\sum_{k=1}^{N}%
\dbinom{X-1}{k-1}\dbinom{Y}{N-k}\\\text{(by (\ref{pf.thm.vandermonde.pf.2.5}%
))}}}  &  =\dfrac{X}{N}\sum_{k=1}^{N}\dbinom{X-1}{k-1}\dbinom{Y}{N-k}\\
&  =\sum_{k=1}^{N}\underbrace{\dfrac{X}{N}\dbinom{X-1}{k-1}}%
_{\substack{=\dfrac{k}{N}\dbinom{X}{k}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.7}),}\\\text{applied to }a=k\text{)}}}\dbinom
{Y}{N-k}=\sum_{k=1}^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}.
\end{align*}
Compared with%
\begin{align*}
&  \sum_{k=0}^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}\\
&  =\underbrace{\dfrac{0}{N}}_{=0}\dbinom{X}{0}\dbinom{Y}{N-0}+\sum_{k=1}%
^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}=\sum_{k=1}^{N}\dfrac{k}{N}%
\dbinom{X}{k}\dbinom{Y}{N-k},
\end{align*}
this yields%
\begin{equation}
\dfrac{X}{N}\dbinom{X+Y-1}{N-1}=\sum_{k=0}^{N}\dfrac{k}{N}\dbinom{X}{k}%
\dbinom{Y}{N-k}. \label{pf.thm.vandermonde.pf.2.11}%
\end{equation}


We also have%
\begin{align*}
\dfrac{Y}{N}\underbrace{\dbinom{X+Y-1}{N-1}}_{\substack{=\sum_{k=0}%
^{N-1}\dbinom{X}{k}\dbinom{Y-1}{N-k-1}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.6}))}}}  &  =\dfrac{Y}{N}\sum_{k=0}^{N-1}%
\dbinom{X}{k}\dbinom{Y-1}{N-k-1}\\
&  =\sum_{k=0}^{N-1}\dbinom{X}{k}\underbrace{\dfrac{Y}{N}\dbinom{Y-1}{N-k-1}%
}_{\substack{=\dfrac{N-k}{N}\dbinom{Y}{N-k}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.8}),}\\\text{applied to }a=N-k\text{)}}}\\
&  =\sum_{k=0}^{N-1}\dbinom{X}{k}\dfrac{N-k}{N}\dbinom{Y}{N-k}=\sum
_{k=0}^{N-1}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}.
\end{align*}
Compared with%
\begin{align*}
\sum_{k=0}^{N}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}  &  =\sum_{k=0}%
^{N-1}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}+\underbrace{\dfrac{N-N}{N}%
}_{=0}\dbinom{X}{N}\dbinom{Y}{N-N}\\
&  =\sum_{k=0}^{N-1}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k},
\end{align*}
this yields%
\begin{equation}
\dfrac{Y}{N}\dbinom{X+Y-1}{N-1}=\sum_{k=0}^{N}\dfrac{N-k}{N}\dbinom{X}%
{k}\dbinom{Y}{N-k}. \label{pf.thm.vandermonde.pf.2.12}%
\end{equation}


Now, (\ref{eq.binom.X-1}) (applied to $n=N$) yields%
\[
\dbinom{X}{N}=\dfrac{X}{N}\dbinom{X-1}{N-1}.
\]
Substituting $X+Y$ for $X$ in this equality, we obtain%
\begin{align*}
\dbinom{X+Y}{N}  &  =\underbrace{\dfrac{X+Y}{N}}_{=\dfrac{X}{N}+\dfrac{Y}{N}%
}\dbinom{X+Y-1}{N-1}=\left(  \dfrac{X}{N}+\dfrac{Y}{N}\right)  \dbinom
{X+Y-1}{N-1}\\
&  =\underbrace{\dfrac{X}{N}\dbinom{X+Y-1}{N-1}}_{\substack{=\sum_{k=0}%
^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.11}))}}}+\underbrace{\dfrac{Y}{N}\dbinom
{X+Y-1}{N-1}}_{\substack{=\sum_{k=0}^{N}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom
{Y}{N-k}\\\text{(by (\ref{pf.thm.vandermonde.pf.2.12}))}}}\\
&  =\sum_{k=0}^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}+\sum_{k=0}%
^{N}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}\\
&  =\sum_{k=0}^{N}\left(  \underbrace{\dfrac{k}{N}+\dfrac{N-k}{N}}%
_{=1}\right)  \dbinom{X}{k}\dbinom{Y}{N-k}=\sum_{k=0}^{N}\dbinom{X}{k}%
\dbinom{Y}{N-k}.
\end{align*}
This proves (\ref{pf.thm.vandermonde.pf.2.goal}). In other words, Theorem
\ref{thm.vandermonde} holds for $n=N$. This completes the induction step.
Thus, the induction proof of Theorem \ref{thm.vandermonde} is complete.
\end{proof}

Let us give some sample applications of Theorem \ref{thm.vandermonde}:

\begin{proposition}
\label{prop.vandermonde.consequences}

\textbf{(a)} For every $x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and
$n\in\mathbb{N}$, we have
\[
\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}.
\]


\textbf{(b)} For every $x\in\mathbb{N}$ and $y\in\mathbb{Z}$, we have%
\[
\dbinom{x+y}{x}=\sum_{k=0}^{x}\dbinom{x}{k}\dbinom{y}{k}.
\]


\textbf{(c)} For every $n\in\mathbb{N}$, we have%
\[
\dbinom{2n}{n}=\sum_{k=0}^{n}\dbinom{n}{k}^{2}.
\]


\textbf{(d)} For every $x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and
$n\in\mathbb{N}$, we have%
\[
\dbinom{x-y}{n}=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{n-k}%
\dbinom{k+y-1}{k}.
\]


\textbf{(e)} For every $x\in\mathbb{N}$ and $y\in\mathbb{Z}$ and
$n\in\mathbb{N}$ with $x\leq n$, we have%
\[
\dbinom{y-x-1}{n-x}=\sum_{k=0}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}%
{x}\dbinom{y}{n-k}.
\]


\textbf{(f)} For every $x\in\mathbb{N}$ and $y\in\mathbb{N}$ and
$n\in\mathbb{N}$, we have%
\[
\dbinom{n+1}{x+y+1}=\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}.
\]


\textbf{(g)} For every $x\in\mathbb{Z}$ and $y\in\mathbb{N}$ and
$n\in\mathbb{N}$ satisfying $x+y\geq0$ and $n\geq x$, we have%
\[
\dbinom{x+y}{n}=\sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}.
\]

\end{proposition}

\begin{remark}
I have learnt Proposition \ref{prop.vandermonde.consequences} \textbf{(f)}
from \href{http://www.artofproblemsolving.com/community/c6h447278}{the AoPS
forum}. Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} is a
generalization of Proposition \ref{prop.vandermonde.consequences} \textbf{(b)}.
\end{remark}

\begin{proof}
[Proof of Proposition \ref{prop.vandermonde.consequences}.]\textbf{(a)} Let
$x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and $n\in\mathbb{N}$. Theorem
\ref{thm.vandermonde} yields $\dbinom{X+Y}{n}=\sum_{k=0}^{n}\dbinom{X}%
{k}\dbinom{Y}{n-k}$. Substituting $x$ and $y$ for $X$ and $Y$ in this
equality, we obtain $\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom
{y}{n-k}$. Thus, Proposition \ref{prop.vandermonde.consequences} \textbf{(a)}
is proven.

\textbf{(b)} Let $x\in\mathbb{N}$ and $y\in\mathbb{Z}$. Proposition
\ref{prop.vandermonde.consequences} \textbf{(a)} (applied to $y$, $x$ and $x$
instead of $x$, $y$ and $n$) yields%
\[
\dbinom{y+x}{x}=\sum_{k=0}^{x}\dbinom{y}{k}\dbinom{x}{x-k}.
\]
Compared with
\[
\sum_{k=0}^{x}\underbrace{\dbinom{x}{k}}_{\substack{=\dbinom{x}{x-k}%
\\\text{(by (\ref{eq.binom.symm}), applied to }m=x\text{ and }n=k\text{)}%
}}\dbinom{y}{k}=\sum_{k=0}^{x}\dbinom{x}{x-k}\dbinom{y}{k}=\sum_{k=0}%
^{x}\dbinom{y}{k}\dbinom{x}{x-k},
\]
this yields $\dbinom{y+x}{x}=\sum_{k=0}^{x}\dbinom{x}{k}\dbinom{y}{k}$. Since
$y+x=x+y$, this rewrites as $\dbinom{x+y}{x}=\sum_{k=0}^{x}\dbinom{x}%
{k}\dbinom{y}{k}$. This proves Proposition \ref{prop.vandermonde.consequences}
\textbf{(b)}.

\textbf{(c)} Let $n\in\mathbb{N}$. Applying Proposition
\ref{prop.vandermonde.consequences} \textbf{(b)} to $x=n$ and $y=n$, we obtain%
\[
\dbinom{n+n}{n}=\sum_{k=0}^{n}\underbrace{\dbinom{n}{k}\dbinom{n}{k}%
}_{=\dbinom{n}{k}^{2}}=\sum_{k=0}^{n}\dbinom{n}{k}^{2}.
\]
Since $n+n=2n$, this rewrites as $\dbinom{2n}{n}=\sum_{k=0}^{n}\dbinom{n}%
{k}^{2}$. This proves Proposition \ref{prop.vandermonde.consequences}
\textbf{(c)}.

\textbf{(d)} Let $x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and $n\in\mathbb{N}$.
Proposition \ref{prop.vandermonde.consequences} \textbf{(a)} (applied to $-y$
instead of $y$) yields%
\begin{align*}
\dbinom{x+\left(  -y\right)  }{n}  &  =\sum_{k=0}^{n}\dbinom{x}{k}\dbinom
{-y}{n-k}=\sum_{k=0}^{n}\dbinom{x}{n-k}\underbrace{\dbinom{-y}{n-\left(
n-k\right)  }}_{=\dbinom{-y}{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n-k\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=0}^{n}\dbinom{x}{n-k}\underbrace{\dbinom{-y}{k}}%
_{\substack{=\left(  -1\right)  ^{k}\dbinom{k-\left(  -y\right)  -1}%
{k}\\\text{(by (\ref{eq.binom.upper-neg}), applied to }-y\text{ and }k\text{
instead of }n\text{ and }m\text{)}}}\\
&  =\sum_{k=0}^{n}\underbrace{\dbinom{x}{n-k}\left(  -1\right)  ^{k}%
}_{=\left(  -1\right)  ^{k}\dbinom{x}{n-k}}\underbrace{\dbinom{k-\left(
-y\right)  -1}{k}}_{=\dbinom{k+y-1}{k}}\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{n-k}\dbinom{k+y-1}{k}.
\end{align*}
Since $x+\left(  -y\right)  =x-y$, this rewrites as $\dbinom{x-y}{n}%
=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{n-k}\dbinom{k+y-1}{k}$. This
proves Proposition \ref{prop.vandermonde.consequences} \textbf{(d)}.

\textbf{(e)} Let $x\in\mathbb{N}$ and $y\in\mathbb{Z}$ and $n\in\mathbb{N}$ be
such that $x\leq n$. From $x\in\mathbb{N}$, we obtain $0\leq x$ and thus
$0\leq x\leq n$. We notice that every integer $k\geq x$ satisfies%
\begin{equation}
\dbinom{k}{k-x}=\dbinom{k}{x} \label{pf.prop.vandermonde.consequences.e.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.vandermonde.consequences.e.1}):} Let
$k$ be an integer such that $k\geq x$. Thus, $k-x\in\mathbb{N}$. Also, $k\geq
k-x$ (since $x\in\mathbb{N}$). Hence, (\ref{eq.binom.symm}) (applied to $k$
and $x$ instead of $m$ and $n$) yields $\dbinom{k}{x}=\dbinom{k}{k-x}$. This
proves (\ref{pf.prop.vandermonde.consequences.e.1}).}. Furthermore,
$n-x\in\mathbb{N}$ (since $x\leq n$). Hence, we can apply Proposition
\ref{prop.vandermonde.consequences} \textbf{(a)} to $y$, $-x-1$ and $n-x$
instead of $x$, $y$ and $n$. As a result, we obtain%
\begin{align*}
\dbinom{y+\left(  -x-1\right)  }{n-x}  &  =\sum_{k=0}^{n-x}\dbinom{y}%
{k}\underbrace{\dbinom{-x-1}{\left(  n-x\right)  -k}}_{\substack{=\left(
-1\right)  ^{\left(  n-x\right)  -k}\dbinom{\left(  \left(  n-x\right)
-k\right)  -\left(  -x-1\right)  -1}{\left(  n-x\right)  -k}\\\text{(by
(\ref{eq.binom.upper-neg}), applied to }-x-1\text{ and }\left(  n-x\right)
-k\\\text{instead of }m\text{ and }n\text{)}}}\\
&  =\sum_{k=0}^{n-x}\dbinom{y}{k}\left(  -1\right)  ^{\left(  n-x\right)
-k}\underbrace{\dbinom{\left(  \left(  n-x\right)  -k\right)  -\left(
-x-1\right)  -1}{\left(  n-x\right)  -k}}_{\substack{=\dbinom{n-k}{\left(
n-x\right)  -k}\\\text{(since }\left(  \left(  n-x\right)  -k\right)  -\left(
-x-1\right)  -1=n-k\text{)}}}\\
&  =\sum_{k=0}^{n-x}\dbinom{y}{k}\left(  -1\right)  ^{\left(  n-x\right)
-k}\dbinom{n-k}{\left(  n-x\right)  -k}\\
&  =\underbrace{\sum_{k=n-\left(  n-x\right)  }^{n}}_{\substack{=\sum
_{k=x}^{n}\\\text{(since }n-\left(  n-x\right)  =x\text{)}}}\dbinom{y}%
{n-k}\underbrace{\left(  -1\right)  ^{\left(  n-x\right)  -\left(  n-k\right)
}}_{\substack{=\left(  -1\right)  ^{k-x}\\\text{(since }\left(  n-x\right)
-\left(  n-k\right)  =k-x\text{)}}}\underbrace{\dbinom{n-\left(  n-k\right)
}{\left(  n-x\right)  -\left(  n-k\right)  }}_{\substack{=\dbinom{k}%
{k-x}\\\text{(since }n-\left(  n-k\right)  =k\\\text{and }\left(  n-x\right)
-\left(  n-k\right)  =k-x\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n-k\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=x}^{n}\dbinom{y}{n-k}\left(  -1\right)  ^{k-x}\underbrace{\dbinom
{k}{k-x}}_{\substack{=\dbinom{k}{x}\\\text{(by
(\ref{pf.prop.vandermonde.consequences.e.1}))}}}=\sum_{k=x}^{n}\dbinom{y}%
{n-k}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\\
&  =\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}.
\end{align*}
Compared with%
\begin{align*}
&  \sum_{k=0}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}\\
&  =\sum_{k=0}^{x-1}\left(  -1\right)  ^{k-x}\underbrace{\dbinom{k}{x}%
}_{\substack{=0\\\text{(by (\ref{eq.binom.0}), applied to }k\text{ and
}x\\\text{instead of }m\text{ and }n\text{ (since }k<x\text{))}}}\dbinom
{y}{n-k}+\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq x\leq n\right) \\
&  =\underbrace{\sum_{k=0}^{x-1}\left(  -1\right)  ^{k-x}0\dbinom{y}{n-k}%
}_{=0}+\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}%
{n-k}=\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k},
\end{align*}
this yields%
\[
\dbinom{y+\left(  -x-1\right)  }{n-x}=\sum_{k=0}^{n}\left(  -1\right)
^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}.
\]
In other words,
\[
\dbinom{y-x-1}{n-x}=\sum_{k=0}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}%
{x}\dbinom{y}{n-k}%
\]
(since $y+\left(  -x-1\right)  =y-x-1$). This proves Proposition
\ref{prop.vandermonde.consequences} \textbf{(e)}.

\textbf{(f)} Let $x\in\mathbb{N}$ and $y\in\mathbb{N}$ and $n\in\mathbb{N}$.
We must be in one of the following two cases:

\textit{Case 1:} We have $n<x+y$.

\textit{Case 2:} We have $n\geq x+y$.

Let us first consider Case 1. In this case, we have $n<x+y$. Thus,
$n+1<x+y+1$. Therefore, $\dbinom{n+1}{x+y+1}=0$ (by (\ref{eq.binom.0}),
applied to $n+1$ and $x+y+1$ instead of $m$ and $n$). But every $k\in\left\{
0,1,\ldots,n\right\}  $ satisfies $\dbinom{k}{x}\dbinom{n-k}{y}=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  0,1,\ldots,n\right\}  $.
We need to show that $\dbinom{k}{x}\dbinom{n-k}{y}=0$.
\par
If we have $k<x$, then we have $\dbinom{k}{x}=0$ (by (\ref{eq.binom.0}),
applied to $k$ and $x$ instead of $m$ and $n$). Therefore, if we have $k<x$,
then $\underbrace{\dbinom{k}{x}}_{=0}\dbinom{n-k}{y}=0$. Hence, for the rest
of this proof of $\dbinom{k}{x}\dbinom{n-k}{y}=0$, we can WLOG assume that we
don't have $k<x$. Assume this.
\par
We have $k\geq x$ (since we don't have $k<x$), and thus $n-\underbrace{k}%
_{\geq x}\leq n-x<y$ (since $n<x+y$). Hence, $\dbinom{n-k}{y}=0$ (by
(\ref{eq.binom.0}), applied to $n-k$ and $y$ instead of $m$ and $n$).
Therefore, $\dbinom{k}{x}\underbrace{\dbinom{n-k}{y}}_{=0}=0$, qed.}. Hence,
$\sum_{k=0}^{n}\underbrace{\dbinom{k}{x}\dbinom{n-k}{y}}_{=0}=\sum_{k=0}%
^{n}0=0$. Compared with $\dbinom{n+1}{x+y+1}=0$, this yields $\dbinom
{n+1}{x+y+1}=\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}$. Thus, Proposition
\ref{prop.vandermonde.consequences} \textbf{(f)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $n\geq x+y$. Hence, $n-y\geq
x\geq0$ (since $x\in\mathbb{N}$), so that $\left(  n-y\right)  -x\in
\mathbb{N}$. Also, $n-y\in\mathbb{N}$ (since $n-y\geq0$). Moreover, $x\leq
n-y$. Therefore, we can apply Proposition \ref{prop.vandermonde.consequences}
\textbf{(e)} to $-y-1$ and $n-y$ instead of $y$ and $n$. As a result, we
obtain%
\begin{align}
&  \dbinom{\left(  -y-1\right)  -x-1}{\left(  n-y\right)  -x}\nonumber\\
&  =\sum_{k=0}^{n-y}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\underbrace{\dbinom
{-y-1}{\left(  n-y\right)  -k}}_{\substack{=\left(  -1\right)  ^{\left(
n-y\right)  -k}\dbinom{\left(  \left(  n-y\right)  -k\right)  -\left(
-y-1\right)  -1}{\left(  n-y\right)  -k}\\\text{(by (\ref{eq.binom.upper-neg}%
), applied to }-y-1\text{ and }\left(  n-y\right)  -k\\\text{instead of
}m\text{ and }n\text{)}}}\nonumber\\
&  =\sum_{k=0}^{n-y}\left(  -1\right)  ^{k-x}\underbrace{\dbinom{k}{x}\left(
-1\right)  ^{\left(  n-y\right)  -k}}_{=\left(  -1\right)  ^{\left(
n-y\right)  -k}\dbinom{k}{x}}\underbrace{\dbinom{\left(  \left(  n-y\right)
-k\right)  -\left(  -y-1\right)  -1}{\left(  n-y\right)  -k}}%
_{\substack{=\dbinom{n-k}{\left(  n-y\right)  -k}\\\text{(since }\left(
\left(  n-y\right)  -k\right)  -\left(  -y-1\right)  -1=n-k\text{)}%
}}\nonumber\\
&  =\sum_{k=0}^{n-y}\underbrace{\left(  -1\right)  ^{k-x}\left(  -1\right)
^{\left(  n-y\right)  -k}}_{\substack{=\left(  -1\right)  ^{\left(
k-x\right)  +\left(  \left(  n-y\right)  -k\right)  }=\left(  -1\right)
^{n-x-y}\\\text{(since }\left(  k-x\right)  +\left(  \left(  n-y\right)
-k\right)  =n-x-y\text{)}}}\dbinom{k}{x}\dbinom{n-k}{\left(  n-y\right)
-k}\nonumber\\
&  =\sum_{k=0}^{n-y}\left(  -1\right)  ^{n-x-y}\dbinom{k}{x}\dbinom
{n-k}{\left(  n-y\right)  -k}=\left(  -1\right)  ^{n-x-y}\sum_{k=0}%
^{n-y}\dbinom{k}{x}\dbinom{n-k}{\left(  n-y\right)  -k}%
.\label{pf.prop.vandermonde.consequences.f.0}%
\end{align}


But every $k\in\left\{  0,1,\ldots,n-y\right\}  $ satisfies%
\begin{equation}
\dbinom{n-k}{\left(  n-y\right)  -k}=\dbinom{n-k}{y}
\label{pf.prop.vandermonde.consequences.f.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.vandermonde.consequences.f.0}):} Let
$k\in\left\{  0,1,\ldots,n-y\right\}  $. Then, $k\in\mathbb{N}$ and $n-y\geq
k$. From $n-y\geq k$, we obtain $n\geq y+k$, so that $n-k\geq y$. Thus,
$n-k\geq y\geq0$, so that $n-k\in\mathbb{N}$. Hence, (\ref{eq.binom.symm})
(applied to $n-k$ and $y$ instead of $m$ and $n$) yields $\dbinom{n-k}%
{y}=\dbinom{n-k}{\left(  n-k\right)  -y}=\dbinom{n-k}{\left(  n-y\right)  -k}$
(since $\left(  n-k\right)  -y=\left(  n-y\right)  -k$). This proves
(\ref{pf.prop.vandermonde.consequences.f.1}).}. Thus,
(\ref{pf.prop.vandermonde.consequences.f.0}) yields%
\begin{align*}
\dbinom{\left(  -y-1\right)  -x-1}{\left(  n-y\right)  -x}  &  =\left(
-1\right)  ^{n-x-y}\sum_{k=0}^{n-y}\dbinom{k}{x}\underbrace{\dbinom
{n-k}{\left(  n-y\right)  -k}}_{\substack{=\dbinom{n-k}{y}\\\text{(by
(\ref{pf.prop.vandermonde.consequences.f.1}))}}}\\
&  =\left(  -1\right)  ^{n-x-y}\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}.
\end{align*}
Compared with%
\begin{align*}
\dbinom{\left(  -y-1\right)  -x-1}{\left(  n-y\right)  -x}  &
=\underbrace{\left(  -1\right)  ^{\left(  n-y\right)  -x}}_{\substack{=\left(
-1\right)  ^{n-x-y}\\\text{(since }\left(  n-y\right)  -x=n-x-y\text{)}%
}}\underbrace{\dbinom{\left(  \left(  n-y\right)  -x\right)  -\left(  \left(
-y-1\right)  -x-1\right)  -1}{\left(  n-y\right)  -x}}_{\substack{=\dbinom
{n+1}{n-x-y}\\\text{(since }\left(  \left(  n-y\right)  -x\right)  -\left(
\left(  -y-1\right)  -x-1\right)  -1=n+1\\\text{and }\left(  n-y\right)
-x=n-x-y\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.binom.upper-neg}), applied to }\left(  -y-1\right)
-x-1\text{ and }\left(  n-y\right)  -x\\
\text{instead of }m\text{ and }n
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n-x-y}\dbinom{n+1}{n-x-y},
\end{align*}
this yields%
\[
\left(  -1\right)  ^{n-x-y}\dbinom{n+1}{n-x-y}=\left(  -1\right)  ^{n-x-y}%
\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}.
\]
We can cancel $\left(  -1\right)  ^{n-x-y}$ from this equality (because
$\left(  -1\right)  ^{n-x-y}\neq0$). As a result, we obtain%
\begin{equation}
\dbinom{n+1}{n-x-y}=\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}.
\label{pf.prop.vandermonde.consequences.f.5}%
\end{equation}


But $0\leq n-y$ (since $n-y\in\mathbb{N}$) and $n-y\leq n$ (since
$y\in\mathbb{N}$). Also, every $k\in\left\{  n-y+1,n-y+2,\ldots,n\right\}  $
satisfies%
\begin{equation}
\dbinom{n-k}{y}=0 \label{pf.prop.vandermonde.consequences.f.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.vandermonde.consequences.f.2}):} Let
$k\in\left\{  n-y+1,n-y+2,\ldots,n\right\}  $. Then, $k\leq n$ and $k>n-y$.
Hence, $n-k\in\mathbb{N}$ (since $k\leq n$) and $n-\underbrace{k}%
_{>n-y}<n-\left(  n-y\right)  =y$. Therefore, (\ref{eq.binom.0}) (applied to
$n-k$ and $y$ instead of $m$ and $n$) yields $\dbinom{n-k}{y}=0$. This proves
(\ref{pf.prop.vandermonde.consequences.f.2}).}. Hence,%
\begin{align}
\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}  &  =\sum_{k=0}^{n-y}\dbinom{k}%
{x}\dbinom{n-k}{y}+\sum_{k=n-y+1}^{n}\dbinom{k}{x}\underbrace{\dbinom{n-k}{y}%
}_{\substack{=0\\\text{(by (\ref{pf.prop.vandermonde.consequences.f.2}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq n-y\leq n\right) \nonumber\\
&  =\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}+\underbrace{\sum
_{k=n-y+1}^{n}\dbinom{k}{x}0}_{=0}=\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom
{n-k}{y}\nonumber\\
&  =\dbinom{n+1}{n-x-y}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.vandermonde.consequences.f.5})}\right)  .
\label{pf.prop.vandermonde.consequences.f.9}%
\end{align}


Finally, $n+1\in\mathbb{N}$ and $x+y+1\in\mathbb{N}$ (since $x\in\mathbb{N}$
and $y\in\mathbb{N}$) and $\underbrace{n}_{\geq x+y}+1\geq x+y+1$. Hence,
(\ref{eq.binom.symm}) (applied to $n+1$ and $x+y+1$ instead of $m$ and $n$)
yields
\[
\dbinom{n+1}{x+y+1}=\dbinom{n+1}{\left(  n+1\right)  -\left(  x+y+1\right)
}=\dbinom{n+1}{n-x-y}.
\]
Comparing this with (\ref{pf.prop.vandermonde.consequences.f.9}), we obtain%
\[
\dbinom{n+1}{x+y+1}=\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}.
\]


Thus, Proposition \ref{prop.vandermonde.consequences} \textbf{(f)} is proven
in Case 2.

We have now proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(f)} in both Cases 1 and 2; thus, Proposition
\ref{prop.vandermonde.consequences} \textbf{(f)} always holds.

\textbf{(g)} Let $x\in\mathbb{Z}$ and $y\in\mathbb{N}$ and $n\in\mathbb{N}$ be
such that $x+y\geq0$ and $n\geq x$. We must be in one of the following two cases:

\textit{Case 1:} We have $x+y<n$.

\textit{Case 2:} We have $x+y\geq n$.

Let us first consider Case 1. In this case, we have $x+y<n$. Thus,
$\dbinom{x+y}{n}=0$ (by (\ref{eq.binom.0}), applied to $m=x+y$). But every
$k\in\left\{  0,1,\ldots,x+y\right\}  $ satisfies $\dbinom{y}{n+k-x}%
=0$\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  0,1,\ldots,x+y\right\}
$. Then, $k\geq0$, so that $n+\underbrace{k}_{\geq0}-x\geq n-x>y$ (since
$n>x+y$ (since $x+y<n$)). In other words, $y<n+k-x$. Hence, $\dbinom{y}%
{n+k-x}=0$ (by (\ref{eq.binom.0}), applied to $y$ and $n+k-x$ instead of $m$
and $n$). Qed.}. Thus, $\sum_{k=0}^{x+y}\dbinom{x}{k}\underbrace{\dbinom
{y}{n+k-x}}_{=0}=\sum_{k=0}^{x+y}\dbinom{x}{k}0=0$. Compared with
$\dbinom{x+y}{n}=0$, this yields $\dbinom{x+y}{n}=\sum_{k=0}^{x+y}\dbinom
{x}{k}\dbinom{y}{n+k-x}$. Thus, Proposition
\ref{prop.vandermonde.consequences} \textbf{(g)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $x+y\geq n$. Hence,
$\dbinom{x+y}{n}=\dbinom{x+y}{x+y-n}$ (by (\ref{eq.binom.symm}), applied to
$m=x+y$). Also, $x+y-n\in\mathbb{N}$ (since $x+y\geq n$). Therefore,
Proposition \ref{prop.vandermonde.consequences} \textbf{(a)} (applied to
$x+y-n$ instead of $n$) yields%
\[
\dbinom{x+y}{x+y-n}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{x+y-n-k}.
\]
Since $\dbinom{x+y}{n}=\dbinom{x+y}{x+y-n}$, this rewrites as
\begin{equation}
\dbinom{x+y}{n}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{x+y-n-k}.
\label{pf.prop.vandermonde.consequences.g.4}%
\end{equation}
But every $k\in\left\{  0,1,\ldots,x+y-n\right\}  $ satisfies $\dbinom
{y}{x+y-n-k}=\dbinom{y}{n+k-x}$\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  0,1,\ldots,x+y-n\right\}  $. Then, $0\leq k\leq x+y-n$. Hence,
$x+y-n\geq k$, so that $x+y-n-k\in\mathbb{N}$. Also, $y\geq x+y-n-k$ (since
$y-\left(  x+y-n-k\right)  =\underbrace{n}_{\geq x}+\underbrace{k}_{\geq
0}-x\geq x+0-x=0$). Therefore, (\ref{eq.binom.symm}) (applied to $y$ and
$x+y-n-k$ instead of $m$ and $n$) yields $\dbinom{y}{x+y-n-k}=\dbinom
{y}{y-\left(  x+y-n-k\right)  }=\dbinom{y}{n+k-x}$ (since $y-\left(
x+y-n-k\right)  =n+k-x$), qed.}. Hence,
(\ref{pf.prop.vandermonde.consequences.g.4}) becomes%
\begin{equation}
\dbinom{x+y}{n}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\underbrace{\dbinom{y}%
{x+y-n-k}}_{=\dbinom{y}{n+k-x}}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom
{y}{n+k-x}. \label{pf.prop.vandermonde.consequences.g.6}%
\end{equation}


On the other hand, we have $0\leq n\leq x+y$ and thus $0\leq x+y-n\leq x+y$.
But every $k\in\mathbb{N}$ satisfying $k>x+y-n$ satisfies
\begin{equation}
\dbinom{y}{n+k-x}=0 \label{pf.prop.vandermonde.consequences.g.7}%
\end{equation}
\footnote{\textit{Proof.} Let $k\in\mathbb{N}$ be such that $k>x+y-n$. Then,
$n+\underbrace{k}_{>x+y-n}-x>n+\left(  x+y-n\right)  -x=y$. In other words,
$y<n+k-x$. Hence, (\ref{eq.binom.0}) (applied to $y$ and $n+k-x$ instead of
$m$ and $n$) yields $\dbinom{y}{n+k-x}=0$, qed.}. Hence,
\begin{align*}
&  \sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}\\
&  =\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{n+k-x}+\sum_{k=\left(
x+y-n\right)  +1}^{x+y}\dbinom{x}{k}\underbrace{\dbinom{y}{n+k-x}%
}_{\substack{=0\\\text{(by (\ref{pf.prop.vandermonde.consequences.g.7}) (since
}k>x+y-n\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq x+y-n\leq x+y\right) \\
&  =\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{n+k-x}+\underbrace{\sum
_{k=\left(  x+y-n\right)  +1}^{x+y}\dbinom{x}{k}0}_{=0}=\sum_{k=0}%
^{x+y-n}\dbinom{x}{k}\dbinom{y}{n+k-x}.
\end{align*}
Compared with (\ref{pf.prop.vandermonde.consequences.g.6}), this yields%
\[
\dbinom{x+y}{n}=\sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}.
\]
This proves Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} in
Case 2.

Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} is thus proven in
each of the two Cases 1 and 2. Therefore, Proposition
\ref{prop.vandermonde.consequences} \textbf{(g)} holds in full generality.
\end{proof}

\begin{remark}
The proof of Proposition \ref{prop.vandermonde.consequences} given above
illustrates a useful technique: the use of upper negation (i.e., the equality
(\ref{eq.binom.upper-neg})) to transform one equality into another. In a nutshell,

\begin{itemize}
\item we have proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(d)} by applying Proposition \ref{prop.vandermonde.consequences}
\textbf{(a)} to $-y$ instead of $y$, and then rewriting the result using upper negation;

\item we have proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(e)} by applying Proposition \ref{prop.vandermonde.consequences}
\textbf{(a)} to $y$, $-x-1$ and $n-x$ instead of $x$, $y$ and $n$, and then
rewriting the resulting identity using upper negation;

\item we have proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(f)} by applying Proposition \ref{prop.vandermonde.consequences}
\textbf{(e)} to $-y-1$ and $n-y$ instead of $y$ and $n$, and rewriting the
resulting identity using upper negation.
\end{itemize}

Thus, by substitution and rewriting using upper negation, one single equality
(namely, Proposition \ref{prop.vandermonde.consequences} \textbf{(a)}) has
morphed into three other equalities. Note, in particular, that no negative
numbers appear in Proposition \ref{prop.vandermonde.consequences}
\textbf{(f)}, but yet we proved it by substituting negative values for $x$ and
$y$.
\end{remark}

\subsection{Further results}

\begin{exercise}
\label{exe.ps1.1.2}Let $n$ be a nonnegative integer. Prove that there exist
\textbf{nonnegative} integers $c_{i,j}$ for all $0\leq i\leq n$ and $0\leq
j\leq n$ such that%
\begin{equation}
\dbinom{XY}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{X}{i}\dbinom{Y}{j}
\label{eq.exe.1.2.claim}%
\end{equation}
(an equality between polynomials in two variables $X$ and $Y$).
\end{exercise}

Notice that the integers $c_{i,j}$ in Exercise \ref{exe.ps1.1.2} can depend on
the $n$ (besides depending on $i$ and $j$). We just have not included the $n$
in the notation because it is fixed.

\begin{exercise}
\label{exe.ps1.1.3}Let $a$, $b$ and $c$ be three nonnegative integers. Prove
that the polynomial $\dbinom{aX+b}{c}$ in the variable $X$ (this is a
polynomial in $X$ of degree $\leq c$) can be written as a sum $\sum_{i=0}%
^{c}d_{i}\dbinom{X}{i}$ with \textbf{nonnegative} $d_{i}$.
\end{exercise}

\begin{proposition}
\label{exe.ps1.1.4}Let $a$ and $b$ be two nonnegative integers. There exist
\textbf{nonnegative} integers $e_{0},e_{1},\ldots,e_{a+b}$ such that%
\[
\dbinom{X}{a}\dbinom{X}{b}=\sum_{i=0}^{a+b}e_{i}\dbinom{X}{i}%
\]
(an equality between polynomials in $X$).
\end{proposition}

\begin{proof}
[First proof of Proposition \ref{exe.ps1.1.4}.]For every $N\in\mathbb{N}$, we
let $\left[  N\right]  $ denote the $N$-element set $\left\{  1,2,\ldots
,N\right\}  $.

For every set $S$, we let an $S$\textit{-junction} mean a pair $\left(
A,B\right)  $, where $A$ is an $a$-element subset of $S$ and where $B$ is a
$b$-element subset of $S$ such that $A\cup B=S$. (We do not mention $a$ and
$b$ in our notation, because $a$ and $b$ are fixed.)

For example, if $a=2$ and $b=3$, then $\left(  \left\{  1,4\right\}  ,\left\{
2,3,4\right\}  \right)  $ is a $\left[  4\right]  $-junction, and $\left(
\left\{  2,4\right\}  ,\left\{  1,4,6\right\}  \right)  $ is a $\left\{
1,2,4,6\right\}  $-junction, but $\left(  \left\{  1,3\right\}  ,\left\{
2,3,5\right\}  \right)  $ is not a $\left[  5\right]  $-junction (since
$\left\{  1,3\right\}  \cup\left\{  2,3,5\right\}  \neq\left[  5\right]  $).

For every $i\in\mathbb{N}$, we let $e_{i}$ be the number of all $\left[
i\right]  $-junctions. Then, if $S$ is any $i$-element set, then%
\begin{equation}
e_{i}\text{ is the number of all }S\text{-junctions} \label{sol.ps1.1.4.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps1.1.4.1}):} Let $S$ be any $i$-element
set. We know that $e_{i}$ is the number of all $\left[  i\right]  $-junctions.
We want to prove that $e_{i}$ is the number of all $S$-junctions. Roughly
speaking, this is obvious, because we can \textquotedblleft relabel the
elements of $S$ as $1,2,\ldots,i$\textquotedblright\ (since $S$ is an
$i$-element set), and then the $S$-junctions become precisely the $\left[
i\right]  $-junctions.
\par
Here is a formal way to make this argument: The sets $\left[  i\right]  $ and
$S$ have the same number of elements (indeed, both are $i$-element sets).
Hence, there exists a bijection $\phi:S\rightarrow\left[  i\right]  $. Fix
such a $\phi$. Now, the $S$-junctions are in a 1-to-1 correspondence with the
$\left[  i\right]  $-junctions (namely, to every $S$-junction $\left(
A,B\right)  $ corresponds the $\left[  i\right]  $-junction $\left(
\phi\left(  A\right)  ,\phi\left(  B\right)  \right)  $, and conversely, to
every $\left[  i\right]  $-junction $\left(  A^{\prime},B^{\prime}\right)  $
corresponds the $S$-junction $\left(  \phi^{-1}\left(  A^{\prime}\right)
,\phi^{-1}\left(  B^{\prime}\right)  \right)  $). Hence, the number of all
$S$-junctions equals the number of $\left[  i\right]  $-junctions. Since the
latter number is $e_{i}$, this shows that the former number is also $e_{i}$.
This proves (\ref{sol.ps1.1.4.1}).}.

Now, let us show that%
\begin{equation}
\dbinom{x}{a}\dbinom{x}{b}=\sum_{i=0}^{a+b}e_{i}\dbinom{x}{i}
\label{sol.ps1.1.4.xclaim}%
\end{equation}
for every $x\in\mathbb{N}$.

\textit{Proof of (\ref{sol.ps1.1.4.xclaim}):} Let $x\in\mathbb{N}$. How many
ways are there to choose a pair $\left(  A,B\right)  $ consisting of an
$a$-element subset $A$ of $\left[  x\right]  $ and a $b$-element subset $B$ of
$\left[  x\right]  $ ?

Let us give two different answers to this question. The first answer is the
straightforward one: To choose a pair $\left(  A,B\right)  $ consisting of an
$a$-element subset $A$ of $\left[  x\right]  $ and a $b$-element subset $B$ of
$\left[  x\right]  $, we need to choose an $a$-element subset $A$ of $\left[
x\right]  $ and a $b$-element subset $B$ of $\left[  x\right]  $. There are
$\dbinom{x}{a}\dbinom{x}{b}$ total ways to do this (since there are
$\dbinom{x}{a}$ choices for $A$\ \ \ \ \footnote{This follows from
(\ref{eq.binom.subsets}).}, and $\dbinom{x}{b}$ choices for $B$%
\ \ \ \ \footnote{Again, this follows from (\ref{eq.binom.subsets}).}, and
these choices are independent). In other words, the number of all pairs
$\left(  A,B\right)  $ consisting of an $a$-element subset $A$ of $\left[
x\right]  $ and a $b$-element subset $B$ of $\left[  x\right]  $ equals
$\dbinom{x}{a}\dbinom{x}{b}$.

On the other hand, here is a more imaginative procedure to choose a pair
$\left(  A,B\right)  $ consisting of an $a$-element subset $A$ of $\left[
x\right]  $ and a $b$-element subset $B$ of $\left[  x\right]  $:

\begin{enumerate}
\item We choose how many elements the union $A\cup B$ will have. In other
words, we choose an $i\in\mathbb{N}$ that will satisfy $\left\vert A\cup
B\right\vert =i$. This $i$ must be an integer between $0$ and $a+b$
(inclusive)\footnote{\textit{Proof.} Clearly, $i$ cannot be smaller than $0$.
But $i$ also cannot be larger than $a+b$ (since $i$ will have to satisfy
$i=\left\vert A\cup B\right\vert \leq\underbrace{\left\vert A\right\vert
}_{=a}+\underbrace{\left\vert B\right\vert }_{=b}=a+b$). Thus, $i$ must be an
integer between $0$ and $a+b$ (inclusive).}.

\item We choose a subset $S$ of $\left[  x\right]  $, which will serve as the
union $A\cup B$. This subset $S$ must be an $i$-element subset of $\left[
x\right]  $ (because we will have $\left\vert \underbrace{S}_{=A\cup
B}\right\vert =\left\vert A\cup B\right\vert =i$). Thus, there are $\dbinom
{x}{i}$ ways to choose it (since we need to choose an $i$-element subset of
$\left[  x\right]  $).

\item Now, it remains to choose the pair $\left(  A,B\right)  $ itself. This
pair must be a pair of subsets of $\left[  x\right]  $ satisfying $\left\vert
A\right\vert =a$, $\left\vert B\right\vert =b$, $A\cup B=S$ and $\left\vert
A\cup B\right\vert =i$. We can forget about the $\left\vert A\cup B\right\vert
=i$ condition, since it automatically follows from $A\cup B=S$ (because
$\left\vert S\right\vert =i$). So we need to choose a pair $\left(
A,B\right)  $ of subsets of $\left[  x\right]  $ satisfying $\left\vert
A\right\vert =a$, $\left\vert B\right\vert =b$ and $A\cup B=S$. In other
words, we need to choose a pair $\left(  A,B\right)  $ of subsets of $S$
satisfying $\left\vert A\right\vert =a$, $\left\vert B\right\vert =b$ and
$A\cup B=S$\ \ \ \ \footnote{Here, we have replaced \textquotedblleft subsets
of $\left[  x\right]  $\textquotedblright\ by \textquotedblleft subsets of
$S$\textquotedblright, because the condition $A\cup B=S$ forces $A$ and $B$ to
be subsets of $S$.}. In other words, we need to choose an $S$-junction (since
this is how an $S$-junction was defined). This can be done in exactly $e_{i}$
ways (according to (\ref{sol.ps1.1.4.1})).
\end{enumerate}

Thus, in total, there are $\sum_{i=0}^{a+b}\dbinom{x}{i}e_{i}$ ways to perform
this procedure. Hence, the total number of all pairs $\left(  A,B\right)  $
consisting of an $a$-element subset $A$ of $\left[  x\right]  $ and a
$b$-element subset $B$ of $\left[  x\right]  $ equals $\sum_{i=0}^{a+b}%
\dbinom{x}{i}e_{i}$. But earlier, we have shown that this number is
$\dbinom{x}{a}\dbinom{x}{b}$. Comparing these two results, we conclude that
$\dbinom{x}{a}\dbinom{x}{b}=\sum_{i=0}^{a+b}\dbinom{x}{i}e_{i}=\sum
_{i=0}^{a+b}e_{i}\dbinom{x}{i}$. Thus, (\ref{sol.ps1.1.4.xclaim}) is proven.

Now, we define two polynomials $P$ and $Q$ in the indeterminate $X$ with
rational coefficients by setting%
\[
P=\dbinom{X}{a}\dbinom{X}{b};\ \ \ \ \ \ \ \ \ \ Q=\sum_{i=0}^{a+b}%
e_{i}\dbinom{X}{i}.
\]
The equality (\ref{sol.ps1.1.4.xclaim}) (which we have proven) states that
$P\left(  x\right)  =Q\left(  x\right)  $ for all $x\in\mathbb{N}$. Thus,
Lemma \ref{lem.polyeq} \textbf{(b)} yields that $P=Q$. Recalling how $P$ and
$Q$ are defined, we see that this rewrites as $\dbinom{X}{a}\dbinom{X}{b}%
=\sum_{i=0}^{a+b}e_{i}\dbinom{X}{i}$. This proves Proposition
\ref{exe.ps1.1.4}.
\end{proof}

\begin{proof}
[Second proof of Proposition \ref{exe.ps1.1.4}.]Here is an algebraic proof of
Proposition \ref{exe.ps1.1.4} (based on a suggestion of math.stackexchange
user tcamps in a comment on
\href{http://math.stackexchange.com/questions/1342384}{question \#1342384}).

Theorem \ref{thm.vandermonde} (applied to $n=b$) yields%
\[
\dbinom{X+Y}{b}=\sum_{k=0}^{b}\dbinom{X}{k}\dbinom{Y}{b-k}.
\]
This is a polynomial identity in $X$ and $Y$; we can thus substitute $X-a$ and
$a$ for $X$ and $Y$. As a result of this substitution, we obtain%
\[
\dbinom{\left(  X-a\right)  +a}{b}=\sum_{k=0}^{b}\dbinom{X-a}{k}\dbinom
{a}{b-k}.
\]
Since $\left(  X-a\right)  +a=X$, this rewrites as%
\begin{align*}
\dbinom{X}{b}  &  =\sum_{k=0}^{b}\dbinom{X-a}{k}\dbinom{a}{b-k}=\sum
_{i=a}^{a+b}\dbinom{X-a}{i-a}\underbrace{\dbinom{a}{b-\left(  i-a\right)  }%
}_{=\dbinom{a}{a+b-i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i-a\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{i=a}^{a+b}\dbinom{X-a}{i-a}\dbinom{a}{a+b-i}.
\end{align*}
Multiplying both sides of this identity with $\dbinom{X}{a}$, we obtain%
\begin{align}
\dbinom{X}{a}\dbinom{X}{b}  &  =\dbinom{X}{a}\sum_{i=a}^{a+b}\dbinom{X-a}%
{i-a}\dbinom{a}{a+b-i}=\sum_{i=a}^{a+b}\underbrace{\dbinom{X}{a}\dbinom
{X-a}{i-a}}_{\substack{=\dbinom{X}{i}\dbinom{i}{a}\\\text{(by
(\ref{eq.binom.trinom-rev}))}}}\dbinom{a}{a+b-i}\nonumber\\
&  =\sum_{i=a}^{a+b}\dbinom{X}{i}\dbinom{i}{a}\dbinom{a}{a+b-i}=\sum
_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}.
\label{sol.ps1.1.4.sol2.thm}%
\end{align}


Now, let us define $a+b+1$ nonnegative integers $e_{0},e_{1},\ldots,e_{a+b}$
by%
\begin{equation}
e_{i}=\left\{
\begin{array}
[c]{c}%
\dbinom{i}{a}\dbinom{a}{a+b-i},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq a;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  0,1,\ldots
,a+b\right\}  . \label{sol.ps1.1.4.sol2.ei}%
\end{equation}
Then,%
\begin{align*}
\sum_{i=0}^{a+b}e_{i}\dbinom{X}{i}  &  =\sum_{i=a}^{a+b}\dbinom{i}{a}%
\dbinom{a}{a+b-i}\dbinom{X}{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by our
definition of }e_{0},e_{1},\ldots,e_{a+b}\right) \\
&  =\dbinom{X}{a}\dbinom{X}{b}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps1.1.4.sol2.thm})}\right)  .
\end{align*}
Thus, Proposition \ref{exe.ps1.1.4} is proven again.
\end{proof}

\begin{remark}
Comparing our two proofs of Proposition \ref{exe.ps1.1.4}, it is natural to
suspect that the $e_{0},e_{1},\ldots,e_{a+b}$ defined in the First proof are
identical with the $e_{0},e_{1},\ldots,e_{a+b}$ defined in the Second proof.
This actually follows from general principles (namely, from the word
\textquotedblleft unique\textquotedblright\ in Proposition
\ref{prop.hartshorne} \textbf{(a)}), but there is also a simple combinatorial
reason. Namely, let $i\in\left\{  0,1,\ldots,a+b\right\}  $. We shall show
that the $e_{i}$ defined in the First proof equals the $e_{i}$ defined in the
Second proof.

The $e_{i}$ defined in the First proof is the number of all $\left[  i\right]
$-junctions. An $\left[  i\right]  $-junction is a pair $\left(  A,B\right)
$, where $A$ is an $a$-element subset of $\left[  i\right]  $ and where $B$ is
a $b$-element subset of $\left[  i\right]  $ such that $A\cup B=\left[
i\right]  $. Here is a way to construct an $\left[  i\right]  $-junction:

\begin{itemize}
\item First, we pick the set $A$. There are $\dbinom{i}{a}$ ways to do this,
since $A$ has to be an $a$-element subset of the $i$-element set $\left[
i\right]  $.

\item Then, we pick the set $B$. This has to be a $b$-element subset of the
$i$-element set $\left[  i\right]  $ satisfying $A\cup B=\left[  i\right]  $.
The equality $A\cup B=\left[  i\right]  $ means that $B$ has to contain the
$i-a$ element of $\left[  i\right]  \setminus A$; but the remaining $b-\left(
i-a\right)  =a+b-i$ elements of $B$ can be chosen arbitrarily among the $a$
elements of $A$. Thus, there are $\dbinom{a}{a+b-i}$ ways to choose $B$ (since
we have to choose $a+b-i$ elements of $B$ among the $a$ elements of $A$).
\end{itemize}

Thus, the number of all $\left[  i\right]  $-junctions is $\dbinom{i}%
{a}\dbinom{a}{a+b-i}$. This can be rewritten in the form $\left\{
\begin{array}
[c]{c}%
\dbinom{i}{a}\dbinom{a}{a+b-i},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq a;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ (because if $i<a$, then $\dbinom{i}{a}=0$ and thus $\dbinom{i}%
{a}\dbinom{a}{a+b-i}=0$). Thus, we have shown that the number of all $\left[
i\right]  $-junctions is $\left\{
\begin{array}
[c]{c}%
\dbinom{i}{a}\dbinom{a}{a+b-i},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq a;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $. In other words, the $e_{i}$ defined in the First proof equals the
$e_{i}$ defined in the Second proof.
\end{remark}

Here is an assortment of other identities that involve binomial coefficients:

\begin{proposition}
\label{prop.binom.bin-id}\textbf{(a)} Every $x\in\mathbb{Z}$, $y\in\mathbb{Z}$
and $n\in\mathbb{N}$ satisfy $\left(  x+y\right)  ^{n}=\sum_{k=0}^{n}%
\dbinom{n}{k}x^{k}y^{n-k}$.

\textbf{(b)} Every $n\in\mathbb{N}$ satisfies $\sum_{k=0}^{n}\dbinom{n}%
{k}=2^{n}$.

\textbf{(c)} Every $n\in\mathbb{N}$ satisfies $\sum_{k=0}^{n}\left(
-1\right)  ^{k}\dbinom{n}{k}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n\neq0
\end{cases}
$.

\textbf{(d)} Every $n\in\mathbb{Z}$, $i\in\mathbb{N}$ and $a\in\mathbb{N}$
satisfying $i\geq a$ satisfy $\dbinom{n}{i}\dbinom{i}{a}=\dbinom{n}{a}%
\dbinom{n-a}{i-a}$.

\textbf{(e)} Every $n\in\mathbb{N}$ and $m\in\mathbb{Z}$ satisfy $\sum
_{i=0}^{n}\dbinom{n}{i}\dbinom{m+i}{n}=\sum_{i=0}^{n}\dbinom{n}{i}\dbinom
{m}{i}2^{i}$.

\textbf{(f)} Every $a\in\mathbb{N}$, $b\in\mathbb{N}$ and $x\in\mathbb{Z}$
satisfy $\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{x+i}{a+b}=\dbinom
{x}{a}\dbinom{x}{b}$.

\textbf{(g)} Every $a\in\mathbb{N}$, $b\in\mathbb{N}$ and $x\in\mathbb{Z}$
satisfy $\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{a+b+x-i}{a+b}%
=\dbinom{a+x}{a}\dbinom{b+x}{b}$.
\end{proposition}

(Parts \textbf{(e)} and \textbf{(f)} of Proposition \ref{prop.binom.bin-id}
are from
\href{http://artofproblemsolving.com/community/c6h626709p4577721}{AoPS}. Part
\textbf{(g)} is a restatement of \cite[(6.93)]{Gould-I}.)

\begin{proof}
[Proof of Proposition \ref{prop.binom.bin-id}.]\textbf{(a)} Let $x\in
\mathbb{Z}$, $y\in\mathbb{Z}$ and $n\in\mathbb{N}$. Substituting $X=x$ and
$Y=y$ into (\ref{eq.binom.binomial}), we obtain $\left(  x+y\right)  ^{n}%
=\sum_{k=0}^{n}\dbinom{n}{k}x^{k}y^{n-k}$. This proves Proposition
\ref{prop.binom.bin-id} \textbf{(a)}.

\textbf{(b)} Let $n\in\mathbb{N}$. Applying Proposition
\ref{prop.binom.bin-id} \textbf{(a)} to $x=1$ and $y=1$, we obtain $\left(
1+1\right)  ^{n}=\sum_{k=0}^{n}\dbinom{n}{k}\underbrace{1^{k}}_{=1}%
\underbrace{1^{n-k}}_{=1}=\sum_{k=0}^{n}\dbinom{n}{k}$, thus $\sum_{k=0}%
^{n}\dbinom{n}{k}=\left(  \underbrace{1+1}_{=2}\right)  ^{n}=2^{n}$. This
proves Proposition \ref{prop.binom.bin-id} \textbf{(b)}.

\textbf{(c)} Let $n\in\mathbb{N}$. Applying Proposition
\ref{prop.binom.bin-id} \textbf{(a)} to $x=-1$ and $y=1$, we obtain $\left(
-1+1\right)  ^{n}=\sum_{k=0}^{n}\dbinom{n}{k}\left(  -1\right)  ^{k}%
\underbrace{1^{n-k}}_{=1}=\sum_{k=0}^{n}\dbinom{n}{k}\left(  -1\right)
^{k}=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}$, thus $\sum_{k=0}%
^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}=\left(  \underbrace{-1+1}%
_{=0}\right)  ^{n}=0^{n}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n\neq0
\end{cases}
$. This proves Proposition \ref{prop.binom.bin-id} \textbf{(c)}.

\textbf{(d)} Let $n\in\mathbb{Z}$, $i\in\mathbb{N}$ and $a\in\mathbb{N}$ be
such that $i\geq a$. Substituting $n$ for $X$ in the equality
(\ref{eq.binom.trinom-rev}), we obtain $\dbinom{n}{i}\dbinom{i}{a}=\dbinom
{n}{a}\dbinom{n-a}{i-a}$. This proves Proposition \ref{prop.binom.bin-id}
\textbf{(d)}.

\textbf{(e)} Let $n\in\mathbb{N}$ and $m\in\mathbb{Z}$. Clearly, every
$p\in\mathbb{N}$ satisfies%
\begin{align}
\sum_{i=0}^{p}\dbinom{p}{i}  &  =\sum_{k=0}^{p}\dbinom{p}{k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}i\text{ as }k\right) \nonumber\\
&  =2^{p} \label{pf.prop.binom.bin-id.e.4}%
\end{align}
(by Proposition \ref{prop.binom.bin-id} \textbf{(b)}, applied to $p$ instead
of $n$).

Now, let $i\in\left\{  0,1,\ldots,n\right\}  $. Applying Proposition
\ref{prop.vandermonde.consequences} \textbf{(a)} to $x=i$ and $y=m$, we obtain%
\begin{align}
&  \dbinom{i+m}{n}\nonumber\\
&  =\sum_{k=0}^{n}\dbinom{i}{k}\dbinom{m}{n-k}\nonumber\\
&  =\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}+\sum_{k=i+1}^{n}%
\underbrace{\dbinom{i}{k}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}),
applied to }i\text{ and }k\\\text{instead of }m\text{ and }n\text{ (since
}i<k\text{))}}}\dbinom{m}{n-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq
i\leq n\right) \nonumber\\
&  =\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}+\underbrace{\sum_{k=i+1}%
^{n}0\dbinom{m}{n-k}}_{=0}=\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}.
\label{pf.prop.binom.bin-id.e.1}%
\end{align}


Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.prop.binom.bin-id.e.1}) for every $i\in\left\{  0,1,\ldots,n\right\}
$. Now,%
\begin{align*}
&  \sum_{i=0}^{n}\dbinom{n}{i}\underbrace{\dbinom{m+i}{n}}_{\substack{=\dbinom
{i+m}{n}=\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}\\\text{(by
(\ref{pf.prop.binom.bin-id.e.1}))}}}\\
&  =\sum_{i=0}^{n}\dbinom{n}{i}\left(  \sum_{k=0}^{i}\dbinom{i}{k}\dbinom
{m}{n-k}\right)  =\underbrace{\sum_{i=0}^{n}\sum_{k=0}^{i}}_{=\sum_{k=0}%
^{n}\sum_{i=k}^{n}}\underbrace{\dbinom{n}{i}\dbinom{i}{k}}_{\substack{=\dbinom
{n}{k}\dbinom{n-k}{i-k}\\\text{(by Proposition \ref{prop.binom.bin-id}
\textbf{(d)},}\\\text{applied to }a=k\text{ (since }i\geq k\text{))}}%
}\dbinom{m}{n-k}\\
&  =\sum_{k=0}^{n}\sum_{i=k}^{n}\dbinom{n}{k}\dbinom{n-k}{i-k}\dbinom{m}%
{n-k}=\sum_{k=0}^{n}\dbinom{n}{k}\dbinom{m}{n-k}\sum_{i=k}^{n}\dbinom
{n-k}{i-k}\\
&  =\sum_{k=0}^{n}\underbrace{\dbinom{n}{k}}_{\substack{=\dbinom{n}%
{n-k}\\\text{(by (\ref{eq.binom.symm}), applied to }n\text{ and }%
k\\\text{instead of }m\text{ and }n\text{ (since }n\geq k\text{))}}}\dbinom
{m}{n-k}\sum_{i=0}^{n-k}\dbinom{n-k}{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}i-k\text{ in the second sum}\right) \\
&  =\sum_{k=0}^{n}\dbinom{n}{n-k}\dbinom{m}{n-k}\sum_{i=0}^{n-k}\dbinom
{n-k}{i}=\sum_{k=0}^{n}\dbinom{n}{k}\dbinom{m}{k}\underbrace{\sum_{i=0}%
^{k}\dbinom{k}{i}}_{\substack{=2^{k}\\\text{(by
(\ref{pf.prop.binom.bin-id.e.4}), applied to }p=k\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}n-k\text{ in the first sum}\right) \\
&  =\sum_{k=0}^{n}\dbinom{n}{k}\dbinom{m}{k}2^{k}=\sum_{i=0}^{n}\dbinom{n}%
{i}\dbinom{m}{i}2^{i}%
\end{align*}
(here, we have renamed the summation index $k$ as $i$). This proves
Proposition \ref{prop.binom.bin-id} \textbf{(e)}.

\textbf{(f)} Let $a\in\mathbb{N}$ and $b\in\mathbb{N}$. Let us first work with
polynomials in the indeterminate $X$ (rather than functions in the variable
$x\in\mathbb{Z}$). Recall that%
\begin{equation}
\dbinom{X}{a}\dbinom{X}{b}=\sum_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}%
{a+b-i}\dbinom{X}{i}. \label{pf.prop.binom.bin-id.f.1}%
\end{equation}
(Indeed, this is precisely the identity (\ref{sol.ps1.1.4.sol2.thm}) which was
proven in the Second proof of Proposition \ref{exe.ps1.1.4}.)

Clearly,%
\begin{align}
&  \sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\nonumber\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}+\sum
_{i=b+1}^{a+b}\dbinom{a}{i}\underbrace{\dbinom{b}{i}}_{\substack{=0\\\text{(by
(\ref{eq.binom.0}), applied to }b\text{ and }i\\\text{instead of }m\text{ and
}n\text{ (since }b<i\text{))}}}\dbinom{X+i}{a+b}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq b\leq a+b\right) \nonumber\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}+\underbrace{\sum
_{i=b+1}^{a+b}\dbinom{a}{i}0\dbinom{X+i}{a+b}}_{=0}\nonumber\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}.
\label{pf.prop.binom.bin-id.f.2}%
\end{align}


Theorem \ref{thm.vandermonde} (applied to $n=a+b$) yields%
\begin{equation}
\dbinom{X+Y}{a+b}=\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{Y}{a+b-k}.
\label{pf.prop.binom.bin-id.f.2a}%
\end{equation}


For every $i\in\left\{  0,1,\ldots,b\right\}  $, we have%
\[
\dbinom{X+i}{a+b}=\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{i}{a+b-k}.
\]
(This follows by substituting $Y=i$ in (\ref{pf.prop.binom.bin-id.f.2a}).)
Hence,%
\begin{align*}
&  \sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{b}{i}\underbrace{\dbinom{X+i}{a+b}%
}_{=\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{i}{a+b-k}}\\
&  =\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{b}{i}\sum_{k=0}^{a+b}\dbinom{X}%
{k}\dbinom{i}{a+b-k}\\
&  =\underbrace{\sum_{i=0}^{a+b}\sum_{k=0}^{a+b}}_{=\sum_{k=0}^{a+b}\sum
_{i=0}^{a+b}}\dbinom{a}{i}\underbrace{\dbinom{b}{i}\dbinom{X}{k}\dbinom
{i}{a+b-k}}_{=\dbinom{i}{a+b-k}\dbinom{b}{i}\dbinom{X}{k}}\\
&  =\sum_{k=0}^{a+b}\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom
{b}{i}\dbinom{X}{k}=\sum_{k=0}^{a+b}\dbinom{X}{k}\sum_{i=0}^{a+b}\dbinom{a}%
{i}\dbinom{i}{a+b-k}\dbinom{b}{i}.
\end{align*}
Compared with (\ref{pf.prop.binom.bin-id.f.2}), this yields%
\begin{align}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\nonumber\\
&  =\sum_{k=0}^{a+b}\dbinom{X}{k}\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom
{i}{a+b-k}\dbinom{b}{i}. \label{pf.prop.binom.bin-id.f.3}%
\end{align}
But for every $k\in\left\{  0,1,\ldots,a+b\right\}  $, we have%
\[
\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom{b}{i}=\dbinom{a}%
{a+b-k}\sum_{j=0}^{k}\dbinom{k-b}{k-j}\dbinom{b}{a+b-j}%
\]
\footnote{\textit{Proof.} Let $k\in\left\{  0,1,\ldots,a+b\right\}  $. Then,
$a+b-k\in\left\{  0,1,\ldots,a+b\right\}  $, so that $0\leq a+b-k\leq a+b$.
Now,%
\begin{align*}
&  \sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom{b}{i}\\
&  =\sum_{i=0}^{\left(  a+b-k\right)  -1}\dbinom{a}{i}\underbrace{\dbinom
{i}{a+b-k}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}), applied to }i\text{
and}\\a+b-k\text{ instead of }m\text{ and }n\\\text{(since }i<a+b-k\text{))}%
}}\dbinom{b}{i}+\sum_{i=a+b-k}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom
{b}{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq a+b-k\leq a+b\right) \\
&  =\underbrace{\sum_{i=0}^{\left(  a+b-k\right)  -1}\dbinom{a}{i}0\dbinom
{b}{i}}_{=0}+\sum_{i=a+b-k}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom{b}{i}\\
&  =\sum_{i=a+b-k}^{a+b}\underbrace{\dbinom{a}{i}\dbinom{i}{a+b-k}%
}_{\substack{=\dbinom{a}{a+b-k}\dbinom{a-\left(  a+b-k\right)  }{i-\left(
a+b-k\right)  }\\\text{(by Proposition \ref{prop.binom.bin-id} \textbf{(d)},
applied to}\\a\text{ and }a+b-k\text{ instead of }n\text{ and }a\text{ (since
}i\geq a+b-k\text{))}}}\dbinom{b}{i}\\
&  =\sum_{i=a+b-k}^{a+b}\dbinom{a}{a+b-k}\dbinom{a-\left(  a+b-k\right)
}{i-\left(  a+b-k\right)  }\dbinom{b}{i}\\
&  =\sum_{j=0}^{k}\dbinom{a}{a+b-k}\underbrace{\dbinom{a-\left(  a+b-k\right)
}{\left(  a+b-j\right)  -\left(  a+b-k\right)  }}_{\substack{=\dbinom
{k-b}{k-j}\\\text{(since }a-\left(  a+b-k\right)  =k-b\text{ and}\\\left(
a+b-j\right)  -\left(  a+b-k\right)  =k-j\text{)}}}\dbinom{b}{a+b-j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }a+b-j\text{
for }i\text{ in the sum}\right) \\
&  =\sum_{j=0}^{k}\dbinom{a}{a+b-k}\dbinom{k-b}{k-j}\dbinom{b}{a+b-j}%
=\dbinom{a}{a+b-k}\sum_{j=0}^{k}\dbinom{k-b}{k-j}\dbinom{b}{a+b-j},
\end{align*}
qed.}. Hence, (\ref{pf.prop.binom.bin-id.f.3}) becomes%
\begin{align}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\nonumber\\
&  =\sum_{k=0}^{a+b}\dbinom{X}{k}\underbrace{\sum_{i=0}^{a+b}\dbinom{a}%
{i}\dbinom{i}{a+b-k}\dbinom{b}{i}}_{=\dbinom{a}{a+b-k}\sum_{j=0}^{k}%
\dbinom{k-b}{k-j}\dbinom{b}{a+b-j}}\nonumber\\
&  =\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{a}{a+b-k}\sum_{j=0}^{k}\dbinom
{k-b}{k-j}\dbinom{b}{a+b-j}\nonumber\\
&  =\sum_{i=0}^{a+b}\dbinom{X}{i}\dbinom{a}{a+b-i}\sum_{j=0}^{i}\dbinom
{i-b}{i-j}\dbinom{b}{a+b-j} \label{pf.prop.binom.bin-id.f.7}%
\end{align}
(here, we renamed the summation index $k$ as $i$ in the first sum).
Furthermore, every $i\in\left\{  0,1,\ldots,a+b\right\}  $ satisfies
\[
\sum_{j=0}^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}=\dbinom{i}{a}%
\]
\footnote{\textit{Proof.} Let $i\in\left\{  0,1,\ldots,a+b\right\}  $. Thus,
$0\leq i\leq a+b$. We have
\begin{align}
\sum_{j=0}^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}  &  =\sum_{k=0}%
^{i}\underbrace{\dbinom{i-b}{i-\left(  i-k\right)  }}_{\substack{=\dbinom
{i-b}{k}\\\text{(since }i-\left(  i-k\right)  =k\text{)}}}\underbrace{\dbinom
{b}{a+b-\left(  i-k\right)  }}_{\substack{=\dbinom{b}{\left(  a+b\right)
+k-i}\\\text{(since }a+b-\left(  i-k\right)  =\left(  a+b\right)
+k-i\text{))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i-k\text{ for
}j\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=0}^{i}\dbinom{i-b}{k}\dbinom{b}{\left(  a+b\right)  +k-i}.
\label{pf.prop.binom.bin-id.f.8.pf.1}%
\end{align}
On the other hand, we have $b\in\mathbb{N}$, $\left(  i-b\right)  +b=i\geq0$
and $a\geq i-b$ (since $a+b\geq i$). Therefore, we can apply Proposition
\ref{prop.vandermonde.consequences} \textbf{(g)} to $i-b$, $b$ and $a$ instead
of $x$, $y$ and $n$. As a result, we obtain%
\[
\dbinom{\left(  i-b\right)  +b}{a}=\sum_{k=0}^{\left(  i-b\right)  +b}%
\dbinom{i-b}{k}\underbrace{\dbinom{b}{a+k-\left(  i-b\right)  }}%
_{\substack{=\dbinom{b}{\left(  a+b\right)  +k-i}\\\text{(since }a+k-\left(
i-b\right)  =\left(  a+b\right)  +k-i\text{)}}}=\sum_{k=0}^{\left(
i-b\right)  +b}\dbinom{i-b}{k}\dbinom{b}{\left(  a+b\right)  +k-i}.
\]
Since $\left(  i-b\right)  +b=i$, this rewrites as
\[
\dbinom{i}{a}=\sum_{k=0}^{i}\dbinom{i-b}{k}\dbinom{b}{\left(  a+b\right)
+k-i}.
\]
Compared with (\ref{pf.prop.binom.bin-id.f.8.pf.1}), this yields%
\[
\sum_{j=0}^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}=\dbinom{i}{a},
\]
qed.}.

Hence, (\ref{pf.prop.binom.bin-id.f.7}) becomes%
\begin{align*}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\\
&  =\sum_{i=0}^{a+b}\dbinom{X}{i}\dbinom{a}{a+b-i}\underbrace{\sum_{j=0}%
^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}}_{=\dbinom{i}{a}}\\
&  =\sum_{i=0}^{a+b}\underbrace{\dbinom{X}{i}\dbinom{a}{a+b-i}\dbinom{i}{a}%
}_{=\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}}=\sum_{i=0}^{a+b}\dbinom{i}%
{a}\dbinom{a}{a+b-i}\dbinom{X}{i}\\
&  =\sum_{i=0}^{a-1}\underbrace{\dbinom{i}{a}}_{\substack{=0\\\text{(by
(\ref{eq.binom.0}), applied to }i\text{ and }a\\\text{instead of }m\text{ and
}n\text{ (since }i<a\text{))}}}\dbinom{a}{a+b-i}\dbinom{X}{i}+\sum_{i=a}%
^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq a\leq a+b\right) \\
&  =\underbrace{\sum_{i=0}^{a-1}0\dbinom{a}{a+b-i}\dbinom{X}{i}}_{=0}%
+\sum_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}\\
&  =\sum_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}=\dbinom{X}%
{a}\dbinom{X}{b}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.binom.bin-id.f.1})}\right)  .
\end{align*}
Substituting $X=x$ in this equality, we obtain%
\[
\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{x+i}{a+b}=\dbinom{x}{a}%
\dbinom{x}{b}.
\]
This proves Proposition \ref{prop.binom.bin-id} \textbf{(f)}.

\textbf{(g)} From (\ref{eq.binom.upper-neg}) (applied to $m=-x-1$ and $n=a$),
we obtain $\dbinom{-x-1}{a}=\left(  -1\right)  ^{a}\dbinom{a-\left(
-x-1\right)  -1}{a}=\left(  -1\right)  ^{a}\dbinom{a+x}{a}$ (since $a-\left(
-x-1\right)  -1=a+x$). The same argument (applied to $b$ instead of $a$) shows
that $\dbinom{-x-1}{b}=\left(  -1\right)  ^{b}\dbinom{b+x}{b}$.

Now, Proposition \ref{prop.binom.bin-id} \textbf{(f)} (applied to $-x-1$
instead of $x$) shows that%
\begin{align}
\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{\left(  -x-1\right)  +i}{a+b}
&  =\underbrace{\dbinom{-x-1}{a}}_{=\left(  -1\right)  ^{a}\dbinom{a+x}{a}%
}\underbrace{\dbinom{-x-1}{b}}_{=\left(  -1\right)  ^{b}\dbinom{b+x}{b}%
}\nonumber\\
&  =\left(  -1\right)  ^{a}\dbinom{a+x}{a}\left(  -1\right)  ^{b}\dbinom
{b+x}{b}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{a}\left(  -1\right)  ^{b}}_{=\left(
-1\right)  ^{a+b}}\dbinom{a+x}{a}\dbinom{b+x}{b}\nonumber\\
&  =\left(  -1\right)  ^{a+b}\dbinom{a+x}{a}\dbinom{b+x}{b}.
\label{pf.prop.binom.bin-id.g.3}%
\end{align}
But every $i\in\left\{  0,1,\ldots,b\right\}  $ satisfies%
\begin{align*}
\dbinom{\left(  -x-1\right)  +i}{a+b}  &  =\left(  -1\right)  ^{a+b}%
\dbinom{a+b-\left(  \left(  -x-1\right)  +i\right)  -1}{a+b}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.binom.upper-neg}), applied to
}m=\left(  -x-1\right)  +i\text{ and }n=a+b\right) \\
&  =\left(  -1\right)  ^{a+b}\dbinom{a+b+x-i}{a+b}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a+b-\left(  \left(  -x-1\right)
+i\right)  -1=a+b+x-i\right)  .
\end{align*}
Hence,%
\begin{align*}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\underbrace{\dbinom{\left(
-x-1\right)  +i}{a+b}}_{=\left(  -1\right)  ^{a+b}\dbinom{a+b+x-i}{a+b}}\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\left(  -1\right)  ^{a+b}%
\dbinom{a+b+x-i}{a+b}=\left(  -1\right)  ^{a+b}\sum_{i=0}^{b}\dbinom{a}%
{i}\dbinom{b}{i}\dbinom{a+b+x-i}{a+b}.
\end{align*}
Comparing this with (\ref{pf.prop.binom.bin-id.g.3}), we obtain%
\[
\left(  -1\right)  ^{a+b}\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}%
\dbinom{a+b+x-i}{a+b}=\left(  -1\right)  ^{a+b}\dbinom{a+x}{a}\dbinom{b+x}%
{b}.
\]
We can cancel $\left(  -1\right)  ^{a+b}$ from this equality (since $\left(
-1\right)  ^{a+b}\neq0$), and thus obtain $\sum_{i=0}^{b}\dbinom{a}{i}%
\dbinom{b}{i}\dbinom{a+b+x-i}{a+b}=\dbinom{a+x}{a}\dbinom{b+x}{b}$. This
proves Proposition \ref{prop.binom.bin-id} \textbf{(g)}.
\end{proof}

Many more examples of equalities with binomial coefficients, as well as
advanced tactics for proving such equalities, can be found in \cite[Chapter
5]{GKP}.

\subsection{Additional exercises}

This section contains some further exercises. These will not be used in the
rest of the notes, and they can be skipped at will\footnote{The same, of
course, can be said for many of the standard exercises.}. I am not planning to
provide solutions for them.

\begin{addexercise}
\label{exeadd.prop.vandermonde.consequences.f}Find a different proof of
Proposition \ref{prop.vandermonde.consequences} \textbf{(f)} that uses a
double-counting argument (i.e., counting some combinatorial objects in two
different ways, and then concluding that the results are equal).

[\textbf{Hint:} How many $\left(  x+y+1\right)  $-element subsets does the set
$\left\{  1,2,\ldots,n+1\right\}  $ have? Now, for a given $k\in\left\{
0,1,\ldots,n\right\}  $, how many $\left(  x+y+1\right)  $-element subsets
whose $\left(  x+1\right)  $-th smallest element is $k$ does the set $\left\{
1,2,\ldots,n+1\right\}  $ have?]
\end{addexercise}

\begin{addexercise}
\label{exeadd.multichoose}Let $n\in\mathbb{N}$ and $k\in\mathbb{N}$ be fixed.
Show that the number of all $k$-tuples $\left(  a_{1},a_{2},\ldots
,a_{k}\right)  \in\mathbb{N}^{k}$ satisfying $a_{1}+a_{2}+\cdots+a_{k}=n$
equals $\dbinom{n+k-1}{k}$.
\end{addexercise}

\begin{remark}
Additional exercise \ref{exeadd.multichoose} can be restated in terms of
multisets. Namely, let $n\in\mathbb{N}$ and $k\in\mathbb{N}$ be fixed. Also,
fix a $k$-element set $K$. Then, the number of $n$-element multisets whose
elements all belong to $K$ is $\dbinom{n+k-1}{k}$. Indeed, we can WLOG assume
that $K=\left\{  1,2,\ldots,k\right\}  $ (otherwise, just relabel the elements
of $K$); then, the multisets whose elements all belong to $K$ are in bijection
with the $k$-tuples $\left(  a_{1},a_{2},\ldots,a_{k}\right)  \in
\mathbb{N}^{k}$. The bijection sends a multiset $M$ to the $k$-tuple $\left(
m_{1}\left(  M\right)  ,m_{2}\left(  M\right)  ,\ldots,m_{k}\left(  M\right)
\right)  $, where each $m_{i}\left(  M\right)  $ is the multiplicity of the
element $i$ in $M$. The size of a multiset $M$ corresponds to the sum
$a_{1}+a_{2}+\cdots+a_{k}$ of the entries of the resulting $k$-tuple; thus, we
get a bijection between

\begin{itemize}
\item the $n$-element multisets whose elements all belong to $K$
\end{itemize}

and

\begin{itemize}
\item the $k$-tuples $\left(  a_{1},a_{2},\ldots,a_{k}\right)  \in
\mathbb{N}^{k}$ satisfying $a_{1}+a_{2}+\cdots+a_{k}=n$.
\end{itemize}

As a consequence, Additional exercise \ref{exeadd.multichoose} shows that the
number of the former multisets is $\dbinom{n+k-1}{k}$.

Similarly, we can reinterpret the classical combinatorial interpretation of
$\dbinom{k}{n}$ (as the number of $n$-element subsets of $\left\{
1,2,\ldots,k\right\}  $) as follows: The number of all $k$-tuples $\left(
a_{1},a_{2},\ldots,a_{k}\right)  \in\left\{  0,1\right\}  ^{k}$ satisfying
$a_{1}+a_{2}+\cdots+a_{k}=n$ equals $\dbinom{k}{n}$.
\end{remark}

\begin{addexercise}
\label{exeadd.multichoose-app}Let $n\in\mathbb{N}$ and $k\in\mathbb{N}$ be
such that $n\geq k$. Prove that%
\[
\sum_{u=0}^{k}\dbinom{n+u-1}{u}\dbinom{n}{k-2u}=\dbinom{n+k-1}{k}.
\]
Here, $\dbinom{a}{b}$ is defined to be $0$ when $b<0$.
\end{addexercise}

\begin{noncompile}
See
\href{http://web.mit.edu/~darij/www/QEDMO4P13.pdf}{http://web.mit.edu/\symbol{126}%
darij/www/QEDMO4P13.pdf} for a solution to the preceding exercise.
\end{noncompile}

\begin{addexercise}
\label{exeadd.bininv}Let $N\in\mathbb{N}$. The \textit{binomial transform} of
a finite sequence $\left(  f_{0},f_{1},\ldots,f_{N}\right)  \in\mathbb{Z}%
^{N+1}$ is defined to be the sequence $\left(  g_{0},g_{1},\ldots
,g_{N}\right)  $ defined by%
\[
g_{n}=\sum_{i=0}^{n}\left(  -1\right)  ^{i}\dbinom{n}{i}f_{i}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\left\{  0,1,\ldots,N\right\}  .
\]


\textbf{(a)} Let $\left(  f_{0},f_{1},\ldots,f_{N}\right)  \in\mathbb{Z}%
^{N+1}$ be a finite sequence of integers. Let $\left(  g_{0},g_{1}%
,\ldots,g_{N}\right)  $ be the binomial transform of $\left(  f_{0}%
,f_{1},\ldots,f_{N}\right)  $. Show that $\left(  f_{0},f_{1},\ldots
,f_{N}\right)  $ is, in turn, the binomial transform of $\left(  g_{0}%
,g_{1},\ldots,g_{N}\right)  $.

\textbf{(b)} Find the binomial transform of the sequence $\left(
1,1,\ldots,1\right)  $.

\textbf{(c)} For any given $a\in\mathbb{N}$, find the binomial transform of
the sequence $\left(  \dbinom{0}{a},\dbinom{1}{a},\ldots,\dbinom{N}{a}\right)
$.

\textbf{(d)} For any given $q\in\mathbb{Z}$, find the binomial transform of
the sequence $\left(  q^{0},q^{1},\ldots,q^{N}\right)  $.

\textbf{(e)} Find the binomial transform of the sequence $\left(
1,0,1,0,1,0,\ldots\right)  $ (this ends with $1$ if $N$ is even, and with $0$
if $N$ is odd).

\textbf{(f)} Let $B:\mathbb{Z}^{N+1}\rightarrow\mathbb{Z}^{N+1}$ be the map
which sends every sequence $\left(  f_{0},f_{1},\ldots,f_{N}\right)
\in\mathbb{Z}^{N+1}$ to its binomial transform $\left(  g_{0},g_{1}%
,\ldots,g_{N}\right)  \in\mathbb{Z}^{N+1}$. Thus, part \textbf{(a)} of this
exercise states that $B^{2}=\operatorname*{id}$.

On the other hand, let $W:\mathbb{Z}^{N+1}\rightarrow\mathbb{Z}^{N+1}$ be the
map which sends every sequence $\left(  f_{0},f_{1},\ldots,f_{N}\right)
\in\mathbb{Z}^{N+1}$ to $\left(  \left(  -1\right)  ^{N}f_{N},\left(
-1\right)  ^{N}f_{N-1},\ldots,\left(  -1\right)  ^{N}f_{0}\right)
\in\mathbb{Z}^{N+1}$. It is rather clear that $W^{2}=\operatorname*{id}$.

Show that, furthermore, $B\circ W\circ B=W\circ B\circ W$ and $\left(  B\circ
W\right)  ^{3}=\operatorname*{id}$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.AoPS333199}For any $n\in\mathbb{N}$ and $m\in\mathbb{N}$, define
a polynomial $Z_{m,n}\in\mathbb{Z}\left[  X\right]  $ by%
\[
Z_{m,n}=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\left(
X^{n-k}-1\right)  ^{m}.
\]
Show that $Z_{m,n}=Z_{n,m}$ for any $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
\end{addexercise}

\begin{noncompile}
Two solutions to the preceding exercise are sketched in
\href{http://www.artofproblemsolving.com/community/c6h333199p1782800}{http://www.artofproblemsolving.com/community/c6h333199p1782800}
. More generally, $Y_{m,n}=Y_{n,m}$, where $Y_{m,n}$ is the polynomial%
\[
\sum_{k=0}^{n}Y^{k}\dbinom{n}{k}\left(  X^{n-k}+Y\right)  ^{m}.
\]

\end{noncompile}

\begin{addexercise}
\label{exeadd.AoPS262752}Let $n\in\mathbb{N}$. Prove%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\]
(an identity between polynomials in $\mathbb{Q}\left[  X\right]  $).

[\textbf{Hint:} It is enough to prove this when $X$ is replaced by a
nonnegative integer $r$ (why?). Now that you have gotten rid of polynomials,
introduce new polynomials. Namely, compute the coefficient of $X^{n}$ in
$\left(  1+X\right)  ^{r}\left(  1-X\right)  ^{r}$. Compare with the
coefficient of $X^{n}$ in $\left(  1-X^{2}\right)  ^{r}$.]
\end{addexercise}

\begin{noncompile}
Source: \cite[(5.55)]{GKP}; see also
\href{http://www.artofproblemsolving.com/community/c6h262752}{http://www.artofproblemsolving.com/community/c6h262752}
.
\end{noncompile}

\section{\label{chp.recur}Recurrent sequences}

\subsection{Basics}

Two of the most famous integer sequences defined recursively are the Fibonacci
sequence and the Lucas sequence:

\begin{itemize}
\item The \href{https://en.wikipedia.org/wiki/Fibonacci_number}{Fibonacci
sequence} is the sequence $\left(  f_{0},f_{1},f_{2},\ldots\right)  $ of
integers which is defined recursively by $f_{0}=0$, $f_{1}=1$, and
$f_{n}=f_{n-1}+f_{n-2}$ for all $n\geq2$. Its first terms are%
\begin{align*}
f_{0}  &  =0,\ \ \ \ \ \ \ \ \ \ f_{1}=1,\ \ \ \ \ \ \ \ \ \ f_{2}%
=1,\ \ \ \ \ \ \ \ \ \ f_{3}=2,\ \ \ \ \ \ \ \ \ \ f_{4}%
=3,\ \ \ \ \ \ \ \ \ \ f_{5}=5,\\
f_{6}  &  =8,\ \ \ \ \ \ \ \ \ \ f_{7}=13,\ \ \ \ \ \ \ \ \ \ f_{8}%
=21,\ \ \ \ \ \ \ \ \ \ f_{9}=34,\ \ \ \ \ \ \ \ \ \ f_{10}=55,\\
f_{11}  &  =89,\ \ \ \ \ \ \ \ \ \ f_{12}=144,\ \ \ \ \ \ \ \ \ \ f_{13}=233.
\end{align*}
(Some authors prefer to start the sequence at $f_{1}$ rather than $f_{0}$; of
course, the recursive definition then needs to be modified to require
$f_{2}=1$ instead of $f_{0}=0$.)

\item The \href{https://en.wikipedia.org/wiki/Lucas_number}{Lucas sequence} is
the sequence $\left(  \ell_{0},\ell_{1},\ell_{2},\ldots\right)  $ of integers
which is defined recursively by $\ell_{0}=2$, $\ell_{1}=1$, and $\ell_{n}%
=\ell_{n-1}+\ell_{n-2}$ for all $n\geq2$. Its first terms are%
\begin{align*}
\ell_{0}  &  =2,\ \ \ \ \ \ \ \ \ \ \ell_{1}=1,\ \ \ \ \ \ \ \ \ \ \ell
_{2}=3,\ \ \ \ \ \ \ \ \ \ \ell_{3}=4,\ \ \ \ \ \ \ \ \ \ \ell_{4}%
=7,\ \ \ \ \ \ \ \ \ \ \ell_{5}=11,\\
\ell_{6}  &  =18,\ \ \ \ \ \ \ \ \ \ \ell_{7}=29,\ \ \ \ \ \ \ \ \ \ \ell
_{8}=47,\ \ \ \ \ \ \ \ \ \ \ell_{9}=76,\ \ \ \ \ \ \ \ \ \ \ell_{10}=123,\\
\ell_{11}  &  =199,\ \ \ \ \ \ \ \ \ \ \ell_{12}=322,\ \ \ \ \ \ \ \ \ \ \ell
_{13}=521.
\end{align*}

\end{itemize}

A lot of papers have been written about these two sequences, the relations
between them, and the identities that hold for their terms.\footnote{See
\href{https://oeis.org/A000045}{https://oeis.org/A000045} and
\href{https://oeis.org/A000032}{https://oeis.org/A000032} for an overview of
their properties.} One of their most striking properties is that they can be
computed explicitly, albeit using irrational numbers. In fact, the
\textit{Binet formula} says that the $n$-th Fibonacci number $f_{n}$ can be
computed by%
\begin{equation}
f_{n}=\dfrac{1}{\sqrt{5}}\varphi^{n}-\dfrac{1}{\sqrt{5}}\psi^{n},
\label{eq.binet.f}%
\end{equation}
where $\varphi=\dfrac{1+\sqrt{5}}{2}$ and $\psi=\dfrac{1-\sqrt{5}}{2}$ are the
two solutions of the quadratic equation $X^{2}-X-1=0$. (The number $\varphi$
is known as the \textit{golden ratio}; the number $\psi$ can be obtained from
it by $\psi=1-\varphi=-1/\varphi$.) A similar formula, using the very same
numbers $\varphi$ and $\psi$, exists for the Lucas numbers:%
\begin{equation}
\ell_{n}=\varphi^{n}+\psi^{n}. \label{eq.binet.l}%
\end{equation}


\begin{remark}
How easy is it to compute $f_{n}$ and $\ell_{n}$ using the formulas
(\ref{eq.binet.f}) and (\ref{eq.binet.l})?

This is a nontrivial question. Indeed, if you are careless, you may find them
rather useless. For instance, if you try to compute $f_{n}$ using the formula
(\ref{eq.binet.f}) and using approximate values for the irrational numbers
$\varphi$ and $\psi$, then you might end up with a wrong value for $f_{n}$,
because the error in the approximate value for $\varphi$ propagates when you
take $\varphi$ to the $n$-th power. (And for high enough $n$, the error will
become larger than $1$, so you will not be able to get the correct value by
rounding.) The greater $n$ is, the more precise you need a value for $\varphi$
to approximate $f_{n}$ this way. Thus, approximating $\varphi$ is not a good
way to compute $f_{n}$. (Actually, the opposite is true: You can use
(\ref{eq.binet.f}) to approximate $\varphi$ by computing Fibonacci numbers.
Namely, it is easy to show that $\varphi=\lim\limits_{n\rightarrow\infty
}\dfrac{f_{n}}{f_{n-1}}$.)

A better approach to using (\ref{eq.binet.f}) is to work with the exact values
of $\varphi$ and $\psi$. To do so, you need to know how to add, subtract,
multiply and divide real numbers of the form $a+b\sqrt{5}$ with $a,b\in
\mathbb{Q}$ without ever using approximations. (Clearly, $\varphi$, $\psi$ and
$\sqrt{5}$ all have this form.) There are rules for this, which are simple to
check:%
\begin{align*}
\left(  a+b\sqrt{5}\right)  +\left(  c+d\sqrt{5}\right)   &  =\left(
a+c\right)  +\left(  b+d\right)  \sqrt{5};\\
\left(  a+b\sqrt{5}\right)  -\left(  c+d\sqrt{5}\right)   &  =\left(
a-c\right)  +\left(  b-d\right)  \sqrt{5};\\
\left(  a+b\sqrt{5}\right)  \cdot\left(  c+d\sqrt{5}\right)   &  =\left(
ac+5bd\right)  +\left(  bc+ad\right)  \sqrt{5};\\
\dfrac{a+b\sqrt{5}}{c+d\sqrt{5}}  &  =\dfrac{\left(  ac-5bd\right)  +\left(
bc-ad\right)  \sqrt{5}}{c^{2}-5d^{2}}\ \ \ \ \ \ \ \ \ \ \text{for }\left(
c,d\right)  \neq\left(  0,0\right)  .
\end{align*}
(The last rule is an instance of \textquotedblleft rationalizing the
denominator\textquotedblright.) These rules give you a way to exactly compute
things like $\varphi^{n}$, $\dfrac{1}{\sqrt{5}}\varphi^{n}$, $\psi^{n}$ and
$\dfrac{1}{\sqrt{5}}\psi^{n}$, and thus also $f_{n}$ and $\ell_{n}$. If you
use
\href{https://en.wikipedia.org/wiki/Exponentiation_by_squaring}{exponentiation
by squaring} to compute $n$-th powers, this actually becomes a fast algorithm
(a lot faster than just computing $f_{n}$ and $\ell_{n}$ using the
recurrence). So, yes, (\ref{eq.binet.f}) and (\ref{eq.binet.l}) are useful.
\end{remark}

We shall now study a generalization of both the Fibonacci and the Lucas
sequences, and generalize (\ref{eq.binet.f}) and (\ref{eq.binet.l}) to a
broader class of sequences.

\begin{definition}
\label{def.abrec} If $a$ and $b$ are two complex numbers, then a sequence
$\left(  x_{0},x_{1},x_{2},\ldots\right)  $ of complex numbers will be called
$\left(  a,b\right)  $\textit{-recurrent} if every $n\geq2$ satisfies%
\[
x_{n}=ax_{n-1}+bx_{n-2}.
\]

\end{definition}

So, the Fibonacci sequence and the Lucas sequences are $\left(  1,1\right)
$-recurrent. An $\left(  a,b\right)  $-recurrent sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $ is fully determined by the four values $a$, $b$,
$x_{0}$ and $x_{1}$, and can be constructed for any choice of these four
values. Here are some further examples of $\left(  a,b\right)  $-recurrent sequences:

\begin{itemize}
\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
2,-1\right)  $-recurrent if and only if every $n\geq2$ satisfies
$x_{n}=2x_{n-1}-x_{n-2}$. In other words, a sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $ is $\left(  2,-1\right)  $-recurrent if and only
if every $n\geq2$ satisfies $x_{n}-x_{n-1}=x_{n-1}-x_{n-2}$. In other words, a
sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(  2,-1\right)
$-recurrent if and only if $x_{1}-x_{0}=x_{2}-x_{1}=x_{3}-x_{2}=\cdots$. In
other words, the $\left(  2,-1\right)  $-recurrent sequences are precisely the
arithmetic progressions.

\item Geometric progressions are also $\left(  a,b\right)  $-recurrent for
appropriate $a$ and $b$. Namely, any geometric progression $\left(
u,uq,uq^{2},uq^{3},\ldots\right)  $ is $\left(  q,0\right)  $-recurrent, since
every $n\geq2$ satisfies $uq^{n}=q\cdot uq^{n-1}+0\cdot uq^{n-2}$. However,
not every $\left(  q,0\right)  $-recurrent sequence $\left(  x_{0},x_{1}%
,x_{2},\ldots\right)  $ is a geometric progression (since the condition
$x_{n}=qx_{n-1}+0x_{n-2}$ for all $n\geq2$ says nothing about $x_{0}$, and
thus $x_{0}$ can be arbitrary).

\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
0,1\right)  $-recurrent if and only if every $n\geq2$ satisfies $x_{n}%
=x_{n-2}$. In other words, a sequence $\left(  x_{0},x_{1},x_{2}%
,\ldots\right)  $ is $\left(  0,1\right)  $-recurrent if and only if it has
the form $\left(  u,v,u,v,u,v,\ldots\right)  $ for two complex numbers $u$ and
$v$.

\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
1,0\right)  $-recurrent if and only if every $n\geq2$ satisfies $x_{n}%
=x_{n-1}$. In other words, a sequence $\left(  x_{0},x_{1},x_{2}%
,\ldots\right)  $ is $\left(  1,0\right)  $-recurrent if and only if it has
the form $\left(  u,v,v,v,v,\ldots\right)  $ for two complex numbers $u$ and
$v$. Notice that $u$ is not required to be equal to $v$, because we never
claimed that $x_{n}=x_{n-1}$ holds for $n=1$.

\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
1,-1\right)  $-recurrent if and only if every $n\geq2$ satisfies
$x_{n}=x_{n-1}-x_{n-2}$. Curiously, it turns out that every such sequence is
$6$-periodic (i.e., it satisfies $x_{n+6}=x_{n}$ for every $n\in\mathbb{N}$),
because every $n\in\mathbb{N}$ satisfies%
\begin{align*}
x_{n+6}  &  =\underbrace{x_{n+5}}_{=x_{n+4}-x_{n+3}}-x_{n+4}=\left(
x_{n+4}-x_{n+3}\right)  -x_{n+4}=-\underbrace{x_{n+3}}_{=x_{n+2}-x_{n+1}}\\
&  =-\left(  \underbrace{x_{n+2}}_{=x_{n+1}-x_{n}}-x_{n+1}\right)  =-\left(
x_{n+1}-x_{n}-x_{n+1}\right)  =x_{n}.
\end{align*}
More precisely, a sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is
$\left(  1,0\right)  $-recurrent if and only if it has the form $\left(
u,v,v-u,-u,-v,u-v,\ldots\right)  $ (where the \textquotedblleft$\ldots
$\textquotedblright\ stands for \textquotedblleft repeat the preceding $6$
values over and over\textquotedblright\ here) for two complex numbers $u$ and
$v$.

\item The above three examples notwithstanding, most $\left(  a,b\right)
$-recurrent sequences of course are not periodic. However, here is another
example which provides a great supply of non-periodic $\left(  a,b\right)
$-recurrent sequences and, at the same time, explains why we get so many
periodic ones: If $\alpha$ is any angle, then the sequences%
\begin{align*}
&  \left(  \sin\left(  0\alpha\right)  ,\sin\left(  1\alpha\right)
,\sin\left(  2\alpha\right)  ,\ldots\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
&  \left(  \cos\left(  0\alpha\right)  ,\cos\left(  1\alpha\right)
,\cos\left(  2\alpha\right)  ,\ldots\right)
\end{align*}
are $\left(  2\cos\alpha,-1\right)  $-recurrent. More generally, if $\alpha$
and $\beta$ are two angles, then the sequence%
\[
\left(  \sin\left(  \beta+0\alpha\right)  ,\sin\left(  \beta+1\alpha\right)
,\sin\left(  \beta+2\alpha\right)  ,\ldots\right)
\]
is $\left(  2\cos\alpha,-1\right)  $-recurrent\footnote{\textit{Proof.} Let
$\alpha$ and $\beta$ be two angles. We need to show that the sequence $\left(
\sin\left(  \beta+0\alpha\right)  ,\sin\left(  \beta+1\alpha\right)
,\sin\left(  \beta+2\alpha\right)  ,\ldots\right)  $ is $\left(  2\cos
\alpha,-1\right)  $-recurrent. In other words, we need to prove that%
\[
\sin\left(  \beta+n\alpha\right)  =2\cos\alpha\sin\left(  \beta+\left(
n-1\right)  \alpha\right)  +\left(  -1\right)  \sin\left(  \beta+\left(
n-2\right)  \alpha\right)
\]
for every $n\geq2$. So fix $n\geq2$.
\par
One of the well-known trigonometric identities states that $\sin x+\sin
y=2\sin\dfrac{x+y}{2}\cos\dfrac{x-y}{2}$ for any two angles $x$ and $y$.
Applying this to $x=\beta+n\alpha$ and $y=\beta+\left(  n-2\right)  \alpha$,
we obtain%
\begin{align*}
\sin\left(  \beta+n\alpha\right)  +\sin\left(  \beta+\left(  n-2\right)
\alpha\right)   &  =2\sin\underbrace{\dfrac{\left(  \beta+n\alpha\right)
+\left(  \beta+\left(  n-2\right)  \alpha\right)  }{2}}_{=\beta+\left(
n-1\right)  \alpha}\cos\underbrace{\dfrac{\left(  \beta+n\alpha\right)
-\left(  \beta+\left(  n-2\right)  \alpha\right)  }{2}}_{=\alpha}\\
&  =2\sin\left(  \beta+\left(  n-1\right)  \alpha\right)  \cos\alpha
=2\cos\alpha\sin\left(  \beta+\left(  n-1\right)  \alpha\right)  .
\end{align*}
Hence,
\begin{align*}
\sin\left(  \beta+n\alpha\right)   &  =2\cos\alpha\sin\left(  \beta+\left(
n-1\right)  \alpha\right)  -\sin\left(  \beta+\left(  n-2\right)
\alpha\right) \\
&  =2\cos\alpha\sin\left(  \beta+\left(  n-1\right)  \alpha\right)  +\left(
-1\right)  \sin\left(  \beta+\left(  n-2\right)  \alpha\right)  ,
\end{align*}
qed.}. When $\alpha\in2\pi\mathbb{Q}$ (that is, some integer multiple of
$\alpha$ equals some integer multiple of $2\pi$), this sequence is periodic.
\end{itemize}

\subsection{Explicit formulas (\`{a} la Binet)}

Now, we can get an explicit formula (similar to (\ref{eq.binet.f}) and
(\ref{eq.binet.l})) for every term of an $\left(  a,b\right)  $-recurrent
sequence (in terms of $a$, $b$, $x_{0}$ and $x_{1}$) in the case when
$a^{2}+4b\neq0$. Here is how this works:

\begin{remark}
\label{rmk.binet}Let $a$ and $b$ be complex numbers such that $a^{2}+4b\neq0$.
Let $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ be an $\left(  a,b\right)
$-recurrent sequence. We want to construct an explicit formula for each
$x_{n}$ in terms of $x_{0}$, $x_{1}$, $a$ and $b$.

To do so, we let $q_{+}$ and $q_{-}$ be the two solutions of the quadratic
equation $X^{2}-aX-b=0$, namely%
\[
q_{+}=\dfrac{a+\sqrt{a^{2}+4b}}{2}\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ q_{-}=\dfrac{a-\sqrt{a^{2}+4b}}{2}.
\]
We notice that $q_{+}\neq q_{-}$ (since $a^{2}+4b\neq0$). It is easy to see
that the sequences $\left(  1,q_{+},q_{+}^{2},q_{+}^{3},\ldots\right)  $ and
$\left(  1,q_{-},q_{-}^{2},q_{-}^{3},\ldots\right)  $ are $\left(  a,b\right)
$-recurrent. As a consequence, for any two complex numbers $\lambda_{+}$ and
$\lambda_{-}$, the sequence%
\[
\left(  \lambda_{+}+\lambda_{-},\lambda_{+}q_{+}+\lambda_{-}q_{-},\lambda
_{+}q_{+}^{2}+\lambda_{-}q_{-}^{2},\ldots\right)
\]
(the $n$-th term of this sequence, with $n$ starting at $0$, is $\lambda
_{+}q_{+}^{n}+\lambda_{-}q_{-}^{n}$) must also be $\left(  a,b\right)
$-recurrent (check this!). We denote this sequence by $L_{\lambda_{+}%
,\lambda_{-}}$.

We now need to find two complex numbers $\lambda_{+}$ and $\lambda_{-}$ such
that this sequence $L_{\lambda_{+},\lambda_{-}}$ is our sequence $\left(
x_{0},x_{1},x_{2},\ldots\right)  $. In order to do so, we only need to ensure
that $\lambda_{+}+\lambda_{-}=x_{0}$ and $\lambda_{+}q_{+}+\lambda_{-}%
q_{-}=x_{1}$ (because once this holds, it will follow that the sequences
$L_{\lambda_{+},\lambda_{-}}$ and $\left(  x_{0},x_{1},x_{2},\ldots\right)  $
have the same first two terms; and this will yield that these two sequences
are identical, because two $\left(  a,b\right)  $-recurrent sequences with the
same first two terms must be identical). That is, we need to solve the system
of linear equations%
\[
\left\{
\begin{array}
[c]{c}%
\lambda_{+}+\lambda_{-}=x_{0};\\
\lambda_{+}q_{+}+\lambda_{-}q_{-}=x_{1}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{in the unknowns }\lambda_{+}\text{ and
}\lambda_{-}.
\]
Thanks to $q_{+}\neq q_{-}$, this system has a unique solution:%
\[
\lambda_{+}=\dfrac{x_{1}-q_{-}x_{0}}{q_{+}-q_{-}};\ \ \ \ \ \ \ \ \ \ \lambda
_{-}=\dfrac{q_{+}x_{0}-x_{1}}{q_{+}-q_{-}}.
\]
Thus, if we set $\left(  \lambda_{+},\lambda_{-}\right)  $ to be this
solution, then $\left(  x_{0},x_{1},x_{2},\ldots\right)  =L_{\lambda
_{+},\lambda_{-}}$, so that%
\begin{equation}
x_{n}=\lambda_{+}q_{+}^{n}+\lambda_{-}q_{-}^{n}
\label{rmk.recursive.binet-general}%
\end{equation}
for every nonnegative integer $n$. This is an explicit formula, at least if
the square roots do not disturb you. When $x_{0}=x_{1}=a=b=1$, you get the
famous Binet formula (\ref{eq.binet.f}) for the Fibonacci sequence.
\end{remark}

In the next exercise you will see what happens if the $a^{2}+4b\neq0$
condition does not hold.

\begin{exercise}
\label{exe.ps2.2.1}Let $a$ and $b$ be complex numbers such that $a^{2}+4b=0$.
Consider an $\left(  a,b\right)  $-recurrent sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $. Find an explicit formula for each $x_{n}$ in
terms of $x_{0}$, $x_{1}$, $a$ and $b$.

[\textbf{Note:} The polynomial $X^{2}-aX-b$ has a double root here. Unlike the
case of two distinct roots studied above, you won't see any radicals here. The
explicit formula really deserves the name \textquotedblleft
explicit\textquotedblright.]
\end{exercise}

Remark \ref{rmk.binet} and Exercise \ref{exe.ps2.2.1}, combined, solve the
problem of finding an explicit formula for any term of an $\left(  a,b\right)
$-recurrent sequence when $a$ and $b$ are complex numbers, at least if you
don't mind having square roots in your formula. Similar tactics can be used to
find explicit forms for the more general case of sequences satisfying
\textquotedblleft homogeneous linear recurrences with constant
coefficients\textquotedblright\footnote{These are sequences $\left(
x_{0},x_{1},x_{2},\ldots\right)  $ which satisfy%
\[
\left(  x_{n}=c_{1}x_{n-1}+c_{2}x_{n-2}+\cdots+c_{k}x_{n-k}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq k\right)
\]
for a fixed $k\in\mathbb{N}$ and a fixed $k$-tuple $\left(  c_{1},c_{2}%
,\ldots,c_{k}\right)  $ of complex numbers. When $k=2$, these are the $\left(
c_{1},c_{2}\right)  $-recurrent sequences.}, although instead of square roots
you will now need roots of higher-degree polynomials. (See \cite[\S 22.3.2
(\textquotedblleft Solving Homogeneous Linear Recurrences\textquotedblright%
)]{LeLeMe16} for an outline of this; see also \cite[Topic \textquotedblleft
Linear Recurrences\textquotedblright]{Hefferon} for a linear-algebraic introduction.)

\subsection{Further results}

Here are some more exercises from the theory of recurrent sequences. I am not
going particularly deep here, but we may encounter generalizations later.

First, an example: If we \textquotedblleft split\textquotedblright\ the
Fibonacci sequence $\left(  f_{0},f_{1},f_{2},\ldots\right)  =\left(
1,1,2,3,5,8,\ldots\right)  $ into two subsequences $\left(  f_{0},f_{2}%
,f_{4},\ldots\right)  =\left(  1,2,5,13,\ldots\right)  $ and $\left(
f_{1},f_{3},f_{5},\ldots\right)  =\left(  1,3,8,21,\ldots\right)  $ (each of
which contains every other Fibonacci number), then it turns out that each of
these two subsequences is $\left(  3,-1\right)  $-recurrent\footnote{In other
words, we have $f_{2n}=3f_{2\left(  n-1\right)  }+\left(  -1\right)
f_{2\left(  n-2\right)  }$ and $f_{2n+1}=3f_{2\left(  n-1\right)  +1}+\left(
-1\right)  f_{2\left(  n-2\right)  +1}$ for every $n\geq2$.}. This is rather
easy to prove, but one can always ask for generalizations: What happens if we
start with an arbitrary $\left(  a,b\right)  $-recurrent sequence, instead of
the Fibonacci numbers? What happens if we split it into three, four or more
subsequences? The answer is rather nice:

\begin{exercise}
\label{exe.ps2.2.2}Let $a$ and $b$ be complex numbers. Let $\left(
x_{0},x_{1},x_{2},\ldots\right)  $ be an $\left(  a,b\right)  $-recurrent sequence.

\textbf{(a)} Prove that the sequences $\left(  x_{0},x_{2},x_{4}%
,\ldots\right)  $ and $\left(  x_{1},x_{3},x_{5},\ldots\right)  $ are $\left(
c,d\right)  $-recurrent for some complex numbers $c$ and $d$. Find these $c$
and $d$.

\textbf{(b)} Prove that the sequences $\left(  x_{0},x_{3},x_{6}%
,\ldots\right)  $, $\left(  x_{1},x_{4},x_{7},\ldots\right)  $ and $\left(
x_{2},x_{5},x_{8},\ldots\right)  $ are $\left(  c,d\right)  $-recurrent for
some (other) complex numbers $c$ and $d$.

\textbf{(c)} For every nonnegative integers $N$ and $K$, prove that the
sequence $\left(  x_{K},x_{N+K},x_{2N+K},x_{3N+K},\ldots\right)  $ is $\left(
c,d\right)  $-recurrent for some complex numbers $c$ and $d$ which depend only
on $N$, $a$ and $b$ (but not on $K$ or $x_{0}$ or $x_{1}$).
\end{exercise}

The next exercise gives a combinatorial interpretation of the Fibonacci numbers:

\begin{exercise}
\label{exe.ps2.2.3}Recall that the Fibonacci numbers $f_{0},f_{1},f_{2}%
,\ldots$ are defined recursively by $f_{0}=0$, $f_{1}=1$ and $f_{n}%
=f_{n-1}+f_{n-2}$ for all $n\geq2$. For every positive integer $n$, show that
$f_{n}$ is the number of subsets $I$ of $\left\{  1,2,\ldots,n-2\right\}  $
such that no two elements of $I$ are consecutive (i.e., there exists no
$i\in\mathbb{Z}$ such that both $i$ and $i+1$ belong to $I$). For instance,
for $n=5$, these subsets are $\varnothing$, $\left\{  1\right\}  $, $\left\{
2\right\}  $, $\left\{  3\right\}  $ and $\left\{  1,3\right\}  $.
\end{exercise}

Notice that $\left\{  1,2,\ldots,-1\right\}  $ is to be understood as the
empty set (since there are no integers $x$ satisfying $1\leq x\leq-1$). (So
Exercise \ref{exe.ps2.2.3}, applied to $n=1$, says that $f_{1}$ is the number
of subsets $I$ of the empty set such that no two elements of $I$ are
consecutive. This is correct, because the empty set has only one subset, which
of course is empty and thus has no consecutive elements; and the Fibonacci
number $f_{1}$ is precisely $1$.)

\begin{remark}
\label{rmk.fib.dominos}Exercise \ref{exe.ps2.2.3} is equivalent to another
known combinatorial interpretation of the Fibonacci numbers.

Namely, let $n$ be a positive integer. Consider a rectangular table of
dimensions $2\times\left(  n-1\right)  $ (that is, with $2$ rows and $n-1$
columns). How many ways are there to subdivide this table into dominos? (A
\textit{domino} means a set of two adjacent boxes.)

For $n=5$, there are $5$ ways:%
\begin{align*}
&
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
%{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
%& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
%\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
\end{tabular}%
%EndExpansion
\ ,\ \ \ \ \ \ \ \ \ \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
%& \phantom{a} \\ \cline{3-4}
%\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
& \phantom{a} \\ \cline{3-4}
\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ ,\ \ \ \ \ \ \ \ \ \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
%& \phantom{a} \\ \cline{1-2}
%\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
& \phantom{a} \\ \cline{1-2}
\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ ,\\
&
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
%& \phantom{a} \\ \cline{2-3}
%\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
& \phantom{a} \\ \cline{2-3}
\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ ,\ \ \ \ \ \ \ \ \ \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
%\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%EndExpansion
\ .
\end{align*}
In the general case, there are $f_{n-1}$ ways. Why?

As promised, this result is equivalent to Exercise \ref{exe.ps2.2.3}. Let us
see why. Let $P$ be a way to subdivide the table into dominos. We say that a
\textit{horizontal domino} is a domino which consists of two adjacent boxes in
the same row; similarly, we define a vertical domino. It is easy to see that
(in the subdivision $P$) each column of the table is covered either by a
single vertical domino, or by two horizontal dominos (in which case either
both of them \textquotedblleft begin\textquotedblright\ in this column, or
both of them \textquotedblleft end\textquotedblright\ in this column). Let
$J\left(  P\right)  $ be the set of all $i\in\left\{  1,2,\ldots,n-1\right\}
$ such that the $i$-th column of the table is covered by two horizontal
dominos, both of which \textquotedblleft begin\textquotedblright\ in this
column. For instance,%
\begin{align*}
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
%{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
%& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
%\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  1,3\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
%& \phantom{a} \\ \cline{3-4}
%\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
& \phantom{a} \\ \cline{3-4}
\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  3\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
%& \phantom{a} \\ \cline{1-2}
%\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
& \phantom{a} \\ \cline{1-2}
\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  1\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
%& \phantom{a} \\ \cline{2-3}
%\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
& \phantom{a} \\ \cline{2-3}
\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  2\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
%\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\varnothing.
\end{align*}
It is easy to see that the set $J\left(  P\right)  $ is a subset of $\left\{
1,2,\ldots,n-2\right\}  $ containing no two consecutive integers. Moreover,
this set $J\left(  P\right)  $ uniquely determines $P$, and for every subset
$I$ of $\left\{  1,2,\ldots,n-2\right\}  $ containing no two consecutive
integers, there exists some way $P$ to subdivide the table into dominos such
that $J\left(  P\right)  =I$.

Hence, the number of all ways to subdivide the table into dominos equals the
number of all subsets $I$ of $\left\{  1,2,\ldots,n-2\right\}  $ containing no
two consecutive integers. Exercise \ref{exe.ps2.2.3} says that this latter
number is $f_{n-1}$; therefore, so is the former number.

(I have made this remark because I found it instructive. If you merely want a
proof that the number of all ways to subdivide the table into dominos equals
$f_{n-1}$, then I guess it is easier to just prove it by induction without
taking the detour through Exercise \ref{exe.ps2.2.3}.)
\end{remark}

Either Exercise \ref{exe.ps2.2.3} or Remark \ref{rmk.fib.dominos} can be used
to prove properties of Fibonacci numbers in a combinatorial way; see
\cite{BenQui-fib} for some examples of such proofs.

Here is another formula for certain recursive sequences, coming out of a
recent paper on cluster algebras\footnote{Specifically, Exercise
\ref{exe.ps2.2.S} is part of \cite[Definition 1]{LS2}, but I have reindexed
the sequence and fixed the missing upper bound in the sum.}:

\begin{exercise}
\label{exe.ps2.2.S}Let $r\in\mathbb{Z}$. Define a sequence $\left(
c_{0},c_{1},c_{2},\ldots\right)  $ of integers recursively by $c_{0}=0$,
$c_{1}=1$ and $c_{n}=rc_{n-1}-c_{n-2}$ for all $n\geq2$. Show that%
\begin{equation}
c_{n}=\sum_{i=0}^{n-1}\left(  -1\right)  ^{i}\dbinom{n-1-i}{i}r^{n-1-2i}
\label{eq.exe.2.S}%
\end{equation}
for every $n\in\mathbb{N}$. Here, we use the following convention: Any
expression of the form $a\cdot b$, where $a$ is $0$, has to be interpreted as
$0$, even if $b$ is undefined.\footnotemark
\end{exercise}

\footnotetext{The purpose of this convention is to make sure that the right
hand side of (\ref{eq.exe.2.S}) is well-defined, even though the expression
$r^{n-1-2i}$ that appears in it might be undefined (it will be undefined when
$r=0$ and $n-1-2i<0$).
\par
Of course, the downside of this convention is that we might not have $a\cdot
b=b\cdot a$ (because $a\cdot b$ might be well-defined while $b\cdot a$ is not,
or vice versa).}

\subsection{Additional exercises}

This section contains some further exercises. As the earlier \textquotedblleft
additional exercises\textquotedblright, these will not be relied on in the
rest of this text, and solutions will not be provided.

\begin{addexercise}
\label{exeadd.rec.qn-rn}Let $q$ and $r$ be two complex numbers. Prove that the
sequence $\left(  q^{0}-r^{0},q^{1}-r^{1},q^{2}-r^{2},\ldots\right)  $ is
$\left(  a,b\right)  $-recurrent for two appropriately chosen $a$ and $b$.
Find these $a$ and $b$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.fib.floor}Let $\varphi$ be the golden ratio (i.e., the real
number $\dfrac{1+\sqrt{5}}{2}$). Let $\left(  f_{0},f_{1},f_{2},\ldots\right)
$ be the Fibonacci sequence.

\textbf{(a)} Show that $f_{n+1}-\varphi f_{n}=\dfrac{1}{\sqrt{5}}\psi^{n}$ for
every $n\in\mathbb{N}$, where $\psi=\dfrac{1-\sqrt{5}}{2}$. (Notice that
$\psi=\dfrac{1-\sqrt{5}}{2}\approx-0.618$ lies between $-1$ and $0$, and thus
the powers $\psi^{n}$ converge to $0$ as $n\rightarrow\infty$. So
$f_{n+1}-\varphi f_{n}\rightarrow0$ as $n\rightarrow\infty$, and consequently
$\dfrac{f_{n+1}}{f_{n}}\rightarrow\varphi$ as well.)

\textbf{(b)} Show that%
\[
f_{n}=\operatorname*{round}\left(  \dfrac{1}{\sqrt{5}}\varphi^{n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{N}.
\]
Here, if $x$ is a real number, then $\operatorname*{round}x$ denotes the
integer closest to $x$ (where, in case of a tie, we take the higher of the two candidates\footnotemark).
\end{addexercise}

\footnotetext{This does not really matter in our situation, because $\dfrac
{1}{\sqrt{5}}\varphi^{n}$ will never be a half-integer.}

\begin{addexercise}
\label{exeadd.fib.zeckendorf}Let $\left(  f_{0},f_{1},f_{2},\ldots\right)  $
be the Fibonacci sequence. A set $I$ of integers is said to be
\textit{lacunar} if no two elements of $I$ are consecutive (i.e., there exists
no $i\in I$ such that $i+1\in I$). Show that, for every $n\in\mathbb{N}$,
there exists a unique lacunar subset $S$ of $\left\{  2,3,4,\ldots\right\}  $
such that $n=\sum_{s\in S}f_{s}$.

(For example, if $n=17$, then $S=\left\{  2,4,7\right\}  $, because
$17=1+3+13=f_{2}+f_{4}+f_{7}$.)
\end{addexercise}

\begin{remark}
The representation of $n$ in the form $n=\sum_{s\in S}f_{s}$ in Exercise
\ref{exeadd.fib.zeckendorf} is known as the
\textit{\href{https://en.wikipedia.org/wiki/Zeckendorf's_theorem}{\textit{Zeckendorf
representation}}} of $n$. It has a number of interesting properties and trivia
related to it; for example, there is
\href{https://www.encyclopediaofmath.org/index.php/Zeckendorf_representation}{a
rule of thumb for converting miles into kilometers that uses it}. It can also
be used to define a curious \textquotedblleft Fibonacci
multiplication\textquotedblright\ operation on nonnegative integers
\cite{Knuth-fib}.
\end{remark}

\begin{addexercise}
\label{exeadd.fib.zeckendids}Let $\left(  f_{0},f_{1},f_{2},\ldots\right)  $
be the Fibonacci sequence.

\textbf{(a)} Prove the identities%
\begin{align*}
1f_{n}  &  =f_{n}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq0;\\
2f_{n}  &  =f_{n-2}+f_{n+1}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2;\\
3f_{n}  &  =f_{n-2}+f_{n+2}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2;\\
4f_{n}  &  =f_{n-2}+f_{n}+f_{n+2}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2.
\end{align*}


\textbf{(b)} Notice that the right hand sides of these identities have a
specific form: they are sums of $f_{n+t}$ for $t$ ranging over a lacunar
subset of $\mathbb{Z}$. (See Additional exercise \ref{exeadd.fib.zeckendorf}
for the definition of \textquotedblleft lacunar\textquotedblright.) Try to
find similar identities for $5f_{n}$ and $6f_{n}$.

\textbf{(c)} Prove that such identities exist in general. More precisely,
prove the following: Let $T$ be a finite set, and $a_{t}$ be an integer for
every $t\in T$. Then, there exists a unique lacunar subset $S$ of $\mathbb{Z}$
such that
\begin{align*}
\sum\limits_{t\in T}f_{n+a_{t}}  &  =\sum\limits_{s\in S}f_{n+s}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ which}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{ satisfies }n\geq\max\left(
\left\{  -a_{t}\mid t\in T\right\}  \cup\left\{  -s\mid s\in S\right\}
\right)  .
\end{align*}
(The condition $n\geq\max\left(  \left\{  -a_{t}\mid t\in T\right\}
\cup\left\{  -s\mid s\in S\right\}  \right)  $ merely ensures that all the
$f_{n+a_{t}}$ and $f_{n+s}$ are well-defined.)
\end{addexercise}

\begin{remark}
Additional exercise \ref{exeadd.fib.zeckendids} \textbf{(c)} is \cite[Theorem
1]{Gri-zeck}. I'd be delighted to see other proofs!

Similarly I am highly interested in analogues of Additional exercises
\ref{exeadd.fib.zeckendorf} and \ref{exeadd.fib.zeckendids} for other $\left(
a,b\right)  $-recurrent sequences (e.g., Lucas numbers).
\end{remark}

\begin{addexercise}
\label{exeadd.rec.addition}\textbf{(a)} Let $\left(  f_{0},f_{1},f_{2}%
,\ldots\right)  $ be the Fibonacci sequence. Prove that $f_{m+n}=f_{m}%
f_{n+1}+f_{m-1}f_{n}$ for any positive integer $m$ and any $n\in\mathbb{N}$.

\textbf{(b)} Generalize to $\left(  a,b\right)  $-recurrent sequences in general.
\end{addexercise}

\begin{addexercise}
\label{exeadd.rec.fibonomial}\textbf{(a)} Let $\left(  f_{0},f_{1}%
,f_{2},\ldots\right)  $ be the Fibonacci sequence. For every $n\in\mathbb{N}$
and $k\in\mathbb{N}$ satisfying $0\leq k\leq n$, define a rational number
$\dbinom{n}{k}_{F}$ by%
\[
\dbinom{n}{k}_{F}=\dfrac{f_{n}f_{n-1}\cdots f_{n-k+1}}{f_{k}f_{k-1}\cdots
f_{1}}.
\]
This is called the $\left(  n,k\right)  $-th \textit{Fibonomial coefficient}
(in analogy to the binomial coefficient $\dbinom{n}{k}=\dfrac{n\left(
n-1\right)  \cdots\left(  n-k+1\right)  }{k\left(  k-1\right)  \cdots1}$).

Show that $\dbinom{n}{k}_{F}$ is an integer.

\textbf{(b)} Try to extend as many identities for binomial coefficients as you
can to Fibonomial coefficients.

\textbf{(c)} Generalize to $\left(  a,b\right)  $-recurrent sequences in general.
\end{addexercise}

\section{\label{chp.perm}Permutations}

This chapter is devoted to permutations. We first recall how they are defined.

\subsection{Permutations and the symmetric group}

\begin{definition}
\label{def.composition}First, let us stipulate, once and for all, how we
define the composition of two maps: If $X$, $Y$ and $Z$ are three sets, and if
$\alpha:X\rightarrow Y$ and $\beta:Y\rightarrow Z$ are two maps, then
$\beta\circ\alpha$ denotes the map from $X$ to $Z$ which sends every $x\in X$
to $\beta\left(  \alpha\left(  x\right)  \right)  $. This map $\beta
\circ\alpha$ is called the \textit{composition} of $\beta$ and $\alpha$ (and
is sometimes abbreviated as $\beta\alpha$). This is the classical notation for
composition of maps, and the reason why I am so explicitly reminding you of it
is that some people (e.g., Herstein in \cite{Herstein}) use a different
convention that conflicts with it: They write maps \textquotedblleft on the
right\textquotedblright\ (i.e., they denote the image of an element $x\in X$
under the map $\alpha:X\rightarrow Y$ by $x^{\alpha}$ or $x\alpha$ instead of
$\alpha\left(  x\right)  $), and they define composition \textquotedblleft the
other way round\textquotedblright\ (i.e., they write $\alpha\circ\beta$ for
what we call $\beta\circ\alpha$). They have reasons for what they are doing,
but we prefer the classical notation because most of the literature agrees
with it.
\end{definition}

\begin{definition}
Let us also recall what it means for two maps to be \textit{inverse}.

Let $X$ and $Y$ be two sets. Two maps $f : X \to Y$ and $g : Y \to X$ are said
to be \textit{mutually inverse} if they satisfy $g \circ f = \operatorname{id}%
_{X}$ and $f \circ g = \operatorname{id}_{Y}$. (In other words, two maps $f :
X \to Y$ and $g : Y \to X$ are mutually inverse if and only if every $x \in X$
satisfies $g\left(  f\left(  x\right)  \right)  = x$ and every $y \in Y$
satisfies $f\left(  g\left(  y\right)  \right)  = y$.)

Let $f : X \to Y$ is a map. If there exists a map $g : Y \to X$ such that $f$
and $g$ are mutually inverse, then this map $g$ is unique (this is easy to
check) and is called the \textit{inverse} of $f$ and denoted by $f^{-1}$. In
this case, the map $f$ is said to be \textit{invertible}. It is easy to see
that if $g$ is the inverse of $f$, then $f$ is the inverse of $g$.

It is well-known that a map $f : X \to Y$ is invertible if and only if $f$ is
bijective (i.e., both injective and surjective). The words ``invertible'' and
``bijective'' are thus synonyms (at least when used for a map between two sets
-- in other situations, they can be rather different). Nevertheless, both of
them are commonly used, often by the same authors (since they convey slightly
different mental images).

A bijective map is also called a \textit{bijection} or a \textit{1-to-1
correspondence} (or a \textit{one-to-one correspondence}). When there is a
bijection from $X$ to $Y$, one says that the elements of $X$ are \textit{in
bijection with} (or \textit{in one-to-one correspondence with}) the elements
of $Y$. It is well-known that two sets $X$ and $Y$ have the same cardinality
if and only if there exists a bijection from $X$ to $Y$.
\end{definition}

\begin{definition}
\label{def.permutation}A \textit{permutation} of a set $X$ means a bijection
from $X$ to $X$. The permutations of a given set $X$ can be composed (i.e., if
$\alpha$ and $\beta$ are two permutations of $X$, then so is $\alpha\circ
\beta$) and have inverses (which, again, are permutations of $X$). More precisely:

\begin{itemize}
\item If $\alpha$ and $\beta$ are two permutations of a given set $X$, then
the composition $\alpha\circ\beta$ is again a permutation of $X$.

\item Any three permutations $\alpha$, $\beta$ and $\gamma$ of $X$ satisfy
$\left(  \alpha\circ\beta\right)  \circ\gamma=\alpha\circ\left(  \beta
\circ\gamma\right)  $. (This holds, more generally, for arbitrary maps which
can be composed.)

\item The identity map $\operatorname*{id}:X\rightarrow X$ (this is the map
which sends every element $x\in X$ to itself) is a permutation of $X$; it is
also called the \textit{identity permutation}. Every permutation $\alpha$ of
$X$ satisfies $\operatorname*{id}\circ\alpha=\alpha$ and $\alpha
\circ\operatorname*{id}=\alpha$. (Again, this can be generalized to arbitrary maps.)

\item For every permutation $\alpha$ of $X$, the inverse map $\alpha^{-1}$ is
well-defined and is again a permutation of $X$. We have $\alpha\circ
\alpha^{-1}=\operatorname*{id}$ and $\alpha^{-1}\circ\alpha=\operatorname*{id}%
$.
\end{itemize}

In the lingo of algebraists, these four properties show that the set of all
permutations of $X$ is a
\href{https://en.wikipedia.org/?title=Group (mathematics)}{group} whose binary
operation is composition, and whose neutral element is the identity
permutation $\operatorname*{id}:X\rightarrow X$. This group is known as the
\textit{symmetric group of the set }$X$. (We will define the notion of a group
later, in Definition \ref{def.group}; thus you might not understand the
preceding two sentences at this point. If you do not care about groups, you
should just remember that the symmetric group of $X$ is the set of all
permutations of $X$.)
\end{definition}

\begin{remark}
Some authors define a permutation of a finite set $X$ to mean a list of all
elements of $X$, each occurring exactly once. This is \textbf{not} the meaning
that the word \textquotedblleft permutation\textquotedblright\ has in these
notes! It is a different notion which, for historical reasons, has been called
\textquotedblleft permutation\textquotedblright\ as well. On
\href{https://en.wikipedia.org/wiki/Permutation\%23Definition_and_one-line_notation}{the
Wikipedia page for \textquotedblleft permutation\textquotedblright}, the two
notions are called \textquotedblleft active\textquotedblright\ and
\textquotedblleft passive\textquotedblright, respectively: An
\textquotedblleft active\textquotedblright\ permutation of $X$ means a
bijection from $X$ to $X$ (that is, a permutation of $X$ in our meaning of
this word), whereas a \textquotedblleft passive\textquotedblright\ permutation
of $X$ means a list of all elements of $X$, each occurring exactly once. For
example, if $X=\left\{  \text{\textquotedblleft cat\textquotedblright,
\textquotedblleft dog\textquotedblright, \textquotedblleft
archaeopteryx\textquotedblright}\right\}  $, then the map%
\begin{align*}
\text{\textquotedblleft cat\textquotedblright\ }  &  \mapsto\text{
\textquotedblleft archaeopteryx\textquotedblright},\\
\text{\textquotedblleft archaeopteryx\textquotedblright\ }  &  \mapsto\text{
\textquotedblleft dog\textquotedblright},\\
\text{\textquotedblleft dog\textquotedblright\ }  &  \mapsto\text{
\textquotedblleft cat\textquotedblright}%
\end{align*}
is an \textquotedblleft active\textquotedblright\ permutation of $X$, whereas
the list $\left(  \text{\textquotedblleft dog\textquotedblright,
\textquotedblleft cat\textquotedblright, \textquotedblleft
archaeopteryx\textquotedblright}\right)  $ is a \textquotedblleft
passive\textquotedblright\ permutation of $X$.

When $X$ is the set $\left\{  1,2,\ldots,n\right\}  $ for some $n\in
\mathbb{N}$, then it is possible to equate each \textquotedblleft
active\textquotedblright\ permutation of $X$ with a \textquotedblleft
passive\textquotedblright\ permutation of $X$ (namely, its one-line notation,
defined below). More generally, this can be done when $X$ comes with a fixed
total order. In general, if $X$ is a finite set, then the number of
\textquotedblleft active\textquotedblright\ permutations of $X$ equals the
number of \textquotedblleft passive\textquotedblright\ permutations of $X$
(and both numbers equal $\left\vert X\right\vert !$), but until you fix some
ordering of the elements of $X$, there is no \textquotedblleft
natural\textquotedblright\ way to match the \textquotedblleft
passive\textquotedblright\ permutations with the \textquotedblleft
active\textquotedblright\ ones. (And when $X$ is infinite, the notion of a
\textquotedblleft passive\textquotedblright\ permutation is not even well-defined.)

To reiterate: For us, the word \textquotedblleft permutation\textquotedblright%
\ shall always mean an \textquotedblleft active\textquotedblright\ permutation!
\end{remark}

Recall that $\mathbb{N}=\left\{  0,1,2,\ldots\right\}  $. Fix $n\in\mathbb{N}$.

Let $S_{n}$ be the symmetric group of the set $\left\{  1,2,\ldots,n\right\}
$. This is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $. It contains the identity permutation $\operatorname*{id}\in
S_{n}$ which sends every $i\in\left\{  1,2,\ldots,n\right\}  $ to $i$. A
well-known fact states that the size of this group is $\left\vert
S_{n}\right\vert =n!$ (that is, there are exactly $n!$ permutations of
$\left\{  1,2,\ldots,n\right\}  $).

We will often write a permutation $\sigma\in S_{n}$ as the list $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $ of its values. This is known as the \textit{one-line
notation} for permutations (because it is a single-rowed list, as opposed to
e.g. the two-line notation which is a two-rowed
table).\footnote{Combinatorialists often omit the parentheses and the commas
(i.e., they just write $\sigma\left(  1\right)  \sigma\left(  2\right)
\cdots\sigma\left(  n\right)  $, hoping that noone will mistake this for a
product), since there is unfortunately another notation for permutations (the
\textit{cycle notation}) which also writes them as lists (actually, lists of
lists) but where the lists have a different meaning.} For instance, the
permutation in $S_{3}$ which sends $1$ to $2$, $2$ to $1$ and $3$ to $3$ is
written $\left(  2,1,3\right)  $ in one-line notation.

The exact relation between lists and permutations is given by the following
simple fact:

\begin{proposition}
\label{prop.perms.lists}Let $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $.

\textbf{(a)} If $\sigma\in S_{n}$, then each element of $\left[  n\right]  $
appears exactly once in the list $\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.

\textbf{(b)} If $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $ is a list of
elements of $\left[  n\right]  $ such that each element of $\left[  n\right]
$ appears exactly once in this list $\left(  p_{1},p_{2},\ldots,p_{n}\right)
$, then there exists a unique permutation $\sigma\in S_{n}$ such that $\left(
p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.

\textbf{(c)} Let $k\in\left\{  0,1,\ldots,n\right\}  $. If $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ is a list of some elements of $\left[
n\right]  $ such that $p_{1},p_{2},\ldots,p_{k}$ are distinct, then there
exists a permutation $\sigma\in S_{n}$ such that $\left(  p_{1},p_{2}%
,\ldots,p_{k}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  k\right)  \right)  $.
\end{proposition}

At this point, let us clarify what we mean by \textquotedblleft
distinct\textquotedblright: Several objects $u_{1},u_{2},\ldots,u_{k}$ are
said to be \textit{distinct} if every $i\in\left\{  1,2,\ldots,k\right\}  $
and $j\in\left\{  1,2,\ldots,k\right\}  $ satisfying $i\neq j$ satisfy
$u_{i}\neq u_{j}$. (Some people call this \textquotedblleft pairwise
distinct\textquotedblright.) So, for example, the numbers $2,1,6$ are
distinct, but the numbers $6,1,6$ are not (although $6$ and $1$ are distinct).
Instead of saying that some objects $u_{1},u_{2},\ldots,u_{k}$ are distinct,
we can also say that \textquotedblleft the list $\left(  u_{1},u_{2}%
,\ldots,u_{k}\right)  $ has no repetitions\textquotedblright\footnote{A
repetition just means an element which occurs more than once in the list. It
does not matter whether the occurrences are at consecutive positions or not.}.

\begin{remark}
The $\sigma$ in Proposition \ref{prop.perms.lists} \textbf{(b)} is uniquely
determined, but the $\sigma$ in Proposition \ref{prop.perms.lists}
\textbf{(c)} is not (in general). More precisely, in Proposition
\ref{prop.perms.lists} \textbf{(c)}, there are $\left(  n-k\right)  !$
possible choices of $\sigma$ that work. (This is easy to check.)
\end{remark}

\begin{proof}
[Proof of Proposition \ref{prop.perms.lists}.]Proposition
\ref{prop.perms.lists} is a really basic fact and its proof is simple. I am
going to present the proof at high detail in order to make sure you correctly
understand every notion involved in it; if you find it obvious, you are
(probably) getting it right and you don't need to read my boring proof.

Recall that $S_{n}$ is the set of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $. In other words, $S_{n}$ is the set of all
permutations of the set $\left[  n\right]  $ (since $\left\{  1,2,\ldots
,n\right\}  =\left[  n\right]  $).

\textbf{(a)} Let $\sigma\in S_{n}$. Let $i\in\left[  n\right]  $.

We have $\sigma\in S_{n}$. In other words, $\sigma$ is a permutation of
$\left[  n\right]  $ (since $S_{n}$ is the set of all permutations of the set
$\left[  n\right]  $). In other words, $\sigma$ is a bijective map $\left[
n\right]  \rightarrow\left[  n\right]  $. Hence, $\sigma$ is both surjective
and injective.

Now, we make the following two observations:

\begin{itemize}
\item The number $i$ appears in the list $\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)
$\ \ \ \ \footnote{\textit{Proof.} The map $\sigma$ is surjective. Hence,
there exists some $j\in\left[  n\right]  $ such that $i=\sigma\left(
j\right)  $. In other words, the number $i$ appears in the list $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $. Qed.}.

\item The number $i$ appears at most once in the list $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)
$\ \ \ \ \footnote{\textit{Proof.} Let us assume the contrary (for the sake of
contradiction). Thus, $i$ appears more than once in the list $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $. In other words, $i$ appears at least twice in this list.
In other words, there exist two distinct elements $p$ and $q$ of $\left[
n\right]  $ such that $\sigma\left(  p\right)  =i$ and $\sigma\left(
q\right)  =i$. Consider these $p$ and $q$.
\par
We have $p\neq q$ (since $p$ and $q$ are distinct), so that $\sigma\left(
p\right)  \neq\sigma\left(  q\right)  $ (since $\sigma$ is injective). This
contradicts $\sigma\left(  p\right)  =i=\sigma\left(  q\right)  $. This
contradiction proves that our assumption was wrong, qed.}.
\end{itemize}

Combining these two observations, we conclude that the number $i$ appears
exactly once in the list $\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.

Let us now forget that we fixed $i$. We thus have shown that if $i\in\left[
n\right]  $, then $i$ appears exactly once in the list $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.
In other words, each element of $\left[  n\right]  $ appears exactly once in
the list $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $. This proves Proposition
\ref{prop.perms.lists} \textbf{(a)}.

\textbf{(b)} Let $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $ be a list of
elements of $\left[  n\right]  $ such that each element of $\left[  n\right]
$ appears exactly once in this list $\left(  p_{1},p_{2},\ldots,p_{n}\right)
$.

We have $p_{i}\in\left[  n\right]  $ for every $i\in\left[  n\right]  $ (since
$\left(  p_{1},p_{2},\ldots,p_{n}\right)  $ is a list of elements of $\left[
n\right]  $).

We define a map $\tau:\left[  n\right]  \rightarrow\left[  n\right]  $ by
setting%
\begin{equation}
\left(  \tau\left(  i\right)  =p_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left[  n\right]  \right)  . \label{pf.prop.perms.lists.b.1}%
\end{equation}
(This is well-defined, because we have $p_{i}\in\left[  n\right]  $ for every
$i\in\left[  n\right]  $.) The map $\tau$ is
injective\footnote{\textit{Proof.} Let $u$ and $v$ be two elements of $\left[
n\right]  $ such that $\tau\left(  u\right)  =\tau\left(  v\right)  $. We
shall show that $u=v$.
\par
Indeed, we assume the contrary (for the sake of contradiction). Thus, $u\neq
v$.
\par
The definition of $\tau\left(  u\right)  $ shows that $\tau\left(  u\right)
=p_{u}$. But we also have $\tau\left(  u\right)  =\tau\left(  v\right)
=p_{v}$ (by the definition of $\tau\left(  v\right)  $). Now, the element
$\tau\left(  u\right)  $ of $\left[  n\right]  $ appears (at least) twice in
the list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $: once at the $u$-th
position (since $\tau\left(  u\right)  =p_{u}$), and again at the $v$-th
position (since $\tau\left(  u\right)  =p_{v}$). (And these are two distinct
positions, because $u\neq v$.)
\par
But let us recall that each element of $\left[  n\right]  $ appears exactly
once in this list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. Hence, no
element of $\left[  n\right]  $ appears more than once in the list $\left(
p_{1},p_{2},\ldots,p_{n}\right)  $. In particular, $\tau\left(  u\right)  $
cannot appear more than once in this list $\left(  p_{1},p_{2},\ldots
,p_{n}\right)  $. This contradicts the fact that $\tau\left(  u\right)  $
appears twice in the list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $.
\par
This contradiction shows that our assumption was wrong. Hence, $u=v$ is
proven.
\par
Now, let us forget that we fixed $u$ and $v$. We thus have proven that if $u$
and $v$ are two elements of $\left[  n\right]  $ such that $\tau\left(
u\right)  =\tau\left(  v\right)  $, then $u=v$. In other words, the map $\tau$
is injective. Qed.} and surjective\footnote{\textit{Proof.} Let $u\in\left[
n\right]  $. Each element of $\left[  n\right]  $ appears exactly once in the
list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. Applying this to the element
$u$ of $\left[  n\right]  $, we conclude that $u$ appears exactly once in the
list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. In other words, there exists
exactly one $i\in\left[  n\right]  $ such that $u=p_{i}$. Consider this $i$.
The definition of $\tau$ yields $\tau\left(  i\right)  =p_{i}$. Compared with
$u=p_{i}$, this yields $\tau\left(  i\right)  =u$.
\par
Hence, there exists a $j\in\left[  n\right]  $ such that $\tau\left(
j\right)  =u$ (namely, $j=i$).
\par
Let us now forget that we fixed $u$. We thus have proven that for every
$u\in\left[  n\right]  $, there exists a $j\in\left[  n\right]  $ such that
$\tau\left(  j\right)  =u$. In other words, the map $\tau$ is surjective.
Qed.}. Hence, the map $\tau$ is bijective. In other words, $\tau$ is a
permutation of $\left[  n\right]  $ (since $\tau$ is a map $\left[  n\right]
\rightarrow\left[  n\right]  $). In other words, $\tau\in S_{n}$ (since
$S_{n}$ is the set of all permutations of the set $\left[  n\right]  $).
Clearly, $\left(  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots
,\tau\left(  n\right)  \right)  =\left(  p_{1},p_{2},\ldots,p_{n}\right)  $
(because of (\ref{pf.prop.perms.lists.b.1})), so that $\left(  p_{1}%
,p_{2},\ldots,p_{n}\right)  =\left(  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  n\right)  \right)  $.

Hence, there exists a permutation $\sigma\in S_{n}$ such that \newline$\left(
p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $ (namely,
$\sigma=\tau$). Moreover, there exists \textbf{at most one} such
permutation\footnote{\textit{Proof.} Let $\sigma_{1}$ and $\sigma_{2}$ be two
permutations $\sigma\in S_{n}$ such that $\left(  p_{1},p_{2},\ldots
,p_{n}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $. Thus, $\sigma_{1}$ is a
permutation in $S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)
=\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  $, and $\sigma_{2}$ is a
permutation in $S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)
=\left(  \sigma_{2}\left(  1\right)  ,\sigma_{2}\left(  2\right)
,\ldots,\sigma_{2}\left(  n\right)  \right)  $.
\par
We have $\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  =\left(  p_{1},p_{2},\ldots
,p_{n}\right)  =\left(  \sigma_{2}\left(  1\right)  ,\sigma_{2}\left(
2\right)  ,\ldots,\sigma_{2}\left(  n\right)  \right)  $. In other words,
every $i\in\left[  n\right]  $ satisfies $\sigma_{1}\left(  i\right)
=\sigma_{2}\left(  i\right)  $. In other words, $\sigma_{1}=\sigma_{2}$.
\par
Let us now forget that we fixed $\sigma_{1}$ and $\sigma_{2}$. We thus have
shown that if $\sigma_{1}$ and $\sigma_{2}$ are two permutations $\sigma\in
S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)  =\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $, then $\sigma_{1}=\sigma_{2}$. In other words, any two
permutations $\sigma\in S_{n}$ such that $\left(  p_{1},p_{2},\ldots
,p_{n}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $ must be equal to each other. In
other words, there exists \textbf{at most one} permutation $\sigma\in S_{n}$
such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.
Qed.}. Combining the claims of the previous two sentences, we conclude that
there exists a unique permutation $\sigma\in S_{n}$ such that $\left(
p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $. This
proves Proposition \ref{prop.perms.lists} \textbf{(b)}.

\textbf{(c)} Let $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ be a list of some
elements of $\left[  n\right]  $ such that $p_{1},p_{2},\ldots,p_{k}$ are
distinct. Thus, the list $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ contains
$k$ of the $n$ elements of $\left[  n\right]  $ (because $p_{1},p_{2}%
,\ldots,p_{k}$ are distinct). Let $q_{1},q_{2},\ldots,q_{n-k}$ be the
remaining $n-k$ elements of $\left[  n\right]  $ (listed in any arbitrary
order, with no repetition). Then, $\left(  p_{1},p_{2},\ldots,p_{k}%
,q_{1},q_{2},\ldots,q_{n-k}\right)  $ is a list of all $n$ elements of
$\left[  n\right]  $, with no repetitions\footnote{It has no repetitions
because:
\par
\begin{itemize}
\item there are no repetitions among $p_{1},p_{2},\ldots,p_{k}$;
\par
\item there are no repetitions among $q_{1},q_{2},\ldots,q_{n-k}$;
\par
\item the two lists $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ and $\left(
q_{1},q_{2},\ldots,q_{n-k}\right)  $ have no elements in common (because we
defined $q_{1},q_{2},\ldots,q_{n-k}$ to be the \textquotedblleft
remaining\textquotedblright\ $n-k$ elements of $\left[  n\right]  $, where
\textquotedblleft remaining\textquotedblright\ means \textquotedblleft not
contained in the list $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $%
\textquotedblright).
\end{itemize}
}. In other words, each element of $\left[  n\right]  $ appears exactly once
in this list $\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  $ (and each entry in this list is an element of $\left[
n\right]  $). Hence, we can apply Proposition \ref{prop.perms.lists}
\textbf{(b)} to $\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  $ instead of $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. As
a consequence, we conclude that there exists a unique permutation $\sigma\in
S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $. Let $\tau$ be this $\sigma$.

Thus, $\tau\in S_{n}$ is a permutation such that
\[
\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots,q_{n-k}\right)  =\left(
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  n\right)
\right)  .
\]
Now,%
\begin{align*}
&  \left(  p_{1},p_{2},\ldots,p_{k}\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  }_{=\left(  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  n\right)  \right)  }\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  n\right)
\right)  \right) \\
&  =\left(  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(
k\right)  \right)  .
\end{align*}
Hence, there exists a permutation $\sigma\in S_{n}$ such that \newline$\left(
p_{1},p_{2},\ldots,p_{k}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  k\right)  \right)  $ (namely,
$\sigma=\tau$). This proves Proposition \ref{prop.perms.lists} \textbf{(c)}.
\end{proof}

\subsection{Inversions, lengths and the permutations $s_{i} \in S_{n}$}

For each $i\in\left\{  1,2,\ldots,n-1\right\}  $, let $s_{i}$ be the
permutation in $S_{n}$ that switches $i$ with $i+1$ but leaves all other
numbers unchanged. Formally speaking, $s_{i}$ is the permutation in $S_{n}$
given by%
\[
\left(  s_{i}\left(  k\right)  =%
\begin{cases}
i+1, & \text{if }k=i;\\
i, & \text{if }k=i+1;\\
k, & \text{if }k\notin\left\{  i,i+1\right\}
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }k\in\left\{  1,2,\ldots,n\right\}  \right)
.
\]
Thus, in one-line notation%
\[
s_{i}=\left(  1,2,\ldots,i-1,i+1,i,i+2,\ldots,n\right)  .
\]
Notice that $s_{i}^{2}=\operatorname*{id}$ for every $i\in\left\{
1,2,\ldots,n-1\right\}  $. (Here, we are using the notation $\alpha^{2}$ for
$\alpha\circ\alpha$, where $\alpha$ is a permutation in $S_{n}$.)

\begin{exercise}
\label{exe.ps2.2.4}\textbf{(a)} Show that $s_{i}\circ s_{i+1}\circ
s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}$ for all $i\in\left\{  1,2,\ldots
,n-2\right\}  $.

\textbf{(b)} Show that every permutation $\sigma\in S_{n}$ can be written as a
composition of several permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). For example, if $n=3$, then the permutation
$\left(  3,1,2\right)  $ in $S_{3}$ can be written as the composition
$s_{2}\circ s_{1}$, while the permutation $\left(  3,2,1\right)  $ in $S_{3}$
can be written as the composition $s_{1}\circ s_{2}\circ s_{1}$ or also as the
composition $s_{2}\circ s_{1}\circ s_{2}$.

[\textbf{Hint:} If you do not immediately see why this works, consider reading further.]

\textbf{(c)} Let $w_{0}$ denote the permutation in $S_{n}$ which sends each
$k\in\left\{  1,2,\ldots,n\right\}  $ to $n+1-k$. (In one-line notation, this
$w_{0}$ is written as $\left(  n,n-1,\ldots,1\right)  $.) Find an
\textbf{explicit} way to write $w_{0}$ as a composition of several
permutations of the form $s_{i}$ (with $i\in\left\{  1,2,\ldots,n-1\right\}  $).
\end{exercise}

\begin{remark}
Symmetric groups appear in almost all parts of mathematics; unsurprisingly,
there is no universally accepted notation for them. We are using the notation
$S_{n}$ for the $n$-th symmetric group; other common notations for it are
$\mathfrak{S}_{n}$, $\Sigma_{n}$ and $\operatorname*{Sym}\left(  n\right)  $.
The permutations that we call $s_{1},s_{2},\ldots,s_{n-1}$ are often called
$\sigma_{1},\sigma_{2},\ldots,\sigma_{n-1}$. As already mentioned in
Definition \ref{def.composition}, some people write the composition of maps
\textquotedblleft backwards\textquotedblright, which causes their $\sigma
\circ\tau$ to be our $\tau\circ\sigma$, etc.. (Sadly, most authors are so sure
that their notation is standard that they never bother to define it.)

In the language of group theory, the statement of Exercise \ref{exe.ps2.2.4}
\textbf{(b)} says (or, more precisely, yields) that the permutations
$s_{1},s_{2},\ldots,s_{n-1}$ generate the group $S_{n}$.
\end{remark}

\begin{definition}
If $\sigma\in S_{n}$ is a permutation, then an \textit{inversion} of $\sigma$
means a pair $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$
and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. For instance, the
inversions of the permutation $\left(  3,1,2\right)  $ in $S_{3}$ are $\left(
1,2\right)  $ and $\left(  1,3\right)  $ (because $3>1$ and $3>2$), while the
only inversion of the permutation $\left(  1,3,2\right)  $ in $S_{3}$ is
$\left(  2,3\right)  $ (since $3>2$).

If $\sigma\in S_{n}$ is a permutation, then the \textit{length} of $\sigma$
means the number of inversions of $\sigma$. This length is denoted by
$\ell\left(  \sigma\right)  $; it is a nonnegative integer.
\end{definition}

Any $\sigma\in S_{n}$ satisfies $0\leq\ell\left(  \sigma\right)  \leq
\dbinom{n}{2}$ (since the number of inversions of $\sigma$ is clearly no
larger than the total number of pairs $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$; but the latter number is $\dbinom{n}{2}$). The
only permutation in $S_{n}$ having length $0$ is the identity permutation
$\operatorname*{id}=\left(  1,2,\ldots,n\right)  \in S_{n}$%
\ \ \ \ \footnote{The fact that the identity permutation $\operatorname*{id}%
\in S_{n}$ has length $\ell\left(  \operatorname*{id}\right)  =0$ is trivial.
The fact that it is the only one such permutation is easy (it essentially
follows from Exercise \ref{exe.ps2.2.5} \textbf{(d)}).}.

\begin{exercise}
\label{exe.ps2.2.5}\textbf{(a)} Show that every permutation $\sigma\in S_{n}$
and every $k\in\left\{  1,2,\ldots,n-1\right\}  $ satisfy%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  \label{eq.exe.2.5.a.1}%
\end{equation}
and%
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  <\sigma^{-1}\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  >\sigma^{-1}\left(  k+1\right)
\end{array}
\right.  . \label{eq.exe.2.5.a.2}%
\end{equation}


\textbf{(b)} Show that any two permutations $\sigma$ and $\tau$ in $S_{n}$
satisfy $\ell\left(  \sigma\circ\tau\right)  \equiv\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \operatorname{mod}2$.

\textbf{(c)} Show that any two permutations $\sigma$ and $\tau$ in $S_{n}$
satisfy $\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  $.

\textbf{(d)} If $\sigma\in S_{n}$ is a permutation satisfying $\sigma\left(
1\right)  \leq\sigma\left(  2\right)  \leq\cdots\leq\sigma\left(  n\right)  $,
then show that $\sigma=\operatorname*{id}$.

\textbf{(e)} Let $\sigma\in S_{n}$. Show that $\sigma$ can be written as a
composition of $\ell\left(  \sigma\right)  $ permutations of the form $s_{k}$
(with $k\in\left\{  1,2,\ldots,n-1\right\}  $).

\textbf{(f)} Let $\sigma\in S_{n}$. Then, show that $\ell\left(
\sigma\right)  =\ell\left(  \sigma^{-1}\right)  $.

\textbf{(g)} Let $\sigma\in S_{n}$. Show that $\ell\left(  \sigma\right)  $ is
the smallest $N\in\mathbb{N}$ such that $\sigma$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $).
\end{exercise}

\begin{example}
\label{exa.2.5}Let us justify Exercise \ref{exe.ps2.2.5} \textbf{(a)} on an
example. The solution to Exercise \ref{exe.ps2.2.5} \textbf{(a)} given below
is essentially a (tiresome) formalization of the ideas seen in this example.

Let $n=5$, $k=3$ and $\sigma=\left(  4,2,1,5,3\right)  $ (written in one-line
notation). Then, $\sigma\circ s_{k}=\left(  4,2,5,1,3\right)  $; this is the
permutation obtained by switching the $k$-th and the $\left(  k+1\right)  $-th
entry of $\sigma$ (where the word \textquotedblleft entry\textquotedblright%
\ refers to the one-line notation). On the other hand, $s_{k}\circ
\sigma=\left(  3,2,1,5,4\right)  $; this is the permutation obtained by
switching the entry $k$ with the entry $k+1$ of $\sigma$. Mind the difference
between these two operations.

The inversions of $\sigma=\left(  4,2,1,5,3\right)  $ are $\left(  1,2\right)
$, $\left(  1,3\right)  $, $\left(  1,5\right)  $, $\left(  2,3\right)  $ and
$\left(  4,5\right)  $. These are the pairs $\left(  i,j\right)  $ of
positions such that $i$ is before $j$ (that is, $i<j$) but the $i$-th entry of
$\sigma$ is larger than the $j$-th entry of $\sigma$ (that is, $\sigma\left(
i\right)  >\sigma\left(  j\right)  $). In other words, these are the pairs of
positions at which the entries of $\sigma$ are out of order. On the other
hand, the inversions of $s_{k}\circ\sigma=\left(  3,2,1,5,4\right)  $ are
$\left(  1,2\right)  $, $\left(  1,3\right)  $, $\left(  2,3\right)  $ and
$\left(  4,5\right)  $. These are precisely the inversions of $\sigma$ except
for $\left(  1,5\right)  $. This is no surprise: In fact, $s_{k}\circ\sigma$
is obtained from $\sigma$ by switching the entry $k$ with the entry $k+1$, and
this operation clearly preserves all inversions other than the one that is
directly being turned around (i.e., the inversion $\left(  i,j\right)  $ where
$\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}  =\left\{
k,k+1\right\}  $; in our case, this is the inversion $\left(  1,5\right)  $).
In general, when $\sigma^{-1}\left(  k\right)  >\sigma^{-1}\left(  k+1\right)
$ (that is, when $k$ appears further left than $k+1$ in the one-line notation
of $\sigma$), the inversions of $s_{k}\circ\sigma$ are the inversions of
$\sigma$ except for $\left(  \sigma^{-1}\left(  k+1\right)  ,\sigma
^{-1}\left(  k\right)  \right)  $. Therefore, in this case, the number of
inversions of $s_{k}\circ\sigma$ equals the number of inversions of $\sigma$
plus $1$. That is, in this case, $\ell\left(  s_{k}\circ\sigma\right)
=\ell\left(  \sigma\right)  +1$. When $\sigma^{-1}\left(  k\right)
<\sigma^{-1}\left(  k+1\right)  $, a similar argument shows $\ell\left(
s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)  -1$. This explains why
(\ref{eq.exe.2.5.a.2}) holds (although formalizing this argument will be tedious).

The inversions of $\sigma\circ s_{k}=\left(  4,2,5,1,3\right)  $ are $\left(
1,2\right)  $, $\left(  1,4\right)  $, $\left(  1,5\right)  $, $\left(
2,4\right)  $, $\left(  3,4\right)  $ and $\left(  3,5\right)  $. Unlike the
inversions of $s_{k}\circ\sigma$, these are not directly related to the
inversions of $\sigma$, so the argument in the previous paragraph does not
prove (\ref{eq.exe.2.5.a.1}). However, instead of considering inversions of
$\sigma$, one can consider inversions of $\sigma^{-1}$. These are even more
intuitive: They are the pairs of integers $\left(  i,j\right)  $ with $1\leq
i<j\leq n$ such that $i$ appears further right than $j$ in the one-line
notation of $\sigma$. For instance, the inversions of $\sigma^{-1}$ are
$\left(  1,2\right)  $, $\left(  1,4\right)  $, $\left(  2,4\right)  $,
$\left(  3,4\right)  $ and $\left(  3,5\right)  $, whereas the inversions of
$\left(  \sigma\circ s_{k}\right)  ^{-1}$ are all of these and also $\left(
1,5\right)  $. But there is no need to repeat our proof of
(\ref{eq.exe.2.5.a.2}); it is easier to deduce (\ref{eq.exe.2.5.a.1}) from
(\ref{eq.exe.2.5.a.2}) by applying (\ref{eq.exe.2.5.a.2}) to $\sigma^{-1}$
instead of $\sigma$ and appealing to Exercise \ref{exe.ps2.2.5} \textbf{(f)}.
(Again, see the solution below for the details.)
\end{example}

Notice that Exercise \ref{exe.ps2.2.5} \textbf{(e)} immediately yields
Exercise \ref{exe.ps2.2.4} \textbf{(b)}.

\begin{remark}
When $n=0$ or $n=1$, we have $\left\{  1,2,\ldots,n-1\right\}  =\varnothing$.
Hence, Exercise \ref{exe.ps2.2.4} \textbf{(e)} looks strange in the case when
$n=0$ or $n=1$, because in this case, there are no permutations of the form
$s_{k}$ to begin with. Nevertheless, it is correct. Indeed, when $n=0$ or
$n=1$, there is only one permutation $\sigma\in S_{n}$, namely the identity
permutation $\operatorname*{id}$, and it has length $\ell\left(
\sigma\right)  =\ell\left(  \operatorname*{id}\right)  =0$. Thus, in this
case, Exercise \ref{exe.ps2.2.4} \textbf{(e)} claims that $\operatorname*{id}$
can be written as a composition of $0$ permutations of the form $s_{k}$ (with
$k\in\left\{  1,2,\ldots,n-1\right\}  $). This is true: Even from an empty set
we can always pick $0$ elements; and the composition of $0$ permutations will
be $\operatorname*{id}$.
\end{remark}

\begin{remark}
The word \textquotedblleft length\textquotedblright\ for $\ell\left(
\sigma\right)  $ can be confusing: It does not refer to the length of the
$n$-tuple $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $ (which is $n$). The reason why it
is called \textquotedblleft length\textquotedblright\ is Exercise
\ref{exe.ps2.2.5} \textbf{(g)}: it says that $\ell\left(  \sigma\right)  $ is
the smallest number of permutations of the form $s_{k}$ which can be
multiplied to give $\sigma$; thus, it is the smallest possible length of an
expression of $\sigma$ as a product of $s_{k}$'s.

The use of the word \textquotedblleft length\textquotedblright, unfortunately,
is not standard across literature. Some authors call \textquotedblleft Coxeter
length\textquotedblright\ what we call \textquotedblleft
length\textquotedblright, and use the word \textquotedblleft
length\textquotedblright\ itself for a different notion.
\end{remark}

\begin{exercise}
\label{exe.ps2.2.6}Let $\sigma\in S_{n}$. In Exercise \ref{exe.ps2.2.4}
\textbf{(b)}, we have seen that $\sigma$ can be written as a composition of
several permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots
,n-1\right\}  $). Usually there will be several ways to do so (for instance,
$\operatorname*{id}=s_{1}\circ s_{1}=s_{2}\circ s_{2}=\cdots=s_{n-1}\circ
s_{n-1}$). Show that, whichever of these ways we take, the number of
permutations composed will be congruent to $\ell\left(  \sigma\right)  $
modulo $2$.
\end{exercise}

\subsection{\label{sect.sign}The sign of a permutation}

\begin{definition}
\label{def.perm.sign}We define the \textit{sign} of a permutation $\sigma\in
S_{n}$ as the integer $\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$. We
denote this sign by $\left(  -1\right)  ^{\sigma}$ or $\operatorname*{sign}%
\sigma$ or $\operatorname*{sgn}\sigma$. We say that a permutation $\sigma$ is
\textit{even} if its sign is $1$ (that is, if $\ell\left(  \sigma\right)  $ is
even), and \textit{odd} if its sign is $-1$ (that is, if $\ell\left(
\sigma\right)  $ is odd).
\end{definition}

Signs of permutations have the following properties:

\begin{itemize}
\item The sign of the identity permutation $\operatorname*{id}\in S_{n}$ is
$\left(  -1\right)  ^{\operatorname*{id}}=1$ (because the definition of
$\left(  -1\right)  ^{\operatorname*{id}}$ yields $\left(  -1\right)
^{\operatorname*{id}}=\left(  -1\right)  ^{\ell\left(  \operatorname*{id}%
\right)  }=1$ (since $\ell\left(  \operatorname*{id}\right)  =0$)). In other
words, $\operatorname*{id}\in S_{n}$ is even.

\item For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the sign of the
permutation $s_{k}\in S_{n}$ is $\left(  -1\right)  ^{s_{k}}=-1$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Applying (\ref{eq.exe.2.5.a.1}) to $\sigma=\operatorname*{id}$, we obtain
\begin{align*}
\ell\left(  \operatorname*{id}\circ s_{k}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \operatorname*{id}\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if
}\operatorname*{id}\left(  k\right)  <\operatorname*{id}\left(  k+1\right)
;\\
\ell\left(  \operatorname*{id}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if
}\operatorname*{id}\left(  k\right)  >\operatorname*{id}\left(  k+1\right)
\end{array}
\right. \\
&  =\underbrace{\ell\left(  \operatorname*{id}\right)  }_{=0}%
+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{id}\left(  k\right)
=k<k+1=\operatorname*{id}\left(  k+1\right)  \right) \\
&  =1.
\end{align*}
This rewrites as $\ell\left(  s_{k}\right)  =1$ (since $\operatorname*{id}%
\circ s_{k}=s_{k}$). Now, the definition of $\left(  -1\right)  ^{s_{k}}$
yields $\left(  -1\right)  ^{s_{k}}=\left(  -1\right)  ^{\ell\left(
s_{k}\right)  }=-1$ (since $\ell\left(  s_{k}\right)  =1$), qed.}.

\item If $\sigma$ and $\tau$ are two permutations in $S_{n}$, then $\left(
-1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(
-1\right)  ^{\tau}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$ and
$\tau\in S_{n}$. Exercise \ref{exe.ps2.2.5} \textbf{(b)} yields $\ell\left(
\sigma\circ\tau\right)  \equiv\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  \operatorname{mod}2$, so that $\left(  -1\right)  ^{\ell\left(
\sigma\circ\tau\right)  }=\left(  -1\right)  ^{\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  }=\left(  -1\right)  ^{\ell\left(  \sigma\right)
}\cdot\left(  -1\right)  ^{\ell\left(  \tau\right)  }$. But the definition of
the sign of a permutation yields $\left(  -1\right)  ^{\sigma\circ\tau
}=\left(  -1\right)  ^{\ell\left(  \sigma\circ\tau\right)  }$, $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$ and
$\left(  -1\right)  ^{\tau}=\left(  -1\right)  ^{\ell\left(  \tau\right)  }$.
Hence, $\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\ell\left(
\sigma\circ\tau\right)  }=\underbrace{\left(  -1\right)  ^{\ell\left(
\sigma\right)  }}_{=\left(  -1\right)  ^{\sigma}}\cdot\underbrace{\left(
-1\right)  ^{\ell\left(  \tau\right)  }}_{=\left(  -1\right)  ^{\tau}}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$, qed.}.

\item If $\sigma\in S_{n}$, then $\left(  -1\right)  ^{\sigma^{-1}}=\left(
-1\right)  ^{\sigma}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$.
The definition of $\left(  -1\right)  ^{\sigma^{-1}}$ yields $\left(
-1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\ell\left(  \sigma^{-1}\right)
}$. But recall that $\ell\left(  \sigma\right)  =\ell\left(  \sigma
^{-1}\right)  $. The definition of $\left(  -1\right)  ^{\sigma}$ yields
$\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)
}=\left(  -1\right)  ^{\ell\left(  \sigma^{-1}\right)  }$ (since $\ell\left(
\sigma\right)  =\ell\left(  \sigma^{-1}\right)  $). Compared with $\left(
-1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\ell\left(  \sigma^{-1}\right)
}$, this yields $\left(  -1\right)  ^{\sigma^{-1}}=\left(  -1\right)
^{\sigma}$, qed.}.
\end{itemize}

The first and the third of these properties are often summarized as the
statement that \textquotedblleft sign is a group homomorphism from the group
$S_{n}$ to the multiplicative group $\left\{  1,-1\right\}  $%
\textquotedblright. In this statement, \textquotedblleft
sign\textquotedblright\ means the map from $S_{n}$ to $\left\{  1,-1\right\}
$ which sends every permutation $\sigma$ to its sign $\left(  -1\right)
^{\ell\left(  \sigma\right)  }$, and the \textquotedblleft multiplicative
group $\left\{  1,-1\right\}  $\textquotedblright\ means the group $\left\{
1,-1\right\}  $ whose binary operation is multiplication.

We have defined the sign of a permutation $\sigma\in S_{n}$. More generally,
it is possible to define the sign of a permutation of an arbitrary finite set
$X$, even though the length of such a permutation is not defined!\footnote{How
does it work? If $X$ is a finite set, then we can always find a bijection
$\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$.
(Constructing such a bijection is tantamount to writing down a list of all
elements of $X$, with no duplicates.) Given such a bijection $\phi$, we can
define the sign of any permutation $\sigma$ of $X$ as follows:%
\begin{equation}
\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\phi\circ\sigma\circ
\phi^{-1}}. \label{eq.ps2.S(X).sign.teaser}%
\end{equation}
Here, the right hand side is well-defined because $\phi\circ\sigma\circ
\phi^{-1}$ is a permutation of $\left\{  1,2,\ldots,n\right\}  $. What is not
immediately obvious is that this sign is independent on the choice of $\phi$,
and that it is a group homomorphism to $\left\{  1,-1\right\}  $ (that is, we
have $\left(  -1\right)  ^{\operatorname*{id}}=1$ and $\left(  -1\right)
^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau
}$). We will prove these facts further below (in Exercise \ref{exe.ps4.2}).}

\begin{exercise}
\label{exe.ps2.2.7}Let $n\geq2$. Show that the number of even permutations in
$S_{n}$ is $n!/2$, and the number of odd permutations in $S_{n}$ is also
$n!/2$.
\end{exercise}

The sign of a permutation is used in the combinatorial definition of the
determinant. Let us briefly show this definition now; we shall return to it
later (in Chapter \ref{chp.det}) to study it in much more detail.

\begin{definition}
\label{def.det.old}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix (say, with complex
entries, although this does not matter much -- it suffices that the entries
can be added and multiplied and the axioms of associativity, distributivity,
commutativity, unity etc. hold). The \textit{determinant} $\det A$ of $A$ is
defined as%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)
}a_{2,\sigma\left(  2\right)  }\cdots a_{n,\sigma\left(  n\right)  }.
\label{eq.det.old}%
\end{equation}

\end{definition}

Let me try to describe the sum (\ref{eq.det.old}) in slightly more visual
terms: The sum (\ref{eq.det.old}) has $n!$ addends, each of which has the form
\textquotedblleft$\left(  -1\right)  ^{\sigma}$ times a
product\textquotedblright. The product has $n$ factors, which are entries of
$A$, and are chosen in such a way that there is exactly one entry taken from
each row and exactly one from each column. Which precise entries are taken
depends on $\sigma$: namely, for each $i$, we take the $\sigma\left(
i\right)  $-th entry from the $i$-th row.

Convince yourself that the classical formulas%
\begin{align*}
\det\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)   &  =a;\\
\det\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)   &  =ad-bc;\\
\det\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)   &  =aei+bfg+cdh-ahf-bdi-ceg
\end{align*}
are particular cases of (\ref{eq.det.old}). Whenever $n\geq2$, the sum in
(\ref{eq.det.old}) contains precisely $n!/2$ plus signs and $n!/2$ minus signs
(because of Exercise \ref{exe.ps2.2.7}).

Definition \ref{def.det.old} is merely one of several equivalent definitions
of the determinant. You will probably see two of them in an average linear
algebra class. Each of them has its own advantages and drawbacks. Definition
\ref{def.det.old} is the most direct, assuming that one knows about the sign
of a permutation.

\subsection{\label{sect.infperm}Infinite permutations}

(This section is optional; it explores some technical material which is useful
in combinatorics, but is not necessary for what follows. I advise the reader
to skip it at the first read.)

We have introduced the notion of a permutation of an arbitrary set; but so
far, we have only studied permutations of finite sets. In this section (which
is tangential to our project; probably nothing from this section will be used
ever after), let me discuss permutations of the infinite set $\left\{
1,2,3,\ldots\right\}  $. (A lot of what I say below can be easily adapted to
the sets $\mathbb{N}$ and $\mathbb{Z}$ as well.)

We recall that a permutation of a set $X$ means a bijection from $X$ to $X$.

Let $S_{\infty}$ be the symmetric group of the set $\left\{  1,2,3,\ldots
\right\}  $. This is the set of all permutations of $\left\{  1,2,3,\ldots
\right\}  $. It contains the identity permutation $\operatorname*{id}\in
S_{\infty}$ which sends every $i\in\left\{  1,2,3,\ldots\right\}  $ to $i$.
The set $S_{\infty}$ is uncountable\footnote{More generally, while a finite
set of size $n$ has $n!$ permutations, an infinite set $S$ has uncountably
many permutations (even if $S$ is countable).}.

We shall try to study $S_{\infty}$ similarly to how we studied $S_{n}$ for
$n\in\mathbb{N}$. However, we soon will notice that the analogy between
$S_{\infty}$ and $S_{n}$ will break down.\footnote{The uncountability of
$S_{\infty}$ is the first hint that $S_{\infty}$ is \textquotedblleft too
large\textquotedblright\ a set to be a good analogue of the finite set $S_{n}%
$.} To amend this, we shall define a subset $S_{\left(  \infty\right)  }$ of
$S_{\infty}$ (mind the parentheses around the \textquotedblleft$\infty
$\textquotedblright) which is smaller and more wieldy, and indeed shares many
of the properties of the finite symmetric group $S_{n}$.

We define $S_{\left(  \infty\right)  }$ as follows:%
\begin{equation}
S_{\left(  \infty\right)  }=\left\{  \sigma\in S_{\infty}\ \mid\ \sigma\left(
i\right)  =i\text{ for all but finitely many }i\in\left\{  1,2,3,\ldots
\right\}  \right\}  . \label{eq.S(infty).def}%
\end{equation}
Let us first explain what \textquotedblleft all but finitely many
$i\in\left\{  1,2,3,\ldots\right\}  $\textquotedblright\ means:

\begin{definition}
\label{def.allbutfin}Let $I$ be a set. Let $\mathcal{A}\left(  i\right)  $ be
a statement for every $i\in I$. Then, we say that \textquotedblleft%
$\mathcal{A}\left(  i\right)  $ for all but finitely many $i\in I$%
\textquotedblright\ if and only if there exists some finite subset $J$ of $I$
such that every $i\in I\setminus J$ satisfies $\mathcal{A}\left(  i\right)  $.\ \ \ \ \footnotemark
\end{definition}

\footnotetext{Thus, the statement \textquotedblleft$\mathcal{A}\left(
i\right)  $ for all but finitely many $i\in I$\textquotedblright\ can be
restated as \textquotedblleft$\mathcal{A}\left(  i\right)  $ holds for all
$i\in I$, apart from finitely many exceptions\textquotedblright\ or as
\textquotedblleft there are only finitely many $i\in I$ which do not satisfy
$\mathcal{A}\left(  i\right)  $\textquotedblright. I prefer the first wording,
because it makes the most sense in constructive logic.
\par
\textbf{Caution:} Do not confuse the words \textquotedblleft all but finitely
many $i\in I$\textquotedblright\ in this definition with the words
\textquotedblleft infinitely many $i\in I$\textquotedblright. For instance, it
is true that $n$ is even for infinitely many $n\in\mathbb{Z}$, but it is not
true that $n$ is even for all but finitely many $n\in\mathbb{Z}$. Conversely,
it is true that $n>1$ for all but finitely many $n\in\left\{  1,2\right\}  $
(because the only $n\in\left\{  1,2\right\}  $ which does not satisfy $n>1$ is
$1$), but it is not true that $n>1$ for infinitely many $n\in\left\{
1,2\right\}  $ (because there are no infinitely many $n\in\left\{
1,2\right\}  $ to begin with).
\par
You will encounter the \textquotedblleft all but finitely
many\textquotedblright\ formulation often in abstract algebra. (Some people
abbreviate it as \textquotedblleft almost all\textquotedblright, but this
abbreviation means other things as well.)} Thus, for a permutation $\sigma\in
S_{\infty}$, we have the following equivalence of statements:%
\begin{align*}
&  \ \left(  \sigma\left(  i\right)  =i\text{ for all but finitely many }%
i\in\left\{  1,2,3,\ldots\right\}  \right) \\
&  \Longleftrightarrow\ \left(  \text{there exists some finite subset }J\text{
of }\left\{  1,2,3,\ldots\right\}  \text{ such that}\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{every }i\in\left\{  1,2,3,\ldots\right\}
\setminus J\text{ satisfies }\sigma\left(  i\right)  =i\right) \\
&  \Longleftrightarrow\ \left(  \text{there exists some finite subset }J\text{
of }\left\{  1,2,3,\ldots\right\}  \text{ such that}\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{the only }i\in\left\{  1,2,3,\ldots
\right\}  \text{ that satisfy }\sigma\left(  i\right)  \neq i\text{ are
elements of }J\right) \\
&  \Longleftrightarrow\ \left(  \text{the set of all }i\in\left\{
1,2,3,\ldots\right\}  \text{ that satisfy }\sigma\left(  i\right)  \neq
i\text{ is}\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{contained in some finite subset }J\text{
of }\left\{  1,2,3,\ldots\right\}  \right) \\
&  \Longleftrightarrow\ \left(  \text{there are only finitely many }%
i\in\left\{  1,2,3,\ldots\right\}  \text{ that satisfy }\sigma\left(
i\right)  \neq i\right)  .
\end{align*}
Hence, (\ref{eq.S(infty).def}) rewrites as follows:%
\[
S_{\left(  \infty\right)  }=\left\{  \sigma\in S_{\infty}\ \mid\ \text{there
are only finitely many }i\in\left\{  1,2,3,\ldots\right\}  \text{ that satisfy
}\sigma\left(  i\right)  \neq i\right\}  .
\]


\begin{example}
Here is an example of a permutation which is in $S_{\infty}$ but not in
$S_{\left(  \infty\right)  }$: Let $\tau$ be the permutation of $\left\{
1,2,3,\ldots\right\}  $ given by%
\[
\left(  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\tau\left(  3\right)
,\tau\left(  4\right)  ,\tau\left(  5\right)  ,\tau\left(  6\right)
,\ldots\right)  =\left(  2,1,4,3,6,5,\ldots\right)  .
\]
(It adds $1$ to every odd positive integer, and subtracts $1$ from every even
positive integer.) Then, $\tau\in S_{\infty}$ but $\tau\notin S_{\left(
\infty\right)  }$.
\end{example}

On the other hand, let us show some examples of permutations in $S_{\left(
\infty\right)  }$. For each $i\in\left\{  1,2,3,\ldots\right\}  $, let $s_{i}$
be the permutation in $S_{\infty}$ that switches $i$ with $i+1$ but leaves all
other numbers unchanged. (This is similar to the permutation $s_{i}$ in
$S_{n}$ that was defined earlier. We have taken the liberty to re-use the name
$s_{i}$, hoping that no confusion will arise.)

Again, we have $s_{i}^{2}=\operatorname*{id}$ for every $i\in\left\{
1,2,3,\ldots\right\}  $ (where $\alpha^{2}$ means $\alpha\circ\alpha$ for any
$\alpha\in S_{\infty}$).

\begin{proposition}
\label{prop.S(infty).si}We have $s_{k}\in S_{\left(  \infty\right)  }$ for
every $k\in\left\{  1,2,3,\ldots\right\}  $.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.S(infty).si}.]Let $k\in\left\{  1,2,3,\ldots
\right\}  $. The permutation $s_{k}$ has been defined as the permutation in
$S_{\infty}$ that switches $k$ with $k+1$ but leaves all other numbers
unchanged. In other words, it satisfies $s_{k}\left(  k\right)  =k+1$,
$s_{k}\left(  k+1\right)  =k$ and%
\begin{equation}
s_{k}\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,3,\ldots\right\}  \text{ such that }i\notin\left\{  k,k+1\right\}  .
\label{pf.prop.S(infty).si.1}%
\end{equation}


Now, every $i\in\left\{  1,2,3,\ldots\right\}  \setminus\left\{
k,k+1\right\}  $ satisfies $s_{k}\left(  i\right)  =i$%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}
\setminus\left\{  k,k+1\right\}  $. Thus, $i\in\left\{  1,2,3,\ldots\right\}
$ and $i\notin\left\{  k,k+1\right\}  $. Hence, (\ref{pf.prop.S(infty).si.1})
shows that $s_{k}\left(  i\right)  =i$, qed.}. Hence, there exists some finite
subset $J$ of $\left\{  1,2,3,\ldots\right\}  $ such that every $i\in\left\{
1,2,3,\ldots\right\}  \setminus J$ satisfies $s_{k}\left(  i\right)  =i$
(namely, $J=\left\{  k,k+1\right\}  $). In other words, $s_{k}\left(
i\right)  =i$ for all but finitely many $i\in\left\{  1,2,3,\ldots\right\}  $.

Thus, $s_{k}$ is an element of $S_{\infty}$ satisfying $s_{k}\left(  i\right)
=i$ for all but finitely many $i\in\left\{  1,2,3,\ldots\right\}  $. Hence,%
\[
s_{k}\in\left\{  \sigma\in S_{\infty}\ \mid\ \sigma\left(  i\right)  =i\text{
for all but finitely many }i\in\left\{  1,2,3,\ldots\right\}  \right\}
=S_{\left(  \infty\right)  }.
\]
This proves Proposition \ref{prop.S(infty).si}.
\end{proof}

Permutations can be composed and inverted, leading to new permutations. Let us
first see that the same is true for elements of $S_{\left(  \infty\right)  }$:

\begin{proposition}
\label{prop.S(infty).group}\textbf{(a)} The identity permutation
$\operatorname*{id}\in S_{\infty}$ of $\left\{  1,2,3,\ldots\right\}  $
satisfies $\operatorname*{id}\in S_{\left(  \infty\right)  }$.

\textbf{(b)} For every $\sigma\in S_{\left(  \infty\right)  }$ and $\tau\in
S_{\left(  \infty\right)  }$, we have $\sigma\circ\tau\in S_{\left(
\infty\right)  }$.

\textbf{(c)} For every $\sigma\in S_{\left(  \infty\right)  }$, we have
$\sigma^{-1}\in S_{\left(  \infty\right)  }$.
\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).group}.]We have defined $S_{\left(
\infty\right)  }$ as the set of all $\sigma\in S_{\infty}$ such that
$\sigma\left(  i\right)  =i$ for all but finitely many $i\in\left\{
1,2,3,\ldots\right\}  $. In other words, $S_{\left(  \infty\right)  }$ is the
set of all $\sigma\in S_{\infty}$ such that there exists a finite subset $K$
of $\left\{  1,2,3,\ldots\right\}  $ such that $\left(  \text{every }%
i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies }\sigma\left(
i\right)  =i\right)  $. As a consequence, we have the following two facts:

\begin{itemize}
\item If $K$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and if
$\gamma\in S_{\infty}$ is a permutation such that%
\begin{equation}
\left(  \text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{
satisfies }\gamma\left(  i\right)  =i\right)  ,
\label{pf.prop.S(infty).group.short.lem.hyp}%
\end{equation}
then%
\begin{equation}
\gamma\in S_{\left(  \infty\right)  }.
\label{pf.prop.S(infty).group.short.lem}%
\end{equation}


\item If $\gamma\in S_{\left(  \infty\right)  }$, then
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists some finite subset }K\text{ of }\left\{  1,2,3,\ldots
\right\} \\
\text{such that every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{
satisfies }\gamma\left(  i\right)  =i
\end{array}
\right)  . \label{pf.prop.S(infty).group.short.lem2}%
\end{equation}

\end{itemize}

We can now step to the actual proof of Proposition \ref{prop.S(infty).group}.

\textbf{(a)} Every $i\in\left\{  1,2,3,\ldots\right\}  \setminus\varnothing$
satisfies $\operatorname*{id}\left(  i\right)  =i$. Thus,
(\ref{pf.prop.S(infty).group.short.lem}) (applied to $K=\varnothing$ and
$\gamma=\operatorname*{id}$) yields $\operatorname*{id}\in S_{\left(
\infty\right)  }$. This proves Proposition \ref{prop.S(infty).group}
\textbf{(a)}.

\textbf{(b)} Let $\sigma\in S_{\left(  \infty\right)  }$ and $\tau\in
S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.short.lem2}) (applied to $\gamma=\sigma$),
we conclude that there exists some finite subset $K$ of $\left\{
1,2,3,\ldots\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}
\setminus K$ satisfies $\sigma\left(  i\right)  =i$. Let us denote this $K$ by
$J_{1}$. Thus, $J_{1}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}
$, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{1}\text{
satisfies }\sigma\left(  i\right)  =i.
\label{pf.prop.S(infty).group.short.b.1}%
\end{equation}


From (\ref{pf.prop.S(infty).group.short.lem2}) (applied to $\gamma=\tau$), we
conclude that there exists some finite subset $K$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus K$
satisfies $\tau\left(  i\right)  =i$. Let us denote this $K$ by $J_{2}$. Thus,
$J_{2}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{2}\text{
satisfies }\tau\left(  i\right)  =i. \label{pf.prop.S(infty).group.short.b.2}%
\end{equation}


The sets $J_{1}$ and $J_{2}$ are finite. Hence, their union $J_{1}\cup J_{2}$
is finite. Moreover,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus\left(  J_{1}\cup
J_{2}\right)  \text{ satisfies }\left(  \sigma\circ\tau\right)  \left(
i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}
\setminus\left(  J_{1}\cup J_{2}\right)  $. Thus, $i\in\left\{  1,2,3,\ldots
\right\}  $ and $i\notin J_{1}\cup J_{2}$.
\par
We have $i\notin J_{1}\cup J_{2}$ and thus $i\notin J_{1}$ (since
$J_{1}\subseteq J_{1}\cup J_{2}$). Hence, $i\in\left\{  1,2,3,\ldots\right\}
\setminus J_{1}$. Similarly, $i\in\left\{  1,2,3,\ldots\right\}  \setminus
J_{2}$. Thus, (\ref{pf.prop.S(infty).group.short.b.2}) yields $\tau\left(
i\right)  =i$. Hence, $\left(  \sigma\circ\tau\right)  \left(  i\right)
=\sigma\left(  \underbrace{\tau\left(  i\right)  }_{=i}\right)  =\sigma\left(
i\right)  =i$ (by (\ref{pf.prop.S(infty).group.short.b.1})), qed.}. Therefore,
(\ref{pf.prop.S(infty).group.short.lem}) (applied to $K=J_{1}\cup J_{2}$ and
$\gamma=\sigma\circ\tau$) yields $\sigma\circ\tau\in S_{\left(  \infty\right)
}$. This proves Proposition \ref{prop.S(infty).group} \textbf{(b)}.

\textbf{(c)} Let $\sigma\in S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.short.lem2}) (applied to $\gamma=\sigma$),
we conclude that there exists some finite subset $K$ of $\left\{
1,2,3,\ldots\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}
\setminus K$ satisfies $\sigma\left(  i\right)  =i$. Consider this $K$. Thus,
$K$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).group.short.c.3}%
\end{equation}


Now,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies
}\sigma^{-1}\left(  i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}  \setminus
K$. Thus, $\sigma\left(  i\right)  =i$ (according to
(\ref{pf.prop.S(infty).group.short.c.3})), so that $\sigma^{-1}\left(
i\right)  =i$, qed.}. Therefore, (\ref{pf.prop.S(infty).group.short.lem})
(applied to $\gamma=\sigma^{-1}$) yields $\sigma^{-1}\in S_{\left(
\infty\right)  }$. This proves Proposition \ref{prop.S(infty).group}
\textbf{(c)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).group}.]Let us record the following facts:

\begin{itemize}
\item If $K$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and if
$\gamma\in S_{\infty}$ is a permutation such that%
\begin{equation}
\left(  \text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{
satisfies }\gamma\left(  i\right)  =i\right)  ,
\label{pf.prop.S(infty).group.lem.hyp}%
\end{equation}
then%
\begin{equation}
\gamma\in S_{\left(  \infty\right)  }. \label{pf.prop.S(infty).group.lem}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.S(infty).group.lem}):} Let $K$ be a
finite subset of $\left\{  1,2,3,\ldots\right\}  $, and let $\gamma\in
S_{\infty}$ be a permutation such that (\ref{pf.prop.S(infty).group.lem.hyp})
holds. We need to prove that $\gamma\in S_{\left(  \infty\right)  }$.
\par
Every $i\in\left\{  1,2,3,\ldots\right\}  \setminus K$ satisfies
$\gamma\left(  i\right)  =i$ (since (\ref{pf.prop.S(infty).group.lem.hyp})
holds). Thus, there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\gamma\left(  i\right)  =i$ (namely, $J=K$). In other words,
$\gamma\left(  i\right)  =i$ for all but finitely many $i\in\left\{
1,2,3,\ldots\right\}  $.
\par
Thus, $\gamma$ is an element of $S_{\infty}$ satisfying $\gamma\left(
i\right)  =i$ for all but finitely many $i\in\left\{  1,2,3,\ldots\right\}  $.
Hence,%
\[
\gamma\in\left\{  \sigma\in S_{\infty}\ \mid\ \sigma\left(  i\right)  =i\text{
for all but finitely many }i\in\left\{  1,2,3,\ldots\right\}  \right\}
=S_{\left(  \infty\right)  }.
\]
This proves (\ref{pf.prop.S(infty).group.lem}).}

\item If $\gamma\in S_{\left(  \infty\right)  }$, then
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists some finite subset }J\text{ of }\left\{  1,2,3,\ldots
\right\} \\
\text{such that every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{
satisfies }\gamma\left(  i\right)  =i
\end{array}
\right)  \label{pf.prop.S(infty).group.lem2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.S(infty).group.lem2}):} Let
$\gamma\in S_{\left(  \infty\right)  }$. Then,
\[
\gamma\in S_{\left(  \infty\right)  }=\left\{  \sigma\in S_{\infty}%
\ \mid\ \sigma\left(  i\right)  =i\text{ for all but finitely many }%
i\in\left\{  1,2,3,\ldots\right\}  \right\}  .
\]
In other words, $\gamma$ is an element of $S_{\infty}$ such that
$\gamma\left(  i\right)  =i$ for all but finitely many $i\in\left\{
1,2,3,\ldots\right\}  $.
\par
Now, we know that $\gamma\left(  i\right)  =i$ for all but finitely many
$i\in\left\{  1,2,3,\ldots\right\}  $. In other words, there exists some
finite subset $J$ of $\left\{  1,2,3,\ldots\right\}  $ such that every
$i\in\left\{  1,2,3,\ldots\right\}  \setminus J$ satisfies $\gamma\left(
i\right)  =i$. This proves (\ref{pf.prop.S(infty).group.lem2}).}.
\end{itemize}

We can now step to the actual proof of Proposition \ref{prop.S(infty).group}.

\textbf{(a)} Every $i\in\left\{  1,2,3,\ldots\right\}  \setminus\varnothing$
satisfies $\operatorname*{id}\left(  i\right)  =i$. Thus,
(\ref{pf.prop.S(infty).group.lem}) (applied to $K=\varnothing$ and
$\gamma=\operatorname*{id}$) yields $\operatorname*{id}\in S_{\left(
\infty\right)  }$. This proves Proposition \ref{prop.S(infty).group}
\textbf{(a)}.

\textbf{(b)} Let $\sigma\in S_{\left(  \infty\right)  }$ and $\tau\in
S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.lem2}) (applied to $\gamma=\sigma$), we
conclude that there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\sigma\left(  i\right)  =i$. Let us denote this $J$ by $J_{1}$.
Thus, $J_{1}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{1}\text{
satisfies }\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).group.b.1}%
\end{equation}


From (\ref{pf.prop.S(infty).group.lem2}) (applied to $\gamma=\tau$), we
conclude that there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\tau\left(  i\right)  =i$. Let us denote this $J$ by $J_{2}$. Thus,
$J_{2}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{2}\text{
satisfies }\tau\left(  i\right)  =i. \label{pf.prop.S(infty).group.b.2}%
\end{equation}


The sets $J_{1}$ and $J_{2}$ are finite. Hence, their union $J_{1}\cup J_{2}$
is finite. Moreover,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus\left(  J_{1}\cup
J_{2}\right)  \text{ satisfies }\left(  \sigma\circ\tau\right)  \left(
i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}
\setminus\left(  J_{1}\cup J_{2}\right)  $. Thus, $i\in\left\{  1,2,3,\ldots
\right\}  $ and $i\notin J_{1}\cup J_{2}$.
\par
If we had $i\in J_{1}$, then we would have $i\in J_{1}\subseteq J_{1}\cup
J_{2}$, which would contradict $i\notin J_{1}\cup J_{2}$. Thus, we cannot have
$i\in J_{1}$. In other words, we have $i\notin J_{1}$. Hence, $i\in\left\{
1,2,3,\ldots\right\}  \setminus J_{1}$. Similarly, $i\in\left\{
1,2,3,\ldots\right\}  \setminus J_{2}$. Thus,
(\ref{pf.prop.S(infty).group.b.2}) yields $\tau\left(  i\right)  =i$. Hence,
$\left(  \sigma\circ\tau\right)  \left(  i\right)  =\sigma\left(
\underbrace{\tau\left(  i\right)  }_{=i}\right)  =\sigma\left(  i\right)  =i$
(by (\ref{pf.prop.S(infty).group.b.1})), qed.}. Therefore,
(\ref{pf.prop.S(infty).group.lem}) (applied to $K=J_{1}\cup J_{2}$ and
$\gamma=\sigma\circ\tau$) yields $\sigma\circ\tau\in S_{\left(  \infty\right)
}$. This proves Proposition \ref{prop.S(infty).group} \textbf{(b)}.

\textbf{(c)} Let $\sigma\in S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.lem2}) (applied to $\gamma=\sigma$), we
conclude that there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\sigma\left(  i\right)  =i$. Consider this $J$. Thus, $J$ is a
finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).group.c.3}%
\end{equation}


Now,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{ satisfies
}\sigma^{-1}\left(  i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}  \setminus
J$. Thus, $\sigma\left(  i\right)  =i$ (according to
(\ref{pf.prop.S(infty).group.c.3})), so that $\sigma^{-1}\left(  i\right)
=i$, qed.}. Therefore, (\ref{pf.prop.S(infty).group.lem}) (applied to $K=J$
and $\gamma=\sigma^{-1}$) yields $\sigma^{-1}\in S_{\left(  \infty\right)  }$.
This proves Proposition \ref{prop.S(infty).group} \textbf{(c)}.
\end{proof}
\end{verlong}

In the language of group theorists, Proposition \ref{prop.S(infty).group} show
that $S_{\left(  \infty\right)  }$ is a subgroup of the group $S_{\infty}$.
The elements of $S_{\left(  \infty\right)  }$ are called the \textit{finitary
permutations of }$\left\{  1,2,3,\ldots\right\}  $, and $S_{\left(
\infty\right)  }$ is called the \textit{finitary symmetric group of }$\left\{
1,2,3,\ldots\right\}  $.

We now have the following analogue of Exercise \ref{exe.ps2.2.4} (without its
part \textbf{(c)}):

\begin{exercise}
\label{exe.ps2.2.4'}\textbf{(a)} Show that $s_{i}\circ s_{i+1}\circ
s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}$ for all $i\in\left\{  1,2,3,\ldots
\right\}  $.

\textbf{(b)} Show that every permutation $\sigma\in S_{\left(  \infty\right)
}$ can be written as a composition of several permutations of the form $s_{k}$
(with $k\in\left\{  1,2,3,\ldots\right\}  $).
\end{exercise}

\begin{remark}
In the language of group theory, the statement of Exercise \ref{exe.ps2.2.4'}
\textbf{(b)} says (or, more precisely, yields) that the permutations
$s_{1},s_{2},s_{3},\ldots$ generate the group $S_{\left(  \infty\right)  }$.
\end{remark}

If $\sigma\in S_{\infty}$ is a permutation, then an \textit{inversion} of
$\sigma$ means a pair $\left(  i,j\right)  $ of integers satisfying $1\leq
i<j$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. This definition
of an inversion is, of course, analogous to the definition of an inversion of
a $\sigma\in S_{n}$; thus we could try to define the length of a $\sigma\in
S_{\infty}$. However, here we run into troubles: A permutation $\sigma\in
S_{\infty}$ might have infinitely many inversions!

It is here that we really need to restrict ourselves to $S_{\left(
\infty\right)  }$. This indeed helps:

\begin{proposition}
\label{prop.S(infty).inversions}Let $\sigma\in S_{\left(  \infty\right)  }$. Then:

\textbf{(a)} There exists some $N\in\left\{  1,2,3,\ldots\right\}  $ such that
every integer $i>N$ satisfies $\sigma\left(  i\right)  =i$.

\textbf{(b)} There are only finitely many inversions of $\sigma$.
\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).inversions}.]\textbf{(a)} We can
apply (\ref{pf.prop.S(infty).group.short.lem2}) to $\gamma=\sigma$. As a
consequence, we obtain that there exists some finite subset $K$ of $\left\{
1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).inversions.short.a.J}%
\end{equation}
Consider this $K$.

The set $K$ is finite. Hence, the set $K\cup\left\{  1\right\}  $ is finite;
this set is also nonempty (since it contains $1$) and a subset of $\left\{
1,2,3,\ldots\right\}  $. Therefore, this set $K\cup\left\{  1\right\}  $ has a
greatest element (since every finite nonempty subset of $\left\{
1,2,3,\ldots\right\}  $ has a greatest element). Let $n$ be this greatest
element. Clearly, $n\in K\cup\left\{  1\right\}  \subseteq\left\{
1,2,3,\ldots\right\}  $, so that $n>0$.

Every $j\in K\cup\left\{  1\right\}  $ satisfies
\begin{equation}
j\leq n \label{pf.prop.S(infty).inversions.short.a.1}%
\end{equation}
(since $n$ is the greatest element of $K\cup\left\{  1\right\}  $). Now, let
$i$ be an integer such that $i>n$. Then, $i>n>0$, so that $i$ is a positive
integer. If we had $i\in K$, then we would have $i\in K\subseteq K\cup\left\{
1\right\}  $ and thus $i\leq n$ (by
(\ref{pf.prop.S(infty).inversions.short.a.1}), applied to $j=i$), which would
contradict $i>n$. Hence, we cannot have $i\in K$. We thus have $i\notin K$.
Since $i\in\left\{  1,2,3,\ldots\right\}  $, this shows that $i\in\left\{
1,2,3,\ldots\right\}  \setminus K$. Thus, $\sigma\left(  i\right)  =i$ (by
(\ref{pf.prop.S(infty).inversions.short.a.J})).

Let us now forget that we fixed $i$. We thus have shown that every integer
$i>n$ satisfies $\sigma\left(  i\right)  =i$. Hence, Proposition
\ref{prop.S(infty).inversions} \textbf{(a)} holds (we can take $N=n$).

\textbf{(b)} Proposition \ref{prop.S(infty).inversions} \textbf{(a)} shows
that there exists some $N\in\left\{  1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every integer }i>N\text{ satisfies }\sigma\left(  i\right)  =i.
\label{pf.prop.S(infty).inversions.short.b.N}%
\end{equation}
Consider such an $N$. We shall now show that
\[
\text{every inversion of }\sigma\text{ is an element of }\left\{
1,2,\ldots,N\right\}  ^{2}.
\]


In fact, let $c$ be an inversion of $\sigma$. We will show that $c$ is an
element of $\left\{  1,2,\ldots,N\right\}  ^{2}$.

We know that $c$ is an inversion of $\sigma$. In other words, $c$ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j$ and $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright). Consider this
$\left(  i,j\right)  $. We then have $i\leq N$\ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.short.b.N}) shows that $\sigma\left(
i\right)  =i$. Also, $i<j$, so that $j>i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.short.b.N}) (applied to $j$ instead of $i$)
shows that $\sigma\left(  j\right)  =j$. Thus, $\sigma\left(  i\right)
=i<j=\sigma\left(  j\right)  $. This contradicts $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. This contradiction shows that our assumption was
wrong, qed.} and $j\leq N$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, $j>N$. Hence, (\ref{pf.prop.S(infty).inversions.short.b.N})
(applied to $j$ instead of $i$) shows that $\sigma\left(  j\right)  =j$. Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  =j>N$. Therefore,
(\ref{pf.prop.S(infty).inversions.short.b.N}) (applied to $\sigma\left(
i\right)  $ instead of $i$) yields $\sigma\left(  \sigma\left(  i\right)
\right)  =\sigma\left(  i\right)  $. But $\sigma$ is a permutation, and thus
an injective map. Hence, from $\sigma\left(  \sigma\left(  i\right)  \right)
=\sigma\left(  i\right)  $, we obtain $\sigma\left(  i\right)  =i$. Thus,
$\sigma\left(  i\right)  =i<j=\sigma\left(  j\right)  $. This contradicts
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. This contradiction shows
that our assumption was wrong, qed.}. Consequently, $\left(  i,j\right)
\in\left\{  1,2,\ldots,N\right\}  ^{2}$. Hence, $c=\left(  i,j\right)
\in\left\{  1,2,\ldots,N\right\}  ^{2}$.

Now, let us forget that we fixed $c$. We thus have shown that if $c$ is an
inversion of $\sigma$, then $c$ is an element of $\left\{  1,2,\ldots
,N\right\}  ^{2}$. In other words, every inversion of $\sigma$ is an element
of $\left\{  1,2,\ldots,N\right\}  ^{2}$. Thus, there are only finitely many
inversions of $\sigma$ (since there are only finitely many elements of
$\left\{  1,2,\ldots,N\right\}  ^{2}$). Proposition
\ref{prop.S(infty).inversions} \textbf{(b)} is thus proven.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).inversions}.]\textbf{(a)} We can
apply (\ref{pf.prop.S(infty).group.lem2}) to $\gamma=\sigma$. As a
consequence, we obtain that there exists some finite subset $J$ of $\left\{
1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).inversions.a.J}%
\end{equation}
Consider this $J$.

The set $J$ is finite. Hence, the set $J\cup\left\{  1\right\}  $ is finite;
this set is also nonempty (since it contains $1$) and a subset of $\left\{
1,2,3,\ldots\right\}  $. Therefore, this set $J\cup\left\{  1\right\}  $ has a
greatest element (since every finite nonempty subset of $\left\{
1,2,3,\ldots\right\}  $ has a greatest element). Let $n$ be this greatest
element. Clearly, $n\in J\cup\left\{  1\right\}  \subseteq\left\{
1,2,3,\ldots\right\}  $, so that $n>0$.

Every $j\in J\cup\left\{  1\right\}  $ satisfies
\begin{equation}
j\leq n \label{pf.prop.S(infty).inversions.a.1}%
\end{equation}
(since $n$ is the greatest element of $J\cup\left\{  1\right\}  $). Now, let
$i$ be an integer such that $i>n$. Then, $i>n>0$, so that $i$ is a positive
integer. If we had $i\in J$, then we would have $i\in J\subseteq J\cup\left\{
1\right\}  $ and thus $i\leq n$ (by (\ref{pf.prop.S(infty).inversions.a.1}),
applied to $j=i$), which would contradict $i>n$. Hence, we cannot have $i\in
J$. We thus have $i\notin J$. Since $i\in\left\{  1,2,3,\ldots\right\}  $,
this shows that $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$. Thus,
$\sigma\left(  i\right)  =i$ (by (\ref{pf.prop.S(infty).inversions.a.J})).

Let us now forget that we fixed $i$. We thus have shown that every integer
$i>n$ satisfies $\sigma\left(  i\right)  =i$. Hence, there exists some
$N\in\left\{  1,2,3,\ldots\right\}  $ such that every integer $i>N$ satisfies
$\sigma\left(  i\right)  =i$ (namely, $N=n$). Proposition
\ref{prop.S(infty).inversions} \textbf{(a)} is thus proven.

\textbf{(b)} Proposition \ref{prop.S(infty).inversions} \textbf{(a)} shows
that there exists some $N\in\left\{  1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every integer }i>N\text{ satisfies }\sigma\left(  i\right)  =i.
\label{pf.prop.S(infty).inversions.b.N}%
\end{equation}
Consider such an $N$. We shall now show that
\[
\text{every inversion of }\sigma\text{ is an element of }\left\{
1,2,\ldots,N\right\}  ^{2}.
\]


In fact, let $c$ be an inversion of $\sigma$. We will show that $c$ is an
element of $\left\{  1,2,\ldots,N\right\}  ^{2}$.

We know that $c$ is an inversion of $\sigma$. In other words, $c$ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j$ and $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright). Consider this
$\left(  i,j\right)  $. We then have $i\leq N$\ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.b.N}) shows that $\sigma\left(  i\right)
=i$. Also, $i<j$, so that $j>i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.b.N}) (applied to $j$ instead of $i$) shows
that $\sigma\left(  j\right)  =j$. Thus, $\sigma\left(  i\right)
=i<j=\sigma\left(  j\right)  $. This contradicts $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. This contradiction shows that our assumption was
wrong, qed.} and $j\leq N$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, $j>N$. Hence, (\ref{pf.prop.S(infty).inversions.b.N}) (applied
to $j$ instead of $i$) shows that $\sigma\left(  j\right)  =j$. Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  =j>N$. Therefore,
(\ref{pf.prop.S(infty).inversions.b.N}) (applied to $\sigma\left(  i\right)  $
instead of $i$) yields $\sigma\left(  \sigma\left(  i\right)  \right)
=\sigma\left(  i\right)  $. But $\sigma$ is a permutation, and thus an
injective map. Hence, from $\sigma\left(  \sigma\left(  i\right)  \right)
=\sigma\left(  i\right)  $, we obtain $\sigma\left(  i\right)  =i$. Thus,
$\sigma\left(  i\right)  =i<j=\sigma\left(  j\right)  $. This contradicts
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. This contradiction shows
that our assumption was wrong, qed.}. From $1\leq i\leq N$, we obtain
$i\in\left\{  1,2,\ldots,N\right\}  $. From $1\leq j\leq N$, we obtain
$j\in\left\{  1,2,\ldots,N\right\}  $. Combining $i\in\left\{  1,2,\ldots
,N\right\}  $ with $j\in\left\{  1,2,\ldots,N\right\}  $, we obtain $\left(
i,j\right)  \in\left\{  1,2,\ldots,N\right\}  ^{2}$. Hence, $c=\left(
i,j\right)  \in\left\{  1,2,\ldots,N\right\}  ^{2}$. In other words, $c$ is an
element of $\left\{  1,2,\ldots,N\right\}  ^{2}$.

Now, let us forget that we fixed $c$. We thus have shown that if $c$ is an
inversion of $\sigma$, then $c$ is an element of $\left\{  1,2,\ldots
,N\right\}  ^{2}$. In other words, every inversion of $\sigma$ is an element
of $\left\{  1,2,\ldots,N\right\}  ^{2}$. Thus, there are only finitely many
inversions of $\sigma$ (since there are only finitely many elements of
$\left\{  1,2,\ldots,N\right\}  ^{2}$). Proposition
\ref{prop.S(infty).inversions} \textbf{(b)} is thus proven.
\end{proof}
\end{verlong}

Actually, Proposition \ref{prop.S(infty).inversions} \textbf{(b)} has a
converse: If a permutation $\sigma\in S_{\infty}$ has only finitely many
inversions, then $\sigma$ belongs to $S_{\left(  \infty\right)  }$. This is
easy to prove; but we won't use this.

If $\sigma\in S_{\left(  \infty\right)  }$ is a permutation, then the
\textit{length} of $\sigma$ means the number of inversions of $\sigma$. This
is well-defined, because there are only finitely many inversions of $\sigma$
(by Proposition \ref{prop.S(infty).inversions} \textbf{(b)}). The length of
$\sigma$ is denoted by $\ell\left(  \sigma\right)  $; it is a nonnegative
integer. The only permutation having length $0$ is the identity permutation
$\operatorname*{id}\in S_{\infty}$.

We have the following analogue of Exercise \ref{exe.ps2.2.5}:

\begin{exercise}
\label{exe.ps2.2.5'}\textbf{(a)} Show that every permutation $\sigma\in
S_{\left(  \infty\right)  }$ and every $k\in\left\{  1,2,3,\ldots\right\}  $
satisfy%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  \label{eq.exe.2.5'.a.1}%
\end{equation}
and%
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  <\sigma^{-1}\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  >\sigma^{-1}\left(  k+1\right)
\end{array}
\right.  . \label{eq.exe.2.5'.a.2}%
\end{equation}


\textbf{(b)} Show that any two permutations $\sigma$ and $\tau$ in $S_{\left(
\infty\right)  }$ satisfy $\ell\left(  \sigma\circ\tau\right)  \equiv
\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  \operatorname{mod}2$.

\textbf{(c)} Show that any two permutations $\sigma$ and $\tau$ in $S_{\left(
\infty\right)  }$ satisfy $\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $.

\textbf{(d)} If $\sigma\in S_{\left(  \infty\right)  }$ is a permutation
satisfying $\sigma\left(  1\right)  \leq\sigma\left(  2\right)  \leq
\sigma\left(  3\right)  \leq\cdots$, then show that $\sigma=\operatorname*{id}%
$.

\textbf{(e)} Let $\sigma\in S_{\left(  \infty\right)  }$. Show that $\sigma$
can be written as a composition of $\ell\left(  \sigma\right)  $ permutations
of the form $s_{k}$ (with $k\in\left\{  1,2,3,\ldots\right\}  $).

\textbf{(f)} Let $\sigma\in S_{\left(  \infty\right)  }$. Then, show that
$\ell\left(  \sigma\right)  =\ell\left(  \sigma^{-1}\right)  $.

\textbf{(g)} Let $\sigma\in S_{\left(  \infty\right)  }$. Show that
$\ell\left(  \sigma\right)  $ is the smallest $N\in\mathbb{N}$ such that
$\sigma$ can be written as a composition of $N$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,3,\ldots\right\}  $).
\end{exercise}

We also have an analogue of Exercise \ref{exe.ps2.2.6}:

\begin{exercise}
\label{exe.ps2.2.6'}Let $\sigma\in S_{\left(  \infty\right)  }$. In Exercise
\ref{exe.ps2.2.4'} \textbf{(b)}, we have seen that $\sigma$ can be written as
a composition of several permutations of the form $s_{k}$ (with $k\in\left\{
1,2,3,\ldots\right\}  $). Usually there will be several ways to do so (for
instance, $\operatorname*{id}=s_{1}\circ s_{1}=s_{2}\circ s_{2}=s_{3}\circ
s_{3}=\cdots$). Show that, whichever of these ways we take, the number of
permutations composed will be congruent to $\ell\left(  \sigma\right)  $
modulo $2$.
\end{exercise}

Having defined the length of a permutation $\sigma\in S_{\left(
\infty\right)  }$, we can now define the sign of such a permutation. Again, we
mimic the definition of the sign of a $\sigma\in S_{n}$:

\begin{definition}
\label{def.perm.sign'}We define the \textit{sign} of a permutation $\sigma\in
S_{\left(  \infty\right)  }$ as the integer $\left(  -1\right)  ^{\ell\left(
\sigma\right)  }$. We denote this sign by $\left(  -1\right)  ^{\sigma}$ or
$\operatorname*{sign}\sigma$ or $\operatorname*{sgn}\sigma$. We say that a
permutation $\sigma$ is \textit{even} if its sign is $1$ (that is, if
$\ell\left(  \sigma\right)  $ is even), and \textit{odd} if its sign is $-1$
(that is, if $\ell\left(  \sigma\right)  $ is odd).
\end{definition}

Signs of permutations have the following properties:

\begin{itemize}
\item The sign of the identity permutation $\operatorname*{id}\in S_{\left(
\infty\right)  }$ is $\left(  -1\right)  ^{\operatorname*{id}}=1$. In other
words, $\operatorname*{id}\in S_{\left(  \infty\right)  }$ is even.

\item For every $k\in\left\{  1,2,3,\ldots\right\}  $, the sign of the
permutation $s_{k}\in S_{\left(  \infty\right)  }$ is $\left(  -1\right)
^{s_{k}}=-1$.

\item If $\sigma$ and $\tau$ are two permutations in $S_{\left(
\infty\right)  }$, then $\left(  -1\right)  ^{\sigma\circ\tau}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$.

\item If $\sigma\in S_{\left(  \infty\right)  }$, then $\left(  -1\right)
^{\sigma^{-1}}=\left(  -1\right)  ^{\sigma}$.
\end{itemize}

The proofs of all these properties are analogous to the proofs of the
analogous properties for permutations in $S_{n}$.

\begin{remark}
We have defined the sign of a permutation $\sigma\in S_{\left(  \infty\right)
}$. No such notion exists for permutations $\sigma\in S_{\infty}$. In fact,
one can show that if an element $\lambda_{\sigma}$ of $\left\{  1,-1\right\}
$ is chosen for each $\sigma\in S_{\infty}$ in such a way that every two
permutations $\sigma,\tau\in S_{\infty}$ satisfy $\left(  -1\right)
^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau
}$, then all of the $\lambda_{\sigma}$ are $1$. (Indeed, this follows from a
result of Oystein Ore; see
\texttt{\href{http://mathoverflow.net/questions/54371}{http://mathoverflow.net/questions/54371}%
} .)
\end{remark}

\begin{remark}
For every $n\in\mathbb{N}$ and every $\sigma\in S_{n}$, we can define a
permutation $\sigma_{\left(  \infty\right)  }\in S_{\left(  \infty\right)  }$
by setting%
\[
\sigma_{\left(  \infty\right)  }\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
\sigma\left(  i\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }i\leq n;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }i>n
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,3,\ldots\right\}
.
\]
Essentially, $\sigma_{\left(  \infty\right)  }$ is the permutation $\sigma$
extended to the set of all positive integers in the laziest possible way: It
just sends each $i>n$ to itself.

For every $n\in\mathbb{N}$, there is an injective map $\mathbf{i}_{n}%
:S_{n}\rightarrow S_{\left(  \infty\right)  }$ defined as follows:%
\[
\mathbf{i}_{n}\left(  \sigma\right)  =\sigma_{\left(  \infty\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\]
This map $\mathbf{i}_{n}$ is an example of what algebraists call a
\textit{group homomorphism}: It satisfies%
\begin{align*}
\mathbf{i}_{n}\left(  \operatorname*{id}\right)   &  =\operatorname*{id};\\
\mathbf{i}_{n}\left(  \sigma\circ\tau\right)   &  =\mathbf{i}_{n}\left(
\sigma\right)  \circ\mathbf{i}_{n}\left(  \tau\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }\sigma,\tau\in S_{n};\\
\mathbf{i}_{n}\left(  \sigma^{-1}\right)   &  =\left(  \mathbf{i}_{n}\left(
\sigma\right)  \right)  ^{-1}\ \ \ \ \ \ \ \ \ \ \text{for all }\sigma\in
S_{n}.
\end{align*}
(This is all easy to check.) Thus, we can consider the image $\mathbf{i}%
_{n}\left(  S_{n}\right)  $ of $S_{n}$ under this map as a \textquotedblleft
copy\textquotedblright\ of $S_{n}$ which is \textquotedblleft just as good as
the original\textquotedblright\ (i.e., composition in this copy behaves in the
same way as composition in the original). It is easy to characterize this copy
inside $S_{\left(  \infty\right)  }$: Namely,%
\[
\mathbf{i}_{n}\left(  S_{n}\right)  =\left\{  \sigma\in S_{\left(
\infty\right)  }\ \mid\ \sigma\left(  i\right)  =i\text{ for all }i>n\right\}
.
\]


Using Proposition \ref{prop.S(infty).inversions} \textbf{(a)}, it is easy to
check that $S_{\left(  \infty\right)  }=\bigcup_{n\in\mathbb{N}}\mathbf{i}%
_{n}\left(  S_{n}\right)  =\mathbf{i}_{0}\left(  S_{0}\right)  \cup
\mathbf{i}_{1}\left(  S_{1}\right)  \cup\mathbf{i}_{2}\left(  S_{2}\right)
\cup\cdots$. Therefore, many properties of $S_{\left(  \infty\right)  }$ can
be derived from analogous properties of $S_{n}$ for finite $n$. For example,
using this tactic, we could easily derive Exercise \ref{exe.ps2.2.5'} from
Exercise \ref{exe.ps2.2.5}, and derive Exercise \ref{exe.ps2.2.6'} from
Exercise \ref{exe.ps2.2.6}. (However, we can just as well solve Exercises
\ref{exe.ps2.2.5'} and \ref{exe.ps2.2.6'} by copying the solutions of
Exercises \ref{exe.ps2.2.5} and \ref{exe.ps2.2.6} and making the necessary
changes; this is how I solve these exercises further below.)
\end{remark}

\subsection{More on lengths of permutations}

Let us summarize some of what we have learnt about permutations. We have
defined the length $\ell\left(  \sigma\right)  $ and the inversions of a
permutation $\sigma\in S_{n}$, where $n$ is a nonnegative integer. We recall
the basic properties of these objects:

\begin{itemize}
\item For each $k\in\left\{  1,2,\ldots,n-1\right\}  $, we defined $s_{k}$ to
be the permutation in $S_{n}$ that switches $k$ with $k+1$ but leaves all
other numbers unchanged. These permutations satisfy $s_{i}^{2}%
=\operatorname*{id}$ for every $i\in\left\{  1,2,\ldots,n-1\right\}  $ and%
\begin{equation}
s_{i}\circ s_{i+1}\circ s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,\ldots,n-2\right\}
\label{eq.perms.braid3}%
\end{equation}
(according to Exercise \ref{exe.ps2.2.4} \textbf{(a)}). Also, it is easy to
check that%
\begin{equation}
s_{i}\circ s_{j}=s_{j}\circ s_{i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,\ldots,n-1\right\}  \text{ with }\left\vert i-j\right\vert
>1. \label{eq.perms.braid2}%
\end{equation}


\item An \textit{inversion} of a permutation $\sigma\in S_{n}$ means a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. The \textit{length}
$\ell\left(  \sigma\right)  $ of a permutation $\sigma\in S_{n}$ is the number
of inversions of $\sigma$.

\item Any two permutations $\sigma\in S_{n}$ and $\tau\in S_{n}$ satisfy%
\begin{equation}
\ell\left(  \sigma\circ\tau\right)  \equiv\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \operatorname{mod}2 \label{eq.perms.length.mod2}%
\end{equation}
(by Exercise \ref{exe.ps2.2.5} \textbf{(b)}) and%
\begin{equation}
\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \label{eq.perms.length.leq}%
\end{equation}
(by Exercise \ref{exe.ps2.2.5} \textbf{(c)}).

\item If $\sigma\in S_{n}$, then $\ell\left(  \sigma\right)  =\ell\left(
\sigma^{-1}\right)  $ (according to Exercise \ref{exe.ps2.2.5} \textbf{(f)}).

\item If $\sigma\in S_{n}$, then $\ell\left(  \sigma\right)  $ is the smallest
$N\in\mathbb{N}$ such that $\sigma$ can be written as a composition of $N$
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). (This follows from Exercise \ref{exe.ps2.2.5} \textbf{(g)}.)
\end{itemize}

By now, we know almost all about the $s_{k}$'s and about the lengths of
permutations that is necessary for studying determinants. (\textquotedblleft
Almost\textquotedblright\ because Exercise \ref{exe.ps4.1ab} below will also
be useful.) I shall now sketch some more advanced properties of these things,
partly as a curiosity, partly to further your intuition; none of these
properties shall be used further below.

First, here is a way to visualize lengths of permutations using graph theory:

Fix $n\in\mathbb{N}$. We define the $n$\textit{-th right Bruhat graph} to be
the (undirected) graph whose vertices are the permutations $\sigma\in S_{n}$,
and whose edges are defined as follows: Two vertices $\sigma\in S_{n}$ and
$\tau\in S_{n}$ are adjacent if and only if there exists a $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\sigma=\tau\circ s_{k}$. (This condition
is clearly symmetric in $\sigma$ and $\tau$: If $\sigma=\tau\circ s_{k}$, then
$\tau=\sigma\circ s_{k}$.) For instance, the $3$-rd right Bruhat graph looks
as follows:%
\[%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& 321 \ar@{-}[dl] \ar@{-}[dr] & \\
%312 \ar@{-}[d] & & 231 \ar@{-}[d] \\
%132 \ar@{-}[dr] & & 213 \ar@{-}[dl] \\
%& 123 &
%}}}%
%BeginExpansion
\xymatrix{
& 321 \ar@{-}[dl] \ar@{-}[dr] & \\
312 \ar@{-}[d] & & 231 \ar@{-}[d] \\
132 \ar@{-}[dr] & & 213 \ar@{-}[dl] \\
& 123 &
}%
%EndExpansion
,
\]
where we are writing permutations in one-line notation (and omitting
parentheses and commas). The $4$-th right Bruhat graph can be seen
\href{https://en.wikipedia.org/wiki/File:Symmetric_group_4;_Cayley_graph_1,2,6_%283D%29.svg}{on
Wikipedia}.\footnote{Don't omit the word \textquotedblleft
right\textquotedblright\ in \textquotedblleft right Bruhat
graph\textquotedblright; else it means a different graph with more edges.}

These graphs have lots of properties. There is a canonical way to direct their
edges: The edge between $\sigma$ and $\tau$ is directed towards the vertex
with the larger length. (The lengths of $\sigma$ and $\tau$ always differ by
$1$ if there is an edge between $\sigma$ and $\tau$.) This way, the $n$-th
right Bruhat graph is an acyclic directed graph. It therefore defines a
partially ordered set, called the \textit{right permutohedron order}%
\footnote{also known as the \textit{right weak order} or \textit{right weak
Bruhat order} (but, again, do not omit the words \textquotedblleft
right\textquotedblright\ and \textquotedblleft weak\textquotedblright)} on
$S_{n}$, whose elements are the permutations $\sigma\in S_{n}$ and whose order
relation is defined as follows: We have $\sigma\leq\tau$ if and only if there
is a directed path from $\sigma$ to $\tau$ in the directed $n$-th right Bruhat
graph. If you know
\href{https://en.wikipedia.org/wiki/Lattice_%28order%29}{the (combinatorial)
notion of a lattice}, you might notice that this right permutohedron order
\href{http://mathoverflow.net/questions/158945/why-is-the-right-permutohedron-order-aka-weak-order-on-s-n-a-lattice}{is
a lattice}.

The word \textquotedblleft permutohedron\textquotedblright\ in
\textquotedblleft permutohedron order\textquotedblright\ hints at what might
be its least expected property: The $n$-th Bruhat graph can be viewed as the
graph formed by the vertices and the edges of a certain polytope in
$n$-dimensional space $\mathbb{R}^{n}$. This polytope -- called the
$n$\textit{-th
\href{https://en.wikipedia.org/wiki/Permutohedron}{\textit{permutohedron}}%
}\footnote{Some spell it \textquotedblleft\textit{permutahedron}%
\textquotedblright\ instead. The word is of relatively recent origin (1969).}
-- is the convex hull of the points $\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $ for
$\sigma\in S_{n}$. These points are its vertices; however, its vertex $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $ corresponds to the vertex $\sigma^{-1}$ (not $\sigma$) of
the $n$-th Bruhat graph. Feel free to roam
\href{https://en.wikipedia.org/wiki/Permutohedron}{its Wikipedia page} for
other (combinatorial and geometric) curiosities.

The notion of a length fits perfectly into this whole picture. For instance,
the length $\ell\left(  \sigma\right)  $ of a permutation $\sigma$ is the
smallest length of a path from $\operatorname*{id}\in S_{n}$ to $\sigma$ on
the $n$-th right Bruhat graph (and this holds no matter whether the graph is
considered to be directed or not). For the undirected Bruhat graphs, we have
something more general:

\begin{exercise}
\label{exe.ps4.0}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ and $\tau\in
S_{n}$. Show that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest
length of a path between $\sigma$ and $\tau$ on the (undirected) $n$-th right
Bruhat graph.
\end{exercise}

(Recall that the length of a path in a graph is defined as the number of edges
along this path.)

How many permutations in $S_{n}$ have a given length? The number is not easy
to compute directly; however, its generating function is nice. Namely,%
\[
\sum_{w\in S_{n}}q^{\ell\left(  w\right)  }=\left(  1+q\right)  \left(
1+q+q^{2}\right)  \cdots\left(  1+q+q^{2}+\cdots+q^{n-1}\right)
\]
(where $q$ is an indeterminate). See \cite[Corollary 1.3.13]{Stanley-EC1} for
a proof (but notice that Stanley denotes $S_{n}$ by $\mathfrak{S}_{n}$, and
$\ell\left(  w\right)  $ by $\operatorname*{inv}\left(  w\right)  $).

\begin{remark}
Much more can be said. Let me briefly mention (without proof) two other
related results.

We can ask ourselves in what different ways a permutation can be written as a
composition of $N$ permutations of the form $s_{k}$. For instance, the
permutation $w_{0}\in S_{3}$ which sends $1$, $2$ and $3$ to $3$, $2$ and $1$,
respectively (that is, $w_{0}=\left(  3,2,1\right)  $ in one-line notation)
can be written as a product of three $s_{k}$'s in the two forms%
\begin{equation}
w_{0}=s_{1}\circ s_{2}\circ s_{1},\ \ \ \ \ \ \ \ \ \ w_{0}=s_{2}\circ
s_{1}\circ s_{2}, \label{eq.tits.example}%
\end{equation}
but can also be written as a product of five $s_{k}$'s (e.g., as $w_{0}%
=s_{1}\circ s_{2}\circ s_{1}\circ s_{2}\circ s_{2}$) or seven $s_{k}$'s or
nine $s_{k}$'s, etc. Are the different representations of $w_{0}$ related?

Clearly, the two representations in (\ref{eq.tits.example}) are connected to
each other by the equality $s_{1}\circ s_{2}\circ s_{1}=s_{2}\circ s_{1}\circ
s_{2}$, which is a particular case of (\ref{eq.perms.braid3}). Also, the
representation $w_{0}=s_{1}\circ s_{2}\circ s_{1}\circ s_{2}\circ s_{2}$
reduces to $w_{0}=s_{1}\circ s_{2}\circ s_{1}$ by \textquotedblleft
cancelling\textquotedblright\ the two adjacent $s_{2}$'s at the end (recall
that $s_{i}\circ s_{i}=s_{i}^{2}=\operatorname*{id}$ for every $i$).

Interestingly, this generalizes. Let $n\in\mathbb{N}$ and $\sigma\in S_{n}$. A
\textit{reduced expression} for $\sigma$ will mean a representation of
$\sigma$ as a composition of $\ell\left(  \sigma\right)  $ permutations of the
form $s_{k}$. (As we know, less than $\ell\left(  \sigma\right)  $ such
permutations do not suffice; thus the name \textquotedblleft
reduced\textquotedblright.) Then, (one of the many versions of)
\textit{Matsumoto's theorem} states that any two reduced expressions of
$\sigma$ can be obtained from one another by a rewriting process, each step of
which is either an application of (\ref{eq.perms.braid3}) (i.e., you pick an
\textquotedblleft$s_{i}\circ s_{i+1}\circ s_{i}$\textquotedblright\ in the
expression and replace it by \textquotedblleft$s_{i+1}\circ s_{i}\circ
s_{i+1}$\textquotedblright, or vice versa) or an application of
(\ref{eq.perms.braid2}) (i.e., you pick an \textquotedblleft$s_{i}\circ s_{j}%
$\textquotedblright\ with $\left\vert i-j\right\vert >1$ and replace it by
\textquotedblleft$s_{j}\circ s_{i}$\textquotedblright, or vice versa). For
instance, for $n=4$ and $\sigma=\left(  4,3,1,2,5\right)  $ (in one-line
notation), the two reduced expressions $\sigma=s_{1}\circ s_{2}\circ
s_{3}\circ s_{1}\circ s_{2}$ and $\sigma=s_{2}\circ s_{3}\circ s_{1}\circ
s_{2}\circ s_{3}$ can be obtained from one another by the following rewriting
process:%
\begin{align*}
s_{1}\circ s_{2}\circ\underbrace{s_{3}\circ s_{1}}_{\substack{=s_{1}\circ
s_{3}\\\text{(by (\ref{eq.perms.braid2}))}}}\circ s_{2}  &  =\underbrace{s_{1}%
\circ s_{2}\circ s_{1}}_{\substack{=s_{2}\circ s_{1}\circ s_{2}\\\text{(by
(\ref{eq.perms.braid3}))}}}\circ s_{3}\circ s_{2}=s_{2}\circ s_{1}%
\circ\underbrace{s_{2}\circ s_{3}\circ s_{2}}_{\substack{=s_{3}\circ
s_{2}\circ s_{3}\\\text{(by (\ref{eq.perms.braid3}))}}}\\
&  =s_{2}\circ\underbrace{s_{1}\circ s_{3}}_{\substack{=s_{3}\circ
s_{1}\\\text{(by (\ref{eq.perms.braid2}))}}}\circ s_{2}\circ s_{3}=s_{2}\circ
s_{3}\circ s_{1}\circ s_{2}\circ s_{3}.
\end{align*}
See, e.g., Williamson's thesis \cite[Corollary 1.2.3]{William03} or Knutson's
notes \cite[\S 2.3]{Knutson} for a proof of this fact. (Knutson, instead of
saying that \textquotedblleft$\sigma=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}$ is a reduced expression for $\sigma$\textquotedblright, says that
\textquotedblleft$k_{1}k_{2}\cdots k_{p}$ is a reduced word for $\sigma
$\textquotedblright.)

Something subtler holds for \textquotedblleft non-reduced\textquotedblright%
\ expressions. Namely, if we have a representation of $\sigma$ as a
composition of some number of permutations of the form $s_{k}$ (not
necessarily $\ell\left(  \sigma\right)  $ of them), then we can transform it
into a reduced expression by a rewriting process which consists of
applications of (\ref{eq.perms.braid3}) and (\ref{eq.perms.braid2}) as before
and also of cancellation steps (i.e., picking an \textquotedblleft$s_{i}\circ
s_{i}$\textquotedblright\ in the expression and removing it). (I don't know of
a place where this is proven elementarily in full detail, but it can easily be
derived from \cite[Corollary 1.2.3 and Corollary 1.1.6]{William03}.)

This all is stated and proven in greater generality in good books on Coxeter
groups, such as \cite{BjoBre05}. We won't need these results in the following,
but they are an example of what one can see if one looks at permutations closely.
\end{remark}

\subsection{More on signs of permutations}

In Section \ref{sect.sign}, we have defined the sign $\left(  -1\right)
^{\sigma}=\operatorname*{sign}\sigma=\operatorname*{sgn}\sigma$ of a
permutation $\sigma$. We recall the most important facts about it:

\begin{itemize}
\item We have $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }$ for every $\sigma\in S_{n}$. (This is the definition of
$\left(  -1\right)  ^{\sigma}$.) Thus, for every $\sigma\in S_{n}$, we have
$\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)
}\in\left\{  1,-1\right\}  $.

\item The permutation $\sigma\in S_{n}$ is said to be \textit{even} if
$\left(  -1\right)  ^{\sigma}=1$, and is said to be \textit{odd} if $\left(
-1\right)  ^{\sigma}=-1$.

\item The sign of the identity permutation $\operatorname*{id}\in S_{n}$ is
$\left(  -1\right)  ^{\operatorname*{id}}=1$.

\item For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the sign of the
permutation $s_{k}\in S_{n}$ is $\left(  -1\right)  ^{s_{k}}=-1$.

\item If $\sigma$ and $\tau$ are two permutations in $S_{n}$, then
\begin{equation}
\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(
-1\right)  ^{\tau}. \label{eq.sign.prod}%
\end{equation}


\item If $\sigma\in S_{n}$, then%
\begin{equation}
\left(  -1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\sigma}.
\label{eq.sign.inverse}%
\end{equation}

\end{itemize}

A simple consequence of the above facts is the following proposition:

\begin{proposition}
\label{prop.sign.prod-of-many}Let $n\in\mathbb{N}$ and $k\in\mathbb{N}$. Let
$\sigma_{1},\sigma_{2},\ldots,\sigma_{k}$ be $k$ permutations in $S_{n}$.
Then,%
\begin{equation}
\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{k}%
}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}%
\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{k}}. \label{eq.sign.prod-of-many}%
\end{equation}

\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.sign.prod-of-many}.]Straightforward induction
over $k$. The induction base (i.e., the case when $k=0$) follows from the fact
that $\left(  -1\right)  ^{\operatorname*{id}}=1$ (since the composition of
$0$ permutations is $\operatorname*{id}$). The induction step is easily done
using (\ref{eq.sign.prod}).
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.sign.prod-of-many}.]We will prove
(\ref{eq.sign.prod-of-many}) by induction over $k$:

\textit{Induction base:} Assume that $k=0$. Then, $\sigma_{1}\circ\sigma
_{2}\circ\cdots\circ\sigma_{k}=\sigma_{1}\circ\sigma_{2}\circ\cdots\circ
\sigma_{0}=\operatorname*{id}$ (since the composition of $0$ permutations is
$\operatorname*{id}$). Hence, $\left(  -1\right)  ^{\sigma_{1}\circ\sigma
_{2}\circ\cdots\circ\sigma_{k}}=\left(  -1\right)  ^{\operatorname*{id}}=1$.
On the other hand, from $k=0$, we obtain $\left(  -1\right)  ^{\sigma_{1}%
}\cdot\left(  -1\right)  ^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)
^{\sigma_{k}}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)
^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{0}}=1$ (since the
product of $0$ integers is $1$). Compared with $\left(  -1\right)
^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{k}}=1$, this yields
$\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{k}%
}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}%
\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{k}}$. Thus,
(\ref{eq.sign.prod-of-many}) is proven for $k=0$. The induction base is thus complete.

\textit{Induction step:} Let $K\in\mathbb{N}$. Assume that
(\ref{eq.sign.prod-of-many}) is proven for $k=K$. We need to prove
(\ref{eq.sign.prod-of-many}) for $k=K+1$.

Let $\sigma_{1},\sigma_{2},\ldots,\sigma_{K+1}$ be $K+1$ permutations in
$S_{n}$. We have assumed that (\ref{eq.sign.prod-of-many}) is proven for
$k=K$. Thus, we can apply (\ref{eq.sign.prod-of-many}) to $k=K$. We thus
conclude $\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ
\sigma_{K}}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)
^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{K}}$.

But $\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K+1}=\left(  \sigma
_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K}\right)  \circ\sigma_{K+1}$, so
that%
\begin{align*}
\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K+1}}
&  =\left(  -1\right)  ^{\left(  \sigma_{1}\circ\sigma_{2}\circ\cdots
\circ\sigma_{K}\right)  \circ\sigma_{K+1}}=\underbrace{\left(  -1\right)
^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K}}}_{=\left(  -1\right)
^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}\cdot\cdots\cdot\left(
-1\right)  ^{\sigma_{K}}}\cdot\left(  -1\right)  ^{\sigma_{K+1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\sigma=\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K}\text{ and }%
\tau=\sigma_{K+1}\right) \\
&  =\left(  \left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)
^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{K}}\right)
\cdot\left(  -1\right)  ^{\sigma_{K+1}}\\
&  =\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}%
\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{K+1}}.
\end{align*}


Let us now forget that we fixed $\sigma_{1},\sigma_{2},\ldots,\sigma_{K+1}$.
We thus have shown that if $\sigma_{1},\sigma_{2},\ldots,\sigma_{K+1}$ are
$K+1$ permutations in $S_{n}$, then $\left(  -1\right)  ^{\sigma_{1}%
\circ\sigma_{2}\circ\cdots\circ\sigma_{K+1}}=\left(  -1\right)  ^{\sigma_{1}%
}\cdot\left(  -1\right)  ^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)
^{\sigma_{K+1}}$. In other words, we have proven (\ref{eq.sign.prod-of-many})
for $k=K+1$. This completes the induction step, and so the induction proof of
(\ref{eq.sign.prod-of-many}) is complete. In other words, Proposition
\ref{prop.sign.prod-of-many} is proven.
\end{proof}
\end{verlong}

We state a few more properties, which should not be difficult by now:

\begin{definition}
\label{def.transpos}Let $n\in\mathbb{N}$. Let $i$ and $j$ be two distinct
elements of $\left\{  1,2,\ldots,n\right\}  $. We let $t_{i,j}$ be the
permutation in $S_{n}$ which switches $i$ with $j$ while leaving all other
elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged. Such a permutation is
called a \textit{transposition} (and is often denoted by $\left(  i,j\right)
$ in literature; but we prefer not to do so, since it is too similar to
one-line notation).
\end{definition}

Notice that the permutations $s_{1},s_{2},\ldots,s_{n-1}$ are transpositions
(namely, $s_{i}=t_{i,i+1}$ for every $i\in\left\{  1,2,\ldots,n-1\right\}  $),
but they are not the only transpositions (when $n\geq3$).

\begin{exercise}
\label{exe.ps4.1ab}Let $n\in\mathbb{N}$. Let $i$ and $j$ be two distinct
elements of $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} Find $\ell\left(  t_{i,j}\right)  $.

\textbf{(b)} Show that $\left(  -1\right)  ^{t_{i,j}}=-1$.
\end{exercise}

\begin{exercise}
\label{exe.ps4.1c}Let $n\in\mathbb{N}$. Let $w_{0}$ denote the permutation in
$S_{n}$ which sends each $k\in\left\{  1,2,\ldots,n\right\}  $ to $n+1-k$.
Compute $\ell\left(  w_{0}\right)  $ and $\left(  -1\right)  ^{w_{0}}$.
\end{exercise}

\begin{exercise}
\label{exe.ps4.2}Let $X$ be a finite set. We want to define the sign of any
permutation of $X$. (We have sketched this definition before (see
(\ref{eq.ps2.S(X).sign.teaser})), but now we shall do it in detail.)

Fix a bijection $\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. (Such a bijection always exists. Indeed, constructing such a
bijection is tantamount to writing down a list of all elements of $X$, with no
duplicates.) For every permutation $\sigma$ of $X$, set%
\[
\left(  -1\right)  _{\phi}^{\sigma}=\left(  -1\right)  ^{\phi\circ\sigma
\circ\phi^{-1}}.
\]
Here, the right hand side is well-defined because $\phi\circ\sigma\circ
\phi^{-1}$ is a permutation of $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} Prove that $\left(  -1\right)  _{\phi}^{\sigma}$ depends only on
the permutation $\sigma$ of $X$, but not on the bijection $\phi$. (In other
words, for a given $\sigma$, any two different choices of $\phi$ will lead to
the same $\left(  -1\right)  _{\phi}^{\sigma}$.)

This allows us to define the \textit{sign} of the permutation $\sigma$ to be
$\left(  -1\right)  _{\phi}^{\sigma}$ for any choice of bijection
$\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $. We denote this sign simply
by $\left(  -1\right)  ^{\sigma}$. (When $X=\left\{  1,2,\ldots,n\right\}  $,
then this sign is clearly the same as the sign $\left(  -1\right)  ^{\sigma}$
we defined before, because we can pick the bijection $\phi=\operatorname*{id}$.)

\textbf{(b)} Show that the permutation $\operatorname*{id}:X\rightarrow X$
satisfies $\left(  -1\right)  ^{\operatorname*{id}}=1$.

\textbf{(c)} Show that $\left(  -1\right)  ^{\sigma\circ\tau}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ for any two permutations
$\sigma$ and $\tau$ of $X$.
\end{exercise}

\begin{remark}
Let $n\in\mathbb{N}$. Recall that a \textit{transposition} in $S_{n}$ means a
permutation of the form $t_{i,j}$, where $i$ and $j$ are two distinct elements
of $\left\{  1,2,\ldots,n\right\}  $. Therefore, if $\tau$ is a transposition
in $S_{n}$, then
\begin{equation}
\left(  -1\right)  ^{\tau}=-1. \label{eq.sign.transposition}%
\end{equation}
(In fact, if $\tau$ is a transposition in $S_{n}$, then $\tau$ can be written
in the form $\tau=t_{i,j}$ for two distinct elements $i$ and $j$ of $\left\{
1,2,\ldots,n\right\}  $; and therefore, for these two elements $i$ and $j$, we
have $\left(  -1\right)  ^{\tau}=\left(  -1\right)  ^{t_{i,j}}=-1$ (according
to Exercise \ref{exe.ps4.1ab} \textbf{(b)}). This proves
(\ref{eq.sign.transposition}).)

Now, let $\sigma\in S_{n}$ be any permutation. Assume that $\sigma$ is written
in the form $\sigma=\tau_{1}\circ\tau_{2}\circ\cdots\circ\tau_{k}$ for some
transpositions $\tau_{1},\tau_{2},\ldots,\tau_{k}$ in $S_{n}$. Then,%
\begin{align}
\left(  -1\right)  ^{\sigma}  &  =\left(  -1\right)  ^{\tau_{1}\circ\tau
_{2}\circ\cdots\circ\tau_{k}}=\underbrace{\left(  -1\right)  ^{\tau_{1}}%
}_{\substack{=-1\\\text{(by (\ref{eq.sign.transposition}))}}}\cdot
\underbrace{\left(  -1\right)  ^{\tau_{2}}}_{\substack{=-1\\\text{(by
(\ref{eq.sign.transposition}))}}}\cdot\cdots\cdot\underbrace{\left(
-1\right)  ^{\tau_{k}}}_{\substack{=-1\\\text{(by (\ref{eq.sign.transposition}%
))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod-of-many}), applied
to }\sigma_{i}=\tau_{i}\right) \nonumber\\
&  =\underbrace{\left(  -1\right)  \cdot\left(  -1\right)  \cdot\cdots
\cdot\left(  -1\right)  }_{k\text{ factors}}=\left(  -1\right)  ^{k}.
\label{eq.sign.prod-of-transpositions}%
\end{align}
Since many permutations can be written as products of transpositions in a
simple way, this formula gives a useful method for computing signs.
\end{remark}

\begin{remark}
Let $n\in\mathbb{N}$. It is not hard to prove that
\begin{equation}
\left(  -1\right)  ^{\sigma}=\prod_{1\leq i<j\leq n}\dfrac{\sigma\left(
i\right)  -\sigma\left(  j\right)  }{i-j}\ \ \ \ \ \ \ \ \ \ \text{for every
}\sigma\in S_{n}. \label{eq.sign.pseudoexplicit}%
\end{equation}
(Of course, it is no easier to compute $\left(  -1\right)  ^{\sigma}$ using
this seemingly explicit formula than by counting inversions.)

We shall prove (\ref{eq.sign.pseudoexplicit}) in Exercise
\ref{exe.perm.sign.pseudoexplicit} \textbf{(c)}.
\end{remark}

\begin{remark}
The sign of a permutation is also called its \textit{signum} or its
\textit{signature}. Different authors define the sign of a permutation
$\sigma$ in different ways. Some (e.g., Hefferon in \cite[Definition
4.7]{Hefferon}) define it as we do; others (e.g., Conrad in \cite{Conrad} or
Hoffman and Kunze in \cite[p. 152]{HoffmanKunze}) define it using
(\ref{eq.sign.prod-of-transpositions}); yet others define it using something
called the \textit{cycle decomposition} of a permutation; some even define it
using (\ref{eq.sign.pseudoexplicit}), or using a similar ratio of two
polynomials. However, it is not hard to check that all of these definitions
are equivalent. (We already know that the first two of them are equivalent.)
\end{remark}

\begin{exercise}
\label{exe.perm.sign.pseudoexplicit}Let $n\in\mathbb{N}$. Let $\sigma\in
S_{n}$.

\textbf{(a)} If $x_{1},x_{2},\ldots,x_{n}$ are $n$ elements of $\mathbb{C}$,
then prove that%
\[
\prod_{1\leq i<j\leq n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(
j\right)  }\right)  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(b)} More generally: For every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$, let $a_{\left(  i,j\right)  }$ be an element of
$\mathbb{C}$. Assume that%
\begin{equation}
a_{\left(  j,i\right)  }=-a_{\left(  i,j\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}. \label{eq.exe.perm.sign.pseudoexplicit.b.skew}%
\end{equation}
Prove that%
\[
\prod_{1\leq i<j\leq n}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }=\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}a_{\left(  i,j\right)  }.
\]


\textbf{(c)} Prove (\ref{eq.sign.pseudoexplicit}).

\textbf{(d)} Use Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} to
give a new solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}.
\end{exercise}

\subsection{Cycles}

Next, we shall discuss another specific class of permutations: the
\textit{cycles}.

\begin{definition}
\label{def.perm.cycles}Let $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $.

Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $i_{1},i_{2},\ldots,i_{k}$ be
$k$ distinct elements of $\left[  n\right]  $. We define $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ to be the permutation in $S_{n}$ which
sends $i_{1},i_{2},\ldots,i_{k}$ to $i_{2},i_{3},\ldots,i_{k},i_{1}$,
respectively, while leaving all other elements of $\left[  n\right]  $ fixed.
In other words, we define $\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}$ to be the permutation in $S_{n}$ given by%
\[
\left(
\begin{array}
[c]{r}%
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  p\right)  =%
\begin{cases}
i_{j+1}, & \text{if }p=i_{j}\text{ for some }j\in\left\{  1,2,\ldots
,k\right\}  ;\\
p, & \text{otherwise}%
\end{cases}
\\
\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left[  n\right]
\end{array}
\right)  ,
\]
where $i_{k+1}$ means $i_{1}$.

(Again, the notation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$
conceals the parameter $n$, which will hopefully not cause any confusion.)

A permutation of the form $\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}$ is said to be a $k$\textit{-cycle} (or sometimes just a
\textit{cycle}, or a \textit{cyclic permutation}). Of course, the name stems
from the fact that it \textquotedblleft cycles\textquotedblright\ through the
elements $i_{1},i_{2},\ldots,i_{k}$ (by sending each of them to the next one
and the last one back to the first) and leaves all other elements unchanged.
\end{definition}

\begin{example}
\label{exa.perm.cycles}Let $n\in\mathbb{N}$. The following facts follow easily
from Definition \ref{def.perm.cycles}:

\textbf{(a)} For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have
$\operatorname*{cyc}\nolimits_{i}=\operatorname*{id}$. In other words, any
$1$-cycle is the identity permutation $\operatorname*{id}$.

\textbf{(b)} If $i$ and $j$ are two distinct elements of $\left\{
1,2,\ldots,n\right\}  $, then $\operatorname*{cyc}\nolimits_{i,j}=t_{i,j}$.
(See Definition \ref{def.transpos} for the definition of $t_{i,j}$.)

\textbf{(c)} If $k\in\left\{  1,2,\ldots,n-1\right\}  $, then
$\operatorname*{cyc}\nolimits_{k,k+1}=s_{k}$.

\textbf{(d)} If $n=5$, then $\operatorname*{cyc}\nolimits_{2,5,3}$ is the
permutation which sends $1$ to $1$, $2$ to $5$, $3$ to $2$, $4$ to $4$, and
$5$ to $3$. (In other words, it is the permutation which is $\left(
1,5,2,4,3\right)  $ in one-line notation.)

\textbf{(e)} If $k\in\left\{  1,2,\ldots,n\right\}  $, and if $i_{1}%
,i_{2},\ldots,i_{k}$ are $k$ pairwise distinct elements of $\left[  n\right]
$, then%
\[
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}=\operatorname*{cyc}%
\nolimits_{i_{2},i_{3},\ldots,i_{k},i_{1}}=\operatorname*{cyc}\nolimits_{i_{3}%
,i_{4},\ldots,i_{k},i_{1},i_{2}}=\cdots=\operatorname*{cyc}\nolimits_{i_{k}%
,i_{1},i_{2},\ldots,i_{k-1}}.
\]
(In less formal words: The $k$-cycle $\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}$ does not change when we cyclically rotate the list
$\left(  i_{1},i_{2},\ldots,i_{k}\right)  $.)
\end{example}

\begin{remark}
What we called $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ in
Definition \ref{def.perm.cycles} is commonly denoted by $\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  $ in the literature. But this latter notation
$\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ would clash with one-line notation
for permutations (the cycle $\operatorname*{cyc}\nolimits_{1,2,3}\in S_{3}$ is
not the same as the permutation which is $\left(  1,2,3\right)  $ in one-line
notation) and also with the standard notation for $k$-tuples. This is why we
prefer to use the notation $\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}$. (That said, we are not going to use $k$-cycles very often.)
\end{remark}

The following exercise gathers some properties of cycles. Parts \textbf{(a)}
and \textbf{(d)} and, to a lesser extent, \textbf{(b)} are fairly important
and you should make sure you know how to solve them. The significantly more
difficult part \textbf{(c)} is more of a curiosity with an interesting proof
(I have not found an application of it so far; skip it if you do not want to
spend time on what is essentially a contest problem).

\begin{exercise}
\label{exe.perm.cycles}Let $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Let $k\in\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} For every $\sigma\in S_{n}$ and every $k$ distinct elements
$i_{1},i_{2},\ldots,i_{k}$ of $\left[  n\right]  $, prove that
\[
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}=\operatorname*{cyc}\nolimits_{\sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  }.
\]


\textbf{(b)} For every $p\in\left\{  0,1,\ldots,n-k\right\}  $, prove that%
\[
\ell\left(  \operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\right)  =k-1.
\]


\textbf{(c)} For every $k$ distinct elements $i_{1},i_{2},\ldots,i_{k}$ of
$\left[  n\right]  $, prove that
\[
\ell\left(  \operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\right)
\geq k-1.
\]


\textbf{(d)} For every $k$ distinct elements $i_{1},i_{2},\ldots,i_{k}$ of
$\left[  n\right]  $, prove that
\[
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
}=\left(  -1\right)  ^{k-1}.
\]

\end{exercise}

\begin{remark}
Exercise \ref{exe.perm.cycles} \textbf{(d)} shows that every $k$-cycle in
$S_{n}$ has sign $\left(  -1\right)  ^{k-1}$. However, the length of a
$k$-cycle need not be $k-1$. Exercise \ref{exe.perm.cycles} \textbf{(c)} shows
that this length is always $\geq k-1$, but it can take other values as well.
For instance, in $S_{4}$, the length of the $3$-cycle $\operatorname*{cyc}%
\nolimits_{1,4,3}$ is $4$. (Another example are the transpositions $t_{i,j}$
from Definition \ref{def.transpos}; these are $2$-cycles but can have length
$>1$.)

I don't know a simple way to describe when equality holds in Exercise
\ref{exe.perm.cycles} \textbf{(c)}. It holds whenever $i_{1},i_{2}%
,\ldots,i_{k}$ are consecutive integers (due to Exercise \ref{exe.perm.cycles}
\textbf{(b)}), but also in some other cases; for example, the $4$-cycle
$\operatorname*{cyc}\nolimits_{1,3,4,2}$ in $S_{4}$ has length $3$.
\end{remark}

\begin{remark}
\label{rmk.perm.cycles.decompose}The main reason why cycles are useful is
that, essentially, every permutation can be \textquotedblleft
decomposed\textquotedblright\ into cycles. We shall not use this fact, but
since it is generally important, let us briefly explain what it means. (You
will probably learn more about it in any standard course on abstract algebra.)

Fix $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}
$. Two cycles $\alpha$ and $\beta$ in $S_{n}$ are said to be \textit{disjoint}
if they can be written as $\alpha=\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}$ and $\beta=\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{\ell}}$ for $k+\ell$ distinct elements $i_{1},i_{2}%
,\ldots,i_{k},j_{1},j_{2},\ldots,j_{\ell}$ of $\left[  n\right]  $. For
example, the two cycles $\operatorname*{cyc}\nolimits_{1,3}$ and
$\operatorname*{cyc}\nolimits_{2,6,7}$ in $S_{8}$ are disjoint, but the two
cycles $\operatorname*{cyc}\nolimits_{1,4}$ and $\operatorname*{cyc}%
\nolimits_{2,4}$ are not. It is easy to see that any two disjoint cycles
$\alpha$ and $\beta$ commute (i.e., satisfy $\alpha\circ\beta=\beta\circ
\alpha$). Therefore, when you see a composition $\alpha_{1}\circ\alpha
_{2}\circ\cdots\circ\alpha_{p}$ of several pairwise disjoint cycles, you can
reorder its factors arbitrarily without changing the result (for example,
$\alpha_{3}\circ\alpha_{1}\circ\alpha_{4}\circ\alpha_{2}=\alpha_{1}\circ
\alpha_{2}\circ\alpha_{3}\circ\alpha_{4}$ if $p=4$).

Now, the fact I am talking about says the following: Every permutation in
$S_{n}$ can be written as a composition of several pairwise disjoint cycles.
For example, let $n=9$, and let $\sigma\in S_{9}$ be the permutation which is
written $\left(  4,6,1,3,5,2,9,8,7\right)  $ in one-line notation (i.e., we
have $\sigma\left(  1\right)  =4$, $\sigma\left(  2\right)  =6$, etc.). Then,
$\sigma$ can be written as a composition of several pairwise disjoint cycles
as follows:%
\begin{equation}
\sigma=\operatorname*{cyc}\nolimits_{1,4,3}\circ\operatorname*{cyc}%
\nolimits_{7,9}\circ\operatorname*{cyc}\nolimits_{2,6}.
\label{eq.rmk.perm.cycles.decompose.1}%
\end{equation}
Indeed, here is how such a decomposition can be found: Let us draw a directed
graph whose vertices are $1,2,\ldots,n$, and which has an arc $i\rightarrow
\sigma\left(  i\right)  $ for every $i\in\left[  n\right]  $. (Thus, it has
$n$ arcs altogether; some of them can be loops.) For our permutation
$\sigma\in S_{9}$, this graph looks as follows:
\begin{equation}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm, thick,main node/.style={circle,fill=blue!20,draw}] \node[main node] (1) {1}; \node[main node] (3) [below right of=1] {3}; \node[main node] (4) [above right of=1] {4}; \node[main node] (2) [above left of=1] {2}; \node[main node] (6) [below left of=1] {6}; \node[main node] (5) [below right of=4] {5}; \node[main node] (7) [above right of=5] {7}; \node[main node] (9) [below right of=5] {9}; \node[main node] (8) [above right of=9] {8}; \path[every node/.style={font=\sffamily\small}] (1) edge (4) (2) edge [bend right] (6) (3) edge (1) (4) edge (3) (5) edge [loop left] (5) (6) edge [bend right] (2) (7) edge [bend right] (9) (8) edge [loop left] (8) (9) edge [bend right] (7); \end{tikzpicture}.
\label{eq.rmk.perm.cycles.decompose.2}%
\end{equation}
Obviously, at each vertex $i$ of this graph, exactly one arc begins (namely,
the arc $i\rightarrow\sigma\left(  i\right)  $). Moreover, since $\sigma$ is
invertible, it is also clear that at each vertex $i$ of this graph, exactly
one arc ends (namely, the arc $\sigma^{-1}\left(  i\right)  \rightarrow i$).
Due to the way we constructed this graph, it is clear that it completely
describes our permutation $\sigma$: Namely, if we want to find $\sigma\left(
i\right)  $ for a given $i\in\left[  n\right]  $, we should just locate the
vertex $i$ on the graph, and follow the arc that begins at this vertex; the
endpoint of this arc will be $\sigma\left(  i\right)  $.

Now, a look at this graph reveals five directed cycles (in the sense of
\textquotedblleft paths which end at the same vertex at which they
begin\textquotedblright, not yet in the sense of \textquotedblleft cyclic
permutations\textquotedblright). The first one passes through the vertices $2$
and $6$; the second passes through the vertices $3$, $1$ and $4$; the third,
through the vertex $5$ (it is what is called a \textquotedblleft trivial
cycle\textquotedblright), and so on. To each of these cycles we can assign a
cyclic permutation in $S_{n}$: namely, if the cycle passes through the
vertices $i_{1},i_{2},\ldots,i_{k}$ (in this order, and with no repetitions),
then we assign to it the cyclic permutation $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\in S_{n}$. The cyclic permutations
assigned to all five directed cycles are pairwise disjoint, and their
composition is%
\[
\operatorname*{cyc}\nolimits_{2,6}\circ\operatorname*{cyc}\nolimits_{3,1,4}%
\circ\operatorname*{cyc}\nolimits_{5}\circ\operatorname*{cyc}\nolimits_{7,9}%
\circ\operatorname*{cyc}\nolimits_{8}.
\]
But this composition must be $\sigma$ (because if we apply this composition to
an element $i\in\left[  n\right]  $, then we obtain the \textquotedblleft next
vertex after $i$\textquotedblright\ on the directed cycle which passes through
$i$; but due to how we constructed our graph, this \textquotedblleft next
vertex\textquotedblright\ will be precisely $\sigma\left(  i\right)  $).
Hence, we have%
\begin{equation}
\sigma=\operatorname*{cyc}\nolimits_{2,6}\circ\operatorname*{cyc}%
\nolimits_{3,1,4}\circ\operatorname*{cyc}\nolimits_{5}\circ\operatorname*{cyc}%
\nolimits_{7,9}\circ\operatorname*{cyc}\nolimits_{8}.
\label{eq.rmk.perm.cycles.decompose.3}%
\end{equation}
Thus, we have found a way to write $\sigma$ as a composition of several
pairwise disjoint cycles. We can rewrite (and even simplify) this
representation a bit: Namely, we can simplify
(\ref{eq.rmk.perm.cycles.decompose.3}) by removing the factors
$\operatorname*{cyc}\nolimits_{5}$ and $\operatorname*{cyc}\nolimits_{8}$
(because both of these factors equal $\operatorname*{id}$); thus we obtain
$\sigma=\operatorname*{cyc}\nolimits_{2,6}\circ\operatorname*{cyc}%
\nolimits_{3,1,4}\circ\operatorname*{cyc}\nolimits_{7,9}$. We can furthermore
switch $\operatorname*{cyc}\nolimits_{2,6}$ with $\operatorname*{cyc}%
\nolimits_{3,1,4}$ (since disjoint cycles commute), therefore obtaining
$\sigma=\operatorname*{cyc}\nolimits_{3,1,4}\circ\operatorname*{cyc}%
\nolimits_{2,6}\circ\operatorname*{cyc}\nolimits_{7,9}$. Next, we can switch
$\operatorname*{cyc}\nolimits_{2,6}$ with $\operatorname*{cyc}\nolimits_{7,9}%
$, obtaining $\sigma=\operatorname*{cyc}\nolimits_{3,1,4}\circ
\operatorname*{cyc}\nolimits_{7,9}\circ\operatorname*{cyc}\nolimits_{2,6}$.
Finally, we can rewrite $\operatorname*{cyc}\nolimits_{3,1,4}$ as
$\operatorname*{cyc}\nolimits_{1,4,3}$, and we obtain
(\ref{eq.rmk.perm.cycles.decompose.1}).

In general, for every $n\in\mathbb{N}$, every permutation $\sigma\in S_{n}$
can be represented as a composition of several pairwise disjoint cycles (which
can be found by drawing a directed graph as in our example above). This
representation is not literally unique, because we can modify it by:

\begin{itemize}
\item adding or removing trivial factors (i.e., factors of the form
$\operatorname*{cyc}\nolimits_{i}=\operatorname*{id}$);

\item switching different cycles;

\item rewriting $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ as
$\operatorname*{cyc}\nolimits_{i_{2},i_{3},\ldots,i_{k},i_{1}}$.
\end{itemize}

However, it is unique \textbf{up to these modifications}; in other words, any
two representations of $\sigma$ as a composition of several pairwise disjoint
cycles can be transformed into one another by such modifications.

The proofs of all these statements are fairly easy. (One does have to check
certain things, e.g., that the directed graph really consists of disjoint
directed cycles.)

Representing a permutation $\sigma\in S_{n}$ as a composition of several
pairwise disjoint cycles can be done very quickly, and thus gives a quick way
to find $\left(  -1\right)  ^{\sigma}$ (because Exercise \ref{exe.perm.cycles}
\textbf{(d)} tells us how to find the sign of a $k$-cycle). This is
significantly faster than counting inversions of $\sigma$.
\end{remark}

\subsection{Additional exercises}

Permutations and symmetric groups are a staple of combinatorics; there are
countless results involving them. For an example, B\'{o}na's book
\cite{Bona12}, as well as significant parts of Stanley's \cite{Stanley-EC1}
and \cite{Stanley-EC2} are devoted to them. In this section, I shall only give
a haphazard selection of exercises, which are not relevant to the rest of
these notes (thus can be skipped at will). I am not planning to provide
solutions for all of them.

\begin{addexercise}
\label{exeadd.perm.order}Let $n\in\mathbb{N}$. Let $d=\operatorname{lcm}%
\left(  1,2,\ldots,n\right)  $.

\textbf{(a)} Show that $\pi^{d}=\operatorname*{id}$ for every $\pi\in S_{n}$.

\textbf{(b)} Let $k$ be an integer such that every $\pi\in S_{n}$ satisfies
$\pi^{k}=\operatorname*{id}$. Show that $d\mid k$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.perm.sigmacrosstau}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $\sigma\in S_{n}$ and $\tau\in S_{m}$. We define a permutation
$\sigma\times\tau$ of the set $\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $ by setting%
\[
\left(  \sigma\times\tau\right)  \left(  a,b\right)  =\left(  \sigma\left(
a\right)  ,\tau\left(  b\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}\left(  a,b\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  .
\]


\textbf{(a)} Prove that $\sigma\times\tau$ is a well-defined permutation.

\textbf{(b)} Prove that $\sigma\times\tau=\left(  \sigma\times
\operatorname*{id}\right)  \circ\left(  \operatorname*{id}\times\tau\right)  $.

\textbf{(c)} According to Exercise \ref{exe.ps4.2} (applied to $X=\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $), the permutation
$\sigma\times\tau$ has a well-defined sign $\left(  -1\right)  ^{\sigma
\times\tau}$. Prove that $\left(  -1\right)  ^{\sigma\times\tau}=\left(
\left(  -1\right)  ^{\sigma}\right)  ^{m}\left(  \left(  -1\right)  ^{\tau
}\right)  ^{n}$.
\end{addexercise}

The next two additional exercises concern the inversions of a permutation.
They use the following definition:

\begin{definition}
\label{def.Inv}Let $n\in\mathbb{N}$. For every $\sigma\in S_{n}$, we let
$\operatorname*{Inv}\sigma$ denote the set of all inversions of $\sigma$.
\end{definition}

Exercise \ref{exe.ps2.2.5} \textbf{(c)} shows that any $n\in\mathbb{N}$ and
any two permutations $\sigma$ and $\tau$ in $S_{n}$ satisfy the inequality
$\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  $. In the following exercise, we will see when this
inequality becomes an equality:

\begin{addexercise}
\label{exeadd.perm.Inv.sub}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ and
$\tau\in S_{n}$.

\textbf{(a)} Prove that $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $.

\textbf{(b)} Prove that $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if
$\operatorname*{Inv}\left(  \sigma^{-1}\right)  \subseteq\operatorname*{Inv}%
\left(  \tau^{-1}\circ\sigma^{-1}\right)  $.

\textbf{(c)} Prove that $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}%
\tau$ holds if and only if $\ell\left(  \tau\right)  =\ell\left(  \tau
\circ\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $.

\textbf{(d)} Assume that $\operatorname*{Inv}\sigma=\operatorname*{Inv}\tau$.
Prove that $\sigma=\tau$.
\end{addexercise}

Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(d)} shows that if two
permutations in $S_{n}$ have the same set of inversions, then they are equal.
In other words, a permutation in $S_{n}$ is uniquely determined by its set of
inversions. The next additional exercise shows what set of inversions a
permutation can have:

\begin{addexercise}
\label{exeadd.perm.invset}Let $n\in\mathbb{N}$. Let $G=\left\{  \left(
i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  $.

A subset $U$ of $G$ is said to be \textit{transitive} if every $a,b,c\in
\left\{  1,2,\ldots,n\right\}  $ satisfying $\left(  a,b\right)  \in U$ and
$\left(  b,c\right)  \in U$ also satisfy $\left(  a,c\right)  \in U$.

A subset $U$ of $G$ is said to be \textit{inversive} if there exists a
$\sigma\in S_{n}$ such that $U=\operatorname*{Inv}\sigma$.

Let $U$ be a subset of $G$. Prove that $U$ is inversive if and only if both
$U$ and $G\setminus U$ are transitive.
\end{addexercise}

\section{\label{chp.det}An introduction to determinants}

In this chapter, we will define and study determinants in a combinatorial way
(in the spirit of Hefferon's book \cite{Hefferon}, Gill Williamson's notes
\cite[Chapter 3]{Gill}, Laue's notes \cite{Laue-det} and Zeilberger's paper
\cite{Zeilbe}\footnote{My notes differ from these sources in the following:
\par
\begin{itemize}
\item Hefferon's book \cite{Hefferon} is an introductory textbook for a first
course in Linear Algebra, and so treats rather little of the theory of
determinants (noticeably less than what we do). It is, however, a good
introduction into the \textquotedblleft other part\textquotedblright\ of
linear algebra (i.e., the theory of vector spaces and linear maps), and puts
determinants into the context of that other part, which makes some of their
properties appear less mysterious. (Like many introductory textbooks, it only
discusses matrices over fields, not over commutative rings; it also uses more
handwaving in the proofs.)
\par
\item Zeilberger's paper \cite{Zeilbe} mostly proves advanced results (apart
from its Section 5, which proves our Theorem \ref{thm.det(AB)}). I would
recommend reading it after reading this chapter.
\par
\item Laue's notes \cite{Laue-det} are a brief introduction to determinants
that prove the main results in just 14 pages (although at the cost of terser
writing and stronger assumptions on the reader's preknowledge). If you read
these notes, make sure to pay attention to the \textquotedblleft Prerequisites
and some Terminology\textquotedblright\ section, as it explains the (unusual)
notations used in these notes.
\par
\item Gill Williamson's \cite[Chapter 3]{Gill} probably comes the closest to
what I am doing below (and is highly recommended, not least because it goes
much further into various interesting directions!). My notes are more
elementary and more detailed in what they do.
\end{itemize}
}). Nowadays, students usually learn about determinants in the context of
linear algebra, after having made the acquaintance of vector spaces, matrices,
linear transformations, Gaussian elimination etc.; this approach to
determinants (which I like to call the \textquotedblleft linear-algebraic
approach\textquotedblright) has certain advantages and certain disadvantages
to our combinatorial approach\footnote{Its main advantage is that it gives
more motivation and context. However, the other (combinatorial) approach
requires less preknowledge and involves less technical subtleties (for
example, it defines the determinant directly by an explicit formula, while the
linear-algebraic approach defines it implicitly by a list of conditions which
happen to determine it uniquely), which is why I have chosen it. (Of course,
it helped that the combinatorial approach is, well, combinatorial.)}.

We shall study determinants of matrices over \textit{commutative
rings}.\footnote{This is a rather general setup, which includes determinants
of matrices with real entries, of matrices with complex entries, of matrices
with polynomial entries, and many other situations. One benefit of working
combinatorially is that studying determinants in this general setup is no more
difficult than studying them in more restricted settings.} First, let us
define what these words (\textquotedblleft commutative ring\textquotedblright,
\textquotedblleft matrix\textquotedblright\ and \textquotedblleft
determinant\textquotedblright) mean.

\subsection{\label{sect.commring}Commutative rings}

\begin{definition}
If $\mathbb{K}$ is a set, then a \textit{binary operation} on $\mathbb{K}$
means a map from $\mathbb{K}\times\mathbb{K}$ to $\mathbb{K}$. (In other
words, it means a function which takes two elements of $\mathbb{K}$ as input,
and returns an element of $\mathbb{K}$ as output.) For instance, the map from
$\mathbb{Z}\times\mathbb{Z}$ to $\mathbb{Z}$ which sends every pair $\left(
a,b\right)  \in\mathbb{Z}\times\mathbb{Z}$ to $3a-b$ is a binary operation on
$\mathbb{Z}$.

Sometimes, a binary operation $f$ on a set $\mathbb{K}$ will be
\textit{written infix}. This means that the image of $\left(  a,b\right)
\in\mathbb{K}\times\mathbb{K}$ under $f$ will be denoted by $afb$ instead of
$f\left(  a,b\right)  $. For instance, the binary operation $+$ on the set
$\mathbb{Z}$ (which sends a pair $\left(  a,b\right)  $ of integers to their
sum $a+b$) is commonly written infix, because one writes $a+b$ and not
$+\left(  a,b\right)  $ for the sum of $a$ and $b$.
\end{definition}

\begin{definition}
\label{def.commring}A \textit{commutative ring} means a set $\mathbb{K}$
endowed with

\begin{itemize}
\item two binary operations called \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright, and denoted by $+$ and $\cdot$,
respectively, and both written infix\footnotemark, and

\item two elements called $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$
\end{itemize}

such that the following axioms are satisfied:

\begin{itemize}
\item \textit{Commutativity of addition:} We have $a+b=b+a$ for all
$a\in\mathbb{K}$ and $b\in\mathbb{K}$.

\item \textit{Commutativity of multiplication:} We have $ab=ba$ for all
$a\in\mathbb{K}$ and $b\in\mathbb{K}$. Here and in the following, $ab$ is
shorthand for $a\cdot b$ (as is usual for products of numbers).

\item \textit{Associativity of addition:} We have $a+\left(  b+c\right)
=\left(  a+b\right)  +c$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$ and
$c\in\mathbb{K}$.

\item \textit{Associativity of multiplication:} We have $a\left(  bc\right)
=\left(  ab\right)  c$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$ and
$c\in\mathbb{K}$.

\item \textit{Neutrality of }$0$\textit{:} We have $a+0_{\mathbb{K}%
}=0_{\mathbb{K}}+a=a$ for all $a\in\mathbb{K}$.

\item \textit{Existence of additive inverses:} For every $a\in\mathbb{K}$,
there exists an element $a^{\prime}\in\mathbb{K}$ such that $a+a^{\prime
}=a^{\prime}+a=0_{\mathbb{K}}$. This $a^{\prime}$ is commonly denoted by $-a$
and called the \textit{additive inverse} of $a$. (It is easy to check that it
is unique.)

\item \textit{Unitality (a.k.a. neutrality of }$1$\textit{):} We have
$1_{\mathbb{K}}a=a1_{\mathbb{K}}=a$ for all $a\in\mathbb{K}$.

\item \textit{Annihilation:} We have $0_{\mathbb{K}}a=a0_{\mathbb{K}%
}=0_{\mathbb{K}}$ for all $a\in\mathbb{K}$.

\item \textit{Distributivity:} We have $a\left(  b+c\right)  =ab+ac$ and
$\left(  a+b\right)  c=ac+bc$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$ and
$c\in\mathbb{K}$.
\end{itemize}
\end{definition}

\footnotetext{i.e., we write $a+b$ for the image of $\left(  a,b\right)
\in\mathbb{K}\times\mathbb{K}$ under the binary operation called
\textquotedblleft addition\textquotedblright, and we write $a\cdot b$ for the
image of $\left(  a,b\right)  \in\mathbb{K}\times\mathbb{K}$ under the binary
operation called \textquotedblleft multiplication\textquotedblright}(Some of
these axioms are redundant, in the sense that they can be derived from others.
For instance, the equality $\left(  a+b\right)  c=ac+bc$ can be derived from
the axiom $a\left(  b+c\right)  =ab+ac$ using commutativity of multiplication.
Also, annihilation follows from the other axioms\footnote{In fact, let
$a\in\mathbb{K}$. Distributivity yields $\left(  0_{\mathbb{K}}+0_{\mathbb{K}%
}\right)  a=0_{\mathbb{K}}a+0_{\mathbb{K}}a$, so that $0_{\mathbb{K}%
}a+0_{\mathbb{K}}a=\underbrace{\left(  0_{\mathbb{K}}+0_{\mathbb{K}}\right)
}_{\substack{=0_{\mathbb{K}}\\\text{(by neutrality of }0_{\mathbb{K}}\text{)}%
}}a=0_{\mathbb{K}}a$. Adding $-\left(  0_{\mathbb{K}}a\right)  $ on the left,
we obtain $-\left(  0_{\mathbb{K}}a\right)  +\left(  0_{\mathbb{K}%
}a+0_{\mathbb{K}}a\right)  =-\left(  0_{\mathbb{K}}a\right)  +0_{\mathbb{K}}%
a$. But $-\left(  0_{\mathbb{K}}a\right)  +0_{\mathbb{K}}a=0_{\mathbb{K}}$ (by
the definition of $-\left(  0_{\mathbb{K}}a\right)  $), and associativity of
addition shows that $-\left(  0_{\mathbb{K}}a\right)  +\left(  0_{\mathbb{K}%
}a+0_{\mathbb{K}}a\right)  =\underbrace{\left(  -\left(  0_{\mathbb{K}%
}a\right)  +0_{\mathbb{K}}a\right)  }_{=0_{\mathbb{K}}}+0_{\mathbb{K}%
}a=0_{\mathbb{K}}+0_{\mathbb{K}}a=0_{\mathbb{K}}a$ (by neutrality of
$0_{\mathbb{K}}$), so that $0_{\mathbb{K}}a=-\left(  0_{\mathbb{K}}a\right)
+\left(  0_{\mathbb{K}}a+0_{\mathbb{K}}a\right)  =-\left(  0_{\mathbb{K}%
}a\right)  +0_{\mathbb{K}}a=0_{\mathbb{K}}$. Thus, $0_{\mathbb{K}%
}a=0_{\mathbb{K}}$ is proven. Similarly one can show $a0_{\mathbb{K}%
}=0_{\mathbb{K}}$. Therefore, annihilation follows from the other axioms.}.
The reasons why we have chosen these axioms and not fewer (or more, or others)
are somewhat a matter of taste. For example, I like to explicitly require
annihilation, because it is an important axiom in the definition of a
\textit{semiring}, where it no longer follows from the others.)

\begin{definition}
As we have seen in Definition \ref{def.commring}, a commutative ring consists
of a set $\mathbb{K}$, two binary operations on this set named $+$ and $\cdot
$, and two elements of this set named $0$ and $1$. Thus, formally speaking, we
should encode a commutative ring as the $5$-tuple $\left(  \mathbb{K}%
,+,\cdot,0_{\mathbb{K}},1_{\mathbb{K}}\right)  $. Sometimes we will actually
do so; but most of the time, we will refer to the commutative ring just as the
\textquotedblleft commutative ring $\mathbb{K}$\textquotedblright, hoping that
the other four entries of the $5$-tuple (namely, $+$, $\cdot$, $0_{\mathbb{K}%
}$ and $1_{\mathbb{K}}$) are clear from the context. This kind of abbreviation
is commonplace in mathematics; it is called \textquotedblleft\textit{pars pro
toto}\textquotedblright\ (because we are referring to a large structure by the
same symbol as for a small part of it, and hoping that the rest can be
inferred from the context). It is an example of what is called
\textquotedblleft abuse of notation\textquotedblright.

The elements $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$ of a commutative ring
$\mathbb{K}$ are called the \textit{zero} and the \textit{unity}%
\footnotemark\ of $\mathbb{K}$. They are usually denoted by $0$ and $1$
(without the subscript $\mathbb{K}$) when this can cause no confusion (and,
unfortunately, often also when it can). They are not always identical with the
actual integers $0$ and $1$.

The binary operations $+$ and $\cdot$ in Definition \ref{def.commring} are
also usually not identical with the binary operations $+$ and $\cdot$ on the
set of integers, and are denoted by $+_{\mathbb{K}}$ and $\cdot_{\mathbb{K}}$
when confusion can arise.

The set $\mathbb{K}$ is called the \textit{underlying set} of the commutative
ring $\mathbb{K}$. Let us again remind ourselves that the underlying set of a
commutative ring $\mathbb{K}$ is just a part of the data of $\mathbb{K}$.
\end{definition}

\footnotetext{Some people say \textquotedblleft unit\textquotedblright%
\ instead of \textquotedblleft unity\textquotedblright, but other people use
the word \textquotedblleft unit\textquotedblright\ for something different,
which makes every use of this word a potential pitfall.}Here are some examples
and non-examples of rings:\footnote{The following list of examples is long,
and some of these examples rely on knowledge that you might not have yet. As
usual with examples, you need not understand them all. When I say that Laurent
polynomial rings are examples of commutative rings, I do not assume that you
know what Laurent polynomials are; I merely want to ensure that, \textbf{if}
you have already encountered Laurent polynomials, then you get to know that
they form a commutative ring.}

\begin{itemize}
\item The sets $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$
(endowed with the usual addition, the usual multiplication, the usual $0$ and
the usual $1$) are commutative rings. (Notice that existence of
\textbf{multiplicative} inverses is not required!)

\item The set $\mathbb{N}$ of nonnegative integers (again endowed with the
usual addition, the usual multiplication, the usual $0$ and the usual $1$) is
\textbf{not} a commutative ring. It fails the existence of additive inverses.
(Of course, negative numbers exist, but this does not count because they don't
lie in $\mathbb{N}$.)

\item We can define a commutative ring $\mathbb{Z}^{\prime}$ as follows: We
define a binary operation $\widetilde{\times}$ on $\mathbb{Z}$ (written infix)
by
\[
\left(  a\widetilde{\times}b=-ab\ \ \ \ \ \ \ \ \ \ \text{for all }\left(
a,b\right)  \in\mathbb{Z}\times\mathbb{Z}\right)  .
\]
Now, let $\mathbb{Z}^{\prime}$ be the \textbf{set} $\mathbb{Z}$, endowed with
the usual addition $+$ and the (unusual) multiplication $\widetilde{\times}$,
with the zero $0_{\mathbb{Z}^{\prime}}=0$ and with the unity $1_{\mathbb{Z}%
^{\prime}}=-1$. It is easy to check that $\mathbb{Z}^{\prime}$ is a
commutative ring\footnote{Notice that we have named this new commutative ring
$\mathbb{Z}^{\prime}$, not $\mathbb{Z}$ (despite having $\mathbb{Z}^{\prime
}=\mathbb{Z}$ as sets). The reason is that if we had named it $\mathbb{Z}$,
then we could no longer speak of \textquotedblleft the commutative ring
$\mathbb{Z}$\textquotedblright\ without being ambiguous (we would have to
specify every time whether we mean the usual multiplication or the unusual
one).}; it is an example of a commutative ring whose unity is clearly
\textbf{not} equal to the integer $1$ (which is why it is important to never
omit the subscript $\mathbb{Z}^{\prime}$ in $1_{\mathbb{Z}^{\prime}}$ here).

That said, $\mathbb{Z}^{\prime}$ is not a very interesting ring: It is
essentially \textquotedblleft a copy of $\mathbb{Z}$, except that every
integer $n$ has been renamed as $-n$\textquotedblright. To formalize this
intuition, we would need to introduce the notion of a \textit{ring
isomorphism}, which we don't want to do right here; but the idea is that the
bijection%
\[
\varphi:\mathbb{Z}\rightarrow\mathbb{Z}^{\prime},\ \ \ \ \ \ \ \ \ \ n\mapsto
-n
\]
satisfies%
\begin{align*}
\varphi\left(  a+b\right)   &  =\varphi\left(  a\right)  +\varphi\left(
b\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }\left(  a,b\right)  \in
\mathbb{Z}\times\mathbb{Z};\\
\varphi\left(  a\cdot b\right)   &  =\varphi\left(  a\right)
\widetilde{\times}\varphi\left(  b\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}\left(  a,b\right)  \in\mathbb{Z}\times\mathbb{Z};\\
\varphi\left(  0\right)   &  =0_{\mathbb{Z}^{\prime}};\\
\varphi\left(  1\right)   &  =1_{\mathbb{Z}^{\prime}},
\end{align*}
and thus the ring $\mathbb{Z}^{\prime}$ can be viewed as the ring $\mathbb{Z}$
with its elements \textquotedblleft relabelled\textquotedblright\ using this bijection.

\item The polynomial rings $\mathbb{Z}\left[  x\right]  $, $\mathbb{Q}\left[
a,b\right]  $, $\mathbb{C}\left[  z_{1},z_{2},\ldots,z_{n}\right]  $ are
commutative rings. Laurent polynomial rings are also commutative rings. (Do
not worry if you have not seen these rings yet.)

\item The set of all functions $\mathbb{Q}\rightarrow\mathbb{Q}$ is a
commutative ring, where addition and multiplication are defined pointwise
(i.e., addition is defined by $\left(  f+g\right)  \left(  x\right)  =f\left(
x\right)  +g\left(  x\right)  $ for all $x\in\mathbb{Q}$, and multiplication
is defined by $\left(  fg\right)  \left(  x\right)  =f\left(  x\right)  \cdot
g\left(  x\right)  $ for all $x\in\mathbb{Q}$), where the zero is the
\textquotedblleft constant-$0$\textquotedblright\ function (sending every
$x\in\mathbb{Q}$ to $0$), and where the unity is the \textquotedblleft
constant-$1$\textquotedblright\ function (sending every $x\in\mathbb{Q}$ to
$1$). Of course, the same construction works if we consider functions
$\mathbb{R}\rightarrow\mathbb{C}$, or functions $\mathbb{C}\rightarrow
\mathbb{Q}$, or functions $\mathbb{N}\rightarrow\mathbb{Q}$, instead of
functions $\mathbb{Q}\rightarrow\mathbb{Q}$.\ \ \ \ \footnote{But not if we
consider functions $\mathbb{Q}\rightarrow\mathbb{N}$; such functions might
fail the existence of additive inverses.
\par
Generally, if $X$ is any set and $\mathbb{K}$ is any commutative ring, then
the set of all functions $X\rightarrow\mathbb{K}$ is a commutative ring, where
addition and multiplication are defined pointwise, where the zero is the
\textquotedblleft constant-$0_{\mathbb{K}}$\textquotedblright\ function, and
where the unity is the \textquotedblleft constant-$1_{\mathbb{K}}%
$\textquotedblright\ function.}

\item The set $\mathbb{S}$ of all real numbers of the form $a+b\sqrt{5}$ with
$a,b\in\mathbb{Q}$ (endowed with the usual notions of \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright\ defined on $\mathbb{R}$) is a commutative
ring\footnote{To prove this, we argue as follows:
\par
\begin{itemize}
\item Addition and multiplication are indeed two binary operations on
$\mathbb{S}$. This is because the sum of two elements of $\mathbb{S}$ is an
element of $\mathbb{S}$ (namely, $\left(  a+b\sqrt{5}\right)  +\left(
c+d\sqrt{5}\right)  =\left(  a+c\right)  +\left(  b+d\right)  \sqrt{5}$), and
so is their product (namely, $\left(  a+b\sqrt{5}\right)  \cdot\left(
c+d\sqrt{5}\right)  =\left(  ac+5bd\right)  +\left(  bc+ad\right)  \sqrt{5}$).
\par
\item All axioms of a commutative ring are satisfied for $\mathbb{S}$, except
maybe the existence of additive inverses. This is simply because the addition
and the multiplication in $\mathbb{S}$ are \textquotedblleft
inherited\textquotedblright\ from $\mathbb{R}$, and clearly all these axioms
come with the inheritance.
\par
\item Existence of additive inverses also holds in $\mathbb{S}$, because the
additive inverse of $a+b\sqrt{5}$ is $\left(  -a\right)  +\left(  -b\right)
\sqrt{5}$.
\end{itemize}
}.

\item We could define a different ring structure on the set $\mathbb{S}$ (that
is, a commutative ring which, as a set, is identical with $\mathbb{S}$, but
has a different choice of operations) as follows: We define a binary operation
$\ast$ on $\mathbb{S}$ by setting%
\[
\left(  a+b\sqrt{5}\right)  \ast\left(  c+d\sqrt{5}\right)  =ac+bd\sqrt
{5}\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  a,b\right)  \in\mathbb{Q}%
\times\mathbb{Q}.
\]
\footnote{This is well-defined, because every element of $\mathbb{S}$ can be
written in the form $a+b\sqrt{5}$ for a \textbf{unique} pair $\left(
a,b\right)  \in\mathbb{Q}\times\mathbb{Q}$. This is a consequence of the
irrationality of $\sqrt{5}$.} Now, let $\mathbb{S}^{\prime}$ be the set
$\mathbb{S}$, endowed with the usual addition $+$ and the (unusual)
multiplication $\ast$, with the zero $0_{\mathbb{S}^{\prime}}=0$ and with the
unity $1_{\mathbb{S}^{\prime}}=1+\sqrt{5}$ (not the integer $1$). It is easy
to check that $\mathbb{S}^{\prime}$ is a commutative ring\footnote{Again, we
do not call it $\mathbb{S}$, in order to be able to distinguish between
different ring structures.}. The \textbf{sets} $\mathbb{S}$ and $\mathbb{S}%
^{\prime}$ are identical, but the \textbf{commutative rings} $\mathbb{S}$ and
$\mathbb{S}^{\prime}$ are not\footnote{Keep in mind that, due to our
\textquotedblleft pars pro toto\textquotedblright\ notation, \textquotedblleft
commutative ring $\mathbb{S}$\textquotedblright\ means more than
\textquotedblleft set $\mathbb{S}$\textquotedblright.}: For example, the ring
$\mathbb{S}^{\prime}$ has two nonzero elements whose product is $0$ (namely,
$1\ast\sqrt{5}=0$), whereas the ring $\mathbb{S}$ has no such things. This
shows that not only do we have $\mathbb{S}^{\prime}\neq\mathbb{S}$ as
commutative rings, but there is also no way to regard $\mathbb{S}^{\prime}$ as
\textquotedblleft a copy of $\mathbb{S}$ with its elements
renamed\textquotedblright\ (in the same way as we have regarded $\mathbb{Z}%
^{\prime}$ as \textquotedblleft a copy of $\mathbb{Z}$ with its elements
renamed\textquotedblright). This example should stress the point that a
commutative ring $\mathbb{K}$ is not just a set; it is a set endowed with two
operations ($+$ and $\cdot$) and two elements ($0_{\mathbb{K}}$ and
$1_{\mathbb{K}}$), and these operations and elements are no less important
than the set.

\item The set $\mathbb{S}_{3}$ of all real numbers of the form $a+b\sqrt[3]%
{5}$ with $a,b\in\mathbb{Q}$ (endowed with the usual addition, the usual
multiplication, the usual $0$ and the usual $1$) is \textbf{not} a commutative
ring. Indeed, multiplication is not a binary operation on this set
$\mathbb{S}_{3}$: It does not always send two elements of $\mathbb{S}_{3}$ to
an element of $\mathbb{S}_{3}$. For instance, $\left(  1+1\sqrt[3]{5}\right)
\left(  1+1\sqrt[3]{5}\right)  =1+2\sqrt[3]{5}+\left(  \sqrt[3]{5}\right)
^{2}$ is not in $\mathbb{S}_{3}$.

\item The set of all $2\times2$-matrices over $\mathbb{Q}$ is \textbf{not} a
commutative ring, because commutativity of multiplication does not hold for
this set. (In general, $AB\neq BA$ for matrices.)

\item If you like the empty set, you will enjoy the \textit{zero ring}. This
is the commutative ring which is defined as the one-element set $\left\{
0\right\}  $, with zero and unity both being $0$ (nobody said that they have
to be distinct!), with addition given by $0+0=0$ and with multiplication given
by $0\cdot0=0$. Of course, it is not an empty set\footnote{A commutative ring
cannot be empty, as it contains at least one element (namely, $0$).}, but it
plays a similar role in the world of commutative rings as the empty set does
in the world of sets: It carries no information itself, but things would break
if it were to be excluded\footnote{Some authors \textbf{do} prohibit the zero
ring from being a commutative ring (by requiring every commutative ring to
satisfy $0\neq1$). I think most of them run into difficulties from this
decision sooner or later.}.

Notice that the zero and the unity of the zero ring are identical, i.e., we
have $0_{\mathbb{K}}=1_{\mathbb{K}}$. This shows why it is dangerous to omit
the subscripts and just denote the zero and the unity by $0$ and $1$; in fact,
you don't want to rewrite the equality $0_{\mathbb{K}}=1_{\mathbb{K}}$ as
\textquotedblleft$0=1$\textquotedblright! (Most algebraists make a compromise
between wanting to omit the subscripts and having to clarify what $0$ and $1$
mean: They say that \textquotedblleft$0=1$ in $\mathbb{K}$\textquotedblright%
\ to mean \textquotedblleft$0_{\mathbb{K}}=1_{\mathbb{K}}$\textquotedblright.)

Generally, a \textit{trivial ring} is defined to be a commutative ring
containing only one element (which then necessarily is both the zero and the
unity of this ring). The addition and the multiplication of a trivial ring are
uniquely determined (since there is only one possible value that a sum or a
product could take). Every trivial ring can be viewed as the zero ring with
its element $0$ relabelled.\footnote{In more formal terms, the preceding
statement would say that \textquotedblleft every trivial ring is isomorphic to
the zero ring\textquotedblright.}

\item In set theory, the \textit{symmetric difference} of two sets $A$ and $B$
is defined to be the set $\left(  A\cup B\right)  \setminus\left(  A\cap
B\right)  =\left(  A\setminus B\right)  \cup\left(  B\setminus A\right)  $.
This symmetric difference is denoted by $A\bigtriangleup B$. Now, let $S$ be
any set. Let $2^{S}$ denote the powerset of $S$ (that is, the set of all
subsets of $S$). It is easy to check that the following ten properties hold:%
\begin{align*}
A\bigtriangleup B  &  =B\bigtriangleup A\ \ \ \ \ \ \ \ \ \ \text{for any sets
}A\text{ and }B\text{;}\\
A\cap B  &  =B\cap A\ \ \ \ \ \ \ \ \ \ \text{for any sets }A\text{ and
}B\text{;}\\
\left(  A\bigtriangleup B\right)  \bigtriangleup C  &  =A\bigtriangleup\left(
B\bigtriangleup C\right)  \ \ \ \ \ \ \ \ \ \ \text{for any sets }A\text{,
}B\text{ and }C\text{;}\\
\left(  A\cap B\right)  \cap C  &  =A\cap\left(  B\cap C\right)
\ \ \ \ \ \ \ \ \ \ \text{for any sets }A\text{, }B\text{ and }C\text{;}\\
A\bigtriangleup\varnothing &  =\varnothing\bigtriangleup
A=A\ \ \ \ \ \ \ \ \ \ \text{for any set }A\text{;}\\
A\bigtriangleup A  &  =\varnothing\ \ \ \ \ \ \ \ \ \ \text{for any set
}A\text{;}\\
A\cap S  &  =S\cap A=A\ \ \ \ \ \ \ \ \ \ \text{for any subset }A\text{ of
}S\text{;}\\
\varnothing\cap A  &  =A\cap\varnothing=\varnothing
\ \ \ \ \ \ \ \ \ \ \text{for any set }A\text{;}\\
A\cap\left(  B\bigtriangleup C\right)   &  =\left(  A\cap B\right)
\bigtriangleup\left(  A\cap C\right)  \ \ \ \ \ \ \ \ \ \ \text{for any sets
}A\text{, }B\text{ and }C\text{;}\\
\left(  A\bigtriangleup B\right)  \cap C  &  =\left(  A\cap C\right)
\bigtriangleup\left(  B\cap C\right)  \ \ \ \ \ \ \ \ \ \ \text{for any sets
}A\text{, }B\text{ and }C\text{.}%
\end{align*}
Therefore, $2^{S}$ becomes a commutative ring, where the addition is defined
to be the operation $\bigtriangleup$, the multiplication is defined to be the
operation $\cap$, the zero is defined to be the set $\varnothing$, and the
unity is defined to be the set $S$.\ \ \ \ \footnote{The ten properties listed
above show that the axioms of a commutative ring are satisfied for $\left(
2^{S},\bigtriangleup,\cap,\varnothing,S\right)  $. In particular, the sixth
property shows that every subset $A$ of $S$ has an additive inverse -- namely,
itself. Of course, it is unusual for an element of a commutative ring to be
its own additive inverse, but in this example it happens all the time!}

The commutative ring $2^{S}$ has the property that $a^{2}=a$ for every
$a\in2^{S}$. (This simply means that $A\cap A=A$ for every $A\subseteq S$.)
Commutative rings that have this property are called
\textit{\href{https://en.wikipedia.org/wiki/Boolean_ring}{\textit{Boolean
rings}}}. (Of course, $2^{S}$ is the eponymic example for a Boolean ring; but
there are also others.)

\item For every positive integer $n$, the residue classes of integers modulo
$n$ form a commutative ring, which is called $\mathbb{Z}/n\mathbb{Z}$ or
$\mathbb{Z}_{n}$ (depending on the author). This ring has $n$ elements (often
called \textquotedblleft integers modulo $n$\textquotedblright). When $n$ is a
composite number (e.g., $n=6$), this ring has the property that products of
nonzero\footnote{An element $a$ of a commutative ring $\mathbb{K}$ is said to
be \textit{nonzero} if $a\neq0_{\mathbb{K}}$. (This is not the same as saying
that $a$ is not the integer $0$, because the integer $0$ might not be
$0_{\mathbb{K}}$.)} elements can be zero (e.g., we have $2\cdot3\equiv
0\operatorname{mod}6$); this means that there is no way to define division by
all nonzero elements in this ring (even if we are allowed to create
fractions). Notice that $\mathbb{Z}/1\mathbb{Z}$ is a trivial ring.

We notice that if $n$ is a positive integer, and if $\mathbb{K}$ is the
commutative ring $\mathbb{Z}/n\mathbb{Z}$, then $\underbrace{1_{\mathbb{K}%
}+1_{\mathbb{K}}+\cdots+1_{\mathbb{K}}}_{n\text{ times}}=0_{\mathbb{K}}$
(because the left hand side of this equality is the residue class of $n$
modulo $n$, while the right hand side is the residue class of $0$ modulo $n$,
and these two residue classes are clearly equal).

\item Let us try to define \textquotedblleft division by
zero\textquotedblright. So, we introduce a new symbol $\infty$, and we try to
extend the addition on $\mathbb{Q}$ to the set $\mathbb{Q}\cup\left\{
\infty\right\}  $ by setting $a+\infty=\infty$ for all $a\in\mathbb{Q}%
\cup\left\{  \infty\right\}  $. We might also try to extend the multiplication
in some way, and perhaps to add some more elements (such as another symbol
$-\infty$ to serve as the product $\left(  -1\right)  \infty$). I claim that
(whatever we do with the multiplication, and whatever new elements we add) we
do not get a commutative ring. Indeed, assume the contrary. Thus, there exists
a commutative ring $\mathbb{W}$ which contains $\mathbb{Q}\cup\left\{
\infty\right\}  $ as a subset, and which has $a+\infty=\infty$ for all
$a\in\mathbb{Q}$. Thus, in $\mathbb{W}$, we have $1+\infty=\infty=0+\infty$.
Adding $\left(  -1\right)  \infty$ to both sides of this equality, we obtain
$1+\infty+\left(  -1\right)  \infty=0+\infty+\left(  -1\right)  \infty$, so
that $1=0$\ \ \ \ \footnote{because $\infty+\left(  -1\right)  \infty
=1\infty+\left(  -1\right)  \infty=\underbrace{\left(  1+\left(  -1\right)
\right)  }_{=0}\infty=0\infty=0$}; but this is absurd. Hence, we have found a
contradiction. This is why \textquotedblleft division by zero is
impossible\textquotedblright: One can define objects that behave like
\textquotedblleft infinity\textquotedblright\ (and they \textbf{are} useful),
but they break various standard rules such as the axioms of a commutative
ring. In contrast to this, adding a \textquotedblleft number\textquotedblright%
\ $i$ satisfying $i^{2}=-1$ to the real numbers is harmless: The complex
numbers $\mathbb{C}$ are still a commutative ring.

\item Here is an \textquotedblleft almost-ring\textquotedblright\ beloved to
many combinatorialists: The \textit{max-plus semiring} $\mathbb{T}$ (also
called the \textit{tropical semiring}\footnote{Caution: Both of these names
mean many other things as well.}). We create a new symbol $-\infty$, and we
set $\mathbb{T}=\mathbb{Z}\cup\left\{  -\infty\right\}  $ as sets, but we do
\textbf{not} \textquotedblleft inherit\textquotedblright\ the addition and the
multiplication from $\mathbb{Z}$. Instead, we denote the \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright\ operations on $\mathbb{Z}$ by $+_{\mathbb{Z}%
}$ and $\cdot_{\mathbb{Z}}$, and we define two new \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright\ operations $+_{\mathbb{T}}$ and
$\cdot_{\mathbb{T}}$ on $\mathbb{T}$ as follows:%
\begin{align*}
a+_{\mathbb{T}}b  &  =\max\left\{  a,b\right\}  ;\\
a\cdot_{\mathbb{T}}b  &  =a+_{\mathbb{Z}}b.
\end{align*}
(Here, we set $\max\left\{  -\infty,n\right\}  =\max\left\{  n,-\infty
\right\}  =n$ and $\left(  -\infty\right)  +_{\mathbb{Z}}n=n+_{\mathbb{Z}%
}\left(  -\infty\right)  =-\infty$ for every $n\in\mathbb{T}$.)

It turns out that the set $\mathbb{T}$ endowed with the two operations
$+_{\mathbb{Z}}$ and $\cdot_{\mathbb{Z}}$, the zero $0_{\mathbb{T}}=-\infty$
and the unity $1_{\mathbb{T}}=0$ comes rather close to being a commutative
ring. It satisfies all axioms of a commutative ring except for the existence
of additive inverses. Such a structure is called a \textit{semiring}. Other
examples of semirings are $\mathbb{N}$ and a reasonably defined $\mathbb{N}%
\cup\left\{  \infty\right\}  $ (with $0\infty=0$ and $a\infty=\infty$ for all
$a>0$).
\end{itemize}

If $\mathbb{K}$ is a commutative ring, then we can define a subtraction in
$\mathbb{K}$, even though we have not required a subtraction operation as part
of the definition of a commutative ring $\mathbb{K}$. Namely, the
\textit{subtraction} of a commutative ring $\mathbb{K}$ is the binary
operation $-$ on $\mathbb{K}$ (again written infix) defined as follows: For
every $a \in\mathbb{K}$ and $b \in\mathbb{K}$, set $a-b = a+b^{\prime}$, where
$b^{\prime}$ is the additive inverse of $b$. It is not hard to check that
$a-b$ is the unique element $c$ of $\mathbb{K}$ satisfying $a = b+c$; thus,
subtraction is ``the undoing of addition'' just as in the classical situation
of integers. Again, the notation $-$ for the subtraction of $\mathbb{K}$ is
denoted by $-_{\mathbb{K}}$ whenever a confusion with the subtraction of
integers could arise.

Whenever $a$ is an element of a commutative ring $\mathbb{K}$, we write $-a$
for the additive inverse of $a$. This is the same as $0_{\mathbb{K}} - a$.

The intuition for commutative rings is essentially that all computations that
can be made with the operations $+$, $-$ and $\cdot$ on integers can be
similarly made in a commutative ring. For instance, if $a_{1},a_{2}%
,\ldots,a_{n}$ are $n$ elements of a commutative ring, then the sum
$a_{1}+a_{2}+\cdots+a_{n}$ is well-defined, and can be computed by adding the
elements $a_{1},a_{2},\ldots,a_{n}$ to each other in any order\footnote{For
instance, we can compute the sum $a+b+c+d$ of four elements $a,b,c,d$ in many
ways: For example, we can first add $a$ and $b$, then add $c$ and $d$, and
finally add the two results; alternatively, we can first add $a$ and $b$, then
add $d$ to the result, then add $c$ to the result. In a commutative ring, all
such ways lead to the same result. To prove this is a slightly tedious
induction argument that uses commutativity and associativity.}. The same holds
for products. If $n$ is an integer and $a$ is an element of a commutative ring
$\mathbb{K}$, then we define an element $na$ of $\mathbb{K}$ by%
\[
na=%
\begin{cases}
\underbrace{a+a+\cdots+a}_{n\text{ addends}}, & \text{if }n\geq0;\\
-\underbrace{a+a+\cdots+a}_{-n\text{ addends}}, & \text{if }n<0
\end{cases}
.
\]
\footnote{Notice that this definition of $na$ is \textbf{not} a particular
case of the product of two elements of $\mathbb{K}$, because $n$ is not an
element of $\mathbb{K}$.}

If $n$ is a nonnegative integer and $a$ is an element of a commutative ring
$\mathbb{K}$, then $a^{n}$ is a well-defined element of $\mathbb{K}$ (namely,
$a^{n}=\underbrace{a\cdot a\cdot\cdots\cdot a}_{n\text{ factors}}$). The
following identities hold:%
\begin{align}
-\left(  a+b\right)   &  =\left(  -a\right)  +\left(  -b\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K};\label{eq.rings.-(a+b)}\\
-\left(  -a\right)   &  =a\ \ \ \ \ \ \ \ \ \ \text{for }a\in\mathbb{K}%
;\label{eq.rings.-(-a)}\\
-\left(  ab\right)   &  =\left(  -a\right)  b=a\left(  -b\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K};\label{eq.rings.-(ab)}\\
-\left(  na\right)   &  =\left(  -n\right)  a=n\left(  -a\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a\in\mathbb{K}\text{ and }n\in\mathbb{Z}%
;\label{eq.rings.-(na)}\\
n\left(  ab\right)   &  =\left(  na\right)  b=a\left(  nb\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K}\text{ and }n\in\mathbb{Z}%
;\label{eq.rings.nab}\\
\left(  nm\right)  a  &  =n\left(  ma\right)  \ \ \ \ \ \ \ \ \ \ \text{for
}a\in\mathbb{K}\text{ and }n,m\in\mathbb{Z};\label{eq.rings.nma}\\
0^{n}  &  =%
\begin{cases}
0, & \text{if }n>0;\\
1, & \text{if }n=0
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for }n\in\mathbb{N};\label{eq.rings.0**n}\\
a^{n+m}  &  =a^{n}a^{m}\ \ \ \ \ \ \ \ \ \ \text{for }a\in\mathbb{K}\text{ and
}n,m\in\mathbb{N};\label{eq.rings.a**(n+m)}\\
\left(  ab\right)  ^{n}  &  =a^{n}b^{n}\ \ \ \ \ \ \ \ \ \ \text{for }%
a,b\in\mathbb{K}\text{ and }n\in\mathbb{N};\label{eq.rings.-(ab)**n}\\
\left(  a+b\right)  ^{n}  &  =\sum_{k=0}^{n}\dbinom{n}{k}a^{k}b^{n-k}%
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K}\text{ and }n\in\mathbb{N}.
\label{eq.rings.(a+b)**n}%
\end{align}
Here, we are using the standard notations $+$, $\cdot$, $0$ and $1$ for the
addition, the multiplication, the zero and the unity of $\mathbb{K}$, because
confusion (e.g., confusion of the $0$ with the integer $0$) is rather
unlikely.\footnote{For instance, in the statement \textquotedblleft$-\left(
a+b\right)  =\left(  -a\right)  +\left(  -b\right)  $ for $a,b\in\mathbb{K}%
$\textquotedblright, it is clear that the $+$ can only stand for the addition
of $\mathbb{K}$ and not (say) for the addition of integers (since $a$, $b$,
$-a$ and $-b$ are elements of $\mathbb{K}$, not (generally) integers). The
only statement whose meaning is ambiguous is \textquotedblleft$0^{n}=%
\begin{cases}
0, & \text{if }n>0;\\
1, & \text{if }n=0.
\end{cases}
$ for $n\in\mathbb{N}$\textquotedblright. In this statement, the
\textquotedblleft$0$\textquotedblright\ in \textquotedblleft$n>0$%
\textquotedblright\ and the \textquotedblleft$0$\textquotedblright\ in
\textquotedblleft$n=0$\textquotedblright\ clearly mean the integer $0$ (since
they are being compared with the integer $n$), but the other two appearances
of \textquotedblleft$0$\textquotedblright\ and the \textquotedblleft%
$1$\textquotedblright\ are ambiguous. I hope that the context makes it clear
enough that they mean the zero and the unity of $\mathbb{K}$ (and not the
integers $0$ and $1$).} We shall keep doing so in the following, apart from
situations where confusion can realistically occur.\footnote{Notice that the
equalities (\ref{eq.rings.nab}) and (\ref{eq.rings.nma}) are \textbf{not}
particular cases of the associativity of multiplication which we required to
hold for $\mathbb{K}$. Indeed, the latter associativity says that $a\left(
bc\right)  =\left(  ab\right)  c$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$
and $c\in\mathbb{K}$. But in (\ref{eq.rings.nab}) and (\ref{eq.rings.nma}),
the $n$ is an integer, not an element of $\mathbb{K}$.}

Furthermore, finite sums such as $\sum_{s\in S}a_{s}$ (where $S$ is a finite
set, and $a_{s}\in\mathbb{K}$ for every $s\in S$), and finite products such as
$\prod_{s\in S}a_{s}$ (where $S$ is a finite set, and $a_{s}\in\mathbb{K}$ for
every $s\in S$) are defined whenever $\mathbb{K}$ is a commutative ring.
Again, the definition is the same as for numbers, and these sums and products
behave as they do for numbers. For example, Exercise
\ref{exe.perm.sign.pseudoexplicit} still holds if we replace \textquotedblleft%
$\mathbb{C}$\textquotedblright\ by \textquotedblleft$\mathbb{K}$%
\textquotedblright\ in it (and the same solution proves it) whenever
$\mathbb{K}$ is a commutative ring.

\begin{remark}
The notion of a \textquotedblleft commutative ring\textquotedblright\ is not
fully standardized; there exist several competing definitions:

For some people, a \textquotedblleft commutative ring\textquotedblright\ is
\textit{not} endowed with an element $1$ (although it \textbf{can} have such
an element), and, consequently, does not have to satisfy the unitality axiom.
According to their definition, for example, the set $\left\{  0,2,4,6,\ldots
\right\}  =\left\{  2n\ \mid\ n\in\mathbb{N}\right\}  $ is a commutative ring
(with the usual addition and multiplication). (In contrast, our definition of
a \textquotedblleft commutative ring\textquotedblright\ does not accept
$\left\{  0,2,4,6,\ldots\right\}  $ as a commutative ring, because it does not
contain any element which would fill the role of $1$.) These people tend to
use the notation \textquotedblleft commutative ring with
unity\textquotedblright\ (or \textquotedblleft commutative ring with
$1$\textquotedblright) to mean a commutative ring which is endowed with a $1$
and satisfies the unitality axiom (i.e., what we call a \textquotedblleft
commutative ring\textquotedblright).

On the other hand, there are authors who use the word \textquotedblleft
ring\textquotedblright\ for what we call \textquotedblleft commutative
ring\textquotedblright. These are mostly the authors who work with commutative
rings all the time and find the name \textquotedblleft commutative
ring\textquotedblright\ too long.

When you are reading about rings, it is important to know which meaning of
\textquotedblleft ring\textquotedblright\ the author is subscribing to. (Often
this can be inferred from the examples given.)
\end{remark}

\subsection{Matrices}

We have briefly defined determinants in Definition \ref{def.det.old}, but we
haven't done much with them. This will be amended now. But let us first recall
the definitions of basic notions in matrix algebra.

In the following, we fix a commutative ring $\mathbb{K}$. The elements of
$\mathbb{K}$ will be called \textit{scalars} (to distinguish them from
\textit{vectors} and \textit{matrices}, which we will soon discuss, and which
are structures containing several elements of $\mathbb{K}$).

If you feel uncomfortable with commutative rings, you are free to think that
$\mathbb{K}=\mathbb{Q}$ or $\mathbb{K}=\mathbb{C}$ in the following; but
everything I am doing works for any commutative ring unless stated otherwise.

Given two nonnegative integers $n$ and $m$, an $n\times m$\textit{-matrix}
(or, more precisely, $n\times m$\textit{-matrix over} $\mathbb{K}$) means a
rectangular table with $n$ rows and $m$ columns whose entries are elements of
$\mathbb{K}$.\ \ \ \ \footnote{Formally speaking, this means that an $n\times
m$-matrix is a map from $\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $ to $\mathbb{K}$. We represent such a map as a
rectangular table by writing the image of $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $ into the cell in
the $i$-th row and the $j$-th column.} For instance, when $\mathbb{K}%
=\mathbb{Q}$, the table $\left(
\begin{array}
[c]{ccc}%
1 & -2/5 & 4\\
1/3 & -1/2 & 0
\end{array}
\right)  $ is a $2\times3$-matrix. A \textit{matrix} simply means an $n\times
m$-matrix for some $n\in\mathbb{N}$ and $m\in\mathbb{N}$. These $n$ and $m$
are said to be the \textit{dimensions} of the matrix.

If $A$ is an $n\times m$-matrix, and if $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,m\right\}  $, then the $\left(  i,j\right)
$\textit{-th entry of }$A$ means the entry of $A$ in row $i$ and column $j$.
For instance, the $\left(  1,2\right)  $-th entry of the matrix $\left(
\begin{array}
[c]{ccc}%
1 & -2/5 & 4\\
1/3 & -1/2 & 0
\end{array}
\right)  $ is $-2/5$.

If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, and if we are given an element
$a_{i,j}\in\mathbb{K}$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $, then we use the
notation $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ for the
$n\times m$-matrix whose $\left(  i,j\right)  $-th entry is $a_{i,j}$ for all
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $. Thus,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m}%
\end{array}
\right)  .
\]
The letters $i$ and $j$ are not set in stone; they are bound variables like
the $k$ in \textquotedblleft$\sum_{k=1}^{n}k$\textquotedblright. Thus, you are
free to write $\left(  a_{x,y}\right)  _{1\leq x\leq n,\ 1\leq y\leq m}$ or
$\left(  a_{j,i}\right)  _{1\leq j\leq n,\ 1\leq i\leq m}$ instead of $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ (and we will use this
freedom eventually).\footnote{Many authors love to abbreviate
\textquotedblleft$a_{i,j}$\textquotedblright\ by \textquotedblleft$a_{ij}%
$\textquotedblright\ (hoping that the reader will not mistake the subscript
\textquotedblleft$ij$\textquotedblright\ for a product or (in the case where
$i$ and $j$ are single-digit numbers) for a two-digit number). The only
advantage of this abbreviation that I am aware of is that it saves you a
comma; I do not understand why it is so popular. But you should be aware of it
in case you are reading other texts.}

Matrices can be added if they share the same dimensions: If $n$ and $m$ are
two nonnegative integers, and if $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$ are two $n\times m$-matrices, then $A+B$ means the $n\times
m$-matrix $\left(  a_{i,j}+b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.
Thus, matrices are added \textquotedblleft entry by entry\textquotedblright;
for example, $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  +\left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & c^{\prime}\\
d^{\prime} & e^{\prime} & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a+a^{\prime} & b+b^{\prime} & c+c^{\prime}\\
d+d^{\prime} & e+e^{\prime} & f+f^{\prime}%
\end{array}
\right)  $. Similarly, subtraction is defined: If $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$, then $A-B=\left(  a_{i,j}-b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$.

Similarly, one can define the product of a scalar $\lambda\in\mathbb{K}$ with
a matrix $A$: If $\lambda\in\mathbb{K}$ is a scalar, and if $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ is an $n\times m$-matrix,
then $\lambda A$ means the $n\times m$-matrix $\left(  \lambda a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$.

Defining the product of two matrices is trickier. Matrices are \textbf{not}
multiplied \textquotedblleft entry by entry\textquotedblright; this would not
be a very interesting definition. Instead, their product is defined as
follows: If $n$, $m$ and $\ell$ are three nonnegative integers, then the
product $AB$ of an $n\times m$-matrix $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ with an $m\times\ell$-matrix $B=\left(  b_{i,j}\right)
_{1\leq i\leq m,\ 1\leq j\leq\ell}$ means the $n\times\ell$-matrix%
\[
\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
\ell}.
\]
This definition looks somewhat counterintuitive, so let me comment on it.
First of all, for $AB$ to be defined, $A$ and $B$ are \textbf{not} required to
have the same dimensions; instead, $A$ must have as many columns as $B$ has
rows. The resulting matrix $AB$ then has as many rows as $A$ and as many
columns as $B$. Every entry of $AB$ is a sum of products of an entry of $A$
with an entry of $B$ (not a single such product). More precisely, the $\left(
i,j\right)  $-th entry of $AB$ is a sum of products of an entry in the $i$-th
row of $A$ with the respective entry in the $j$-th column of $B$. For example,%
\[
\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & d^{\prime} & g^{\prime}\\
b^{\prime} & e^{\prime} & h^{\prime}\\
c^{\prime} & f^{\prime} & i^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime}+bb^{\prime}+cc^{\prime} & ad^{\prime}+be^{\prime}+cf^{\prime} &
ag^{\prime}+bh^{\prime}+ci^{\prime}\\
da^{\prime}+eb^{\prime}+fc^{\prime} & dd^{\prime}+ee^{\prime}+ff^{\prime} &
dg^{\prime}+eh^{\prime}+fi^{\prime}%
\end{array}
\right)  .
\]


The multiplication of matrices is not commutative! It is easy to find examples
of two matrices $A$ and $B$ for which the products $AB$ and $BA$ are distinct,
or one of them is well-defined but the other is not\footnote{This happens if
$A$ has as many columns as $B$ has rows, but $B$ does not have as many columns
as $A$ has rows.}.

For every $n\in\mathbb{N}$, we let $I_{n}$ denote the $n\times n$-matrix
$\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, where
$\delta_{i,j}$ is defined to be $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.\ \ \ \ \footnote{Here, $0$ and $1$ mean the zero and the unity of
$\mathbb{K}$ (which may and may not be the integers $0$ and $1$).} This matrix
$I_{n}$ looks as follows:%
\[
I_{n}=\left(
\begin{array}
[c]{cccc}%
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  .
\]
It has the property that $I_{n}B=B$ for every $m\in\mathbb{N}$ and every
$n\times m$-matrix $B$; also, $AI_{n}=A$ for every $k\in\mathbb{N}$ and every
$k\times n$-matrix $A$. (Proving this is a good way to check that you
understand how matrices are multiplied.) The matrix $I_{n}$ is called the
$n\times n$ \textit{identity matrix}. (Some call it $E_{n}$ or just $I$, when
the value of $n$ is clear from the context.)

Matrix multiplication is associative: If $n,m,k,\ell\in\mathbb{N}$, and if $A$
is an $n\times m$-matrix, $B$ is an $m\times k$-matrix, and $C$ is a
$k\times\ell$-matrix, then $A\left(  BC\right)  =\left(  AB\right)  C$. The
proof of this is straightforward using our definition of products of
matrices\footnote{Check that $A\left(  BC\right)  $ and $\left(  AB\right)  C$
both are equal to the matrix $\left(  \sum_{u=1}^{m}\sum_{v=1}^{k}%
a_{i,u}b_{u,v}c_{v,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}$.}. This
associativity allows us to write products like $ABC$ without parentheses. By
induction, we can see that longer products such as $A_{1}A_{2}\cdots A_{k}$
for arbitrary $k\in\mathbb{N}$ can also be bracketed at will, because all
bracketings lead to the same result (e.g., for four matrices $A$, $B$, $C$ and
$D$, we have $A\left(  B\left(  CD\right)  \right)  =A\left(  \left(
BC\right)  D\right)  =\left(  AB\right)  \left(  CD\right)  =\left(  A\left(
BC\right)  \right)  D=\left(  \left(  AB\right)  C\right)  D$, provided that
the dimensions of the matrices are appropriate to make sense of the products).
We define an empty product of $n\times n$-matrices to be the $n\times n$
identity matrix $I_{n}$.

For every $n\times n$-matrix $A$ and every $k\in\mathbb{N}$, we can thus
define an $n\times n$-matrix $A^{k}$ by $A^{k}=\underbrace{AA\cdots
A}_{k\text{ factors}}$. In particular, $A^{0}=I_{n}$ (since we defined an
empty product of $n\times n$-matrices to be $I_{n}$).

Further properties of matrix multiplication are easy to state and to prove:

\begin{itemize}
\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$, $k\in\mathbb{N}$ and
$\lambda\in\mathbb{K}$, every $n\times m$-matrix $A$ and every $m\times
k$-matrix $B$, we have $\lambda\left(  AB\right)  =\left(  \lambda A\right)
B=A\left(  \lambda B\right)  $. (This allows us to write $\lambda AB$ for each
of the matrices $\lambda\left(  AB\right)  $, $\left(  \lambda A\right)  B$
and $A\left(  \lambda B\right)  $.)

\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$ and $k\in\mathbb{N}$, every
two $n\times m$-matrices $A$ and $B$, and every $m\times k$-matrix $C$, we
have $\left(  A+B\right)  C=AC+BC$.

\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$ and $k\in\mathbb{N}$, every
$n\times m$-matrix $A$, and every two $m\times k$-matrices $B$ and $C$, we
have $A\left(  B+C\right)  =AB+AC$.

\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$, $\lambda\in\mathbb{K}$ and
$\mu\in\mathbb{K}$, and every $n\times m$-matrix $A$, we have $\lambda\left(
\mu A\right)  =\left(  \lambda\mu\right)  A$. (This allows us to write
$\lambda\mu A$ for both $\lambda\left(  \mu A\right)  $ and $\left(
\lambda\mu\right)  A$.)
\end{itemize}

For given $n\in\mathbb{N}$ and $m\in\mathbb{N}$, we let $\mathbb{K}^{n\times
m}$ denote the set of all $n\times m$-matrices. (This is one of the two
standard notations for this set; the other is $\operatorname*{M}%
\nolimits_{n,m}\left(  \mathbb{K}\right)  $.)

For given $n\in\mathbb{N}$ and $m\in\mathbb{N}$, we define the $n\times
m$\textit{ zero matrix} to be the $n\times m$-matrix whose all entries are $0$
(that is, the $n\times m$-matrix $\left(  0\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$). We denote this matrix by $0_{n\times m}$. If $A$ is any $n\times
m$-matrix, then the $n\times m$-matrix $-A$ is defined to be $0_{n\times m}-A$.

A \textit{square matrix} is a matrix which has as many rows as it has columns;
in other words, a square matrix is an $n\times n$-matrix for some
$n\in\mathbb{N}$. If $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ is a square matrix, then the $n$-tuple $\left(  a_{1,1},a_{2,2}%
,\ldots,a_{n,n}\right)  $ is called the \textit{diagonal} of $A$. (Some
authors abbreviate $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
by $\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$; this notation has some mild
potential for confusion, though\footnote{The comma between \textquotedblleft%
$i$\textquotedblright\ and \textquotedblleft$j$\textquotedblright\ in
\textquotedblleft$1\leq i,j\leq n$\textquotedblright\ can be understood either
to separate $i$ from $j$, or to separate the inequality $1\leq i$ from the
inequality $j\leq n$. I remember seeing this ambiguity causing a real
misunderstanding.}.) The entries of the diagonal of $A$ are called the
\textit{diagonal entries} of $A$.

For a given $n\in\mathbb{N}$, the product of two $n\times n$-matrices is
always well-defined, and is an $n\times n$-matrix again. The set
$\mathbb{K}^{n\times n}$ satisfies all the axioms of a commutative ring except
for commutativity of multiplication. This makes it into what is commonly
called a \textit{noncommutative ring}\footnote{A \textit{noncommutative ring}
is defined in the same way as we defined a commutative ring, except for the
fact that commutativity of multiplication is removed from the list of axioms.
(The words \textquotedblleft noncommutative ring\textquotedblright\ do not
imply that commutativity of multiplication must be false for this ring; they
merely say that commutativity of multiplication is \textbf{not required} to
hold for it. For example, the noncommutative ring $\mathbb{K}^{n\times n}$ is
actually commutative when $n\leq1$ or when $\mathbb{K}$ is a trivial ring.)
\par
Instead of saying \textquotedblleft noncommutative ring\textquotedblright,
many algebraists just say \textquotedblleft ring\textquotedblright. We shall,
however, keep the word \textquotedblleft noncommutative\textquotedblright\ in
order to avoid confusion.}. We shall study noncommutative rings later (in
Section \ref{sect.noncommring}).

\subsection{Determinants}

Square matrices have determinants. Let us recall how determinants are defined:

\begin{definition}
\label{def.det}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. The \textit{determinant}
$\det A$ of $A$ is defined as%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)
}a_{2,\sigma\left(  2\right)  }\cdots a_{n,\sigma\left(  n\right)  }.
\label{eq.det}%
\end{equation}
In other words,%
\begin{align}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{a_{1,\sigma\left(  1\right)  }a_{2,\sigma\left(  2\right)  }\cdots
a_{n,\sigma\left(  n\right)  }}_{=\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}}\label{eq.det.eq.1}\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }. \label{eq.det.eq.2}%
\end{align}

\end{definition}

For example, the determinant of a $1\times1$ matrix $\left(
\begin{array}
[c]{c}%
a_{1,1}%
\end{array}
\right)  $ is%
\begin{align}
\det\left(
\begin{array}
[c]{c}%
a_{1,1}%
\end{array}
\right)   &  =\sum_{\sigma\in S_{1}}\left(  -1\right)  ^{\sigma}%
a_{1,\sigma\left(  1\right)  }=\underbrace{\left(  -1\right)
^{\operatorname*{id}}}_{=1}a_{1,1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the only permutation }\sigma\in
S_{1}\text{ is }\operatorname*{id}\right) \nonumber\\
&  =a_{1,1}. \label{eq.det.small.1x1}%
\end{align}
The determinant of a $2\times2$ matrix $\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  $ is%
\begin{align*}
\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)   &  =\sum_{\sigma\in S_{2}}\left(  -1\right)  ^{\sigma}%
a_{1,\sigma\left(  1\right)  }a_{2,\sigma\left(  2\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}%
\underbrace{a_{1,\operatorname*{id}\left(  1\right)  }}_{=a_{1,1}%
}\underbrace{a_{2,\operatorname*{id}\left(  2\right)  }}_{=a_{2,2}%
}+\underbrace{\left(  -1\right)  ^{s_{1}}}_{=-1}\underbrace{a_{1,s_{1}\left(
1\right)  }}_{=a_{1,2}}\underbrace{a_{2,s_{1}\left(  2\right)  }}_{=a_{2,1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the only permutations }\sigma\in
S_{2}\text{ are }\operatorname*{id}\text{ and }s_{1}\right) \\
&  =a_{1,1}a_{2,2}-a_{1,2}a_{2,1}.
\end{align*}
Similarly, for a $3\times3$-matrix, the formula is%
\begin{align}
\det\left(
\begin{array}
[c]{ccc}%
a_{1,1} & a_{1,2} & a_{1,3}\\
a_{2,1} & a_{2,2} & a_{2,3}\\
a_{3,1} & a_{3,2} & a_{3,3}%
\end{array}
\right)   &  =a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}%
a_{2,1}a_{3,2}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}%
-a_{1,3}a_{2,2}a_{3,1}. \label{eq.det.small.3x3}%
\end{align}
Also, the determinant of the $0\times0$-matrix is $1$\ \ \ \ \footnote{In more
details:
\par
There is only one $0\times0$-matrix; it has no rows and no columns and no
entries. According to (\ref{eq.det.eq.2}), its determinant is
\begin{align*}
\sum_{\sigma\in S_{0}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}%
^{0}a_{i,\sigma\left(  i\right)  }}_{=\left(  \text{empty product}\right)
=1}  &  =\sum_{\sigma\in S_{0}}\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the only }%
\sigma\in S_{0}\text{ is }\operatorname*{id}\right) \\
&  =1.
\end{align*}
}. (This might sound like hairsplitting, but being able to work with
$0\times0$-matrices simplifies some proofs by induction, because it allows one
to take $n=0$ as an induction base.)

The equality (\ref{eq.det.eq.2}) (or, equivalently, (\ref{eq.det.eq.1})) is
known as the \textit{Leibniz formula}. Out of several known ways to define the
determinant, it is probably the most direct. In practice, however, computing a
determinant using (\ref{eq.det.eq.2}) quickly becomes impractical when $n$ is
high (since the sum has $n!$ terms). In most situations that occur both
\href{http://arxiv.org/abs/math/9902004v3}{in mathematics} and
\href{https://en.wikipedia.org/wiki/Determinant#Calculation}{in applications},
determinants can be computed in various simpler ways.

Some authors write $\left\vert A\right\vert $ instead of $\det A$ for the
determinant of a square matrix $A$. I do not like this notation, as it clashes
(in the case of $1\times1$-matrices) with the notation $\left\vert
a\right\vert $ for the absolute value of a real number $a$.

Here is a first example of a determinant which ends up very simple:

\begin{example}
\label{exam.xiyj}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$
elements of $\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$ further
elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(
x_{i}y_{j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. What is $\det A$ ?

For $n=0$, we have $\det A=1$ (since the $0\times0$-matrix has determinant $1$).

For $n=1$, we have $A=\left(
\begin{array}
[c]{c}%
x_{1}y_{1}%
\end{array}
\right)  $ and thus $\det A=x_{1}y_{1}$.

For $n=2$, we have $A=\left(
\begin{array}
[c]{cc}%
x_{1}y_{1} & x_{1}y_{2}\\
x_{2}y_{1} & x_{2}y_{2}%
\end{array}
\right)  $ and thus $\det A=\left(  x_{1}y_{1}\right)  \left(  x_{2}%
y_{2}\right)  -\left(  x_{1}y_{2}\right)  \left(  x_{2}y_{1}\right)  =0$.

What do you expect for greater values of $n$ ? The pattern might not be clear
at this point yet, but if you compute further examples, you will realize that
$\det A=0$ also holds for $n=3$, for $n=4$, for $n=5$... This suggests that
$\det A=0$ for every $n\geq2$. How to prove this?

Let $n\geq2$. Then, (\ref{eq.det.eq.1}) (applied to $a_{i,j}=x_{i}y_{j}$)
yields%
\begin{align}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\left(  x_{1}y_{\sigma\left(  1\right)  }\right)  \left(
x_{2}y_{\sigma\left(  2\right)  }\right)  \cdots\left(  x_{n}y_{\sigma\left(
n\right)  }\right)  }_{=\left(  x_{1}x_{2}\cdots x_{n}\right)  \left(
y_{\sigma\left(  1\right)  }y_{\sigma\left(  2\right)  }\cdots y_{\sigma
\left(  n\right)  }\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  x_{1}x_{2}\cdots
x_{n}\right)  \underbrace{\left(  y_{\sigma\left(  1\right)  }y_{\sigma\left(
2\right)  }\cdots y_{\sigma\left(  n\right)  }\right)  }_{\substack{=y_{1}%
y_{2}\cdots y_{n}\\\text{(since }\sigma\text{ is a permutation)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  x_{1}x_{2}\cdots
x_{n}\right)  \left(  y_{1}y_{2}\cdots y_{n}\right) \nonumber\\
&  =\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\right)  \left(
x_{1}x_{2}\cdots x_{n}\right)  \left(  y_{1}y_{2}\cdots y_{n}\right)  .
\label{eq.exam.xiyj.detA1}%
\end{align}
Now, every $\sigma\in S_{n}$ is either even or odd (but not both), and thus we
have%
\begin{align*}
&  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=1\\\text{(since }%
\sigma\text{ is even)}}}+\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is
odd}}}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=-1\\\text{(since
}\sigma\text{ is odd)}}}\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}1}_{=\left(  \text{the number of even permutations }\sigma\in S_{n}\right)
\cdot1}+\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is odd}%
}}\left(  -1\right)  }_{=\left(  \text{the number of odd permutations }%
\sigma\in S_{n}\right)  \cdot\left(  -1\right)  }\\
&  =\underbrace{\left(  \text{the number of even permutations }\sigma\in
S_{n}\right)  }_{\substack{=n!/2\\\text{(by Exercise \ref{exe.ps2.2.7})}%
}}\cdot1\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the number of odd
permutations }\sigma\in S_{n}\right)  }_{\substack{=n!/2\\\text{(by Exercise
\ref{exe.ps2.2.7})}}}\cdot\left(  -1\right) \\
&  =\left(  n!/2\right)  \cdot1+\left(  n!/2\right)  \cdot\left(  -1\right)
=0.
\end{align*}
Hence, (\ref{eq.exam.xiyj.detA1}) becomes $\det A=\underbrace{\left(
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\right)  }_{=0}\left(
x_{1}x_{2}\cdots x_{n}\right)  \left(  y_{1}y_{2}\cdots y_{n}\right)  =0$, as
we wanted to prove.

We will eventually learn a simpler way to prove this.
\end{example}

\begin{example}
\label{exam.xi+yj}Here is an example similar to Example \ref{exam.xiyj}, but subtler.

Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of
$\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$ further elements of
$\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(  x_{i}+y_{j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. What is $\det A$ ?

For $n=0$, we have $\det A=1$ again.

For $n=1$, we have $A=\left(
\begin{array}
[c]{c}%
x_{1}+y_{1}%
\end{array}
\right)  $ and thus $\det A=x_{1}+y_{1}$.

For $n=2$, we have $A=\left(
\begin{array}
[c]{cc}%
x_{1}+y_{1} & x_{1}+y_{2}\\
x_{2}+y_{1} & x_{2}+y_{2}%
\end{array}
\right)  $ and thus $\det A=\left(  x_{1}+y_{1}\right)  \left(  x_{2}%
+y_{2}\right)  -\left(  x_{1}+y_{2}\right)  \left(  x_{2}+y_{1}\right)
=-\left(  y_{1}-y_{2}\right)  \left(  x_{1}-x_{2}\right)  $.

However, it turns out that for every $n\geq3$, we again have $\det A=0$. This
is harder to prove than the similar claim in Example \ref{exam.xiyj}. We will
eventually see how to do it easily, but as for now let me outline a direct
proof. (I am being rather telegraphic here; do not worry if you do not
understand the following argument, as there will be easier and more detailed
proofs below.)

From (\ref{eq.det.eq.1}), we obtain%
\begin{equation}
\det A=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
x_{1}+y_{\sigma\left(  1\right)  }\right)  \left(  x_{2}+y_{\sigma\left(
2\right)  }\right)  \cdots\left(  x_{n}+y_{\sigma\left(  n\right)  }\right)  .
\label{eq.exam.xi+yj.detA1}%
\end{equation}
If we expand the product $\left(  x_{1}+y_{\sigma\left(  1\right)  }\right)
\left(  x_{2}+y_{\sigma\left(  2\right)  }\right)  \cdots\left(
x_{n}+y_{\sigma\left(  n\right)  }\right)  $, we obtain a sum of $2^{n}$
terms:%
\[
\left(  x_{1}+y_{\sigma\left(  1\right)  }\right)  \left(  x_{2}%
+y_{\sigma\left(  2\right)  }\right)  \cdots\left(  x_{n}+y_{\sigma\left(
n\right)  }\right)  =\sum_{I\subseteq\left[  n\right]  }\left(  \prod_{i\in
I}x_{i}\right)  \left(  \prod_{i\in\left[  n\right]  \setminus I}%
y_{\sigma\left(  i\right)  }\right)
\]
(where $\left[  n\right]  $ means the set $\left\{  1,2,\ldots,n\right\}  $).
Thus, (\ref{eq.exam.xi+yj.detA1}) becomes%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
x_{1}+y_{\sigma\left(  1\right)  }\right)  \left(  x_{2}+y_{\sigma\left(
2\right)  }\right)  \cdots\left(  x_{n}+y_{\sigma\left(  n\right)  }\right) \\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{I\subseteq\left[
n\right]  }\left(  \prod_{i\in I}x_{i}\right)  \left(  \prod_{i\in\left[
n\right]  \setminus I}y_{\sigma\left(  i\right)  }\right) \\
&  =\sum_{I\subseteq\left[  n\right]  }\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in I}x_{i}\right)  \left(  \prod
_{i\in\left[  n\right]  \setminus I}y_{\sigma\left(  i\right)  }\right)  .
\end{align*}
We want to prove that this is $0$. In order to do so, it clearly suffices to
show that every $I\subseteq\left[  n\right]  $ satisfies%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in I}%
x_{i}\right)  \left(  \prod_{i\in\left[  n\right]  \setminus I}y_{\sigma
\left(  i\right)  }\right)  =0. \label{eq.exam.xi+yj.detA4}%
\end{equation}
So let us fix $I\subseteq\left[  n\right]  $, and try to prove
(\ref{eq.exam.xi+yj.detA4}). We must be in one of the following two cases:

\begin{description}
\item[Case 1:] The set $\left[  n\right]  \setminus I$ has at least two
elements. In this case, let us pick two distinct elements $a$ and $b$ of this
set, and split the set $S_{n}$ into disjoint two-element subsets by pairing up
every even permutation $\sigma\in S_{n}$ with the odd permutation
$t_{a,b}\circ\sigma$ (where $t_{a,b}$ is as defined in Definition
\ref{def.transpos}). The addends on the left hand side of
(\ref{eq.exam.xi+yj.detA4}) corresponding to two permutations paired up cancel
out each other, and thus the whole left hand side of
(\ref{eq.exam.xi+yj.detA4}) is $0$. Thus, (\ref{eq.exam.xi+yj.detA4}) is
proven in Case 1.

\item[Case 2:] The set $\left[  n\right]  \setminus I$ has at most one
element. In this case, the set $I$ has at least two elements (it is here that
we use $n\geq3$). Pick two distinct elements $c$ and $d$ of $I$, and split the
set $S_{n}$ into disjoint two-element subsets by pairing up every even
permutation $\sigma\in S_{n}$ with the odd permutation $\sigma\circ t_{c,d}$.
Again, the addends on the left hand side of (\ref{eq.exam.xi+yj.detA4})
corresponding to two permutations paired up cancel out each other, and thus
the whole left hand side of (\ref{eq.exam.xi+yj.detA4}) is $0$. This proves
(\ref{eq.exam.xi+yj.detA4}) in Case 2.
\end{description}

We thus have proven (\ref{eq.exam.xi+yj.detA4}) in both cases. So $\det A=0$
is proven. This was a tricky argument, and shows the limits of the usefulness
of (\ref{eq.det.eq.1}).
\end{example}

We shall now discuss basic properties of the determinant.

\begin{exercise}
\label{exe.ps4.3}Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ be an $n\times n$-matrix. Assume that $a_{i,j}=0$ for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$. Show
that%
\[
\det A=a_{1,1}a_{2,2}\cdots a_{n,n}.
\]

\end{exercise}

\begin{definition}
An $n\times n$-matrix $A$ satisfying the assumption of Exercise
\ref{exe.ps4.3} is said to be \textit{lower-triangular} (because its entries
above the main diagonal are $0$, and thus its nonzero entries are concentrated
in the triangular region southwest of the main diagonal). Exercise
\ref{exe.ps4.3} thus says that the determinant of a lower-triangular matrix is
the product of its diagonal entries. For instance, $\det\left(
\begin{array}
[c]{ccc}%
a & 0 & 0\\
b & c & 0\\
d & e & f
\end{array}
\right)  =acf$.
\end{definition}

\begin{example}
Let $n\in\mathbb{N}$. The $n\times n$ identity matrix $I_{n}$ is
lower-triangular, and its diagonal entries are $1,1,\ldots,1$. Hence, Exercise
\ref{exe.ps4.3} shows that its determinant is $\det\left(  I_{n}\right)
=1\cdot1\cdot\cdots\cdot1=1$.
\end{example}

\begin{definition}
The \textit{transpose} of a matrix $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ is defined to be the matrix $\left(  a_{j,i}\right)
_{1\leq i\leq m,\ 1\leq j\leq n}$. It is denoted by $A^{T}$. For instance,
$\left(
\begin{array}
[c]{ccc}%
1 & 2 & -1\\
4 & 0 & 1
\end{array}
\right)  ^{T}=\left(
\begin{array}
[c]{cc}%
1 & 4\\
2 & 0\\
-1 & 1
\end{array}
\right)  $.
\end{definition}

\begin{remark}
I have seen various other notations for the transpose of a matrix $A$. Some of
them are $A^{t}$ (with a lower case $t$) and $^{T}A$ and $^{t}A$.
\end{remark}

\begin{exercise}
\label{exe.ps4.4}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Show
that $\det\left(  A^{T}\right)  =\det A$.
\end{exercise}

The transpose of a lower-triangular $n\times n$-matrix is an upper-triangular
$n\times n$-matrix (i.e., an $n\times n$-matrix whose entries below the main
diagonal are $0$). Thus, combining Exercise \ref{exe.ps4.3} with Exercise
\ref{exe.ps4.4}, we see that the determinant of an upper-triangular matrix is
the product of its diagonal entries.

Here is yet another simple property of determinants that follows directly from
their definition:

\begin{proposition}
\label{prop.det.scale}Let $n\in\mathbb{N}$ and $\lambda\in\mathbb{K}$. Let $A$
be an $n\times n$-matrix. Then, $\det\left(  \lambda A\right)  =\lambda
^{n}\det A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.det.scale}.]Write $A$ in the form $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, $\lambda A=\left(
\lambda a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition
of $\lambda A$). Hence, (\ref{eq.det.eq.2}) (applied to $\lambda A$ and
$\lambda a_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det\left(  \lambda A\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}\left(  \lambda a_{i,\sigma\left(
i\right)  }\right)  }_{=\lambda^{n}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\lambda^{n}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\lambda^{n}\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}=\lambda^{n}\det A.
\end{align*}
Proposition \ref{prop.det.scale} is thus proven.
\end{proof}

\begin{exercise}
\label{exe.ps4.5}Let $a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p$ be elements of
$\mathbb{K}$.

\textbf{(a)} Find a simple formula for the determinant%
\[
\det\left(
\begin{matrix}
a & b & c & d\\
l & 0 & 0 & e\\
k & 0 & 0 & f\\
j & i & h & g
\end{matrix}
\right)  .
\]


\textbf{(b)} Find a simple formula for the determinant%
\[
\det\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
f & 0 & 0 & 0 & g\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
\ell & m & n & o & p
\end{array}
\right)  .
\]
(Do not mistake the \textquotedblleft$o$\textquotedblright\ for a
\textquotedblleft$0$\textquotedblright.)

[\textbf{Hint:} Part \textbf{(b)} is simpler than part \textbf{(a)}.]
\end{exercise}

In the next exercises, we shall talk about rows and columns; let us first make
some pedantic remarks about these notions.

If $n\in\mathbb{N}$, then an $n\times1$-matrix is said to be a \textit{column
vector} of length\footnote{The word \textquotedblleft length\textquotedblright%
\ here has nothing to do with the length of a segment in geometry. Probably,
among the words used in mathematics, \textquotedblleft
length\textquotedblright\ is one of those with the most different meanings.}
$n$, whereas a $1\times n$-matrix is said to be a \textit{row vector} of
length $n$. Column vectors and row vectors store exactly the same kind of data
(namely, $n$ elements of $\mathbb{K}$), so you might wonder why I make a
difference between them (and also why I distinguish them from $n$-tuples of
elements of $\mathbb{K}$, which also contain precisely the same kind of data).
The reason for this is that column vectors and row vectors behave differently
under matrix multiplication: For example,%
\[
\left(
\begin{array}
[c]{c}%
a\\
b
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
ac & ad\\
bc & bd
\end{array}
\right)
\]
is not the same as%
\[
\left(
\begin{array}
[c]{cc}%
a & b
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
c\\
d
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
ac+bd
\end{array}
\right)  .
\]
If we would identify column vectors with row vectors, then this would cause contradictions.

The reason to distinguish between row vectors and $n$-tuples is subtler: We
have defined row vectors only for a commutative ring $\mathbb{K}$, whereas
$n$-tuples can be made out of elements of any set. As a consequence, the sum
of two row vectors is well-defined (since row vectors are matrices and thus
can be added entry by entry), whereas the sum of two $n$-tuples is not.
Similarly, we can take the product $\lambda v$ of an element $\lambda
\in\mathbb{K}$ with a row vector $v$ (by multiplying every entry of $v$ by
$\lambda$), but such a thing does not make sense for general $n$-tuples. These
differences between row vectors and $n$-tuples, however, cause no clashes of
notation if we use the same notations for both types of object. Thus, we are
often going to identify a row vector $\left(
\begin{array}
[c]{cccc}%
a_{1} & a_{2} & \cdots & a_{n}%
\end{array}
\right)  $ with the $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{K}^{n}$. Thus, $\mathbb{K}^{n}$ becomes the set of all row vectors
of length $n$.\ \ \ \ \footnote{Some algebraists, instead, identify column
vectors with $n$-tuples, so that $\mathbb{K}^{n}$ is then the set of all
column vectors of length $n$. This is a valid convention as well, but one must
be careful not to use it simultaneously with the other convention (i.e., with
the convention that row vectors are identified with $n$-tuples); this is why
we will not use it.}

The column vectors of length $n$ are in 1-to-1 correspondence with the row
vectors of length $n$, and this correspondence is given by taking the
transpose: The column vector $v$ corresponds to the row vector $v^{T}$, and
conversely, the row vector $w$ corresponds to the column vector $w^{T}$. In
particular, every column vector $\left(
\begin{array}
[c]{c}%
a_{1}\\
a_{2}\\
\vdots\\
a_{n}%
\end{array}
\right)  $ can be rewritten in the form $\left(
\begin{array}
[c]{cccc}%
a_{1} & a_{2} & \cdots & a_{n}%
\end{array}
\right)  ^{T}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  ^{T}$. We shall often
write it in the latter form, just because it takes up less space on paper.

The rows of a matrix are row vectors; the columns of a matrix are column
vectors. Thus, terms like \textquotedblleft the sum of two rows of a matrix
$A$\textquotedblright\ or \textquotedblleft$-3$ times a column of a matrix
$A$\textquotedblright\ make sense: Rows and columns are vectors, and thus can
be added (when they have the same length) and multiplied by elements of
$\mathbb{K}$.

Let $n\in\mathbb{N}$ and $j\in\left\{  1,2,\ldots,n\right\}  $. If $v$ is a
column vector of length $n$ (that is, an $n\times1$-matrix), then the
$j$\textit{-th entry of }$v$ means the $\left(  j,1\right)  $-th entry of $v$.
If $v$ is a row vector of length $n$ (that is, a $1\times n$-matrix), then the
$j$\textit{-th entry of }$v$ means the $\left(  1,j\right)  $-th entry of $v$.
For example, the $2$-nd entry of the row vector $\left(
\begin{array}
[c]{ccc}%
a & b & c
\end{array}
\right)  $ is $b$.

\begin{exercise}
\label{exe.ps4.6}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Prove
the following:

\textbf{(a)} If $B$ is an $n\times n$-matrix obtained from $A$ by switching
two rows, then $\det B=-\det A$. (\textquotedblleft Switching two
rows\textquotedblright\ means \textquotedblleft switching two distinct
rows\textquotedblright, of course.)

\textbf{(b)} If $B$ is an $n\times n$-matrix obtained from $A$ by switching
two columns, then $\det B=-\det A$.

\textbf{(c)} If a row of $A$ consists of zeroes, then $\det A=0$.

\textbf{(d)} If a column of $A$ consists of zeroes, then $\det A=0$.

\textbf{(e)} If $A$ has two equal rows, then $\det A=0$.

\textbf{(f)} If $A$ has two equal columns, then $\det A=0$.

\textbf{(g)} Let $\lambda\in\mathbb{K}$ and $k\in\left\{  1,2,\ldots
,n\right\}  $. If $B$ is the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th row by $\lambda$ (that is, multiplying every entry of
the $k$-th row by $\lambda$), then $\det B=\lambda\det A$.

\textbf{(h)} Let $\lambda\in\mathbb{K}$ and $k\in\left\{  1,2,\ldots
,n\right\}  $. If $B$ is the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th column by $\lambda$, then $\det B=\lambda\det A$.

\textbf{(i)} Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $A^{\prime}$ be an
$n\times n$-matrix whose rows equal the corresponding rows of $A$ except
(perhaps) the $k$-th row. Let $B$ be the $n\times n$-matrix obtained from $A$
by adding the $k$-th row of $A^{\prime}$ to the $k$-th row of $A$ (that is, by
adding every entry of the $k$-th row of $A^{\prime}$ to the corresponding
entry of the $k$-th row of $A$). Then, $\det B=\det A+\det A^{\prime}$.

\textbf{(j)} Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $A^{\prime}$ be an
$n\times n$-matrix whose columns equal the corresponding columns of $A$ except
(perhaps) the $k$-th column. Let $B$ be the $n\times n$-matrix obtained from
$A$ by adding the $k$-th column of $A^{\prime}$ to the $k$-th column of $A$.
Then, $\det B=\det A+\det A^{\prime}$.
\end{exercise}

\begin{example}
Let me visualize Exercise \ref{exe.ps4.6} \textbf{(i)} on an example, as it
has a somewhat daunting statement.

Set $n=3$ and $k=2$. Set $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $. Then, a matrix $A^{\prime}$ satisfying the conditions of Exercise
\ref{exe.ps4.6} \textbf{(i)} has the form $A^{\prime}=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d^{\prime} & e^{\prime} & f^{\prime}\\
g & h & i
\end{array}
\right)  $. For such a matrix $A^{\prime}$, we obtain $B=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d+d^{\prime} & e+e^{\prime} & f+f^{\prime}\\
g & h & i
\end{array}
\right)  $. Exercise \ref{exe.ps4.6} \textbf{(i)} then claims that $\det
B=\det A+\det A^{\prime}$.
\end{example}

Parts \textbf{(a)}, \textbf{(c)}, \textbf{(e)}, \textbf{(g)} and \textbf{(i)}
of Exercise \ref{exe.ps4.6} are often united under the slogan
\textquotedblleft the determinant of a matrix is multilinear and alternating
in its rows\textquotedblright\footnote{Specifically, parts \textbf{(c)},
\textbf{(g)} and \textbf{(i)} say that it is \textquotedblleft
multilinear\textquotedblright, while parts \textbf{(a)} and \textbf{(e)} are
responsible for the \textquotedblleft alternating\textquotedblright.}.
Similarly, parts \textbf{(b)}, \textbf{(d)}, \textbf{(f)}, \textbf{(h)} and
\textbf{(j)} are combined under the slogan \textquotedblleft the determinant
of a matrix is multilinear and alternating in its columns\textquotedblright.
Many texts on linear algebra (for example, \cite{HoffmanKunze}) use these
properties as the \textbf{definition} of the determinant\footnote{More
precisely, they define a \textit{determinant function} to be a function
$F:\mathbb{K}^{n\times n}\rightarrow\mathbb{K}$ which is multilinear and
alternating in the rows of a matrix (i.e., which satisfies parts \textbf{(a)},
\textbf{(c)}, \textbf{(e)}, \textbf{(g)} and \textbf{(i)} of Exercise
\ref{exe.ps4.6} if every appearance of \textquotedblleft$\det$%
\textquotedblright\ is replaced by \textquotedblleft$F$\textquotedblright\ in
this Exercise) and which satisfies $F\left(  I_{n}\right)  =1$. Then, they
show that there is (for each $n\in\mathbb{N}$) exactly one determinant
function $F:\mathbb{K}^{n\times n}\rightarrow\mathbb{K}$. They then denote
this function by $\det$. This is a rather slick definition of a determinant,
but it has the downside that it requires showing that there is exactly one
determinant function (which is often not easier than our approach).}; this is
a valid approach, but I prefer to use Definition \ref{def.det} instead, since
it is more explicit.

\begin{exercise}
\label{exe.ps4.6k}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Prove the following:

\textbf{(a)} If we add a scalar multiple of a row of $A$ to another row of
$A$, then the determinant of $A$ does not change. (A \textit{scalar multiple}
of a row vector $v$ means a row vector of the form $\lambda v$, where
$\lambda\in\mathbb{K}$.)

\textbf{(b)} If we add a scalar multiple of a column of $A$ to another column
of $A$, then the determinant of $A$ does not change. (A \textit{scalar
multiple} of a column vector $v$ means a column vector of the form $\lambda
v$, where $\lambda\in\mathbb{K}$.)
\end{exercise}

\begin{example}
Let us visualize Exercise \ref{exe.ps4.6k} \textbf{(a)}. Set $n=3$ and
$A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $. If we add $-2$ times the second row of $A$ to the first row of
$A$, then we obtain the matrix $\left(
\begin{array}
[c]{ccc}%
a+\left(  -2\right)  d & b+\left(  -2\right)  e & c+\left(  -2\right)  f\\
d & e & f\\
g & h & i
\end{array}
\right)  $. Exercise \ref{exe.ps4.6k} \textbf{(a)} now claims that this new
matrix has the same determinant as $A$ (because $-2$ times the second row of
$A$ is a scalar multiple of the second row of $A$).

Notice the word \textquotedblleft another\textquotedblright\ in Exercise
\ref{exe.ps4.6k}. Adding a scalar multiple of a row of $A$ to \textbf{the
same} row of $A$ will likely change the determinant.
\end{example}

\Needspace{20\baselineskip}

\begin{remark}
\label{exam.xi+yj.2}Exercise \ref{exe.ps4.6k} lets us prove the claim of
Example \ref{exam.xi+yj} in a much simpler way.

Namely, let $n$ and $x_{1},x_{2},\ldots,x_{n}$ and $y_{1},y_{2},\ldots,y_{n}$
and $A$ be as in Example \ref{exam.xi+yj}. Assume that $n\geq3$. We want to
show that $\det A=0$.

The matrix $A$ has at least three rows (since $n\geq3$), and looks as follows:%
\[
A=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & x_{1}+y_{2} & x_{1}+y_{3} & x_{1}+y_{4} & \cdots & x_{1}+y_{n}\\
x_{2}+y_{1} & x_{2}+y_{2} & x_{2}+y_{3} & x_{2}+y_{4} & \cdots & x_{2}+y_{n}\\
x_{3}+y_{1} & x_{3}+y_{2} & x_{3}+y_{3} & x_{3}+y_{4} & \cdots & x_{3}+y_{n}\\
x_{4}+y_{1} & x_{4}+y_{2} & x_{4}+y_{3} & x_{4}+y_{4} & \cdots & x_{4}+y_{n}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & x_{n}+y_{2} & x_{n}+y_{3} & x_{n}+y_{4} & \cdots & x_{n}+y_{n}%
\end{array}
\right)
\]
(where the presence of terms like $x_{4}$ and $y_{4}$ does not mean that the
variables $x_{4}$ and $y_{4}$ exist, in the same way as one can write
\textquotedblleft$x_{1},x_{2},\ldots,x_{k}$\textquotedblright\ even if $k=1$
or $k=0$). Thus, if we subtract the first row of $A$ from the second row of
$A$, then we obtain the matrix%
\[
A^{\prime}=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & x_{1}+y_{2} & x_{1}+y_{3} & x_{1}+y_{4} & \cdots & x_{1}+y_{n}\\
x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & \cdots & x_{2}-x_{1}\\
x_{3}+y_{1} & x_{3}+y_{2} & x_{3}+y_{3} & x_{3}+y_{4} & \cdots & x_{3}+y_{n}\\
x_{4}+y_{1} & x_{4}+y_{2} & x_{4}+y_{3} & x_{4}+y_{4} & \cdots & x_{4}+y_{n}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & x_{n}+y_{2} & x_{n}+y_{3} & x_{n}+y_{4} & \cdots & x_{n}+y_{n}%
\end{array}
\right)
\]
(because $\left(  x_{2}+y_{j}\right)  -\left(  x_{1}+y_{j}\right)
=x_{2}-x_{1}$ for every $j$). The transformation we just did (subtracting a
row from another row) does not change the determinant of the matrix (by
Exercise \ref{exe.ps4.6k} \textbf{(a)}, because subtracting a row from another
row is tantamount to adding the $\left(  -1\right)  $-multiple of the former
row to the latter), and thus we have $\det A^{\prime}=\det A$.

We notice that each entry of the second row of $A^{\prime}$ equals
$x_{2}-x_{1}$.

Next, we subtract the first row of $A^{\prime}$ from the third row of
$A^{\prime}$, and obtain the matrix%
\[
A^{\prime\prime}=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & x_{1}+y_{2} & x_{1}+y_{3} & x_{1}+y_{4} & \cdots & x_{1}+y_{n}\\
x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & \cdots & x_{2}-x_{1}\\
x_{3}-x_{1} & x_{3}-x_{1} & x_{3}-x_{1} & x_{3}-x_{1} & \cdots & x_{3}-x_{1}\\
x_{4}+y_{1} & x_{4}+y_{2} & x_{4}+y_{3} & x_{4}+y_{4} & \cdots & x_{4}+y_{n}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & x_{n}+y_{2} & x_{n}+y_{3} & x_{n}+y_{4} & \cdots & x_{n}+y_{n}%
\end{array}
\right)  .
\]
Again, the determinant is unchanged (because of Exercise \ref{exe.ps4.6k}
\textbf{(a)}), so we have $\det A^{\prime\prime}=\det A^{\prime}=\det A$.

We notice that each entry of the second row of $A^{\prime\prime}$ equals
$x_{2}-x_{1}$ (indeed, these entries have been copied over from $A^{\prime}$),
and that each entry of the third row of $A^{\prime\prime}$ equals $x_{3}%
-x_{1}$.

Next, we subtract the first column of $A^{\prime\prime}$ from each of the
other columns of $A^{\prime\prime}$. This gives us the matrix%
\begin{equation}
A^{\prime\prime\prime}=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & y_{2}-y_{1} & y_{3}-y_{1} & y_{4}-y_{1} & \cdots & y_{n}-y_{1}\\
x_{2}-x_{1} & 0 & 0 & 0 & \cdots & 0\\
x_{3}-x_{1} & 0 & 0 & 0 & \cdots & 0\\
x_{4}+y_{1} & y_{2}-y_{1} & y_{3}-y_{1} & y_{4}-y_{1} & \cdots & y_{n}-y_{1}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & y_{2}-y_{1} & y_{3}-y_{1} & y_{4}-y_{1} & \cdots & y_{n}-y_{1}%
\end{array}
\right)  . \label{eq.exam.xi+yj.2.4}%
\end{equation}
This step, again, has not changed the determinant (because Exercise
\ref{exe.ps4.6k} \textbf{(b)} shows that subtracting a column from another
column does not change the determinant, and what we did was doing $n-1$ such
transformations). Thus, $\det A^{\prime\prime\prime}=\det A^{\prime\prime
}=\det A$.

Now, let us write the matrix $A^{\prime\prime\prime}$ in the form
$A^{\prime\prime\prime}=\left(  a_{i,j}^{\prime\prime\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. (Thus, $a_{i,j}^{\prime\prime\prime}$ is the
$\left(  i,j\right)  $-th entry of $A^{\prime\prime\prime}$ for every $\left(
i,j\right)  $.) Then, (\ref{eq.det.eq.1}) (applied to $A^{\prime\prime\prime}$
instead of $A$) yields%
\begin{equation}
\det A^{\prime\prime\prime}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime}a_{2,\sigma\left(
2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(  n\right)  }%
^{\prime\prime\prime}. \label{eq.exam.xi+yj.2.5}%
\end{equation}
I claim that
\begin{equation}
a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime}a_{2,\sigma\left(
2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(  n\right)  }%
^{\prime\prime\prime}=0\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\label{eq.exam.xi+yj.2.6}%
\end{equation}


\textit{Proof of (\ref{eq.exam.xi+yj.2.6}):} Let $\sigma\in S_{n}$. Then,
$\sigma$ is injective, and thus $\sigma\left(  2\right)  \neq\sigma\left(
3\right)  $. Therefore, at least one of the integers $\sigma\left(  2\right)
$ and $\sigma\left(  3\right)  $ must be $\neq1$ (because otherwise, we would
have $\sigma\left(  2\right)  =1=\sigma\left(  3\right)  $, contradicting
$\sigma\left(  2\right)  \neq\sigma\left(  3\right)  $). We WLOG assume that
$\sigma\left(  2\right)  \neq1$. But a look at (\ref{eq.exam.xi+yj.2.4})
reveals that all entries of the second row of $A^{\prime\prime\prime}$ are
zero except for the first entry. Thus, $a_{2,j}^{\prime\prime\prime}=0$ for
every $j\neq1$. Applied to $j=\sigma\left(  2\right)  $, this yields
$a_{2,\sigma\left(  2\right)  }^{\prime\prime\prime}=0$ (since $\sigma\left(
2\right)  \neq1$). Hence, $a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime
}a_{2,\sigma\left(  2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(
n\right)  }^{\prime\prime\prime}=0$ (because if $0$ appears as a factor in a
product, then the whole product must be $0$). This proves
(\ref{eq.exam.xi+yj.2.6}).

Now, (\ref{eq.exam.xi+yj.2.5}) becomes%
\[
\det A^{\prime\prime\prime}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime}%
a_{2,\sigma\left(  2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(
n\right)  }^{\prime\prime\prime}}_{\substack{=0\\\text{(by
(\ref{eq.exam.xi+yj.2.6}))}}}=\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}0=0.
\]
Compared with $\det A^{\prime\prime\prime}=\det A$, this yields $\det A=0$.
Thus, $\det A=0$ is proven again.
\end{remark}

\begin{remark}
\label{exam.xmax}Here is another example for the use of Exercise
\ref{exe.ps4.6k}.

Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of
$\mathbb{K}$. Let $A$ be the matrix $\left(  x_{\max\left\{  i,j\right\}
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. (Recall that $\max S$ denotes the
greatest element of a nonempty set $S$.)

For example, if $n=4$, then%
\[
A=\left(
\begin{array}
[c]{cccc}%
x_{1} & x_{2} & x_{3} & x_{4}\\
x_{2} & x_{2} & x_{3} & x_{4}\\
x_{3} & x_{3} & x_{3} & x_{4}\\
x_{4} & x_{4} & x_{4} & x_{4}%
\end{array}
\right)  .
\]


We want to find $\det A$. First, let us subtract the first row of $A$ from
each of the other rows of $A$. Thus we obtain a new matrix $A^{\prime}$. The
determinant has not changed (according to Exercise \ref{exe.ps4.6k}
\textbf{(a)}); i.e., we have $\det A^{\prime}=\det A$. Here is how $A^{\prime
}$ looks like in the case when $n=4$:%
\begin{equation}
A^{\prime}=\left(
\begin{array}
[c]{cccc}%
x_{1} & x_{2} & x_{3} & x_{4}\\
x_{2}-x_{1} & 0 & 0 & 0\\
x_{3}-x_{1} & x_{3}-x_{2} & 0 & 0\\
x_{4}-x_{1} & x_{4}-x_{2} & x_{4}-x_{3} & 0
\end{array}
\right)  . \label{eq.exam.xmax.5}%
\end{equation}
Notice the many zeroes; zeroes are useful when computing determinants. To
generalize the pattern we see on (\ref{eq.exam.xmax.5}), we write the matrix
$A^{\prime}$ in the form $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ (so that $a_{i,j}^{\prime}$ is the $\left(
i,j\right)  $-th entry of $A^{\prime}$ for every $\left(  i,j\right)  $).
Then, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we have%
\begin{equation}
a_{i,j}^{\prime}=\left\{
\begin{array}
[c]{c}%
x_{\max\left\{  i,j\right\}  },\ \ \ \ \ \ \ \ \ \ \text{if }i=1;\\
x_{\max\left\{  i,j\right\}  }-x_{\max\left\{  1,j\right\}  }%
,\ \ \ \ \ \ \ \ \ \ \text{if }i>1
\end{array}
\right.  \label{eq.exam.xmax.4}%
\end{equation}
(since we obtained the matrix $A^{\prime}$ by subtracting the first row of $A$
from each of the other rows of $A$). Hence, for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $1<i\leq j$, we have%
\begin{align}
a_{i,j}^{\prime}  &  =x_{\max\left\{  i,j\right\}  }-x_{\max\left\{
1,j\right\}  }=x_{j}-x_{j}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\max\left\{  i,j\right\}  =j\text{ (because }i\leq j\text{)}\\
\text{and }\max\left\{  1,j\right\}  =j\text{ (because }1<j\text{)}%
\end{array}
\right) \nonumber\\
&  =0. \label{eq.exam.xmax.6}%
\end{align}
This is the general explanation for the six $0$'s in (\ref{eq.exam.xmax.5}).
We notice also that the first row of the matrix $A^{\prime}$ is $\left(
x_{1},x_{2},\ldots,x_{n}\right)  $.

Now, we want to transform $A^{\prime}$ further. Namely, we first switch the
first row with the second row; then we switch the second row (which used to be
the first row) with the third row; then, the third row with the fourth row,
and so on, until we finally switch the $\left(  n-1\right)  $-th row with the
$n$-th row. As a result of these $n-1$ switches, the first row has moved all
the way down to the bottom, past all the other rows. We denote the resulting
matrix by $A^{\prime\prime}$. For instance, if $n=4$, then%
\begin{equation}
A^{\prime\prime}=\left(
\begin{array}
[c]{cccc}%
x_{2}-x_{1} & 0 & 0 & 0\\
x_{3}-x_{1} & x_{3}-x_{2} & 0 & 0\\
x_{4}-x_{1} & x_{4}-x_{2} & x_{4}-x_{3} & 0\\
x_{1} & x_{2} & x_{3} & x_{4}%
\end{array}
\right)  . \label{eq.exam.xmax.9}%
\end{equation}
This is a lower-triangular matrix. To see that this holds in the general case,
we write the matrix $A^{\prime\prime}$ in the form $A^{\prime\prime}=\left(
a_{i,j}^{\prime\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (so that
$a_{i,j}^{\prime\prime}$ is the $\left(  i,j\right)  $-th entry of
$A^{\prime\prime}$ for every $\left(  i,j\right)  $). Then, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, we have%
\begin{equation}
a_{i,j}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
a_{i+1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i<n;\\
a_{1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i=n
\end{array}
\right.  \label{eq.exam.xmax.10}%
\end{equation}
(because the first row of $A^{\prime}$ has become the $n$-th row of
$A^{\prime\prime}$, whereas every other row has moved up one step). In
particular, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}$ satisfying $1\leq i<j\leq n$, we have%
\begin{align*}
a_{i,j}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
a_{i+1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i<n;\\
a_{1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i=n
\end{array}
\right.  =a_{i+1,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<j\leq
n\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.exam.xmax.6}), applied to }i+1\text{ instead of }i\\
\text{(because }i<j\text{ yields }i+1\leq j\text{)}%
\end{array}
\right)  .
\end{align*}
This shows that $A^{\prime\prime}$ is indeed lower-triangular. Hence, Exercise
\ref{exe.ps4.3} (applied to $A^{\prime\prime}$ and $a_{i,j}^{\prime\prime}$
instead of $A$ and $a_{i,j}$) shows that $\det A^{\prime\prime}=a_{1,1}%
^{\prime\prime}a_{2,2}^{\prime\prime}\cdots a_{n,n}^{\prime\prime}$.

Using (\ref{eq.exam.xmax.10}) and (\ref{eq.exam.xmax.4}), it is easy to see
that every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
a_{i,i}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
x_{i+1}-x_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i<n;\\
x_{n},\ \ \ \ \ \ \ \ \ \ \text{if }i=n
\end{array}
\right.  . \label{eq.exam.xmax.15}%
\end{equation}
(This is precisely the pattern you would guess from the diagonal entries in
(\ref{eq.exam.xmax.9}).) Now, multiplying the equalities
(\ref{eq.exam.xmax.15}) for all $i\in\left\{  1,2,\ldots,n\right\}  $, we
obtain $a_{1,1}^{\prime\prime}a_{2,2}^{\prime\prime}\cdots a_{n,n}%
^{\prime\prime}=\left(  x_{2}-x_{1}\right)  \left(  x_{3}-x_{2}\right)
\cdots\left(  x_{n}-x_{n-1}\right)  x_{n}$. Thus,%
\begin{equation}
\det A^{\prime\prime}=a_{1,1}^{\prime\prime}a_{2,2}^{\prime\prime}\cdots
a_{n,n}^{\prime\prime}=\left(  x_{2}-x_{1}\right)  \left(  x_{3}-x_{2}\right)
\cdots\left(  x_{n}-x_{n-1}\right)  x_{n}. \label{eq.exam.xmax.17}%
\end{equation}


But we want $\det A$, not $\det A^{\prime\prime}$. First, let us find $\det
A^{\prime}$. Recall that $A^{\prime\prime}$ was obtained from $A^{\prime}$ by
switching rows, repeatedly -- namely, $n-1$ times. Every time we switch two
rows in a matrix, its determinant gets multiplied by $-1$ (because of Exercise
\ref{exe.ps4.6} \textbf{(a)}). Hence, $n-1$ such switches cause the
determinant to be multiplied by $\left(  -1\right)  ^{n-1}$. Since
$A^{\prime\prime}$ was obtained from $A^{\prime}$ by $n-1$ such switches, we
thus conclude that $\det A^{\prime\prime}=\left(  -1\right)  ^{n-1}\det
A^{\prime}$, so that%
\begin{align*}
\det A^{\prime}  &  =\underbrace{\dfrac{1}{\left(  -1\right)  ^{n-1}}%
}_{=\left(  -1\right)  ^{n-1}}\underbrace{\det A^{\prime\prime}}_{=\left(
x_{2}-x_{1}\right)  \left(  x_{3}-x_{2}\right)  \cdots\left(  x_{n}%
-x_{n-1}\right)  x_{n}}\\
&  =\left(  -1\right)  ^{n-1}\left(  x_{2}-x_{1}\right)  \left(  x_{3}%
-x_{2}\right)  \cdots\left(  x_{n}-x_{n-1}\right)  x_{n}.
\end{align*}


Finally, recall that $\det A^{\prime}=\det A$, so that%
\[
\det A=\det A^{\prime}=\left(  -1\right)  ^{n-1}\left(  x_{2}-x_{1}\right)
\left(  x_{3}-x_{2}\right)  \cdots\left(  x_{n}-x_{n-1}\right)  x_{n}.
\]

\end{remark}

\subsection{$\det\left(  AB\right)  $}

Next, a lemma that will come handy in a more important proof:

\begin{lemma}
\label{lem.det.sigma}Let $n\in\mathbb{N}$. Let $\left[  n\right]  $ denote the
set $\left\{  1,2,\ldots,n\right\}  $. Let $\kappa:\left[  n\right]
\rightarrow\left[  n\right]  $ be a map. Let $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $B_{\kappa}$
be the $n\times n$-matrix $\left(  b_{\kappa\left(  i\right)  ,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

\textbf{(a)} If $\kappa\in S_{n}$, then $\det\left(  B_{\kappa}\right)
=\left(  -1\right)  ^{\kappa}\cdot\det B$.

\textbf{(b)} If $\kappa\notin S_{n}$, then $\det\left(  B_{\kappa}\right)  =0$.
\end{lemma}

\begin{remark}
Lemma \ref{lem.det.sigma} \textbf{(a)} simply says that if we permute the rows
of a square matrix, then its determinant gets multiplied by the sign of the
permutation used. For instance, let $n=3$ and $B=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $. If $\kappa$ is the permutation $\left(  2,3,1\right)  $ (in
one-line notation), then $B_{\kappa}=\left(
\begin{array}
[c]{ccc}%
d & e & f\\
g & h & i\\
a & b & c
\end{array}
\right)  $, and Lemma \ref{lem.det.sigma} \textbf{(a)} says that $\det\left(
B_{\kappa}\right)  =\underbrace{\left(  -1\right)  ^{\kappa}}_{=1}\cdot\det
B=\det B$.

Of course, a similar result holds for permutations of columns.
\end{remark}

\begin{remark}
Exercise \ref{exe.ps4.6} \textbf{(a)} is a particular case of Lemma
\ref{lem.det.sigma} \textbf{(a)}. Indeed, if $B$ is an $n\times n$-matrix
obtained from $A$ by switching the $u$-th and the $v$-th row (where $u$ and
$v$ are two distinct elements of $\left\{  1,2,\ldots,n\right\}  $), then
$B=\left(  a_{t_{u,v}\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ (where $A$ is written in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$).
\end{remark}

\begin{proof}
[Proof of Lemma \ref{lem.det.sigma}.]Recall that $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  $. In other words, $S_{n}$ is
the set of all permutations of $\left[  n\right]  $ (since $\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $). In other words, $S_{n}$ is the set of all
bijective maps $\left[  n\right]  \rightarrow\left[  n\right]  $.

\textbf{(a)} Assume that $\kappa\in S_{n}$. We define a map $\Phi
:S_{n}\rightarrow S_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma\circ\kappa\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in S_{n}.
\]
We also define a map $\Psi:S_{n}\rightarrow S_{n}$ by%
\[
\Psi\left(  \sigma\right)  =\sigma\circ\kappa^{-1}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\]
The maps $\Phi$ and $\Psi$ are mutually inverse\footnote{\textit{Proof.} Every
$\sigma\in S_{n}$ satisfies%
\begin{align*}
\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)   &  =\Psi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\sigma\circ\kappa}\right)
=\Psi\left(  \sigma\circ\kappa\right)  =\sigma\circ\underbrace{\kappa
\circ\kappa^{-1}}_{=\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\Psi\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
Thus, $\Psi\circ\Phi=\operatorname*{id}$. Similarly, $\Phi\circ\Psi
=\operatorname*{id}$. Combined with $\Psi\circ\Phi=\operatorname*{id}$, this
yields that the maps $\Phi$ and $\Psi$ are mutually inverse, qed.}. Hence, the
map $\Phi$ is a bijection.

We have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,
(\ref{eq.det.eq.2}) (applied to $B$ and $b_{i,j}$ instead of $A$ and $a_{i,j}%
$) yields%
\begin{equation}
\det B=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i=1}^{n}}_{=\prod_{i\in\left[  n\right]  }}b_{i,\sigma\left(  i\right)
}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }. \label{pf.lem.det.sigma.a.detB}%
\end{equation}


Now, $B_{\kappa}=\left(  b_{\kappa\left(  i\right)  ,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B_{\kappa}$ and
$b_{\kappa\left(  i\right)  ,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  B_{\kappa}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left[  n\right]  }%
}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\Phi\left(  \sigma\right)
}\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  } \label{pf.lem.det.sigma.a.1}%
\end{align}
(here, we have substituted $\Phi\left(  \sigma\right)  $ for $\sigma$ in the
sum, since $\Phi$ is a bijection).

But every $\sigma\in S_{n}$ satisfies $\left(  -1\right)  ^{\Phi\left(
\sigma\right)  }=\left(  -1\right)  ^{\kappa}\cdot\left(  -1\right)  ^{\sigma
}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $\Phi\left(
\sigma\right)  =\sigma\circ\kappa$, so that%
\begin{align*}
\left(  -1\right)  ^{\Phi\left(  \sigma\right)  }  &  =\left(  -1\right)
^{\sigma\circ\kappa}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)
^{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\tau=\kappa\right) \\
&  =\left(  -1\right)  ^{\kappa}\cdot\left(  -1\right)  ^{\sigma},
\end{align*}
qed.} and $\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }$\ \ \ \ \footnote{\textit{Proof.}
Let $\sigma\in S_{n}$. We have $\Phi\left(  \sigma\right)  =\sigma\circ\kappa
$. Thus, for every $i\in\left[  n\right]  $, we have $\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  =\left(  \sigma\circ\kappa\right)
\left(  i\right)  =\sigma\left(  \kappa\left(  i\right)  \right)  $. Hence,
$\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }%
$.
\par
But $\kappa\in S_{n}$. In other words, $\kappa$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, hence a bijection from
$\left[  n\right]  $ to $\left[  n\right]  $. Therefore, we can substitute
$\kappa\left(  i\right)  $ for $i$ in the product $\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }$. We thus obtain $\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }%
$. Comparing this with $\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)  \left(  i\right)
}=\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(
\kappa\left(  i\right)  \right)  }$, we obtain $\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)
\left(  i\right)  }=\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(  i\right)
}$, qed.}. Thus, (\ref{pf.lem.det.sigma.a.1}) becomes%
\begin{align*}
\det\left(  B_{\kappa}\right)   &  =\sum_{\sigma\in S_{n}}\underbrace{\left(
-1\right)  ^{\Phi\left(  \sigma\right)  }}_{=\left(  -1\right)  ^{\kappa}%
\cdot\left(  -1\right)  ^{\sigma}}\underbrace{\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)
\left(  i\right)  }}_{=\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(
i\right)  }}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\kappa}\cdot\left(
-1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(  i\right)
}\\
&  =\left(  -1\right)  ^{\kappa}\cdot\underbrace{\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(  i\right)
}}_{\substack{=\det B\\\text{(by (\ref{pf.lem.det.sigma.a.detB}))}}}=\left(
-1\right)  ^{\kappa}\cdot\det B.
\end{align*}
This proves Lemma \ref{lem.det.sigma} \textbf{(a)}.

\textbf{(b)} Assume that $\kappa\notin S_{n}$.

The following fact is well-known: If $U$ is a finite set, then every injective
map $U\rightarrow U$ is bijective\footnote{\textit{Proof.} Let $U$ be a finite
set, and let $f$ be an injective map $U\rightarrow U$. We must show that $f$
is bijective.
\par
Since $f$ is injective, we have $\left\vert f\left(  U\right)  \right\vert
=\left\vert U\right\vert $. Thus, $f\left(  U\right)  $ is a subset of $U$
which has size $\left\vert U\right\vert $. But the only such subset is $U$
itself (since $U$ is a finite set). Therefore, $f\left(  U\right)  $ must be
$U$ itself. In other words, the map $f$ is surjective. Hence, $f$ is bijective
(since $f$ is injective and surjective), qed.}. We can apply this to
$U=\left[  n\right]  $, and thus conclude that every injective map $\left[
n\right]  \rightarrow\left[  n\right]  $ is bijective. Therefore, if the map
$\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ were injective, then
$\kappa$ would be bijective and therefore would be an element of $S_{n}$
(since $S_{n}$ is the set of all bijective maps $\left[  n\right]
\rightarrow\left[  n\right]  $); but this would contradict the fact that
$\kappa\notin S_{n}$. Hence, the map $\kappa:\left[  n\right]  \rightarrow
\left[  n\right]  $ cannot be injective. Therefore, there exist two distinct
elements $a$ and $b$ of $\left[  n\right]  $ such that $\kappa\left(
a\right)  =\kappa\left(  b\right)  $. Consider these $a$ and $b$. Then,
$\kappa\circ t_{a,b}=\kappa$\ \ \ \ \footnote{\textit{Proof.} We are going to
show that every $i\in\left[  n\right]  $ satisfies $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
So let $i\in\left[  n\right]  $. We shall show that $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
The definition of $t_{a,b}$ shows that $t_{a,b}$ is the permutation in $S_{n}$
which switches $a$ with $b$ while leaving all other elements of $\left\{
1,2,\ldots,n\right\}  $ unchanged. In other words, we have $t_{a,b}\left(
a\right)  =b$, and $t_{a,b}\left(  b\right)  =a$, and $t_{a,b}\left(
j\right)  =j$ for every $j\in\left[  n\right]  \setminus\left\{  a,b\right\}
$.
\par
Now, we have $i\in\left[  n\right]  $. Thus, we are in one of the following
three cases:
\par
\textit{Case 1:} We have $i=a$.
\par
\textit{Case 2:} We have $i=b$.
\par
\textit{Case 3:} We have $i\in\left[  n\right]  \setminus\left\{  a,b\right\}
$.
\par
Let us first consider Case 1. In this case, we have $i=a$, so that $\left(
\kappa\circ t_{a,b}\right)  \left(  \underbrace{i}_{=a}\right)  =\left(
\kappa\circ t_{a,b}\right)  \left(  a\right)  =\kappa\left(
\underbrace{t_{a,b}\left(  a\right)  }_{=b}\right)  =\kappa\left(  b\right)
$. Compared with $\kappa\left(  \underbrace{i}_{=a}\right)  =\kappa\left(
a\right)  =\kappa\left(  b\right)  $, this yields $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $. Thus, $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ is
proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $i=b$, so that $\left(
\kappa\circ t_{a,b}\right)  \left(  \underbrace{i}_{=b}\right)  =\left(
\kappa\circ t_{a,b}\right)  \left(  b\right)  =\kappa\left(
\underbrace{t_{a,b}\left(  b\right)  }_{=a}\right)  =\kappa\left(  a\right)
=\kappa\left(  b\right)  $. Compared with $\kappa\left(  \underbrace{i}%
_{=b}\right)  =\kappa\left(  b\right)  $, this yields $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $. Thus, $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ is
proven in Case 2.
\par
Let us finally consider Case 3. In this case, we have $i\in\left[  n\right]
\setminus\left\{  a,b\right\}  $. Hence, $t_{a,b}\left(  i\right)  =i$ (since
$t_{a,b}\left(  j\right)  =j$ for every $j\in\left[  n\right]  \setminus
\left\{  a,b\right\}  $). Therefore, $\left(  \kappa\circ t_{a,b}\right)
\left(  i\right)  =\kappa\left(  \underbrace{t_{a,b}\left(  i\right)  }%
_{=i}\right)  =\kappa\left(  i\right)  $. Thus, $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ is proven in Case
3.
\par
We now have shown $\left(  \kappa\circ t_{a,b}\right)  \left(  i\right)
=\kappa\left(  i\right)  $ in each of the three Cases 1, 2 and 3. Hence,
$\left(  \kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(
i\right)  $ always holds.
\par
Now, let us forget that we fixed $i$. We thus have shown that $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ for
every $i\in\left[  n\right]  $. In other words, $\kappa\circ t_{a,b}=\kappa$,
qed.}. Exercise \ref{exe.ps4.1ab} \textbf{(b)} (applied to $i=a$ and $j=b$)
yields $\left(  -1\right)  ^{t_{a,b}}=-1$.

Let $A_{n}$ be the set of all even permutations in $S_{n}$. Let $C_{n}$ be the
set of all odd permutations in $S_{n}$.

We have $\sigma\circ t_{a,b}\in C_{n}$ for every $\sigma\in A_{n}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in A_{n}$. Then, $\sigma$ is an
even permutation in $S_{n}$ (since $A_{n}$ is the set of all even permutations
in $S_{n}$). Hence, $\left(  -1\right)  ^{\sigma}=1$. Now, (\ref{eq.sign.prod}%
) (applied to $\tau=t_{a,b}$) yields $\left(  -1\right)  ^{\sigma\circ
t_{a,b}}=\underbrace{\left(  -1\right)  ^{\sigma}}_{=1}\cdot
\underbrace{\left(  -1\right)  ^{t_{a,b}}}_{=-1}=-1$. Thus, the permutation
$\sigma\circ t_{a,b}$ is odd. Hence, $\sigma\circ t_{a,b}$ is an odd
permutation in $S_{n}$. In other words, $\sigma\circ t_{a,b}\in C_{n}$ (since
$C_{n}$ is the set of all odd permutations in $S_{n}$), qed.}. Hence, we can
define a map $\Phi:A_{n}\rightarrow C_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma\circ t_{a,b}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in A_{n}.
\]
Consider this map $\Phi$. Furthermore, we have $\sigma\circ\left(
t_{a,b}\right)  ^{-1}\in A_{n}$ for every $\sigma\in C_{n}$%
\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in C_{n}$. Then, $\sigma$ is an
odd permutation in $S_{n}$ (since $C_{n}$ is the set of all odd permutations
in $S_{n}$). Hence, $\left(  -1\right)  ^{\sigma}=-1$.
\par
Applying (\ref{eq.sign.inverse}) to $t_{a,b}$ instead of $\sigma$, we obtain
$\left(  -1\right)  ^{\left(  t_{a,b}\right)  ^{-1}}=\left(  -1\right)
^{t_{a,b}}=-1$. Now, (\ref{eq.sign.prod}) (applied to $\tau=\left(
t_{a,b}\right)  ^{-1}$) yields $\left(  -1\right)  ^{\sigma\circ\left(
t_{a,b}\right)  ^{-1}}=\underbrace{\left(  -1\right)  ^{\sigma}}_{=-1}%
\cdot\underbrace{\left(  -1\right)  ^{\left(  t_{a,b}\right)  ^{-1}}}%
_{=-1}=\left(  -1\right)  \cdot\left(  -1\right)  =1$. Thus, the permutation
$\sigma\circ\left(  t_{a,b}\right)  ^{-1}$ is even. Hence, $\sigma\circ\left(
t_{a,b}\right)  ^{-1}$ is an even permutation in $S_{n}$. In other words,
$\sigma\circ\left(  t_{a,b}\right)  ^{-1}\in A_{n}$ (since $A_{n}$ is the set
of all even permutations in $S_{n}$), qed.}. Thus, we can define a map
$\Psi:C_{n}\rightarrow A_{n}$ by%
\[
\Psi\left(  \sigma\right)  =\sigma\circ\left(  t_{a,b}\right)  ^{-1}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in C_{n}.
\]


(We could have simplified our life a bit by noticing that $\left(
t_{a,b}\right)  ^{-1}=t_{a,b}$, so that the maps $\Phi$ and $\Psi$ are given
by the same formula, albeit defined on different domains. But I wanted to
demonstrate a use of (\ref{eq.sign.inverse}).)

The maps $\Phi$ and $\Psi$ are mutually inverse\footnote{\textit{Proof.} Every
$\sigma\in A_{n}$ satisfies%
\begin{align*}
\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)   &  =\Psi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\sigma\circ\tau_{a,b}}\right)
=\Psi\left(  \sigma\circ\tau_{a,b}\right)  =\sigma\circ\underbrace{\tau
_{a,b}\circ\left(  \tau_{a,b}\right)  ^{-1}}_{=\operatorname*{id}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Psi\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
Thus, $\Psi\circ\Phi=\operatorname*{id}$. Similarly, $\Phi\circ\Psi
=\operatorname*{id}$. Combined with $\Psi\circ\Phi=\operatorname*{id}$, this
yields that the maps $\Phi$ and $\Psi$ are mutually inverse, qed.}. Hence, the
map $\Psi$ is a bijection. Moreover, every $\sigma\in C_{n}$ satisfies%
\begin{equation}
\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }.
\label{pf.lem.det.sigma.b.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.sigma.b.2}):} Let $\sigma\in
C_{n}$. The map $t_{a,b}$ is a permutation of $\left[  n\right]  $, thus a
bijection $\left[  n\right]  \rightarrow\left[  n\right]  $. Hence, we can
substitute $t_{a,b}\left(  i\right)  $ for $i$ in the product $\prod
_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  i\right)  }$. Thus we obtain%
\[
\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  t_{a,b}\left(  i\right)  \right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  t_{a,b}\left(  i\right)  \right)  }%
=\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }%
\]
(since every $i\in\left[  n\right]  $ satisfies $\kappa\left(  t_{a,b}\left(
i\right)  \right)  =\underbrace{\left(  \kappa\circ t_{a,b}\right)  }%
_{=\kappa}\left(  i\right)  =\kappa\left(  i\right)  $ and
\[
\underbrace{\left(  \Psi\left(  \sigma\right)  \right)  }_{=\sigma\circ\left(
t_{a,b}\right)  ^{-1}}\left(  t_{a,b}\left(  i\right)  \right)  =\left(
\sigma\circ\left(  t_{a,b}\right)  ^{-1}\right)  \left(  t_{a,b}\left(
i\right)  \right)  =\sigma\left(  \underbrace{\left(  t_{a,b}\right)
^{-1}\left(  t_{a,b}\left(  i\right)  \right)  }_{=i}\right)  =\sigma\left(
i\right)
\]
). This proves (\ref{pf.lem.det.sigma.b.2}).}

We have $B_{\kappa}=\left(  b_{\kappa\left(  i\right)  ,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B_{\kappa}$
and $b_{\kappa\left(  i\right)  ,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det\left(  B_{\kappa}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left[  n\right]  }%
}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}}_{\substack{=\sum_{\sigma\in A_{n}}\\\text{(since }A_{n}\text{ is
the}\\\text{set of all even}\\\text{permutations}\\\text{in }S_{n}\text{)}%
}}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=1\\\text{(since
}\sigma\text{ is even)}}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\text{ is odd}}}}_{\substack{=\sum_{\sigma\in C_{n}%
}\\\text{(since }C_{n}\text{ is the}\\\text{set of all odd}%
\\\text{permutations}\\\text{in }S_{n}\text{)}}}\underbrace{\left(  -1\right)
^{\sigma}}_{\substack{=-1\\\text{(since }\sigma\text{ is odd)}}}\prod
_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)
}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every permutation }\sigma\in
S_{n}\text{ is either even or odd, but not both}\right) \\
&  =\sum_{\sigma\in A_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }+\sum_{\sigma\in C_{n}}\left(  -1\right)
\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }\\
&  =\sum_{\sigma\in A_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }-\sum_{\sigma\in C_{n}}\prod_{i\in\left[
n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }=0,
\end{align*}
since%
\begin{align*}
&  \sum_{\sigma\in A_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in C_{n}}\underbrace{\prod_{i\in\left[  n\right]  }%
b_{\kappa\left(  i\right)  ,\left(  \Psi\left(  \sigma\right)  \right)
\left(  i\right)  }}_{\substack{=\prod_{i\in\left[  n\right]  }b_{\kappa
\left(  i\right)  ,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.det.sigma.b.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\Psi\left(
\sigma\right)  \text{ for }\sigma\text{, since the map }\Psi\text{ is a
bijection}\right) \\
&  =\sum_{\sigma\in C_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }.
\end{align*}
This proves Lemma \ref{lem.det.sigma} \textbf{(b)}.
\end{proof}

Now let us state a basic formula for products of sums in a commutative ring:

\begin{lemma}
\label{lem.prodrule}For every $n\in\mathbb{N}$, let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$. For every $i\in\left[  n\right]  $, let $p_{i,1}%
,p_{i,2},\ldots,p_{i,m_{i}}$ be finitely many elements of $\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m_{1}\right]  \times\left[  m_{2}\right]
\times\cdots\times\left[  m_{n}\right]  }\prod_{i=1}^{n}p_{i,k_{i}}.
\]


(\textbf{Pedantic remark:} If $n=0$, then the Cartesian product $\left[
m_{1}\right]  \times\left[  m_{2}\right]  \times\cdots\times\left[
m_{n}\right]  $ has no factors; it is what is called an \textit{empty
Cartesian product}. It is understood to be a $1$-element set, and its single
element is the $0$-tuple $\left(  {}\right)  $ (also known as the empty list).)
\end{lemma}

I tend to refer to Lemma \ref{lem.prodrule} as the \textit{product rule}
(since it is related to the product rule for joint probabilities); I think it
has no really widespread name. However, it is a fundamental algebraic fact
that is used very often and tacitly (I suspect that most mathematicians have
never thought of it as being a theorem that needs to be proven). The idea
behind Lemma \ref{lem.prodrule} is that if you expand the product%
\begin{align*}
&  \prod_{i=1}^{n}\sum_{k=1}^{m_{i}}p_{i,k}\\
&  =\prod_{i=1}^{n}\left(  p_{i,1}+p_{i,2}+\cdots+p_{i,m_{i}}\right) \\
&  =\left(  p_{1,1}+p_{1,2}+\cdots+p_{1,m_{1}}\right)  \left(  p_{2,1}%
+p_{2,2}+\cdots+p_{2,m_{2}}\right)  \cdots\left(  p_{n,1}+p_{n,2}%
+\cdots+p_{n,m_{n}}\right)  ,
\end{align*}
then you get a sum of $m_{1}m_{2}\cdots m_{n}$ terms, each of which has the
form $p_{1,k_{1}}p_{2,k_{2}}\cdots p_{n,k_{n}}=\prod_{i=1}^{n}p_{i,k_{i}}$ for
some $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]
\times\left[  m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  $. (More
precisely, it is the sum of all such terms.) A formal proof of Lemma
\ref{lem.prodrule} could be obtained by induction over $n$ using the
distributivity axiom\footnote{and the observation that the $n$-tuples $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]  \times\left[
m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  $ are in bijection with
the pairs $\left(  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)  ,k_{n}\right)  $
of an $\left(  n-1\right)  $-tuple $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)
\in\left[  m_{1}\right]  \times\left[  m_{2}\right]  \times\cdots\times\left[
m_{n-1}\right]  $ and an element $k_{n}\in\left[  m_{n}\right]  $}. For the
details (if you care about them), see the solution to the following exercise:

\begin{exercise}
\label{exe.prodrule}Prove Lemma \ref{lem.prodrule}.
\end{exercise}

We shall use a corollary of Lemma \ref{lem.prodrule}:

\begin{lemma}
\label{lem.prodrule2}For every $n\in\mathbb{N}$, let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. For every $i\in\left[  n\right]  $,
let $p_{i,1},p_{i,2},\ldots,p_{i,m}$ be $m$ elements of $\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\kappa:\left[  n\right]
\rightarrow\left[  m\right]  }\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.prodrule2}.]For the sake of completeness, let us give
this proof.

Lemma \ref{lem.prodrule} (applied to $m_{i}=m$ for every $i\in\left[
n\right]  $) yields
\begin{equation}
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\underbrace{\left[  m\right]  \times\left[  m\right]
\times\cdots\times\left[  m\right]  }_{n\text{ factors}}}\prod_{i=1}%
^{n}p_{i,k_{i}}. \label{pf.lem.prodrule2.1}%
\end{equation}


Let $\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]  \right)
$ denote the set of all functions from $\left[  n\right]  $ to $\left[
m\right]  $. Now, let $\Phi$ be the map from $\operatorname*{Map}\left(
\left[  n\right]  ,\left[  m\right]  \right)  $ to $\underbrace{\left[
m\right]  \times\left[  m\right]  \times\cdots\times\left[  m\right]
}_{n\text{ factors}}$ given by%
\[
\Phi\left(  \kappa\right)  =\left(  \kappa\left(  1\right)  ,\kappa\left(
2\right)  ,\ldots,\kappa\left(  n\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\kappa\in\operatorname*{Map}\left(
\left[  n\right]  ,\left[  m\right]  \right)  .
\]
So the map $\Phi$ takes a function $\kappa$ from $\left[  n\right]  $ to
$\left[  m\right]  $, and outputs the list $\left(  \kappa\left(  1\right)
,\kappa\left(  2\right)  ,\ldots,\kappa\left(  n\right)  \right)  $ of all its
values. Clearly, the map $\Phi$ is injective (since a function $\kappa
\in\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]  \right)  $
can be reconstructed from the list $\left(  \kappa\left(  1\right)
,\kappa\left(  2\right)  ,\ldots,\kappa\left(  n\right)  \right)  =\Phi\left(
\kappa\right)  $) and surjective (since every list of $n$ elements of $\left[
m\right]  $ is the list of values of some function $\kappa\in
\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]  \right)  $).
Thus, $\Phi$ is bijective. Therefore, we can substitute $\Phi\left(
\kappa\right)  $ for $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ in the sum
$\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\underbrace{\left[
m\right]  \times\left[  m\right]  \times\cdots\times\left[  m\right]
}_{n\text{ factors}}}\prod_{i=1}^{n}p_{i,k_{i}}$. In other words, we can
substitute $\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)
,\ldots,\kappa\left(  n\right)  \right)  $ for $\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  $ in this sum (since $\Phi\left(  \kappa\right)
=\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)  ,\ldots
,\kappa\left(  n\right)  \right)  $ for each $\kappa\in\operatorname*{Map}%
\left(  \left[  n\right]  ,\left[  m\right]  \right)  $). We thus obtain%
\begin{align*}
\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\underbrace{\left[
m\right]  \times\left[  m\right]  \times\cdots\times\left[  m\right]
}_{n\text{ factors}}}\prod_{i=1}^{n}p_{i,k_{i}}  &  =\underbrace{\sum
_{\kappa\in\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]
\right)  }}_{=\sum_{\kappa:\left[  n\right]  \rightarrow\left[  m\right]  }%
}\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }\\
&  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[  m\right]  }\prod
_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\end{align*}
Thus, (\ref{pf.lem.prodrule2.1}) becomes%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\underbrace{\left[  m\right]  \times\left[  m\right]
\times\cdots\times\left[  m\right]  }_{n\text{ factors}}}\prod_{i=1}%
^{n}p_{i,k_{i}}=\sum_{\kappa:\left[  n\right]  \rightarrow\left[  m\right]
}\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\]
Lemma \ref{lem.prodrule2} is proven.
\end{proof}

Now we are ready to prove what is probably the most important property of determinants:

\begin{theorem}
\label{thm.det(AB)}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two $n\times
n$-matrices. Then,%
\[
\det\left(  AB\right)  =\det A\cdot\det B.
\]

\end{theorem}

\begin{proof}
[Proof of Theorem \ref{thm.det(AB)}.]Write $A$ and $B$ in the forms $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. The definition of $AB$ thus
yields $AB=\left(  \sum_{k=1}^{n}a_{i,k}b_{k,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Therefore, (\ref{eq.det.eq.2}) (applied to $AB$ and
$\sum_{k=1}^{n}a_{i,k}b_{k,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}\left(  \sum_{k=1}^{n}a_{i,k}b_{k,\sigma\left(
i\right)  }\right)  }_{\substack{=\sum_{\kappa:\left[  n\right]
\rightarrow\left[  n\right]  }\prod_{i=1}^{n}\left(  a_{i,\kappa\left(
i\right)  }b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }\right)
\\\text{(by Lemma \ref{lem.prodrule2}, applied to }m=n\\\text{and }%
p_{i,k}=a_{i,k}b_{k,\sigma\left(  i\right)  }\text{)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\kappa:\left[
n\right]  \rightarrow\left[  n\right]  }\underbrace{\prod_{i=1}^{n}\left(
a_{i,\kappa\left(  i\right)  }b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }\right)  }_{=\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  \left(  \prod_{i=1}^{n}b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\kappa:\left[
n\right]  \rightarrow\left[  n\right]  }\left(  \prod_{i=1}^{n}a_{i,\kappa
\left(  i\right)  }\right)  \left(  \prod_{i=1}^{n}b_{\kappa\left(  i\right)
,\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[  n\right]  }\left(
\prod_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)  \left(  \sum_{\sigma\in
S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}b_{\kappa\left(  i\right)
,\sigma\left(  i\right)  }\right)  . \label{pf.thm.det(AB).4}%
\end{align}


Now, for every $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $, we
let $B_{\kappa}$ be the $n\times n$-matrix $\left(  b_{\kappa\left(  i\right)
,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, for every $\kappa:\left[
n\right]  \rightarrow\left[  n\right]  $, the equality (\ref{eq.det.eq.2})
(applied to $B_{\kappa}$ and $b_{\kappa\left(  i\right)  ,j}$ instead of $A$
and $a_{i,j}$) yields%
\begin{equation}
\det\left(  B_{\kappa}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }.
\label{pf.thm.det(AB).5}%
\end{equation}
Thus, (\ref{pf.thm.det(AB).4}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[
n\right]  }\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)
\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }\right)
}_{\substack{=\det\left(  B_{\kappa}\right)  \\\text{(by
(\ref{pf.thm.det(AB).5}))}}}\\
&  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[  n\right]  }\left(
\prod_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)  \det\left(  B_{\kappa
}\right) \\
&  =\underbrace{\sum_{\substack{\kappa:\left[  n\right]  \rightarrow\left[
n\right]  ;\\\kappa\in S_{n}}}}_{\substack{=\sum_{\kappa\in S_{n}%
}\\\text{(since every }\kappa\in S_{n}\text{ automatically}\\\text{is a map
}\left[  n\right]  \rightarrow\left[  n\right]  \text{)}}}\left(  \prod
_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)  \underbrace{\det\left(
B_{\kappa}\right)  }_{\substack{=\left(  -1\right)  ^{\kappa}\cdot\det
B\\\text{(by Lemma \ref{lem.det.sigma} \textbf{(a)})}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{\kappa:\left[  n\right]
\rightarrow\left[  n\right]  ;\\\kappa\notin S_{n}}}\left(  \prod_{i=1}%
^{n}a_{i,\kappa\left(  i\right)  }\right)  \underbrace{\det\left(  B_{\kappa
}\right)  }_{\substack{=0\\\text{(by Lemma \ref{lem.det.sigma} \textbf{(b)})}%
}}\\
&  =\sum_{\kappa\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  \left(  -1\right)  ^{\kappa}\cdot\det B+\underbrace{\sum
_{\substack{\kappa:\left[  n\right]  \rightarrow\left[  n\right]
;\\\kappa\notin S_{n}}}\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  0}_{=0}\\
&  =\sum_{\kappa\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  \left(  -1\right)  ^{\kappa}\cdot\det B=\sum_{\sigma\in S_{n}%
}\left(  \prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  \left(
-1\right)  ^{\sigma}\cdot\det B\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}\kappa\text{ as }\sigma\right) \\
&  =\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=\det
A\\\text{(by (\ref{eq.det.eq.2}))}}}\cdot\det B=\det A\cdot\det B.
\end{align*}
This proves Theorem \ref{thm.det(AB)}.
\end{proof}

\begin{remark}
The analogue of Theorem \ref{thm.det(AB)} with addition instead of
multiplication does not hold. If $A$ and $B$ are two $n\times n$-matrices for
some $n\in\mathbb{N}$, then $\det\left(  A+B\right)  $ does usually
\textbf{not} equal $\det A+\det B$.
\end{remark}

We shall now show several applications of Theorem \ref{thm.det(AB)}. First, a
simple corollary:

\begin{corollary}
\label{cor.det.product}Let $n\in\mathbb{N}$.

\textbf{(a)} If $B_{1},B_{2},\ldots,B_{k}$ are finitely many $n\times
n$-matrices, then $\det\left(  B_{1}B_{2}\cdots B_{k}\right)  =\prod_{i=1}%
^{k}\det\left(  B_{i}\right)  $.

\textbf{(b)} If $B$ is any $n\times n$-matrix, and $k\in\mathbb{N}$, then
$\det\left(  B^{k}\right)  =\left(  \det B\right)  ^{k}$.
\end{corollary}

\begin{vershort}


\begin{proof}
[Proof of Corollary \ref{cor.det.product}.]Corollary \ref{cor.det.product}
easily follows from Theorem \ref{thm.det(AB)} by induction over $k$. (The
induction base, $k=0$, relies on the fact that the product of $0$ matrices is
$I_{n}$ and has determinant $\det\left(  I_{n}\right)  =1$.) We leave the
details to the reader.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Corollary \ref{cor.det.product}.]\textbf{(a)} Let $B_{1}%
,B_{2},\ldots,B_{k}$ be finitely many $n\times n$-matrices. We shall show that%
\begin{equation}
\det\left(  B_{1}B_{2}\cdots B_{u}\right)  =\prod_{i=1}^{u}\det\left(
B_{i}\right)  \label{pf.cor.det.product.a.1}%
\end{equation}
for every $u\in\left\{  0,1,\ldots,k\right\}  $.

We shall prove (\ref{pf.cor.det.product.a.1}) by induction over $u$:

\textit{Induction base:} We have $\det\left(  \underbrace{B_{1}B_{2}\cdots
B_{0}}_{=\left(  \text{empty product of }n\times n\text{-matrices}\right)
=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$ and $\prod_{i=1}^{0}\det\left(
B_{i}\right)  =\left(  \text{empty product of elements of }\mathbb{K}\right)
=1$. Hence, $\det\left(  B_{1}B_{2}\cdots B_{0}\right)  =1=\prod_{i=1}^{0}%
\det\left(  B_{i}\right)  $. In other words, (\ref{pf.cor.det.product.a.1})
holds for $u=0$. This completes the induction base.

\textit{Induction step:} Let $U\in\left\{  0,1,\ldots,k\right\}  $ be
positive. Assume that (\ref{pf.cor.det.product.a.1}) holds for $u=U-1$. We
need to show that (\ref{pf.cor.det.product.a.1}) holds for $u=U$.

We have assumed that (\ref{pf.cor.det.product.a.1}) holds for $u=U-1$. In
other words, $\det\left(  B_{1}B_{2}\cdots B_{U-1}\right)  =\prod_{i=1}%
^{U-1}\det\left(  B_{i}\right)  $. Now,%
\begin{align*}
\det\underbrace{\left(  B_{1}B_{2}\cdots B_{U}\right)  }_{=\left(  B_{1}%
B_{2}\cdots B_{U-1}\right)  B_{U}}  &  =\det\left(  \left(  B_{1}B_{2}\cdots
B_{U-1}\right)  B_{U}\right)  =\underbrace{\det\left(  B_{1}B_{2}\cdots
B_{U-1}\right)  }_{=\prod_{i=1}^{U-1}\det\left(  B_{i}\right)  }\cdot
\det\left(  B_{U}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}A=B_{1}B_{2}\cdots B_{U-1}\text{ and }B=B_{U}\right) \\
&  =\left(  \prod_{i=1}^{U-1}\det\left(  B_{i}\right)  \right)  \cdot
\det\left(  B_{U}\right)  =\prod_{i=1}^{U}\det\left(  B_{i}\right)  .
\end{align*}
In other words, (\ref{pf.cor.det.product.a.1}) holds for $u=U$. This completes
the induction step. Thus, (\ref{pf.cor.det.product.a.1}) is proven by induction.

Now, (\ref{pf.cor.det.product.a.1}) (applied to $u=k$) yields $\det\left(
B_{1}B_{2}\cdots B_{k}\right)  =\prod_{i=1}^{k}\det\left(  B_{i}\right)  $.
Corollary \ref{cor.det.product} \textbf{(a)} is thus proven.

\textbf{(b)} Let $B$ be any $n\times n$-matrix. Applying Corollary
\ref{cor.det.product} \textbf{(a)} to $B_{i}=B$, we obtain $\det\left(
\underbrace{BB\cdots B}_{k\text{ factors}}\right)  =\prod_{i=1}^{k}\det
B=\left(  \det B\right)  ^{k}$. Since $\underbrace{BB\cdots B}_{k\text{
factors}}=B^{k}$, this rewrites as $\det\left(  B^{k}\right)  =\left(  \det
B\right)  ^{k}$. Corollary \ref{cor.det.product} \textbf{(b)} is thus proven.
\end{proof}
\end{verlong}

\begin{example}
\label{exam.det(AB).fibo}Recall that the Fibonacci sequence is the sequence
$\left(  f_{0},f_{1},f_{2},\ldots\right)  $ of integers which is defined
recursively by $f_{0}=0$, $f_{1}=1$, and $f_{n}=f_{n-1}+f_{n-2}$ for all
$n\geq2$. We shall prove that%
\begin{equation}
f_{n+1}f_{n-1}-f_{n}^{2}=\left(  -1\right)  ^{n}\ \ \ \ \ \ \ \ \ \ \text{for
every positive integer }n. \label{eq.exam.det(AB).fibo.1}%
\end{equation}
(This is a classical fact known as
\href{https://en.wikipedia.org/wiki/Cassini_and_Catalan_identities}{the
\textit{Cassini identity}} and easy to prove by induction, but we shall prove
it differently to illustrate the use of determinants.)

Let $B$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  $ (over the ring $\mathbb{Z}$). It is easy to see that $\det B=-1$.
But for every positive integer $n$, we have%
\begin{equation}
B^{n}=\left(
\begin{array}
[c]{cc}%
f_{n+1} & f_{n}\\
f_{n} & f_{n-1}%
\end{array}
\right)  . \label{eq.exam.det(AB).fibo.2}%
\end{equation}
Indeed, (\ref{eq.exam.det(AB).fibo.2}) can be easily proven by induction over
$n$: For $n=1$ it is clear by inspection; if it holds for $n=N$, then for
$n=N+1$ it follows from%
\begin{align*}
B^{N+1}  &  =\underbrace{B^{N}}_{\substack{=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  \\\text{(by the induction hypothesis)}}}\underbrace{B}_{=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  }=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
f_{N+1}\cdot1+f_{N}\cdot1 & f_{N+1}\cdot1+f_{N}\cdot0\\
f_{N}\cdot1+f_{N-1}\cdot1 & f_{N}\cdot1+f_{N-1}\cdot0
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of a product of two
matrices}\right) \\
&  =\left(
\begin{array}
[c]{cc}%
f_{N+1}+f_{N} & f_{N+1}\\
f_{N}+f_{N-1} & f_{N}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
f_{N+2} & f_{N+1}\\
f_{N+1} & f_{N}%
\end{array}
\right)
\end{align*}
(since $f_{N+1}+f_{N}=f_{N+2}$ and $f_{N}+f_{N-1}=f_{N+1}$).

Now, let $n$ be a positive integer. Then, (\ref{eq.exam.det(AB).fibo.2})
yields%
\[
\det\left(  B^{n}\right)  =\det\left(
\begin{array}
[c]{cc}%
f_{n+1} & f_{n}\\
f_{n} & f_{n-1}%
\end{array}
\right)  =f_{n+1}f_{n-1}-f_{n}^{2}.
\]
On the other hand, Corollary \ref{cor.det.product} \textbf{(b)} (applied to
$k=n$) yields $\det\left(  B^{n}\right)  =\left(  \underbrace{\det B}%
_{=-1}\right)  ^{n}=\left(  -1\right)  ^{n}$. Hence, $f_{n+1}f_{n-1}-f_{n}%
^{2}=\det\left(  B^{n}\right)  =\left(  -1\right)  ^{n}$. This proves
(\ref{eq.exam.det(AB).fibo.1}).
\end{example}

We can generalize (\ref{eq.exam.det(AB).fibo.1}) as follows:

\begin{exercise}
\label{exe.ps4.det.fibo}Let $a$ and $b$ be two complex numbers. Let $\left(
x_{0},x_{1},x_{2},\ldots\right)  $ be a sequence of complex numbers such that
every $n\geq2$ satisfies%
\begin{equation}
x_{n}=ax_{n-1}+bx_{n-2}. \label{eq.det.fibo.rec}%
\end{equation}
(We called such sequences \textquotedblleft$\left(  a,b\right)  $%
-recurrent\textquotedblright\ in Definition \ref{def.abrec}.) Let
$k\in\mathbb{N}$. Prove that
\begin{equation}
x_{n+1}x_{n-k-1}-x_{n}x_{n-k}=\left(  -b\right)  ^{n-k-1}\left(  x_{k+2}%
x_{0}-x_{k+1}x_{1}\right)  . \label{eq.det.fibo.claim}%
\end{equation}
for every integer $n>k$.
\end{exercise}

We notice that (\ref{eq.exam.det(AB).fibo.1}) can be obtained by applying
(\ref{eq.det.fibo.claim}) to $a=1$, $b=1$, $x_{i}=f_{i}$ and $k=0$. Thus,
(\ref{eq.det.fibo.claim}) is a generalization of (\ref{eq.exam.det(AB).fibo.1}%
). Notice that you could have easily come up with the identity
(\ref{eq.det.fibo.claim}) by trying to generalize the proof of
(\ref{eq.exam.det(AB).fibo.1}) we gave; in contrast, it is not that
straightforward to guess the general formula (\ref{eq.det.fibo.claim}) from
the classical proof of (\ref{eq.exam.det(AB).fibo.1}) by induction. So the
proof of (\ref{eq.exam.det(AB).fibo.1}) using determinants has at least the
advantage of pointing to a generalization.

\begin{example}
\label{exam.xiyj.3}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$
elements of $\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$ further
elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(
x_{i}y_{j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. In Example
\ref{exam.xiyj}, we have shown that $\det A=0$ if $n\geq2$. We can now prove
this in a simpler way.

Namely, let $n\geq2$. Define an $n\times n$-matrix $B$ by $B=\left(
\begin{array}
[c]{ccccc}%
x_{1} & 0 & 0 & \cdots & 0\\
x_{2} & 0 & 0 & \cdots & 0\\
x_{3} & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n} & 0 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first column of $B$ is $\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  ^{T}$, while all other columns are filled with zeroes.)
Define an $n\times n$-matrix $C$ by $C=\left(
\begin{array}
[c]{ccccc}%
y_{1} & y_{2} & y_{3} & \cdots & y_{n}\\
0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first row of $C$ is $\left(  y_{1},y_{2},\ldots
,y_{n}\right)  $, while all other rows are filled with zeroes.)

The second row of $C$ consists of zeroes (and this second row indeed exists,
because $n\geq2$). Thus, Exercise \ref{exe.ps4.6} \textbf{(c)} (applied to $C$
instead of $A$) yields $\det C=0$. Similarly, using Exercise \ref{exe.ps4.6}
\textbf{(d)}, we can show that $\det B=0$. Now, Theorem \ref{thm.det(AB)}
(applied to $B$ and $C$ instead of $A$ and $B$) yields $\det\left(  BC\right)
=\det B\cdot\underbrace{\det C}_{=0}=0$. But what is $BC$ ?

Write $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$, and write $C$ in the form $C=\left(  c_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Then, the definition of $BC$ yields%
\[
BC=\left(  \sum_{k=1}^{n}b_{i,k}c_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Therefore, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}$, the $\left(  i,j\right)  $-th entry of the matrix $BC$ is%
\[
\sum_{k=1}^{n}b_{i,k}c_{k,j}=\underbrace{b_{i,1}}_{=x_{i}}\underbrace{c_{1,j}%
}_{=y_{j}}+\sum_{k=2}^{n}\underbrace{b_{i,k}}_{=0}\underbrace{c_{k,j}}%
_{=0}=x_{i}y_{j}+\underbrace{\sum_{k=2}^{n}0\cdot0}_{=0}=x_{i}y_{j}.
\]
But this is the same as the $\left(  i,j\right)  $-th entry of the matrix $A$.
Thus, every entry of $BC$ equals the corresponding entry of $A$. Hence,
$BC=A$, so that $\det\left(  BC\right)  =\det A$. Thus, $\det A=\det\left(
BC\right)  =0$, just as we wanted to show.
\end{example}

\begin{example}
\label{exam.xi+yj.3}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$
further elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(
x_{i}+y_{j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. In Example
\ref{exam.xi+yj}, we have shown that $\det A=0$ if $n\geq3$.

We can now prove this in a simpler way. The argument is similar to Example
\ref{exam.xiyj.3}, and so I will be very brief:

Let $n\geq3$. Define an $n\times n$-matrix $B$ by $B=\left(
\begin{array}
[c]{ccccc}%
x_{1} & 1 & 0 & \cdots & 0\\
x_{2} & 1 & 0 & \cdots & 0\\
x_{3} & 1 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n} & 1 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first column of $B$ is $\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  ^{T}$, the second column is $\left(  1,1,\ldots
,1\right)  ^{T}$, while all other columns are filled with zeroes.) Define an
$n\times n$-matrix $C$ by $C=\left(
\begin{array}
[c]{ccccc}%
1 & 1 & 1 & \cdots & 1\\
y_{1} & y_{2} & y_{3} & \cdots & y_{n}\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first row of $C$ is $\left(  1,1,\ldots,1\right)  $,
the second row is $\left(  y_{1},y_{2},\ldots,y_{n}\right)  $, while all other
rows are filled with zeroes.) It is now easy to show that $BC=A$ (check
this!), but both $\det B$ and $\det C$ are $0$ (due to having a column or a
row filled with zeroes). Thus, again, we obtain $\det A=0$.
\end{example}

\begin{exercise}
\label{exe.ps4.pascal}Let $n\in\mathbb{N}$. Let $A$ be the $n\times n$-matrix
$\left(  \dbinom{i+j-2}{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(
\begin{array}
[c]{cccc}%
\dbinom{0}{0} & \dbinom{1}{0} & \cdots & \dbinom{n-1}{0}\\
\dbinom{1}{1} & \dbinom{2}{1} & \cdots & \dbinom{n}{1}\\
\vdots & \vdots & \ddots & \vdots\\
\dbinom{n-1}{n-1} & \dbinom{n}{n-1} & \cdots & \dbinom{2n-2}{n-1}%
\end{array}
\right)  $. (This matrix is a piece of Pascal's triangle \textquotedblleft
rotated by $45^{\circ}$\textquotedblright. For example, for $n=4$, we have
$A=\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
1 & 2 & 3 & 4\\
1 & 3 & 6 & 10\\
1 & 4 & 10 & 20
\end{array}
\right)  $.)

Show that $\det A=1$.
\end{exercise}

The matrix $A$ in Exercise \ref{exe.ps4.pascal} is one of the so-called
\textit{Pascal matrices}; see \cite{EdelStrang} for an enlightening exposition
of some of its properties (but beware of the fact that the very first page
reveals a significant part of the solution of Exercise \ref{exe.ps4.pascal}).

\begin{remark}
There exists a more general notion of a matrix, in which the rows and the
columns are indexed not necessarily by integers from $1$ to $n$ (for some
$n\in\mathbb{N}$), but rather by arbitrary objects. For instance, this more
general notion allows us to speak of a matrix with two rows labelled
\textquotedblleft spam\textquotedblright\ and \textquotedblleft
eggs\textquotedblright, and with three columns labelled $0$, $3$ and $\infty$.
(It thus has $6$ entries, such as the $\left(  \text{\textquotedblleft
spam\textquotedblright},3\right)  $-th entry or the $\left(
\text{\textquotedblleft eggs\textquotedblright},\infty\right)  $-th entry.)
This notion of matrices is more general and more flexible than the one used
above (e.g., it allows for infinite matrices), although it has some drawbacks
(e.g., notions such as \textquotedblleft lower-triangular\textquotedblright%
\ are not defined per se, because there might be no canonical way to order the
rows and the columns; also, infinite matrices cannot always be multiplied). We
might want to define the determinant of such a matrix. Of course, this only
makes sense when the rows of the matrix are indexed by the same objects as its
columns (this essentially says that the matrix is a \textquotedblleft square
matrix\textquotedblright\ in a reasonably general sense). So, let $X$ be a
set, and $A$ be a \textquotedblleft generalized matrix\textquotedblright%
\ whose rows and columns are both indexed by the elements of $X$. We want to
define $\det A$. We assume that $X$ is finite (indeed, while $\det A$
sometimes makes sense for infinite $X$, this only happens under some rather
restrictive conditions). Then, we can define $\det A$ by
\[
\det A=\sum_{\sigma\in S_{X}}\left(  -1\right)  ^{\sigma}\prod_{i\in
X}a_{i,\sigma\left(  i\right)  },
\]
where $S_{X}$ denotes the set of all permutations of $X$. This relies on a
definition of $\left(  -1\right)  ^{\sigma}$ for every $\sigma\in S_{X}$;
fortunately, we have provided such a definition in Exercise \ref{exe.ps4.2}.
\end{remark}

We shall see more about determinants later. So far we have barely scratched
the surface. Huge collections of problems and examples on the computation of
determinants can be found in \cite{Prasolov} and \cite{Krattenthaler} (and, if
you can be bothered with 100-years-old notation and level of rigor, in
\cite{Muir} -- one of the most comprehensive collections of \textquotedblleft
forgotten tales\textquotedblright\ in mathematics).

Let us finish this section with a brief remark on the geometrical use of determinants.

\begin{remark}
Let us consider the Euclidean plane $\mathbb{R}^{2}$ with its Cartesian
coordinate system and its origin $0$. If $A=\left(  x_{A},y_{A}\right)  $ and
$B=\left(  x_{B},y_{B}\right)  $ are two points on $\mathbb{R}^{2}$, then the
area of the triangle $0AB$ is $\dfrac{1}{2}\left\vert \det\left(
\begin{array}
[c]{cc}%
x_{A} & x_{B}\\
y_{A} & y_{B}%
\end{array}
\right)  \right\vert $. The absolute value here reflects the fact that
determinants can be negative, while areas must always be $\geq0$ (although
they can be $0$ when $0$, $A$ and $B$ are collinear); however, it makes
working with areas somewhat awkward. This can be circumvented by the notion of
a \textit{signed area}. (The signed area of a triangle $ABC$ is its regular
area if the triangle is \textquotedblleft directed clockwise\textquotedblright%
, and otherwise it is the negative of its area.) The signed area of the
triangle $0AB$ is $\det\left(
\begin{array}
[c]{cc}%
x_{A} & x_{B}\\
y_{A} & y_{B}%
\end{array}
\right)  $.

If $A=\left(  x_{A},y_{A}\right)  $, $B=\left(  x_{B},y_{B}\right)  $ and
$C=\left(  x_{C},y_{C}\right)  $ are three points in $\mathbb{R}^{2}$, then
the area of triangle $ABC$ is $\dfrac{1}{2}\det\left(
\begin{array}
[c]{ccc}%
x_{A} & x_{B} & x_{C}\\
y_{A} & y_{B} & y_{C}\\
1 & 1 & 1
\end{array}
\right)  $.

Similar formulas hold for tetrahedra: If $A=\left(  x_{A},y_{A},z_{A}\right)
$, $B=\left(  x_{B},y_{B},z_{B}\right)  $ and $C=\left(  x_{C},y_{C}%
,z_{C}\right)  $ are three points in $\mathbb{R}^{3}$, then the signed volume
of the tetrahedron $0ABC$ is $\dfrac{1}{6}\det\left(
\begin{array}
[c]{ccc}%
x_{A} & x_{B} & x_{C}\\
y_{A} & y_{B} & y_{C}\\
z_{A} & z_{B} & z_{C}%
\end{array}
\right)  $. (Again, take the absolute value for the non-signed volume.) There
is a $4\times4$ determinant formula for the signed volume of a general
tetrahedron $ABCD$.

One can generalize the notion of a triangle in $\mathbb{R}^{2}$ and the notion
of a tetrahedron in $\mathbb{R}^{3}$ to a notion of a \textit{simplex} in
$\mathbb{R}^{n}$. Then, one can try to define a notion of volume for these
objects. Determinants provide a way to do this. (Obviously, they don't allow
you to define the volume of a general \textquotedblleft convex
body\textquotedblright\ like a sphere, and even for simplices it is not
a-priori clear that they satisfy the standard properties that one would expect
them to have -- e.g., that the \textquotedblleft volume\textquotedblright\ of
a simplex does not change when one moves this simplex. But for the algebraic
part of analytic geometry, they are mostly sufficient. To define
\textquotedblleft volumes\textquotedblright\ for general convex bodies, one
needs calculus and the theory of integration in $\mathbb{R}^{n}$; but this
theory, too, uses determinants.)
\end{remark}

\subsection{The Cauchy-Binet formula}

This section is devoted to the Cauchy-Binet formula: a generalization of
Theorem \ref{thm.det(AB)} which is less well-known than the latter, but still
comes useful. This formula appears in the literature in various forms; we
follow \href{http://planetmath.org/cauchybinetformula}{the one on PlanetMath}
(although we use different notations).

First, we introduce a notation for \textquotedblleft picking out some rows of
a matrix and throwing away the rest\textquotedblright\ (and also the analogous
thing for columns):

\begin{definition}
\label{def.rowscols}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be an $n\times m$-matrix.

\textbf{(a)} If $i_{1},i_{2},\ldots,i_{u}$ are some elements of $\left\{
1,2,\ldots,n\right\}  $, then we let $\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A$ denote the $u\times m$-matrix $\left(  a_{i_{x}%
,j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}$. For instance, if $A=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $, then $\operatorname*{rows}\nolimits_{3,1,4}A=\left(
\begin{array}
[c]{ccc}%
c & c^{\prime} & c^{\prime\prime}\\
a & a^{\prime} & a^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $. For every $p\in\left\{  1,2,\ldots,u\right\}  $, we have%
\begin{align}
&  \left(  \text{the }p\text{-th row of }\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right) \nonumber\\
&  =\left(  a_{i_{p},1},a_{i_{p},2},\ldots,a_{i_{p},m}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A=\left(  a_{i_{x},j}\right)  _{1\leq x\leq u,\ 1\leq
j\leq m}\right) \nonumber\\
&  =\left(  \text{the }i_{p}\text{-th row of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right)  . \label{eq.def.rowscols.a.row=row}%
\end{align}
Thus, $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A$ is the
$u\times m$-matrix whose rows (from top to bottom) are the rows labelled
$i_{1},i_{2},\ldots,i_{u}$ of the matrix $A$.

\textbf{(b)} If $j_{1},j_{2},\ldots,j_{v}$ are some elements of $\left\{
1,2,\ldots,m\right\}  $, then we let $\operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}A$ denote the $n\times v$-matrix $\left(  a_{i,j_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}$. For instance, if $A=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  $, then $\operatorname*{cols}\nolimits_{3,2}A=\left(
\begin{array}
[c]{cc}%
a^{\prime\prime} & a^{\prime}\\
b^{\prime\prime} & b^{\prime}\\
c^{\prime\prime} & c^{\prime}%
\end{array}
\right)  $. For every $q\in\left\{  1,2,\ldots,v\right\}  $, we have%
\begin{align}
&  \left(  \text{the }q\text{-th column of }\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right) \nonumber\\
&  =\left(  a_{1,j_{q}},a_{2,j_{q}},\ldots,a_{n,j_{q}}\right)  ^{T}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}A=\left(  a_{i,j_{y}}\right)  _{1\leq i\leq n,\ 1\leq
y\leq v}\right) \nonumber\\
&  =\left(  \text{the }j_{q}\text{-th column of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right)  . \label{eq.def.rowscols.b.col=col}%
\end{align}
Thus, $\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A$ is the
$n\times v$-matrix whose columns (from left to right) are the columns labelled
$j_{1},j_{2},\ldots,j_{v}$ of the matrix $A$.
\end{definition}

Now we can state the \textit{Cauchy-Binet formula}:

\begin{theorem}
\label{thm.cauchy-binet}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be
an $n\times m$-matrix, and let $B$ be an $m\times n$-matrix. Then,%
\begin{equation}
\det\left(  AB\right)  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
. \label{eq.thm.cauchy-binet}%
\end{equation}

\end{theorem}

\begin{remark}
\label{rmk.cauchy-binet.sumsign}The summation sign $\sum_{1\leq g_{1}%
<g_{2}<\cdots<g_{n}\leq m}$ in (\ref{eq.thm.cauchy-binet}) is an abbreviation
for
\begin{equation}
\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}}}.
\label{eq.rmk.cauchy-binet.real-meaning}%
\end{equation}
In particular, if $n=0$, then it signifies a summation over all $0$-tuples of
elements of $\left\{  1,2,\ldots,m\right\}  $ (because in this case, the chain
of inequalities $g_{1}<g_{2}<\cdots<g_{n}$ is a tautology); such a sum always
has exactly one addend (because there is exactly one $0$-tuple).

When both $n$ and $m$ equal $0$, then the notation $\sum_{1\leq g_{1}%
<g_{2}<\cdots<g_{n}\leq m}$ is slightly confusing: It appears to mean an empty
summation (because $1\leq m$ does not hold). But as we said, we mean this
notation to be an abbreviation for (\ref{eq.rmk.cauchy-binet.real-meaning}),
which signifies a sum with exactly one addend. But this is enough pedantry for
now; for $n>0$, the notation $\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}$
fortunately means exactly what it seems to mean.
\end{remark}

Before we prove Theorem \ref{thm.cauchy-binet}, let us give some examples for
its use. First, here is a simple fact:

\begin{lemma}
\label{lem.increasing-sequences}Let $n\in\mathbb{N}$.

\textbf{(a)} There exists exactly one $n$-tuple $\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n}$, namely the $n$-tuple $\left(  1,2,\ldots,n\right)
$.

\textbf{(b)} Let $m\in\mathbb{N}$ be such that $m<n$. Then, there exists no
$n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots
,m\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$.
\end{lemma}

As for its intuitive meaning, Lemma \ref{lem.increasing-sequences} can be
viewed as a \textquotedblleft pigeonhole principle\textquotedblright\ for
strictly increasing sequences: Part \textbf{(b)} says (roughly speaking) that
there is no way to squeeze a strictly increasing sequence $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ of $n$ numbers into the set $\left\{
1,2,\ldots,m\right\}  $ when $m<n$; part \textbf{(a)} says (again, informally)
that the only such sequence for $m=n$ is $\left(  1,2,\ldots,n\right)  $.

\begin{exercise}
\label{exe.lem.increasing-sequences} Give a formal proof of Lemma
\ref{lem.increasing-sequences}. (Do not bother doing this if you do not
particularly care about formal proofs and find Lemma
\ref{lem.increasing-sequences} obvious enough.)
\end{exercise}

\begin{example}
\label{exam.cauchy-binet.nxn}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
$n\times n$-matrices. It is easy to check that $\operatorname*{cols}%
\nolimits_{1,2,\ldots,n}A=A$ and $\operatorname*{rows}\nolimits_{1,2,\ldots
,n}B=B$. Now, Theorem \ref{thm.cauchy-binet} (applied to $m=n$) yields%
\begin{equation}
\det\left(  AB\right)  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq n}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
. \label{eq.exam.cauchy-binet.nxn.1}%
\end{equation}
But Lemma \ref{lem.increasing-sequences} \textbf{(a)} yields that there exists
exactly one $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$, namely the
$n$-tuple $\left(  1,2,\ldots,n\right)  $. Hence, the sum on the right hand
side of (\ref{eq.exam.cauchy-binet.nxn.1}) has exactly one addend: namely, the
addend for $\left(  g_{1},g_{2},\ldots,g_{n}\right)  =\left(  1,2,\ldots
,n\right)  $. Therefore, this sum simplifies as follows:%
\begin{align*}
&  \sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq n}\det\left(  \operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \\
&  =\det\left(  \underbrace{\operatorname*{cols}\nolimits_{1,2,\ldots,n}%
A}_{=A}\right)  \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{1,2,\ldots,n}B}_{=B}\right)  =\det A\cdot\det B.
\end{align*}
Hence, (\ref{eq.exam.cauchy-binet.nxn.1}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq n}%
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}%
}B\right) \\
&  =\det A\cdot\det B.
\end{align*}
This, of course, is the statement of Theorem \ref{thm.det(AB)}. Hence, Theorem
\ref{thm.det(AB)} is a particular case of Theorem \ref{thm.cauchy-binet}.
\end{example}

\begin{example}
\label{exam.cauchy-binet.0}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such
that $m<n$. Thus, Lemma \ref{lem.increasing-sequences} \textbf{(b)} shows that
there exists no $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$.

Now, let $A$ be an $n\times m$-matrix, and let $B$ be an $m\times n$-matrix.
Then, Theorem \ref{thm.cauchy-binet} yields%
\begin{align}
\det\left(  AB\right)   &  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}%
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}%
}B\right) \nonumber\\
&  =\left(  \text{empty sum}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since there exists no }n\text{-tuple }\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}\\
\text{satisfying }g_{1}<g_{2}<\cdots<g_{n}%
\end{array}
\right) \nonumber\\
&  =0. \label{eq.exam.cauchy-binet.0}%
\end{align}
This identity allows us to compute $\det A$ in Example \ref{exam.xiyj.3} in a
simpler way: Instead of defining two $n\times n$-matrices $B$ and $C$ by
$B=\left(
\begin{array}
[c]{ccccc}%
x_{1} & 0 & 0 & \cdots & 0\\
x_{2} & 0 & 0 & \cdots & 0\\
x_{3} & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n} & 0 & 0 & \cdots & 0
\end{array}
\right)  $ and $C=\left(
\begin{array}
[c]{ccccc}%
y_{1} & y_{2} & y_{3} & \cdots & y_{n}\\
0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $, it suffices to define an $n\times1$-matrix $B^{\prime}$ by
$B^{\prime}=\left(  x_{1},x_{2},\ldots,x_{n}\right)  ^{T}$ and a $1\times
n$-matrix $C^{\prime}$ by $C^{\prime}=\left(  y_{1},y_{2},\ldots,y_{n}\right)
$, and argue that $A=B^{\prime}C^{\prime}$. (We leave the details to the
reader.) Similarly, Example \ref{exam.xi+yj.3} could be dealt with.
\end{example}

\begin{remark}
The equality (\ref{eq.exam.cauchy-binet.0}) can also be derived from Theorem
\ref{thm.det(AB)}. Indeed, let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such
that $m<n$. Let $A$ be an $n\times m$-matrix, and let $B$ be an $m\times
n$-matrix. Notice that $n-m>0$ (since $m<n$). Let $A^{\prime}$ be the $n\times
n$-matrix obtained from $A$ by appending $n-m$ new columns to the right of $A$
and filling these columns with zeroes. (For example, if $n=4$ and $m=2$ and
$A=\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}\\
a_{3,1} & a_{3,2}\\
a_{4,1} & a_{4,2}%
\end{array}
\right)  $, then $A^{\prime}=\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & 0 & 0\\
a_{2,1} & a_{2,2} & 0 & 0\\
a_{3,1} & a_{3,2} & 0 & 0\\
a_{4,1} & a_{4,2} & 0 & 0
\end{array}
\right)  $.) Also, let $B^{\prime}$ be the $n\times n$-matrix obtained from
$B$ by appending $n-m$ new rows to the bottom of $B$ and filling these rows
with zeroes. (For example, if $n=4$ and $m=2$ and $B=\left(
\begin{array}
[c]{cccc}%
b_{1,1} & b_{1,2} & b_{1,3} & b_{1,4}\\
b_{2,1} & b_{2,2} & b_{2,3} & b_{2,4}%
\end{array}
\right)  $, then $B^{\prime}=\left(
\begin{array}
[c]{cccc}%
b_{1,1} & b_{1,2} & b_{1,3} & b_{1,4}\\
b_{2,1} & b_{2,2} & b_{2,3} & b_{2,4}\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  $.) Then, it is easy to check that $AB=A^{\prime}B^{\prime}$ (in
fact, just compare corresponding entries of $AB$ and $A^{\prime}B^{\prime}$).
But recall that $n-m>0$. Hence, the matrix $A^{\prime}$ has a column
consisting of zeroes (namely, its last column). Thus, Exercise \ref{exe.ps4.6}
\textbf{(d)} (applied to $A^{\prime}$ instead of $A$) shows that $\det\left(
A^{\prime}\right)  =0$. Now,
\begin{align*}
\det\left(  \underbrace{AB}_{=A^{\prime}B^{\prime}}\right)   &  =\det\left(
A^{\prime}B^{\prime}\right)  =\underbrace{\det\left(  A^{\prime}\right)
}_{=0}\cdot\det\left(  B^{\prime}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}A^{\prime}\text{ and }B^{\prime}\text{ instead of }A\text{ and }B\right) \\
&  =0.
\end{align*}
Thus, (\ref{eq.exam.cauchy-binet.0}) is proven again.
\end{remark}

\begin{example}
\label{exam.cauchy-binet.1}Let us see what Theorem \ref{thm.cauchy-binet} says
for $n=1$. Indeed, let $m\in\mathbb{N}$; let $A=\left(  a_{1},a_{2}%
,\ldots,a_{m}\right)  $ be a $1\times m$-matrix (i.e., a row vector of length
$m$), and let $B=\left(  b_{1},b_{2},\ldots,b_{m}\right)  ^{T}$ be an
$m\times1$-matrix (i.e., a column vector of length $m$). Then, $AB$ is the
$1\times1$-matrix $\left(
\begin{array}
[c]{c}%
\sum_{k=1}^{m}a_{k}b_{k}%
\end{array}
\right)  $. Thus,
\begin{equation}
\det\left(  AB\right)  =\det\left(
\begin{array}
[c]{c}%
\sum_{k=1}^{m}a_{k}b_{k}%
\end{array}
\right)  =\sum_{k=1}^{m}a_{k}b_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.det.small.1x1})}\right)  . \label{eq.exam.cauchy-binet.1}%
\end{equation}
What would we obtain if we tried to compute $\det\left(  AB\right)  $ using
Theorem \ref{thm.cauchy-binet}? Theorem \ref{thm.cauchy-binet} (applied to
$n=1$) yields%
\begin{align*}
\det\left(  AB\right)   &  =\underbrace{\sum_{1\leq g_{1}\leq m}}%
_{=\sum_{g_{1}=1}^{m}}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{g_{1}}A}_{=\left(
\begin{array}
[c]{c}%
a_{g_{1}}%
\end{array}
\right)  }\right)  \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{g_{1}}B}_{=\left(
\begin{array}
[c]{c}%
b_{g_{1}}%
\end{array}
\right)  }\right) \\
&  =\sum_{g_{1}=1}^{m}\underbrace{\det\left(
\begin{array}
[c]{c}%
a_{g_{1}}%
\end{array}
\right)  }_{\substack{=a_{g_{1}}\\\text{(by (\ref{eq.det.small.1x1}))}}%
}\cdot\underbrace{\det\left(
\begin{array}
[c]{c}%
b_{g_{1}}%
\end{array}
\right)  }_{\substack{=b_{g_{1}}\\\text{(by (\ref{eq.det.small.1x1}))}}%
}=\sum_{g_{1}=1}^{m}a_{g_{1}}\cdot b_{g_{1}}.
\end{align*}
This is, of course, the same result as that of (\ref{eq.exam.cauchy-binet.1})
(with the summation index $k$ renamed as $g_{1}$). So we did not gain any
interesting insight from applying Theorem \ref{thm.cauchy-binet} to $n=1$.
\end{example}

\begin{example}
\label{exam.cauchy-binet.2}Let us try a slightly less trivial case. Indeed,
let $m\in\mathbb{N}$; let $A=\left(
\begin{array}
[c]{cccc}%
a_{1} & a_{2} & \cdots & a_{m}\\
a_{1}^{\prime} & a_{2}^{\prime} & \cdots & a_{m}^{\prime}%
\end{array}
\right)  $ be a $2\times m$-matrix, and let $B=\left(
\begin{array}
[c]{cc}%
b_{1} & b_{1}^{\prime}\\
b_{2} & b_{2}^{\prime}\\
\vdots & \vdots\\
b_{m} & b_{m}^{\prime}%
\end{array}
\right)  $ be an $m\times2$-matrix. Then, $AB$ is the $2\times2$-matrix
$\left(
\begin{array}
[c]{cc}%
\sum_{k=1}^{m}a_{k}b_{k} & \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\\
\sum_{k=1}^{m}a_{k}^{\prime}b_{k} & \sum_{k=1}^{m}a_{k}^{\prime}b_{k}^{\prime}%
\end{array}
\right)  $. Hence,%
\begin{align}
\det\left(  AB\right)   &  =\det\left(
\begin{array}
[c]{cc}%
\sum_{k=1}^{m}a_{k}b_{k} & \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\\
\sum_{k=1}^{m}a_{k}^{\prime}b_{k} & \sum_{k=1}^{m}a_{k}^{\prime}b_{k}^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{k}b_{k}\right)  \left(  \sum_{k=1}^{m}%
a_{k}^{\prime}b_{k}^{\prime}\right)  -\left(  \sum_{k=1}^{m}a_{k}^{\prime
}b_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\right)  .
\label{eq.exam.cauchy-binet.2.1}%
\end{align}


On the other hand, Theorem \ref{thm.cauchy-binet} (now applied to $n=2$)
yields
\begin{align*}
\det\left(  AB\right)   &  =\sum_{1\leq g_{1}<g_{2}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2}}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2}}B\right) \\
&  =\sum_{1\leq i<j\leq m}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{i,j}A}_{=\left(
\begin{array}
[c]{cc}%
a_{i} & a_{j}\\
a_{i}^{\prime} & a_{j}^{\prime}%
\end{array}
\right)  }\right)  \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{i,j}B}_{=\left(
\begin{array}
[c]{cc}%
b_{i} & b_{i}^{\prime}\\
b_{j} & b_{j}^{\prime}%
\end{array}
\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we renamed the summation indices }g_{1}\text{ and }g_{2}\\
\text{as }i\text{ and }j\text{, since double subscripts are annoying}%
\end{array}
\right) \\
&  =\sum_{1\leq i<j\leq m}\underbrace{\det\left(
\begin{array}
[c]{cc}%
a_{i} & a_{j}\\
a_{i}^{\prime} & a_{j}^{\prime}%
\end{array}
\right)  }_{=a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime}}\cdot\underbrace{\det
\left(
\begin{array}
[c]{cc}%
b_{i} & b_{i}^{\prime}\\
b_{j} & b_{j}^{\prime}%
\end{array}
\right)  }_{=b_{i}b_{j}^{\prime}-b_{j}b_{i}^{\prime}}\\
&  =\sum_{1\leq i<j\leq m}\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime
}\right)  \cdot\left(  b_{i}b_{j}^{\prime}-b_{j}b_{i}^{\prime}\right)  .
\end{align*}
Compared with (\ref{eq.exam.cauchy-binet.2.1}), this yields%
\begin{align}
&  \left(  \sum_{k=1}^{m}a_{k}b_{k}\right)  \left(  \sum_{k=1}^{m}%
a_{k}^{\prime}b_{k}^{\prime}\right)  -\left(  \sum_{k=1}^{m}a_{k}^{\prime
}b_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\right) \nonumber\\
&  =\sum_{1\leq i<j\leq m}\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime
}\right)  \cdot\left(  b_{i}b_{j}^{\prime}-b_{j}b_{i}^{\prime}\right)  .
\label{eq.exam.cauchy-binet.2.binet-cauchy}%
\end{align}
This identity is called
\href{https://en.wikipedia.org/wiki/Binet-Cauchy_identity}{the
\textit{Binet-Cauchy identity}} (I am not kidding -- look it up on the
Wikipedia). It is fairly easy to prove by direct computation; thus, using
Theorem \ref{thm.cauchy-binet} to prove it was quite an overkill. However,
(\ref{eq.exam.cauchy-binet.2.binet-cauchy}) might not be very easy to come up
with, whereas deriving it from Theorem \ref{thm.cauchy-binet} is
straightforward. (And Theorem \ref{thm.cauchy-binet} is easier to memorize
than (\ref{eq.exam.cauchy-binet.2.binet-cauchy}).)

Here is a neat application of (\ref{eq.exam.cauchy-binet.2.binet-cauchy}): If
$a_{1},a_{2},\ldots,a_{m}$ and $a_{1}^{\prime},a_{2}^{\prime},\ldots
,a_{m}^{\prime}$ are real numbers, then
(\ref{eq.exam.cauchy-binet.2.binet-cauchy}) (applied to $b_{k}=a_{k}$ and
$b_{k}^{\prime}=a_{k}^{\prime}$) yields%
\begin{align*}
&  \left(  \sum_{k=1}^{m}a_{k}a_{k}\right)  \left(  \sum_{k=1}^{m}%
a_{k}^{\prime}a_{k}^{\prime}\right)  -\left(  \sum_{k=1}^{m}a_{k}^{\prime
}a_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}a_{k}^{\prime}\right) \\
&  =\sum_{1\leq i<j\leq m}\underbrace{\left(  a_{i}a_{j}^{\prime}-a_{j}%
a_{i}^{\prime}\right)  \cdot\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime
}\right)  }_{=\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime}\right)  ^{2}%
\geq0}\geq\sum_{1\leq i<j\leq m}0=0,
\end{align*}
so that%
\[
\left(  \sum_{k=1}^{m}a_{k}a_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}^{\prime
}a_{k}^{\prime}\right)  \geq\left(  \sum_{k=1}^{m}a_{k}^{\prime}a_{k}\right)
\left(  \sum_{k=1}^{m}a_{k}a_{k}^{\prime}\right)  .
\]
In other words,%
\[
\left(  \sum_{k=1}^{m}a_{k}^{2}\right)  \left(  \sum_{k=1}^{m}\left(
a_{k}^{\prime}\right)  ^{2}\right)  \geq\left(  \sum_{k=1}^{m}a_{k}%
a_{k}^{\prime}\right)  ^{2}.
\]
This is the famous
\href{https://en.wikipedia.org/wiki/Cauchy-Schwarz_inequality}{Cauchy-Schwarz
inequality}.
\end{example}

Let us now prepare for the proof of Theorem \ref{thm.cauchy-binet}. First
comes a fact which should be fairly clear:

\begin{proposition}
\label{prop.sorting}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ integers.

\textbf{(a)} There exists a permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq
a_{\sigma\left(  n\right)  }$.

\textbf{(b)} If $\sigma\in S_{n}$ is such that $a_{\sigma\left(  1\right)
}\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }%
$, then, for every $i\in\left\{  1,2,\ldots,n\right\}  $, the value
$a_{\sigma\left(  i\right)  }$ depends only on $a_{1},a_{2},\ldots,a_{n}$ and
$i$ (but not on $\sigma$).

\textbf{(c)} Assume that the integers $a_{1},a_{2},\ldots,a_{n}$ are distinct.
Then, there is a \textbf{unique} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$.
\end{proposition}

Let me explain why this proposition should be intuitively
obvious.\footnote{See the solution of Exercise \ref{exe.sorting.basics}
further below for a formal proof of this proposition.} Proposition
\ref{prop.sorting} \textbf{(a)} says that any list $\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)  $ of $n$ integers can be sorted in weakly increasing
order by means of a permutation $\sigma\in S_{n}$. Proposition
\ref{prop.sorting} \textbf{(b)} says that the result of this sorting process
is independent of how the sorting happened (although the permutation $\sigma$
will sometimes be non-unique). Proposition \ref{prop.sorting} \textbf{(c)}
says that if the integers $a_{1},a_{2},\ldots,a_{n}$ are distinct, then the
permutation $\sigma\in S_{n}$ which sorts the list $\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)  $ in increasing order is uniquely determined as well. We
required $a_{1},a_{2},\ldots,a_{n}$ to be $n$ integers for the sake of
simplicity, but we could just as well have required them to be elements of any
\textit{totally ordered set} (i.e., any set with a less-than relation
satisfying some standard axioms).

The next fact looks slightly scary, but is still rather simple:

\begin{lemma}
\label{lem.cauchy-binet.EI}For every $n\in\mathbb{N}$, let $\left[  n\right]
$ denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. We let $\mathbf{E}$ be the subset%
\[
\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots,k_{n}\text{ are
distinct}\right\}
\]
of $\left[  m\right]  ^{n}$. We let $\mathbf{I}$ be the subset%
\[
\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}
\]
of $\left[  m\right]  ^{n}$. Then, the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is well-defined and is a bijection.
\end{lemma}

The intuition for Lemma \ref{lem.cauchy-binet.EI} is that every $n$-tuple of
distinct elements of $\left\{  1,2,\ldots,m\right\}  $ can be represented
uniquely as a permuted version of a strictly increasing\footnote{An $n$-tuple
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ is said to be \textit{strictly
increasing} if and only if $k_{1}<k_{2}<\cdots<k_{n}$.} $n$-tuple of elements
of $\left\{  1,2,\ldots,m\right\}  $, and therefore, specifying an $n$-tuple
of distinct elements of $\left\{  1,2,\ldots,m\right\}  $ is tantamount to
specifying a strictly increasing $n$-tuple of elements of $\left\{
1,2,\ldots,m\right\}  $ and a permutation $\sigma\in S_{n}$ which says how
this $n$-tuple is to be permuted.\footnote{For instance, the $4$-tuple
$\left(  4,1,6,2\right)  $ of distinct elements of $\left\{  1,2,\ldots
,7\right\}  $ can be specified by specifying the strictly increasing $4$-tuple
$\left(  1,2,4,6\right)  $ (which is its sorted version) and the permutation
$\pi\in S_{4}$ which sends $1,2,3,4$ to $3,1,4,2$, respectively (that is,
$\pi=\left(  3,1,4,2\right)  $ in one-line notation). In the terminology of
Lemma \ref{lem.cauchy-binet.EI}, the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
sends $\left(  \left(  1,2,4,6\right)  ,\pi\right)  $ to $\left(
4,1,6,2\right)  $.} This is not a formal proof, but this should explain why
Lemma \ref{lem.cauchy-binet.EI} is usually applied throughout mathematics
without even mentioning it as a statement. If desired, a formal proof of Lemma
\ref{lem.cauchy-binet.EI} can be obtained using Proposition \ref{prop.sorting}%
.\footnote{Again, see the solution of Exercise \ref{exe.sorting.basics}
further below for such a proof.}

\begin{exercise}
\label{exe.sorting.basics}Prove Proposition \ref{prop.sorting} and Lemma
\ref{lem.cauchy-binet.EI}. (Ignore this exercise if you find these two facts
sufficiently obvious and are uninterested in the details of their proofs.)
\end{exercise}

Before we return to Theorem \ref{thm.cauchy-binet}, let me make a digression
about sorting:

\begin{exercise}
\label{exe.sorting.nmu}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such that
$n\geq m$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$ integers. Let $b_{1}%
,b_{2},\ldots,b_{m}$ be $m$ integers. Assume that%
\begin{equation}
a_{i}\leq b_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,m\right\}  . \label{eq.exe.sorting.nmu.ass}%
\end{equation}


Let $\sigma\in S_{n}$ be such that $a_{\sigma\left(  1\right)  }\leq
a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$. Let
$\tau\in S_{m}$ be such that $b_{\tau\left(  1\right)  }\leq b_{\tau\left(
2\right)  }\leq\cdots\leq b_{\tau\left(  m\right)  }$. Then,%
\[
a_{\sigma\left(  i\right)  }\leq b_{\tau\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,m\right\}  .
\]

\end{exercise}

\begin{remark}
\label{rmk.sorting.nmu.interpret}Loosely speaking, Exercise
\ref{exe.sorting.nmu} says the following: If two lists $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ and $\left(  b_{1},b_{2},\ldots,b_{m}\right)  $
of integers have the property that each entry of the first list is $\leq$ to
the corresponding entry of the second list (as long as the latter is
well-defined), then this property still holds after both lists are sorted in
increasing order, provided that we have $n\geq m$ (that is, the first list is
at least as long as the second list).

A consequence of Exercise \ref{exe.sorting.nmu} is the following curious fact,
known as the \textquotedblleft non-messing-up phenomenon\textquotedblright%
\ (\cite[Theorem 1]{Tenner-NMU}): If we start with a matrix filled with
integers, then sort the entries of each row of the matrix in increasing order,
and then sort the entries of each column of the resulting matrix in increasing
order, then the final matrix still has sorted rows (i.e., the entries of each
row are still sorted). That is, the sorting of the columns did not
\textquotedblleft mess up\textquotedblright\ the sortedness of the rows. For
example, if we start with the matrix $\left(
\begin{array}
[c]{cccc}%
1 & 3 & 2 & 5\\
2 & 1 & 4 & 2\\
3 & 1 & 6 & 0
\end{array}
\right)  $, then sorting the entries of each row gives us the matrix $\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 5\\
1 & 2 & 2 & 4\\
0 & 1 & 3 & 6
\end{array}
\right)  $, and then sorting the entries of each column results in the matrix
$\left(
\begin{array}
[c]{cccc}%
0 & 1 & 2 & 4\\
1 & 2 & 3 & 5\\
1 & 2 & 3 & 6
\end{array}
\right)  $. The rows of this matrix are still sorted, as the \textquotedblleft
non-messing-up phenomenon\textquotedblright\ predicts. To prove this
phenomenon in general, it suffices to show that any entry in the resulting
matrix is $\leq$ to the entry directly below it (assuming that the latter
exists); but this follows easily from Exercise \ref{exe.sorting.nmu}.
\end{remark}

We are now ready to prove Theorem \ref{thm.cauchy-binet}.

\begin{vershort}


\begin{proof}
[Proof of Theorem \ref{thm.cauchy-binet}.]We shall use the notations of Lemma
\ref{lem.cauchy-binet.EI}.

Write the $n\times m$-matrix $A$ as $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Write the $m\times n$-matrix $B$ as $B=\left(
b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. The definition of $AB$ thus
yields $AB=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Therefore, (\ref{eq.det.eq.2}) (applied to $AB$ and
$\sum_{k=1}^{m}a_{i,k}b_{k,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det\left(  AB\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.short.1}%
\end{equation}
But for every $\sigma\in S_{n}$, we have%
\begin{align*}
\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}}\underbrace{\prod_{i=1}^{n}\left(  a_{i,k_{i}}b_{k_{i}%
,\sigma\left(  i\right)  }\right)  }_{=\left(  \prod_{i=1}^{n}a_{i,k_{i}%
}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)
}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.prodrule}, applied to
}m_{i}=n\text{ and }p_{i,k}=a_{i,k}b_{k,\sigma\left(  i\right)  }\right) \\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }\right)  .
\end{align*}
Hence, (\ref{pf.thm.cauchy-binet.short.1}) rewrites as%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }\right) \nonumber\\
&  =\underbrace{\sum_{\sigma\in S_{n}}\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n}}}_{=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}}\sum_{\sigma\in S_{n}}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(
i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.short.3}%
\end{align}
But every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfies%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }=\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
\label{pf.thm.cauchy-binet.short.detrows}%
\end{equation}
\footnote{\textit{Proof.} Let $\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}$. Recall that $B=\left(  b_{i,j}\right)  _{1\leq
i\leq m,\ 1\leq j\leq n}$. Hence, the definition of $\operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ gives us%
\[
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{i},j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ and
$b_{k_{i},j}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  },
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.short.3}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }_{=\det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  }\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\label{pf.thm.cauchy-binet.short.4}%
\end{align}


But for every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}$ satisfying $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$,
we have%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=0 \label{pf.thm.cauchy-binet.short.6a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.short.6a}):} Let $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ be such that
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$. Then, the
integers $k_{1},k_{2},\ldots,k_{n}$ are not distinct (because $\mathbf{E}$ is
the set of all $n$-tuples in $\left[  m\right]  ^{n}$ whose entries are
distinct). Thus, there exist two distinct elements $p$ and $q$ of $\left[
n\right]  $ such that $k_{p}=k_{q}$. Consider these $p$ and $q$. But
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ is the $n\times
n$-matrix whose rows (from top to bottom) are the rows labelled $k_{1}%
,k_{2},\ldots,k_{n}$ of the matrix $B$. Since $k_{p} = k_{q}$, this shows that
the $p$-th row and the $q$-th row of the matrix $\operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ are equal. Hence, the matrix
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ has two equal rows
(since $p$ and $q$ are distinct). Therefore, Exercise \ref{exe.ps4.6}
\textbf{(e)} (applied to $\operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B$ instead of $A$) yields $\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  =0$, qed.}. Therefore, in the
sum on the right hand side of (\ref{pf.thm.cauchy-binet.short.4}), all the
addends corresponding to $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}$ satisfying $\left(  k_{1},k_{2},\ldots,k_{n}\right)
\notin\mathbf{E}$ evaluate to $0$. We can therefore remove all these addends
from the sum. The remaining addends are those corresponding to $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}$. Therefore,
(\ref{pf.thm.cauchy-binet.short.4}) becomes
\begin{equation}
\det\left(  AB\right)  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\label{pf.thm.cauchy-binet.short.7}%
\end{equation}


On the other hand, Lemma \ref{lem.cauchy-binet.EI} yields that the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is well-defined and is a bijection. Hence, we can substitute $\left(
g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots
,g_{\sigma\left(  n\right)  }\right)  $ for $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  $ in the sum on the right hand side of
(\ref{pf.thm.cauchy-binet.short.7}). We thus obtain%
\begin{align*}
&  \sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}}\left(
\prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  =\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(
i\right)  }}\right)  \det\left(  \operatorname*{rows}\nolimits_{g_{\sigma
\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(
n\right)  }}B\right)  .
\end{align*}
Thus, (\ref{pf.thm.cauchy-binet.short.7}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \nonumber\\
&  =\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(
i\right)  }}\right)  \det\left(  \operatorname*{rows}\nolimits_{g_{\sigma
\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(
n\right)  }}B\right)  . \label{pf.thm.cauchy-binet.short.9}%
\end{align}


But every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
and every $\sigma\in S_{n}$ satisfy%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
\label{pf.thm.cauchy-binet.short.6b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.short.6b}):} Let $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ and $\sigma\in
S_{n}$. We have $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}%
}B=\left(  b_{k_{i},j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (as we have
seen in one of the previous footnotes) and $\operatorname*{rows}%
\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(  2\right)  }%
,\ldots,k_{\sigma\left(  n\right)  }}B=\left(  b_{k_{\sigma\left(  i\right)
},j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (for similar reasons). Hence,
we can apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $\sigma$,
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ and
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B$ instead of $\kappa$, $B$
and $B_{\kappa}$. As a consequence, we obtain
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\]
This proves (\ref{pf.thm.cauchy-binet.short.6b}).}. Hence,
(\ref{pf.thm.cauchy-binet.short.9}) becomes%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\underbrace{\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)
,\sigma\right)  \in\mathbf{I}\times S_{n}}}_{=\sum_{\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\mathbf{I}}\sum_{\sigma\in S_{n}}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \underbrace{\det\left(
\operatorname*{rows}\nolimits_{g_{\sigma\left(  1\right)  },g_{\sigma\left(
2\right)  },\ldots,g_{\sigma\left(  n\right)  }}B\right)  }%
_{\substack{=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  \\\text{(by
(\ref{pf.thm.cauchy-binet.short.6b}), applied to }k_{i}=g_{i}\text{)}%
}}\nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\sum
_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }%
}\right)  \left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\left(
\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  .
\label{pf.thm.cauchy-binet.short.10}%
\end{align}
But every $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ satisfies
$\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}=\det\left(  \operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\mathbf{I}$. We have $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$. Thus, the definition of $\operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A$ yields%
\[
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A=\left(  a_{i,g_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq n}=\left(  a_{i,g_{j}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $y$ as $j$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A$ and
$a_{i,g_{j}}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}=\sum_{\sigma\in S_{n}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)
^{\sigma},
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.short.10}) becomes%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}%
}\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)  ^{\sigma
}\right)  }_{=\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A\right)  }\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
. \label{pf.thm.cauchy-binet.short.15}%
\end{align}
Finally, we recall that $\mathbf{I}$ was defined as the set $\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  $. Thus, summing over all $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ means the same as summing over all
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left[  m\right]  ^{n}$
satisfying $g_{1}<g_{2}<\cdots<g_{n}$. In other words,%
\[
\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}=\sum
_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left[  m\right]
^{n};\\g_{1}<g_{2}<\cdots<g_{n}}}=\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}%
\]
(an equality between summation signs -- hopefully its meaning is obvious).
Hence, (\ref{pf.thm.cauchy-binet.short.15}) becomes%
\[
\det\left(  AB\right)  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
.
\]
This proves Theorem \ref{thm.cauchy-binet}.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Theorem \ref{thm.cauchy-binet}.]We shall use the notations of Lemma
\ref{lem.cauchy-binet.EI}. Recall that%
\[
\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots,k_{n}\text{ are
distinct}\right\}
\]
and%
\begin{align*}
\mathbf{I}  &  =\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\} \\
&  =\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left\{
1,2,\ldots,m\right\}  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  m\right]  =\left\{
1,2,\ldots,m\right\}  \right) \\
&  =\left\{  \left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,m\right\}  ^{n}\ \mid\ g_{1}<g_{2}<\cdots<g_{n}\right\}
\end{align*}
(here, we renamed the index $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ as
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  $).

Lemma \ref{lem.cauchy-binet.EI} says that the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is well-defined and is a bijection.

Write the $n\times m$-matrix $A$ as $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Write the $m\times n$-matrix $B$ as $B=\left(
b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. The definition of $AB$ thus
yields $AB=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Therefore, (\ref{eq.det.eq.2}) (applied to $AB$ and
$\sum_{k=1}^{m}a_{i,k}b_{k,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det\left(  AB\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.1}%
\end{equation}
But for every $\sigma\in S_{n}$, we have%
\begin{align*}
\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in
\underbrace{\left[  m\right]  \times\left[  m\right]  \times\cdots
\times\left[  m\right]  }_{n\text{ factors}}}\prod_{i=1}^{n}\left(
a_{i,k_{i}}b_{k_{i},\sigma\left(  i\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.prodrule}, applied to
}m_{i}=n\text{ and }p_{i,k}=a_{i,k}b_{k,\sigma\left(  i\right)  }\right) \\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\underbrace{\prod_{i=1}^{n}\left(  a_{i,k_{i}}b_{k_{i},\sigma\left(
i\right)  }\right)  }_{=\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{\left[  m\right]
\times\left[  m\right]  \times\cdots\times\left[  m\right]  }_{n\text{
factors}}=\left[  m\right]  ^{n}\right) \\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }\right)  .
\end{align*}
Hence, (\ref{pf.thm.cauchy-binet.1}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(
i\right)  }\right)  }_{=\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}}\left(  \prod
_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(
i\right)  }\right) \nonumber\\
&  =\underbrace{\sum_{\sigma\in S_{n}}\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n}}}_{=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}}\sum_{\sigma\in S_{n}}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(
i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.3}%
\end{align}
But every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfies $\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}b_{k_{i},\sigma\left(  i\right)  }=\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n}$. The definition of
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ yields
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}$ (since $B=\left(  b_{i,j}%
\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$). Thus,%
\[
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{i},j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ and
$b_{k_{i},j}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  },
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.3}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }_{=\det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  }\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  . \label{pf.thm.cauchy-binet.4}%
\end{align}


Next, let us make two observations:

\begin{itemize}
\item Every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}$ satisfying $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$
satisfies%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=0 \label{pf.thm.cauchy-binet.6a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.6a}):} Let $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left[  m\right]  ^{n}$ be such that
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \notin\mathbf{E}$.
\par
Let us first show that there exist two distinct elements $p$ and $q$ of
$\left[  n\right]  $ such that $g_{p}=g_{q}$. Indeed, we assume the contrary.
Thus, there do not exist two distinct elements $p$ and $q$ of $\left[
n\right]  $ such that $g_{p}=g_{q}$. In other words, every two distinct
elements $p$ and $q$ of $\left[  n\right]  $ satisfy $g_{p}\neq g_{q}$. In
other words, the integers $g_{1},g_{2},\ldots,g_{n}$ are distinct. Hence,
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that the integers
$k_{1},k_{2},\ldots,k_{n}$ are distinct (since $\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\left[  m\right]  ^{n}$ and since the integers
$g_{1},g_{2},\ldots,g_{n}$ are distinct). In other words,%
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ \text{the
integers }k_{1},k_{2},\ldots,k_{n}\text{ are distinct}\right\}  =\mathbf{E}.
\]
This contradicts $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \notin\mathbf{E}$.
This contradiction proves that our assumption was wrong. Hence, we have proven
that there exist two distinct elements $p$ and $q$ of $\left[  n\right]  $
such that $g_{p}=g_{q}$. Consider these $p$ and $q$. Now,
(\ref{eq.def.rowscols.a.row=row}) (applied to $m$, $n$, $n$, $g_{r}$, $B$ and
$b_{i,j}$ instead of $n$, $m$, $u$, $i_{r}$, $A$ and $a_{i,j}$) yields%
\begin{align}
&  \left(  \text{the }p\text{-th row of }\operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\left(  \text{the }\underbrace{g_{p}}_{=g_{q}}\text{-th row of }B\right)
=\left(  \text{the }g_{q}\text{-th row of }B\right)  .
\label{pf.thm.cauchy-binet.6a.pf.1}%
\end{align}
On the other hand, (\ref{eq.def.rowscols.a.row=row}) (applied to $m$, $n$,
$n$, $g_{r}$, $B$, $b_{i,j}$ and $q$ instead of $n$, $m$, $u$, $i_{r}$, $A$,
$a_{i,j}$ and $p$) yields%
\begin{equation}
\left(  \text{the }q\text{-th row of }\operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right)  =\left(  \text{the }g_{q}\text{-th row of
}B\right)  .\nonumber
\end{equation}
Compared with (\ref{pf.thm.cauchy-binet.6a.pf.1}), this yields
\[
\left(  \text{the }p\text{-th row of }\operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right)  =\left(  \text{the }q\text{-th row of
}\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  .
\]
Thus, the matrix $\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B$
has two equal rows (namely, the $p$-th row and the $q$-th row), because $p$
and $q$ are distinct. Therefore, Exercise \ref{exe.ps4.6} \textbf{(e)}
(applied to $\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B$
instead of $A$) yields $\det\left(  \operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right)  =0$.
\par
Let us now forget that we fixed $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $.
We thus have shown that every $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left[  m\right]  ^{n}$ satisfying $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \notin\mathbf{E}$ satisfies $\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  =0$. Renaming $\left(
g_{1},g_{2},\ldots,g_{n}\right)  $ as $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  $ in this result, we obtain the following: Every $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ satisfying
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$ satisfies
$\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=0$. This proves (\ref{pf.thm.cauchy-binet.6a}).}.

\item Every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}$ and every $\sigma\in S_{n}$ satisfy%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  \label{pf.thm.cauchy-binet.6b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.6b}):} Let $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ and $\sigma\in
S_{n}$. The definition of $\operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B$ yields $\operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B=\left(  b_{k_{x},j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}$
(since $B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$). Thus,%
\[
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{i},j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). On the other hand, the definition of
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B$ yields
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B=\left(  b_{k_{\sigma\left(
x\right)  },j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}$ (since $B=\left(
b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$). Thus,%
\[
\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B=\left(  b_{k_{\sigma\left(
x\right)  },j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{\sigma
\left(  i\right)  },j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). So we know that $\operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{i},j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ and $\operatorname*{rows}\nolimits_{k_{\sigma\left(
1\right)  },k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }%
}B=\left(  b_{k_{\sigma\left(  i\right)  },j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$. Hence, we can apply Lemma \ref{lem.det.sigma} \textbf{(a)} to
$\sigma$, $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ and
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B$ instead of $\kappa$, $B$
and $B_{\kappa}$. As a consequence, we obtain
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\]
This proves (\ref{pf.thm.cauchy-binet.6b}).}
\end{itemize}

Now, (\ref{pf.thm.cauchy-binet.4}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
\\
&  =\underbrace{\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n};\\\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}}}_{\substack{=\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}\\\text{(since }\mathbf{E}\subseteq\left[  m\right]
^{n}\text{)}}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n};\\\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \notin\mathbf{E}}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\underbrace{\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cauchy-binet.6a}) (since }\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \notin\mathbf{E}\text{))}}}\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}}\left(
\prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m\right]  ^{n};\\\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \notin\mathbf{E}}}\left(  \prod_{i=1}^{n}a_{i,k_{i}%
}\right)  0}_{=0}\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}}\left(
\prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  =\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(
i\right)  }}\right)  \det\left(  \operatorname*{rows}\nolimits_{g_{\sigma
\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(
n\right)  }}B\right)
\end{align*}
(here, we have substituted $\left(  g_{\sigma\left(  1\right)  }%
,g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(  n\right)  }\right)  $
for $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ in the sum, because the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is a bijection). Thus,%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\underbrace{\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)
,\sigma\right)  \in\mathbf{I}\times S_{n}}}_{=\sum_{\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\mathbf{I}}\sum_{\sigma\in S_{n}}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \underbrace{\det\left(
\operatorname*{rows}\nolimits_{g_{\sigma\left(  1\right)  },g_{\sigma\left(
2\right)  },\ldots,g_{\sigma\left(  n\right)  }}B\right)  }%
_{\substack{=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  \\\text{(by
(\ref{pf.thm.cauchy-binet.6b}), applied to }k_{i}=g_{i}\text{)}}}\nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\sum
_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }%
}\right)  \left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\left(
\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  .
\label{pf.thm.cauchy-binet.10}%
\end{align}
But every $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ satisfies
$\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}=\det\left(  \operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\mathbf{I}$. Then,%
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $g_{1}<g_{2}<\cdots<g_{n}$.
\par
Now, the definition of $\operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A$ yields $\operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A=\left(  a_{i,g_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq n}$
(since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$). Hence,%
\[
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A=\left(  a_{i,g_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq n}=\left(  a_{i,g_{j}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $y$ as $j$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A$ and
$a_{i,g_{j}}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}=\sum_{\sigma\in S_{n}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)
^{\sigma},
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.10}) becomes%
\begin{align*}
&  \det\left(  AB\right) \\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}%
}\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)  ^{\sigma
}\right)  }_{=\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A\right)  }\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right) \\
&  =\underbrace{\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}%
}_{\substack{=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}%
}}\\\text{(since }\mathbf{I}=\left\{  \left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n}\ \mid\ g_{1}<g_{2}<\cdots
<g_{n}\right\}  \text{)}}}\det\left(  \operatorname*{cols}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \\
&  =\underbrace{\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}}%
}}_{\substack{=\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\\\text{(since we
defined }\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\\\text{to be an
abbreviation for}\\\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}}}\text{)}%
}}\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}%
}A\right)  \cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}B\right) \\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
.
\end{align*}
This proves Theorem \ref{thm.cauchy-binet}.
\end{proof}
\end{verlong}

\subsection{Prelude to Laplace expansion}

Next we shall show a fact which will allow us to compute some determinants by induction:

\begin{theorem}
\label{thm.laplace.pre}Let $n$ be a positive integer. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Assume that%
\begin{equation}
a_{n,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,\ldots
,n-1\right\}  . \label{eq.thm.laplace.pre.ass}%
\end{equation}
Then, $\det A=a_{n,n}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right)  $.
\end{theorem}

The assumption (\ref{eq.thm.laplace.pre.ass}) says that the last row of the
matrix $A$ consists entirely of zeroes, apart from its last entry $a_{n,n}$
(which may and may not be $0$). Theorem \ref{thm.laplace.pre} states that,
under this assumption, the determinant can be obtained by multiplying this
last entry $a_{n,n}$ with the determinant of the $\left(  n-1\right)
\times\left(  n-1\right)  $-matrix obtained by removing both the last row and
the last column from $A$. For example, for $n=3$, Theorem
\ref{thm.laplace.pre} states that%
\[
\det\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
0 & 0 & g
\end{array}
\right)  =g\det\left(
\begin{array}
[c]{cc}%
a & b\\
d & e
\end{array}
\right)  .
\]


Theorem \ref{thm.laplace.pre} is a particular case of \textit{Laplace
expansion}, which is a general recursive formula for the determinants that we
will encounter further below. But Theorem \ref{thm.laplace.pre} already has
noticeable applications of its own, which is why I have chosen to start with
this particular case.

The proof of Theorem \ref{thm.laplace.pre} essentially relies on the following fact:

\begin{lemma}
\label{lem.laplace.lem}Let $n$ be a positive integer. Let $\left(
a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}$ be an $\left(
n-1\right)  \times\left(  n-1\right)  $-matrix. Then,%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }%
=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.laplace.lem}.]We define a subset $T$ of $S_{n}$ by%
\[
T=\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  .
\]


(In other words, $T$ is the set of all $\tau\in S_{n}$ such that if we write
$\tau$ in one-line notation, then $\tau$ ends with an $n$.)

Now, we shall construct two mutually inverse maps between $S_{n-1}$ and $T$.

\begin{vershort}
For every $\sigma\in S_{n-1}$, we define a map $\widehat{\sigma}:\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $ by setting%
\[
\widehat{\sigma}\left(  i\right)  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n\right\}  .
\]
It is straightforward to see that this map $\widehat{\sigma}$ is well-defined
and belongs to $T$. Thus, we can define a map $\Phi:S_{n-1}\rightarrow T$ by
setting%
\[
\Phi\left(  \sigma\right)  =\widehat{\sigma}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in S_{n-1}.
\]

\end{vershort}

\begin{verlong}
For every $\sigma\in S_{n-1}$, we define a map $\widehat{\sigma}:\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $ by setting%
\[
\left(  \widehat{\sigma}\left(  i\right)  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n\right\}
\right)  .
\]
This map $\widehat{\sigma}$ is well-defined\footnote{\textit{Proof.} Let
$\sigma\in S_{n-1}$. Thus, $\sigma$ is a permutation of the set $\left\{
1,2,\ldots,n-1\right\}  $ (since $S_{n-1}$ is the set of all permutations of
the set $\left\{  1,2,\ldots,n-1\right\}  $). In other words, $\sigma$ is a
bijection $\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots
,n-1\right\}  $.
\par
We have $n\in\left\{  1,2,\ldots,n\right\}  $ (since $n$ is positive). Now,
for every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\in%
\begin{cases}
\left\{  1,2,\ldots,n\right\}  , & \text{if }i<n;\\
\left\{  1,2,\ldots,n\right\}  , & \text{if }i=n
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sigma\left(  i\right)  \in\left\{  1,2,\ldots,n-1\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  \text{ in the case when }i<n\text{,}\\
\text{and since }n\in\left\{  1,2,\ldots,n\right\}  \text{ in the case when
}i=n
\end{array}
\right) \\
&  =\left\{  1,2,\ldots,n\right\}  .
\end{align*}
In other words, the map $\widehat{\sigma}$ is well-defined.} and belongs to
$T$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Thus, $\sigma$
is a permutation of the set $\left\{  1,2,\ldots,n-1\right\}  $ (since
$S_{n-1}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n-1\right\}  $). In other words, $\sigma$ is a bijection $\left\{
1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots,n-1\right\}  $. Hence,
the map $\sigma$ is injective and surjective.
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  n\right)
=%
\begin{cases}
\sigma\left(  n\right)  , & \text{if }n<n;\\
n, & \text{if }n=n
\end{cases}
=n$ (since $n=n$).
\par
Let us now show that $\widehat{\sigma}$ is injective.
\par
\textit{Proof that }$\widehat{\sigma}$ \textit{is injective:} Let $p$ and $q$
be two elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$\widehat{\sigma}\left(  p\right)  =\widehat{\sigma}\left(  q\right)  $. We
shall prove that $p=q$.
\par
We can WLOG assume that $p\geq q$ (since otherwise, we can simply switch $p$
with $q$). Assume this.
\par
Let us first assume that $p=n$. Then, $\widehat{\sigma}\left(  \underbrace{p}%
_{=n}\right)  =\widehat{\sigma}\left(  n\right)  =n$. Therefore, if we had
$q<n$, then we would have%
\begin{align*}
n  &  =\widehat{\sigma}\left(  p\right)  =\widehat{\sigma}\left(  q\right)  =%
\begin{cases}
\sigma\left(  q\right)  , & \text{if }q<n;\\
q, & \text{if }q=n
\end{cases}
=\sigma\left(  q\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }q<n\right)
\\
&  <n\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\sigma\left(  q\right)
\in\left\{  1,2,\ldots,n-1\right\}  \right)  ,
\end{align*}
which is absurd. Hence, we cannot have $q<n$. Thus, we must have $q\geq n$.
Therefore, $q=n$ (since $q\in\left\{  1,2,\ldots,n\right\}  $), so that
$p=n=q$.
\par
Now, let us forget that we have assumed that $p=n$. We thus have shown that
$p=q$ in the case when $p=n$. Hence, for the rest of this proof of $p=q$, we
can WLOG assume that we don't have $p=n$. Assume this.
\par
We have $p\neq n$ (since we don't have $p=n$). Since $p\in\left\{
1,2,\ldots,n\right\}  $ and $p\neq n$, we have $p\in\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  $,
so that $p<n$. The definition of $\widehat{\sigma}$ yields $\widehat{\sigma
}\left(  p\right)  =%
\begin{cases}
\sigma\left(  p\right)  , & \text{if }p<n;\\
p, & \text{if }p=n
\end{cases}
=\sigma\left(  p\right)  $ (since $p<n$).
\par
Now, $p\geq q$, so that $q\leq p<n$. The definition of $\widehat{\sigma}$
yields $\widehat{\sigma}\left(  q\right)  =%
\begin{cases}
\sigma\left(  q\right)  , & \text{if }q<n;\\
q, & \text{if }q=n
\end{cases}
=\sigma\left(  q\right)  $ (since $q<n$). Also, since $q<n$, we have
$q\in\left\{  1,2,\ldots,n-1\right\}  $.
\par
Now, $\sigma\left(  p\right)  =\widehat{\sigma}\left(  p\right)
=\widehat{\sigma}\left(  q\right)  =\sigma\left(  q\right)  $. Hence, $p=q$
(since the map $\sigma$ is injective). This completes our proof of $p=q$.
\par
Now, let us forget that we fixed $p$ and $q$. We thus have shown that if $p$
and $q$ are two elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$\widehat{\sigma}\left(  p\right)  =\widehat{\sigma}\left(  q\right)  $, then
$p=q$. In other words, the map $\widehat{\sigma}$ is injective.
\par
\textit{Proof that }$\widehat{\sigma}$ \textit{is surjective:} Let
$g\in\left\{  1,2,\ldots,n\right\}  $. We shall show that $g\in\widehat{\sigma
}\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
Indeed, if $g=n$, then $g=n=\widehat{\sigma}\left(  \underbrace{n}%
_{\in\left\{  1,2,\ldots,n\right\}  }\right)  \in\widehat{\sigma}\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. Hence, for the rest of this proof
of $g\in\widehat{\sigma}\left(  \left\{  1,2,\ldots,n\right\}  \right)  $, we
can WLOG assume that we don't have $g=n$. Assume this.
\par
We have $g\neq n$ (since we don't have $g=n$). Since $g\in\left\{
1,2,\ldots,n\right\}  $ and $g\neq n$, we have $g\in\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}
=\sigma\left(  \left\{  1,2,\ldots,n-1\right\}  \right)  $ (since the map
$\sigma$ is surjective). In other words, there exists an $h\in\left\{
1,2,\ldots,n-1\right\}  $ such that $g=\sigma\left(  h\right)  $. Consider
this $h$. We have $h\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $ and $h<n$ (since $h\in\left\{  1,2,\ldots,n-1\right\}
$). The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(
h\right)  =%
\begin{cases}
\sigma\left(  h\right)  , & \text{if }h<n;\\
h, & \text{if }h=n
\end{cases}
=\sigma\left(  h\right)  $ (since $h<n$). Compared with $g=\sigma\left(
h\right)  $, this yields $g=\widehat{\sigma}\left(  \underbrace{h}%
_{\in\left\{  1,2,\ldots,n\right\}  }\right)  \in\widehat{\sigma}\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. Thus, $g\in\widehat{\sigma}\left(
\left\{  1,2,\ldots,n\right\}  \right)  $ is proven.
\par
Now, let us forget that we fixed $g$. We thus have shown that $g\in
\widehat{\sigma}\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ for every
$g\in\left\{  1,2,\ldots,n\right\}  $. In other words, $\left\{
1,2,\ldots,n\right\}  \subseteq\widehat{\sigma}\left(  \left\{  1,2,\ldots
,n\right\}  \right)  $. In other words, the map $\widehat{\sigma}$ is
surjective.
\par
We now know that the map $\widehat{\sigma}$ is both injective and surjective.
In other words, $\widehat{\sigma}$ is bijective. Hence, $\widehat{\sigma}$ is
a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. In other words, $\widehat{\sigma}$ is a permutation
of $\left\{  1,2,\ldots,n\right\}  $. In other words, $\widehat{\sigma}\in
S_{n}$ (since $S_{n}$ is the set of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $).
\par
Thus, $\widehat{\sigma}$ is an element of $S_{n}$ and satisfies
$\widehat{\sigma}\left(  n\right)  =n$. In other words, $\widehat{\sigma}$ is
an element $\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other
words,%
\[
\widehat{\sigma}\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  =T,
\]
qed.}. Thus, we can define a map $\Phi:S_{n-1}\rightarrow T$ by setting%
\[
\left(  \Phi\left(  \sigma\right)  =\widehat{\sigma}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n-1}\right)  .
\]

\end{verlong}

Loosely speaking, for every $\sigma\in S_{n-1}$, the permutation $\Phi\left(
\sigma\right)  =\widehat{\sigma}\in T$ is obtained by writing $\sigma$ in
one-line notation and appending $n$ on its right. For example, if $n=4$ and if
$\sigma\in S_{3}$ is the permutation that is written as $\left(  2,3,1\right)
$ in one-line notation, then $\Phi\left(  \sigma\right)  =\widehat{\sigma}$ is
the permutation that is written as $\left(  2,3,1,4\right)  $ in one-line notation.

\begin{vershort}
On the other hand, for every $\gamma\in T$, we define a map $\overline{\gamma
}:\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots,n-1\right\}
$ by setting
\[
\overline{\gamma}\left(  i\right)  =\gamma\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-1\right\}  .
\]
It is straightforward to see that this map $\overline{\gamma}$ is well-defined
and belongs to $S_{n-1}$. Hence, we can define a map $\Psi:T\rightarrow
S_{n-1}$ by setting%
\[
\Psi\left(  \gamma\right)  =\overline{\gamma}\ \ \ \ \ \ \ \ \ \ \text{for
every }\gamma\in T.
\]

\end{vershort}

\begin{verlong}
On the other hand, for every $\gamma\in T$, we define a map $\overline{\gamma
}:\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots,n-1\right\}
$ by setting
\[
\left(  \overline{\gamma}\left(  i\right)  =\gamma\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-1\right\}
\right)  .
\]
This map $\overline{\gamma}$ is well-defined\footnote{\textit{Proof.} Let
$\gamma\in T$. Thus, $\gamma\in T=\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $. In other words, $\gamma$ is an element $\tau$ of
$S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words, $\gamma$ is an
element of $S_{n}$ and satisfies $\gamma\left(  n\right)  =n$.
\par
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of the
set $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$\gamma$ is a bijection $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Hence, the map $\gamma$ is surjective and injective.
\par
Let $i\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $i<n$ and thus $i\neq n$.
Also, $i\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $. Thus, $\gamma\left(  i\right)  $ is well-defined and belongs to
$\left\{  1,2,\ldots,n\right\}  $. Also, $i\neq n$. Hence, $\gamma\left(
i\right)  \neq\gamma\left(  n\right)  $ (since the map $\gamma$ is injective),
so that $\gamma\left(  i\right)  \neq\gamma\left(  n\right)  =n$. Combined
with $\gamma\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  $, this shows
that $\gamma\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  \setminus
\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  $.
\par
Now, let us forget that we fixed $i$. We thus have shown that $\gamma\left(
i\right)  \in\left\{  1,2,\ldots,n-1\right\}  $ for every $i\in\left\{
1,2,\ldots,n-1\right\}  $. In other words, the map $\overline{\gamma}$ is
well-defined. Qed.} and belongs to $S_{n-1}$\ \ \ \ \footnote{\textit{Proof.}
Let $\gamma\in T$. Thus, $\gamma\in T=\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  n\right)  =n\right\}  $. In other words, $\gamma$ is an element
$\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words,
$\gamma$ is an element of $S_{n}$ and satisfies $\gamma\left(  n\right)  =n$.
\par
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of the
set $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$\gamma$ is a bijection $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Hence, the map $\gamma$ is surjective and injective.
\par
Let us now show that $\overline{\gamma}$ is injective.
\par
\textit{Proof that }$\overline{\gamma}$ \textit{is injective:} Let $p$ and $q$
be two elements of $\left\{  1,2,\ldots,n-1\right\}  $ such that
$\overline{\gamma}\left(  p\right)  =\overline{\gamma}\left(  q\right)  $. We
shall prove that $p=q$.
\par
We have $\overline{\gamma}\left(  p\right)  =\gamma\left(  p\right)  $ (by the
definition of $\overline{\gamma}$) and $\overline{\gamma}\left(  q\right)
=\gamma\left(  q\right)  $ (by the definition of $\overline{\gamma}$). Thus,
$\gamma\left(  p\right)  =\overline{\gamma}\left(  p\right)  =\overline
{\gamma}\left(  q\right)  =\gamma\left(  q\right)  $. Therefore, $p=q$ (since
the map $\gamma$ is injective).
\par
Now, let us forget that we fixed $p$ and $q$. We thus have shown that if $p$
and $q$ are two elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$\overline{\gamma}\left(  p\right)  =\overline{\gamma}\left(  q\right)  $,
then $p=q$. In other words, the map $\overline{\gamma}$ is injective.
\par
\textit{Proof that }$\overline{\gamma}$ \textit{is surjective:} Let
$g\in\left\{  1,2,\ldots,n-1\right\}  $. We shall show that $g\in
\overline{\gamma}\left(  \left\{  1,2,\ldots,n-1\right\}  \right)  $.
\par
Indeed, $g\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  =\gamma\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ (since
the map $\gamma$ is surjective). Hence, there exists an $h\in\left\{
1,2,\ldots,n\right\}  $ such that $g=\gamma\left(  h\right)  $. Consider this
$h$.
\par
We have $g\in\left\{  1,2,\ldots,n-1\right\}  $, thus $g<n$, thus $g\neq n$.
\par
If we had $h=n$, then we would have $g=\gamma\left(  \underbrace{h}%
_{=n}\right)  =\gamma\left(  n\right)  =n$, which would contradict $g\neq n$.
Hence, we cannot have $h=n$. We thus have $h\neq n$. Combined with
$h\in\left\{  1,2,\ldots,n\right\}  $, this shows that $h\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots
,n-1\right\}  $. Thus, $\overline{\gamma}\left(  h\right)  $ is well-defined.
The definition of $\overline{\gamma}$ shows that $\overline{\gamma}\left(
h\right)  =\gamma\left(  h\right)  =g$.
\par
Hence, $g=\overline{\gamma}\left(  \underbrace{h}_{\in\left\{  1,2,\ldots
,n-1\right\}  }\right)  \in\overline{\gamma}\left(  \left\{  1,2,\ldots
,n-1\right\}  \right)  $.
\par
Now, let us forget that we fixed $g$. We thus have shown that $g\in
\overline{\gamma}\left(  \left\{  1,2,\ldots,n-1\right\}  \right)  $ for every
$g\in\left\{  1,2,\ldots,n-1\right\}  $. In other words, $\left\{
1,2,\ldots,n-1\right\}  \subseteq\overline{\gamma}\left(  \left\{
1,2,\ldots,n-1\right\}  \right)  $. In other words, the map $\overline{\gamma
}$ is surjective.
\par
We now know that the map $\overline{\gamma}$ is both injective and surjective.
In other words, $\overline{\gamma}$ is bijective. Hence, $\overline{\gamma}$
is a bijective map $\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{
1,2,\ldots,n-1\right\}  $. In other words, $\overline{\gamma}$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $. In other words,
$\overline{\gamma}\in S_{n-1}$ (since $S_{n-1}$ is the set of all permutations
of the set $\left\{  1,2,\ldots,n-1\right\}  $), qed.}. Hence, we can define a
map $\Psi:T\rightarrow S_{n-1}$ by setting%
\[
\left(  \Psi\left(  \gamma\right)  =\overline{\gamma}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\gamma\in T\right)  .
\]

\end{verlong}

Loosely speaking, for every $\gamma\in T$, the permutation $\Psi\left(
\gamma\right)  =\overline{\gamma}\in S_{n-1}$ is obtained by writing $\gamma$
in one-line notation and removing the $n$ (which is the rightmost entry in the
one-line notation, because $\gamma\left(  n\right)  =n$). For example, if
$n=4$ and if $\gamma\in S_{4}$ is the permutation that is written as $\left(
2,3,1,4\right)  $ in one-line notation, then $\Psi\left(  \gamma\right)
=\overline{\gamma}$ is the permutation that is written as $\left(
2,3,1\right)  $ in one-line notation.

\begin{vershort}
The maps $\Phi$ and $\Psi$ are mutually inverse.\footnote{This should be clear
enough from the descriptions we gave using one-line notation. A formal proof
is straightforward.} Thus, the map $\Phi$ is a bijection.
\end{vershort}

\begin{verlong}
We have $\Phi\circ\Psi=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $\gamma\in T$. Then, $\gamma\in T=\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  n\right)  =n\right\}  $. In other words, $\gamma$ is an element
$\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words,
$\gamma$ is an element of $S_{n}$ and satisfies $\gamma\left(  n\right)  =n$.
\par
Let $\sigma=\overline{\gamma}$. Then, the definition of $\Phi$ yields
$\Phi\left(  \sigma\right)  =\widehat{\sigma}$. Also, the definition of $\Psi$
yields $\Psi\left(  \gamma\right)  =\overline{\gamma}=\sigma$. Now, $\left(
\Phi\circ\Psi\right)  \left(  \gamma\right)  =\Phi\left(  \underbrace{\Psi
\left(  \gamma\right)  }_{=\sigma}\right)  =\Phi\left(  \sigma\right)
=\widehat{\sigma}$.
\par
Both $\widehat{\sigma}$ and $\gamma$ are elements of $T$, therefore elements
of $S_{n}$ (since $T\subseteq S_{n}$), therefore permutations of the set
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $), therefore
bijective maps $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $.
\par
Let $i\in\left\{  1,2,\ldots,n\right\}  $. We shall show that $\widehat{\sigma
}\left(  i\right)  =\gamma\left(  i\right)  $.
\par
If $i=n$, then%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =n\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i=n\right) \\
&  =\gamma\left(  \underbrace{n}_{=i}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\gamma\left(  n\right)  =n\right) \\
&  =\gamma\left(  i\right)  .
\end{align*}
Hence, $\widehat{\sigma}\left(  i\right)  =\gamma\left(  i\right)  $ is proven
in the case when $i=n$. Therefore, for the rest of the proof of
$\widehat{\sigma}\left(  i\right)  =\gamma\left(  i\right)  $, we can WLOG
assume that we don't have $i=n$. Assume this.
\par
We have $i\neq n$ (since we don't have $i=n$). Combined with $i\in\left\{
1,2,\ldots,n\right\}  $, this shows that $i\in\left\{  1,2,\ldots,n\right\}
\setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  $. Hence,
$i<n$. Now,%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =\underbrace{\sigma}_{=\overline{\gamma}}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<n\right) \\
&  =\overline{\gamma}\left(  i\right)  =\gamma\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overline{\gamma
}\right)  .
\end{align*}
Thus, $\widehat{\sigma}\left(  i\right)  =\gamma\left(  i\right)  $ is proven.
\par
Let us now forget that we fixed $i$. We thus have proven that $\widehat{\sigma
}\left(  i\right)  =\gamma\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,n\right\}  $. In other words, $\widehat{\sigma}=\gamma$ (since both
$\widehat{\sigma}$ and $\gamma$ are maps $\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}  $). Hence, $\left(  \Phi\circ
\Psi\right)  \left(  \gamma\right)  =\widehat{\sigma}=\gamma
=\operatorname*{id}\left(  \gamma\right)  $.
\par
Let us now forget that we fixed $\gamma$. We have thus proven that $\left(
\Phi\circ\Psi\right)  \left(  \gamma\right)  =\operatorname*{id}\left(
\gamma\right)  $ for every $\gamma\in T$. In other words, $\Phi\circ
\Psi=\operatorname*{id}$, qed.} and $\Psi\circ\Phi=\operatorname*{id}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Let $\varepsilon
=\widehat{\sigma}$. Then, the definition of $\Psi$ yields $\Phi\left(
\sigma\right)  =\widehat{\sigma}=\varepsilon$. Also, the definition of $\Psi$
yields $\Psi\left(  \varepsilon\right)  =\overline{\varepsilon}$. Now,
$\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)  =\Psi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\varepsilon}\right)  =\Psi\left(
\varepsilon\right)  =\overline{\varepsilon}$.
\par
Both $\overline{\varepsilon}$ and $\sigma$ are elements of $S_{n-1}$,
therefore permutations of the set $\left\{  1,2,\ldots,n-1\right\}  $ (since
$S_{n-1}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n-1\right\}  $), therefore bijective maps $\left\{  1,2,\ldots,n-1\right\}
\rightarrow\left\{  1,2,\ldots,n-1\right\}  $.
\par
Let $i\in\left\{  1,2,\ldots,n-1\right\}  $. Thus, $i\leq n-1<n$. Now,%
\begin{align*}
\overline{\varepsilon}\left(  i\right)   &  =\underbrace{\varepsilon
}_{=\widehat{\sigma}}\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\overline{\varepsilon}\right) \\
&  =\widehat{\sigma}\left(  i\right)  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =\sigma\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i<n\right)  .
\end{align*}
\par
Let us now forget that we fixed $i$. We thus have proven that $\overline
{\varepsilon}\left(  i\right)  =\sigma\left(  i\right)  $ for every
$i\in\left\{  1,2,\ldots,n-1\right\}  $. In other words, $\overline
{\varepsilon}=\sigma$ (since both $\overline{\varepsilon}$ and $\sigma$ are
maps $\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots
,n-1\right\}  $). Hence, $\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)
=\overline{\varepsilon}=\sigma=\operatorname*{id}\left(  \sigma\right)  $.
\par
Let us now forget that we fixed $\sigma$. We have thus proven that $\left(
\Psi\circ\Phi\right)  \left(  \sigma\right)  =\operatorname*{id}\left(
\sigma\right)  $ for every $\sigma\in S_{n-1}$. In other words, $\Psi\circ
\Phi=\operatorname*{id}$, qed.}. These two equalities show that the maps
$\Phi$ and $\Psi$ are mutually inverse. Hence, the map $\Phi$ is a bijection.
\end{verlong}

\begin{vershort}
It is fairly easy to see that every $\sigma\in S_{n-1}$ satisfies%
\begin{equation}
\left(  -1\right)  ^{\widehat{\sigma}}=\left(  -1\right)  ^{\sigma}
\label{pf.lem.laplace.lem.short.-1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.lem.short.-1}):} Let
$\sigma\in S_{n-1}$. We want to prove that $\left(  -1\right)
^{\widehat{\sigma}}=\left(  -1\right)  ^{\sigma}$. It is clearly sufficient to
show that $\ell\left(  \widehat{\sigma}\right)  =\ell\left(  \sigma\right)  $
(because $\left(  -1\right)  ^{\widehat{\sigma}}=\left(  -1\right)
^{\ell\left(  \widehat{\sigma}\right)  }$ and $\left(  -1\right)  ^{\sigma
}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$). In order to do so, it
is sufficient to show that the inversions of $\widehat{\sigma}$ are precisely
the inversions of $\sigma$ (because $\ell\left(  \widehat{\sigma}\right)  $ is
the number of inversions of $\widehat{\sigma}$, whereas $\ell\left(
\sigma\right)  $ is the number of inversions of $\sigma$).
\par
If $\left(  i,j\right)  $ is an inversion of $\sigma$, then $\left(
i,j\right)  $ is an inversion of $\widehat{\sigma}$ (because if $\left(
i,j\right)  $ is an inversion of $\sigma$, then both $i$ and $j$ are $<n$, and
thus the definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(
i\right)  =\sigma\left(  i\right)  $ and $\widehat{\sigma}\left(  j\right)
=\sigma\left(  j\right)  $). In other words, every inversion of $\sigma$ is an
inversion of $\widehat{\sigma}$.
\par
On the other hand, let $\left(  u,v\right)  $ be an inversion of
$\widehat{\sigma}$. We shall prove that $\left(  u,v\right)  $ is an inversion
of $\sigma$.
\par
Indeed, $\left(  u,v\right)  $ is an inversion of $\widehat{\sigma}$. In other
words, $\left(  u,v\right)  $ is a pair of integers satisfying $1\leq u<v\leq
n$ and $\widehat{\sigma}\left(  u\right)  >\widehat{\sigma}\left(  v\right)
$.
\par
If we had $v=n$, then we would have $\widehat{\sigma}\left(  u\right)
>\widehat{\sigma}\left(  \underbrace{v}_{=n}\right)  =\widehat{\sigma}\left(
n\right)  =n$ (by the definition of $\widehat{\sigma}$), which would
contradict $\widehat{\sigma}\left(  u\right)  \in\left\{  1,2,\ldots
,n\right\}  $. Thus, we cannot have $v=n$. We therefore have $v<n$, so that
$v\leq n-1$. Now, $1\leq u<v\leq n-1$. Thus, both $\sigma\left(  u\right)  $
and $\sigma\left(  v\right)  $ are well-defined. The definition of
$\widehat{\sigma}$ yields $\widehat{\sigma}\left(  u\right)  =\sigma\left(
u\right)  $ (since $u\leq n-1<n$) and $\widehat{\sigma}\left(  v\right)
=\sigma\left(  v\right)  $ (since $v\leq n-1<n$), so that $\sigma\left(
u\right)  =\widehat{\sigma}\left(  u\right)  >\widehat{\sigma}\left(
v\right)  =\sigma\left(  v\right)  $. Thus, $\left(  u,v\right)  $ is a pair
of integers satisfying $1\leq u<v\leq n-1$ and $\sigma\left(  u\right)
>\sigma\left(  v\right)  $. In other words, $\left(  u,v\right)  $ is an
inversion of $\sigma$.
\par
We thus have shown that every inversion of $\widehat{\sigma}$ is an inversion
of $\sigma$. Combining this with the fact that every inversion of $\sigma$ is
an inversion of $\widehat{\sigma}$, we thus conclude that the inversions of
$\widehat{\sigma}$ are precisely the inversions of $\sigma$. As we have
already said, this finishes the proof of (\ref{pf.lem.laplace.lem.short.-1}).}
and%
\begin{equation}
\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(  i\right)  }=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  } \label{pf.lem.laplace.lem.short.prod}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.lem.short.prod}):} Let
$\sigma\in S_{n-1}$. The definition of $\widehat{\sigma}$ yields
$\widehat{\sigma}\left(  i\right)  =\sigma\left(  i\right)  $ for every
$i\in\left\{  1,2,\ldots,n-1\right\}  $. Thus, $a_{i,\widehat{\sigma}\left(
i\right)  }=a_{i,\sigma\left(  i\right)  }$ for every $i\in\left\{
1,2,\ldots,n-1\right\}  $. Hence, $\prod_{i=1}^{n-1}%
\underbrace{a_{i,\widehat{\sigma}\left(  i\right)  }}_{=a_{i,\sigma\left(
i\right)  }}=\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }$, qed.}.
\end{vershort}

\begin{verlong}
For every $m\in\mathbb{N}$ and every $\sigma\in S_{m}$, let
$\operatorname*{Inv}\left(  \sigma\right)  $ be the set of all inversions of
the permutation $\sigma$. Thus, for every $m\in\mathbb{N}$ and $\sigma\in
S_{m}$, we have%
\begin{align}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \nonumber\\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\label{pf.lem.laplace.lem.l}%
\end{align}

\end{verlong}

\begin{verlong}
Moreover, every $\sigma\in S_{n-1}$ satisfies
\[
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  \subseteq
\operatorname*{Inv}\left(  \sigma\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Let $c\in
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $. Thus, $c$ is an
inversion of $\widehat{\sigma}$ (since $\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $ is the set of all inversions of $\widehat{\sigma}%
$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\widehat{\sigma}\left(  i\right)
>\widehat{\sigma}\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion\textquotedblright).
\par
We have $\widehat{\sigma}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}
$, thus $\widehat{\sigma}\left(  i\right)  \leq n$. From $\widehat{\sigma
}\left(  i\right)  >\widehat{\sigma}\left(  j\right)  $, we obtain
$\widehat{\sigma}\left(  j\right)  <\widehat{\sigma}\left(  i\right)  \leq n$
and thus $\widehat{\sigma}\left(  j\right)  \leq n-1$ (since $\widehat{\sigma
}\left(  j\right)  $ and $n$ are integers).
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  n\right)
=%
\begin{cases}
\sigma\left(  n\right)  , & \text{if }n<n;\\
n, & \text{if }n=n
\end{cases}
=n$ (since $n=n$). Hence, $\widehat{\sigma}\left(  n\right)  =n\neq
\widehat{\sigma}\left(  j\right)  $ (since $\widehat{\sigma}\left(  j\right)
<\widehat{\sigma}\left(  n\right)  $), so that $n\neq j$. Hence, $j\neq n$, so
that $j<n$ (since $j\leq n$). Hence, $1\leq i<j\leq n-1$.
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  i\right)
=%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
=\sigma\left(  i\right)  $ (since $i<n$). The definition of $\widehat{\sigma}$
yields $\widehat{\sigma}\left(  j\right)  =%
\begin{cases}
\sigma\left(  j\right)  , & \text{if }j<n;\\
n, & \text{if }j=n
\end{cases}
=\sigma\left(  j\right)  $ (since $j<n$). Now, $\sigma\left(  i\right)
=\widehat{\sigma}\left(  i\right)  >\widehat{\sigma}\left(  j\right)
=\sigma\left(  j\right)  $.
\par
Now, we know that $\left(  i,j\right)  $ is a pair of integers satisfying
$1\leq i<j\leq n-1$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $.
In other words, $\left(  i,j\right)  $ is an inversion of $\sigma$ (by the
definition of an \textquotedblleft inversion\textquotedblright). In other
words, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $
(since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of all
inversions of $\sigma$). Thus, $c=\left(  i,j\right)  \in\operatorname*{Inv}%
\left(  \sigma\right)  $.
\par
Let us now forget that we fixed $c$. We thus have proven that $c\in
\operatorname*{Inv}\left(  \sigma\right)  $ for every $c\in\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  $. In other words, $\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  \subseteq\operatorname*{Inv}\left(
\sigma\right)  $, qed.} and%
\[
\operatorname*{Inv}\left(  \sigma\right)  \subseteq\operatorname*{Inv}\left(
\widehat{\sigma}\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Let $c\in
\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $c$ is an inversion of
$\sigma$ (since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of all
inversions of $\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $
of integers satisfying $1\leq i<j\leq n-1$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $ (by the definition of an \textquotedblleft
inversion\textquotedblright).
\par
We have $j\leq n-1<n$ and thus $i<j<n$. Since $1\leq i\leq n-1\leq n$, we have
$i\in\left\{  1,2,\ldots,n\right\}  $. Since $1\leq j\leq n-1\leq n$, we have
$j\in\left\{  1,2,\ldots,n\right\}  $.
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  i\right)
=%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
=\sigma\left(  i\right)  $ (since $i<n$). The definition of $\widehat{\sigma}$
yields $\widehat{\sigma}\left(  j\right)  =%
\begin{cases}
\sigma\left(  j\right)  , & \text{if }j<n;\\
n, & \text{if }j=n
\end{cases}
=\sigma\left(  j\right)  $ (since $j<n$). Now, $\widehat{\sigma}\left(
i\right)  =\sigma\left(  i\right)  >\sigma\left(  j\right)  =\widehat{\sigma
}\left(  j\right)  $.
\par
Now, we know that $\left(  i,j\right)  $ is a pair of integers satisfying
$1\leq i<j\leq n$ and $\widehat{\sigma}\left(  i\right)  >\widehat{\sigma
}\left(  j\right)  $. In other words, $\left(  i,j\right)  $ is an inversion
of $\widehat{\sigma}$ (by the definition of an \textquotedblleft
inversion\textquotedblright). In other words, $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ (since
$\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ is the set of all
inversions of $\widehat{\sigma}$). Thus, $c=\left(  i,j\right)  \in
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $.
\par
Let us now forget that we fixed $c$. We thus have proven that $c\in
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ for every $c\in
\operatorname*{Inv}\left(  \sigma\right)  $. In other words,
$\operatorname*{Inv}\left(  \sigma\right)  \subseteq\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $, qed.} and%
\[
\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
\widehat{\sigma}\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. We have proven the two
relations $\operatorname*{Inv}\left(  \sigma\right)  \subseteq
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ and $\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  \subseteq\operatorname*{Inv}\left(
\sigma\right)  $. Combining these two relations, we obtain
$\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $. Qed.} and%
\[
\ell\left(  \sigma\right)  =\ell\left(  \widehat{\sigma}\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. We have proven that
$\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $. Now, (\ref{pf.lem.laplace.lem.l}) (applied to
$m=n-1$) yields $\ell\left(  \sigma\right)  =\left\vert
\underbrace{\operatorname*{Inv}\left(  \sigma\right)  }_{=\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  }\right\vert =\left\vert \operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  \right\vert $. On the other hand,
(\ref{pf.lem.laplace.lem.l}) (applied to $n-1$ and $\widehat{\sigma}$ instead
of $m$ and $\sigma$) yields $\ell\left(  \widehat{\sigma}\right)  =\left\vert
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  \right\vert $. Comparing
this with $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}\left(
\widehat{\sigma}\right)  \right\vert $, we obtain $\ell\left(  \sigma\right)
=\ell\left(  \widehat{\sigma}\right)  $, qed.} and%
\begin{equation}
\left(  -1\right)  ^{\widehat{\sigma}}=\left(  -1\right)  ^{\sigma}
\label{pf.lem.laplace.lem.-1}%
\end{equation}
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. We have proven that
$\ell\left(  \sigma\right)  =\ell\left(  \widehat{\sigma}\right)  $. But the
definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(  -1\right)
^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$. Also, the
definition of $\left(  -1\right)  ^{\widehat{\sigma}}$ yields $\left(
-1\right)  ^{\widehat{\sigma}}=\left(  -1\right)  ^{\ell\left(
\widehat{\sigma}\right)  }$. Thus,%
\begin{align*}
\left(  -1\right)  ^{\sigma}  &  =\left(  -1\right)  ^{\ell\left(
\sigma\right)  }=\left(  -1\right)  ^{\ell\left(  \widehat{\sigma}\right)
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\right)
=\ell\left(  \widehat{\sigma}\right)  \right) \\
&  =\left(  -1\right)  ^{\widehat{\sigma}}.
\end{align*}
This proves (\ref{pf.lem.laplace.lem.-1}).} and%
\begin{equation}
\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(  i\right)  }=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  } \label{pf.lem.laplace.lem.prod}%
\end{equation}
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. For every $i\in\left\{
1,2,\ldots,n-1\right\}  $, the element $\widehat{\sigma}\left(  i\right)  $ is
well-defined (since $i\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $) and satisfies%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =\sigma\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<n\text{
(since }i\leq n-1\text{ (since }i\in\left\{  1,2,\ldots,n-1\right\}
\text{))}\right)  .
\end{align*}
Hence, for every $i\in\left\{  1,2,\ldots,n-1\right\}  $, we have
$a_{i,\widehat{\sigma}\left(  i\right)  }=a_{i,\sigma\left(  i\right)  }$
(since $\widehat{\sigma}\left(  i\right)  =\sigma\left(  i\right)  $). Thus,
$\prod_{i=1}^{n-1}\underbrace{a_{i,\widehat{\sigma}\left(  i\right)  }%
}_{=a_{i,\sigma\left(  i\right)  }}=\prod_{i=1}^{n-1}a_{i,\sigma\left(
i\right)  }$. This proves (\ref{pf.lem.laplace.lem.prod}).}.
\end{verlong}

\begin{vershort}
Now,
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}%
}}_{\substack{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }=\sum_{\sigma\in T}\\\text{(since }\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  =T\text{)}}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in T}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{i,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n-1}}\underbrace{\left(
-1\right)  ^{\Phi\left(  \sigma\right)  }\prod_{i=1}^{n-1}a_{i,\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\left(
-1\right)  ^{\widehat{\sigma}}\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(
i\right)  }\\\text{(since }\Phi\left(  \sigma\right)  =\widehat{\sigma
}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\Phi\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }\Phi:S_{n-1}\rightarrow T\text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{\sigma\in S_{n-1}}\underbrace{\left(  -1\right)  ^{\widehat{\sigma}%
}}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{pf.lem.laplace.lem.short.-1}))}}}\underbrace{\prod_{i=1}^{n-1}%
a_{i,\widehat{\sigma}\left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.laplace.lem.short.prod}))}}}=\sum_{\sigma\in S_{n-1}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }.
\end{align*}
Compared with%
\begin{align*}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)   &  =\sum_{\sigma\in S_{n-1}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.det.eq.2}), applied to }n-1\text{ and}\\
\left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\text{ instead of
}n\text{ and }A
\end{array}
\right)  ,
\end{align*}
this yields%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }%
=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  .
\]
This proves Lemma \ref{lem.laplace.lem}.
\end{vershort}

\begin{verlong}
Now,%
\begin{align}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}%
}}_{\substack{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }=\sum_{\sigma\in T}\\\text{(since }\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  =T\text{)}}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\nonumber\\
&  =\sum_{\sigma\in T}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{i,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n-1}}\underbrace{\left(
-1\right)  ^{\Phi\left(  \sigma\right)  }\prod_{i=1}^{n-1}a_{i,\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\left(
-1\right)  ^{\widehat{\sigma}}\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(
i\right)  }\\\text{(since }\Phi\left(  \sigma\right)  =\widehat{\sigma
}\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\Phi\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }\Phi:S_{n-1}\rightarrow T\text{ is a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{\sigma\in S_{n-1}}\underbrace{\left(  -1\right)  ^{\widehat{\sigma}%
}}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{pf.lem.laplace.lem.-1}))}}}\underbrace{\prod_{i=1}^{n-1}%
a_{i,\widehat{\sigma}\left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }\\\text{(by (\ref{pf.lem.laplace.lem.prod}%
))}}}\nonumber\\
&  =\sum_{\sigma\in S_{n-1}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }. \label{pf.lem.laplace.lem.almostthere}%
\end{align}
But (\ref{eq.det.eq.2}) (applied to $n-1$ and $\left(  a_{i,j}\right)  _{1\leq
i\leq n-1,\ 1\leq j\leq n-1}$ instead of $n$ and $A$) yields%
\[
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  =\sum_{\sigma\in S_{n-1}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }.
\]
Compared with (\ref{pf.lem.laplace.lem.almostthere}), this yields%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }%
=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  .
\]
This proves Lemma \ref{lem.laplace.lem}.
\end{verlong}
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm.laplace.pre}.]Every permutation $\sigma\in S_{n}$
satisfying $\sigma\left(  n\right)  \neq n$ satisfies
\begin{equation}
a_{n,\sigma\left(  n\right)  }=0 \label{pf.thm.laplace.pre.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.laplace.pre.1}):} Let $\sigma\in
S_{n}$ be a permutation satisfying $\sigma\left(  n\right)  \neq n$. Since
$\sigma\left(  n\right)  \in\left\{  1,2,\ldots,n\right\}  $ and
$\sigma\left(  n\right)  \neq n$, we have $\sigma\left(  n\right)  \in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots
,n-1\right\}  $. Hence, (\ref{eq.thm.laplace.pre.ass}) (applied to
$j=\sigma\left(  n\right)  $) shows that $a_{n,\sigma\left(  n\right)  }=0$,
qed.}.

From (\ref{eq.det.eq.2}), we obtain%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{=\left(
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\right)  a_{n,\sigma\left(
n\right)  }}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\right)  a_{n,\sigma\left(
n\right)  }\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)
}\right)  \underbrace{a_{n,\sigma\left(  n\right)  }}_{\substack{=a_{n,n}%
\\\text{(since }\sigma\left(  n\right)  =n\text{)}}}+\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  \neq n}}\left(  -1\right)  ^{\sigma}\left(
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\right)
\underbrace{a_{n,\sigma\left(  n\right)  }}_{\substack{=0\\\text{(by
(\ref{pf.thm.laplace.pre.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every }\sigma\in S_{n}\text{
satisfies either }\sigma\left(  n\right)  =n\text{ or }\sigma\left(  n\right)
\neq n\text{ (but not both)}\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)
}\right)  a_{n,n}+\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
n\right)  \neq n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }\right)  0}_{=0}\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)
}\right)  a_{n,n}=a_{n,n}\cdot\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  =n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)  \\\text{(by Lemma
\ref{lem.laplace.lem})}}}\\
&  =a_{n,n}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq
j\leq n-1}\right)  .
\end{align*}
This proves Theorem \ref{thm.laplace.pre}.
\end{proof}

\subsection{The Vandermonde determinant}

An example for an application of Theorem \ref{thm.laplace.pre} is the famous
\textit{Vandermonde determinant}:

\begin{theorem}
\label{thm.vander-det}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$. Then:

\textbf{(a)} We have%
\[
\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(b)} We have%
\[
\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(c)} We have%
\[
\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(d)} We have%
\[
\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\]

\end{theorem}

\begin{remark}
For $n=4$, the four matrices appearing in Theorem \ref{thm.vander-det} are%
\begin{align*}
\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3} & x_{1}^{2} & x_{1} & 1\\
x_{2}^{3} & x_{2}^{2} & x_{2} & 1\\
x_{3}^{3} & x_{3}^{2} & x_{3} & 1\\
x_{4}^{3} & x_{4}^{2} & x_{4} & 1
\end{array}
\right)  ,\\
\left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3} & x_{2}^{3} & x_{3}^{3} & x_{4}^{3}\\
x_{1}^{2} & x_{2}^{2} & x_{3}^{2} & x_{4}^{2}\\
x_{1} & x_{2} & x_{3} & x_{4}\\
1 & 1 & 1 & 1
\end{array}
\right)  ,\\
\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
1 & x_{1} & x_{1}^{2} & x_{1}^{3}\\
1 & x_{2} & x_{2}^{2} & x_{2}^{3}\\
1 & x_{3} & x_{3}^{2} & x_{3}^{3}\\
1 & x_{4} & x_{4}^{2} & x_{4}^{3}%
\end{array}
\right)  ,\\
\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
x_{1} & x_{2} & x_{3} & x_{4}\\
x_{1}^{2} & x_{2}^{2} & x_{3}^{2} & x_{4}^{2}\\
x_{1}^{3} & x_{2}^{3} & x_{3}^{3} & x_{4}^{3}%
\end{array}
\right)  .
\end{align*}
It is clear that the second of these four matrices is the transpose of the
first; the fourth is the transpose of the third; and the fourth is obtained
from the second by rearranging the rows in opposite order. Thus, the four
parts of Theorem \ref{thm.vander-det} are rather easily seen to be equivalent.
(We shall prove part \textbf{(a)} and derive the others from it.) Nevertheless
it is useful to have seen them all.
\end{remark}

Theorem \ref{thm.vander-det} is a classical result (known as the
\textit{\href{https://en.wikipedia.org/wiki/Vandermonde_matrix}{\textit{Vandermonde
determinant}}}, although \href{http://arxiv.org/abs/1204.4716}{it is unclear}
whether it has been proven by Vandermonde): Almost all texts on linear algebra
mention it (or, rather, at least one of its four parts), although some only
prove it in lesser generality. It is a fundamental result that has various
applications to abstract algebra, number theory, coding theory, combinatorics
and numerical mathematics.

Theorem \ref{thm.vander-det} has many known proofs\footnote{For four
combinatorial proofs, see \cite{Gessel-Vand}, \cite[\S 5.3]{Aigner07},
\cite[\S 12.9]{Loehr-BC} and \cite{BenDre-Vand}. (Specifically,
\cite{Gessel-Vand} and \cite{BenDre-Vand} prove Theorem \ref{thm.vander-det}
\textbf{(c)}, whereas \cite[\S 5.3]{Aigner07} and \cite[\S 12.9]{Loehr-BC}
prove Theorem \ref{thm.vander-det} \textbf{(b)}. But as we will see, the four
parts of Theorem \ref{thm.vander-det} are easily seen to be equivalent to each
other.)}. My favorite proof (of Theorem \ref{thm.vander-det} \textbf{(c)}
only, but as I said the other parts are easily seen to be equivalent) is given
in \cite[Theorem 1]{GriHyp}. In these notes, I will show another proof, which
has the advantage of demonstrating how to use Theorem \ref{thm.laplace.pre}.

\begin{example}
\label{exa.vander-det.3}Let $x,y,z\in\mathbb{K}$. Let $A=\left(
\begin{array}
[c]{ccc}%
1 & x & x^{2}\\
1 & y & y^{2}\\
1 & z & z^{2}%
\end{array}
\right)  $. Then, (\ref{eq.det.small.3x3}) shows that%
\begin{align}
\det A  &  =1yz^{2}+xy^{2}\cdot1+x^{2}\cdot1z-1y^{2}z-x\cdot1z^{2}-x^{2}%
y\cdot1\nonumber\\
&  =yz^{2}+xy^{2}+x^{2}z-y^{2}z-xz^{2}-x^{2}y=yz\left(  z-y\right)  +zx\left(
x-z\right)  +xy\left(  y-x\right)  . \label{eq.exa.vander-det.3.1}%
\end{align}
On the other hand, Theorem \ref{thm.vander-det} \textbf{(c)} (applied to
$n=3$, $x_{1}=x$, $x_{2}=y$ and $x_{3}=z$) yields $\det A=\left(  y-x\right)
\left(  z-x\right)  \left(  z-y\right)  $. Compared with
(\ref{eq.exa.vander-det.3.1}), this yields%
\begin{equation}
\left(  y-x\right)  \left(  z-x\right)  \left(  z-y\right)  =yz\left(
z-y\right)  +zx\left(  x-z\right)  +xy\left(  y-x\right)  .
\label{eq.exa.vander-det.3.2}%
\end{equation}
You might have encountered this curious identity as a trick of use in contest
problems. When $x,y,z$ are three distinct complex numbers, we can divide
(\ref{eq.exa.vander-det.3.2}) by $\left(  y-x\right)  \left(  z-x\right)
\left(  z-y\right)  $, and obtain%
\[
1=\dfrac{yz}{\left(  y-x\right)  \left(  z-x\right)  }+\dfrac{zx}{\left(
z-y\right)  \left(  x-y\right)  }+\dfrac{xy}{\left(  x-z\right)  \left(
y-z\right)  }.
\]

\end{example}

Before we prove Theorem \ref{thm.vander-det}, let us see (in greater
generality) what happens to the determinant of a matrix if we rearrange the
rows in opposite order:

\begin{lemma}
\label{lem.vander-det.lem-rearr}Let $n\in\mathbb{N}$. Let $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Then,%
\[
\det\left(  \left(  a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.vander-det.lem-rearr}.]Let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $. Define a permutation $w_{0}$
in $S_{n}$ as in Exercise \ref{exe.ps4.1c}. In the solution of Exercise
\ref{exe.ps4.1c}, we have shown that $\left(  -1\right)  ^{w_{0}}=\left(
-1\right)  ^{n\left(  n-1\right)  /2}$.

\begin{verlong}
Now, $w_{0}\in S_{n}$. In other words, $w_{0}$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$w_{0}$ is a permutation of the set $\left[  n\right]  $ (since $\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $). In other words, $w_{0}$ is a
bijective map $\left[  n\right]  \rightarrow\left[  n\right]  $.
\end{verlong}

Now, we can apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, $w_{0}$ and $\left(
a_{w_{0}\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
instead of $B$, $\kappa$ and $B_{\kappa}$. As a result, we obtain%
\begin{align}
\det\left(  \left(  a_{w_{0}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\underbrace{\left(  -1\right)  ^{w_{0}}%
}_{=\left(  -1\right)  ^{n\left(  n-1\right)  /2}}\cdot\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\label{pf.lem.vander-det.lem-rearr.1}%
\end{align}


\begin{vershort}
But $w_{0}\left(  i\right)  =n+1-i$ for every $i\in\left\{  1,2,\ldots
,n\right\}  $ (by the definition of $w_{0}$). Thus,
(\ref{pf.lem.vander-det.lem-rearr.1}) rewrites as $\det\left(  \left(
a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(
-1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  $. This proves Lemma
\ref{lem.vander-det.lem-rearr}.
\end{vershort}

\begin{verlong}
However, $w_{0}\left(  i\right)  =n+1-i$ for every $i\in\left\{
1,2,\ldots,n\right\}  $ (by the definition of $w_{0}$). Hence, $a_{w_{0}%
\left(  i\right)  ,j}=a_{n+1-i,j}$ for every $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. Thus,%
\[
\det\left(  \left(  \underbrace{a_{w_{0}\left(  i\right)  ,j}}_{=a_{n+1-i,j}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\det\left(  \left(
a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  ,
\]
so that%
\begin{align*}
\det\left(  \left(  a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)   &  =\det\left(  \left(  a_{w_{0}\left(  i\right)  ,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This proves Lemma \ref{lem.vander-det.lem-rearr}.
\end{verlong}
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm.vander-det}.]\textbf{(a)} For every $u\in\left\{
0,1,\ldots,n\right\}  $, let $A_{u}$ be the $u\times u$-matrix $\left(
x_{i}^{u-j}\right)  _{1\leq i\leq u,\ 1\leq j\leq u}$.

Now, let us show that%
\begin{equation}
\det\left(  A_{u}\right)  =\prod_{1\leq i<j\leq u}\left(  x_{i}-x_{j}\right)
\label{pf.thm.vander-det.a.goal}%
\end{equation}
for every $u\in\left\{  0,1,\ldots,n\right\}  $.

\textit{Proof of (\ref{pf.thm.vander-det.a.goal}):} We will prove
(\ref{pf.thm.vander-det.a.goal}) by induction over $u$:

\textit{Induction base:} The matrix $A_{0}$ is a $0\times0$-matrix and thus
has determinant $\det\left(  A_{0}\right)  =1$. On the other hand, the product
$\prod_{1\leq i<j\leq0}\left(  x_{i}-x_{j}\right)  $ is an empty product
(i.e., a product of $0$ elements of $\mathbb{K}$) and thus equals $1$ as well.
Hence, both $\det\left(  A_{0}\right)  $ and $\prod_{1\leq i<j\leq0}\left(
x_{i}-x_{j}\right)  $ equal $1$. Thus, $\det\left(  A_{0}\right)
=\prod_{1\leq i<j\leq0}\left(  x_{i}-x_{j}\right)  $. In other words,
(\ref{pf.thm.vander-det.a.goal}) holds for $u=0$. The induction base is thus complete.

\textit{Induction step:} Let $U\in\left\{  1,2,\ldots,n\right\}  $. Assume
that (\ref{pf.thm.vander-det.a.goal}) holds for $u=U-1$. We need to prove that
(\ref{pf.thm.vander-det.a.goal}) holds for $u=U$.

Recall that $A_{U}=\left(  x_{i}^{U-j}\right)  _{1\leq i\leq U,\ 1\leq j\leq
U}$ (by the definition of $A_{U}$).

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, define
$b_{i,j}\in\mathbb{K}$ by%
\[
b_{i,j}=%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
.
\]
Let $B$ be the $U\times U$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$. For example, if $U=4$, then%
\begin{align*}
A_{U}  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3} & x_{1}^{2} & x_{1} & 1\\
x_{2}^{3} & x_{2}^{2} & x_{2} & 1\\
x_{3}^{3} & x_{3}^{2} & x_{3} & 1\\
x_{4}^{3} & x_{4}^{2} & x_{4} & 1
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
B  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3}-x_{4}x_{1}^{2} & x_{1}^{2}-x_{4}x_{1} & x_{1}-x_{4} & 1\\
x_{2}^{3}-x_{4}x_{2}^{2} & x_{2}^{2}-x_{4}x_{2} & x_{2}-x_{4} & 1\\
x_{3}^{3}-x_{4}x_{3}^{2} & x_{3}^{2}-x_{4}x_{3} & x_{3}-x_{4} & 1\\
x_{4}^{3}-x_{4}x_{4}^{2} & x_{4}^{2}-x_{4}x_{4} & x_{4}-x_{4} & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3}-x_{4}x_{1}^{2} & x_{1}^{2}-x_{4}x_{1} & x_{1}-x_{4} & 1\\
x_{2}^{3}-x_{4}x_{2}^{2} & x_{2}^{2}-x_{4}x_{2} & x_{2}-x_{4} & 1\\
x_{3}^{3}-x_{4}x_{3}^{2} & x_{3}^{2}-x_{4}x_{3} & x_{3}-x_{4} & 1\\
0 & 0 & 0 & 1
\end{array}
\right)  .
\end{align*}
We claim that $\det B=\det\left(  A_{U}\right)  $. Indeed, here are two ways
to prove this:

\textit{First proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Exercise
\ref{exe.ps4.6k} \textbf{(b)} shows that the determinant of a $U\times
U$-matrix does not change if we subtract a multiple of one of its columns from
another column. Now, let us subtract $x_{U}$ times the $2$-nd column of
$A_{U}$ from the $1$-st column, then subtract $x_{U}$ times the $3$-rd column
of the resulting matrix from the $2$-nd column, and so on, all the way until
we finally subtract $x_{U}$ times the $U$-th column of the matrix from the
$\left(  U-1\right)  $-st column\footnote{So, all in all, we subtract the
$x_{U}$-multiple of each column from its neighbor to its left, but the order
in which we are doing it (namely, from left to right) is important: It means
that the column we are subtracting is unchanged from $A_{U}$. (If we would be
doing these subtractions from right to left instead, then the columns to be
subtracting would be changed by the preceding steps.)}. The resulting matrix
is $B$ (according to our definition of $B$). Thus, $\det B=\det\left(
A_{U}\right)  $ (since our subtractions never change the determinant). This
proves $\det B=\det\left(  A_{U}\right)  $.

\textit{Second proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Here is
another way to prove that $\det B=\det\left(  A_{U}\right)  $, with some less handwaving.

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, we
define $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
-x_{U}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
.
\]
Let $C$ be the $U\times U$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$.

For example, if $U=4$, then%
\[
C=\left(
\begin{array}
[c]{cccc}%
1 & 0 & 0 & 0\\
-x_{4} & 1 & 0 & 0\\
0 & -x_{4} & 1 & 0\\
0 & 0 & -x_{4} & 1
\end{array}
\right)  .
\]


\begin{vershort}
The matrix $C$ is lower-triangular, and thus Exercise \ref{exe.ps4.3} shows
that its determinant is $\det C=\underbrace{c_{1,1}}_{=1}\underbrace{c_{2,2}%
}_{=1}\cdots\underbrace{c_{U,U}}_{=1}=1$.
\end{vershort}

\begin{verlong}
We have $c_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$ be such that
$i<j$. Then, $i\neq j$ (since $i<j$) and $i\neq j+1$ (since $i<j<j+1$). Thus,
we have neither $i=j$ nor $i=j+1$. Now, the definition of $c_{i,j}$ yields
$c_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
-x_{U}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
=0$ (since neither $i=j$ nor $i=j+1$), qed.}. Hence, Exercise \ref{exe.ps4.3}
(applied to $U$, $C$ and $c_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{align*}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{U,U}=\prod_{i=1}^{U}\underbrace{c_{i,i}%
}_{\substack{=%
\begin{cases}
1, & \text{if }i=i;\\
-x_{U}, & \text{if }i=i+1;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }c_{i,i}\text{)}}}=\prod_{i=1}^{U}\underbrace{%
\begin{cases}
1, & \text{if }i=i;\\
-x_{U}, & \text{if }i=i+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=1\\\text{(since }i=i\text{)}}}\\
&  =\prod_{i=1}^{U}1=1.
\end{align*}

\end{verlong}

\begin{vershort}
On the other hand, it is easy to see that $B=A_{U}C$ (check this!). Thus,
Theorem \ref{thm.det(AB)} yields $\det B=\det\left(  A_{U}\right)
\cdot\underbrace{\det C}_{=1}=\det\left(  A_{U}\right)  $. So we have proven
$\det B=\det\left(  A_{U}\right)  $ again.
\end{vershort}

\begin{verlong}
On the other hand, every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U\right\}  ^{2}$ satisfy%
\begin{equation}
b_{i,j}=\sum_{k=1}^{U}x_{i}^{U-k}c_{k,j} \label{pf.thm.vander-det.B=AUC.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.vander-det.B=AUC.pf.1}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$. We must prove
(\ref{pf.thm.vander-det.B=AUC.pf.1}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,U\right\}  $ and $j\in\left\{  1,2,\ldots,U\right\}
$. We are in one of the following two cases:
\par
\textit{Case 1:} We have $j<U$.
\par
\textit{Case 2:} We have $j\geq U$.
\par
Let us first consider Case 1. In this case, we have $j<U$. Hence,
$j\in\left\{  1,2,\ldots,U-1\right\}  $ (since $j\in\left\{  1,2,\ldots
,U\right\}  $). Now, the definition of $b_{i,j}$ yields%
\begin{equation}
b_{i,j}=%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j+1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
=x_{i}^{U-j}-x_{U}x_{i}^{U-j-1} \label{pf.thm.vander-det.B=AUC.pf.1.pf.1}%
\end{equation}
(since $j<U$). On the other hand, $j\in\left\{  1,2,\ldots,U-1\right\}  $, so
that $j+1\in\left\{  2,3,\ldots,U\right\}  \subseteq\left\{  1,2,\ldots
,U\right\}  $. Thus, $j$ and $j+1$ are two distinct elements of $\left\{
1,2,\ldots,U\right\}  $.
\par
Now,%
\begin{align*}
&  \underbrace{\sum_{k=1}^{U}}_{=\sum_{k\in\left\{  1,2,\ldots,U\right\}  }%
}x_{i}^{U-k}\underbrace{c_{k,j}}_{\substack{=%
\begin{cases}
1, & \text{if }k=j;\\
-x_{U}, & \text{if }k=j+1;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }c_{k,j}\text{)}}}\\
&  =\sum_{k\in\left\{  1,2,\ldots,U\right\}  }x_{i}^{U-k}%
\begin{cases}
1, & \text{if }k=j;\\
-x_{U}, & \text{if }k=j+1;\\
0, & \text{otherwise}%
\end{cases}
\\
&  =x_{i}^{U-j}\underbrace{%
\begin{cases}
1, & \text{if }j=j;\\
-x_{U}, & \text{if }j=j+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=1\\\text{(since }j=j\text{)}}}+\underbrace{x_{i}^{U-\left(
j+1\right)  }}_{=x_{i}^{U-j-1}}\underbrace{%
\begin{cases}
1, & \text{if }j+1=j;\\
-x_{U}, & \text{if }j+1=j+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=-x_{U}\\\text{(since }j+1=j+1\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{k\in\left\{  1,2,\ldots,U\right\}
;\\k\notin\left\{  j,j+1\right\}  }}x_{i}^{U-k}\underbrace{%
\begin{cases}
1, & \text{if }k=j;\\
-x_{U}, & \text{if }k=j+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=0\\\text{(since neither }k=j\text{ nor }k=j+1\\\text{(since
}k\notin\left\{  j,j+1\right\}  \text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }k=j\text{ and for }k=j+1\text{
from}\\
\text{the sum, because }j\text{ and }j+1\text{ are two distinct elements of
}\left\{  1,2,\ldots,U\right\}
\end{array}
\right) \\
&  =x_{i}^{U-j}1+x_{i}^{U-j-1}\left(  -x_{U}\right)  +\underbrace{\sum
_{\substack{k\in\left\{  1,2,\ldots,U\right\}  ;\\k\notin\left\{
j,j+1\right\}  }}x_{i}^{U-k}0}_{=0}=x_{i}^{U-j}1+x_{i}^{U-j-1}\left(
-x_{U}\right)  =x_{i}^{U-j}-x_{i}^{U-j-1}x_{U}=x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}.
\end{align*}
Comparing this with (\ref{pf.thm.vander-det.B=AUC.pf.1.pf.1}), we obtain
$b_{i,j}=\sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}$. Thus,
(\ref{pf.thm.vander-det.B=AUC.pf.1}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $j\geq U$. Thus, $j=U$
(since $j\in\left\{  1,2,\ldots,U\right\}  $). Now, the definition of
$b_{i,j}$ yields%
\begin{equation}
b_{i,j}=%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j+1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
=1 \label{pf.thm.vander-det.B=AUC.pf.1.pf.2}%
\end{equation}
(since $j=U$).
\par
Now, let $k\in\left\{  1,2,\ldots,U-1\right\}  $. Then, $k\leq U-1<U=j$ and
thus $k\neq j$. Also, $k<j<j+1$ and thus $k\neq j+1$. Hence, neither $k=j$ nor
$k=j+1$ (since $k\neq j$ and $k\neq j+1$). But the definition of $c_{k,j}$
yields%
\[
c_{k,j}=%
\begin{cases}
1, & \text{if }k=j;\\
-x_{U}, & \text{if }k=j+1;\\
0, & \text{otherwise}%
\end{cases}
=0\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }k=j\text{ nor
}k=j+1\right)  .
\]
Let us now forget that we fixed $k$. We thus have shown that
\begin{equation}
c_{k,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots
,U-1\right\}  . \label{pf.thm.vander-det.B=AUC.pf.1.pf.3}%
\end{equation}
Now,
\begin{align*}
&  \sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}=\sum_{k=1}^{U-1}x_{i}^{U-k}%
\underbrace{c_{k,j}}_{\substack{=0\\\text{(by
(\ref{pf.thm.vander-det.B=AUC.pf.1.pf.3}))}}}+\underbrace{x_{i}^{U-U}}%
_{=x_{i}^{0}=1}\underbrace{c_{U,j}}_{\substack{=%
\begin{cases}
1, & \text{if }U=j;\\
-x_{U}, & \text{if }U=j+1;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }c_{U,j}\text{)}}}\\
&  =\underbrace{\sum_{k=1}^{U-1}x_{i}^{U-k}0}_{=0}+1\underbrace{%
\begin{cases}
1, & \text{if }U=j;\\
-x_{U}, & \text{if }U=j+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=1\\\text{(since }U=j\text{)}}}=0+1\cdot1=1.
\end{align*}
Compared with (\ref{pf.thm.vander-det.B=AUC.pf.1.pf.2}), this yields
$b_{i,j}=\sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}$. Thus,
(\ref{pf.thm.vander-det.B=AUC.pf.1}) is proven in Case 2.
\par
We have now proven (\ref{pf.thm.vander-det.B=AUC.pf.1}) in each of the two
Cases 1 and 2. Since these two Cases cover all possibilities, this yields that
(\ref{pf.thm.vander-det.B=AUC.pf.1}) always holds. Qed.}. Now,%
\[
B=\left(  \underbrace{b_{i,j}}_{\substack{=\sum_{k=1}^{U}x_{i}^{U-k}%
c_{k,j}\\\text{(by (\ref{pf.thm.vander-det.B=AUC.pf.1}))}}}\right)  _{1\leq
i\leq U,\ 1\leq j\leq U}=\left(  \sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}\right)
_{1\leq i\leq U,\ 1\leq j\leq U}.
\]
Compared with%
\begin{align*}
A_{U}C  &  =\left(  \sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of the product }A_{U}C\text{,}\\
\text{since }A_{U}=\left(  x_{i}^{U-j}\right)  _{1\leq i\leq U,\ 1\leq j\leq
U}\text{ and }C=\left(  c_{i,j}\right)  _{1\leq i\leq U,\ 1\leq j\leq U}%
\end{array}
\right)  ,
\end{align*}
this yields $B=A_{U}C$. Hence,%
\begin{align*}
\det\underbrace{B}_{=A_{U}C}  &  =\det\left(  A_{U}C\right)  =\det\left(
A_{U}\right)  \cdot\underbrace{\det C}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)}, applied to }U\text{, }A_{U}\text{ and }C\\
\text{instead of }n\text{, }A\text{ and }B
\end{array}
\right) \\
&  =\det\left(  A_{U}\right)  .
\end{align*}
Thus, $\det B=\det\left(  A_{U}\right)  $ is proven again.
\end{verlong}

[\textit{Remark:} It is instructive to compare the two proofs of $\det
B=\det\left(  A_{U}\right)  $ given above. They are close kin, although they
might look different at first. In the first proof, we argued that $B$ can be
obtained from $A_{U}$ by subtracting multiples of some columns from others; in
the second, we argued that $B=A_{U}C$ for a specific lower-triangular matrix
$C$. But a look at the matrix $C$ makes it clear that multiplying a $U\times
U$-matrix with $C$ on the right (i.e., transforming a $U\times U$-matrix $X$
into the matrix $XC$) is tantamount to subtracting multiples of some columns
from others, in the way we did it to $A_{U}$ to obtain $B$. So the main
difference between the two proofs is that the first proof used a step-by-step
procedure to obtain $B$ from $A_{U}$, whereas the second proof obtained $B$
from $A_{U}$ by a single-step operation (namely, multiplication by a matrix
$C$).]

Next, we observe that for every $j\in\left\{  1,2,\ldots,U-1\right\}  $, we
have%
\begin{align*}
b_{U,j}  &  =%
\begin{cases}
x_{U}^{U-j}-x_{U}x_{U}^{U-j-1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,j}\right) \\
&  =x_{U}^{U-j}-\underbrace{x_{U}x_{U}^{U-j-1}}_{=x_{U}^{\left(  U-j-1\right)
+1}=x_{U}^{U-j}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<U\text{ (since
}j\in\left\{  1,2,\ldots,U-1\right\}  \text{)}\right) \\
&  =x_{U}^{U-j}-x_{U}^{U-j}=0.
\end{align*}
Hence, Theorem \ref{thm.laplace.pre} (applied to $U$, $B$ and $b_{i,j}$
instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{equation}
\det B=b_{U,U}\cdot\det\left(  \left(  b_{i,j}\right)  _{1\leq i\leq
U-1,\ 1\leq j\leq U-1}\right)  . \label{pf.thm.vander-det.detB=prod}%
\end{equation}
Let $B^{\prime}$ denote the $\left(  U-1\right)  \times\left(  U-1\right)
$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$.

The definition of $b_{U,U}$ yields%
\begin{align*}
b_{U,U}  &  =%
\begin{cases}
x_{U}^{U-U}-x_{U}x_{U}^{U-U-1}, & \text{if }U<U;\\
1, & \text{if }U=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,U}\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }U=U\right)  .
\end{align*}
Thus, (\ref{pf.thm.vander-det.detB=prod}) becomes%
\[
\det B=\underbrace{b_{U,U}}_{=1}\cdot\det\left(  \underbrace{\left(
b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}}_{=B^{\prime}}\right)
=\det\left(  B^{\prime}\right)  .
\]
Compared with $\det B=\det\left(  A_{U}\right)  $, this yields%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  .
\label{pf.thm.vander-det.detAU=detB'}%
\end{equation}


Now, let us take a closer look at $B^{\prime}$. Indeed, every $\left(
i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$ satisfies%
\begin{align}
b_{i,j}  &  =%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{i,j}\right)
\nonumber\\
&  =\underbrace{x_{i}^{U-j}}_{=x_{i}x_{i}^{U-j-1}}-x_{U}x_{i}^{U-j-1}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j<U\text{ (since }j\in\left\{  1,2,\ldots,U-1\right\} \\
\text{(since }\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}
^{2}\text{))}%
\end{array}
\right) \nonumber\\
&  =x_{i}x_{i}^{U-j-1}-x_{U}x_{i}^{U-j-1}=\left(  x_{i}-x_{U}\right)
\underbrace{x_{i}^{U-j-1}}_{=x_{i}^{\left(  U-1\right)  -j}}=\left(
x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}.
\label{pf.thm.vander-det.bij-small}%
\end{align}
Hence,%
\begin{equation}
B^{\prime}=\left(  \underbrace{b_{i,j}}_{\substack{=\left(  x_{i}%
-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}\\\text{(by
(\ref{pf.thm.vander-det.bij-small}))}}}\right)  _{1\leq i\leq U-1,\ 1\leq
j\leq U-1}=\left(  \left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}. \label{pf.thm.vander-det.B'}%
\end{equation}
On the other hand, the definition of $A_{U-1}$ yields
\begin{equation}
A_{U-1}=\left(  x_{i}^{\left(  U-1\right)  -j}\right)  _{1\leq i\leq
U-1,\ 1\leq j\leq U-1}. \label{pf.thm.vander-det.AU-1}%
\end{equation}


Now, we claim that%
\begin{equation}
\det\left(  B^{\prime}\right)  =\det\left(  A_{U-1}\right)  \cdot\prod
_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  . \label{pf.thm.vander-det.detB'=}%
\end{equation}
Indeed, here are two ways to prove this:

\textit{First proof of (\ref{pf.thm.vander-det.detB'=}):} Comparing the
formulas (\ref{pf.thm.vander-det.B'}) and (\ref{pf.thm.vander-det.AU-1}), we
see that the matrix $B^{\prime}$ is obtained from the matrix $A_{U-1}$ by
multiplying the first row by $x_{1}-x_{U}$, the second row by $x_{2}-x_{U}$,
and so on, and finally the $\left(  U-1\right)  $-st row by $x_{U-1}-x_{U}$.
But every time we multiply a row of a $\left(  U-1\right)  \times\left(
U-1\right)  $-matrix by some scalar $\lambda\in\mathbb{K}$, the determinant of
the matrix gets multiplied by $\lambda$ (because of Exercise \ref{exe.ps4.6}
\textbf{(g)}). Hence, the determinant of $B^{\prime}$ is obtained from that of
$A_{U-1}$ by first multiplying by $x_{1}-x_{U}$, then multiplying by
$x_{2}-x_{U}$, and so on, and finally multiplying with $x_{U-1}-x_{U}$. In
other words,%
\[
\det\left(  B^{\prime}\right)  =\det\left(  A_{U-1}\right)  \cdot\prod
_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\]
This proves (\ref{pf.thm.vander-det.detB'=}).

\textit{Second proof of (\ref{pf.thm.vander-det.detB'=}):} For every $\left(
i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$, we define $d_{i,j}%
\in\mathbb{K}$ by%
\[
d_{i,j}=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=j;\\
0, & \text{otherwise}%
\end{cases}
.
\]
Let $D$ be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix
$\left(  d_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$.

For example, if $U=4$, then%
\[
D=\left(
\begin{array}
[c]{ccc}%
x_{1}-x_{4} & 0 & 0\\
0 & x_{2}-x_{4} & 0\\
0 & 0 & x_{3}-x_{4}%
\end{array}
\right)  .
\]


\begin{vershort}
The matrix $D$ is lower-triangular (actually, diagonal), and thus Exercise
\ref{exe.ps4.3} shows that its determinant is $\det D=\left(  x_{1}%
-x_{U}\right)  \left(  x_{2}-x_{U}\right)  \cdots\left(  x_{U-1}-x_{U}\right)
=\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  $.
\end{vershort}

\begin{verlong}
We have $d_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U-1\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$ be such that
$i<j$. Then, we don't have $i=j$ (since $i<j$). Now, the definition of
$d_{i,j}$ yields $d_{i,j}=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=j;\\
0, & \text{otherwise}%
\end{cases}
=0$ (since we don't have $i=j$), qed.}. Hence, Exercise \ref{exe.ps4.3}
(applied to $U-1$, $D$ and $d_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{align*}
\det D  &  =d_{1,1}d_{2,2}\cdots d_{U-1,U-1}=\prod_{i=1}^{U-1}%
\underbrace{d_{i,i}}_{\substack{=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=i;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }d_{i,i}\text{)}}}=\prod_{i=1}^{U-1}\underbrace{%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=i;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=x_{i}-x_{U}\\\text{(since }i=i\text{)}}}\\
&  =\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}

\end{verlong}

\begin{vershort}
On the other hand, it is easy to see that $B^{\prime}=DA_{U-1}$ (check this!).
Thus, Theorem \ref{thm.det(AB)} yields
\[
\det\left(  B^{\prime}\right)  =\det D\cdot\det\left(  A_{U-1}\right)
=\det\left(  A_{U-1}\right)  \cdot\underbrace{\det D}_{=\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  }=\det\left(  A_{U-1}\right)  \cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\]
Thus, (\ref{pf.thm.vander-det.detB'=}) is proven again.
\end{vershort}

\begin{verlong}
On the other hand, every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U-1\right\}  ^{2}$ satisfy%
\begin{equation}
b_{i,j}=\sum_{k=1}^{U-1}d_{i,k}x_{k}^{\left(  U-1\right)  -j}
\label{pf.thm.vander-det.B'=DAU-1.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.vander-det.B'=DAU-1.pf.1}):} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$. We must prove
(\ref{pf.thm.vander-det.B'=DAU-1.pf.1}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,U-1\right\}  $ and $j\in\left\{  1,2,\ldots
,U-1\right\}  $. Now,%
\begin{align*}
&  \underbrace{\sum_{k=1}^{U-1}}_{=\sum_{k\in\left\{  1,2,\ldots,U-1\right\}
}}\underbrace{d_{i,k}}_{\substack{=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=k;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }d_{i,k}\text{)}}}x_{k}^{\left(  U-1\right)
-j}\\
&  =\sum_{k\in\left\{  1,2,\ldots,U-1\right\}  }%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=k;\\
0, & \text{otherwise}%
\end{cases}
x_{k}^{\left(  U-1\right)  -j}\\
&  =\underbrace{%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=i;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=x_{i}-x_{U}\\\text{(since }i=i\text{)}}}x_{i}^{\left(
U-1\right)  -j}+\sum_{\substack{k\in\left\{  1,2,\ldots,U-1\right\}  ;\\k\neq
i}}\underbrace{%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=k;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=0\\\text{(since we don't have }i=k\\\text{(since }i\neq k\text{
(since }k\neq i\text{)))}}}x_{k}^{\left(  U-1\right)  -j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=i\text{ from the sum}\right) \\
&  =\left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}%
+\underbrace{\sum_{\substack{k\in\left\{  1,2,\ldots,U-1\right\}  ;\\k\neq
i}}0x_{k}^{\left(  U-1\right)  -j}}_{=0}=\left(  x_{i}-x_{U}\right)
x_{i}^{\left(  U-1\right)  -j}=b_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.vander-det.bij-small})}\right)  ,
\end{align*}
and therefore (\ref{pf.thm.vander-det.B'=DAU-1.pf.1}) is proven.}. Now,%
\[
B^{\prime}=\left(  \underbrace{b_{i,j}}_{\substack{=\sum_{k=1}^{U-1}%
d_{i,k}x_{k}^{\left(  U-1\right)  -j}\\\text{(by
(\ref{pf.thm.vander-det.B'=DAU-1.pf.1}))}}}\right)  _{1\leq i\leq U-1,\ 1\leq
j\leq U-1}=\left(  \sum_{k=1}^{U-1}d_{i,k}x_{k}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}.
\]
Compared with%
\begin{align*}
DA_{U-1}  &  =\left(  \sum_{k=1}^{U-1}d_{i,k}x_{k}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of the product }DA_{U-1}\text{,}\\
\text{since }D=\left(  d_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq
U-1}\text{ and }A_{U-1}=\left(  x_{i}^{\left(  U-1\right)  -j}\right)  _{1\leq
i\leq U-1,\ 1\leq j\leq U-1}%
\end{array}
\right)  ,
\end{align*}
this yields $B^{\prime}=DA_{U-1}$. Hence,%
\begin{align*}
\det\left(  \underbrace{B^{\prime}}_{=DA_{U-1}}\right)   &  =\det\left(
DA_{U-1}\right)  =\det D\cdot\det\left(  A_{U-1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)}, applied to }U-1\text{, }D\text{ and
}A_{U-1}\\
\text{instead of }n\text{, }A\text{ and }B
\end{array}
\right) \\
&  =\det\left(  A_{U-1}\right)  \cdot\underbrace{\det D}_{=\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  }=\det\left(  A_{U-1}\right)  \cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}
Thus, (\ref{pf.thm.vander-det.detB'=}) is proven again.
\end{verlong}

[\textit{Remark:} Again, our two proofs of (\ref{pf.thm.vander-det.detB'=})
are closely related: the first one reveals $B^{\prime}$ as the result of a
step-by-step process applied to $A_{U-1}$, while the second shows how
$B^{\prime}$ can be obtained from $A_{U-1}$ by a single multiplication.
However, here (in contrast to the proofs of $\det B=\det\left(  A_{U}\right)
$), the step-by-step process involves transforming rows (not columns), and the
multiplication is a multiplication from the left (we have $B^{\prime}%
=DA_{U-1}$, not $B^{\prime}=A_{U-1}D$).]

Now, (\ref{pf.thm.vander-det.detAU=detB'}) becomes%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  =\det\left(
A_{U-1}\right)  \cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\label{pf.thm.vander-det.detAU=}%
\end{equation}


But we have assumed that (\ref{pf.thm.vander-det.a.goal}) holds for $u=U-1$.
In other words,%
\[
\det\left(  A_{U-1}\right)  =\underbrace{\prod_{1\leq i<j\leq U-1}}%
_{=\prod_{j=1}^{U-1}\prod_{i=1}^{j-1}}\left(  x_{i}-x_{j}\right)  =\prod
_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  .
\]
Hence, (\ref{pf.thm.vander-det.detAU=}) yields%
\begin{align*}
\det\left(  A_{U}\right)   &  =\underbrace{\det\left(  A_{U-1}\right)
}_{=\prod_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  }\cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right) \\
&  =\left(  \prod_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)
\right)  \cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}
Compared with%
\begin{align*}
\underbrace{\prod_{1\leq i<j\leq U}}_{=\prod_{j=1}^{U}\prod_{i=1}^{j-1}%
}\left(  x_{i}-x_{j}\right)   &  =\prod_{j=1}^{U}\prod_{i=1}^{j-1}\left(
x_{i}-x_{j}\right)  =\left(  \prod_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(
x_{i}-x_{j}\right)  \right)  \cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the factor for
}j=U\text{ from the product}\right)  ,
\end{align*}
this yields $\det\left(  A_{U}\right)  =\prod_{1\leq i<j\leq U}\left(
x_{i}-x_{j}\right)  $. In other words, (\ref{pf.thm.vander-det.a.goal}) holds
for $u=U$. This completes the induction step.

Now, (\ref{pf.thm.vander-det.a.goal}) is proven by induction. Hence, we can
apply (\ref{pf.thm.vander-det.a.goal}) to $u=n$. As the result, we obtain
$\det\left(  A_{n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
$. Since $A_{n}=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A_{n}$), this rewrites as $\det\left(  \left(
x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(a)}.

\textbf{(b)} The definition of the transpose of a matrix yields $\left(
\left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
^{T}=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\det\left(  \underbrace{\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}\right)  =\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(a)}). Compared with%
\[
\det\left(  \left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)
\]
(by Exercise \ref{exe.ps4.4}, applied to $A=\left(  x_{j}^{n-i}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$), this yields%
\[
\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
This proves Theorem \ref{thm.vander-det} \textbf{(b)}.

\textbf{(d)} Applying Lemma \ref{lem.vander-det.lem-rearr} to $a_{i,j}%
=x_{j}^{n-i}$, we obtain%
\begin{align*}
\det\left(  \left(  x_{j}^{n-\left(  n+1-i\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\left(  -1\right)  ^{n\left(  n-1\right)
/2}\underbrace{\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  }_{\substack{=\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by Theorem \ref{thm.vander-det} \textbf{(b)})}}}\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
This rewrites as%
\begin{equation}
\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  \label{pf.thm.vander-det.d.0}%
\end{equation}
(since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies $x_{j}^{n-\left(  n+1-i\right)  }=x_{j}^{i-1}$).

\begin{vershort}
Now, in the solution to Exercise \ref{exe.ps4.1c}, we have shown that the
number of all pairs $\left(  i,j\right)  $ of integers satisfying $1\leq
i<j\leq n$ is $n\left(  n-1\right)  /2$. In other words,%
\begin{equation}
\left(  \text{the number of all }\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}\text{ such that }i<j\right)  =n\left(  n-1\right)
/2. \label{pf.thm.vander-det.d.short.1}%
\end{equation}
Now,
\begin{align*}
\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)   &  =\prod_{1\leq i<j\leq
n}\underbrace{\left(  x_{j}-x_{i}\right)  }_{=\left(  -1\right)  \left(
x_{i}-x_{j}\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we renamed the index }\left(  j,i\right) \\
\text{as }\left(  i,j\right)  \text{ in the product}%
\end{array}
\right) \\
&  =\prod_{1\leq i<j\leq n}\left(  \left(  -1\right)  \left(  x_{i}%
-x_{j}\right)  \right) \\
&  =\underbrace{\left(  -1\right)  ^{\left(  \text{the number of all }\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}\text{ such that
}i<j\right)  }}_{\substack{=\left(  -1\right)  ^{n\left(  n-1\right)
/2}\\\text{(by (\ref{pf.thm.vander-det.d.short.1}))}}}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right) \\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
Compared with (\ref{pf.thm.vander-det.d.0}), this yields $\det\left(  \left(
x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
j<i\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(d)}.
\end{vershort}

\begin{verlong}
Let $G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq
n\right\}  $. Then, $\left\vert G\right\vert =n\left(  n-1\right)  /2$. (This
was proven in the solution to Exercise \ref{exe.ps4.1c}.) Now,%
\begin{align*}
\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)   &  =\underbrace{\prod
_{1\leq i<j\leq n}}_{\substack{=\prod_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\1\leq i<j\leq n}}=\prod_{\left(  i,j\right)  \in
G}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}%
\ \mid\ 1\leq i<j\leq n\right\}  \text{)}}}\underbrace{\left(  x_{j}%
-x_{i}\right)  }_{=\left(  -1\right)  \left(  x_{i}-x_{j}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
j,i\right)  \text{ as }\left(  i,j\right)  \text{ in the product}\right) \\
&  =\prod_{\left(  i,j\right)  \in G}\left(  \left(  -1\right)  \left(
x_{i}-x_{j}\right)  \right)  =\underbrace{\left(  -1\right)  ^{\left\vert
G\right\vert }}_{\substack{=\left(  -1\right)  ^{n\left(  n-1\right)
/2}\\\text{(since }\left\vert G\right\vert =n\left(  n-1\right)  /2\text{)}%
}}\underbrace{\prod_{\left(  i,j\right)  \in G}}_{\substack{=\prod
_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\1\leq i<j\leq
n}}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}%
\ \mid\ 1\leq i<j\leq n\right\}  \text{)}}}\left(  x_{i}-x_{j}\right) \\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\underbrace{\prod
_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\1\leq i<j\leq n}}}%
_{=\prod_{1\leq i<j\leq n}}\left(  x_{i}-x_{j}\right)  =\left(  -1\right)
^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
Compared with (\ref{pf.thm.vander-det.d.0}), this yields $\det\left(  \left(
x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
j<i\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(d)}.
\end{verlong}

\begin{vershort}
\textbf{(c)} We can derive Theorem \ref{thm.vander-det} \textbf{(c)} from
Theorem \ref{thm.vander-det} \textbf{(d)} in the same way as we derived part
\textbf{(b)} from \textbf{(a)}.
\end{vershort}

\begin{verlong}
\textbf{(c)} The definition of the transpose of a matrix yields $\left(
\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
^{T}=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\det\left(  \underbrace{\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}\right)  =\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}%
-x_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(d)}). Compared with%
\[
\det\left(  \left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)
\]
(by Exercise \ref{exe.ps4.4}, applied to $A=\left(  x_{i}^{j-1}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$), this yields $\det\left(  \left(
x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
j<i\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(c)}.
\end{verlong}
\end{proof}

\begin{remark}
\label{rmk.vander-det.sign}One consequence of Theorem \ref{thm.vander-det} is
a new solution to Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)}:

Namely, let $n\in\mathbb{N}$ and $\sigma\in S_{n}$. Let $x_{1},x_{2}%
,\ldots,x_{n}$ be $n$ elements of $\mathbb{C}$ (or of any commutative ring).
Then, Theorem \ref{thm.vander-det} \textbf{(a)} yields
\[
\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
On the other hand, Theorem \ref{thm.vander-det} \textbf{(a)} (applied to
$x_{\sigma\left(  1\right)  },x_{\sigma\left(  2\right)  },\ldots
,x_{\sigma\left(  n\right)  }$ instead of $x_{1},x_{2},\ldots,x_{n}$) yields%
\begin{equation}
\det\left(  \left(  x_{\sigma\left(  i\right)  }^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{\sigma\left(
i\right)  }-x_{\sigma\left(  j\right)  }\right)  .
\label{exe.rmk.vander-det.sign.2}%
\end{equation}
But Lemma \ref{lem.det.sigma} \textbf{(a)} (applied to $B=\left(  x_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, $\kappa=\sigma$ and
$B_{\kappa}=\left(  x_{\sigma\left(  i\right)  }^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$) yields%
\begin{align*}
\det\left(  \left(  x_{\sigma\left(  i\right)  }^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\left(  -1\right)  ^{\sigma}\cdot
\underbrace{\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  }_{=\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  }\\
&  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  .
\end{align*}
Compared with (\ref{exe.rmk.vander-det.sign.2}), this yields%
\[
\prod_{1\leq i<j\leq n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(
j\right)  }\right)  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\]
Thus, Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} is solved.
However, Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)} cannot be
solved this way.
\end{remark}

\begin{exercise}
\label{exe.vander-det.s1}Let $n$ be a positive integer. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Prove that%
\[
\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(  x_{1}+x_{2}%
+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
(For example, when $n=4$, this states that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
x_{1}^{4} & x_{1}^{2} & x_{1} & 1\\
x_{2}^{4} & x_{2}^{2} & x_{2} & 1\\
x_{3}^{4} & x_{3}^{2} & x_{3} & 1\\
x_{4}^{4} & x_{4}^{2} & x_{4} & 1
\end{array}
\right) \\
&  =\left(  x_{1}+x_{2}+x_{3}+x_{4}\right)  \left(  x_{1}-x_{2}\right)
\left(  x_{1}-x_{3}\right)  \left(  x_{1}-x_{4}\right)  \left(  x_{2}%
-x_{3}\right)  \left(  x_{2}-x_{4}\right)  \left(  x_{3}-x_{4}\right)  .
\end{align*}
)
\end{exercise}

\begin{remark}
\label{rmk.vander-det.schur}We can try to generalize Vandermonde's
determinant. Namely, let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$
nonnegative integers. Let $A$ be the $n\times n$-matrix
\[
\left(  x_{i}^{a_{j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(
\begin{array}
[c]{cccc}%
x_{1}^{a_{1}} & x_{1}^{a_{2}} & \cdots & x_{1}^{a_{n}}\\
x_{2}^{a_{1}} & x_{2}^{a_{2}} & \cdots & x_{2}^{a_{n}}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n}^{a_{1}} & x_{n}^{a_{2}} & \cdots & x_{n}^{a_{n}}%
\end{array}
\right)  .
\]
What can we say about $\det A$ ?

Theorem \ref{thm.vander-det} says that if $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  =\left(  n-1,n-2,\ldots,0\right)  $, then $\det A=\prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  $.

Exercise \ref{exe.vander-det.s1} says that if $n>0$ and $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  =\left(  n,n-2,n-3,\ldots,0\right)  $, then $\det
A=\left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  $.

This suggests a general pattern: We would suspect that for every $\left(
a_{1},a_{2},\ldots,a_{n}\right)  $, there is a polynomial $P_{\left(
a_{1},a_{2},\ldots,a_{n}\right)  }$ in $n$ indeterminates $X_{1},X_{2}%
,\ldots,X_{n}$ such that%
\[
\det A=P_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  }\cdot\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\]


It turns out that this is true. Moreover, this polynomial $P_{\left(
a_{1},a_{2},\ldots,a_{n}\right)  }$ is:

\begin{itemize}
\item zero if two of $a_{1},a_{2},\ldots,a_{n}$ are equal;

\item homogeneous of degree $a_{1}+a_{2}+\cdots+a_{n}-\dbinom{n}{2}$;

\item symmetric in $X_{1},X_{2},\ldots,X_{n}$.
\end{itemize}

For example,%
\begin{align*}
P_{\left(  n-1,n-2,\ldots,0\right)  }  &  =1;\\
P_{\left(  n,n-2,n-3,\ldots,0\right)  }  &  =\sum_{i=1}^{n}x_{i}=x_{1}%
+x_{2}+\cdots+x_{n};\\
P_{\left(  n,n-1,\ldots,n-k+1,n-k-1,n-k-2,\ldots,0\right)  }  &  =\sum_{1\leq
i_{1}<i_{2}<\cdots<i_{k}\leq n}x_{i_{1}}x_{i_{2}}\cdots x_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  0,1,\ldots,n\right\}  ;\\
P_{\left(  n+1,n-2,n-3,\ldots,0\right)  }  &  =\sum_{1\leq i\leq j\leq n}%
x_{i}x_{j};\\
P_{\left(  n+1,n-1,n-3,n-4,\ldots,0\right)  }  &  =\sum_{1\leq i<j\leq
n}\left(  x_{i}^{2}x_{j}+x_{i}x_{j}^{2}\right)  +2\sum_{1\leq i\leq j\leq
k\leq n}x_{i}x_{j}x_{k}.
\end{align*}


But this polynomial $P_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  }$ can
actually be described rather explicitly for general $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $; it is a so-called \textit{Schur polynomial} (at
least when $a_{1}>a_{2}>\cdots>a_{n}$; otherwise it is either zero or $\pm$ a
Schur polynomial). See \cite[The Bi-Alternant Formula]{Stembridge},
\cite[Theorem 7.15.1]{Stanley-EC1} or \cite{Leeuwen-aS} for the details.
(Notice that \cite{Leeuwen-aS} uses the notation $\varepsilon\left(
\sigma\right)  $ for the sign of a permutation $\sigma$.) The theory of Schur
polynomials shows, in particular, that all coefficients of the polynomial
$P_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  }$ have equal sign (which is
positive if $a_{1}>a_{2}>\cdots>a_{n}$).
\end{remark}

\begin{remark}
\label{rmk.vander-det.secret-ints}There are plenty other variations on the
Vandermonde determinant. For instance, one can try replacing the powers
$x_{i}^{j-1}$ by binomial coefficients $\dbinom{x_{i}}{j-1}$ in Theorem
\ref{thm.vander-det} \textbf{(c)}, at least when these binomial coefficients
are well-defined (e.g., when the $x_{1},x_{2},\ldots,x_{n}$ are complex
numbers). The result is rather nice: If $x_{1},x_{2},\ldots,x_{n}$ are any $n$
complex numbers, then%
\[
\prod_{1\leq i<j\leq n}\dfrac{x_{i}-x_{j}}{i-j}=\det\left(  \left(
\dbinom{x_{i}}{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\]
(This is proven, e.g., in \cite[Corollary 11]{GriHyp} and \cite[\S 9, Example
5]{AndDos}.) This has the surprising consequence that, whenever $x_{1}%
,x_{2},\ldots,x_{n}$ are $n$ integers, the product $\prod_{1\leq i<j\leq
n}\dfrac{x_{i}-x_{j}}{i-j}$ is itself an integer (because it is the
determinant of a matrix whose entries are integers). This is a nontrivial
result! (A more elementary proof appears in \cite[\S 3, Example 8]{AndDos}.)

Another \textquotedblleft secret integer\textquotedblright\ (i.e., rational
number which turns out to be an integer for non-obvious reasons) is%
\begin{equation}
\dfrac{H\left(  a\right)  H\left(  b\right)  H\left(  c\right)  H\left(
a+b+c\right)  }{H\left(  b+c\right)  H\left(  c+a\right)  H\left(  a+b\right)
}, \label{eq.rmk.vander-det.secret-ints.H}%
\end{equation}
where $a,b,c$ are three nonnegative integers, and where $H\left(  n\right)  $
(for $n\in\mathbb{N}$) denotes the \textit{hyperfactorial} of $n$, defined by%
\[
H\left(  n\right)  =\prod_{k=0}^{n-1}k!=0!\cdot1!\cdot\cdots\cdot\left(
n-1\right)  !.
\]
I am aware of two proofs of the fact that
(\ref{eq.rmk.vander-det.secret-ints.H}) gives an integer for every
$a,b,c\in\mathbb{N}$: One proof is combinatorial, and argues that
(\ref{eq.rmk.vander-det.secret-ints.H}) is the number of \textit{plane
partitions inside an }$a\times b\times c$\textit{-box} (see \cite[last
equality in \S 7.21]{Stanley-EC2} for a proof), or, equivalently, the number
of \textit{rhombus tilings of a hexagon with sidelengths }$a,b,c,a,b,c$ (see
\cite{Eisenk-planepartits} for a precise statement). Another proof (see
\cite[Theorem 0]{GriHyp}) exhibits (\ref{eq.rmk.vander-det.secret-ints.H}) as
the determinant of a matrix, again using the Vandermonde determinant!

(None of the references to \cite{GriHyp} makes any claim of precedence;
actually, I am rather sure of the opposite, i.e., that none of my proofs in
\cite{GriHyp} are new.)
\end{remark}

For some more exercises related to Vandermonde determinants, see \cite[Chapter
1, problems 1.12--1.22]{Prasolov}. Here comes one of them:

\begin{exercise}
\label{exe.vander-det.xi+yj}Let $n$ be a positive integer. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $y_{1},y_{2}%
,\ldots,y_{n}$ be $n$ elements of $\mathbb{K}$.

\textbf{(a)} For every $m\in\left\{  0,1,\ldots,n-2\right\}  $, prove that%
\[
\det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0.
\]


\textbf{(b)} Prove that%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}


[\textbf{Hint:} Use the binomial theorem.]

\textbf{(c)} Let $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  \in\mathbb{K}%
^{n}$ be an $n$-tuple of elements of $\mathbb{K}$. Let $P\left(  X\right)
\in\mathbb{K}\left[  X\right]  $ be the polynomial $\sum_{k=0}^{n-1}p_{k}%
X^{k}$. Prove that%
\begin{align*}
&  \det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}

\end{exercise}

Notice how Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)} generalizes
Example \ref{exam.xi+yj} (for $n\geq3$).

\subsection{Invertible elements in commutative rings, and fields}

We shall now interrupt our study of determinants for a moment. Let us define
the notion of inverses in $\mathbb{K}$. (Recall that $\mathbb{K}$ is a
commutative ring.)

\begin{definition}
\label{def.rings.inverse}Let $a\in\mathbb{K}$. Then, an element $b\in
\mathbb{K}$ is said to be an \textit{inverse} of $a$ if it satisfies $ab=1$
and $ba=1$.
\end{definition}

Of course, the two conditions $ab=1$ and $ba=1$ in Definition
\ref{def.rings.inverse} are equivalent, since $ab=ba$ for every $a\in
\mathbb{K}$ and $b\in\mathbb{K}$. Nevertheless, we have given both conditions,
because this way the similarity between the inverse of an element of
$\mathbb{K}$ and the inverse of a map becomes particularly clear.

For example, the element $1$ of $\mathbb{Z}$ is its own inverse (since
$1\cdot1=1$), and the element $-1$ of $\mathbb{Z}$ is its own inverse as well
(since $\left(  -1\right)  \cdot\left(  -1\right)  =1$). These elements $1$
and $-1$ are the only elements of $\mathbb{Z}$ which have an inverse in
$\mathbb{Z}$. However, in the larger commutative ring $\mathbb{Q}$, every
nonzero element $a$ has an inverse (namely, $\dfrac{1}{a}$).

\begin{proposition}
\label{prop.rings.inverse-uni}Let $a\in\mathbb{K}$. Then, there exists at most
one inverse of $a$ in $\mathbb{K}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.rings.inverse-uni}.]Let $b$ and $b^{\prime}$
be any two inverses of $a$ in $\mathbb{K}$. Since $b$ is an inverse of $a$ in
$\mathbb{K}$, we have $ab=1$ and $ba=1$ (by the definition of an
\textquotedblleft inverse of $a$\textquotedblright). Since $b^{\prime}$ is an
inverse of $a$ in $\mathbb{K}$, we have $ab^{\prime}=1$ and $b^{\prime}a=1$
(by the definition of an \textquotedblleft inverse of $a$\textquotedblright).
Now, comparing $b\underbrace{ab^{\prime}}_{=1}=b$ with $\underbrace{ba}%
_{=1}b^{\prime}=b^{\prime}$, we obtain $b=b^{\prime}$.

Let us now forget that we fixed $b$ and $b^{\prime}$. We thus have shown that
if $b$ and $b^{\prime}$ are two inverses of $a$ in $\mathbb{K}$, then
$b=b^{\prime}$. In other words, any two inverses of $a$ in $\mathbb{K}$ are
equal. In other words, there exists at most one inverse of $a$ in $\mathbb{K}%
$. This proves Proposition \ref{prop.rings.inverse-uni}.
\end{proof}

\begin{definition}
\label{def.rings.invertible}\textbf{(a)} An element $a\in\mathbb{K}$ is said
to be \textit{invertible} (or, more precisely, \textit{invertible in
}$\mathbb{K}$) if and only if there exists an inverse of $a$ in $\mathbb{K}$.
In this case, this inverse of $a$ is unique (by Proposition
\ref{prop.rings.inverse-uni}), and thus will be called \textit{the inverse of
}$a$ and denoted by $a^{-1}$.

\textbf{(b)} It is clear that the unity $1$ of $\mathbb{K}$ is invertible
(having inverse $1$). Also, the product of any two invertible elements $a$ and
$b$ of $\mathbb{K}$ is again invertible (having inverse $\left(  ab\right)
^{-1}=a^{-1}b^{-1}$).

\textbf{(c)} If $a$ and $b$ are two elements of $\mathbb{K}$ such that $a$ is
invertible (in $\mathbb{K}$), then we write $\dfrac{b}{a}$ (or $b/a$) for the
product $ba^{-1}$. These fractions behave just as fractions of integers
behave: For example, if $a,b,c,d$ are four elements of $\mathbb{K}$ such that
$a$ and $c$ are invertible, then $\dfrac{b}{a}+\dfrac{d}{c}=\dfrac{bc+da}{ac}$
and $\dfrac{b}{a}\cdot\dfrac{d}{c}=\dfrac{bd}{ac}$ (and the product $ac$ is
indeed invertible, so that the fractions $\dfrac{bc+da}{ac}$ and $\dfrac
{bd}{ac}$ actually make sense).
\end{definition}

Of course, the meaning of the word \textquotedblleft
invertible\textquotedblright\ depends on the ring $\mathbb{K}$. For example,
the integer $2$ is invertible in $\mathbb{Q}$ (because $\dfrac{1}{2}$ is an
inverse of $2$ in $\mathbb{Q}$), but not invertible in $\mathbb{Z}$ (since it
has no inverse in $\mathbb{Z}$). Thus, it is important to say
\textquotedblleft invertible in $\mathbb{K}$\textquotedblright\ unless the
context makes it clear what $\mathbb{K}$ is.

One can usually work with invertible elements in commutative rings in the same
way as one works with nonzero rational numbers. For example, if $a$ is an
invertible element of $\mathbb{K}$, then we can define $a^{n}$ not only for
all $n\in\mathbb{N}$, but also for all $n\in\mathbb{Z}$ (by setting
$a^{n}=\left(  a^{-1}\right)  ^{-n}$ for all negative integers $n$). Of
course, when $n=-1$, this is consistent with our notation $a^{-1}$ for the
inverse of $a$.

Next, we define the notion of a \textit{field}\footnote{We are going to use
the following simple fact: A commutative ring $\mathbb{K}$ is a trivial ring
if and only if $0_{\mathbb{K}}=1_{\mathbb{K}}$.
\par
\textit{Proof.} Assume that $\mathbb{K}$ is a trivial ring. Thus, $\mathbb{K}$
has only one element. Hence, both $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$ have
to equal this one element. Therefore, $0_{\mathbb{K}}=1_{\mathbb{K}}$.
\par
Now, forget that we assumed that $\mathbb{K}$ is a trivial ring. We thus have
proven that
\begin{equation}
\text{if }\mathbb{K}\text{ is a trivial ring, then }0_{\mathbb{K}%
}=1_{\mathbb{K}}\text{.} \label{eq.fn.field.trivring.1}%
\end{equation}
\par
Conversely, assume that $0_{\mathbb{K}}=1_{\mathbb{K}}$. Then, every
$a\in\mathbb{K}$ satisfies $a=a\cdot\underbrace{1_{\mathbb{K}}}%
_{=0_{\mathbb{K}}}=a\cdot0_{\mathbb{K}}=0_{\mathbb{K}}\in\left\{
0_{\mathbb{K}}\right\}  $. In other words, $\mathbb{K}\subseteq\left\{
0_{\mathbb{K}}\right\}  $. Combining this with $\left\{  0_{\mathbb{K}%
}\right\}  \subseteq\mathbb{K}$, we obtain $\mathbb{K}=\left\{  0_{\mathbb{K}%
}\right\}  $. Hence, $\mathbb{K}$ has only one element. In other words,
$\mathbb{K}$ is a trivial ring.
\par
Now, forget that we assumed that $0_{\mathbb{K}}=1_{\mathbb{K}}$. We thus have
proven that
\[
\text{if }0_{\mathbb{K}}=1_{\mathbb{K}}\text{, then }\mathbb{K}\text{ is a
trivial ring.}%
\]
Combining this with (\ref{eq.fn.field.trivring.1}), we conclude that
$\mathbb{K}$ is a trivial ring if and only if $0_{\mathbb{K}}=1_{\mathbb{K}}%
$.}.

\begin{definition}
A commutative ring $\mathbb{K}$ is said to be a \textit{field} if it satisfies
the following two properties:

\begin{itemize}
\item We have $0_{\mathbb{K}}\neq1_{\mathbb{K}}$ (that is, $\mathbb{K}$ is not
a trivial ring).

\item Every element of $\mathbb{K}$ is either zero or invertible.
\end{itemize}
\end{definition}

For example, $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$ are fields, whereas
polynomial rings such as $\mathbb{Q}\left[  x\right]  $ or $\mathbb{R}\left[
a,b\right]  $ are not fields\footnote{For example, the polynomial $x$ is not
invertible in $\mathbb{Q}\left[  x\right]  $.}. For $n$ being a positive
integer, the ring $\mathbb{Z}/n\mathbb{Z}$ (that is, the ring of residue
classes of integers modulo $n$) is a field if and only if $n$ is a prime number.

Linear algebra (i.e., the study of matrices and linear transformations)
becomes much easier (in many aspects) when $\mathbb{K}$ is a
field\footnote{Many properties of a matrix over a field (such as its rank) are
not even well-defined over an arbitrary commutative ring.}. This is one of the
main reasons why most courses on linear algebra work over fields only (or
begin by working over fields and only later move to the generality of
commutative rings). In these notes we are almost completely limiting ourselves
to the parts of matrix theory which work over any commutative ring.
Nevertheless, let us comment on how determinants can be computed fast when
$\mathbb{K}$ is a field.

\Needspace{30\baselineskip}

\begin{remark}
\label{rmk.laplace.pre.gauss-alg}Assume that $\mathbb{K}$ is a field. If $A$
is an $n\times n$-matrix over $\mathbb{K}$, then the determinant of $A$ can be
computed using (\ref{eq.det.eq.1})... but in practice, you probably do not
\textbf{want} to compute it this way, since the right hand side of
(\ref{eq.det.eq.1}) contains a sum of $n!$ terms.

It turns out that there is an algorithm to compute $\det A$, which is
(usually) a lot faster. It is a version of the Gaussian elimination algorithm
commonly used for solving systems of linear equations.

Let us illustrate it on an example: Set%
\[
n=4,\ \ \ \ \ \ \ \ \ \ \mathbb{K}=\mathbb{Q}\ \ \ \ \ \ \ \ \ \ \text{and
}A=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
0 & -1 & 0 & 2\\
2 & 4 & -2 & 3\\
5 & 1 & 3 & 5
\end{array}
\right)  .
\]
We want to find $\det A$.

Exercise \ref{exe.ps4.6k} \textbf{(b)} shows that if we add a scalar multiple
of a column of a matrix to another column of this matrix, then the determinant
of the matrix does not change. Now, by adding appropriate scalar multiples of
the fourth column of $A$ to the first three columns of $A$, we can make sure
that the first three entries of the fourth row of $A$ become zero: Namely, we
have to

\begin{itemize}
\item add $\left(  -1\right)  $ times the fourth column of $A$ to the first
column of $A$;

\item add $\left(  -1/5\right)  $ times the fourth column of $A$ to the second
column of $A$;

\item add $\left(  -3/5\right)  $ times the fourth column of $A$ to the third
column of $A$.
\end{itemize}

These additions can be performed in any order, since none of them
\textquotedblleft interacts\textquotedblright\ with any other (more precisely,
none of them uses any entries that another of them changes). As we know, none
of these additions changes the determinant of the matrix.

Having performed these three additions, we end up with the matrix%
\begin{equation}
A^{\prime}=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
-2 & -7/5 & -6/5 & 2\\
-1 & 17/5 & -19/5 & 3\\
0 & 0 & 0 & 5
\end{array}
\right)  . \label{eq.rmk.laplace.pre.gauss-alg.3}%
\end{equation}
We have $\det\left(  A^{\prime}\right)  =\det A$ (because $A^{\prime}$ was
obtained from $A$ by three operations which do not change the determinant).
Moreover, the fourth row of $A^{\prime}$ contains only one nonzero entry --
namely, its last entry. In other words, if we write $A^{\prime}$ in the form
$A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq4,\ 1\leq j\leq4}$,
then $a_{4,j}^{\prime}=0$ for every $j\in\left\{  1,2,3\right\}  $. Thus,
Theorem \ref{thm.laplace.pre} (applied to $4$, $A^{\prime}$ and $a_{i,j}%
^{\prime}$ instead of $n$, $A$ and $a_{i,j}$) shows that%
\begin{align*}
\det\left(  A^{\prime}\right)   &  =\underbrace{a_{4,4}^{\prime}}_{=5}%
\cdot\det\left(  \underbrace{\left(  a_{i,j}^{\prime}\right)  _{1\leq
i\leq3,\ 1\leq j\leq3}}_{=\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  }\right) \\
&  =5\cdot\det\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  .
\end{align*}
Comparing this with $\det\left(  A^{\prime}\right)  =\det A$, we obtain%
\[
\det A=5\cdot\det\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  .
\]


Thus, we have reduced the problem of computing $\det A$ (the determinant of a
$4\times4$-matrix) to the problem of computing $\det\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  $ (the determinant of a $3\times3$-matrix). Likewise, we can try to
reduce the latter problem to the computation of the determinant of a
$2\times2$-matrix, and then further to the computation of the determinant of a
$1\times1$-matrix. (In our example, we obtain $\det A=-140$ at the end.)

This looks like a viable algorithm (which is, furthermore, fairly fast:
essentially as fast as Gaussian elimination). But does it always work? It
turns out that it \textbf{almost} always works. There are cases in which it
can get \textquotedblleft stuck\textquotedblright, and it needs to be modified
to deal with these cases.

Namely, what can happen is that the $\left(  n,n\right)  $-th entry of the
matrix $A$ could be $0$. Again, let us observe this on an example: Set $n=4$
and $A=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
0 & -1 & 0 & 2\\
2 & 4 & -2 & 3\\
5 & 1 & 3 & 0
\end{array}
\right)  $. Then, we cannot turn the first three entries of the fourth row of
$A$ into zeroes by adding appropriate multiples of the fourth column to the
first three columns. (Whatever multiples we add, the fourth row stays
unchanged.) However, we can now switch the second row of $A$ with the fourth
row. This operation produces the matrix $B=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
5 & 1 & 3 & 0\\
2 & 4 & -2 & 3\\
0 & -1 & 0 & 2
\end{array}
\right)  $, which satisfies $\det B=-\det A$ (by Exercise \ref{exe.ps4.6}
\textbf{(a)}). Thus, it suffices to compute $\det B$; and this can be done as above.

The reason why we switched the second row of $A$ with the fourth row is that
the last entry of the second row of $A$ was nonzero. In general, we need to
find a $k\in\left\{  1,2,\ldots,n\right\}  $ such that the last entry of the
$k$-th row of $A$ is nonzero, and switch the $k$-th row of $A$ with the $n$-th
row. But what if no such $k$ exists? In this case, we need another way to
compute $\det A$. It turns out that this is very easy: If there is no
$k\in\left\{  1,2,\ldots,n\right\}  $ such that the last entry of the $k$-th
row of $A$ is nonzero, then the last column of $A$ consists of zeroes, and
thus Exercise \ref{exe.ps4.6} \textbf{(d)} shows that $\det A=0$.

When $\mathbb{K}$ is not a field, this algorithm breaks (or, at least,
\textbf{can} break). Indeed, it relies on the fact that the $\left(
n,n\right)  $-th entry of the matrix $A$ is either zero or invertible. Over a
commutative ring $\mathbb{K}$, it might be neither. For example, if we had
tried to work with $\mathbb{K}=\mathbb{Z}$ (instead of $\mathbb{K}=\mathbb{Q}%
$) in our above example, then we would not be able to add $\left(
-1/5\right)  $ times the fourth column of $A$ to the second column of $A$
(because $-1/5\notin\mathbb{Z}=\mathbb{K}$). Fortunately, of course,
$\mathbb{Z}$ is a subset of $\mathbb{Q}$ (and its operations $+$ and $\cdot$
are consistent with those of $\mathbb{Q}$), so that we can just perform the
whole algorithm over $\mathbb{Q}$ instead of $\mathbb{Z}$. However, we aren't
always in luck: Some commutative rings $\mathbb{K}$ cannot be
\textquotedblleft embedded\textquotedblright\ into fields in the way
$\mathbb{Z}$ is embedded into $\mathbb{Q}$. (For instance, $\mathbb{Z}%
/4\mathbb{Z}$ cannot be embedded into a field.)

Nevertheless, there \textbf{are} reasonably fast algorithms for computing
determinants over any commutative ring; see \cite[\S 2]{Rote}.
\end{remark}

\subsection{The Cauchy determinant}

Now, we can state another classical formula for a determinant: the
\textit{Cauchy determinant}. In one of its many forms, it says the following:

\begin{exercise}
\label{exe.cauchy-det}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$. Let $y_{1},y_{2},\ldots,y_{n}$ be $n$ elements
of $\mathbb{K}$. Assume that $x_{i}+y_{j}$ is invertible in $\mathbb{K}$ for
every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,
prove that%
\[
\det\left(  \left(  \dfrac{1}{x_{i}+y_{j}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  =\frac{\prod_{1\leq i<j\leq n}\left(  \left(  x_{i}%
-x_{j}\right)  \left(  y_{i}-y_{j}\right)  \right)  }{\prod_{\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}\left(  x_{i}+y_{j}\right)
}.
\]

\end{exercise}

There is a different version of the Cauchy determinant floating around in
literature; it differs from Exercise \ref{exe.cauchy-det} in that each
\textquotedblleft$x_{i}+y_{j}$\textquotedblright\ is replaced by
\textquotedblleft$x_{i}-y_{j}$\textquotedblright, and in that
\textquotedblleft$y_{i}-y_{j}$\textquotedblright\ is replaced by
\textquotedblleft$y_{j}-y_{i}$\textquotedblright. Of course, this version is
nothing else than the result of applying Exercise \ref{exe.cauchy-det} to
$-y_{1},-y_{2},\ldots,-y_{n}$ instead of $y_{1},y_{2},\ldots,y_{n}$.

\begin{exercise}
\label{exe.cauchy-det-lem}Let $n$ be a positive integer. Let $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix
such that $a_{n,n}$ is invertible (in $\mathbb{K}$). Prove that
\begin{equation}
\det\left(  \left(  a_{i,j}a_{n,n}-a_{i,n}a_{n,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right)  =a_{n,n}^{n-2}\cdot\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\label{eq.exe.cauchy-det-lem}%
\end{equation}

\end{exercise}

Exercise \ref{exe.cauchy-det-lem} is known as the \textit{Chi\'o pivotal
condensation theorem}.

\begin{remark}
Exercise \ref{exe.cauchy-det-lem} gives a way to reduce the computation of an
$n\times n$-determinant (the one on the right hand side of
(\ref{eq.exe.cauchy-det-lem})) to the computation of an $\left(  n-1\right)
\times\left(  n-1\right)  $-determinant (the one on the left hand side),
provided that $a_{n,n}$ is invertible. If this reminds you of Remark
\ref{rmk.laplace.pre.gauss-alg}, you are thinking right...
\end{remark}

\begin{remark}
Exercise \ref{exe.cauchy-det-lem} holds even without the assumption that
$a_{n,n}$ be invertible, as long as we assume (instead) that $n\geq2$. (If we
don't assume that $n\geq2$, then the $a_{n,n}^{n-2}$ on the right hand side of
(\ref{eq.exe.cauchy-det-lem}) will not be defined for non-invertible $a_{n,n}%
$.) Proving this is beyond these notes, though.
\end{remark}

The next exercises are just diversions; they have nothing to do with
invertibility or with the Cauchy determinant.

\begin{exercise}
\label{exe.det.schur-lem}Let $n\in\mathbb{N}$. Let $\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $b_{1}$,
$b_{2}$, $...$, $b_{n}$ be $n$ elements of $\mathbb{K}$. Prove that%
\[
\sum\limits_{k=1}^{n}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(  b_{1}+b_{2}+\cdots
+b_{n}\right)  \det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  ,
\]
where $\delta_{j,k}$ means the nonnegative integer $%
\begin{cases}
1, & \text{if }j=k;\\
0, & \text{if }j\neq k
\end{cases}
$. Equivalently (in more reader-friendly terms): Prove that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1}b_{1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1}b_{2} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1}b_{n} & a_{n,2} & \cdots & a_{n,n}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2}b_{1} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2}b_{2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2}b_{n} & \cdots & a_{n,n}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\cdots+\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,n}b_{1}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}b_{2}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}b_{n}%
\end{array}
\right) \\
&  =\left(  b_{1}+b_{2}+\cdots+b_{n}\right)  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}%
\end{array}
\right)  .
\end{align*}

\end{exercise}

\begin{exercise}
\label{exe.det.a1a2anx}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ elements of $\mathbb{K}$. Let $x\in\mathbb{K}$. Prove that%
\[
\det\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]

\end{exercise}

\begin{exercise}
\label{exe.det.2diags}Let $n>1$ be an integer. Let $a_{1},a_{2},\ldots,a_{n}$
be $n$ elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n}$ be $n$
elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix%
\begin{align*}
&  \left(
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\\
&  =\left(
\begin{array}
[c]{cccccc}%
a_{1} & 0 & 0 & \cdots & 0 & b_{n}\\
b_{1} & a_{2} & 0 & \cdots & 0 & 0\\
0 & b_{2} & a_{3} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{n-1} & 0\\
0 & 0 & 0 & \cdots & b_{n-1} & a_{n}%
\end{array}
\right)  .
\end{align*}
Prove that%
\[
\det A=a_{1}a_{2}\cdots a_{n}+\left(  -1\right)  ^{n-1}b_{1}b_{2}\cdots
b_{n}.
\]

\end{exercise}

\begin{remark}
If we replace \textquotedblleft$i\equiv j+1\operatorname{mod}n$%
\textquotedblright\ by \textquotedblleft$i\equiv j+2\operatorname{mod}%
n$\textquotedblright\ in Exercise \ref{exe.det.2diags}, then the pattern can
break. For instance, for $n=4$ we have%
\[
\det\left(
\begin{array}
[c]{cccc}%
a_{1} & 0 & b_{3} & 0\\
0 & a_{2} & 0 & b_{4}\\
b_{1} & 0 & a_{3} & 0\\
0 & b_{2} & 0 & a_{4}%
\end{array}
\right)  =\left(  a_{2}a_{4}-b_{2}b_{4}\right)  \left(  a_{1}a_{3}-b_{1}%
b_{3}\right)  ,
\]
which is not of the form $a_{1}a_{2}a_{3}a_{4}\pm b_{1}b_{2}b_{3}b_{4}$
anymore. Can you guess for which $d\in\left\{  1,2,\ldots,n-1\right\}  $ we
can replace \textquotedblleft$i\equiv j+1\operatorname{mod}n$%
\textquotedblright\ by \textquotedblleft$i\equiv j+d\operatorname{mod}%
n$\textquotedblright\ in Exercise \ref{exe.det.2diags} and still get a formula
of the form $\det A=a_{1}a_{2}\cdots a_{n}\pm b_{1}b_{2}\cdots b_{n}$ ?
\end{remark}

\subsection{Laplace expansion}

We shall now state Laplace expansion in full. We begin with an example:

\begin{example}
\label{exa.laplace.3x3}Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq3,\ 1\leq
j\leq3}$ be a $3\times3$-matrix. From (\ref{eq.det.small.3x3}), we obtain%
\begin{equation}
\det A=a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}%
a_{3,2}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3}a_{2,2}a_{3,1}.
\label{eq.exa.laplace.3x3.1}%
\end{equation}
On the right hand side of this equality, we have six terms, each of which
contains either $a_{2,1}$ or $a_{2,2}$ or $a_{2,3}$. Let us combine the two
terms containing $a_{2,1}$ and factor out $a_{2,1}$, then do the same with the
two terms containing $a_{2,2}$, and with the two terms containing $a_{2,3}$.
As a result, (\ref{eq.exa.laplace.3x3.1}) becomes%
\begin{align}
&  \det A\nonumber\\
&  =a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}%
-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3}a_{2,2}a_{3,1}\nonumber\\
&  =a_{2,1}\underbrace{\left(  a_{1,3}a_{3,2}-a_{1,2}a_{3,3}\right)  }%
_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,3} & a_{1,2}\\
a_{3,3} & a_{3,2}%
\end{array}
\right)  }+a_{2,2}\underbrace{\left(  a_{1,1}a_{3,3}-a_{1,3}a_{3,1}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  }+a_{2,3}\underbrace{\left(  a_{1,2}a_{3,1}-a_{1,1}a_{3,2}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,1}\\
a_{3,2} & a_{3,1}%
\end{array}
\right)  }\nonumber\\
&  =a_{2,1}\det\left(
\begin{array}
[c]{cc}%
a_{1,3} & a_{1,2}\\
a_{3,3} & a_{3,2}%
\end{array}
\right)  +a_{2,2}\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  +a_{2,3}\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,1}\\
a_{3,2} & a_{3,1}%
\end{array}
\right)  . \label{eq.exa.laplace.3x3.2}%
\end{align}
This is a nice formula with an obvious pattern: The right hand side can be
rewritten as $\sum_{q=1}^{3}a_{2,q}\det\left(  B_{2,q}\right)  $, where
$B_{2,q}=\left(
\begin{array}
[c]{cc}%
a_{1,q+2} & a_{1,q+1}\\
a_{3,q+2} & a_{3,q+1}%
\end{array}
\right)  $ (where we set $a_{i,4}=a_{i,1}$ and $a_{i,5}=a_{i,2}$ for all
$i\in\left\{  1,2,3\right\}  $). Notice the cyclic symmetry (with respect to
the index of the column) in this formula! Unfortunately, in this exact form,
the formula does not generalize to bigger matrices (or even to smaller: the
analogue for a $2\times2$-matrix would be $\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  =-a_{2,1}a_{1,2}+a_{2,2}a_{1,1}$, which has a minus sign unlike
$\sum_{q=1}^{3}a_{2,q}\det\left(  B_{2,q}\right)  $).

However, we can slightly modify our formula, sacrificing the cyclic symmetry
but making it generalize. Namely, let us rewrite $a_{1,3}a_{3,2}%
-a_{1,2}a_{3,3}$ as $-\left(  a_{1,2}a_{3,3}-a_{1,3}a_{3,2}\right)  $ and
$a_{1,2}a_{3,1}-a_{1,1}a_{3,2}$ as $\left(  a_{1,1}a_{3,2}-a_{1,2}%
a_{3,1}\right)  $; we thus obtain
\begin{align}
&  \det A\nonumber\\
&  =a_{2,1}\underbrace{\left(  a_{1,3}a_{3,2}-a_{1,2}a_{3,3}\right)
}_{=-\left(  a_{1,2}a_{3,3}-a_{1,3}a_{3,2}\right)  }+a_{2,2}\left(
a_{1,1}a_{3,3}-a_{1,3}a_{3,1}\right)  +a_{2,3}\underbrace{\left(
a_{1,2}a_{3,1}-a_{1,1}a_{3,2}\right)  }_{=-\left(  a_{1,1}a_{3,2}%
-a_{1,2}a_{3,1}\right)  }\nonumber\\
&  =-a_{2,1}\underbrace{\left(  a_{1,2}a_{3,3}-a_{1,3}a_{3,2}\right)  }%
_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,3}\\
a_{3,2} & a_{3,3}%
\end{array}
\right)  }+a_{2,2}\underbrace{\left(  a_{1,1}a_{3,3}-a_{1,3}a_{3,1}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  }-a_{2,3}\underbrace{\left(  a_{1,1}a_{3,2}-a_{1,2}a_{3,1}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{3,1} & a_{3,2}%
\end{array}
\right)  }\nonumber\\
&  =-a_{2,1}\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,3}\\
a_{3,2} & a_{3,3}%
\end{array}
\right)  +a_{2,2}\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  -a_{2,3}\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{3,1} & a_{3,2}%
\end{array}
\right) \nonumber\\
&  =\sum_{q=1}^{3}\left(  -1\right)  ^{q}a_{2,q}\det\left(  C_{2,q}\right)  ,
\label{eq.exa.laplace.3x3.3}%
\end{align}
where $C_{2,q}$ means the matrix obtained from $A$ by crossing out the $2$-nd
row and the $q$-th column. This formula (unlike (\ref{eq.exa.laplace.3x3.2}))
involves powers of $-1$, but it can be generalized.

How? First, we notice that we can find a similar formula by factoring out
$a_{1,1},a_{1,2},a_{1,3}$ (instead of $a_{2,1},a_{2,2},a_{2,3}$); this formula
will be%
\[
\det A=\sum_{q=1}^{3}\left(  -1\right)  ^{q-1}a_{1,q}\det\left(
C_{1,q}\right)  ,
\]
where $C_{1,q}$ means the matrix obtained from $A$ by crossing out the $1$-st
row and the $q$-th column. This formula, and (\ref{eq.exa.laplace.3x3.3}),
suggest the following generalization: If $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ is an $n\times n$-matrix, and if $p\in\left\{
1,2,\ldots,n\right\}  $, then%
\begin{equation}
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(
C_{p,q}\right)  , \label{eq.exa.laplace.3x3.4}%
\end{equation}
where $C_{p,q}$ means the matrix obtained from $A$ by crossing out the $p$-th
row and the $q$-th column. (The only part of this formula which is not easy to
guess is $\left(  -1\right)  ^{p+q}$; you might need to compute several
particular cases to guess this pattern. Of course, you could also have guessed
$\left(  -1\right)  ^{p-q}$ or $\left(  -1\right)  ^{q-p}$ instead, because
$\left(  -1\right)  ^{p+q}=\left(  -1\right)  ^{p-q}=\left(  -1\right)
^{q-p}$.)

The formula (\ref{eq.exa.laplace.3x3.4}) is what is usually called the Laplace
expansion with respect to the $p$-th row. We will prove it below (Theorem
\ref{thm.laplace.gen} \textbf{(a)}), and we will also prove an analogous
\textquotedblleft Laplace expansion with respect to the $q$-th
column\textquotedblright\ (Theorem \ref{thm.laplace.gen} \textbf{(b)}).
\end{example}

Let us first define a notation:

\begin{definition}
\label{def.submatrix}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be an $n\times m$-matrix.
Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{  1,2,\ldots
,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some elements of $\left\{
1,2,\ldots,m\right\}  $. Then, we define $\operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ to be the $u\times v$-matrix
$\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}$.

When $i_{1}<i_{2}<\cdots<i_{u}$ and $j_{1}<j_{2}<\cdots<j_{v}$, the matrix
$\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A$ can be obtained from $A$ by crossing out all rows other than
the $i_{1}$-th, the $i_{2}$-th, etc., the $i_{u}$-th row and crossing out all
columns other than the $j_{1}$-th, the $j_{2}$-th, etc., the $j_{v}$-th
column. Thus, in this case, $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ is called a \textit{submatrix} of
$A$.
\end{definition}

For example, if $n=3$, $m=4$ and $A=\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
e & f & g & h\\
i & j & k & \ell
\end{array}
\right)  $, then $\operatorname*{sub}\nolimits_{1,3}^{2,3,4}A=\left(
\begin{array}
[c]{ccc}%
b & c & d\\
j & k & \ell
\end{array}
\right)  $ (this is a submatrix of $A$) and $\operatorname*{sub}%
\nolimits_{2,3}^{3,1,1}A=\left(
\begin{array}
[c]{ccc}%
g & e & e\\
k & i & i
\end{array}
\right)  $ (this is not, in general, a submatrix of $A$).

The following properties follow trivially from the definitions:

\begin{proposition}
\label{prop.submatrix.easy}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix. Recall the notations introduced in Definition
\ref{def.rowscols}.

\textbf{(a)} We have $\operatorname*{sub}\nolimits_{1,2,\ldots,n}%
^{1,2,\ldots,m}A=A$.

\textbf{(b)} If $i_{1},i_{2},\ldots,i_{u}$ are some elements of $\left\{
1,2,\ldots,n\right\}  $, then%
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots,m}A.
\]


\textbf{(c)} If $j_{1},j_{2},\ldots,j_{v}$ are some elements of $\left\{
1,2,\ldots,m\right\}  $, then%
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}}A.
\]


\textbf{(d)} Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some elements of
$\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)
=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]


\textbf{(e)} Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some elements of
$\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}A\right)  ^{T}=\operatorname*{sub}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}^{i_{1},i_{2},\ldots,i_{u}}\left(  A^{T}\right)  .
\]

\end{proposition}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.submatrix.easy}.]Write the matrix $A$ in the
form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.

\textbf{(a)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{sub}\nolimits_{1,2,\ldots
,n}^{1,2,\ldots,m}A$ yields%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{1,2,\ldots,m}A  &  =\left(
a_{x,y}\right)  _{1\leq x\leq n,\ 1\leq y\leq m}=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right) \\
&  =A.
\end{align*}
This proves Proposition \ref{prop.submatrix.easy} \textbf{(a)}.

\textbf{(b)} Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $. The definition of $\operatorname*{rows}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}A$ yields $\operatorname*{rows}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(  a_{i_{x},j}\right)  _{1\leq
x\leq u,\ 1\leq j\leq m}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$). On the other hand, we have $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$. Hence, the definition of
$\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots,m}A$
yields
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots
,m}A=\left(  a_{i_{x},y}\right)  _{1\leq x\leq u,\ 1\leq y\leq m}=\left(
a_{i_{x},j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}%
\]
(here, we renamed the index $\left(  x,y\right)  $ as $\left(  x,j\right)  $).
Comparing this with $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}A=\left(  a_{i_{x},j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}$, we obtain
$\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots,m}A$. This proves Proposition
\ref{prop.submatrix.easy} \textbf{(b)}.

\textbf{(c)} Let $j_{1},j_{2},\ldots,j_{v}$ be some elements of $\left\{
1,2,\ldots,m\right\}  $. The definition of $\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A$ yields $\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i,j_{y}}\right)  _{1\leq
i\leq n,\ 1\leq y\leq v}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$). On the other hand, we have $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$. Hence, the definition of
$\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}}A$
yields%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}%
}A=\left(  a_{x,j_{y}}\right)  _{1\leq x\leq n,\ 1\leq y\leq v}=\left(
a_{i,j_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}%
\]
(here, we renamed the index $\left(  x,y\right)  $ as $\left(  i,y\right)  $).
Comparing this with $\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}%
}A=\left(  a_{i,j_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}$, we obtain
$\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}}A$. This proves Proposition
\ref{prop.submatrix.easy} \textbf{(c)}.

\textbf{(d)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ yields $\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}$.

On the other hand, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$. Thus, the definition of $\operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}A$ yields
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i,j_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}=\left(  a_{i,j_{j}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq v}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  i,j\right)
$)\ \ \ \ \footnote{The notation $j_{j}$ might look fishy, since it uses the
letter \textquotedblleft$j$\textquotedblright\ in two unrelated meanings: Its
meaning in \textquotedblleft$j_{1},j_{2},\ldots,j_{v}$\textquotedblright\ has
nothing to do with its meaning with \textquotedblleft$1\leq j\leq
v$\textquotedblright. However, it is easy to distinguish between these two
kinds of \textquotedblleft$j$\textquotedblright, because the former always
appears with a subscript, whereas the latter never does.}. Hence, the
definition of $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}\left(
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)  $ yields%
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}\left(
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)  =\left(
a_{i_{x},j_{j}}\right)  _{1\leq x\leq n,\ 1\leq j\leq v}=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq n,\ 1\leq y\leq v}%
\]
(here, we renamed the index $\left(  x,j\right)  $ as $\left(  x,y\right)  $).
Compared with $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}%
^{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq
u,\ 1\leq y\leq v}$, this yields%
\begin{equation}
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)  .
\label{pf.prop.submatrix.easy.d.1}%
\end{equation}


Furthermore, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A$ yields
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(  a_{i_{x}%
,j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}=\left(  a_{i_{i},j}\right)
_{1\leq i\leq u,\ 1\leq j\leq m}%
\]
\footnote{The double use of the letter \textquotedblleft$i$\textquotedblright%
\ in \textquotedblleft$i_{i}$\textquotedblright\ might appear confusing. The
first \textquotedblleft$i$\textquotedblright\ is part of the notation $i_{k}$
for $k\in\left\{  1,2,\ldots,u\right\}  $; the second \textquotedblleft%
$i$\textquotedblright\ is an element of $\left\{  1,2,\ldots,u\right\}  $.
These two \textquotedblleft$i$\textquotedblright s are unrelated to each
other. I hope the reader can easily tell them apart by the fact that the
\textquotedblleft$i$\textquotedblright\ that is part of the notation $i_{k}$
always appears with a subscript, whereas the second \textquotedblleft%
$i$\textquotedblright\ never does.} (here, we renamed the index $\left(
x,j\right)  $ as $\left(  i,j\right)  $). Hence, the definition of
$\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  $ yields%
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  =\left(
a_{i_{i},j_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq n,\ 1\leq y\leq v}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  x,y\right)  $).
Compared with $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}%
^{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq
u,\ 1\leq y\leq v}$, this yields%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}%
}\left(  \operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]
Combining this with (\ref{pf.prop.submatrix.easy.d.1}), we obtain%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)
=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]
This proves Proposition \ref{prop.submatrix.easy} \textbf{(d)}.

\textbf{(e)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ yields $\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}=\left(
a_{i_{i},j_{j}}\right)  _{1\leq i\leq u,\ 1\leq j\leq v}$ (here, we have
renamed the index $\left(  x,y\right)  $ as $\left(  i,j\right)  $).
Therefore, the definition of $\left(  \operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A\right)  ^{T}$ yields $\left(
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A\right)  ^{T}=\left(  a_{i_{j},j_{i}}\right)  _{1\leq i\leq
v,\ 1\leq j\leq u}=\left(  a_{i_{y},j_{x}}\right)  _{1\leq x\leq v,\ 1\leq
y\leq u}$ (here, we have renamed the index $\left(  i,j\right)  $ as $\left(
x,y\right)  $).

On the other hand, the definition of $A^{T}$ yields $A^{T}=\left(
a_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. Hence, the definition of
$\operatorname*{sub}\nolimits_{j_{1},j_{2},\ldots,j_{v}}^{i_{1},i_{2}%
,\ldots,i_{u}}\left(  A^{T}\right)  $ yields $\operatorname*{sub}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}^{i_{1},i_{2},\ldots,i_{u}}\left(
A^{T}\right)  =\left(  a_{i_{y},j_{x}}\right)  _{1\leq x\leq v,\ 1\leq y\leq
u}$. Comparing this with $\left(  \operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A\right)  ^{T}=\left(
a_{i_{y},j_{x}}\right)  _{1\leq x\leq v,\ 1\leq y\leq u}$, we obtain%
\[
\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}A\right)  ^{T}=\operatorname*{sub}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}^{i_{1},i_{2},\ldots,i_{u}}\left(  A^{T}\right)  .
\]
This proves Proposition \ref{prop.submatrix.easy} \textbf{(e)}.
\end{proof}
\end{verlong}

\begin{definition}
Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$ objects. Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\left(  a_{1},a_{2}%
,\ldots,\widehat{a_{i}},\ldots,a_{n}\right)  $ shall mean the list $\left(
a_{1},a_{2},\ldots,a_{i-1},a_{i+1},a_{i+2},\ldots,a_{n}\right)  $ (that is,
the list $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ with its $i$-th entry
removed). (Thus, the \textquotedblleft hat\textquotedblright\ over the $a_{i}$
means that this $a_{i}$ is being omitted from the list.)

For example, $\left(  1^{2},2^{2},\ldots,\widehat{5}^{2},\ldots,8^{2}\right)
=\left(  1^{2},2^{2},3^{2},4^{2},6^{2},7^{2},8^{2}\right)  $.
\end{definition}

\begin{definition}
\label{def.submatrix.minor}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix. For every $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,m\right\}  $, we let $A_{\sim i,\sim j}$ be the
$\left(  n-1\right)  \times\left(  m-1\right)  $-matrix $\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{i},\ldots,n}^{1,2,\ldots,\widehat{j},\ldots
,m}A$. (Thus, $A_{\sim i,\sim j}$ is the matrix obtained from $A$ by crossing
out the $i$-th row and the $j$-th column.)

For example, if $n=m=3$ and $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $, then $A_{\sim1,\sim2}=\left(
\begin{array}
[c]{cc}%
d & f\\
g & i
\end{array}
\right)  $ and $A_{\sim3,\sim2}=\left(
\begin{array}
[c]{cc}%
a & c\\
d & f
\end{array}
\right)  $.
\end{definition}

The notation $A_{\sim i,\sim j}$ introduced in Definition
\ref{def.submatrix.minor} is not very standard; but there does not seem to be
a standard one\footnote{For example, Gill Williamson uses the notation
$A\left(  i\mid j\right)  $ in \cite[Chapter 3]{Gill}.}.

Now we can finally state Laplace expansion:

\begin{theorem}
\label{thm.laplace.gen}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.

\textbf{(a)} For every $p\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim
p,\sim q}\right)  .
\]


\textbf{(b)} For every $q\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\det A=\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim
p,\sim q}\right)  .
\]

\end{theorem}

Theorem \ref{thm.laplace.gen} \textbf{(a)} is known as the \textit{Laplace
expansion along the }$p$\textit{-th row} (or \textit{Laplace expansion with
respect to the }$p$\textit{-th row}), whereas Theorem \ref{thm.laplace.gen}
\textbf{(b)} is known as the \textit{Laplace expansion along the }%
$q$\textit{-th column} (or \textit{Laplace expansion with respect to the }%
$q$\textit{-th column}). Notice that Theorem \ref{thm.laplace.gen}
\textbf{(a)} is equivalent to the formula (\ref{eq.exa.laplace.3x3.4}),
because the $A_{\sim p,\sim q}$ in Theorem \ref{thm.laplace.gen} \textbf{(a)}
is precisely what we called $C_{p,q}$ in (\ref{eq.exa.laplace.3x3.4}).

We prepare the field for the proof of Theorem \ref{thm.laplace.gen} with a few lemmas.

\begin{vershort}
\begin{lemma}
\label{lem.laplace.gpshort}For every $n\in\mathbb{N}$, let $\left[  n\right]
$ denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$. For every $p\in\left[  n\right]  $, we define a
permutation $g_{p}\in S_{n}$ by $g_{p}=\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}$ (where we are using the notations of Definition
\ref{def.perm.cycles}).

\textbf{(a)} We have $\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)
,\ldots,g_{p}\left(  n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $ for every $p\in\left[  n\right]  $.

\textbf{(b)} We have $\left(  -1\right)  ^{g_{p}}=\left(  -1\right)  ^{n-p}$
for every $p\in\left[  n\right]  $.

\textbf{(c)} Let $p\in\left[  n\right]  $. We define a map%
\[
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}
\]
by%
\[
\left(  g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n-1\right]  \right)  .
\]
This map $g_{p}^{\prime}$ is well-defined and bijective.

\textbf{(d)} Let $p\in\left[  n\right]  $ and $q\in\left[  n\right]  $. We
define a map
\[
T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\]
by
\[
\left(  T\left(  \sigma\right)  =g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  \right)  .
\]
Then, this map $T$ is well-defined and bijective.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.laplace.gpshort}.]\textbf{(a)} This is trivial.

\textbf{(b)} Let $p\in\left[  n\right]  $. Exercise \ref{exe.perm.cycles}
\textbf{(d)} (applied to $k=p+1$ and $\left(  i_{1},i_{2},\ldots,i_{k}\right)
=\left(  p,p+1,\ldots,n\right)  $) yields%
\[
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}}=\left(
-1\right)  ^{n-\left(  p+1\right)  -1}=\left(  -1\right)  ^{n-p-2}=\left(
-1\right)  ^{n-p}.
\]
Now, $g_{p}=\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}$, so that $\left(
-1\right)  ^{g_{p}}=\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}}=\left(  -1\right)  ^{n-p}$. This proves Lemma
\ref{lem.laplace.gpshort} \textbf{(b)}.

\textbf{(c)} We have $g_{p}\left(  n\right)  =p$ (since $g_{p}%
=\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}$). Also, $g_{p}$ is injective
(since $g_{p}$ is a permutation). Therefore, for every $i\in\left[
n-1\right]  $, we have%
\begin{align*}
g_{p}\left(  i\right)   &  \neq g_{p}\left(  n\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq n\text{ (because }i\in\left[
n-1\right]  \text{) and since }g_{p}\text{ is injective}\right) \\
&  =p,
\end{align*}
so that $g_{p}\left(  i\right)  \in\left[  n\right]  \setminus\left\{
p\right\}  $. This shows that the map $g_{p}^{\prime}$ is well-defined.

To prove that $g_{p}^{\prime}$ is bijective, we can construct its inverse.
Indeed, for every $i\in\left[  n\right]  \setminus\left\{  p\right\}  $, we
have%
\[
\left(  g_{p}\right)  ^{-1}\left(  i\right)  \neq n\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\neq p=g_{p}\left(  n\right)  \right)
\]
and thus $\left(  g_{p}\right)  ^{-1}\left(  i\right)  \in\left[  n-1\right]
$. Hence, we can define a map $h:\left[  n\right]  \setminus\left\{
p\right\}  \rightarrow\left[  n-1\right]  $ by%
\[
\left(  h\left(  i\right)  =\left(  g_{p}\right)  ^{-1}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n\right]  \setminus\left\{
p\right\}  \right)  .
\]
It is straightforward to check that the maps $g_{p}^{\prime}$ and $h$ are
mutually inverse. Thus, $g_{p}^{\prime}$ is bijective. Lemma
\ref{lem.laplace.gpshort} \textbf{(c)} is thus proven.

\textbf{(d)} We have $g_{p}\left(  n\right)  =p$ (since $g_{p}%
=\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}$) and $g_{q}\left(  n\right)
=q$ (similarly). Hence, $\left(  g_{p}\right)  ^{-1}\left(  p\right)  =n$
(since $g_{p}\left(  n\right)  =p$) and $\left(  g_{q}\right)  ^{-1}\left(
q\right)  =n$ (since $g_{q}\left(  n\right)  =q$).

For every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  $, we have $\sigma\left(  n\right)  =n$ and thus
\[
\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\right)  \left(
p\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  p\right)  }_{=n}\right)  \right)  =g_{q}\left(
\underbrace{\sigma\left(  n\right)  }_{=n}\right)  =g_{q}\left(  n\right)  =q
\]
and therefore $g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in\left\{
\tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  $. Thus, the map $T$ is well-defined.

We can also define a map
\[
Q:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\]
by%
\[
\left(  Q\left(  \sigma\right)  =\left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  \right)  .
\]
The well-definedness of $Q$ can be checked similarly to how we proved the
well-definedness of $T$. It is straightforward to verify that the maps $Q$ and
$T$ are mutually inverse. Thus, $T$ is bijective. This completes the proof of
Lemma \ref{lem.laplace.gpshort} \textbf{(d)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{lemma}
\label{lem.laplace.gp}For every $n\in\mathbb{N}$, let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$. We recall that, for each $k\in\left\{  1,2,\ldots
,n-1\right\}  $, we have defined $s_{k}$ to be the permutation in $S_{n}$ that
switches $k$ with $k+1$ but leaves all other numbers unchanged.

For every $p\in\left[  n\right]  $, we define a permutation $g_{p}\in S_{n}$
by%
\[
g_{p}=s_{p}\circ s_{p+1}\circ\cdots\circ s_{n-1}.
\]
(Thus, for $n>0$ and $p=n$, we have $g_{n}=s_{n}\circ s_{n+1}\circ\cdots\circ
s_{n-1}=\left(  \text{a composition of }0\text{ permutations}\right)
=\operatorname*{id}$.)

\textbf{(a)} We have $g_{p}\left(  i\right)  =i$ for every $p\in\left[
n\right]  $ and every $i\in\left[  n\right]  $ satisfying $i<p$.

\textbf{(b)} We have $g_{p}\left(  i\right)  =i+1$ for every $p\in\left[
n\right]  $ and every $i\in\left[  n\right]  $ satisfying $p\leq i<n$.

\textbf{(c)} We have $g_{p}\left(  n\right)  =p$ for every $p\in\left[
n\right]  $.

\textbf{(d)} We have $\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)
,\ldots,g_{p}\left(  n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $ for every $p\in\left[  n\right]  $.

\textbf{(e)} We have $\left(  -1\right)  ^{g_{p}}=\left(  -1\right)  ^{n-p}$
for every $p\in\left[  n\right]  $.

\textbf{(f)} Let $p\in\left[  n\right]  $. We define a map%
\[
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}
\]
by%
\[
\left(  g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n-1\right]  \right)  .
\]
This map $g_{p}^{\prime}$ is well-defined and bijective.

\textbf{(g)} Let $p\in\left[  n\right]  $ and $q\in\left[  n\right]  $. We
define a map
\[
T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\]
by
\[
\left(  T\left(  \sigma\right)  =g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  \right)  .
\]
Then, this map $T$ is well-defined and bijective.
\end{lemma}

\begin{remark}
Let $n\in\mathbb{N}$. Let us use the notations of Lemma \ref{lem.laplace.gp}
and of Definition \ref{def.perm.cycles}. Then, $g_{p}=\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}$ (as follows from parts \textbf{(a)}, \textbf{(b)}
and \textbf{(c)} of Lemma \ref{lem.laplace.gp}). From this viewpoint, it
appears weird that I have not defined $g_{p}$ as $\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}$. The reason is merely that I wanted to avoid using
cycles (an aesthetical choice).

Parts \textbf{(a)}, \textbf{(b)} and \textbf{(c)} of Lemma
\ref{lem.laplace.gp} can be viewed as an analogue of (\ref{sol.ps2.2.4.c.aik}).
\end{remark}

\begin{proof}
[Proof of Lemma \ref{lem.laplace.gp}.]Let us first recall that if $n>0$, then
$g_{n}$ is well-defined (since $n\in\left[  n\right]  $) and satisfies
\begin{align*}
g_{n}  &  =s_{n}\circ s_{n+1}\circ\cdots\circ s_{n-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }g_{n}\right) \\
&  =\left(  \text{a composition of }0\text{ permutations}\right)
=\operatorname*{id}.
\end{align*}
Moreover, every $p\in\left[  n-1\right]  $ satisfies%
\begin{equation}
g_{p}=s_{p}\circ g_{p+1} \label{pf.lem.laplace.gp.recgp}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.gp.recgp}):} Let $p\in\left[
n-1\right]  $. Then, $p\in\left[  n-1\right]  =\left\{  1,2,\ldots
,p-1\right\}  $, so that $p+1\in\left\{  2,3,\ldots,p\right\}  \subseteq
\left\{  1,2,\ldots,p\right\}  =\left[  p\right]  $. Hence, $g_{p+1}$ is
well-defined. The definition of $g_{p+1}$ yields $g_{p+1}=s_{p+1}\circ
s_{\left(  p+1\right)  +1}\circ\cdots\circ s_{n-1}=s_{p+1}\circ s_{p+2}%
\circ\cdots\circ s_{n-1}$. Now, $p\in\left[  n-1\right]  \subseteq\left[
n\right]  $, and thus $g_{p}$ is well-defined. The definition of $g_{p}$
yields%
\[
g_{p}=s_{p}\circ s_{p+1}\circ\cdots\circ s_{n-1}=s_{p}\circ\underbrace{\left(
s_{p+1}\circ s_{p+2}\circ\cdots\circ s_{n-1}\right)  }_{=g_{p+1}}=s_{p}\circ
g_{p+1}.
\]
This proves (\ref{pf.lem.laplace.gp.recgp}).}. Moreover, every $p\in\left[
n-1\right]  $ satisfies
\begin{align}
&  s_{p}\left(  p\right)  =p+1;\label{pf.lem.laplace.gp.sp.1}\\
&  s_{p}\left(  p+1\right)  =p;\label{pf.lem.laplace.gp.sp.2}\\
&  \left(  s_{p}\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left[  n\right]  \setminus\left\{  p,p+1\right\}  \right)  .
\label{pf.lem.laplace.gp.sp.3}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.gp.sp.1}),
(\ref{pf.lem.laplace.gp.sp.2}) and (\ref{pf.lem.laplace.gp.sp.3}):} Let
$p\in\left[  n-1\right]  $. Recall that $s_{p}$ is defined as the permutation
in $S_{n}$ that switches $p$ with $p+1$ but leaves all other numbers
unchanged. Thus, the permutation $s_{p}$ switches $p$ with $p+1$. In other
words, we have $s_{p}\left(  p\right)  =p+1$ and $s_{p}\left(  p+1\right)
=p$. This proves (\ref{pf.lem.laplace.gp.sp.1}) and
(\ref{pf.lem.laplace.gp.sp.2}). Furthermore, the permutation $s_{p}$ leaves
all other numbers unchanged (where \textquotedblleft other\textquotedblright%
\ means \textquotedblleft other than $p$ and $p+1$\textquotedblright). In
other words, $s_{p}\left(  i\right)  =i$ for every $i\in\left[  n\right]
\setminus\left\{  p,p+1\right\}  $. This proves (\ref{pf.lem.laplace.gp.sp.3}%
).}

\textbf{(a)} Fix $i\in\left[  n\right]  $. Thus, $i\in\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $, so that $1\leq i\leq n$. Let us prove that
\begin{equation}
g_{n-q}\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
0,1,\ldots,n-i-1\right\}  . \label{pf.lem.laplace.gp.a.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.a.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.a.claim}) by induction over $q$:

\textit{Induction base:} If $0\in\left\{  0,1,\ldots,n-i-1\right\}  $, then
$g_{n-0}\left(  i\right)  =i$\ \ \ \ \footnote{\textit{Proof.} Assume that
$0\in\left\{  0,1,\ldots,n-i-1\right\}  $. Thus, $0\leq0\leq n-i-1$, so that
$0\leq n-i-1$ and therefore $1\leq n-\underbrace{i}_{\geq0}\leq n$. Hence,
$n\geq1>0$, and thus $g_{n}=\operatorname*{id}$ (as we know). Hence,
$\underbrace{g_{n-0}}_{=g_{n}=\operatorname*{id}}\left(  i\right)
=\operatorname*{id}\left(  i\right)  =i$, qed.}. In other words,
(\ref{pf.lem.laplace.gp.a.claim}) holds for $q=0$. Thus, the induction base is complete.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,n-i-1\right\}  $ be
positive. Assume that (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q-1$. We
need to show that (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q$.

Now, we have $Q\leq n-i-1$ (since $Q\in\left\{  0,1,\ldots,n-i-1\right\}  $)
and $Q\geq1$ (since $Q$ is positive and belongs to $\left\{  0,1,\ldots
,n-i-1\right\}  $). Hence, $n-\underbrace{Q}_{\substack{\leq
n-i-1<n-1\\\text{(since }i\geq1>0\text{)}}}>n-\left(  n-1\right)  =1$. Hence,
$n-Q\geq1$. Also, $n-\underbrace{Q}_{\geq1}\leq n-1$. Combining $n-Q\geq1$
with $n-Q\leq n-1$, we obtain $1\leq n-Q\leq n-1$ and thus $n-Q\in\left[
n-1\right]  $. Hence, (\ref{pf.lem.laplace.gp.recgp}) (applied to $p=n-Q$)
shows that $g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$.

We have $Q\leq n-i-1$, thus $n-\underbrace{Q}_{\leq n-i-1}-1\geq n-\left(
n-i-1\right)  -1=i$, so that $i\leq n-Q-1<n-Q$ and thus $i\neq n-Q$. Also,
$i<n-Q<n-Q+1$ and thus $i\neq n-Q+1$. Combining $i\neq n-Q$ with $i\neq
n-Q+1$, we obtain $i\notin\left\{  n-Q,n-Q+1\right\}  $. Combined with
$i\in\left[  n\right]  $, this yields $i\in\left[  n\right]  \setminus\left\{
n-Q,n-Q+1\right\}  $. Hence, (\ref{pf.lem.laplace.gp.sp.3}) (applied to
$p=n-Q$) shows that $s_{n-Q}\left(  i\right)  =i$.

But we assumed that (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q-1$. In
other words, we have $g_{n-\left(  Q-1\right)  }\left(  i\right)  =i$. Since
$n-\left(  Q-1\right)  =n-Q+1$, this rewrites as $g_{n-Q+1}\left(  i\right)
=i$. Now,%
\[
\underbrace{g_{n-Q}}_{=s_{n-Q}\circ g_{n-Q+1}}\left(  i\right)  =\left(
s_{n-Q}\circ g_{n-Q+1}\right)  \left(  i\right)  =s_{n-Q}\left(
\underbrace{g_{n-Q+1}\left(  i\right)  }_{=i}\right)  =s_{n-Q}\left(
i\right)  =i.
\]
In other words, (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.a.claim}) is complete.

Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.gp.a.claim}) for every $i\in\left[  n\right]  $.

Now, let $p\in\left[  n\right]  $ and $i\in\left[  n\right]  $ be such that
$i<p$. Then, $i<p$, so that $p>i$, and thus $p\geq i+1$ (since both $p$ and
$i$ are integers). Hence, $n-\underbrace{p}_{\geq i+1}\leq n-\left(
i+1\right)  =n-i-1$. Also, $p\leq n$ (since $p\in\left[  n\right]  $), so that
$n-p\geq0$. Combining this with $n-p\leq n-i-1$, we obtain $n-p\in\left\{
0,1,\ldots,n-i-1\right\}  $. Hence, we can apply
(\ref{pf.lem.laplace.gp.a.claim}) to $q=n-p$. We thus obtain $g_{n-\left(
n-p\right)  }\left(  i\right)  =i$. Since $n-\left(  n-p\right)  =p$, this
rewrites as $g_{p}\left(  i\right)  =i$. This proves Lemma
\ref{lem.laplace.gp} \textbf{(a)}.

\textbf{(b)} Fix $i\in\left[  n\right]  $ such that $i<n$. Thus, $i\in\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $, so that $i\geq1$. Combined with
$i<n$, this yields $1\leq i<n$.

We have $1\leq i<n$, so that $i\in\left\{  1,2,\ldots,n-1\right\}  $ and thus
$i+1\in\left\{  2,3,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots,n\right\}
=\left[  n\right]  $.

Let us prove that
\begin{equation}
g_{n-q}\left(  i\right)  =i+1\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
n-i,n-i+1,\ldots,n-1\right\}  . \label{pf.lem.laplace.gp.b.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.b.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.b.claim}) by induction over $q$:

\textit{Induction base:} If $n-i\in\left\{  n-i,n-i+1,\ldots,n-1\right\}  $,
then $g_{n-\left(  n-i\right)  }\left(  i\right)  =i+1$%
\ \ \ \ \footnote{\textit{Proof.} Assume that $n-i\in\left\{  n-i,n-i+1,\ldots
,n-1\right\}  $. We have $i+1\in\left[  n\right]  $. Hence, $g_{i+1}$ is
well-defined. Also, $i<i+1$. Hence, Lemma \ref{lem.laplace.gp} \textbf{(a)}
(applied to $p=i+1$) shows that $g_{i+1}\left(  i\right)  =i$.
\par
Now, $i\in\left\{  1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $. Hence,
(\ref{pf.lem.laplace.gp.recgp}) (applied to $p=i$) yields $g_{i}=s_{i}\circ
g_{i+1}$. Hence,%
\[
\underbrace{g_{i}}_{=s_{i}\circ g_{i+1}}\left(  i\right)  =\left(  s_{i}\circ
g_{i+1}\right)  \left(  i\right)  =s_{i}\left(  \underbrace{g_{i+1}\left(
i\right)  }_{=i}\right)  =s_{i}\left(  i\right)  =i+1
\]
(by (\ref{pf.lem.laplace.gp.sp.1}), applied to $p=i$). Since $n-\left(
n-i\right)  =i$, we now have $g_{n-\left(  n-i\right)  }\left(  i\right)
=g_{i}\left(  i\right)  =i+1$. Qed.}. In other words,
(\ref{pf.lem.laplace.gp.b.claim}) holds for $q=n-i$. Thus, the induction base
is complete.

\textit{Induction step:} Let $Q\in\left\{  n-i,n-i+1,\ldots,n-1\right\}  $ be
such that $Q>n-i$. Assume that (\ref{pf.lem.laplace.gp.b.claim}) holds for
$q=Q-1$. We need to show that (\ref{pf.lem.laplace.gp.b.claim}) holds for
$q=Q$.

Now, we have $Q\leq n-1$ (since $Q\in\left\{  n-i,n-i+1,\ldots,n-1\right\}
$). Hence, $n-\underbrace{Q}_{\leq n-1}\geq n-\left(  n-1\right)  =1$. Also,
$n-\underbrace{Q}_{>n-i}<n-\left(  n-i\right)  =i<n$, so that $n-Q\leq n-1$
(since both $n-Q$ and $n$ are integers). Combining $n-Q\geq1$ with $n-Q\leq
n-1$, we obtain $1\leq n-Q\leq n-1$ and thus $n-Q\in\left[  n-1\right]  $.
Hence, (\ref{pf.lem.laplace.gp.recgp}) (applied to $p=n-Q$) shows that
$g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$.

We have $Q>n-i$, thus $n-\underbrace{Q}_{>n-i}<n-\left(  n-i\right)  =i$.
Hence, $i>n-Q$. Adding $1$ to both sides of this inequality, we obtain
$i+1>n-Q+1$, so that $i+1\neq n-Q+1$. Also, $i+1>n-Q+1>n-Q$, so that
$i+1\notin n-Q$. Combining $i+1\neq n-Q$ with $i+1\neq n-Q+1$, we obtain
$i+1\notin\left\{  n-Q,n-Q+1\right\}  $. Combined with $i+1\in\left[
n\right]  $, this yields $i+1\in\left[  n\right]  \setminus\left\{
n-Q,n-Q+1\right\}  $. Hence, (\ref{pf.lem.laplace.gp.sp.3}) (applied to $n-Q$
and $i+1$ instead of $p$ and $i$) shows that $s_{n-Q}\left(  i+1\right)  =i+1$.

But we assumed that (\ref{pf.lem.laplace.gp.b.claim}) holds for $q=Q-1$. In
other words, we have $g_{n-\left(  Q-1\right)  }\left(  i\right)  =i+1$. Since
$n-\left(  Q-1\right)  =n-Q+1$, this rewrites as $g_{n-Q+1}\left(  i\right)
=i+1$. Now,%
\[
\underbrace{g_{n-Q}}_{=s_{n-Q}\circ g_{n-Q+1}}\left(  i\right)  =\left(
s_{n-Q}\circ g_{n-Q+1}\right)  \left(  i\right)  =s_{n-Q}\left(
\underbrace{g_{n-Q+1}\left(  i\right)  }_{=i+1}\right)  =s_{n-Q}\left(
i+1\right)  =i+1.
\]
In other words, (\ref{pf.lem.laplace.gp.b.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.b.claim}) is complete.

Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.gp.b.claim}) for every $i\in\left[  n\right]  $.

Now, let $p\in\left[  n\right]  $ and every $i\in\left[  n\right]  $
satisfying $p\leq i<n$. Combining $n-\underbrace{p}_{\leq i}\geq n-i$ with
$n-\underbrace{p}_{\substack{\geq1\\\text{(since }p\in\left[  n\right]
\text{)}}}\leq n-1$, we obtain $n-i\leq n-p\leq n-1$, so that $n-p\in\left\{
n-i,n-i+1,\ldots,n-1\right\}  $. Hence, we can apply
(\ref{pf.lem.laplace.gp.b.claim}) to $q=n-p$. We thus obtain $g_{n-\left(
n-p\right)  }\left(  i\right)  =i+1$. Since $n-\left(  n-p\right)  =p$, this
rewrites as $g_{p}\left(  i\right)  =i+1$. This proves Lemma
\ref{lem.laplace.gp} \textbf{(b)}.

\textbf{(c)} Let us first show that%
\begin{equation}
g_{n-q}\left(  n\right)  =n-q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
0,1,\ldots,n-1\right\}  . \label{pf.lem.laplace.gp.c.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.c.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.c.claim}) by induction over $q$:

\textit{Induction base:} If $0\in\left\{  0,1,\ldots,n-1\right\}  $, then
$g_{n-0}\left(  n\right)  =n-0$\ \ \ \ \footnote{\textit{Proof.} Assume that
$0\in\left\{  0,1,\ldots,n-1\right\}  $. Thus, $0\leq0\leq n-1$, so that
$0\leq n-1$ and therefore $1\leq n$. Hence, $n\geq1>0$, and thus
$g_{n}=\operatorname*{id}$ (as we know). Hence, $\underbrace{g_{n-0}}%
_{=g_{n}=\operatorname*{id}}\left(  n\right)  =\operatorname*{id}\left(
n\right)  =n=n-0$, qed.}. In other words, (\ref{pf.lem.laplace.gp.c.claim})
holds for $q=0$. Thus, the induction base is complete.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,n-1\right\}  $ be
positive. Assume that (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q-1$. We
need to show that (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q$.

We have $Q\leq n-1$ (since $Q\in\left\{  0,1,\ldots,n-1\right\}  $) and
$Q\geq1$ (since $Q$ is positive and belongs to $\left\{  0,1,\ldots
,n-1\right\}  $). Hence, $n-\underbrace{Q}_{\leq n-1}\geq n-\left(
n-1\right)  =1$. Also, $n-\underbrace{Q}_{\geq1}\leq n-1$. Combining
$n-Q\geq1$ with $n-Q\leq n-1$, we obtain $1\leq n-Q\leq n-1$ and thus
$n-Q\in\left[  n-1\right]  $. Hence, (\ref{pf.lem.laplace.gp.recgp}) (applied
to $p=n-Q$) shows that $g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$.

We have $n-Q\in\left[  n-1\right]  $ (since $1\leq n-Q\leq n-1$). Hence,
$s_{n-Q}\left(  n-Q+1\right)  =n-Q$ (by (\ref{pf.lem.laplace.gp.sp.2}),
applied to $p=n-Q$).

But we assumed that (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q-1$. In
other words, we have $g_{n-\left(  Q-1\right)  }\left(  n\right)  =n-\left(
Q-1\right)  $. Since $n-\left(  Q-1\right)  =n-Q+1$, this rewrites as
$g_{n-Q+1}\left(  n\right)  =n-Q+1$. Now,%
\[
\underbrace{g_{n-Q}}_{=s_{n-Q}\circ g_{n-Q+1}}\left(  n\right)  =\left(
s_{n-Q}\circ g_{n-Q+1}\right)  \left(  n\right)  =s_{n-Q}\left(
\underbrace{g_{n-Q+1}\left(  n\right)  }_{=n-Q+1}\right)  =s_{n-Q}\left(
n-Q+1\right)  =n-Q
\]


In other words, (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.c.claim}) is complete.

Now, let $p\in\left[  n\right]  $. Then, $p\in\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $, so that $n-p\in\left\{  0,1,\ldots,n-1\right\}  $.
Thus, we can apply (\ref{pf.lem.laplace.gp.c.claim}) to $q=n-p$. We thus
obtain $g_{n-\left(  n-p\right)  }\left(  n\right)  =n-\left(  n-p\right)  $.
Since $n-\left(  n-p\right)  =p$, this rewrites as $g_{p}\left(  n\right)
=p$. This proves Lemma \ref{lem.laplace.gp} \textbf{(c)}.

\textbf{(d)} Let $p\in\left[  n\right]  $. We have $g_{p}\left(  i\right)  =i$
for every $i\in\left\{  1,2,\ldots,p-1\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,p-1\right\}  $.
Thus, $i\geq1$ and $i\leq p-1$. Combining $i\geq1$ with $i\leq p-1\leq p\leq
n$ (since $p\in\left[  n\right]  $), we obtain $1\leq i\leq n$, so that
$i\in\left[  n\right]  $. Also, $i\leq p-1<p$. Hence, Lemma
\ref{lem.laplace.gp} \textbf{(a)} shows that we have $g_{p}\left(  i\right)
=i$, qed.}. In other words,%
\[
\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
p-1\right)  \right)  =\left(  1,2,\ldots,p-1\right)  .
\]


Also, we have $g_{p}\left(  i\right)  =i+1$ for every $i\in\left\{
p,p+1,\ldots,n-1\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{
p,p+1,\ldots,n-1\right\}  $. Thus, $i\geq p$ and $i\leq n-1$. Combining $i\geq
p\geq1$ (since $p\in\left[  n\right]  $) with $i\leq n-1\leq n$, we obtain
$1\leq i\leq n$, so that $i\in\left[  n\right]  $. Also, $p\leq i$ (since
$i\geq p$) and $i\leq n-1<n$, so that $p\leq i<n$. Hence, Lemma
\ref{lem.laplace.gp} \textbf{(b)} shows that we have $g_{p}\left(  i\right)
=i+1$, qed.}. In other words,%
\[
\left(  g_{p}\left(  p\right)  ,g_{p}\left(  p+1\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  =\left(  p+1,\left(  p+1\right)  +1,\ldots,\left(
n-1\right)  +1\right)  =\left(  p+1,p+2,\ldots,n\right)  .
\]


Now,%
\begin{align*}
&  \left(  \text{the concatenation of the list }\underbrace{\left(
g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
p-1\right)  \right)  }_{=\left(  1,2,\ldots,p-1\right)  }\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{with the list }\underbrace{\left(
g_{p}\left(  p\right)  ,g_{p}\left(  p+1\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  }_{=\left(  p+1,p+2,\ldots,n\right)  }\right) \\
&  =\left(  \text{the concatenation of the list }\left(  1,2,\ldots
,p-1\right)  \text{ with the list }\left(  p+1,p+2,\ldots,n\right)  \right) \\
&  =\left(  1,2,\ldots,p-1,p+1,p+2,\ldots,n\right)  .
\end{align*}
Comparing this with%
\[
\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  1,2,\ldots
,p-1,p+1,p+2,\ldots,n\right)  ,
\]
we obtain
\begin{align*}
&  \left(  1,2,\ldots,\widehat{p},\ldots,n\right) \\
&  =\left(  \text{the concatenation of the list }\underbrace{\left(
g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
p-1\right)  \right)  }_{=\left(  1,2,\ldots,p-1\right)  }\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{with the list }\underbrace{\left(
g_{p}\left(  p\right)  ,g_{p}\left(  p+1\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  }_{=\left(  p+1,p+2,\ldots,n\right)  }\right) \\
&  =\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots
,g_{p}\left(  n-1\right)  \right)  .
\end{align*}
This proves Lemma \ref{lem.laplace.gp} \textbf{(d)}.

\textbf{(e)} Let us first show that%
\begin{equation}
\left(  -1\right)  ^{g_{n-q}}=\left(  -1\right)  ^{q}%
\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{  0,1,\ldots,n-1\right\}  .
\label{pf.lem.laplace.gp.e.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.e.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.e.claim}) by induction over $q$:

\textit{Induction base:} If $0\in\left\{  0,1,\ldots,n-1\right\}  $, then
$\left(  -1\right)  ^{g_{n-0}}=\left(  -1\right)  ^{0}$%
\ \ \ \ \footnote{\textit{Proof.} Assume that $0\in\left\{  0,1,\ldots
,n-1\right\}  $. Thus, $0\leq0\leq n-1$, so that $0\leq n-1$ and therefore
$1\leq n$. Hence, $n\geq1>0$, and thus $g_{n}=\operatorname*{id}$ (as we
know). Hence, $\left(  -1\right)  ^{g_{n}}=\left(  -1\right)
^{\operatorname*{id}}=1$, so that $\left(  -1\right)  ^{g_{n-0}}=\left(
-1\right)  ^{g_{n}}=1=\left(  -1\right)  ^{0}$, qed.}. In other words,
(\ref{pf.lem.laplace.gp.e.claim}) holds for $q=0$. Thus, the induction base is complete.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,n-1\right\}  $ be
positive. Assume that (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q-1$. We
need to show that (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q$.

We have $Q\leq n-1$ (since $Q\in\left\{  0,1,\ldots,n-1\right\}  $) and
$Q\geq1$ (since $Q$ is positive and belongs to $\left\{  0,1,\ldots
,n-1\right\}  $). Hence, $n-\underbrace{Q}_{\leq n-1}\geq n-\left(
n-1\right)  =1$. Also, $n-\underbrace{Q}_{\geq1}\leq n-1$. Combining
$n-Q\geq1$ with $n-Q\leq n-1$, we obtain $1\leq n-Q\leq n-1$ and thus
$n-Q\in\left\{  1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $. Hence,
(\ref{pf.lem.laplace.gp.recgp}) (applied to $p=n-Q$) shows that $g_{n-Q}%
=s_{n-Q}\circ g_{n-Q+1}$.

Recall that $\left(  -1\right)  ^{s_{k}}=-1$ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. Applying this to $k=n-Q$, we obtain $\left(
-1\right)  ^{s_{n-Q}}=-1$.

But we assumed that (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q-1$. In
other words, we have $\left(  -1\right)  ^{g_{n-\left(  Q-1\right)  }}=\left(
-1\right)  ^{Q-1}$. Since $n-\left(  Q-1\right)  =n-Q+1$, this rewrites as
$\left(  -1\right)  ^{g_{n-Q+1}}=\left(  -1\right)  ^{Q-1}$. Now, from
$g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$, we obtain%
\begin{align*}
\left(  -1\right)  ^{g_{n-Q}}  &  =\left(  -1\right)  ^{s_{n-Q}\circ
g_{n-Q+1}}=\underbrace{\left(  -1\right)  ^{s_{n-Q}}}_{=-1}\cdot
\underbrace{\left(  -1\right)  ^{g_{n-Q+1}}}_{=\left(  -1\right)  ^{Q-1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\sigma=s_{n-Q}\text{ and }\tau=g_{n-Q+1}\right) \\
&  =\left(  -1\right)  \cdot\left(  -1\right)  ^{Q-1}=\left(  -1\right)
^{\left(  Q-1\right)  +1}=\left(  -1\right)  ^{Q}.
\end{align*}


In other words, (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.e.claim}) is complete.

Now, let $p\in\left[  n\right]  $. Then, $p\in\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $, so that $n-p\in\left\{  0,1,\ldots,n-1\right\}  $.
Thus, we can apply (\ref{pf.lem.laplace.gp.e.claim}) to $q=n-p$. We thus
obtain $\left(  -1\right)  ^{g_{n-\left(  n-p\right)  }}=\left(  -1\right)
^{n-p}$. Since $n-\left(  n-p\right)  =p$, this rewrites as $\left(
-1\right)  ^{g_{p}}=\left(  -1\right)  ^{n-p}$. This proves Lemma
\ref{lem.laplace.gp} \textbf{(e)}.

\textbf{(f)} We have $g_{p}\in S_{n}$. In other words, $g_{p}$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of
all permutations of $\left\{  1,2,\ldots,n\right\}  $). In other words,
$g_{p}$ is a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Thus, the map $g_{p}$ is injective and surjective.

For every $i\in\left[  n-1\right]  $, we have $g_{p}\left(  i\right)
\in\left[  n\right]  \setminus\left\{  p\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left[  n-1\right]  $. Then,
$i\in\left[  n-1\right]  =\left\{  1,2,\ldots,n-1\right\}  $, so that $i\leq
n-1<n$ and thus $i\neq n$.
\par
Also, $i\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $. Hence, $g_{p}\left(  i\right)  $ is well-defined. Moreover,
$i\in\left\{  1,2,\ldots,n-1\right\}  $, so that $i\geq1$.
\par
The map $g_{p}$ is injective. Since $i\neq n$, we therefore have $g_{p}\left(
i\right)  \neq g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}).
\par
Combining $g_{p}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $ with $g_{p}\left(  i\right)  \neq p$, we obtain $g_{p}\left(
i\right)  \in\left[  n\right]  \setminus\left\{  p\right\}  $, qed.}. Thus,
the map $g_{p}^{\prime}$ is well-defined. It remains to show that this map
$g_{p}^{\prime}$ is bijective.

For every $j\in\left[  n\right]  \setminus\left\{  p\right\}  $, we have
$\left(  g_{p}\right)  ^{-1}\left(  j\right)  \in\left[  n-1\right]
$\ \ \ \ \footnote{\textit{Proof.} Let $j\in\left[  n\right]  \setminus
\left\{  p\right\}  $. Thus, $j\in\left[  n\right]  $ and $j\neq p$.
\par
Let $i=\left(  g_{p}\right)  ^{-1}\left(  j\right)  $. (This is clearly
well-defined, since $j\in\left[  n\right]  $.) Now, $g_{p}\left(  i\right)
=j$ (since $i=\left(  g_{p}\right)  ^{-1}\left(  j\right)  $). If we had
$i=n$, then we would have $g_{p}\left(  \underbrace{i}_{=n}\right)
=g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp} \textbf{(c)}),
which would contradict $g_{p}\left(  i\right)  =j\neq p$. Hence, we cannot
have $i=n$. We thus have $i\neq n$.
\par
But $i=\left(  g_{p}\right)  ^{-1}\left(  j\right)  \in\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $. Combining this with $i\neq n$, we obtain
$i\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{
1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $. Thus, $\left(  g_{p}\right)
^{-1}\left(  j\right)  =i\in\left[  n-1\right]  $, qed.}. Therefore, we can
define a map $h:\left[  n\right]  \setminus\left\{  p\right\}  \rightarrow
\left[  n-1\right]  $ by%
\[
\left(  h\left(  j\right)  =\left(  g_{p}\right)  ^{-1}\left(  j\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left[  n\right]  \setminus\left\{
p\right\}  \right)  .
\]
Consider this map $h$.

We have $h\circ g_{p}^{\prime}=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} For every $i\in\left[  n-1\right]  $, we
have%
\begin{align*}
\left(  h\circ g_{p}^{\prime}\right)  \left(  i\right)   &  =h\left(
\underbrace{g_{p}^{\prime}\left(  i\right)  }_{\substack{=g_{p}\left(
i\right)  \\\text{(by the definition of }g_{p}^{\prime}\text{)}}}\right)
=h\left(  g_{p}\left(  i\right)  \right)  =\left(  g_{p}\right)  ^{-1}\left(
g_{p}\left(  i\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }h\right) \\
&  =i=\operatorname*{id}\left(  i\right)  .
\end{align*}
Thus, $h\circ g_{p}^{\prime}=\operatorname*{id}$, qed.} and $g_{p}^{\prime
}\circ h=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} For every
$j\in\left[  n\right]  \setminus\left\{  p\right\}  $, we have%
\begin{align*}
\left(  g_{p}^{\prime}\circ h\right)  \left(  j\right)   &  =g_{p}^{\prime
}\left(  \underbrace{h\left(  j\right)  }_{\substack{=\left(  g_{p}\right)
^{-1}\left(  j\right)  \\\text{(by the definition of }h\text{)}}}\right)
=g_{p}^{\prime}\left(  \left(  g_{p}\right)  ^{-1}\left(  j\right)  \right)
=g_{p}\left(  \left(  g_{p}\right)  ^{-1}\left(  j\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }g_{p}^{\prime}\right)
\\
&  =j=\operatorname*{id}\left(  j\right)  .
\end{align*}
Thus, $g_{p}^{\prime}\circ h=\operatorname*{id}$, qed.}. Hence, the maps $h$
and $g_{p}^{\prime}$ are mutually inverse. Thus, the map $g_{p}^{\prime}$ is
invertible. In other words, the map $g_{p}^{\prime}$ is bijective. This
completes the proof of Lemma \ref{lem.laplace.gp} \textbf{(f)}.

\textbf{(g)} For every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $, we have $g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in\left\{  \tau\in S_{n}%
\ \mid\ \tau\left(  n\right)  =n\right\}  $. Thus, $\sigma$ is an element
$\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words,
$\sigma$ is an element of $S_{n}$ and satisfies $\sigma\left(  n\right)  =n$.
\par
We have $g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}). Thus, $\left(  g_{p}\right)  ^{-1}\left(  p\right)  =n$.
Moreover, $g_{q}\left(  n\right)  =q$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}, applied to $q$ instead of $p$). Now,%
\[
\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\right)  \left(
p\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  p\right)  }_{=n}\right)  \right)  =g_{q}\left(
\underbrace{\sigma\left(  n\right)  }_{=n}\right)  =g_{q}\left(  n\right)
=q.
\]
\par
Now, we know that $g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in S_{n}$
(since $g_{q}$, $\sigma$ and $\left(  g_{p}\right)  ^{-1}$ all belong to
$S_{n}$) and satisfies $\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\right)  \left(  p\right)  =q$. In other words, $g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}$ is an element $\tau$ of $S_{n}$ satisfying
$\tau\left(  p\right)  =q$. In other words,%
\[
g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  ,
\]
qed.}. Thus, the map $T$ is well-defined.

Furthermore, for every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $, we have $\left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in\left\{  \tau\in S_{n}%
\ \mid\ \tau\left(  p\right)  =q\right\}  $. Thus, $\sigma$ is an element
$\tau$ of $S_{n}$ satisfying $\tau\left(  p\right)  =q$. In other words,
$\sigma$ is an element of $S_{n}$ and satisfies $\sigma\left(  p\right)  =q$.
\par
We have $g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}). Moreover, $g_{q}\left(  n\right)  =q$ (by Lemma
\ref{lem.laplace.gp} \textbf{(c)}, applied to $q$ instead of $p$), and thus
$\left(  g_{q}\right)  ^{-1}\left(  q\right)  =n$. Now,%
\[
\left(  \left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\right)  \left(
n\right)  =\left(  g_{q}\right)  ^{-1}\left(  \sigma\left(  \underbrace{g_{p}%
\left(  n\right)  }_{=p}\right)  \right)  =\left(  g_{q}\right)  ^{-1}\left(
\underbrace{\sigma\left(  p\right)  }_{=q}\right)  =\left(  g_{q}\right)
^{-1}\left(  q\right)  =n.
\]
\par
Now, we know that $\left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\in S_{n}$
(since $\left(  g_{q}\right)  ^{-1}$, $\sigma$ and $g_{p}$ all belong to
$S_{n}$) and satisfies $\left(  \left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\right)  \left(  n\right)  =n$. In other words, $\left(  g_{q}\right)
^{-1}\circ\sigma\circ g_{p}$ is an element $\tau$ of $S_{n}$ satisfying
$\tau\left(  p\right)  =n$. In other words,%
\[
\left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  ,
\]
qed.}. Hence, we can define a map
\[
Q:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\]
by%
\[
\left(  Q\left(  \sigma\right)  =\left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  \right)  .
\]
Consider this map $Q$.

We have $T\circ Q=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Every
$\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  $
satisfies%
\begin{align*}
\left(  T\circ Q\right)  \left(  \sigma\right)   &  =T\left(
\underbrace{Q\left(  \sigma\right)  }_{\substack{=\left(  g_{q}\right)
^{-1}\circ\sigma\circ g_{p}\\\text{(by the definition of }Q\text{)}}}\right)
=T\left(  \left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\right) \\
&  =\underbrace{g_{q}\circ\left(  g_{q}\right)  ^{-1}}_{=\operatorname*{id}%
}\circ\sigma\circ\underbrace{g_{p}\circ\left(  g_{p}\right)  ^{-1}%
}_{=\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}T\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
In other words, $T\circ Q=\operatorname*{id}$, qed.} and $Q\circ
T=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Every $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $ satisfies%
\begin{align*}
\left(  Q\circ T\right)  \left(  \sigma\right)   &  =Q\left(
\underbrace{T\left(  \sigma\right)  }_{\substack{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}\\\text{(by the definition of }T\text{)}}}\right)
=Q\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\right) \\
&  =\underbrace{\left(  g_{q}\right)  ^{-1}\circ\left(  g_{q}\right)
}_{=\operatorname*{id}}\circ\sigma\circ\underbrace{\left(  g_{p}\right)
^{-1}\circ g_{p}}_{=\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }Q\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
In other words, $Q\circ T=\operatorname*{id}$, qed.}. Hence, the maps $T$ and
$Q$ are mutually inverse. Thus, the map $T$ is invertible. In other words, the
map $T$ is bijective. This completes the proof of Lemma \ref{lem.laplace.gp}
\textbf{(g)}.
\end{proof}
\end{verlong}

Our next step towards the proof of Theorem \ref{thm.laplace.gen} is the
following lemma:

\begin{lemma}
\label{lem.laplace.Apq}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $p\in\left\{
1,2,\ldots,n\right\}  $ and $q\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}}\left(
-1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq p}}a_{i,\sigma\left(  i\right)  }=\left(  -1\right)  ^{p+q}%
\det\left(  A_{\sim p,\sim q}\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.laplace.Apq}.]Let us use all notations introduced in
Lemma \ref{lem.laplace.gpshort}.

We have $p\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence,
$g_{p}$ is well-defined. Similarly, $g_{q}$ is well-defined. We have%
\begin{equation}
\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p},\ldots,n\right)
\label{pf.lem.laplace.Apq.short.indices1}%
\end{equation}
(by Lemma \ref{lem.laplace.gpshort} \textbf{(a)}) and
\begin{equation}
\left(  g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{q},\ldots,n\right)
\label{pf.lem.laplace.Apq.short.indices2}%
\end{equation}
(by Lemma \ref{lem.laplace.gpshort} \textbf{(a)}, applied to $q$ instead of
$p$). Now, the definition of $A_{\sim p,\sim q}$ yields%
\begin{align}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  }^{g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots
,g_{q}\left(  n-1\right)  }A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.laplace.Apq.short.indices1}) and
(\ref{pf.lem.laplace.Apq.short.indices2})}\right) \nonumber\\
&  =\left(  a_{g_{p}\left(  x\right)  ,g_{q}\left(  y\right)  }\right)
_{1\leq x\leq n-1,\ 1\leq y\leq n-1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  }^{g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots
,g_{q}\left(  n-1\right)  }A\right) \nonumber\\
&  =\left(  a_{g_{p}\left(  i\right)  ,g_{q}\left(  j\right)  }\right)
_{1\leq i\leq n-1,\ 1\leq j\leq n-1}\label{pf.lem.laplace.Apq.short.A}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right)  .\nonumber
\end{align}


Also, $\left[  n\right]  $ is nonempty (since $p\in\left[  n\right]  $), and
thus we have $n>0$.

Now, let us recall the map $T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $ defined in Lemma \ref{lem.laplace.gpshort}
\textbf{(d)}. Lemma \ref{lem.laplace.gpshort} \textbf{(d)} says that this map
$T$ is well-defined and bijective. Every $\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $ satisfies%
\begin{equation}
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\left(  -1\right)  ^{p+q}%
\cdot\left(  -1\right)  ^{\sigma} \label{pf.lem.laplace.Apq.short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.short.1}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Applying
Lemma \ref{lem.laplace.gpshort} \textbf{(b)} to $q$ instead of $p$, we obtain
$\left(  -1\right)  ^{g_{q}}=\left(  -1\right)  ^{n-q}=\left(  -1\right)
^{n+q}$ (since $n-q\equiv n+q\operatorname{mod}2$).
\par
The definition of $T\left(  \sigma\right)  $ yields $T\left(  \sigma\right)
=g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}$. Thus,%
\[
\underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\circ g_{p}=g_{q}\circ\sigma\circ\underbrace{\left(
g_{p}\right)  ^{-1}\circ g_{p}}_{=\operatorname*{id}}=g_{q}\circ\sigma,
\]
so that%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{g_{q}\circ\sigma}=\underbrace{\left(  -1\right)  ^{g_{q}}%
}_{=\left(  -1\right)  ^{n+q}}\cdot\left(  -1\right)  ^{\sigma}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
g_{q}\text{ and }\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{T\left(  \sigma\right)  }\cdot\underbrace{\left(  -1\right)
^{g_{p}}}_{\substack{=\left(  -1\right)  ^{n-p}\\\text{(by Lemma
\ref{lem.laplace.gpshort} \textbf{(b)})}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sign.prod}), applied to }T\left(  \sigma\right)  \text{ and }%
g_{p}\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p},
\end{align*}
this yields
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p}=\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
We can divide both sides of this equality by $\left(  -1\right)  ^{n-p}$
(since $\left(  -1\right)  ^{n-p}\in\left\{  1,-1\right\}  $ is clearly an
invertible integer), and thus we obtain%
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\dfrac{\left(  -1\right)
^{n+q}\cdot\left(  -1\right)  ^{\sigma}}{\left(  -1\right)  ^{n-p}%
}=\underbrace{\dfrac{\left(  -1\right)  ^{n+q}}{\left(  -1\right)  ^{n-p}}%
}_{\substack{=\left(  -1\right)  ^{\left(  n+q\right)  -\left(  n-p\right)
}=\left(  -1\right)  ^{p+q}\\\text{(since }\left(  n+q\right)  -\left(
n-p\right)  =p+q\text{)}}}\cdot\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{p+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
This proves (\ref{pf.lem.laplace.Apq.short.1}).} and%
\begin{equation}
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }
\label{pf.lem.laplace.Apq.short.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.short.2}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Let us
recall the map $g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[
n\right]  \setminus\left\{  p\right\}  $ introduced in Lemma
\ref{lem.laplace.gpshort} \textbf{(f)}. Lemma \ref{lem.laplace.gpshort}
\textbf{(c)} says that this map $g_{p}^{\prime}$ is well-defined and
bijective. In other words, $g_{p}^{\prime}$ is a bijection.
\par
Let $i\in\left[  n-1\right]  $. Then, $g_{p}^{\prime}\left(  i\right)
=g_{p}\left(  i\right)  $ (by the definition of $g_{p}^{\prime}$). Also, the
definition of $T$ yields $T\left(  \sigma\right)  =g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}$, so that%
\[
\left(  \underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\right)  \left(  \underbrace{g_{p}^{\prime}\left(
i\right)  }_{=g_{p}\left(  i\right)  }\right)  =\left(  g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}\right)  \left(  g_{p}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  g_{p}\left(  i\right)  \right)  }_{=i}\right)  \right)
=g_{q}\left(  \sigma\left(  i\right)  \right)  .
\]
\par
From $g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)  $ and $\left(
T\left(  \sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  i\right)  \right)  $, we obtain%
\begin{equation}
a_{g_{p}^{\prime}\left(  i\right)  ,\left(  T\left(  \sigma\right)  \right)
\left(  g_{p}^{\prime}\left(  i\right)  \right)  }=a_{g_{p}\left(  i\right)
,g_{q}\left(  \sigma\left(  i\right)  \right)  }.
\label{pf.lem.laplace.Apq.short.2.pf.1}%
\end{equation}
\par
Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.Apq.short.2.pf.1}) for every $i\in\left[  n-1\right]  $.
But now, we have%
\begin{align*}
&  \underbrace{\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}}_{\substack{=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
p}}\\\text{(since }\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}%
}}a_{i,\left(  T\left(  \sigma\right)  \right)  \left(  i\right)  }\\
&  =\underbrace{\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}}%
_{=\prod_{i\in\left[  n\right]  \setminus\left\{  p\right\}  }}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[
n\right]  \setminus\left\{  p\right\}  }a_{i,\left(  T\left(  \sigma\right)
\right)  \left(  i\right)  }=\underbrace{\prod_{i\in\left[  n-1\right]  }%
}_{=\prod_{i=1}^{n-1}}\underbrace{a_{g_{p}^{\prime}\left(  i\right)  ,\left(
T\left(  \sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)
\right)  }}_{\substack{=a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(
i\right)  \right)  }\\\text{(by (\ref{pf.lem.laplace.Apq.short.2.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }g_{p}^{\prime}\left(  i\right)  \text{ for
}i\text{, since}\\
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(
i\right)  \right)  }.
\end{align*}
This proves (\ref{pf.lem.laplace.Apq.short.2}).}.

Now,
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}%
}}_{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }}_{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
n\right)  =n}}}\underbrace{\left(  -1\right)  ^{T\left(  \sigma\right)  }%
}_{\substack{=\left(  -1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma
}\\\text{(by (\ref{pf.lem.laplace.Apq.short.1}))}}}\underbrace{\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }\\\text{(by (\ref{pf.lem.laplace.Apq.short.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }T\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\} \\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }\\
&  =\left(  -1\right)  ^{p+q}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  =n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }}_{\substack{=\det\left(  \left(  a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)
\\\text{(by Lemma \ref{lem.laplace.lem}, applied to }a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\text{ instead of }a_{i,j}\text{)}}}\\
&  =\left(  -1\right)  ^{p+q}\det\left(  \underbrace{\left(  a_{g_{p}\left(
i\right)  ,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}}_{\substack{=A_{\sim p,\sim q}\\\text{(by
(\ref{pf.lem.laplace.Apq.short.A}))}}}\right)  =\left(  -1\right)  ^{p+q}%
\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.laplace.Apq}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.laplace.Apq}.]Let us use all notations introduced in
Lemma \ref{lem.laplace.gp}.

We have $p\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence,
$g_{p}$ is well-defined. Similarly, $g_{q}$ is well-defined. We have%
\[
\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p},\ldots,n\right)
\]
(by Lemma \ref{lem.laplace.gp} \textbf{(d)}) and
\[
\left(  g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{q},\ldots,n\right)
\]
(by Lemma \ref{lem.laplace.gp} \textbf{(d)}, applied to $q$ instead of $p$).
Now, the definition of $A_{\sim p,\sim q}$ yields%
\begin{align}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{g_{q}\left(  1\right)
,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(  n-1\right)  }A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  =\left(  g_{q}\left(  1\right)  ,g_{q}\left(  2\right)
,\ldots,g_{q}\left(  n-1\right)  \right)  \right) \nonumber\\
&  =\operatorname*{sub}\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(
2\right)  ,\ldots,g_{p}\left(  n-1\right)  }^{g_{q}\left(  1\right)
,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(  n-1\right)  }A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  =\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)
,\ldots,g_{p}\left(  n-1\right)  \right)  \right) \nonumber\\
&  =\left(  a_{g_{p}\left(  x\right)  ,g_{q}\left(  y\right)  }\right)
_{1\leq x\leq n-1,\ 1\leq y\leq n-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\operatorname*{sub}\nolimits_{g_{p}\left(  1\right)
,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(  n-1\right)  }^{g_{q}\left(
1\right)  ,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(  n-1\right)  }A\right)
\nonumber\\
&  =\left(  a_{g_{p}\left(  i\right)  ,g_{q}\left(  j\right)  }\right)
_{1\leq i\leq n-1,\ 1\leq j\leq n-1}\label{pf.lem.laplace.Apq.A}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right)  .\nonumber
\end{align}


Also, $\left[  n\right]  $ is nonempty (since $p\in\left[  n\right]  $), and
thus we have $n>0$.

Now, let us recall the map $T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $ defined in Lemma \ref{lem.laplace.gp} \textbf{(g)}.
Lemma \ref{lem.laplace.gp} \textbf{(g)} says that this map $T$ is well-defined
and bijective. Every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $ satisfies%
\begin{equation}
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\left(  -1\right)  ^{p+q}%
\cdot\left(  -1\right)  ^{\sigma} \label{pf.lem.laplace.Apq.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.1}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Applying
Lemma \ref{lem.laplace.gp} \textbf{(e)} to $q$ instead of $p$, we obtain%
\[
\left(  -1\right)  ^{g_{q}}=\left(  -1\right)  ^{n-q}=\left(  -1\right)
^{n+q}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n-q\equiv n+q\operatorname{mod}%
2\right)  .
\]
\par
The definition of $T\left(  \sigma\right)  $ yields $T\left(  \sigma\right)
=g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}$. Thus,%
\[
\underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\circ g_{p}=g_{q}\circ\sigma\circ\underbrace{\left(
g_{p}\right)  ^{-1}\circ g_{p}}_{=\operatorname*{id}}=g_{q}\circ\sigma,
\]
so that%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{g_{q}\circ\sigma}=\underbrace{\left(  -1\right)  ^{g_{q}}%
}_{=\left(  -1\right)  ^{n+q}}\cdot\left(  -1\right)  ^{\sigma}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
g_{q}\text{ and }\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{T\left(  \sigma\right)  }\cdot\underbrace{\left(  -1\right)
^{g_{p}}}_{\substack{=\left(  -1\right)  ^{n-p}\\\text{(by Lemma
\ref{lem.laplace.gp} \textbf{(e)})}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sign.prod}), applied to }T\left(  \sigma\right)  \text{ and }%
g_{p}\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p},
\end{align*}
this yields
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p}=\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
We can divide both sides of this equality by $\left(  -1\right)  ^{n-p}$
(since $\left(  -1\right)  ^{n-p}\in\left\{  1,-1\right\}  $ is clearly an
invertible integer), and thus we obtain%
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\dfrac{\left(  -1\right)
^{n+q}\cdot\left(  -1\right)  ^{\sigma}}{\left(  -1\right)  ^{n-p}%
}=\underbrace{\dfrac{\left(  -1\right)  ^{n+q}}{\left(  -1\right)  ^{n-p}}%
}_{\substack{=\left(  -1\right)  ^{\left(  n+q\right)  -\left(  n-p\right)
}=\left(  -1\right)  ^{p+q}\\\text{(since }\left(  n+q\right)  -\left(
n-p\right)  =p+q\text{)}}}\cdot\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{p+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
This proves (\ref{pf.lem.laplace.Apq.1}).} and%
\begin{equation}
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }
\label{pf.lem.laplace.Apq.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.2}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Let us
recall the map $g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[
n\right]  \setminus\left\{  p\right\}  $ introduced in Lemma
\ref{lem.laplace.gp} \textbf{(f)}. Lemma \ref{lem.laplace.gp} \textbf{(f)}
says that this map $g_{p}^{\prime}$ is well-defined and bijective. In other
words, $g_{p}^{\prime}$ is a bijection.
\par
Let $i\in\left[  n-1\right]  $. Then, $g_{p}^{\prime}\left(  i\right)
=g_{p}\left(  i\right)  $ (by the definition of $g_{p}^{\prime}$). Also, the
definition of $T$ yields $T\left(  \sigma\right)  =g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}$, so that%
\[
\left(  \underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\right)  \left(  \underbrace{g_{p}^{\prime}\left(
i\right)  }_{=g_{p}\left(  i\right)  }\right)  =\left(  g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}\right)  \left(  g_{p}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  g_{p}\left(  i\right)  \right)  }_{=i}\right)  \right)
=g_{q}\left(  \sigma\left(  i\right)  \right)  .
\]
\par
From $g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)  $ and $\left(
T\left(  \sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  i\right)  \right)  $, we obtain%
\begin{equation}
a_{g_{p}^{\prime}\left(  i\right)  ,\left(  T\left(  \sigma\right)  \right)
\left(  g_{p}^{\prime}\left(  i\right)  \right)  }=a_{g_{p}\left(  i\right)
,g_{q}\left(  \sigma\left(  i\right)  \right)  }.
\label{pf.lem.laplace.Apq.2.pf.1}%
\end{equation}
\par
Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.Apq.2.pf.1}) for every $i\in\left[  n-1\right]  $. But
now, we have%
\begin{align*}
&  \underbrace{\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}}_{\substack{=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
p}}\\\text{(since }\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}%
}}a_{i,\left(  T\left(  \sigma\right)  \right)  \left(  i\right)  }\\
&  =\underbrace{\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}}%
_{=\prod_{i\in\left[  n\right]  \setminus\left\{  p\right\}  }}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[
n\right]  \setminus\left\{  p\right\}  }a_{i,\left(  T\left(  \sigma\right)
\right)  \left(  i\right)  }=\underbrace{\prod_{i\in\left[  n-1\right]  }%
}_{\substack{=\prod_{i\in\left\{  1,2,\ldots,n-1\right\}  }\\\text{(since
}\left[  n-1\right]  =\left\{  1,2,\ldots,n-1\right\}  \text{)}}%
}\underbrace{a_{g_{p}^{\prime}\left(  i\right)  ,\left(  T\left(
\sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)  \right)  }%
}_{\substack{=a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }\\\text{(by (\ref{pf.lem.laplace.Apq.2.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }g_{p}^{\prime}\left(  i\right)  \text{ for
}i\text{, since}\\
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\underbrace{\prod_{i\in\left\{  1,2,\ldots,n-1\right\}  }}_{=\prod
_{i=1}^{n-1}}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }=\prod_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(
\sigma\left(  i\right)  \right)  }.
\end{align*}
This proves (\ref{pf.lem.laplace.Apq.2}).}.

Now,
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}%
}}_{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }}_{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
n\right)  =n}}}\underbrace{\left(  -1\right)  ^{T\left(  \sigma\right)  }%
}_{\substack{=\left(  -1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma
}\\\text{(by (\ref{pf.lem.laplace.Apq.1}))}}}\underbrace{\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(  T\left(
\sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)
}\\\text{(by (\ref{pf.lem.laplace.Apq.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }T\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}
\end{array}
\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }\\
&  =\left(  -1\right)  ^{p+q}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  =n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }}_{\substack{=\det\left(  \left(  a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)
\\\text{(by Lemma \ref{lem.laplace.lem}, applied to }a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\text{ instead of }a_{i,j}\text{)}}}\\
&  =\left(  -1\right)  ^{p+q}\det\left(  \underbrace{\left(  a_{g_{p}\left(
i\right)  ,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}}_{\substack{=A_{\sim p,\sim q}\\\text{(by (\ref{pf.lem.laplace.Apq.A}))}%
}}\right)  =\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.laplace.Apq}.
\end{proof}
\end{verlong}

Now, we can finally prove Theorem \ref{thm.laplace.gen}:

\begin{proof}
[Proof of Theorem \ref{thm.laplace.gen}.]\textbf{(a)} Let $p\in\left\{
1,2,\ldots,n\right\}  $. From (\ref{eq.det.eq.2}), we obtain%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{q\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because for every }\sigma\in S_{n}\text{, there exists}\\
\text{exactly one }q\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying
}\sigma\left(  p\right)  =q
\end{array}
\right) \\
&  =\sum_{q\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{i,\sigma\left(
i\right)  }}_{\substack{=a_{p,\sigma\left(  p\right)  }\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)
}\\\text{(here, we have split off the factor for }i=p\text{ from the
product)}}}\\
&  =\underbrace{\sum_{q\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{q=1}^{n}%
}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}}\left(
-1\right)  ^{\sigma}\underbrace{a_{p,\sigma\left(  p\right)  }}%
_{\substack{=a_{p,q}\\\text{(since }\sigma\left(  p\right)  =q\text{)}}%
}\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}%
}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{q=1}^{n}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
p\right)  =q}}\left(  -1\right)  ^{\sigma}a_{p,q}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}%
_{=a_{p,q}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}\\
&  =\sum_{q=1}^{n}a_{p,q}\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(
i\right)  }}_{\substack{=\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  \\\text{(by Lemma \ref{lem.laplace.Apq})}}}\\
&  =\sum_{q=1}^{n}\underbrace{a_{p,q}\left(  -1\right)  ^{p+q}}_{=\left(
-1\right)  ^{p+q}a_{p,q}}\det\left(  A_{\sim p,\sim q}\right)  =\sum_{q=1}%
^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Theorem \ref{thm.laplace.gen} \textbf{(a)}.

\textbf{(b)} Let $q\in\left\{  1,2,\ldots,n\right\}  $. From
(\ref{eq.det.eq.2}), we obtain%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma^{-1}\left(  q\right)  =p}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because for every }\sigma\in S_{n}\text{, there exists}\\
\text{exactly one }p\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying
}\sigma^{-1}\left(  q\right)  =p
\end{array}
\right) \\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\underbrace{\sum
_{\substack{\sigma\in S_{n};\\\sigma^{-1}\left(  q\right)  =p}}}%
_{\substack{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\\\text{(because for any }\sigma\in S_{n}\text{,}\\\text{the statement
}\left(  \sigma^{-1}\left(  q\right)  =p\right)  \\\text{is equivalent to
the}\\\text{statement }\left(  \sigma\left(  p\right)  =q\right)  \text{)}%
}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }a_{i,\sigma\left(  i\right)  }}_{\substack{=a_{p,\sigma\left(
p\right)  }\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}a_{i,\sigma\left(  i\right)  }\\\text{(here, we have split off
the}\\\text{factor for }i=p\text{ from the product)}}}\\
&  =\underbrace{\sum_{p\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{p=1}^{n}%
}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}}\left(
-1\right)  ^{\sigma}\underbrace{a_{p,\sigma\left(  p\right)  }}%
_{\substack{=a_{p,q}\\\text{(since }\sigma\left(  p\right)  =q\text{)}}%
}\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}%
}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{p=1}^{n}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
p\right)  =q}}\left(  -1\right)  ^{\sigma}a_{p,q}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}%
_{=a_{p,q}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}\\
&  =\sum_{p=1}^{n}a_{p,q}\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(
i\right)  }}_{\substack{=\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  \\\text{(by Lemma \ref{lem.laplace.Apq})}}}\\
&  =\sum_{p=1}^{n}\underbrace{a_{p,q}\left(  -1\right)  ^{p+q}}_{=\left(
-1\right)  ^{p+q}a_{p,q}}\det\left(  A_{\sim p,\sim q}\right)  =\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Theorem \ref{thm.laplace.gen} \textbf{(b)}.
\end{proof}

The reader can easily see how Theorem \ref{thm.laplace.gen} \textbf{(b)} could
be (alternatively) proven using Theorem \ref{thm.laplace.gen} \textbf{(a)} and
Exercise \ref{exe.ps4.4}, and how Theorem \ref{thm.laplace.pre} could be seen
as a particular case of Theorem \ref{thm.laplace.gen} \textbf{(a)}.

\begin{remark}
Some books use Laplace expansion to define the notion of a determinant. For
example, one can define the determinant of a square matrix recursively, by
setting the determinant of the $0\times0$-matrix to be $1$, and defining the
determinant of an $n\times n$-matrix $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (with $n>0$) to be $\sum_{q=1}^{n}\left(  -1\right)
^{1+q}a_{1,q}\det\left(  A_{\sim1,\sim q}\right)  $ (assuming that
determinants of $\left(  n-1\right)  \times\left(  n-1\right)  $-matrices such
as $A_{\sim1,\sim q}$ are already defined). Of course, this leads to the same
notion of determinant as the one we are using, because of Theorem
\ref{thm.laplace.gen} \textbf{(a)}.
\end{remark}

\subsection{\label{sect.tridiag}Tridiagonal determinants}

In this section, we shall study the so-called \textit{tridiagonal matrices}: a
class of matrices whose all entries are zero everywhere except in the
\textquotedblleft direct proximity\textquotedblright\ of the diagonal (more
specifically: on the diagonal and \textquotedblleft one level below and one
level above\textquotedblright). We shall find recursive formulas for the
determinants of these matrices. These formulas are a simple example of an
application of Laplace expansion, but also interesting in their own right.

\begin{definition}
\label{def.tridiag}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$
elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n-1}$ be $n-1$ elements
of $\mathbb{K}$ (where we take the position that \textquotedblleft$-1$
elements of $\mathbb{K}$\textquotedblright\ means \textquotedblleft no
elements of $\mathbb{K}$\textquotedblright). Let $c_{1},c_{2},\ldots,c_{n-1}$
be $n-1$ elements of $\mathbb{K}$. We now set%
\[
A=\left(
\begin{array}
[c]{ccccccc}%
a_{1} & b_{1} & 0 & \cdots & 0 & 0 & 0\\
c_{1} & a_{2} & b_{2} & \cdots & 0 & 0 & 0\\
0 & c_{2} & a_{3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{n-2} & b_{n-2} & 0\\
0 & 0 & 0 & \cdots & c_{n-2} & a_{n-1} & b_{n-1}\\
0 & 0 & 0 & \cdots & 0 & c_{n-1} & a_{n}%
\end{array}
\right)  .
\]
(More formally,%
\[
A=\left(
\begin{cases}
a_{i}, & \text{if }i=j;\\
b_{i}, & \text{if }i=j-1;\\
c_{j}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
)

The matrix $A$ is called a \textit{tridiagonal matrix}.

We shall keep the notations $n$, $a_{1},a_{2},\ldots,a_{n}$, $b_{1}%
,b_{2},\ldots,b_{n-1}$, $c_{1},c_{2},\ldots,c_{n-1}$ and $A$ fixed for the
rest of Section \ref{sect.tridiag}.
\end{definition}

Playing around with small examples, one soon notices that the determinants of
tridiagonal matrices are too complicated to have neat explicit formulas in
full generality. For $n\in\left\{  0,1,2,3\right\}  $, the determinants look
as follows:%
\begin{align*}
\det A  &  =\det\left(  \text{the }0\times0\text{-matrix}\right)
=1\ \ \ \ \ \ \ \ \ \ \text{if }n=0;\\
\det A  &  =\det\left(
\begin{array}
[c]{c}%
a_{1}%
\end{array}
\right)  =a_{1}\ \ \ \ \ \ \ \ \ \ \text{if }n=1;\\
\det A  &  =\det\left(
\begin{array}
[c]{cc}%
a_{1} & b_{1}\\
c_{1} & a_{2}%
\end{array}
\right)  =a_{1}a_{2}-b_{1}c_{1}\ \ \ \ \ \ \ \ \ \ \text{if }n=2;\\
\det A  &  =\det\left(
\begin{array}
[c]{ccc}%
a_{1} & b_{1} & 0\\
c_{1} & a_{2} & b_{2}\\
0 & c_{2} & a_{3}%
\end{array}
\right)  =a_{1}a_{2}a_{3}-a_{1}b_{2}c_{2}-a_{3}b_{1}c_{1}%
\ \ \ \ \ \ \ \ \ \ \text{if }n=3.
\end{align*}
(And these formulas get more complicated the larger $n$ becomes.) However, the
many zeroes present in a tridiagonal matrix make it easy to find a recursive
formula for its determinant using Laplace expansion:

\begin{proposition}
\label{prop.tridiag.rec}For every two elements $x$ and $y$ of $\left\{
0,1,\ldots,n\right\}  $ satisfying $x\leq y$, we let $A_{x,y}$ be the $\left(
y-x\right)  \times\left(  y-x\right)  $-matrix%
\[
\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-2} & b_{y-2} & 0\\
0 & 0 & 0 & \cdots & c_{y-2} & a_{y-1} & b_{y-1}\\
0 & 0 & 0 & \cdots & 0 & c_{y-1} & a_{y}%
\end{array}
\right)  =\operatorname*{sub}\nolimits_{x+1,x+2,\ldots,y}^{x+1,x+2,\ldots
,y}A.
\]


\textbf{(a)} We have $\det\left(  A_{x,x}\right)  =1$ for every $x\in\left\{
0,1,\ldots,n\right\}  $.

\textbf{(b)} We have $\det\left(  A_{x,x+1}\right)  =a_{x+1}$ for every
$x\in\left\{  0,1,\ldots,n-1\right\}  $.

\textbf{(c)} For every $x\in\left\{  0,1,\ldots,n\right\}  $ and $y\in\left\{
0,1,\ldots,n\right\}  $ satisfying $x\leq y-2$, we have
\[
\det\left(  A_{x,y}\right)  =a_{y}\det\left(  A_{x,y-1}\right)  -b_{y-1}%
c_{y-1}\det\left(  A_{x,y-2}\right)  .
\]


\textbf{(d)} For every $x\in\left\{  0,1,\ldots,n\right\}  $ and $y\in\left\{
0,1,\ldots,n\right\}  $ satisfying $x\leq y-2$, we have
\[
\det\left(  A_{x,y}\right)  =a_{x+1}\det\left(  A_{x+1,y}\right)
-b_{x+1}c_{x+1}\det\left(  A_{x+2,y}\right)  .
\]


\textbf{(e)} We have $A=A_{0,n}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.tridiag.rec}.]\textbf{(e)} The definition of
$A_{0,n}$ yields
\[
A_{0,n}=\left(
\begin{array}
[c]{ccccccc}%
a_{1} & b_{1} & 0 & \cdots & 0 & 0 & 0\\
c_{1} & a_{2} & b_{2} & \cdots & 0 & 0 & 0\\
0 & c_{2} & a_{3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{n-2} & b_{n-2} & 0\\
0 & 0 & 0 & \cdots & c_{n-2} & a_{n-1} & b_{n-1}\\
0 & 0 & 0 & \cdots & 0 & c_{n-1} & a_{n}%
\end{array}
\right)  =A.
\]
This proves Proposition \ref{prop.tridiag.rec} \textbf{(e)}.

\textbf{(a)} Let $x\in\left\{  0,1,\ldots,n\right\}  $. Then, $A_{x,x}$ is an
$\left(  x-x\right)  \times\left(  x-x\right)  $-matrix, thus a $0\times
0$-matrix. Hence, its determinant is $\det\left(  A_{x,x}\right)  =1$. This
proves Proposition \ref{prop.tridiag.rec} \textbf{(a)}.

\textbf{(b)} Let $x\in\left\{  0,1,\ldots,n-1\right\}  $. The definition of
$A_{x,x+1}$ shows that $A_{x,x+1}$ is the $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
a_{x+1}%
\end{array}
\right)  $. Hence, $\det\left(  A_{x,x+1}\right)  =\det\left(
\begin{array}
[c]{c}%
a_{x+1}%
\end{array}
\right)  =a_{x+1}$. This proves Proposition \ref{prop.tridiag.rec}
\textbf{(b)}.

\textbf{(c)} Let $x\in\left\{  0,1,\ldots,n\right\}  $ and $y\in\left\{
0,1,\ldots,n\right\}  $ be such that $x\leq y-2$. We have%
\begin{equation}
A_{x,y}=\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-2} & b_{y-2} & 0\\
0 & 0 & 0 & \cdots & c_{y-2} & a_{y-1} & b_{y-1}\\
0 & 0 & 0 & \cdots & 0 & c_{y-1} & a_{y}%
\end{array}
\right)  . \label{pf.prop.tridiag.rec.c.Axy}%
\end{equation}
This is a $\left(  y-x\right)  \times\left(  y-x\right)  $-matrix. If we cross
out its $\left(  y-x\right)  $-th row (i.e., its last row) and its $\left(
y-x\right)  $-th column (i.e., its last column), then we obtain $A_{x,y-1}$.
In other words, $\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)
,\sim\left(  y-x\right)  }=A_{x,y-1}$.

Let us write the matrix $A_{x,y}$ in the form $A_{x,y}=\left(  u_{i,j}\right)
_{1\leq i\leq y-x,\ 1\leq j\leq y-x}$. Thus,
\begin{align*}
&  \left(  u_{y-x,1},u_{y-x,2},\ldots,u_{y-x,y-x}\right) \\
&  =\left(  \text{the last row of the matrix }A_{x,y}\right)  =\left(
0,0,\ldots,0,c_{y-1},a_{y}\right)  .
\end{align*}
In other words, we have%
\begin{align}
&  \left(  u_{y-x,q}=0\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,y-x-2\right\}  \right)  ,\label{pf.prop.tridiag.rec.c.1}\\
&  u_{y-x,y-x-1}=c_{y-1},\ \ \ \ \ \ \ \ \ \ \text{and}\nonumber\\
&  u_{y-x,y-x}=a_{y}.\nonumber
\end{align}


Now, Laplace expansion along the $\left(  y-x\right)  $-th row (or, more
precisely, Theorem \ref{thm.laplace.gen} \textbf{(a)}, applied to $y-x$,
$A_{x,y}$, $u_{i,j}$ and $y-x$ instead of $n$, $A$, $a_{i,j}$ and $p$) yields%
\begin{align}
\det\left(  A_{x,y}\right)   &  =\sum_{q=1}^{y-x}\left(  -1\right)  ^{\left(
y-x\right)  +q}u_{y-x,q}\det\left(  \left(  A_{x,y}\right)  _{\sim\left(
y-x\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{y-x-2}\left(  -1\right)  ^{\left(  y-x\right)  +q}%
\underbrace{u_{y-x,q}}_{\substack{=0\\\text{(by (\ref{pf.prop.tridiag.rec.c.1}%
))}}}\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim
q}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{\left(  y-x\right)
+\left(  y-x-1\right)  }}_{=-1}\underbrace{u_{y-x,y-x-1}}_{=c_{y-1}}%
\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x-1\right)  }\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{\left(  y-x\right)
+\left(  y-x\right)  }}_{=1}\underbrace{u_{y-x,y-x}}_{=a_{y}}\det\left(
\underbrace{\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x\right)  }}_{=A_{x,y-1}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }y-x\geq2\text{ (since }x\leq
y-2\text{)}\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{y-x-2}\left(  -1\right)  ^{\left(  y-x\right)
+q}0\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim
q}\right)  }_{=0}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -c_{y-1}\det\left(  \left(  A_{x,y}\right)
_{\sim\left(  y-x\right)  ,\sim\left(  y-x-1\right)  }\right)  +a_{y}%
\det\left(  A_{x,y-1}\right) \nonumber\\
&  =-c_{y-1}\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)
,\sim\left(  y-x-1\right)  }\right)  +a_{y}\det\left(  A_{x,y-1}\right)  .
\label{pf.prop.tridiag.rec.c.3}%
\end{align}


Now, let $B=\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x-1\right)  }$. Thus, (\ref{pf.prop.tridiag.rec.c.3}) becomes%
\begin{align}
\det\left(  A_{x,y}\right)   &  =-c_{y-1}\det\left(  \underbrace{\left(
A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(  y-x-1\right)  }}%
_{=B}\right)  +a_{y}\det\left(  A_{x,y-1}\right) \nonumber\\
&  =-c_{y-1}\det B+a_{y}\det\left(  A_{x,y-1}\right)  .
\label{pf.prop.tridiag.rec.c.3a}%
\end{align}


Now,%
\begin{align}
B  &  =\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x-1\right)  }\nonumber\\
&  =\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-3} & b_{y-3} & 0\\
0 & 0 & 0 & \cdots & c_{y-3} & a_{y-2} & 0\\
0 & 0 & 0 & \cdots & 0 & c_{y-2} & b_{y-1}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{because of
(\ref{pf.prop.tridiag.rec.c.Axy})}\right)  . \label{pf.prop.tridiag.rec.c.5}%
\end{align}


Now, let us write the matrix $B$ in the form $B=\left(  v_{i,j}\right)
_{1\leq i\leq y-x-1,\ 1\leq j\leq y-x-1}$. Thus,
\begin{align*}
&  \left(  v_{1,y-x-1},v_{2,y-x-1},\ldots,v_{y-x-1,y-x-1}\right) \\
&  =\left(  \text{the last column of the matrix }B\right)  =\left(
0,0,\ldots,0,b_{y-1}\right)
\end{align*}
(because of (\ref{pf.prop.tridiag.rec.c.5})). In other words, we have%
\begin{align}
&  \left(  v_{p,y-x-1}=0\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,y-x-2\right\}  \right)  ,\ \ \ \ \ \ \ \ \ \ \text{and}%
\label{pf.prop.tridiag.rec.c.7}\\
&  v_{y-x-1,y-x-1}=b_{y-1}.\nonumber
\end{align}
Now, Laplace expansion along the $\left(  y-x-1\right)  $-th column (or, more
precisely, Theorem \ref{thm.laplace.gen} \textbf{(b)}, applied to $y-x-1$,
$B$, $v_{i,j}$ and $y-x-1$ instead of $n$, $A$, $a_{i,j}$ and $q$) yields%
\begin{align}
\det B  &  =\sum_{p=1}^{y-x-1}\left(  -1\right)  ^{p+\left(  y-x-1\right)
}v_{p,y-x-1}\det\left(  B_{\sim p,\sim\left(  y-x-1\right)  }\right)
\nonumber\\
&  =\sum_{p=1}^{y-x-2}\left(  -1\right)  ^{p+\left(  y-x-1\right)
}\underbrace{v_{p,y-x-1}}_{\substack{=0\\\text{(by
(\ref{pf.prop.tridiag.rec.c.7}))}}}\det\left(  B_{\sim p,\sim\left(
y-x-1\right)  }\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{\left(  y-x-1\right)
+\left(  y-x-1\right)  }}_{=1}\underbrace{v_{y-x-1,y-x-1}}_{=b_{y-1}}%
\det\left(  B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)  }\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }y-x-1\geq1\text{ (since }x\leq
y-2\text{)}\right) \nonumber\\
&  =\underbrace{\sum_{p=1}^{y-x-2}\left(  -1\right)  ^{p+\left(  y-x-1\right)
}0\det\left(  B_{\sim p,\sim\left(  y-x-1\right)  }\right)  }_{=0}+b_{y-1}%
\det\left(  B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)  }\right)
\nonumber\\
&  =b_{y-1}\det\left(  B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)
}\right)  . \label{pf.prop.tridiag.rec.c.9}%
\end{align}
Finally, a look at (\ref{pf.prop.tridiag.rec.c.5}) reveals that%
\[
B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)  }=\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-4} & b_{y-4} & 0\\
0 & 0 & 0 & \cdots & c_{y-4} & a_{y-3} & b_{y-3}\\
0 & 0 & 0 & \cdots & 0 & c_{y-3} & a_{y-2}%
\end{array}
\right)  =A_{x,y-2}.
\]
Hence, (\ref{pf.prop.tridiag.rec.c.9}) becomes%
\[
\det B=b_{y-1}\det\left(  \underbrace{B_{\sim\left(  y-x-1\right)
,\sim\left(  y-x-1\right)  }}_{=A_{x,y-2}}\right)  =b_{y-1}\det\left(
A_{x,y-2}\right)  .
\]
Therefore, (\ref{pf.prop.tridiag.rec.c.3a}) becomes%
\begin{align*}
\det\left(  A_{x,y}\right)   &  =-c_{y-1}\underbrace{\det B}_{=b_{y-1}%
\det\left(  A_{x,y-2}\right)  }+a_{y}\det\left(  A_{x,y-1}\right) \\
&  =-c_{y-1}b_{y-1}\det\left(  A_{x,y-2}\right)  +a_{y}\det\left(
A_{x,y-1}\right) \\
&  =a_{y}\det\left(  A_{x,y-1}\right)  -b_{y-1}c_{y-1}\det\left(
A_{x,y-2}\right)  .
\end{align*}
This proves Proposition \ref{prop.tridiag.rec} \textbf{(c)}.

\textbf{(d)} The proof of Proposition \ref{prop.tridiag.rec} \textbf{(d)} is
similar to the proof of Proposition \ref{prop.tridiag.rec} \textbf{(c)}. The
main difference is that we now have to perform Laplace expansion along the
$1$-st row (instead of the $\left(  y-x\right)  $-th row) and then Laplace
expansion along the $1$-st column (instead of the $\left(  y-x\right)  $-th column).
\end{proof}

Proposition \ref{prop.tridiag.rec} gives us two fast recursive algorithms to
compute $\det A$:

The first algorithm proceeds by recursively computing $\det\left(
A_{0,m}\right)  $ for every $m\in\left\{  0,1,\ldots,n\right\}  $. This is
done using Proposition \ref{prop.tridiag.rec} \textbf{(a)} (for $m=0$),
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (for $m=1$) and Proposition
\ref{prop.tridiag.rec} \textbf{(c)} (to find $\det\left(  A_{0,m}\right)  $
for $m\geq2$ in terms of $\det\left(  A_{0,m-1}\right)  $ and $\det\left(
A_{0,m-2}\right)  $). The final value $\det\left(  A_{0,n}\right)  $ is $\det
A$ (by Proposition \ref{prop.tridiag.rec} \textbf{(e)}).

The second algorithm proceeds by recursively computing $\det\left(
A_{m,n}\right)  $ for every $m\in\left\{  0,1,\ldots,n\right\}  $. This
recursion goes backwards: We start with $m=n$ (where we use Proposition
\ref{prop.tridiag.rec} \textbf{(a)}), then turn to $m=n-1$ (using Proposition
\ref{prop.tridiag.rec} \textbf{(b)}), and then go further and further down
(using Proposition \ref{prop.tridiag.rec} \textbf{(d)} to compute $\det\left(
A_{m,n}\right)  $ in terms of $\det\left(  A_{m+1,n}\right)  $ and
$\det\left(  A_{m+2,n}\right)  $).

So we have two different recursive algorithms leading to one and the same
result. Whenever you have such a thing, you can package up the equivalence of
the two algorithms as an exercise, and try to make it less easy by covering up
the actual goal of the algorithms (in our case, computing $\det A$). In our
case, this leads to the following exercise:

\begin{exercise}
\label{exe.tridiag.isl}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n-1}$ be $n-1$
elements of $\mathbb{K}$.

Define a sequence $\left(  u_{0},u_{1},\ldots,u_{n}\right)  $ of elements of
$\mathbb{K}$ recursively by setting $u_{0}=1$, $u_{1}=a_{1}$ and%
\[
u_{i}=a_{i}u_{i-1}-b_{i-1}u_{i-2}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  2,3,\ldots,n\right\}  .
\]


Define a sequence $\left(  v_{0},v_{1},\ldots,v_{n}\right)  $ of elements of
$\mathbb{K}$ recursively by setting $v_{0}=1$, $v_{1}=a_{n}$ and%
\[
v_{i}=a_{n-i+1}v_{i-1}-b_{n-i+1}v_{i-2}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  2,3,\ldots,n\right\}  .
\]


Prove that $u_{n}=v_{n}$.
\end{exercise}

This exercise generalizes
\href{http://www.artofproblemsolving.com/community/c6h597118p3543340}{IMO
Shortlist 2013 problem A1}\footnote{I have a suspicion that
\href{http://www.artofproblemsolving.com/community/c6h355915}{IMO Shortlist
2009 problem C3} also can be viewed as an equality between two recursive ways
to compute a determinant; but this determinant seems to be harder to find (I
don't think it can be obtained from Proposition \ref{prop.tridiag.rec}).}.

Our recursive algorithms for computing $\det A$ also yield another
observation: The determinant $\det A$ depends not on the $2\left(  n-1\right)
$ elements $b_{1},b_{2},\ldots,b_{n-1},c_{1},c_{2},\ldots,c_{n-1}$ but only on
the products $b_{1}c_{1},b_{2}c_{2},\ldots,b_{n-1}c_{n-1}$.

\begin{exercise}
\label{exe.tridiag.cf}Define $A_{x,y}$ as in Proposition
\ref{prop.tridiag.rec}. Prove that%
\[
\dfrac{\det A}{\det\left(  A_{1,n}\right)  }=a_{1}-\dfrac{b_{1}c_{1}}%
{a_{2}-\dfrac{b_{2}c_{2}}{a_{3}-\dfrac{b_{3}c_{3}}{%
\begin{array}
[c]{ccc}%
a_{4}- &  & \\
& \ddots & \\
&  & -\dfrac{b_{n-2}c_{n-2}}{a_{n-1}-\dfrac{b_{n-1}c_{n-1}}{a_{n}}}%
\end{array}
}}},
\]
provided that all denominators in this equality are invertible.
\end{exercise}

\begin{exercise}
\label{exe.tridiag.fib}Assume that $a_{i}=1$ for all $i\in\left\{
1,2,\ldots,n\right\}  $. Also, assume that $b_{i}=1$ and $c_{i}=-1$ for all
$i\in\left\{  1,2,\ldots,n-1\right\}  $. Let $\left(  f_{0},f_{1},f_{2}%
,\ldots\right)  $ be the Fibonacci sequence (defined as in Chapter
\ref{chp.recur}). Show that $\det A=f_{n+1}$.
\end{exercise}

\begin{remark}
\label{rmk.tridiag.fib-cont}Consider once again the Fibonacci sequence
$\left(  f_{0},f_{1},f_{2},\ldots\right)  $ (defined as in Chapter
\ref{chp.recur}). Let $n$ be a positive integer. Combining the results of
Exercise \ref{exe.tridiag.cf} and Exercise \ref{exe.tridiag.fib} (the details
are left to the reader), we obtain the equality%
\begin{align*}
\dfrac{f_{n+1}}{f_{n}}  &  =1-\dfrac{1\left(  -1\right)  }{1-\dfrac{1\left(
-1\right)  }{1-\dfrac{1\left(  -1\right)  }{%
\begin{array}
[c]{ccc}%
1- &  & \\
& \ddots & \\
&  & -\dfrac{1\left(  -1\right)  }{1-\dfrac{1\left(  -1\right)  }{1}}%
\end{array}
}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{with }n-1\text{ fractions in
total}\right) \\
&  =1+\dfrac{1}{1+\dfrac{1}{1+\dfrac{1}{%
\begin{array}
[c]{ccc}%
1+ &  & \\
& \ddots & \\
&  & +\dfrac{1}{1+\dfrac{1}{1}}%
\end{array}
}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{with }n-1\text{ fractions in
total}\right)  .
\end{align*}
If you know
\href{https://en.wikipedia.org/wiki/Golden_ratio#Alternative_forms}{some
trivia about the golden ratio}, you might recognize this as a part of the
continued fraction for the golden ratio $\varphi$. The whole continued
fraction for $\varphi$ is%
\[
\varphi=1+\dfrac{1}{1+\dfrac{1}{1+\dfrac{1}{%
\begin{array}
[c]{cc}%
1+ & \\
& \ddots
\end{array}
}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{with infinitely many fractions}\right)
.
\]
This hints at the fact that $\lim\limits_{n\rightarrow\infty}\dfrac{f_{n+1}%
}{f_{n}}=\varphi$. (This is easy to prove without continued fractions, of course.)
\end{remark}

\subsection{On block-triangular matrices}

\begin{definition}
\label{def.block2x2}Let $n$, $n^{\prime}$, $m$ and $m^{\prime}$ be four
nonnegative integers.

Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be an
$n\times m$-matrix.

Let $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$ be
an $n\times m^{\prime}$-matrix.

Let $C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$ be
an $n^{\prime}\times m$-matrix.

Let $D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq
m^{\prime}}$ be an $n^{\prime}\times m^{\prime}$-matrix.

Then, $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ will mean the $\left(  n+n^{\prime}\right)  \times\left(
m+m^{\prime}\right)  $-matrix
\[
\left(
\begin{array}
[c]{cccccccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m} & b_{1,1} & b_{1,2} & \cdots &
b_{1,m^{\prime}}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} & b_{2,1} & b_{2,2} & \cdots &
b_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m} & b_{n,1} & b_{n,2} & \cdots &
b_{n,m^{\prime}}\\
c_{1,1} & c_{1,2} & \cdots & c_{1,m} & d_{1,1} & d_{1,2} & \cdots &
d_{1,m^{\prime}}\\
c_{2,1} & c_{2,2} & \cdots & c_{2,m} & d_{2,1} & d_{2,2} & \cdots &
d_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
c_{n^{\prime},1} & c_{n^{\prime},2} & \cdots & c_{n^{\prime},m} &
d_{n^{\prime},1} & d_{n^{\prime},2} & \cdots & d_{n^{\prime},m^{\prime}}%
\end{array}
\right)  .
\]
(Formally speaking, this means that%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\label{eq.def.block2x2.formal}%
\end{equation}
Less formally, we can say that $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ is the matrix obtained by gluing the matrices $A$, $B$, $C$ and $D$
to form one big $\left(  n+n^{\prime}\right)  \times\left(  m+m^{\prime
}\right)  $-matrix, where the right border of $A$ is glued together with the
left border of $B$, the bottom border of $A$ is glued together with the top
border of $C$, etc.)

Do not get fooled by the notation $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $: It is (in general) not a $2\times2$-matrix, but an $\left(
n+n^{\prime}\right)  \times\left(  m+m^{\prime}\right)  $-matrix, and its
entries are not $A$, $B$, $C$ and $D$ but the entries of $A$, $B$, $C$ and $D$.
\end{definition}

\begin{example}
If $A=\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{c}%
b_{1}\\
b_{2}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{cc}%
c_{1} & c_{2}%
\end{array}
\right)  $ and $D=\left(
\begin{array}
[c]{c}%
d
\end{array}
\right)  $, then $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a_{1,1} & a_{1,2} & b_{1}\\
a_{2,1} & a_{2,2} & b_{2}\\
c_{1} & c_{2} & d
\end{array}
\right)  $.
\end{example}

The notation $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ introduced in Definition \ref{def.block2x2} is a particular case of
a more general notation -- the \textit{block-matrix construction} -- for
gluing together multiple matrices with matching dimensions\footnote{This
construction defines an $\left(  n_{1}+n_{2}+\cdots+n_{x}\right)
\times\left(  m_{1}+m_{2}+\cdots+m_{y}\right)  $-matrix%
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
A_{1,1} & A_{1,2} & \cdots & A_{1,y}\\
A_{2,1} & A_{2,2} & \cdots & A_{2,y}\\
\vdots & \vdots & \ddots & \vdots\\
A_{x,1} & A_{x,2} & \cdots & A_{x,y}%
\end{array}
\right)  \label{eq.block-general}%
\end{equation}
whenever you have given two nonnegative integers $x$ and $y$, an $x$-tuple
$\left(  n_{1},n_{2},\ldots,n_{x}\right)  \in\mathbb{N}^{x}$, a $y$-tuple
$\left(  m_{1},m_{2},\ldots,m_{y}\right)  \in\mathbb{N}^{y}$, and an
$n_{i}\times m_{j}$-matrix $A_{i,j}$ for every $i\in\left\{  1,2,\ldots
,x\right\}  $ and every $j\in\left\{  1,2,\ldots,y\right\}  $. I guess you can
guess the definition of this matrix. So you start with an \textquotedblleft%
$x\times y$-matrix of matrices\textquotedblright\ and glue them together to an
$\left(  n_{1}+n_{2}+\cdots+n_{x}\right)  \times\left(  m_{1}+m_{2}%
+\cdots+m_{y}\right)  $-matrix (provided that the dimensions of these matrices
allow them to be glued -- e.g., you cannot glue a $2\times3$-matrix to a
$4\times6$-matrix along its right border, nor on any other border).
\par
It is called \textquotedblleft block-matrix construction\textquotedblright%
\ because the original matrices $A_{i,j}$ appear as \textquotedblleft
blocks\textquotedblright\ in the big matrix (\ref{eq.block-general}). Most
authors define block matrices to be matrices which are \textquotedblleft
partitioned\textquotedblright\ into blocks as in (\ref{eq.block-general});
this is essentially our construction in reverse: Instead of gluing several
\textquotedblleft small\textquotedblright\ matrices into a big one, they study
big matrices partitioned into many small matrices. Of course, the properties
of their \textquotedblleft block matrices\textquotedblright\ are equivalent to
those of our \textquotedblleft block-matrix construction\textquotedblright.}.
We shall only need the particular case that is Definition \ref{def.block2x2}, however.

\begin{definition}
Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Recall that $\mathbb{K}^{n\times
m}$ is the set of all $n\times m$-matrices.

We use $0_{n\times m}$ (or sometimes just $0$) to denote the $n\times
m$\textit{ zero matrix}. (As we recall, this is the $n\times m$-matrix whose
all entries are $0$; in other words, this is the $n\times m$-matrix $\left(
0\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.)
\end{definition}

\begin{exercise}
\label{exe.block2x2.mult}Let $n$, $n^{\prime}$, $m$, $m^{\prime}$, $\ell$ and
$\ell^{\prime}$ be six nonnegative integers. Let $A\in\mathbb{K}^{n\times m}$,
$B\in\mathbb{K}^{n\times m^{\prime}}$, $C\in\mathbb{K}^{n^{\prime}\times m}$,
$D\in\mathbb{K}^{n^{\prime}\times m^{\prime}}$, $A^{\prime}\in\mathbb{K}%
^{m\times\ell}$, $B^{\prime}\in\mathbb{K}^{m\times\ell^{\prime}}$, $C^{\prime
}\in\mathbb{K}^{m^{\prime}\times\ell}$ and $D^{\prime}\in\mathbb{K}%
^{m^{\prime}\times\ell^{\prime}}$. Then, prove that%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  .
\]

\end{exercise}

\begin{remark}
The intuitive meaning of Exercise \ref{exe.block2x2.mult} is that the product
of two matrices in \textquotedblleft block-matrix notation\textquotedblright%
\ can be computed by applying the usual multiplication rule \textquotedblleft
on the level of blocks\textquotedblright, without having to fall back to
multiplying single entries. However, when applying Exercise
\ref{exe.block2x2.mult}, do not forget to check that its conditions are
satisfied. Let me give an example and a non-example:

\textbf{Example:} If $A=\left(
\begin{array}
[c]{c}%
a_{1}\\
a_{2}\\
a_{3}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{cc}%
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}\\
b_{3,1} & b_{3,2}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{c}%
c
\end{array}
\right)  $, $D=\left(
\begin{array}
[c]{cc}%
d_{1} & d_{2}%
\end{array}
\right)  $, $A^{\prime}=\left(
\begin{array}
[c]{cc}%
a_{1}^{\prime} & a_{2}^{\prime}%
\end{array}
\right)  $, $B^{\prime}=\left(
\begin{array}
[c]{cc}%
b_{1}^{\prime} & b_{2}^{\prime}%
\end{array}
\right)  $, $C^{\prime}=\left(
\begin{array}
[c]{cc}%
c_{1,1}^{\prime} & c_{1,2}^{\prime}\\
c_{2,1}^{\prime} & c_{2,2}^{\prime}%
\end{array}
\right)  $ and $D^{\prime}=\left(
\begin{array}
[c]{cc}%
d_{1,1}^{\prime} & d_{1,2}^{\prime}\\
d_{2,1}^{\prime} & d_{2,2}^{\prime}%
\end{array}
\right)  $, then Exercise \ref{exe.block2x2.mult} can be applied (with $n=3$,
$n^{\prime}=1$, $m=1$, $m^{\prime}=2$, $\ell=2$ and $\ell^{\prime}=2$), and
thus we obtain%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  .
\]


\textbf{Non-example:} If $A=\left(
\begin{array}
[c]{c}%
a_{1}\\
a_{2}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{cc}%
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{c}%
c_{1}\\
c_{2}%
\end{array}
\right)  $, $D=\left(
\begin{array}
[c]{cc}%
d_{1,1} & d_{1,2}\\
d_{2,1} & d_{2,2}%
\end{array}
\right)  $, $A^{\prime}=\left(
\begin{array}
[c]{cc}%
a_{1,1}^{\prime} & a_{1,2}^{\prime}\\
a_{2,1}^{\prime} & a_{2,2}^{\prime}%
\end{array}
\right)  $, $B^{\prime}=\left(
\begin{array}
[c]{cc}%
b_{1,1}^{\prime} & b_{1,2}^{\prime}\\
b_{1,1}^{\prime} & b_{2,2}^{\prime}%
\end{array}
\right)  $, $C^{\prime}=\left(
\begin{array}
[c]{cc}%
c_{1}^{\prime} & c_{2}^{\prime}%
\end{array}
\right)  $ and $D^{\prime}=\left(
\begin{array}
[c]{cc}%
d_{1}^{\prime} & d_{2}^{\prime}%
\end{array}
\right)  $, then Exercise \ref{exe.block2x2.mult} cannot be applied, because
there exist no $n,m,\ell\in\mathbb{N}$ such that $A\in\mathbb{K}^{n\times m}$
and $A^{\prime}\in\mathbb{K}^{m\times\ell}$. (Indeed, the number of columns of
$A$ does not equal the number of rows of $A^{\prime}$, but these numbers would
both have to be $m$.) The matrices $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $ still exist in this case, and can even be multiplied, but their
product is not given by a simple formula such as the one in Exercise
\ref{exe.block2x2.mult}. Thus, beware of seeing Exercise
\ref{exe.block2x2.mult} as a panacea for multiplying matrices blockwise.
\end{remark}

\begin{exercise}
\label{exe.block2x2.tridet}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times n$-matrix. Let $B$ be an $n\times m$-matrix. Let $D$ be an
$m\times m$-matrix. Prove that%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{m\times n} & D
\end{array}
\right)  =\det A\cdot\det D.
\]

\end{exercise}

\begin{example}
Exercise \ref{exe.block2x2.tridet} (applied to $n=2$ and $m=3$) yields%
\[
\det\left(
\begin{array}
[c]{ccccc}%
a_{1,1} & a_{1,2} & b_{1,1} & b_{1,2} & b_{1,3}\\
a_{2,1} & a_{2,2} & b_{2,1} & b_{2,2} & b_{2,3}\\
0 & 0 & c_{1,1} & c_{1,2} & c_{1,3}\\
0 & 0 & c_{2,1} & c_{2,2} & c_{2,3}\\
0 & 0 & c_{3,1} & c_{3,2} & c_{3,3}%
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{ccc}%
c_{1,1} & c_{1,2} & c_{1,3}\\
c_{2,1} & c_{2,2} & c_{2,3}\\
c_{3,1} & c_{3,2} & c_{3,3}%
\end{array}
\right)  .
\]

\end{example}

\begin{remark}
Not every determinant of the form $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{m\times n} & D
\end{array}
\right)  $ can be computed using Exercise \ref{exe.block2x2.tridet}. In fact,
Exercise \ref{exe.block2x2.tridet} requires $A$ to be an $n\times n$-matrix
and $D$ to be an $m\times m$-matrix; thus, both $A$ and $D$ have to be square
matrices in order for Exercise \ref{exe.block2x2.tridet} to be applicable. For
instance, Exercise \ref{exe.block2x2.tridet} cannot be applied to compute
$\det\left(
\begin{array}
[c]{ccc}%
a_{1} & b_{1,1} & b_{1,2}\\
a_{2} & b_{2,1} & b_{2,2}\\
0 & c_{1} & c_{2}%
\end{array}
\right)  $.
\end{remark}

\begin{remark}
You might wonder whether Exercise \ref{exe.block2x2.tridet} generalizes to a
formula for $\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ when $A\in\mathbb{K}^{n\times n}$, $B\in\mathbb{K}^{n\times m}$,
$C\in\mathbb{K}^{m\times n}$ and $D\in\mathbb{K}^{m\times m}$. The general
answer is \textquotedblleft No\textquotedblright. However, when $D$ is
invertible, there exists such a formula (involving
\href{https://en.wikipedia.org/wiki/Schur_complement}{the Schur complement}).
Curiously, there is also a formula for the case when $n=m$ and $CD=DC$ (see
\cite[Theorem 3]{Silvest}).
\end{remark}

We notice that Exercise \ref{exe.block2x2.tridet} allows us to solve Exercise
\ref{exe.ps4.5} in a new way.

\begin{exercise}
\label{exe.det.creative}Invent and solve an exercise on computing determinants.
\end{exercise}

\subsection{The adjugate matrix}

We start this section with a variation on Theorem \ref{thm.laplace.gen}:

\begin{proposition}
\label{prop.laplace.0}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $r\in\left\{
1,2,\ldots,n\right\}  $.

\textbf{(a)} For every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfying $p\neq
r$, we have%
\[
0=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\]


\textbf{(b)} For every $q\in\left\{  1,2,\ldots,n\right\}  $ satisfying $q\neq
r$, we have%
\[
0=\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\]

\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.laplace.0}.]\textbf{(a)} Let $p\in\left\{
1,2,\ldots,n\right\}  $ be such that $p\neq r$.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $p$-th
row of $A$ by the $r$-th row of $A$. Thus, the $p$-th and the $r$-th rows of
$C$ are equal. Therefore, the matrix $C$ has two equal rows (since $p\neq r$).
Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$
instead of $A$).

Let us write the $n\times n$-matrix $C$ in the form $C=\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

The $p$-th row of $C$ equals the $r$-th row of $A$ (by the construction of
$C$). In other words,%
\begin{equation}
c_{p,q}=a_{r,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  . \label{pf.prop.laplace.0.short.cpq}%
\end{equation}
On the other hand, the matrix $C$ equals the matrix $A$ in all rows but the
$p$-th one (again, by the construction of $C$). Hence, if we cross out the
$p$-th rows in both $C$ and $A$, then the matrices $C$ and $A$ become equal.
Therefore,%
\begin{equation}
C_{\sim p,\sim q}=A_{\sim p,\sim q}\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left\{  1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.short.CA}%
\end{equation}
(because the construction of $C_{\sim p,\sim q}$ from $C$ involves crossing
out the $p$-th row, and so does the construction of $A_{\sim p,\sim q}$ from
$A$).

Now, $\det C=0$, so that%
\begin{align*}
0  &  =\det C=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\underbrace{c_{p,q}%
}_{\substack{=a_{r,q}\\\text{(by (\ref{pf.prop.laplace.0.short.cpq}))}}%
}\det\left(  \underbrace{C_{\sim p,\sim q}}_{\substack{=A_{\sim p,\sim
q}\\\text{(by (\ref{pf.prop.laplace.0.short.CA}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(a)}, applied to }C\text{ and }c_{i,j}\text{ instead of }A\text{ and
}a_{i,j}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\end{align*}
This proves Proposition \ref{prop.laplace.0} \textbf{(a)}.

\textbf{(b)} This proof is rather similar to the proof of Proposition
\ref{prop.laplace.0} \textbf{(a)}, except that rows are now replaced by
columns. We leave the details to the reader.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.laplace.0}.]\textbf{(a)} Let $p\in\left\{
1,2,\ldots,n\right\}  $ be such that $p\neq r$.

Let $w$ be the $r$-th row of $A$ (regarded, as usual, as a row vector). Thus,
$w=\left(  \text{the }r\text{-th row of }A\right)  $.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $p$-th
row of $A$ by the row vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }C\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{pf.prop.laplace.0.Cu}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq p\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }p\text{-th row of }C\right)  =w.
\label{pf.prop.laplace.0.Cp}%
\end{equation}
The matrix $C$ has two equal rows\footnote{\textit{Proof.} We have $r\neq p$
(since $p\neq r$). Hence, (\ref{pf.prop.laplace.0.Cu}) (applied to $u=r$)
yields%
\begin{align*}
\left(  \text{the }r\text{-th row of }C\right)   &  =\left(  \text{the
}r\text{-th row of }A\right)  =w\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}w=\left(  \text{the }r\text{-th row of }A\right)  \right) \\
&  =\left(  \text{the }p\text{-th row of }C\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.Cp})}\right)  .
\end{align*}
In other words, the $r$-th row of $C$ and the $p$-th row of $C$ are equal.
Since $r\neq p$, this shows that the matrix $C$ has two equal rows. Qed.}.
Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$
instead of $A$).

Let us write the $n\times n$-matrix $C$ in the form $C=\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every $u\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th row of }C\right)  =\left(  c_{u,1}%
,c_{u,2},\ldots,c_{u,n}\right)  . \label{pf.prop.laplace.0.u-th-row}%
\end{equation}
Applying this to $u=p$, we obtain%
\[
\left(  \text{the }p\text{-th row of }C\right)  =\left(  c_{p,1}%
,c_{p,2},\ldots,c_{p,n}\right)  ,
\]
so that%
\begin{align*}
\left(  c_{p,1},c_{p,2},\ldots,c_{p,n}\right)   &  =\left(  \text{the
}p\text{-th row of }C\right)  =w=\left(  \text{the }r\text{-th row of
}A\right) \\
&  =\left(  a_{r,1},a_{r,2},\ldots,a_{r,n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  .
\end{align*}
In other words,%
\begin{equation}
c_{p,q}=a_{r,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  . \label{pf.prop.laplace.0.cpq}%
\end{equation}


On the other hand,%
\begin{equation}
c_{u,q}=a_{u,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  \text{ and }u\in\left\{  1,2,\ldots,n\right\}  \text{
satisfying }u\neq p \label{pf.prop.laplace.0.cuq}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.laplace.0.cuq}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ be such that $u\neq p$. Thus,%
\begin{align*}
\left(  c_{u,1},c_{u,2},\ldots,c_{u,n}\right)   &  =\left(  \text{the
}u\text{-th row of }C\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.laplace.0.u-th-row})}\right) \\
&  =\left(  \text{the }u\text{-th row of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.Cu})}\right) \\
&  =\left(  a_{u,1},a_{u,2},\ldots,a_{u,n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  .
\end{align*}
In other words, $c_{u,q}=a_{u,q}$ for every $q\in\left\{  1,2,\ldots
,n\right\}  $. This proves (\ref{pf.prop.laplace.0.cuq}).}. Now, it is easy to
see that%
\begin{equation}
C_{\sim p,\sim q}=A_{\sim p,\sim q}\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left\{  1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.CA}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.laplace.0.CA}):} Let $q\in\left\{
1,2,\ldots,n\right\}  $.
\par
Let $\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  $. Thus,
$\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $, so that $\left\{  u_{1},u_{2},\ldots,u_{n-1}\right\}
=\left\{  1,2,\ldots,\widehat{p},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  p\right\}  $.
\par
Now, let $x\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $u_{x}\in\left\{
u_{1},u_{2},\ldots,u_{n-1}\right\}  =\left\{  1,2,\ldots,n\right\}
\setminus\left\{  p\right\}  $, so that $u_{x}\neq p$. Hence,%
\begin{equation}
c_{u_{x},q}=a_{u_{x},q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.CA.pf.1}%
\end{equation}
(by (\ref{pf.prop.laplace.0.cuq}), applied to $u=u_{x}$).
\par
Let us now forget that we fixed $x$. We thus have shown that
(\ref{pf.prop.laplace.0.CA.pf.1}) holds for every $x\in\left\{  1,2,\ldots
,n-1\right\}  $.
\par
Let $\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{q},\ldots,n\right)  $. Thus,
$\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  =\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  $.
\par
Now, the definition of $C_{\sim p,\sim q}$ yields%
\begin{align*}
C_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}C=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{v_{1},v_{2},\ldots,v_{n-1}}C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  =\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1}%
,v_{2},\ldots,v_{n-1}}C\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  \right) \\
&  =\left(  \underbrace{c_{u_{x},v_{y}}}_{\substack{=a_{u_{x},v_{y}%
}\\\text{(by (\ref{pf.prop.laplace.0.CA.pf.1}),}\\\text{applied to }%
q=v_{y}\text{)}}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1},v_{2},\ldots,v_{n-1}}C\text{,
since }C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}.
\end{align*}
Compared with%
\begin{align*}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{v_{1},v_{2},\ldots,v_{n-1}}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  =\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1}%
,v_{2},\ldots,v_{n-1}}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  \right) \\
&  =\left(  a_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq
n-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\operatorname*{sub}\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1},v_{2}%
,\ldots,v_{n-1}}A\text{, since }A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ,
\end{align*}
this yields $C_{\sim p,\sim q}=A_{\sim p,\sim q}$. This proves
(\ref{pf.prop.laplace.0.CA}).}. Now, $\det C=0$, so that%
\begin{align*}
0  &  =\det C=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\underbrace{c_{p,q}%
}_{\substack{=a_{r,q}\\\text{(by (\ref{pf.prop.laplace.0.cpq}))}}}\det\left(
\underbrace{C_{\sim p,\sim q}}_{\substack{=A_{\sim p,\sim q}\\\text{(by
(\ref{pf.prop.laplace.0.CA}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(a)}, applied to }C\text{ and }c_{i,j}\text{ instead of }A\text{ and
}a_{i,j}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\end{align*}
This proves Proposition \ref{prop.laplace.0} \textbf{(a)}.

\textbf{(b)} This proof is rather similar to the proof of Proposition
\ref{prop.laplace.0} \textbf{(a)}, except that rows are now replaced by
columns. We leave the details to the reader.
\end{proof}
\end{verlong}

We now can define the \textquotedblleft adjugate\textquotedblright\ of a matrix:

\begin{definition}
\label{def.adj}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. We
define a new $n\times n$-matrix $\operatorname*{adj}A$ by%
\[
\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]


This matrix $\operatorname*{adj}A$ is called the \textit{adjugate} of the
matrix $A$. (Some authors call it the \textquotedblleft
adjunct\textquotedblright\ or \textquotedblleft adjoint\textquotedblright\ or
\textquotedblleft classical adjoint\textquotedblright\ of $A$ instead.
However, beware of the word \textquotedblleft adjoint\textquotedblright: It
means too many different things; in particular it has a second meaning for a matrix.)
\end{definition}

The appearance of $A_{\sim j,\sim i}$ (not $A_{\sim i,\sim j}$) in Definition
\ref{def.adj} might be surprising, but it is not a mistake. We will soon see
what it is good for.

There is also a related notion, namely that of a \textquotedblleft cofactor
matrix\textquotedblright. The \textit{cofactor matrix} of an $n\times
n$-matrix $A$ is defined to be $\left(  \left(  -1\right)  ^{i+j}\det\left(
A_{\sim i,\sim j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. This is,
of course, the transpose $\left(  \operatorname*{adj}A\right)  ^{T}$ of
$\operatorname*{adj}A$. The entries of this matrix are called the
\textit{cofactors} of $A$.

\begin{example}
The adjugate of the $0\times0$-matrix is the $0\times0$-matrix.

The adjugate of a $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)  $ is $\operatorname*{adj}\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  $. (Yes, this shows that all $1\times1$-matrices have the same adjugate.)

The adjugate of a $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $ is $\operatorname*{adj}\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  $.

The adjugate of a $3\times3$-matrix $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $ is $\operatorname*{adj}\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
ei-fh & ch-bi & bf-ce\\
fg-di & ai-cg & cd-af\\
dh-ge & bg-ah & ae-bd
\end{array}
\right)  $.
\end{example}

\begin{proposition}
\label{prop.adj.transpose}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Then, $\operatorname*{adj}\left(  A^{T}\right)  =\left(
\operatorname*{adj}A\right)  ^{T}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.adj.transpose}.]Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. From
$i\in\left\{  1,2,\ldots,n\right\}  $, we obtain $1\leq i\leq n$, so that
$n\geq1$ and thus $n-1\in\mathbb{N}$.

The definition of $A_{\sim i,\sim j}$ yields $A_{\sim i,\sim j}%
=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots,n}^{1,2,\ldots
,\widehat{j},\ldots,n}A$. But the definition of $\left(  A^{T}\right)  _{\sim
j,\sim i}$ yields%
\begin{equation}
\left(  A^{T}\right)  _{\sim j,\sim i}=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{j},\ldots,n}^{1,2,\ldots,\widehat{i},\ldots
,n}\left(  A^{T}\right)  . \label{pf.prop.adj.transpose.1}%
\end{equation}


On the other hand, Proposition \ref{prop.submatrix.easy} \textbf{(e)} (applied
to $m=n$, $u=n-1$, $v=n-1$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(
1,2,\ldots,\widehat{i},\ldots,n\right)  $ and $\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  =\left(  1,2,\ldots,\widehat{j},\ldots,n\right)  $)
yields $\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots
,n}^{1,2,\ldots,\widehat{j},\ldots,n}A\right)  ^{T}=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{j},\ldots,n}^{1,2,\ldots,\widehat{i},\ldots
,n}\left(  A^{T}\right)  $. Compared with (\ref{pf.prop.adj.transpose.1}),
this yields%
\[
\left(  A^{T}\right)  _{\sim j,\sim i}=\left(  \underbrace{\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{i},\ldots,n}^{1,2,\ldots,\widehat{j},\ldots
,n}A}_{=A_{\sim i,\sim j}}\right)  ^{T}=\left(  A_{\sim i,\sim j}\right)
^{T}.
\]
Hence,%
\begin{equation}
\det\left(  \underbrace{\left(  A^{T}\right)  _{\sim j,\sim i}}_{=\left(
A_{\sim i,\sim j}\right)  ^{T}}\right)  =\det\left(  \left(  A_{\sim i,\sim
j}\right)  ^{T}\right)  =\det\left(  A_{\sim i,\sim j}\right)
\label{pf.prop.adj.transpose.4}%
\end{equation}
(by Exercise \ref{exe.ps4.4}, applied to $n-1$ and $A_{\sim i,\sim j}$ instead
of $n$ and $A$).

Let us now forget that we fixed $i$ and $j$. We thus have shown that
(\ref{pf.prop.adj.transpose.4}) holds for every $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $.

Now, $\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(
A_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, and thus
the definition of the transpose of a matrix shows that%
\[
\left(  \operatorname*{adj}A\right)  ^{T}=\left(  \underbrace{\left(
-1\right)  ^{j+i}}_{=\left(  -1\right)  ^{i+j}}\det\left(  A_{\sim i,\sim
j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \left(
-1\right)  ^{i+j}\det\left(  A_{\sim i,\sim j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}.
\]
Compared with%
\begin{align*}
\operatorname*{adj}\left(  A^{T}\right)   &  =\left(  \left(  -1\right)
^{i+j}\underbrace{\det\left(  \left(  A^{T}\right)  _{\sim j,\sim i}\right)
}_{\substack{=\det\left(  A_{\sim i,\sim j}\right)  \\\text{(by
(\ref{pf.prop.adj.transpose.4}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{adj}%
\left(  A^{T}\right)  \right) \\
&  =\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim i,\sim j}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n},
\end{align*}
this yields $\operatorname*{adj}\left(  A^{T}\right)  =\left(
\operatorname*{adj}A\right)  ^{T}$. This proves Proposition
\ref{prop.adj.transpose}.
\end{proof}

The most important property of adjugates, however, is the following fact:

\begin{theorem}
\label{thm.adj.inverse}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Then,%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
(Recall that $I_{n}$ denotes the $n\times n$ identity matrix. Expressions such
as $\operatorname*{adj}A\cdot A$ and $\det A\cdot I_{n}$ have to be understood
as $\left(  \operatorname*{adj}A\right)  \cdot A$ and $\left(  \det A\right)
\cdot I_{n}$, respectively.)
\end{theorem}

\begin{example}
Recall that the adjugate of a $2\times2$-matrix is given by the formula
$\operatorname*{adj}\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  $. Thus, Theorem \ref{thm.adj.inverse} (applied to $n=2$) yields%
\[
\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \cdot I_{2}.
\]
(Of course, $\det\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \cdot I_{2}=\left(  ad-bc\right)  \cdot I_{2}=\left(
\begin{array}
[c]{cc}%
ad-bc & 0\\
0 & ad-bc
\end{array}
\right)  $.)
\end{example}

\begin{vershort}
\begin{proof}
[Proof of Theorem \ref{thm.adj.inverse}.]For any two objects $i$ and $j$, we
define $\delta_{i,j}$ to be the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (by the definition of $I_{n}$), and thus%
\begin{equation}
\det A\cdot\underbrace{I_{n}}_{=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}=\det A\cdot\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{pf.thm.adj.inverse.short.R}%
\end{equation}


On the other hand, let us write the matrix $A$ in the form $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, the definition of the
product of two matrices shows that%
\begin{align}
&  A\cdot\operatorname*{adj}A\nonumber\\
&  =\left(  \sum_{k=1}^{n}a_{i,k}\left(  -1\right)  ^{k+j}\det\left(  A_{\sim
j,\sim k}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }\operatorname*{adj}A=\left(  \left(
-1\right)  ^{i+j}\det\left(  A_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\left(  \sum_{q=1}^{n}\underbrace{a_{i,q}\left(  -1\right)  ^{q+j}%
}_{=\left(  -1\right)  ^{q+j}a_{i,q}}\det\left(  A_{\sim j,\sim q}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }q\right) \nonumber\\
&  =\left(  \sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim
j,\sim q}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\label{pf.thm.adj.inverse.short.L}%
\end{align}


Now, we claim that%
\begin{equation}
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =\det A\cdot\delta_{i,j} \label{pf.thm.adj.inverse.short.twise}%
\end{equation}
for any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

\textit{Proof of (\ref{pf.thm.adj.inverse.short.twise}):} Fix $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. We are in one of the
following two cases:

\textit{Case 1:} We have $i=j$.

\textit{Case 2:} We have $i\neq j$.

Let us consider Case 1 first. In this case, we have $i=j$. Hence,
$\delta_{i,j}=1$. Now, Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to
$p=i$) yields%
\[
\det A=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{i+q}}%
_{\substack{=\left(  -1\right)  ^{q+i}=\left(  -1\right)  ^{q+j}\\\text{(since
}i=j\text{)}}}a_{i,q}\det\left(  \underbrace{A_{\sim i,\sim q}}%
_{\substack{=A_{\sim j,\sim q}\\\text{(since }i=j\text{)}}}\right)
=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  .
\]
In view of $\det A\cdot\underbrace{\delta_{i,j}}_{=1}=\det A$, this rewrites
as
\[
\det A\cdot\delta_{i,j}=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}%
\det\left(  A_{\sim j,\sim q}\right)  .
\]
Thus, (\ref{pf.thm.adj.inverse.short.twise}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $i\neq j$. Hence,
$\delta_{i,j}=0$ and $j\neq i$. Now, Proposition \ref{prop.laplace.0}
\textbf{(a)} (applied to $p=j$ and $r=i$) yields%
\[
0=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{j+q}}_{=\left(  -1\right)
^{q+j}}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  =\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  .
\]
In view of $\det A\cdot\underbrace{\delta_{i,j}}_{=0}=0$, this rewrites as%
\[
\det A\cdot\delta_{i,j}=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}%
\det\left(  A_{\sim j,\sim q}\right)  .
\]
Thus, (\ref{pf.thm.adj.inverse.short.twise}) is proven in Case 2.

We have now proven (\ref{pf.thm.adj.inverse.short.twise}) in each of the two
Cases 1 and 2. Thus, (\ref{pf.thm.adj.inverse.short.twise}) is proven.

Now, (\ref{pf.thm.adj.inverse.short.L}) becomes%
\begin{align}
A\cdot\operatorname*{adj}A  &  =\left(  \underbrace{\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  }%
_{\substack{=\det A\cdot\delta_{i,j}\\\text{(by
(\ref{pf.thm.adj.inverse.short.twise}))}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\nonumber\\
&  =\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\det A\cdot I_{n} \label{pf.thm.adj.inverse.short.part1}%
\end{align}
(by (\ref{pf.thm.adj.inverse.short.R})).

It now remains to prove that $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$.
One way to do this is by mimicking the above proof using Theorem
\ref{thm.laplace.gen} \textbf{(b)} and Proposition \ref{prop.laplace.0}
\textbf{(b)} instead of Theorem \ref{thm.laplace.gen} \textbf{(a)} and
Proposition \ref{prop.laplace.0} \textbf{(a)}. However, here is a slicker proof:

Let us forget that we fixed $A$. We thus have shown that
(\ref{pf.thm.adj.inverse.short.part1}) holds for every $n\times n$-matrix $A$.

Now, let $A$ be any $n\times n$-matrix. Then, we can apply
(\ref{pf.thm.adj.inverse.short.part1}) to $A^{T}$ instead of $A$. We thus
obtain%
\begin{equation}
A^{T}\cdot\operatorname*{adj}\left(  A^{T}\right)  =\underbrace{\det\left(
A^{T}\right)  }_{\substack{=\det A\\\text{(by Exercise \ref{exe.ps4.4})}%
}}\cdot I_{n}=\det A\cdot I_{n}. \label{pf.thm.adj.inverse.short.4}%
\end{equation}


But here are four fundamental properties of transposes which are all easy to check:

\begin{itemize}
\item If $u$, $v$ and $w$ are three nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $Q$ is a $v\times w$-matrix, then%
\begin{equation}
\left(  PQ\right)  ^{T}=Q^{T}P^{T}.
\label{pf.thm.adj.inverse.short.tranposes1}%
\end{equation}


\item Every $u\in\mathbb{N}$ satisfies%
\begin{equation}
\left(  I_{u}\right)  ^{T}=I_{u}. \label{pf.thm.adj.inverse.short.tranposes2}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $\lambda\in\mathbb{K}$, then%
\begin{equation}
\left(  \lambda P\right)  ^{T}=\lambda P^{T}.
\label{pf.thm.adj.inverse.short.tranposes3}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, and if $P$ is a $u\times
v$-matrix, then%
\begin{equation}
\left(  P^{T}\right)  ^{T}=P. \label{pf.thm.adj.inverse.short.tranposes4}%
\end{equation}

\end{itemize}

Now, (\ref{pf.thm.adj.inverse.short.tranposes1}) (applied to $u=n$, $v=n$,
$w=n$, $P=\operatorname*{adj}A$ and $Q=A$) shows that%
\[
\left(  \operatorname*{adj}A\cdot A\right)  ^{T}=A^{T}\cdot\underbrace{\left(
\operatorname*{adj}A\right)  ^{T}}_{\substack{=\operatorname*{adj}\left(
A^{T}\right)  \\\text{(by Proposition \ref{prop.adj.transpose})}}}=A^{T}%
\cdot\operatorname*{adj}\left(  A^{T}\right)  =\det A\cdot I_{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.adj.inverse.short.4}%
)}\right)  .
\]
Hence,%
\begin{align*}
\left(  \underbrace{\left(  \operatorname*{adj}A\cdot A\right)  ^{T}}_{=\det
A\cdot I_{n}}\right)  ^{T}  &  =\left(  \det A\cdot I_{n}\right)  ^{T}=\det
A\cdot\underbrace{\left(  I_{n}\right)  ^{T}}_{\substack{=I_{n}\\\text{(by
(\ref{pf.thm.adj.inverse.short.tranposes2}), applied to }u=n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.adj.inverse.short.tranposes3}), applied to }u=n\text{,
}v=n\text{, }P=I_{n}\text{ and }\lambda=\det A\right) \\
&  =\det A\cdot I_{n}.
\end{align*}
Compared with
\[
\left(  \left(  \operatorname*{adj}A\cdot A\right)  ^{T}\right)
^{T}=\operatorname*{adj}A\cdot A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.adj.inverse.short.tranposes4}), applied to }u=n\text{, }v=n\text{
and }P=\operatorname*{adj}A\cdot A\right)  ,
\]
this yields $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$. Combined with
(\ref{pf.thm.adj.inverse.short.part1}), this yields%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
This proves Theorem \ref{thm.adj.inverse}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Theorem \ref{thm.adj.inverse}.]For any two objects $i$ and $j$, we
define $\delta_{i,j}$ to be the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (by the definition of $I_{n}$), and thus%
\begin{equation}
\det A\cdot\underbrace{I_{n}}_{=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}=\det A\cdot\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{pf.thm.adj.inverse.R}%
\end{equation}


On the other hand, let us write the matrix $A$ in the form $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, the definition of the
product of two matrices shows that%
\begin{align}
A\cdot\operatorname*{adj}A  &  =\left(  \underbrace{\sum_{k=1}^{n}%
a_{i,k}\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
}_{\substack{=\sum_{q=1}^{n}a_{i,q}\left(  -1\right)  ^{q+j}\det\left(
A_{\sim j,\sim q}\right)  \\\text{(here, we renamed the summation index
}k\text{ as }q\text{)}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }\operatorname*{adj}A=\left(  \left(
-1\right)  ^{i+j}\det\left(  A_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\left(  \sum_{q=1}^{n}\underbrace{a_{i,q}\left(  -1\right)  ^{q+j}%
}_{=\left(  -1\right)  ^{q+j}a_{i,q}}\det\left(  A_{\sim j,\sim q}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  =\left(  \sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim
j,\sim q}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\label{pf.thm.adj.inverse.L}%
\end{align}


Now, we claim that%
\begin{equation}
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =\det A\cdot\delta_{i,j} \label{pf.thm.adj.inverse.twise}%
\end{equation}
for any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

\textit{Proof of (\ref{pf.thm.adj.inverse.twise}):} Fix $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Thus, $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We are in one of the
following two cases:

\textit{Case 1:} We have $i=j$.

\textit{Case 2:} We have $i\neq j$.

Let us consider Case 1 first. In this case, we have $i=j$. Hence,
$\delta_{i,j}=1$. Now, Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to
$p=i$) yields%
\[
\det A=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{i+q}}%
_{\substack{=\left(  -1\right)  ^{q+i}=\left(  -1\right)  ^{q+j}\\\text{(since
}i=j\text{)}}}a_{i,q}\det\left(  \underbrace{A_{\sim i,\sim q}}%
_{\substack{=A_{\sim j,\sim q}\\\text{(since }i=j\text{)}}}\right)
=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  .
\]
Hence,%
\[
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =\det A=\det A\cdot\delta_{i,j}%
\]
(since $\det A\cdot\underbrace{\delta_{i,j}}_{=1}=\det A$). Thus,
(\ref{pf.thm.adj.inverse.twise}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $i\neq j$. Hence,
$\delta_{i,j}=0$ and $j\neq i$. Now, Proposition \ref{prop.laplace.0}
\textbf{(a)} (applied to $p=j$ and $r=i$) yields%
\[
0=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{j+q}}_{=\left(  -1\right)
^{q+j}}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  =\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  .
\]
Hence,%
\[
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =0=\det A\cdot\delta_{i,j}%
\]
(since $\det A\cdot\underbrace{\delta_{i,j}}_{=0}=0$). Thus,
(\ref{pf.thm.adj.inverse.twise}) is proven in Case 2.

We have now proven (\ref{pf.thm.adj.inverse.twise}) in each of the two Cases 1
and 2. Thus, (\ref{pf.thm.adj.inverse.twise}) always holds. This completes the
proof of (\ref{pf.thm.adj.inverse.twise}).

Now, (\ref{pf.thm.adj.inverse.L}) becomes%
\begin{align}
A\cdot\operatorname*{adj}A  &  =\left(  \underbrace{\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  }%
_{\substack{=\det A\cdot\delta_{i,j}\\\text{(by
(\ref{pf.thm.adj.inverse.twise}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\nonumber\\
&  =\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\det A\cdot I_{n} \label{pf.thm.adj.inverse.part1}%
\end{align}
(by (\ref{pf.thm.adj.inverse.R})).

It now remains to prove that $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$.
One way to do this is by mimicking the above proof using Theorem
\ref{thm.laplace.gen} \textbf{(b)} and Proposition \ref{prop.laplace.0}
\textbf{(b)} instead of Theorem \ref{thm.laplace.gen} \textbf{(a)} and
Proposition \ref{prop.laplace.0} \textbf{(a)}. However, here is a slicker proof:

Let us forget that we fixed $A$. We thus have shown that
(\ref{pf.thm.adj.inverse.part1}) holds for every $n\times n$-matrix $A$.

Now, let $A$ be any $n\times n$-matrix. Then, we can apply
(\ref{pf.thm.adj.inverse.part1}) to $A^{T}$ instead of $A$. We thus obtain%
\begin{equation}
A^{T}\cdot\operatorname*{adj}\left(  A^{T}\right)  =\underbrace{\det\left(
A^{T}\right)  }_{\substack{=\det A\\\text{(by Exercise \ref{exe.ps4.4})}%
}}\cdot I_{n}=\det A\cdot I_{n}. \label{pf.thm.adj.inverse.4}%
\end{equation}


But here are four fundamental properties of transposes which are all easy to check:

\begin{itemize}
\item If $u$, $v$ and $w$ are three nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $Q$ is a $v\times w$-matrix, then%
\begin{equation}
\left(  PQ\right)  ^{T}=Q^{T}P^{T}. \label{pf.thm.adj.inverse.tranposes1}%
\end{equation}


\item Every $u\in\mathbb{N}$ satisfies%
\begin{equation}
\left(  I_{u}\right)  ^{T}=I_{u}. \label{pf.thm.adj.inverse.tranposes2}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $\lambda\in\mathbb{K}$, then%
\begin{equation}
\left(  \lambda P\right)  ^{T}=\lambda P^{T}.
\label{pf.thm.adj.inverse.tranposes3}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, and if $P$ is a $u\times
v$-matrix, then%
\begin{equation}
\left(  P^{T}\right)  ^{T}=P. \label{pf.thm.adj.inverse.tranposes4}%
\end{equation}

\end{itemize}

Now, (\ref{pf.thm.adj.inverse.tranposes1}) (applied to $u=n$, $v=n$, $w=n$,
$P=\operatorname*{adj}A$ and $Q=A$) shows that%
\[
\left(  \operatorname*{adj}A\cdot A\right)  ^{T}=A^{T}\cdot\underbrace{\left(
\operatorname*{adj}A\right)  ^{T}}_{\substack{=\operatorname*{adj}\left(
A^{T}\right)  \\\text{(by Proposition \ref{prop.adj.transpose})}}}=A^{T}%
\cdot\operatorname*{adj}\left(  A^{T}\right)  =\det A\cdot I_{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.adj.inverse.4})}\right)  .
\]
Hence,%
\begin{align*}
\left(  \underbrace{\left(  \operatorname*{adj}A\cdot A\right)  ^{T}}_{=\det
A\cdot I_{n}}\right)  ^{T}  &  =\left(  \det A\cdot I_{n}\right)  ^{T}=\det
A\cdot\underbrace{\left(  I_{n}\right)  ^{T}}_{\substack{=I_{n}\\\text{(by
(\ref{pf.thm.adj.inverse.tranposes2}), applied to }u=n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.adj.inverse.tranposes3}),
applied to }u=n\text{, }v=n\text{, }P=I_{n}\text{ and }\lambda=\det A\right)
\\
&  =\det A\cdot I_{n}.
\end{align*}
Compared with
\[
\left(  \left(  \operatorname*{adj}A\cdot A\right)  ^{T}\right)
^{T}=\operatorname*{adj}A\cdot A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.adj.inverse.tranposes4}), applied to }u=n\text{, }v=n\text{ and
}P=\operatorname*{adj}A\cdot A\right)  ,
\]
this yields $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$. Combined with
(\ref{pf.thm.adj.inverse.part1}), this yields%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
This proves Theorem \ref{thm.adj.inverse}.
\end{proof}
\end{verlong}

The following is a simple consequence of Theorem \ref{thm.adj.inverse}:

\begin{corollary}
\label{cor.adj.kernel}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Let $v$ be a column vector of length $n$. If $Av=0_{n\times1}$, then $\det
A\cdot v=0_{n\times1}$.

(Recall that $0_{n\times1}$ denotes the $n\times1$ zero matrix, i.e., the
column vector of length $n$ whose all entries are $0$.)
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.adj.kernel}.]Assume that $Av=0_{n\times1}$. It is
easy to see that every $m\in\mathbb{N}$ and every $n\times m$-matrix $B$
satisfy $I_{n}B=B$. Applying this to $m=1$ and $B=v$, we obtain $I_{n}v=v$.

It is also easy to see that every $m\in\mathbb{N}$ and every $m\times
n$-matrix $B$ satisfy $B\cdot0_{n\times1}=0_{n\times1}$. Applying this to
$m=n$ and $B=\operatorname*{adj}A$, we obtain $\operatorname*{adj}%
A\cdot0_{n\times1}=0_{n\times1}$.

Now, Theorem \ref{thm.adj.inverse} yields $\operatorname*{adj}A\cdot A=\det
A\cdot I_{n}$. Hence,%
\[
\underbrace{\left(  \operatorname*{adj}A\cdot A\right)  }_{=\det A\cdot I_{n}%
}v=\left(  \det A\cdot I_{n}\right)  v=\det A\cdot\underbrace{\left(
I_{n}v\right)  }_{=v}=\det A\cdot v.
\]
Compared to%
\begin{align*}
\left(  \operatorname*{adj}A\cdot A\right)  v  &  =\operatorname*{adj}%
A\cdot\underbrace{\left(  Av\right)  }_{=0_{n\times1}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since matrix multiplication is
associative}\right) \\
&  =\operatorname*{adj}A\cdot0_{n\times1}=0_{n\times1},
\end{align*}
this yields $\det A\cdot v=0_{n\times1}$. This proves Corollary
\ref{cor.adj.kernel}.
\end{proof}

\begin{exercise}
\label{exe.adj(AB)}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two $n\times
n$-matrices. Prove that%
\[
\operatorname*{adj}\left(  AB\right)  =\operatorname*{adj}B\cdot
\operatorname*{adj}A.
\]

\end{exercise}

\subsection{Inverting matrices}

We now will study inverses of matrices. We begin with a definition:

\begin{definition}
\label{def.matrices.inverses}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A$ be an $n\times m$-matrix.

\textbf{(a)} A \textit{left inverse} of $A$ means an $m\times n$-matrix $L$
such that $LA=I_{m}$. We say that the matrix $A$ is \textit{left-invertible}
if and only if a left inverse of $A$ exists.

\textbf{(b)} A \textit{right inverse} of $A$ means an $m\times n$-matrix $R$
such that $AR=I_{n}$. We say that the matrix $A$ is \textit{right-invertible}
if and only if a right inverse of $A$ exists.

\textbf{(c)} An \textit{inverse} of $A$ (or \textit{two-sided inverse} of $A$)
means an $m\times n$-matrix $B$ such that $BA=I_{m}$ and $AB=I_{n}$. We say
that the matrix $A$ is \textit{invertible} if and only if an inverse of $A$ exists.

The notions \textquotedblleft left-invertible\textquotedblright,
\textquotedblleft right-invertible\textquotedblright\ and \textquotedblleft
invertible\textquotedblright\ depend on the ring $\mathbb{K}$. We shall
therefore speak of \textquotedblleft left-invertible over $\mathbb{K}%
$\textquotedblright, \textquotedblleft right-invertible over $\mathbb{K}%
$\textquotedblright\ and \textquotedblleft invertible over $\mathbb{K}%
$\textquotedblright\ whenever the context does not unambiguously determine
$\mathbb{K}$.
\end{definition}

The notions of \textquotedblleft left inverse\textquotedblright,
\textquotedblleft right inverse\textquotedblright\ and \textquotedblleft
inverse\textquotedblright\ are not interchangeable (unlike for elements in a
commutative ring). We shall soon see in what cases they are identical; but
first, let us give a few examples.

\begin{example}
\label{exa.matrices.inverses}For this example, set $\mathbb{K}=\mathbb{Z}$.

Let $P$ be the $1\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 2
\end{array}
\right)  $. The matrix $P$ is right-invertible. For instance, $\left(
\begin{array}
[c]{c}%
-1\\
1
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{c}%
3\\
-1
\end{array}
\right)  $ are two right inverses of $P$ (because $P\left(
\begin{array}
[c]{c}%
-1\\
1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  =I_{1}$ and $P\left(
\begin{array}
[c]{c}%
3\\
-1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  =I_{1}$). This example shows that the right inverse of a matrix is
not always unique.

The $2\times1$-matrix $P^{T}=\left(
\begin{array}
[c]{c}%
1\\
2
\end{array}
\right)  $ is left-invertible. The left inverses of $P^{T}$ are the transposes
of the right inverses of $P$.

The matrix $P$ is not left-invertible; the matrix $P^{T}$ is not right-invertible.

Let $Q$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & -1\\
3 & -2
\end{array}
\right)  $. The matrix $Q$ is invertible. Its inverse is $\left(
\begin{array}
[c]{cc}%
-2 & 1\\
-3 & 1
\end{array}
\right)  $ (since $\left(
\begin{array}
[c]{cc}%
-2 & 1\\
-3 & 1
\end{array}
\right)  Q=I_{2}$ and $Q\left(
\begin{array}
[c]{cc}%
-2 & 1\\
-3 & 1
\end{array}
\right)  =I_{2}$). It is not hard to see that this is its only inverse.

Let $R$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 2\\
2 & -1
\end{array}
\right)  $. It can be seen that this matrix is not invertible \textbf{as a
matrix over }$\mathbb{Z}$. On the other hand, if we consider it as a matrix
over $\mathbb{K}=\mathbb{Q}$ instead, then it is invertible, with inverse
$\left(
\begin{array}
[c]{cc}%
1/5 & 2/5\\
2/5 & -1/5
\end{array}
\right)  $.
\end{example}

Of course, any inverse of a matrix $A$ is automatically both a left inverse of
$A$ and a right inverse of $A$. Thus, an invertible matrix $A$ is
automatically both left-invertible and right-invertible.

The following simple fact is an analogue of Proposition
\ref{prop.rings.inverse-uni}:

\begin{proposition}
\label{prop.matrices.inverse-uni}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an $n\times m$-matrix. Let $L$ be a left inverse of $A$. Let $R$ be
a right inverse of $A$.

\textbf{(a)} We have $L=R$.

\textbf{(b)} The matrix $A$ is invertible, and $L=R$ is an inverse of $A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.matrices.inverse-uni}.]We know that $L$ is a
left inverse of $A$. In other words, $L$ is an $m\times n$-matrix such that
$LA=I_{m}$ (by the definition of a \textquotedblleft left
inverse\textquotedblright).

We know that $R$ is a right inverse of $A$. In other words, $R$ is an $m\times
n$-matrix such that $AR=I_{n}$ (by the definition of a \textquotedblleft right
inverse\textquotedblright).

Now, recall that $I_{m}G=G$ for every $k\in\mathbb{N}$ and every $m\times
k$-matrix $G$. Applying this to $k=n$ and $G=R$, we obtain $I_{m}R=R$.

Also, recall that $GI_{n}=G$ for every $k\in\mathbb{N}$ and every $k\times
n$-matrix $G$. Applying this to $k=m$ and $G=L$, we obtain $LI_{n}=L$. Thus,
$L=L\underbrace{I_{n}}_{=AR}=\underbrace{LA}_{=I_{m}}R=I_{m}R=R$. This proves
Proposition \ref{prop.matrices.inverse-uni} \textbf{(a)}.

\textbf{(b)} We have $LA=I_{m}$ and $A\underbrace{L}_{=R}=AR=I_{n}$. Thus, $L$
is an $m\times n$-matrix such that $LA=I_{m}$ and $AL=I_{n}$. In other words,
$L$ is an inverse of $A$ (by the definition of an \textquotedblleft
inverse\textquotedblright). Thus, $L=R$ is an inverse of $A$ (since $L=R$).
This proves Proposition \ref{prop.matrices.inverse-uni} \textbf{(b)}.
\end{proof}

\begin{corollary}
\label{cor.matrices.inverse-uni.cor}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an $n\times m$-matrix.

\textbf{(a)} If $A$ is left-invertible and right-invertible, then $A$ is invertible.

\textbf{(b)} If $A$ is invertible, then there exists exactly one inverse of
$A$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.matrices.inverse-uni.cor}.]\textbf{(a)} Assume
that $A$ is left-invertible and right-invertible. Thus, $A$ has a left inverse
$L$ (since $A$ is left-invertible). Consider this $L$. Also, $A$ has a right
inverse $R$ (since $A$ is right-invertible). Consider this $R$. Proposition
\ref{prop.matrices.inverse-uni} \textbf{(b)} yields that the matrix $A$ is
invertible, and $L=R$ is an inverse of $A$. Corollary
\ref{cor.matrices.inverse-uni.cor} \textbf{(a)} is proven.

\textbf{(b)} Assume that $A$ is invertible. Let $B$ and $B^{\prime}$ be any
two inverses of $A$. Since $B$ is an inverse of $A$, we know that $B$ is an
$m\times n$-matrix such that $BA=I_{m}$ and $AB=I_{n}$ (by the definition of
an \textquotedblleft inverse\textquotedblright). Thus, in particular, $B$ is
an $m\times n$-matrix such that $BA=I_{m}$. In other words, $B$ is a left
inverse of $A$. Since $B^{\prime}$ is an inverse of $A$, we know that
$B^{\prime}$ is an $m\times n$-matrix such that $B^{\prime}A=I_{m}$ and
$AB^{\prime}=I_{n}$ (by the definition of an \textquotedblleft
inverse\textquotedblright). Thus, in particular, $B^{\prime}$ is an $m\times
n$-matrix such that $AB^{\prime}=I_{n}$. In other words, $B^{\prime}$ is a
right inverse of $A$. Now, Proposition \ref{prop.matrices.inverse-uni}
\textbf{(a)} (applied to $L=B$ and $R=B^{\prime}$) shows that $B=B^{\prime}$.

Let us now forget that we fixed $B$ and $B^{\prime}$. We thus have shown that
if $B$ and $B^{\prime}$ are two inverses of $A$, then $B=B^{\prime}$. In other
words, any two inverses of $A$ are equal. In other words, there exists at most
one inverse of $A$. Since we also know that there exists at least one inverse
of $A$ (since $A$ is invertible), we thus conclude that there exists exactly
one inverse of $A$. This proves Corollary \ref{cor.matrices.inverse-uni.cor}
\textbf{(b)}.
\end{proof}

\begin{definition}
Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be an invertible $n\times
m$-matrix. Corollary \ref{cor.matrices.inverse-uni.cor} \textbf{(b)} shows
that there exists exactly one inverse of $A$. Thus, we can speak of
\textquotedblleft\textit{the inverse of }$A$\textquotedblright. We denote this
inverse by $A^{-1}$.
\end{definition}

In contrast to Definition \ref{def.rings.invertible}, we do \textbf{not}
define the notation $B/A$ for two matrices $B$ and $A$ for which $A$ is
invertible. In fact, the trouble with such a notation would be its ambiguity:
should it mean $BA^{-1}$ or $A^{-1}B$ ? (In general, $BA^{-1}$ and $A^{-1}B$
are not the same.) Some authors do write $B/A$ for the matrices $BA^{-1}$ and
$A^{-1}B$ when these matrices are equal; but we shall not have a reason to do so.

Example \ref{exa.matrices.inverses} (and your experiences with a linear
algebra class, if you have taken one) suggest the conjecture that only square
matrices can be invertible. Indeed, this is \textbf{almost} true. There is a
stupid counterexample: If $\mathbb{K}$ is a trivial ring, then every matrix
over $\mathbb{K}$ is invertible\footnote{For example, the $1\times2$-matrix
$\left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)  $ over a trivial ring $\mathbb{K}$ is invertible, having inverse
$\left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}\\
0_{\mathbb{K}}%
\end{array}
\right)  $. If you don't believe me, just check that%
\begin{align*}
\left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}\\
0_{\mathbb{K}}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}\\
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1_{\mathbb{K}} & 0_{\mathbb{K}}\\
0_{\mathbb{K}} & 1_{\mathbb{K}}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0_{\mathbb{K}}%
=1_{\mathbb{K}}\right) \\
&  =I_{2}%
\end{align*}
and $\left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}\\
0_{\mathbb{K}}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1_{\mathbb{K}}%
\end{array}
\right)  =I_{1}$.}. It turns out that this is the only case where nonsquare
matrices can be invertible. Indeed, we have the following:

\begin{theorem}
\label{thm.matrices.inverses.oblong}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an $n\times m$-matrix.

\textbf{(a)} If $A$ is left-invertible and if $n<m$, then $\mathbb{K}$ is a
trivial ring.

\textbf{(b)} If $A$ is right-invertible and if $n>m$, then $\mathbb{K}$ is a
trivial ring.

\textbf{(c)} If $A$ is invertible and if $n\neq m$, then $\mathbb{K}$ is a
trivial ring.
\end{theorem}

\begin{proof}
[Proof of Theorem \ref{thm.matrices.inverses.oblong}.]\textbf{(a)} Assume that
$A$ is left-invertible, and that $n<m$.

The matrix $A$ has a left inverse $L$ (since it is left-invertible). Consider
this $L$.

We know that $L$ is a left inverse of $A$. In other words, $L$ is an $m\times
n$-matrix such that $LA=I_{m}$ (by the definition of a \textquotedblleft left
inverse\textquotedblright). But (\ref{eq.exam.cauchy-binet.0}) (applied to
$m$, $n$, $L$ and $A$ instead of $n$, $m$, $A$ and $B$) yields $\det\left(
LA\right)  =0$ (since $n<m$). Thus, $0=\det\left(  \underbrace{LA}_{=I_{m}%
}\right)  =\det\left(  I_{m}\right)  =1$. Of course, the $0$ and the $1$ in
this equality mean the elements $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$ of
$\mathbb{K}$ (rather than the integers $0$ and $1$); thus, it rewrites as
$0_{\mathbb{K}}=1_{\mathbb{K}}$. In other words, $\mathbb{K}$ is a trivial
ring. This proves Theorem \ref{thm.matrices.inverses.oblong} \textbf{(a)}.

\textbf{(b)} Assume that $A$ is right-invertible, and that $n>m$.

The matrix $A$ has a right inverse $R$ (since it is right-invertible).
Consider this $R$.

We know that $R$ is a right inverse of $A$. In other words, $R$ is an $m\times
n$-matrix such that $AR=I_{n}$ (by the definition of a \textquotedblleft right
inverse\textquotedblright). But (\ref{eq.exam.cauchy-binet.0}) (applied to
$B=R$) yields $\det\left(  AR\right)  =0$ (since $m<n$). Thus, $0=\det\left(
\underbrace{AR}_{=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$. Of course,
the $0$ and the $1$ in this equality mean the elements $0_{\mathbb{K}}$ and
$1_{\mathbb{K}}$ of $\mathbb{K}$ (rather than the integers $0$ and $1$); thus,
it rewrites as $0_{\mathbb{K}}=1_{\mathbb{K}}$. In other words, $\mathbb{K}$
is a trivial ring. This proves Theorem \ref{thm.matrices.inverses.oblong}
\textbf{(b)}.

\textbf{(c)} Assume that $A$ is invertible, and that $n\neq m$. Since $n\neq
m$, we must be in one of the following two cases:

\textit{Case 1:} We have $n<m$.

\textit{Case 2:} We have $n>m$.

Let us first consider Case 1. In this case, we have $n<m$. Now, $A$ is
invertible, and thus left-invertible (since every invertible matrix is
left-invertible). Hence, $\mathbb{K}$ is a trivial ring (according to Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(a)}). Thus, Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(c)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $n>m$. Now, $A$ is
invertible, and thus right-invertible (since every invertible matrix is
right-invertible). Hence, $\mathbb{K}$ is a trivial ring (according to Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(b)}). Thus, Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(c)} is proven in Case 2.

We have thus proven Theorem \ref{thm.matrices.inverses.oblong} \textbf{(c)} in
both Cases 1 and 2. Thus, Theorem \ref{thm.matrices.inverses.oblong}
\textbf{(c)} always holds.
\end{proof}

Theorem \ref{thm.matrices.inverses.oblong} \textbf{(c)} says that the question
whether a matrix is invertible is only interesting for square matrices, unless
the ring $\mathbb{K}$ is given so inexplicitly that we do not know whether it
is trivial or not\footnote{This actually happens rather often in algebra! For
example, rings are often defined by \textquotedblleft generators and
relations\textquotedblright\ (such as \textquotedblleft the ring with
commuting generators $a,b,c$ subject to the relations $a^{2}+b^{2}=c^{2}$ and
$ab=c$\textquotedblright). Sometimes the relations force the ring to become
trivial (for instance, the ring with generator $a$ and relations $a=1$ and
$a^{2}=2$ is clearly the trivial ring, because in this ring we have
$2=a^{2}=1^{2}=1$). Often this is not clear a-priori, and theorems such as
Theorem \ref{thm.matrices.inverses.oblong} can be used to show this. The
triviality of a ring can be a nontrivial statement! (Richman makes this point
in \cite{Richman}.)}. Let us now study the invertibility of a square matrix.
Here, the determinant turns out to be highly useful:

\begin{theorem}
\label{thm.matrices.inverses.square}Let $n\in\mathbb{N}$. Let $A$ be an
$n\times n$-matrix.

\textbf{(a)} The matrix $A$ is invertible if and only if the element $\det A$
of $\mathbb{K}$ is invertible (in $\mathbb{K}$).

\textbf{(b)} If $\det A$ is invertible, then the inverse of $A$ is
$A^{-1}=\dfrac{1}{\det A}\cdot\operatorname*{adj}A$.
\end{theorem}

When $\mathbb{K}$ is a field, the invertible elements of $\mathbb{K}$ are
precisely the nonzero elements of $\mathbb{K}$. Thus, when $\mathbb{K}$ is a
field, the statement of Theorem \ref{thm.matrices.inverses.square}
\textbf{(a)} can be rewritten as \textquotedblleft The matrix $A$ is
invertible if and only if $\det A\neq0$\textquotedblright; this is a
cornerstone of linear algebra. But our statement of Theorem
\ref{thm.matrices.inverses.square} \textbf{(a)} works for an arbitrary
commutative ring $\mathbb{K}$. In particular, it works for $\mathbb{K}%
=\mathbb{Z}$. Here is a consequence:

\begin{corollary}
\label{cor.matrices.inverses.square.ZZ}Let $n\in\mathbb{N}$. Let
$A\in\mathbb{Z}^{n\times n}$ be an $n\times n$-matrix over $\mathbb{Z}$. Then,
the matrix $A$ is invertible if and only if $\det A\in\left\{  1,-1\right\}  $.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.matrices.inverses.square.ZZ}.]If $g$ is an
integer, then $g$ is invertible (in $\mathbb{Z}$) if and only if $g\in\left\{
1,-1\right\}  $. In other words, for every integer $g$, we have the following
equivalence:%
\begin{equation}
\left(  g\text{ is invertible (in }\mathbb{Z}\text{)}\right)
\Longleftrightarrow\left(  g\in\left\{  1,-1\right\}  \right)  .
\label{pf.cor.matrices.inverses.square.ZZ.1}%
\end{equation}


Now, Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} (applied to
$\mathbb{K}=\mathbb{Z}$) yields that the matrix $A$ is invertible if and only
if the element $\det A$ of $\mathbb{Z}$ is invertible (in $\mathbb{Z}$). Thus,
we have the following chain of equivalences:%
\begin{align*}
&  \left(  \text{the matrix }A\text{ is invertible}\right) \\
&  \Longleftrightarrow\ \left(  \det A\text{ is invertible (in }%
\mathbb{Z}\text{)}\right)  \ \Longleftrightarrow\ \left(  \det A\in\left\{
1,-1\right\}  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.cor.matrices.inverses.square.ZZ.1}), applied to }g=\det A\right)  .
\end{align*}
This proves Corollary \ref{cor.matrices.inverses.square.ZZ}.
\end{proof}

Notice that Theorem \ref{thm.matrices.inverses.square} \textbf{(b)} yields an
explicit way to compute the inverse of a square matrix $A$ (provided that we
can compute determinants and the inverse of $\det A$). This is not the fastest
way (at least not when $\mathbb{K}$ is a field), but it is useful for various
theoretical purposes.

\begin{proof}
[Proof of Theorem \ref{thm.matrices.inverses.square}.]\textbf{(a)}
$\Longrightarrow:$\ \ \ \ \footnote{In case you don't know what the notation
\textquotedblleft$\Longrightarrow:$\textquotedblright\ here means:
\par
Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} is an
\textquotedblleft if and only if\textquotedblright\ assertion. In other words,
it asserts that $\mathcal{U}\Longleftrightarrow\mathcal{V}$ for two statements
$\mathcal{U}$ and $\mathcal{V}$. (In our case, $\mathcal{U}$ is the statement
\textquotedblleft the matrix $A$ is invertible\textquotedblright, and
$\mathcal{V}$ is the statement \textquotedblleft the element $\det A$ of
$\mathbb{K}$ is invertible (in $\mathbb{K}$)\textquotedblright.) In order to
prove a statement of the form $\mathcal{U}\Longleftrightarrow\mathcal{V}$, it
is sufficient to prove the implications $\mathcal{U}\Longrightarrow
\mathcal{V}$ and $\mathcal{U}\Longleftarrow\mathcal{V}$. Usually, these two
implications are proven separately (although not always; for instance, in the
proof of Corollary \ref{cor.matrices.inverses.square.ZZ}, we have used a chain
of equivalences to prove $\mathcal{U}\Longleftrightarrow\mathcal{V}$
directly). When writing such a proof, one often uses the abbreviations
\textquotedblleft$\Longrightarrow:$\textquotedblright\ and \textquotedblleft%
$\Longleftarrow:$\textquotedblright\ for \textquotedblleft Here comes the
proof of the implication $\mathcal{U}\Longrightarrow\mathcal{V}$%
:\textquotedblright\ and \textquotedblleft Here comes the proof of the
implication $\mathcal{U}\Longleftarrow\mathcal{V}$:\textquotedblright,
respectively.} Assume that the matrix $A$ is invertible. In other words, an
inverse $B$ of $A$ exists. Consider such a $B$.

The matrix $B$ is an inverse of $A$. In other words, $B$ is an $n\times
n$-matrix such that $BA=I_{n}$ and $AB=I_{n}$ (by the definition of an
\textquotedblleft inverse\textquotedblright). Theorem \ref{thm.det(AB)} yields
$\det\left(  AB\right)  =\det A\cdot\det B$, so that $\det A\cdot\det
B=\det\left(  \underbrace{AB}_{=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$.
Of course, we also have $\det B\cdot\det A=\det A\cdot\det B=1$. Thus, $\det
B$ is an inverse of $\det A$ in $\mathbb{K}$. Therefore, the element $\det A$
is invertible (in $\mathbb{K}$). This proves the $\Longrightarrow$ direction
of Theorem \ref{thm.matrices.inverses.square} \textbf{(a)}.

$\Longleftarrow:$ Assume that the element $\det A$ is invertible (in
$\mathbb{K}$). Thus, its inverse $\dfrac{1}{\det A}$ exists. Theorem
\ref{thm.adj.inverse} yields%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
Now, define an $n\times n$-matrix $B$ by $B=\dfrac{1}{\det A}\cdot
\operatorname*{adj}A$. Then,%
\[
A\underbrace{B}_{=\dfrac{1}{\det A}\cdot\operatorname*{adj}A}=A\cdot\left(
\dfrac{1}{\det A}\cdot\operatorname*{adj}A\right)  =\dfrac{1}{\det A}%
\cdot\underbrace{A\cdot\operatorname*{adj}A}_{=\det A\cdot I_{n}%
}=\underbrace{\dfrac{1}{\det A}\cdot\det A}_{=1}\cdot I_{n}=I_{n}%
\]
and%
\[
\underbrace{B}_{=\dfrac{1}{\det A}\cdot\operatorname*{adj}A}A=\dfrac{1}{\det
A}\cdot\underbrace{\operatorname*{adj}A\cdot A}_{=\det A\cdot I_{n}%
}=\underbrace{\dfrac{1}{\det A}\cdot\det A}_{=1}\cdot I_{n}=I_{n}.
\]


Thus, $B$ is an $n\times n$-matrix such that $BA=I_{n}$ and $AB=I_{n}$. In
other words, $B$ is an inverse of $A$ (by the definition of an
\textquotedblleft inverse\textquotedblright). Thus, an inverse of $A$ exists;
in other words, the matrix $A$ is invertible. This proves the $\Longleftarrow$
direction of Theorem \ref{thm.matrices.inverses.square} \textbf{(a)}.

We have now proven both directions of Theorem
\ref{thm.matrices.inverses.square} \textbf{(a)}. Theorem
\ref{thm.matrices.inverses.square} \textbf{(a)} is thus proven.

\textbf{(b)} Assume that $\det A$ is invertible. Thus, its inverse $\dfrac
{1}{\det A}$ exists. We define an $n\times n$-matrix $B$ by $B=\dfrac{1}{\det
A}\cdot\operatorname*{adj}A$. Then, $B$ is an inverse of $A$%
\ \ \ \ \footnote{We have shown this in our proof of the $\Longleftarrow$
direction of Theorem \ref{thm.matrices.inverses.square} \textbf{(a)}.}. In
other words, $B$ is \textbf{the} inverse of $A$. In other words, $B=A^{-1}$.
Hence, $A^{-1}=B=\dfrac{1}{\det A}\cdot\operatorname*{adj}A$. This proves
Theorem \ref{thm.matrices.inverses.square} \textbf{(b)}.
\end{proof}

\begin{corollary}
\label{cor.matrices.inverse.AB}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
$n\times n$-matrices such that $AB=I_{n}$.

\textbf{(a)} We have $BA=I_{n}$.

\textbf{(b)} The matrix $A$ is invertible, and the matrix $B$ is the inverse
of $A$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.matrices.inverse.AB}.]Theorem \ref{thm.det(AB)}
yields $\det\left(  AB\right)  =\det A\cdot\det B$, so that $\det A\cdot\det
B=\det\left(  \underbrace{AB}_{=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$.
Of course, we also have $\det B\cdot\det A=\det A\cdot\det B=1$. Thus, $\det
B$ is an inverse of $\det A$ in $\mathbb{K}$. Therefore, the element $\det A$
is invertible (in $\mathbb{K}$). Therefore, the matrix $A$ is invertible
(according to the $\Longleftarrow$ direction of Theorem
\ref{thm.matrices.inverses.square} \textbf{(b)}). Thus, the inverse of $A$
exists. Let $C$ be this inverse. Thus, $C$ is a left inverse of $A$ (since
every inverse of $A$ is a left inverse of $A$).

The matrix $B$ is an $n\times n$-matrix satisfying $AB=I_{n}$. In other words,
$B$ is a right inverse of $A$. On the other hand, $C$ is a left inverse of
$A$. Hence, Proposition \ref{prop.matrices.inverse-uni} \textbf{(a)} (applied
to $L=C$ and $R=B$) yields $C=B$. Hence, the matrix $B$ is the inverse of $A$
(since the matrix $C$ is the inverse of $A$). Thus, Corollary
\ref{cor.matrices.inverse.AB} \textbf{(b)} is proven.

Since $B$ is the inverse of $A$, we have $BA=I_{n}$ and $AB=I_{n}$ (by the
definition of an \textquotedblleft inverse\textquotedblright). This proves
Corollary \ref{cor.matrices.inverse.AB} \textbf{(a)}.
\end{proof}

\begin{remark}
Corollary \ref{cor.matrices.inverse.AB} is \textbf{not} obvious! Matrix
multiplication, in general, is not commutative (we have $AB\neq BA$ more often
than not), and there is no reason to expect that $AB=I_{n}$ implies $BA=I_{n}%
$. The fact that this is nevertheless true for square matrices took us quite
some work to prove (we needed, among other things, the notion of an adjugate).
This fact would \textbf{not} hold for rectangular matrices. Nor does it hold
for \textquotedblleft infinite square matrices\textquotedblright: Without
wanting to go into the details of how products of infinite matrices are
defined, I invite you to check that the two infinite matrices $A=\left(
\begin{array}
[c]{cccc}%
0 & 1 & 0 & \cdots\\
0 & 0 & 1 & \cdots\\
0 & 0 & 0 & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{array}
\right)  $ and $B=A^{T}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 0 & \cdots\\
1 & 0 & 0 & \cdots\\
0 & 1 & 0 & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{array}
\right)  $ satisfy $AB=I_{\infty}$ but $BA\neq I_{\infty}$. This makes
Corollary \ref{cor.matrices.inverse.AB} \textbf{(a)} all the more interesting.
\end{remark}

\subsection{\label{sect.noncommring}Noncommutative rings}

I think that here is a good place to introduce two other basic notions from
algebra: that of a noncommutative ring, and that of a group.

\begin{definition}
\label{def.ring}The notion of a \textit{noncommutative ring} is defined in the
same way as we have defined a commutative ring (in Definition
\ref{def.commring}), except that we no longer require the \textquotedblleft
Commutativity of multiplication\textquotedblright\ axiom.
\end{definition}

As I have already said, the word \textquotedblleft
noncommutative\textquotedblright\ (in \textquotedblleft noncommutative
ring\textquotedblright) does not mean that commutativity of multiplication has
to be false in this ring; it only means that commutativity of multiplication
is not required. Thus, every commutative ring is a noncommutative ring.
Therefore, each of the examples of a commutative ring given in Section
\ref{sect.commring} is also an example of a noncommutative ring. Of course, it
is more interesting to see some examples of noncommutative rings which
actually fail to obey commutativity of multiplication. Here are some of these examples:

\begin{itemize}
\item If $n\in\mathbb{N}$ and if $\mathbb{K}$ is a commutative ring, then the
set $\mathbb{K}^{n\times n}$ of matrices becomes a noncommutative ring (when
endowed with the addition and multiplication of matrices, with the zero
$0_{n\times n}$ and with the unity $I_{n}$). This is actually a commutative
ring when $\mathbb{K}$ is trivial or when $n\leq1$, but in all
\textquotedblleft interesting\textquotedblright\ cases it is not commutative.

\item If you have heard of \href{https://en.wikipedia.org/wiki/Quaternion}{the
quaternions}, you should realize that they form a noncommutative ring.

\item Given a commutative ring $\mathbb{K}$ and $n$ distinct symbols
$X_{1},X_{2},\ldots,X_{n}$, we can define a \textit{ring of polynomials in the
\textbf{noncommutative} variables} $X_{1},X_{2},\ldots,X_{n}$ over
$\mathbb{K}$. We do not want to go into the details of its definition at this
point, but let us just mention some examples of its elements: For instance,
the ring of polynomials in the noncommutative variables $X$ and $Y$ over
$\mathbb{Q}$ contains elements such as $1+\dfrac{2}{3}X$, $X^{2}+\dfrac{3}%
{2}Y-7XY+YX$, $2XY$, $2YX$ and $5X^{2}Y-6XYX+7Y^{2}X$ (and of course, the
elements $XY$ and $YX$ are not equal).

\item If $n\in\mathbb{N}$ and if $\mathbb{K}$ is a commutative ring, then the
set of all lower-triangular $n\times n$-matrices over $\mathbb{K}$ becomes a
noncommutative ring (with addition, multiplication, zero and unity defined in
the same way as in $\mathbb{K}^{n\times n}$). This is because the sum and the
product of any two lower-triangular $n\times n$-matrices over $\mathbb{K}$ are
again lower-triangular\footnote{Check this! (For the sum, it is clear, but for
the product, it is an instructive exercise.)}, and because the matrices
$0_{n\times n}$ and $I_{n}$ are lower-triangular).

\item In contrast, the set of all invertible $2\times2$-matrices over
$\mathbb{K}$ is \textbf{not} a noncommutative ring (for example, because the
sum of the two invertible matrices $I_{2}$ and $-I_{2}$ is not invertible).

\item If $\mathbb{K}$ is a commutative ring, then the set of all $3\times
3$-matrices (over $\mathbb{K}$) of the form $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & d & 0\\
0 & 0 & f
\end{array}
\right)  $ (with $a,b,c,d,f\in\mathbb{K}$) is a noncommutative ring (again,
with the same addition, multiplication, zero and unity as for $\mathbb{K}%
^{n\times n}$).\ \ \ \ \footnote{To check this, one needs to prove that the
matrices $0_{3\times3}$ and $I_{3}$ have this form, and that the sum and the
product of any two matrices of this form is again a matrix of this form. All
of this is clear, except for the claim about the product. The latter claim
follows from the computation%
\[
\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & d & 0\\
0 & 0 & f
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & c^{\prime}\\
0 & d^{\prime} & 0\\
0 & 0 & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime} & bd^{\prime}+ab^{\prime} & cf^{\prime}+ac^{\prime}\\
0 & dd^{\prime} & 0\\
0 & 0 & ff^{\prime}%
\end{array}
\right)  .
\]
}

\item On the other hand, if $\mathbb{K}$ is a commutative ring, then the set
of all $3\times3$-matrices (over $\mathbb{K}$) of the form $\left(
\begin{array}
[c]{ccc}%
a & b & 0\\
0 & c & d\\
0 & 0 & f
\end{array}
\right)  $ (with $a,b,c,d,f\in\mathbb{K}$) is \textbf{not} a noncommutative
ring (unless $\mathbb{K}$ is trivial), because products of matrices in this
set are not always in this set\footnote{Indeed, $\left(
\begin{array}
[c]{ccc}%
a & b & 0\\
0 & c & d\\
0 & 0 & f
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & 0\\
0 & c^{\prime} & d^{\prime}\\
0 & 0 & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime} & ab^{\prime}+bc^{\prime} & bd^{\prime}\\
0 & cc^{\prime} & cd^{\prime}+df^{\prime}\\
0 & 0 & ff^{\prime}%
\end{array}
\right)  $ can have $bd^{\prime}\neq0$.}.
\end{itemize}

For the rest of this section, we let $\mathbb{L}$ be a \textbf{noncommutative}
ring. What can we do with elements of $\mathbb{L}$ ? We can do some of the
things that we can do with a commutative ring, but not all of them. For
example, we can still define the sum $a_{1}+a_{2}+\cdots+a_{n}$ and the
product $a_{1}a_{2}\cdots a_{n}$ of $n$ elements of a noncommutative ring. But
we cannot arbitrarily reorder the factors of a product and expect to always
get the same result! (With a sum, we can do this.) We can still define $na$
for any $n\in\mathbb{Z}$ and $a\in\mathbb{L}$ (in the same way as we defined
$na$ for $n\in\mathbb{Z}$ and $a\in\mathbb{K}$ when $\mathbb{K}$ was a
commutative ring). We can still define $a^{n}$ for any $n\in\mathbb{N}$ and
$a\in\mathbb{L}$ (again, in the same fashion as for commutative rings). The
identities (\ref{eq.rings.-(a+b)}), (\ref{eq.rings.-(-a)}),
(\ref{eq.rings.-(ab)}), (\ref{eq.rings.-(na)}), (\ref{eq.rings.nab}),
(\ref{eq.rings.nma}), (\ref{eq.rings.0**n}) and (\ref{eq.rings.a**(n+m)})
still hold when the commutative ring $\mathbb{K}$ is replaced by the
noncommutative ring $\mathbb{L}$; but the identities (\ref{eq.rings.-(ab)**n})
and (\ref{eq.rings.(a+b)**n}) may not (although they \textbf{do} hold if we
additionally assume that $ab=ba$). Finite sums such as $\sum_{s\in S}a_{s}$
(where $S$ is a finite set, and $a_{s}\in\mathbb{L}$ for every $s\in S$) are
well-defined, but finite products such as $\prod_{s\in S}a_{s}$ are not
(unless we specify the order in which their factors are to be multiplied).

We can define matrices over $\mathbb{L}$ in the same way as we have defined
matrices over $\mathbb{K}$. We can even define the determinant of a square
matrix over $\mathbb{L}$ using the formula (\ref{eq.det.eq.1}); however, this
determinant lacks many of the important properties that determinants over
$\mathbb{K}$ have (for instance, it satisfies neither Exercise \ref{exe.ps4.4}
nor Theorem \ref{thm.det(AB)}), and is therefore usually not
studied.\footnote{Some algebraists have come up with subtler notions of
determinants for matrices over noncommutative rings. But I don't want to go in
that direction here.}

We define the notion of an \textit{inverse} of an element $a\in\mathbb{L}$; in
order to do so, we simply replace $\mathbb{K}$ by $\mathbb{L}$ in Definition
\ref{def.rings.inverse}. (Now it suddenly matters that we required both $ab=1$
and $ba=1$ in Definition \ref{def.rings.inverse}.) Proposition
\ref{prop.rings.inverse-uni} still holds (and its proof still works) when
$\mathbb{K}$ is replaced by $\mathbb{L}$.

We define the notion of an \textit{invertible element} of $\mathbb{L}$; in
order to do so, we simply replace $\mathbb{K}$ by $\mathbb{L}$ in Definition
\ref{def.rings.invertible} \textbf{(a)}. We cannot directly replace
$\mathbb{K}$ by $\mathbb{L}$ in Definition \ref{def.rings.invertible}
\textbf{(b)}, because for two invertible elements $a$ and $b$ of $\mathbb{L}$
we do not necessarily have $\left(  ab\right)  ^{-1}=a^{-1}b^{-1}$; but
something very similar holds (namely, $\left(  ab\right)  ^{-1}=b^{-1}a^{-1}%
$). Trying to generalize Definition \ref{def.rings.invertible} \textbf{(c)} to
noncommutative rings is rather hopeless: In general, we cannot bring a
\textquotedblleft noncommutative fraction\textquotedblright\ of the form
$ba^{-1}+dc^{-1}$ to a \textquotedblleft common denominator\textquotedblright.

\begin{example}
\label{exa.rings.invertible-matrices}Let $\mathbb{K}$ be a commutative ring.
Let $n\in\mathbb{N}$. As we know, $\mathbb{K}^{n\times n}$ is a noncommutative
ring. The invertible elements of this ring are exactly the invertible $n\times
n$-matrices. (To see this, just compare the definition of an invertible
element of $\mathbb{K}^{n\times n}$ with the definition of an invertible
$n\times n$-matrix. These definitions are clearly equivalent.)
\end{example}

\subsection{Groups, and the group of units}

Let me finally define the notion of a \textit{group}.

\begin{definition}
\label{def.group}A \textit{group} means a set $G$ endowed with

\begin{itemize}
\item a binary operation called \textquotedblleft
multiplication\textquotedblright\ (or \textquotedblleft
composition\textquotedblright, or just \textquotedblleft binary
operation\textquotedblright), and denoted by $\cdot$, and written infix, and

\item an element called $1_{G}$ (or $e_{G}$)
\end{itemize}

such that the following axioms are satisfied:

\begin{itemize}
\item \textit{Associativity:} We have $a\left(  bc\right)  =\left(  ab\right)
c$ for all $a\in G$, $b\in G$ and $c\in G$. Here and in the following, $ab$ is
shorthand for $a\cdot b$ (as is usual for products of numbers).

\item \textit{Neutrality of }$1$\textit{:} We have $a1_{G}=1_{G}a=a$ for all
$a\in G$.

\item \textit{Existence of inverses:} For every $a\in G$, there exists an
element $a^{\prime}\in G$ such that $aa^{\prime}=a^{\prime}a=1_{G}$. This
$a^{\prime}$ is commonly denoted by $a^{-1}$ and called the \textit{inverse}
of $a$. (It is easy to check that it is unique.)
\end{itemize}
\end{definition}

\begin{definition}
The element $1_{G}$ of a group $G$ is denoted the \textit{neutral element} (or
the \textit{identity}) of $G$.

The binary operation $\cdot$ in Definition \ref{def.group} is usually not
identical with the binary operation $\cdot$ on the set of integers, and is
denoted by $\cdot_{G}$ when confusion can arise.
\end{definition}

The definition of a group has similarities with that of a noncommutative ring.
Viewed from a distance, it may look as if a noncommutative ring would
\textquotedblleft consist\textquotedblright\ of two groups with the same
underlying set. This is not quite correct, though, because the multiplication
in a nontrivial ring does not satisfy the \textquotedblleft existence of
inverses\textquotedblright\ axiom. But it is true that there are two groups in
every noncommutative ring:

\begin{proposition}
\label{prop.ring.groups}Let $\mathbb{L}$ be a noncommutative ring.

\textbf{(a)} The set $\mathbb{L}$, endowed with the \textbf{addition}
$+_{\mathbb{L}}$ (as multiplication) and the element $0_{\mathbb{L}}$ (as
neutral element), is a group. This group is called the \textit{additive group}
of $\mathbb{L}$, and denoted by $\mathbb{L}^{+}$.

\textbf{(b)} Let $\mathbb{L}^{\times}$ denote the set of all invertible
elements of $\mathbb{L}$. Then, the product of two elements of $\mathbb{L}%
^{\times}$ again belongs to $\mathbb{L}^{\times}$. Thus, we can define a
binary operation $\cdot_{\mathbb{L}^{\times}}$ on the set $\mathbb{L}^{\times
}$ (written infix) by
\[
a\cdot_{\mathbb{L}^{\times}}b=ab\ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathbb{L}^{\times}\text{ and }b\in\mathbb{L}^{\times}.
\]


The set $\mathbb{L}^{\times}$, endowed with the multiplication $\cdot
_{\mathbb{L}^{\times}}$ (as multiplication) and the element $1_{\mathbb{L}}$
(as neutral element), is a group. This group is called the \textit{group of
units} of $\mathbb{L}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.ring.groups}.]\textbf{(a)} The addition
$+_{\mathbb{L}}$ is clearly a binary operation on $\mathbb{L}$, and the
element $0_{\mathbb{L}}$ is clearly an element of $\mathbb{L}$. The three
axioms in Definition \ref{def.group} are clearly satisfied for the binary
operation $+_{\mathbb{L}}$ and the element $0_{\mathbb{L}}$%
\ \ \ \ \footnote{In fact, they boil down to the \textquotedblleft
associativity of addition\textquotedblright, \textquotedblleft neutrality of
$0$\textquotedblright\ and \textquotedblleft existence of additive
inverses\textquotedblright\ axioms in the definition of a noncommutative
ring.}. Therefore, the set $\mathbb{L}$, endowed with the addition
$+_{\mathbb{L}}$ (as multiplication) and the element $0_{\mathbb{L}}$ (as
neutral element), is a group. This proves Proposition \ref{prop.ring.groups}
\textbf{(a)}.

\textbf{(b)} If $a\in\mathbb{L}^{\times}$ and $b\in\mathbb{L}^{\times}$, then
$ab\in\mathbb{L}^{\times}$\ \ \ \ \footnote{\textit{Proof.} Let $a\in
\mathbb{L}^{\times}$ and $b\in\mathbb{L}^{\times}$. We have $a\in
\mathbb{L}^{\times}$; in other words, $a$ is an invertible element of
$\mathbb{L}$ (because $\mathbb{L}^{\times}$ is the set of all invertible
elements of $\mathbb{L}$). Thus, the inverse $a^{-1}$ of $a$ is well-defined.
Similarly, the inverse $b^{-1}$ of $b$ is well-defined. Now, since we have%
\[
\left(  b^{-1}a^{-1}\right)  \left(  ab\right)  =b^{-1}\underbrace{a^{-1}%
a}_{=1_{\mathbb{L}}}b=b^{-1}b=1_{\mathbb{L}}%
\]
and%
\[
\left(  ab\right)  \left(  b^{-1}a^{-1}\right)  =a\underbrace{bb^{-1}%
}_{=1_{\mathbb{L}}}a^{-1}=aa^{-1}=1_{\mathbb{L}},
\]
we see that the element $b^{-1}a^{-1}$ of $\mathbb{L}$ is an inverse of $ab$.
Thus, the element $ab$ has an inverse. In other words, $ab$ is invertible. In
other words, $ab\in\mathbb{L}^{\times}$ (since $\mathbb{L}^{\times}$ is the
set of all invertible elements of $\mathbb{L}$), qed.}. In other words, the
product of two elements of $\mathbb{L}^{\times}$ again belongs to
$\mathbb{L}^{\times}$. Thus, we can define a binary operation $\cdot
_{\mathbb{L}^{\times}}$ on the set $\mathbb{L}^{\times}$ (written infix) by
\[
a\cdot_{\mathbb{L}^{\times}}b=ab\ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathbb{L}^{\times}\text{ and }b\in\mathbb{L}^{\times}.
\]
Also, $1_{\mathbb{L}}$ is an invertible element of $\mathbb{L}$ (indeed, its
inverse is $1_{\mathbb{L}}$), and thus an element of $\mathbb{L}^{\times}$.

Now, we need to prove that the set $\mathbb{L}^{\times}$, endowed with the
multiplication $\cdot_{\mathbb{L}^{\times}}$ (as multiplication) and the
element $1_{\mathbb{L}}$ (as neutral element), is a group. In order to do so,
we need to check that the \textquotedblleft associativity\textquotedblright,
\textquotedblleft neutrality of $1$\textquotedblright\ and \textquotedblleft
existence of inverses\textquotedblright\ axioms are satisfied.

The \textquotedblleft associativity\textquotedblright\ axiom follows from the
\textquotedblleft associativity of multiplication\textquotedblright\ axiom in
the definition of a noncommutative ring. The \textquotedblleft neutrality of
$1$\textquotedblright\ axiom follows from the \textquotedblleft
unitality\textquotedblright\ axiom in the definition of a noncommutative ring.
It thus remains to prove that the \textquotedblleft existence of
inverses\textquotedblright\ axiom holds.

Thus, let $a\in\mathbb{L}^{\times}$. We need to show that there exists an
$a^{\prime}\in\mathbb{L}^{\times}$ such that $a\cdot_{\mathbb{L}^{\times}%
}a^{\prime}=a^{\prime}\cdot_{\mathbb{L}^{\times}}a=1_{\mathbb{L}}$ (since
$1_{\mathbb{L}}$ is the neutral element of $\mathbb{L}^{\times}$).

We know that $a$ is an invertible element of $\mathbb{L}$ (sin${}$ce
$a\in\mathbb{L}^{\times}$); it thus has an inverse $a^{-1}$. Now, $a$ itself
is an inverse of $a^{-1}$ (since $aa^{-1}=1_{\mathbb{L}}$ and $a^{-1}%
a=1_{\mathbb{L}}$), and thus the element $a^{-1}$ of $\mathbb{L}$ has an
inverse. In other words, $a^{-1}$ is invertible, so that $a^{-1}\in
\mathbb{L}^{\times}$. The definition of the operation $\cdot_{\mathbb{L}%
^{\times}}$ shows that $a\cdot_{\mathbb{L}^{\times}}a^{-1}=aa^{-1}%
=1_{\mathbb{L}}$ and that $a^{-1}\cdot_{\mathbb{L}^{\times}}a=a^{-1}%
a=1_{\mathbb{L}}$. Hence, there exists an $a^{\prime}\in\mathbb{L}^{\times}$
such that $a\cdot_{\mathbb{L}^{\times}}a^{\prime}=a^{\prime}\cdot
_{\mathbb{L}^{\times}}a=1_{\mathbb{L}}$ (namely, $a^{\prime}=a^{-1}$). Thus we
have proven that the \textquotedblleft existence of inverses\textquotedblright%
\ axiom holds. The proof of Proposition \ref{prop.ring.groups} \textbf{(b)} is
thus complete.
\end{proof}

We now have a plentitude of examples of groups: For every noncommutative ring
$\mathbb{L}$, we have the two groups $\mathbb{L}^{+}$ and $\mathbb{L}^{\times
}$ defined in Proposition \ref{prop.ring.groups}. Another example, for every
set $X$, is the symmetric group of $X$ (endowed with the composition of
permutations as multiplication, and the identity permutation
$\operatorname*{id}:X\rightarrow X$ as the neutral element). (Many other
examples can be found in textbooks on algebra, such as \cite{Artin}.)

\begin{remark}
Throwing all notational ballast aside, we can restate Proposition
\ref{prop.ring.groups} \textbf{(b)} as follows: The set of all invertible
elements of a noncommutative ring $\mathbb{L}$ is a group (where the binary
operation is multiplication). We can apply this to the case where
$\mathbb{L}=\mathbb{K}^{n\times n}$ for a commutative ring $\mathbb{K}$ and an
integer $n\in\mathbb{N}$. Thus, we obtain that the set of all invertible
elements of $\mathbb{K}^{n\times n}$ is a group. Since we know that the
invertible elements of $\mathbb{K}^{n\times n}$ are exactly the invertible
$n\times n$-matrices (by Example \ref{exa.rings.invertible-matrices}), we thus
have shown that the set of all invertible $n\times n$-matrices is a group.
This group is commonly denoted by $\operatorname*{GL}\nolimits_{n}\left(
\mathbb{K}\right)  $.
\end{remark}

\subsection{Cramer's rule}

Let us return to the classical properties of determinants. We have already
proven many, but here is one more: It is an application of determinants to
solving systems of linear equations.

\begin{theorem}
\label{thm.cramer}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let
$b=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}$ be a column vector of length
$n$ (that is, an $n\times1$-matrix).\footnotemark

For every $j\in\left\{  1,2,\ldots,n\right\}  $, let $A_{j}^{\#}$ be the
$n\times n$-matrix obtained from $A$ by replacing the $j$-th column of $A$
with the vector $b$.

\textbf{(a)} We have $A\cdot\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}=\det A\cdot b$.

\textbf{(b)} Assume that the matrix $A$ is invertible. Then,%
\[
A^{-1}b=\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}.
\]

\end{theorem}

\footnotetext{The reader should keep in mind that $\left(  b_{1},b_{2}%
,\ldots,b_{n}\right)  ^{T}$ is just a space-saving way to write $\left(
\begin{array}
[c]{c}%
b_{1}\\
b_{2}\\
\vdots\\
b_{n}%
\end{array}
\right)  $.}Theorem \ref{thm.cramer} (or either part of it) is known as
\textit{Cramer's rule}.

\begin{remark}
A system of $n$ linear equations in $n$ variables $x_{1},x_{2},\ldots,x_{n}$
can be written in the form $Ax=b$, where $A$ is a fixed $n\times n$-matrix and
$b$ is a column vector of length $n$ (and where $x$ is the column vector
$\left(  x_{1},x_{2},\ldots,x_{n}\right)  ^{T}$ containing all the variables).
When the matrix $A$ is invertible, it thus has a unique solution: namely,
$x=A^{-1}b$ (just multiply the equation $Ax=b$ from the left with $A^{-1}$ to
see this), and this solution can be computed using Theorem \ref{thm.cramer}.
This looks nice, but isn't actually all that useful for solving systems of
linear equations: For one thing, this does not immediately help us solve
systems of fewer or more than $n$ equations in $n$ variables; and even in the
case of exactly $n$ equations, the matrix $A$ coming from a system of linear
equations will not always be invertible (and in the more interesting cases, it
will not be). For another thing, at least when $\mathbb{K}$ is a field, there
are faster ways to solve a system of linear equations than anything that
involves computing $n+1$ determinants of $n\times n$-matrices. Theorem
\ref{thm.cramer} nevertheless turns out to be useful in proofs.
\end{remark}

\begin{vershort}
\begin{proof}
[Proof of Theorem \ref{thm.cramer}.]\textbf{(a)} Fix $j\in\left\{
1,2,\ldots,n\right\}  $. Let $C=A_{j}^{\#}$. Thus, $C=A_{j}^{\#}$ is the
$n\times n$-matrix obtained from $A$ by replacing the $j$-th column of $A$
with the vector $b$. In particular, the $j$-th column of $C$ is the vector
$b$. In other words, we have
\begin{equation}
c_{p,j}=b_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,n\right\}  . \label{pf.thm.cramer.short.cpj}%
\end{equation}


Furthermore, the matrix $C$ is equal to the matrix $A$ in all columns but its
$j$-th column (because it is obtained from $A$ by replacing the $j$-th column
of $A$ with the vector $b$). Thus, if we cross out the $j$-th columns in both
matrices $C$ and $A$, then these two matrices become equal. Consequently,%
\begin{equation}
C_{\sim p,\sim j}=A_{\sim p,\sim j}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  \label{pf.thm.cramer.short.CA}%
\end{equation}
(because the matrices $C_{\sim p,\sim j}$ and $A_{\sim p,\sim j}$ are obtained
by crossing out the $p$-th row and the $j$-th column in the matrices $C$ and
$A$, respectively). Now,%
\begin{align}
\det\left(  \underbrace{A_{j}^{\#}}_{=C}\right)   &  =\det C=\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+j}\underbrace{c_{p,j}}_{\substack{=b_{p}\\\text{(by
(\ref{pf.thm.cramer.short.cpj}))}}}\det\left(  \underbrace{C_{\sim p,\sim j}%
}_{\substack{=A_{\sim p,\sim j}\\\text{(by (\ref{pf.thm.cramer.short.CA}))}%
}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(b)}, applied to }C\text{, }c_{i,j}\text{ and }j\text{ instead of
}A\text{, }a_{i,j}\text{ and }q\right) \nonumber\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}\det\left(  A_{\sim p,\sim
j}\right)  . \label{pf.thm.cramer.short.detAshj}%
\end{align}
Let us now forget that we fixed $j$. We thus have proven
(\ref{pf.thm.cramer.short.detAshj}) for every $j\in\left\{  1,2,\ldots
,n\right\}  $. Now, fix $i\in\left\{  1,2,\ldots,n\right\}  $. Then, for every
$p\in\left\{  1,2,\ldots,n\right\}  $ satisfying $p\neq i$, we have%
\begin{equation}
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =0 \label{pf.thm.cramer.short.termkiller1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.short.termkiller1}):} Let
$p\in\left\{  1,2,\ldots,n\right\}  $ be such that $p\neq i$. Hence,
Proposition \ref{prop.laplace.0} \textbf{(a)} (applied to $r=i$) shows that%
\begin{equation}
0=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  . \label{pf.thm.cramer.short.termkiller1.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =b_{p}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}%
a_{i,q}\det\left(  A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.short.termkiller1.pf.1}))}}}=0.
\]
Thus, (\ref{pf.thm.cramer.short.termkiller1}) is proven.}. Also, we have%
\begin{equation}
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =\det A\cdot b_{i} \label{pf.thm.cramer.short.termkiller2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.short.termkiller2}):} Applying
Theorem \ref{thm.laplace.gen} \textbf{(a)} to $p=i$, we obtain%
\begin{equation}
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim
i,\sim q}\right)  . \label{pf.thm.cramer.short.termkiller2.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =b_{i}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}%
a_{i,q}\det\left(  A_{\sim i,\sim q}\right)  }_{\substack{=\det A\\\text{(by
(\ref{pf.thm.cramer.short.termkiller2.pf.1}))}}}=b_{i}\det A=\det A\cdot
b_{i}.
\]
This proves (\ref{pf.thm.cramer.short.termkiller2}).}. Now,%
\begin{align}
&  \sum_{k=1}^{n}a_{i,k}\underbrace{\det\left(  A_{k}^{\#}\right)
}_{\substack{=\sum_{p=1}^{n}\left(  -1\right)  ^{p+k}b_{p}\det\left(  A_{\sim
p,\sim k}\right)  \\\text{(by (\ref{pf.thm.cramer.short.detAshj}), applied to
}j=k\text{)}}}\nonumber\\
&  =\sum_{k=1}^{n}a_{i,k}\sum_{p=1}^{n}\left(  -1\right)  ^{p+k}b_{p}%
\det\left(  A_{\sim p,\sim k}\right)  =\sum_{q=1}^{n}a_{i,q}\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+q}b_{p}\det\left(  A_{\sim p,\sim q}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }q\text{ in the first sum}\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{n}\sum_{p=1}^{n}}_{\substack{=\sum_{p=1}^{n}%
\sum_{q=1}^{n}\\=\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{q=1}^{n}%
}}\underbrace{a_{i,q}\left(  -1\right)  ^{p+q}b_{p}}_{=b_{p}\left(  -1\right)
^{p+q}a_{i,q}}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{q=1}^{n}b_{p}\left(
-1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq i}%
}\underbrace{\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.short.termkiller1}))}}}+\underbrace{\sum_{q=1}^{n}%
b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim q}\right)
}_{\substack{=\det A\cdot b_{i}\\\text{(by
(\ref{pf.thm.cramer.short.termkiller2}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}p=i\text{ from the sum}\right) \nonumber\\
&  =\underbrace{\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq
i}}0}_{=0}+\det A\cdot b_{i}=\det A\cdot b_{i}.
\label{pf.thm.cramer.short.almostthere}%
\end{align}
Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.thm.cramer.short.almostthere}) for every $i\in\left\{  1,2,\ldots
,n\right\}  $. Now, let $d$ be the vector $\left(  \det\left(  A_{1}%
^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}%
^{\#}\right)  \right)  ^{T}$. Thus,%
\[
d=\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\left(
\begin{array}
[c]{c}%
\det\left(  A_{1}^{\#}\right) \\
\det\left(  A_{2}^{\#}\right) \\
\vdots\\
\det\left(  A_{n}^{\#}\right)
\end{array}
\right)  =\left(  \det\left(  A_{i}^{\#}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq1}.
\]
The definition of the product of two matrices shows that%
\begin{align*}
A\cdot d  &  =\left(  \underbrace{\sum_{k=1}^{n}a_{i,k}\det\left(  A_{k}%
^{\#}\right)  }_{\substack{=\det A\cdot b_{i}\\\text{(by
(\ref{pf.thm.cramer.short.almostthere}))}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }d=\left(  \det\left(  A_{i}^{\#}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\right) \\
&  =\left(  \det A\cdot b_{i}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}=\left(
\det A\cdot b_{1},\det A\cdot b_{2},\ldots,\det A\cdot b_{n}\right)  ^{T}.
\end{align*}
Comparing this with%
\[
\det A\cdot\underbrace{b}_{=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}%
}=\det A\cdot\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}=\left(  \det A\cdot
b_{1},\det A\cdot b_{2},\ldots,\det A\cdot b_{n}\right)  ^{T},
\]
we obtain $A\cdot d=\det A\cdot b$. Since $d=\left(  \det\left(  A_{1}%
^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}%
^{\#}\right)  \right)  ^{T}$, we can rewrite this as $A\cdot\left(
\det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots
,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\det A\cdot b$. This proves
Theorem \ref{thm.cramer} \textbf{(a)}.

\textbf{(b)} Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} shows
that the matrix $A$ is invertible if and only if the element $\det A$ of
$\mathbb{K}$ is invertible (in $\mathbb{K}$). Hence, the element $\det A$ of
$\mathbb{K}$ is invertible (since the matrix $A$ is invertible). Thus,
$\dfrac{1}{\det A}$ is well-defined. Clearly, $\underbrace{\dfrac{1}{\det
A}\cdot\det A}_{=1}\cdot b=b$, so that%
\begin{align*}
b  &  =\dfrac{1}{\det A}\cdot\underbrace{\det A\cdot b}_{\substack{=A\cdot
\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}\\\text{(by Theorem
\ref{thm.cramer} \textbf{(a)})}}}\\
&  =\dfrac{1}{\det A}\cdot A\cdot\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}\\
&  =A\cdot\underbrace{\left(  \dfrac{1}{\det A}\cdot\left(  \det\left(
A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(
A_{n}^{\#}\right)  \right)  ^{T}\right)  }_{\substack{=\left(  \dfrac{1}{\det
A}\det\left(  A_{1}^{\#}\right)  ,\dfrac{1}{\det A}\det\left(  A_{2}%
^{\#}\right)  ,\ldots,\dfrac{1}{\det A}\det\left(  A_{n}^{\#}\right)  \right)
^{T}\\=\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}}}\\
&  =A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
Therefore,
\begin{align*}
&  A^{-1}\underbrace{b}_{=A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)
}{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac
{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}}\\
&  =\underbrace{A^{-1}A}_{=I_{n}}\cdot\left(  \dfrac{\det\left(  A_{1}%
^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A}%
,\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}\\
&  =I_{n}\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A}%
,\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(
A_{n}^{\#}\right)  }{\det A}\right)  ^{T}=\left(  \dfrac{\det\left(
A_{1}^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det
A},\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
This proves Theorem \ref{thm.cramer} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Theorem \ref{thm.cramer}.]\textbf{(a)} Fix $j\in\left\{
1,2,\ldots,n\right\}  $. Let $C=A_{j}^{\#}$.

We know that $A_{j}^{\#}$ is the $n\times n$-matrix obtained from $A$ by
replacing the $j$-th column of $A$ with the vector $b$. In other words, $C$ is
the $n\times n$-matrix obtained from $A$ by replacing the $j$-th column of $A$
with the vector $b$ (because $C=A_{j}^{\#}$). In other words, we have%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th column of }C\right)  =\left(
\text{the }u\text{-th column of }A\right)  \right.  \label{pf.thm.cramer.C.1}%
\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq j\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }j\text{-th column of }C\right)  =b.
\label{pf.thm.cramer.C.2}%
\end{equation}


Let us write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every $u\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th column of }A\right)  =\left(  a_{1,u}%
,a_{2,u},\ldots,a_{n,u}\right)  ^{T}. \label{pf.thm.cramer.u-th-col-A}%
\end{equation}


On the other hand, let us write the $n\times n$-matrix $C$ in the form
$C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every
$u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th column of }C\right)  =\left(  c_{1,u}%
,c_{2,u},\ldots,c_{n,u}\right)  ^{T}. \label{pf.thm.cramer.u-th-col-C}%
\end{equation}


Now, it is easy to see that
\begin{equation}
c_{p,j}=b_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,n\right\}  \label{pf.thm.cramer.cpj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.cpj}):} Applying
(\ref{pf.thm.cramer.u-th-col-C}) to $u=j$, we obtain
\[
\left(  \text{the }j\text{-th column of }C\right)  =\left(  c_{1,j}%
,c_{2,j},\ldots,c_{n,j}\right)  ^{T}.
\]
Thus,%
\begin{align*}
\left(  c_{1,j},c_{2,j},\ldots,c_{n,j}\right)  ^{T}  &  =\left(  \text{the
}j\text{-th column of }C\right)  =b\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.cramer.C.2})}\right) \\
&  =\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}.
\end{align*}
Thus,%
\[
\left(  c_{1,j},c_{2,j},\ldots,c_{n,j}\right)  =\left(  \underbrace{\left(
c_{1,j},c_{2,j},\ldots,c_{n,j}\right)  ^{T}}_{=\left(  b_{1},b_{2}%
,\ldots,b_{n}\right)  ^{T}}\right)  ^{T}=\left(  \left(  b_{1},b_{2}%
,\ldots,b_{n}\right)  ^{T}\right)  ^{T}=\left(  b_{1},b_{2},\ldots
,b_{n}\right)  .
\]
In other words, $c_{p,j}=b_{p}$ for every $p\in\left\{  1,2,\ldots,n\right\}
$. This proves (\ref{pf.thm.cramer.cpj}).}. Furthermore,%
\begin{equation}
c_{p,q}=a_{p,q}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \text{ and }q\in\left\{  1,2,\ldots,n\right\}  \text{
satisfying }q\neq j \label{pf.thm.cramer.cpq}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.cpq}):} Let $q\in\left\{
1,2,\ldots,n\right\}  $ be such that $q\neq j$. We need to prove that
$c_{p,q}=a_{p,q}$ for every $p\in\left\{  1,2,\ldots,n\right\}  $.
\par
Applying (\ref{pf.thm.cramer.u-th-col-C}) to $u=q$, we obtain
\[
\left(  \text{the }q\text{-th column of }C\right)  =\left(  c_{1,q}%
,c_{2,q},\ldots,c_{n,q}\right)  ^{T}.
\]
Thus,%
\begin{align*}
\left(  c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  ^{T}  &  =\left(  \text{the
}q\text{-th column of }C\right)  =\left(  \text{the }q\text{-th column of
}A\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.cramer.C.1}),
applied to }u=q\right) \\
&  =\left(  a_{1,q},a_{2,q},\ldots,a_{n,q}\right)  ^{T}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.cramer.u-th-col-A}), applied
to }u=q\right)  .
\end{align*}
Thus,%
\[
\left(  c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  =\left(  \underbrace{\left(
c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  ^{T}}_{=\left(  a_{1,q},a_{2,q}%
,\ldots,a_{n,q}\right)  ^{T}}\right)  ^{T}=\left(  \left(  a_{1,q}%
,a_{2,q},\ldots,a_{n,q}\right)  ^{T}\right)  ^{T}=\left(  a_{1,q}%
,a_{2,q},\ldots,a_{n,q}\right)  .
\]
In other words, $c_{p,q}=a_{p,q}$ for every $p\in\left\{  1,2,\ldots
,n\right\}  $. This proves (\ref{pf.thm.cramer.cpq}).}. Now, it is easy to see
that%
\begin{equation}
C_{\sim p,\sim j}=A_{\sim p,\sim j}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  \label{pf.thm.cramer.CA}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.CA}):} Let $p\in\left\{
1,2,\ldots,n\right\}  $.
\par
Let $\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{j},\ldots,n\right)  $. Thus,
$\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{j}%
,\ldots,n\right)  $, so that $\left\{  u_{1},u_{2},\ldots,u_{n-1}\right\}
=\left\{  1,2,\ldots,\widehat{j},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  j\right\}  $.
\par
Now, let $y\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $u_{y}\in\left\{
u_{1},u_{2},\ldots,u_{n-1}\right\}  =\left\{  1,2,\ldots,n\right\}
\setminus\left\{  j\right\}  $, so that $u_{y}\neq j$. Hence,%
\begin{equation}
c_{p,u_{y}}=a_{p,u_{y}}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \label{pf.thm.cramer.CA.pf.1}%
\end{equation}
(by (\ref{pf.thm.cramer.cpq}), applied to $q=u_{y}$).
\par
Let us now forget that we fixed $y$. We thus have shown that
(\ref{pf.thm.cramer.CA.pf.1}) holds for every $y\in\left\{  1,2,\ldots
,n-1\right\}  $.
\par
Let $\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  $. Thus,
$\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $.
\par
Now, the definition of $C_{\sim p,\sim j}$ yields%
\begin{align*}
C_{\sim p,\sim j}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{j},\ldots,n}C=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{u_{1},u_{2},\ldots,u_{n-1}}C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{j}%
,\ldots,n\right)  =\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1}%
,u_{2},\ldots,u_{n-1}}C\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  v_{1},v_{2},\ldots
,v_{n-1}\right)  \right) \\
&  =\left(  \underbrace{c_{v_{x},u_{y}}}_{\substack{=a_{v_{x},u_{y}%
}\\\text{(by (\ref{pf.thm.cramer.CA.pf.1}),}\\\text{applied to }%
p=v_{x}\text{)}}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1},u_{2},\ldots,u_{n-1}}C\text{,
since }C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{v_{x},u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}.
\end{align*}
Compared with%
\begin{align*}
A_{\sim p,\sim j}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{j},\ldots,n}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{u_{1},u_{2},\ldots,u_{n-1}}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{j}%
,\ldots,n\right)  =\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1}%
,u_{2},\ldots,u_{n-1}}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  v_{1},v_{2},\ldots
,v_{n-1}\right)  \right) \\
&  =\left(  a_{v_{x},u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq
n-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1},u_{2}%
,\ldots,u_{n-1}}A\text{, since }A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ,
\end{align*}
this yields $C_{\sim p,\sim j}=A_{\sim p,\sim j}$. This proves
(\ref{pf.thm.cramer.CA}).}. Now,%
\begin{align}
\det\left(  \underbrace{A_{j}^{\#}}_{=C}\right)   &  =\det C=\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+j}\underbrace{c_{p,j}}_{\substack{=b_{p}\\\text{(by
(\ref{pf.thm.cramer.cpj}))}}}\det\left(  \underbrace{C_{\sim p,\sim j}%
}_{\substack{=A_{\sim p,\sim j}\\\text{(by (\ref{pf.thm.cramer.CA}))}}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(b)}, applied to }C\text{, }c_{i,j}\text{ and }j\text{ instead of
}A\text{, }a_{i,j}\text{ and }q\right) \nonumber\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}\det\left(  A_{\sim p,\sim
j}\right)  . \label{pf.thm.cramer.detAshj}%
\end{align}


Let us now forget that we fixed $j$. We thus have proven
(\ref{pf.thm.cramer.detAshj}) for every $j\in\left\{  1,2,\ldots,n\right\}  $.

Now, fix $i\in\left\{  1,2,\ldots,n\right\}  $. Then, for every $p\in\left\{
1,2,\ldots,n\right\}  $ satisfying $p\neq i$, we have%
\begin{equation}
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =0 \label{pf.thm.cramer.termkiller1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.termkiller1}):} Let
$p\in\left\{  1,2,\ldots,n\right\}  $ be such that $p\neq i$. Hence,
Proposition \ref{prop.laplace.0} \textbf{(a)} (applied to $r=i$) shows that%
\begin{equation}
0=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  . \label{pf.thm.cramer.termkiller1.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =b_{p}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}%
a_{i,q}\det\left(  A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.termkiller1.pf.1}))}}}=0.
\]
Thus, (\ref{pf.thm.cramer.termkiller1}) is proven.}. Also, we have%
\begin{equation}
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =\det A\cdot b_{i} \label{pf.thm.cramer.termkiller2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.termkiller2}):} Applying
Theorem \ref{thm.laplace.gen} \textbf{(a)} to $p=i$, we obtain%
\begin{equation}
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim
i,\sim q}\right)  . \label{pf.thm.cramer.termkiller2.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =b_{i}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}%
a_{i,q}\det\left(  A_{\sim i,\sim q}\right)  }_{\substack{=\det A\\\text{(by
(\ref{pf.thm.cramer.termkiller2.pf.1}))}}}=b_{i}\det A=\det A\cdot b_{i}.
\]
This proves (\ref{pf.thm.cramer.termkiller2}).}. Now,%

\begin{align}
&  \sum_{k=1}^{n}a_{i,k}\det\left(  A_{k}^{\#}\right) \nonumber\\
&  =\sum_{j=1}^{n}a_{i,j}\underbrace{\det\left(  A_{j}^{\#}\right)
}_{\substack{=\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}\det\left(  A_{\sim
p,\sim j}\right)  \\\text{(by (\ref{pf.thm.cramer.detAshj}))}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }j\right) \nonumber\\
&  =\sum_{j=1}^{n}a_{i,j}\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}%
\det\left(  A_{\sim p,\sim j}\right)  =\sum_{q=1}^{n}a_{i,q}\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+q}b_{p}\det\left(  A_{\sim p,\sim q}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}j\text{ as }q\text{ in the first sum}\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{n}\sum_{p=1}^{n}}_{=\sum_{p=1}^{n}\sum_{q=1}^{n}%
}\underbrace{a_{i,q}\left(  -1\right)  ^{p+q}b_{p}}_{=b_{p}\left(  -1\right)
^{p+q}a_{i,q}}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\underbrace{\sum_{p=1}^{n}}_{=\sum_{p\in\left\{  1,2,\ldots,n\right\}  }%
}\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right) \nonumber\\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{q=1}^{n}b_{p}\left(
-1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq i}%
}\underbrace{\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.termkiller1}))}}}+\underbrace{\sum_{q=1}^{n}b_{i}\left(
-1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim q}\right)  }%
_{\substack{=\det A\cdot b_{i}\\\text{(by (\ref{pf.thm.cramer.termkiller2}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}p=i\text{ from the sum}\right) \nonumber\\
&  =\underbrace{\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq
i}}0}_{=0}+\det A\cdot b_{i}=\det A\cdot b_{i}.
\label{pf.thm.cramer.almostthere}%
\end{align}


Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.thm.cramer.almostthere}) for every $i\in\left\{  1,2,\ldots
,n\right\}  $.

Now, let $d$ be the vector $\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}$. Thus,%
\[
d=\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\left(
\begin{array}
[c]{c}%
\det\left(  A_{1}^{\#}\right) \\
\det\left(  A_{2}^{\#}\right) \\
\vdots\\
\det\left(  A_{n}^{\#}\right)
\end{array}
\right)  =\left(  \det\left(  A_{i}^{\#}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq1}.
\]


The definition of the product of two matrices shows that%
\begin{align*}
A\cdot d  &  =\left(  \underbrace{\sum_{k=1}^{n}a_{i,k}\det\left(  A_{k}%
^{\#}\right)  }_{\substack{=\det A\cdot b_{i}\\\text{(by
(\ref{pf.thm.cramer.almostthere}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }d=\left(  \det\left(  A_{i}^{\#}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\right) \\
&  =\left(  \det A\cdot b_{i}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\end{align*}
Comparing this with%
\begin{align*}
\det A\cdot\underbrace{b}_{=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}}  &
=\det A\cdot\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}\\
&  =\left(  \underbrace{\det A\cdot\left(  b_{1},b_{2},\ldots,b_{n}\right)
}_{\substack{=\left(  \det A\cdot b_{1},\det A\cdot b_{2},\ldots,\det A\cdot
b_{n}\right)  \\=\left(  \det A\cdot b_{j}\right)  _{1\leq i\leq1,\ 1\leq
j\leq n}}}\right)  ^{T}\\
&  =\left(  \left(  \det A\cdot b_{j}\right)  _{1\leq i\leq1,\ 1\leq j\leq
n}\right)  ^{T}=\left(  \det A\cdot b_{i}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the transpose of a
matrix}\right)  ,
\end{align*}
we obtain $A\cdot d=\det A\cdot b$. Since $d=\left(  \det\left(  A_{1}%
^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}%
^{\#}\right)  \right)  ^{T}$, we can rewrite this as $A\cdot\left(
\det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots
,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\det A\cdot b$. This proves
Theorem \ref{thm.cramer} \textbf{(a)}.

\textbf{(b)} Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} shows
that the matrix $A$ is invertible if and only if the element $\det A$ of
$\mathbb{K}$ is invertible (in $\mathbb{K}$). Hence, the element $\det A$ of
$\mathbb{K}$ is invertible (since the matrix $A$ is invertible). Thus,
$\dfrac{1}{\det A}$ is well-defined. Clearly, $\underbrace{\dfrac{1}{\det
A}\cdot\det A}_{=1}\cdot b=b$, so that%
\begin{align*}
b  &  =\dfrac{1}{\det A}\cdot\underbrace{\det A\cdot b}_{\substack{=A\cdot
\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}\\\text{(by Theorem
\ref{thm.cramer} \textbf{(a)})}}}\\
&  =\dfrac{1}{\det A}\cdot A\cdot\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}\\
&  =A\cdot\underbrace{\left(  \dfrac{1}{\det A}\cdot\left(  \det\left(
A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(
A_{n}^{\#}\right)  \right)  ^{T}\right)  }_{\substack{=\left(  \dfrac{1}{\det
A}\det\left(  A_{1}^{\#}\right)  ,\dfrac{1}{\det A}\det\left(  A_{2}%
^{\#}\right)  ,\ldots,\dfrac{1}{\det A}\det\left(  A_{n}^{\#}\right)  \right)
^{T}\\=\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}}}\\
&  =A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
Therefore,
\begin{align*}
&  A^{-1}\underbrace{b}_{=A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)
}{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac
{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}}\\
&  =\underbrace{A^{-1}A}_{=I_{n}}\cdot\left(  \dfrac{\det\left(  A_{1}%
^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A}%
,\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}\\
&  =I_{n}\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A}%
,\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(
A_{n}^{\#}\right)  }{\det A}\right)  ^{T}=\left(  \dfrac{\det\left(
A_{1}^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det
A},\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
This proves Theorem \ref{thm.cramer} \textbf{(b)}.
\end{proof}
\end{verlong}

\begin{verlong}
[TODO: section on Pl\"{u}cker \& Carroll]
\end{verlong}

[To be continued!]

\subsection{Additional exercises}

Here are a few more additional exercises, with no solutions given and no
importance to the rest of the text.

\begin{addexercise}
\label{exeadd.det.anotherpattern}Compute the determinant of the horrible
$7\times7$-matrix%
\[
\left(
\begin{array}
[c]{ccccccc}%
a & 0 & 0 & 0 & 0 & 0 & b\\
0 & a^{\prime} & 0 & 0 & 0 & b^{\prime} & 0\\
0 & 0 & a^{\prime\prime} & 0 & b^{\prime\prime} & 0 & 0\\
0 & 0 & 0 & e & 0 & 0 & 0\\
0 & 0 & c^{\prime\prime} & 0 & d^{\prime\prime} & 0 & 0\\
0 & c^{\prime} & 0 & 0 & 0 & d^{\prime} & 0\\
c & 0 & 0 & 0 & 0 & 0 & d
\end{array}
\right)  .
\]

\end{addexercise}

\begin{addexercise}
\label{exeadd.det.delannoy}Recall that the binomial coefficients satisfy the
recurrence relation (\ref{eq.binom.rec.m}), which (visually) says that every
entry of Pascal's triangle is the sum of the two entries left-above it and
right-above it.

Let us now define a variation of Pascal's triangle as follows: Define a
nonnegative integer $\dbinom{m}{n}_{D}$ for every $m\in\mathbb{N}$ and
$n\in\mathbb{N}$ recursively as follows:

\begin{itemize}
\item Set $\dbinom{0}{n}_{D}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n>0
\end{cases}
$ for every $n\in\mathbb{N}$.

\item For every $m\in\mathbb{Z}$ and $n\in\mathbb{Z}$, set $\dbinom{m}{n}%
_{D}=0$ if either $m$ or $n$ is negative.

\item For every positive integer $m$ and every $n\in\mathbb{N}$, set
\[
\dbinom{m}{n}_{D}=\dbinom{m-1}{n-1}_{D}+\dbinom{m-1}{n}_{D}+\dbinom{m-2}%
{n-1}_{D}.
\]

\end{itemize}

(Thus, if we lay these $\dbinom{m}{n}_{D}$ out in the same way as the binomial
coefficients $\dbinom{m}{n}$ in Pascal's triangle, then every entry is the sum
of the three entries left-above it, right-above it, and straight above it.)

The integers $\dbinom{m}{n}_{D}$ are known as the
\href{https://en.wikipedia.org/wiki/Delannoy_number}{Delannoy numbers}.

\textbf{(a)} Show that
\[
\dbinom{n+m}{n}_{D}=\sum_{i=0}^{n}\dbinom{n}{i}\dbinom{m+i}{n}=\sum_{i=0}%
^{n}\dbinom{n}{i}\dbinom{m}{i}2^{i}%
\]
for every $n\in\mathbb{N}$ and $m\in\mathbb{N}$. (The second equality sign
here is a consequence of Proposition \ref{prop.binom.bin-id} \textbf{(e)}.)

\textbf{(b)} Let $n\in\mathbb{N}$. Let $A$ be the $n\times n$-matrix $\left(
\dbinom{i+j-2}{i-1}_{D}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (an analogue
of the matrix $A$ from Exercise \ref{exe.ps4.pascal}). Show that%
\[
\det A=2^{n\left(  n-1\right)  /2}.
\]

\end{addexercise}

\begin{noncompile}
\textbf{(a)} One can prove $\dbinom{n+m}{n}_{D}=\sum_{i=0}^{n}\dbinom{n}%
{i}\dbinom{m+i}{n}$ by strong induction over $n+m$, along the following lines:%
\begin{align*}
&  \sum_{i=0}^{n}\dbinom{n}{i}\underbrace{\dbinom{m+i}{n}}_{=\dbinom
{m+i-1}{n-1}+\dbinom{m+i-1}{n}}\\
&  =\sum_{i=0}^{n}\underbrace{\dbinom{n}{i}}_{=\dbinom{n-1}{i-1}+\dbinom
{n-1}{i}}\dbinom{m+i-1}{n-1}+\underbrace{\sum_{i=0}^{n}\dbinom{n}{i}%
\dbinom{m+i-1}{n}}_{=\dbinom{n+m-1}{n}_{D}}\\
&  =\underbrace{\sum_{i=0}^{n}\dbinom{n-1}{i-1}\dbinom{m+i-1}{n-1}%
}_{\substack{=\sum_{i=0}^{n}\dbinom{n-1}{i-1}\dbinom{m+\left(  i-1\right)
}{n-1}\\=\dbinom{\left(  n-1\right)  +m}{n-1}_{D}}}+\underbrace{\sum_{i=0}%
^{n}\dbinom{n-1}{i}\dbinom{m+i-1}{n-1}}_{\substack{=\sum_{i=0}^{n}\dbinom
{n-1}{i}\dbinom{\left(  m-1\right)  +i}{n-1}\\=\dbinom{\left(  n-1\right)
+\left(  m-1\right)  }{n-1}_{D}}}+\dbinom{n+m-1}{n}_{D}\\
&  =\dbinom{\left(  n-1\right)  +m}{n-1}_{D}+\dbinom{\left(  n-1\right)
+\left(  m-1\right)  }{n-1}_{D}+\dbinom{n+m-1}{n}_{D}=\dbinom{n+m}{n}_{D}.
\end{align*}


\textbf{(b)} Use $\dbinom{n+m}{n}_{D}=\sum_{i=0}^{n}\dbinom{n}{i}\dbinom{m}%
{i}2^{i}$ and the same tactic as in Exercise \ref{exe.ps4.pascal}.
\end{noncompile}

\begin{addexercise}
\label{exeadd.det.charpoly}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix over the commutative ring $\mathbb{K}$. Consider the commutative
ring $\mathbb{K}\left[  X\right]  $ of polynomials in the indeterminate $X$
over $\mathbb{K}$ (that is, polynomials in the indeterminate $X$ with
coefficients lying in $\mathbb{K}$). We can then regard $A$ as a matrix over
the ring $\mathbb{K}\left[  X\right]  $ as well (because every element of
$\mathbb{K}$ can be viewed as a constant polynomial in $\mathbb{K}\left[
X\right]  $).

Consider the $n\times n$-matrix $A+XI_{n}$ over the commutative ring
$\mathbb{K}\left[  X\right]  $. (For example, if $n=2$ and $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $, then $A+XI_{n}=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  +X\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
a+X & b\\
c & d+X
\end{array}
\right)  $. In general, the matrix $A+XI_{n}$ is obtained from $A$ by adding
an $X$ to each diagonal entry.)

The determinant $\det\left(  A+XI_{n}\right)  $ is a polynomial in
$\mathbb{K}\left[  X\right]  $. (For instance, for $n=2$ and $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $, we have%
\begin{align*}
\det\left(  A+XI_{n}\right)   &  =\det\left(
\begin{array}
[c]{cc}%
a+X & b\\
c & d+X
\end{array}
\right)  =\left(  a+X\right)  \left(  d+X\right)  -bc\\
&  =X^{2}+\left(  a+d\right)  X+\left(  ad-bc\right)  .
\end{align*}
)

What can you say about the coefficients of this polynomial? Most importantly,
what are the coefficients before $X^{n}$, before $X^{n-1}$, and before $X^{0}$
? Can you express the other coefficients as sums of determinants?
\end{addexercise}

\begin{addexercise}
\label{exeadd.det.rk1upd}Let $n\in\mathbb{N}$. Let $u$ be a column vector of
length $n$, and let $v$ be a row vector of length $n$. (Thus, $uv$ is an
$n\times n$-matrix, whereas $vu$ is a $1\times1$-matrix.) Let $A$ be an
$n\times n$-matrix. Prove that%
\[
\det\left(  A+uv\right)  =\det A+v\left(  \operatorname*{adj}A\right)  u
\]
(where we regard the $1\times1$-matrix $v\left(  \operatorname*{adj}A\right)
u$ as an element of $\mathbb{K}$).
\end{addexercise}

\begin{addexercise}
\label{exeadd.det.resultant}Let $P=\sum_{k=0}^{d}p_{k}X^{k}$ and $Q=\sum
_{k=0}^{e}q_{k}X^{k}$ be two polynomials over $\mathbb{K}$ (where $p_{0}%
,p_{1},\ldots,p_{d}\in\mathbb{K}$ and $q_{0},q_{1},\ldots,q_{e}\in\mathbb{K}$
are their coefficients). Define a $\left(  d+e\right)  \times\left(
d+e\right)  $-matrix $A$ as follows:

\begin{itemize}
\item For every $k\in\left\{  1,2,\ldots,e\right\}  $, the $k$-th row of $A$
is%
\[
\left(  \underbrace{0,0,\ldots,0}_{k-1\text{ zeroes}},p_{d},p_{d-1}%
,\ldots,p_{1},p_{0},\underbrace{0,0,\ldots,0}_{e-k\text{ zeroes}}\right)  .
\]


\item For every $k\in\left\{  1,2,\ldots,d\right\}  $, the $\left(
e+k\right)  $-th row of $A$ is%
\[
\left(  \underbrace{0,0,\ldots,0}_{k-1\text{ zeroes}},q_{e},q_{e-1}%
,\ldots,q_{1},q_{0},\underbrace{0,0,\ldots,0}_{d-k\text{ zeroes}}\right)  .
\]

\end{itemize}

(For example, if $d=4$ and $e=3$, then%
\[
A=\left(
\begin{array}
[c]{ccccccc}%
p_{4} & p_{3} & p_{2} & p_{1} & p_{0} & 0 & 0\\
0 & p_{4} & p_{3} & p_{2} & p_{1} & p_{0} & 0\\
0 & 0 & p_{4} & p_{3} & p_{2} & p_{1} & p_{0}\\
q_{3} & q_{2} & q_{1} & q_{0} & 0 & 0 & 0\\
0 & q_{3} & q_{2} & q_{1} & q_{0} & 0 & 0\\
0 & 0 & q_{3} & q_{2} & q_{1} & q_{0} & 0\\
0 & 0 & 0 & q_{3} & q_{2} & q_{1} & q_{0}%
\end{array}
\right)  .
\]
)

Assume that the polynomials $P$ and $Q$ have a common root $z$ (that is, there
exists a $z\in\mathbb{K}$ such that $P\left(  z\right)  =0$ and $Q\left(
z\right)  =0$). Show that $\det A=0$.

[\textbf{Hint:} Find a column vector $v$ of length $d+e$ satisfying
$Av=0_{\left(  d+e\right)  \times1}$; then apply Corollary
\ref{cor.adj.kernel}.]
\end{addexercise}

\begin{remark}
The matrix $A$ in Additional exercise \ref{exeadd.det.resultant} is called the
\textit{\href{https://en.wikipedia.org/wiki/Sylvester_matrix}{\textit{Sylvester
matrix}}} of the polynomials $P$ and $Q$ (for degrees $d$ and $e$); its
determinant $\det A$ is known as their
\textit{\href{https://en.wikipedia.org/wiki/Resultant}{\textit{resultant}}}
(at least when $d$ and $e$ are actually the degrees of $P$ and $Q$). According
to the exercise, the condition $\det A=0$ is necessary for $P$ and $Q$ to have
a common root. In the general case, the converse does not hold: For one, you
can always force $\det A$ to be $0$ by taking $d>\deg P$ and $e>\deg Q$ (so
$p_{d}=0$ and $q_{e}=0$, and thus the $1$-st column of $A$ consists of
zeroes). More importantly, the resultant of the two polynomials $X^{3}-1$ and
$X^{2}+X+1$ is $0$, but they only have common roots in $\mathbb{C}$, not in
$\mathbb{R}$. Thus, there is more to common roots than just the vanishing of a determinant.

However, if $\mathbb{K}$ is an algebraically closed field (I won't go into the
details of what this means, but an example of such a field is $\mathbb{C}$),
and if $d=\deg P$ and $e=\deg Q$, then the polynomials $P$ and $Q$ have a
common root \textbf{if and only if} their resultant is $0$.
\end{remark}

\section{Solutions}

This section contains solutions (or, sometimes, solution sketches) to some of
the exercises in the text, as well as occasional remarks. I do not recommend
reading them before trying to solve the problem on your own.

\subsection{Solution to Exercise \ref{exe.ps1.1.1}}

\begin{proof}
[Solution to Exercise \ref{exe.ps1.1.1}.]\textbf{(a)} We prove the claim by
induction over $\left\vert B\right\vert $.

\textit{Induction base:} Assume that $\left\vert B\right\vert =0$. Thus,
$B=\varnothing$, and thus there are no arrows of $Q$ which start at a vertex
in $A$ and at a vertex in $B$. Hence, $\operatorname*{mut}\nolimits_{A,B}Q=Q$,
and this can clearly be obtained from $Q$ by a sequence of mutations at sinks
(namely, by the empty sequence). Thus, Exercise \ref{exe.ps1.1.1} \textbf{(a)}
holds if $\left\vert B\right\vert =0$. This completes the induction
base.\footnote{Yes, this was a completely honest induction base. You don't
need to start at $\left\vert B\right\vert =1$ unless you want to use something
like $\left\vert B\right\vert >1$ in the induction step (but even then, you
should also handle the case $\left\vert B\right\vert =0$ case).}

\textit{Induction step:} Let $N\in\mathbb{N}$. Assume that Exercise
\ref{exe.ps1.1.1} \textbf{(a)} holds whenever $\left\vert B\right\vert =N$. We
now need to prove that Exercise \ref{exe.ps1.1.1} \textbf{(a)} holds whenever
$\left\vert B\right\vert =N+1$.

So let $A$ and $B$ be two subsets of $Q_{0}$ such that $A\cap B=\varnothing$
and $A\cup B=Q_{0}$. Assume that there exists no arrow of $Q$ that starts at a
vertex in $B$ and ends at a vertex in $A$. Assume further that $\left\vert
B\right\vert =N+1$. We need to prove that $\operatorname*{mut}\nolimits_{A,B}%
Q$ can be obtained from $Q$ by a sequence of mutations at sinks.

Notice that $B=Q_{0}\setminus A$ (since $A\cap B=\varnothing$ and $A\cup
B=Q_{0}$).

It is easy to see that there exist some $b\in B$ such that%
\begin{equation}
\text{there is no }e\in Q_{1}\text{ satisfying }t\left(  e\right)  =b\text{
and }s\left(  e\right)  \in B \label{sol.ps1.exe.1.1.a.3}%
\end{equation}
\footnote{\textit{Proof.} Assume the contrary. Thus, for every $b\in B$, there
is an $e\in Q_{1}$ satisfying $t\left(  e\right)  =b$ and $s\left(  e\right)
\in B$. Let us fix such an $e$ (for each $b\in B$), and denote it by $e_{b}$.
\par
Thus, for every $b\in B$, we have $e_{b}\in Q_{1}$ and $t\left(  e_{b}\right)
=b$ and $s\left(  e_{b}\right)  \in B$. We can thus define a sequence $\left(
b_{0},b_{1},b_{2},\ldots\right)  $ of vertices in $B$ recursively as follows:
Set $b_{0}=b$, and set $b_{i+1}=s\left(  e_{b_{i}}\right)  $ for every
$i\in\mathbb{N}$. Thus, $\left(  b_{0},b_{1},b_{2},\ldots\right)  $ is an
infinite sequence of elements of $B$. Since $B$ is a finite set, this sequence
must thus pass through an element twice (to say the least). In other words,
there are two positive integers $u$ and $v$ such that $u<v$ and $b_{u}=b_{v}$.
Consider these $u$ and $v$.
\par
Now, for every $i\in\mathbb{N}$, we have $t\left(  e_{b_{i}}\right)  =b_{i}$
(by the definition of $e_{b_{i}}$) and $s\left(  e_{b_{i}}\right)  =b_{i+1}$.
Thus, for every $i\in\mathbb{N}$, the arrow $e_{b_{i}}$ is an arrow from
$b_{i+1}$ to $b_{i}$. Thus, there is an arrow from $b_{i+1}$ to $b_{i}$ for
every $i\in\mathbb{N}$. In particular, we have an arrow from $b_{v}$ to
$b_{v-1}$, an arrow from $b_{v-1}$ to $b_{v-2}$, etc., and an arrow from
$b_{u+1}$ to $b_{u}$. Since $b_{u}=b_{v}$, these arrows form a cycle in $Q$,
which contradicts the hypothesis that the quiver $Q$ is acyclic. This
contradiction proves that our assumption was wrong, qed.}. Fix such a $b$.
Clearly, $b\notin A$ (since $b\in B=Q_{0}\setminus A$).

Now, $A\cup\left\{  b\right\}  $ and $B\setminus\left\{  b\right\}  $ are two
subsets of $Q_{0}$ such that $\left(  A\cup\left\{  b\right\}  \right)
\cap\left(  B\setminus\left\{  b\right\}  \right)  =\varnothing$ and $\left(
A\cup\left\{  b\right\}  \right)  \cup\left(  B\setminus\left\{  b\right\}
\right)  =Q_{0}$\ \ \ \ \footnote{\textit{Proof.} These are easy exercises in
set theory. Use $A\cap B=\varnothing$ and $A\cup B=Q_{0}$ and $b\in B$.}.
Furthermore, there exists no arrow of $Q$ that starts at a vertex in
$B\setminus\left\{  b\right\}  $ and ends at a vertex in $A\cup\left\{
b\right\}  $\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, there
exists an arrow of $Q$ that starts at a vertex in $B\setminus\left\{
b\right\}  $ and ends at a vertex in $A\cup\left\{  b\right\}  $. Let $e$ be
such an arrow. Then, $s\left(  e\right)  \in B\setminus\left\{  b\right\}  $
and $t\left(  e\right)  \in A\cup\left\{  b\right\}  $.
\par
We have $s\left(  e\right)  \in B\setminus\left\{  b\right\}  \subseteq B$.
Thus, $t\left(  e\right)  \neq b$ (because having $t\left(  e\right)  =b$
would contradict (\ref{sol.ps1.exe.1.1.a.3})). Combined with $t\left(
e\right)  \in A\cup\left\{  b\right\}  $, this yields $t\left(  e\right)
\in\left(  A\cup\left\{  b\right\}  \right)  \setminus\left\{  b\right\}
\subseteq A$. Thus, $e$ is an arrow of $Q$ that starts at a vertex in $B$
(since $s\left(  e\right)  \in B$) and ends at a vertex in $A$ (since
$t\left(  e\right)  \in A$). This contradicts our hypothesis that there exists
no arrow of $Q$ that starts at a vertex in $B$ and ends at a vertex in $A$.
This is the desired contradiction, and so we are done.}. Hence,
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ is a well-defined acyclic quiver. Moreover, since $b\in B$, we
have $\left\vert B\setminus\left\{  b\right\}  \right\vert
=\underbrace{\left\vert B\right\vert }_{=N+1}-1=N+1-1=N$. Thus, Exercise
\ref{exe.ps1.1.1} \textbf{(a)} can be applied to $A\cup\left\{  b\right\}  $
and $B\setminus\left\{  b\right\}  $ instead of $A$ and $B$ (by the induction
hypothesis). As a consequence, we conclude that $\operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$ can be
obtained from $Q$ by a sequence of mutations at sinks.

We shall now prove that $\operatorname*{mut}\nolimits_{A,B}Q$ can be obtained
from $\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus
\left\{  b\right\}  }Q$ by a mutation at a sink. In fact, $b$ is a sink of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
there exists an arrow $e$ of $\operatorname*{mut}\nolimits_{A\cup\left\{
b\right\}  ,B\setminus\left\{  b\right\}  }Q$ which starts at $b$. Consider
this $e$.
\par
Recall that $\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q$ was obtained from $Q$ by turning all arrows
of $Q$ which start at a vertex in $A\cup\left\{  b\right\}  $ and end at a
vertex in $B\setminus\left\{  b\right\}  $. Thus, every arrow of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ which starts at a vertex in $B\setminus\left\{  b\right\}  $
and ends at a vertex in $A\cup\left\{  b\right\}  $ has originally been going
in the opposite direction in $Q$ (because there exists no arrow of $Q$ that
starts at a vertex in $B\setminus\left\{  b\right\}  $ and ends at a vertex in
$A\cup\left\{  b\right\}  $), while all the other arrows of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ have been copied over unchanged from $Q$. The arrow $e$ of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ starts at $b$ (which is not an element of $B\setminus\left\{
b\right\}  $), so it does \textbf{not} start at a vertex in $B\setminus
\left\{  b\right\}  $ and end at a vertex in $A\cup\left\{  b\right\}  $;
therefore, the preceding sentence shows that this arrow $e$ has been copied
over unchanged from $Q$. In other words, the arrow $e$ starts at $b$ when
considered as an arrow of $Q$ as well. In other words, $s\left(  e\right)
=b$. (Recall that the functions $s$ and $t$ are part of the quiver $Q$; thus,
they map every arrow of $Q$ to its starting point and its terminal point,
respectively. The same arrows might have different starting points and
terminal points when regarded as arrows of $\operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$.)
\par
Recall that there exists no arrow of $Q$ that starts at a vertex in $B$ and
ends at a vertex in $A$. Thus, an arrow of $Q$ which starts at a vertex in $B$
must not end at a vertex in $A$. In particular, the arrow $e$ of $Q$ must not
end at a vertex in $A$ (because it starts at $b\in B$). Hence, the arrow $e$
of $Q$ ends at a vertex in $Q_{0}\setminus A=B$. In other words, $t\left(
e\right)  \in B$.
\par
We cannot have $t\left(  e\right)  =s\left(  e\right)  $ (because otherwise,
the arrow $e$ would form a cycle, but the quiver $Q$ is acyclic). Hence,
$t\left(  e\right)  \neq s\left(  e\right)  =b$ (since $e$ starts at $b$).
Combined with $t\left(  e\right)  \in B$, this yields $t\left(  e\right)  \in
B\setminus\left\{  b\right\}  $.
\par
Thus, the arrow $e$ of $Q$ starts at a vertex in $A\cup\left\{  b\right\}  $
(since $s\left(  e\right)  =b\in A\cup\left\{  b\right\}  $) and ends at a
vertex in $B\setminus\left\{  b\right\}  $ (since $t\left(  e\right)  \in
B\setminus\left\{  b\right\}  $). As we know, $\operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$ was
obtained from $Q$ by turning all such arrows. Hence, the arrow $e$ must have
been turned when it became an arrow of $\operatorname*{mut}\nolimits_{A\cup
\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$. But this contradicts
the fact that the arrow $e$ has been copied over unchanged from $Q$. This
contradiction proves that our assumption was wrong, qed.}. Hence, the mutation
$\mu_{b}\left(  \operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q\right)  $ is well-defined. We now have%
\begin{equation}
\operatorname*{mut}\nolimits_{A,B}Q=\mu_{b}\left(  \operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q\right)
\label{sol.ps1.exe.1.1.a.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps1.exe.1.1.a.7}):} We have $Q_{0}%
=A\cup\underbrace{B}_{=\left\{  b\right\}  \cup\left(  B\setminus\left\{
b\right\}  \right)  }=A\cup\left\{  b\right\}  \cup\left(  B\setminus\left\{
b\right\}  \right)  $.
\par
Recall that the quiver $\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q$ was obtained from $Q$ by turning all arrows
of $Q$ which start at a vertex in $A\cup\left\{  b\right\}  $ and end at a
vertex in $B\setminus\left\{  b\right\}  $. Furthermore, the quiver $\mu
_{b}\left(  \operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q\right)  $ was obtained from
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ by turning all arrows ending at $b$. Thus, $\mu_{b}\left(
\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q\right)  $ can be obtained from $Q$ by a two-step process, where
\par
\begin{itemize}
\item in the first step, we turn all arrows of $Q$ which start at a vertex in
$A\cup\left\{  b\right\}  $ and end at a vertex in $B\setminus\left\{
b\right\}  $;
\par
\item in the second step, we turn all arrows ending at $b$.
\end{itemize}
\par
Now, let us analyze what this two-step process does to an arrow of $Q$,
depending on where the arrow starts and ends:
\par
\begin{enumerate}
\item If $e$ is an arrow of $Q$ which ends at a vertex in $A$, then this arrow
never gets turned during our process. Indeed, let $e$ be such an arrow. Then,
$e$ ends at a vertex in $A$, and thus does not end at a vertex in $B$ (since
$A\cap B=\varnothing$); therefore, it does not end at a vertex in
$B\setminus\left\{  b\right\}  $ either. Hence, the first step does not turn
it. Therefore, after the first step, it still does not end at a vertex in $B$
(since it did not end at a vertex in $B$ originally). In particular, it does
not end at $b$ (since $b\in B$). Hence, it does not get turned at the second
step either. So, $e$ never turns, and thus retains its original direction
throughout the process.
\par
\item If $e$ is an arrow of $Q$ which ends at $b$, then this arrow gets turned
once (namely, at the second step). Thus, its direction is reversed at the end
of the process.
\par
\item If $e$ is an arrow of $Q$ which starts at a vertex in $A$ and ends at a
vertex in $B\setminus\left\{  b\right\}  $, then this arrow gets turned once
(namely, at the first step). Here is why: Let $e$ be an arrow of $Q$ which
starts at a vertex in $A$ and ends at a vertex in $B\setminus\left\{
b\right\}  $. Then, $e$ starts at a vertex in $A\cup\left\{  b\right\}  $ and
ends at a vertex in $B\setminus\left\{  b\right\}  $ (since $A\subseteq
A\cup\left\{  b\right\}  $). Thus, it gets turned at the first step. After
this, it becomes an arrow which ends at a vertex in $A$ (because originally it
started at a vertex in $A$), and so it does not end at $b$ (because $b\notin
A$). Therefore, it does not turn at the second step; hence, it has turned
exactly once altogether. Its direction is therefore reversed at the end of the
process.
\par
\item If $e$ is an arrow of $Q$ which starts at $b$ and ends at a vertex in
$B\setminus\left\{  b\right\}  $, then this arrow gets turned twice (once at
each step). Indeed, let $e$ be such an arrow. Then, $e$ starts at a vertex in
$A\cup\left\{  b\right\}  $ (namely, at $b$) and ends at a vertex in
$B\setminus\left\{  b\right\}  $. Hence, it gets turned at the first step.
After that, it ends at $b$ (because it used to start at $b$ before it was
turned), and therefore it gets turned again at the second step. Hence, the
direction of $e$ at the end of the two-step process is again the same as it
was in $Q$.
\par
\item If $e$ is an arrow of $Q$ which starts at a vertex in $B\setminus
\left\{  b\right\}  $ and ends at a vertex in $B\setminus\left\{  b\right\}
$, then this arrow never gets turned. Indeed, it starts at a vertex in
$B\setminus\left\{  b\right\}  $; thus, it does \textbf{not} start at a vertex
in $A\cup\left\{  b\right\}  $ (since $\underbrace{B}_{=Q_{0}\setminus
A}\setminus\left\{  b\right\}  =\left(  Q_{0}\setminus A\right)
\setminus\left\{  b\right\}  =Q_{0}\setminus\left(  A\cup\left\{  b\right\}
\right)  $). Hence, it does not get turned at the first step. Moreover, in
$Q$, this arrow $e$ does not end at $b$ (because it ends at a vertex in
$B\setminus\left\{  b\right\}  $); thus it does not end at $b$ after the first
step either (since it does not get turned at the first step). Hence, it does
not get turned at the second step either. Therefore, $e$ never gets turned,
and thus retains its original direction from $Q$ after the two-step process.
\end{enumerate}
\par
The five cases we have just considered cover all possibilities (because every
arrow $e$ either ends at a vertex in $A$ or ends at $b$ or ends at a vertex in
$B\setminus\left\{  b\right\}  $; and in the latter case, it either starts at
a vertex in $A$, or starts at $b$, or starts at a vertex in $B\setminus
\left\{  b\right\}  $ (since $Q_{0}=A\cup\left\{  b\right\}  \cup\left(
B\setminus\left\{  b\right\}  \right)  $)). From our case analysis, we can
draw the following conclusions:
\par
\begin{itemize}
\item If $e$ is an arrow of $Q$ which starts at a vertex in $A$ and ends at a
vertex in $B$, then the arrow $e$ has reversed its orientation at the end of
the two-step process. (This follows from our Cases 2 and 3 above.)
\par
\item If $e$ is an arrow of $Q$ which starts at a vertex in $B$ or ends at a
vertex in $A$, then this arrow $e$ has the same orientation at the end of the
two-step process as it did in $Q$. (Indeed, let us prove this. Let $e$ be an
arrow of $Q$ which starts at a vertex in $B$ or ends at a vertex in $A$. We
need to show that $e$ has the same orientation at the end of the two-step
process as it did in $Q$. If $e$ ends at a vertex in $A$, then this follows
from our analysis of Case 1. So let us assume that $e$ does not end at a
vertex in $A$. Hence, $e$ must start at a vertex in $B$ (since $e$ starts at a
vertex in $B$ or ends at a vertex in $A$). In other words, $s\left(  e\right)
\in B$. Hence, $t\left(  e\right)  \neq b$ (because if we had $t\left(
e\right)  =b$, then $e$ would contradict (\ref{sol.ps1.exe.1.1.a.3})). But
also $t\left(  e\right)  \notin A$ (since $e$ does not end at a vertex in
$A$), so that $t\left(  e\right)  \in Q_{0}\setminus A=B$ and thus $t\left(
e\right)  \in B\setminus\left\{  b\right\}  $ (since $t\left(  e\right)  \neq
b$). Hence, the arrow $e$ ends at a vertex in $B\setminus\left\{  b\right\}
$. It also starts at a vertex in $B$; thus, it either starts at $b$ or it
starts at a vertex in $B\setminus\left\{  b\right\}  $. Our claim now follows
from our analysis of Case 4 (in the case when $e$ starts at $b$) and from our
analysis of Case 5 (in the case when $e$ starts at a vertex in $B\setminus
\left\{  b\right\}  $). In either case, our claim is proven.)
\end{itemize}
\par
To summarize, the outcome of our two-step process is that every arrow $e$ of
$Q$ which starts at a vertex in $A$ and ends at a vertex in $B$ reverses its
orientation, while all other arrows preserve their orientation. In other
words, the outcome of our two-step process is the same as the outcome of
turning all arrows of $Q$ which start at a vertex in $A$ and end at a vertex
in $B$. But the latter outcome is $\operatorname*{mut}\nolimits_{A,B}Q$
(because this is how $\operatorname*{mut}\nolimits_{A,B}Q$ was defined), while
the former outcome is $\mu_{b}\left(  \operatorname*{mut}\nolimits_{A\cup
\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q\right)  $ (since we know
that $\mu_{b}\left(  \operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q\right)  $ can be obtained from $Q$ by our
two-step process). Thus, we have obtained $\mu_{b}\left(  \operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q\right)
=\operatorname*{mut}\nolimits_{A,B}Q$. This proves (\ref{sol.ps1.exe.1.1.a.7}%
).}. Therefore, $\operatorname*{mut}\nolimits_{A,B}Q$ can be obtained from
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ by a single mutation at a sink (namely, at the sink $b$). Since
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ (in turn) can be obtained from $Q$ by a sequence of mutations
at sinks, this shows that $\operatorname*{mut}\nolimits_{A,B}Q$ can be
obtained from $Q$ by a sequence of mutations at sinks (namely, we first need
to mutate at the sinks that give us $\operatorname*{mut}\nolimits_{A\cup
\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$, and then we have to
mutate at $b$). This proves that Exercise \ref{exe.ps1.1.1} \textbf{(a)} holds
whenever $\left\vert B\right\vert =N+1$. The induction step is complete, and
thus Exercise \ref{exe.ps1.1.1} \textbf{(a)} is solved.

\textbf{(b)} Let $i\in Q_{0}$ be a source in $Q$. Let $A=\left\{  i\right\}  $
and $B=Q_{0}\setminus A$. Then, $A$ and $B$ are two subsets of $Q_{0}$ such
that $A\cap B=\varnothing$ and $A\cup B=Q_{0}$. There exists no arrow of $Q$
that starts at a vertex in $B$ and ends at a vertex in $A$%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, there exists an
arrow of $Q$ which starts at a vertex in $B$ and ends at a vertex in $A$. Let
$e$ be such an arrow. Then, $e$ ends at a vertex in $A$. In other words,
$t\left(  e\right)  \in A=\left\{  i\right\}  $, so that $t\left(  e\right)
=i$. In other words, $e$ ends at $i$. But this is impossible, since $i$ is a
source. This contradiction proves that our assumption was wrong, qed.}. Hence,
the quiver $\operatorname*{mut}\nolimits_{A,B}Q$ is well-defined. Moreover,
this quiver $\operatorname*{mut}\nolimits_{A,B}Q$ is obtained by turning all
arrows of $Q$ which start at a vertex in $A$ and end at a vertex in $B$. But
these arrows are precisely the arrows of $Q$ starting at $i$%
\ \ \ \ \footnote{\textit{Proof.} Each arrow of $Q$ which starts at a vertex
in $A$ and ends at a vertex in $B$ must be an arrow starting at $i$ (because
it starts at a vertex in $A=\left\{  i\right\}  $, but the only vertex in
$\left\{  i\right\}  $ is $i$). It thus remains to prove the converse -- i.e.,
to prove that each arrow of $Q$ starting at $i$ is an arrow of $Q$ which
starts at a vertex in $A$ and ends at a vertex in $B$. So let $e$ be an arrow
of $Q$ starting at $i$. Then, $e$ clearly starts at a vertex in $A$ (since
$i\in\left\{  i\right\}  =A$). It remains to prove that $e$ ends at a vertex
in $B$. But $Q$ is acyclic, and thus we cannot have $s\left(  e\right)
=t\left(  e\right)  $ (since otherwise, the arrow $e$ would form a trivial
cycle). Hence, $s\left(  e\right)  \neq t\left(  e\right)  $. But $s\left(
e\right)  =i$ (since $e$ starts at $i$), so that $t\left(  e\right)  \neq
s\left(  e\right)  =i$ and thus $t\left(  e\right)  \in Q_{0}\setminus
\underbrace{\left\{  i\right\}  }_{=A}=Q_{0}\setminus A=B$. Hence, $e$ ends at
a vertex in $B$. This completes our proof.}. Hence, $\operatorname*{mut}%
\nolimits_{A,B}Q$ is obtained by turning all arrows of $Q$ starting at $i$.
But this is exactly how we defined $\mu_{i}\left(  Q\right)  $. Therefore,
$\operatorname*{mut}\nolimits_{A,B}Q=\mu_{i}\left(  Q\right)  $. Now, Exercise
\ref{exe.ps1.1.1} \textbf{(a)} shows that $\operatorname*{mut}\nolimits_{A,B}%
Q$ can be obtained from $Q$ by a sequence of mutations at sinks. Hence,
$\mu_{i}\left(  Q\right)  $ can be obtained from $Q$ by a sequence of
mutations at sinks (since $\operatorname*{mut}\nolimits_{A,B}Q=\mu_{i}\left(
Q\right)  $). Exercise \ref{exe.ps1.1.1} \textbf{(b)} is proven.

\textbf{(c)} Let $Q^{\prime}$ be any acyclic quiver which can be obtained from
$Q$ by turning some of its arrows. We need to prove that $Q^{\prime}$ can also
be obtained from $Q$ by a sequence of mutations at sinks. But \cite[proof of
Proposition 2.2.8]{Lampe} shows that $Q^{\prime}$ can be obtained from $Q$ by
a sequence of mutations at sinks and sources. Since every mutation at a source
can be simulated by a sequence of mutations at sinks (by Exercise
\ref{exe.ps1.1.1} \textbf{(b)}), this yields that $Q^{\prime}$ can be obtained
from $Q$ by a sequence of mutations at sinks. This solves Exercise
\ref{exe.ps1.1.1} \textbf{(c)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps1.1.2}}

\begin{proof}
[Solution to Exercise \ref{exe.ps1.1.2}.]For every $N\in\mathbb{N}$, we let
$\left[  N\right]  $ denote the $N$-element set $\left\{  1,2,\ldots
,N\right\}  $.

For every $i\in\mathbb{N}$ and $j\in\mathbb{N}$, we define a \textit{filled
}$\left(  i,j\right)  $\textit{-set} to mean a subset $S$ of $\left[
i\right]  \times\left[  j\right]  $ satisfying the following two conditions:

\begin{enumerate}
\item For every $k\in\left[  i\right]  $, at least one element of $S$ has its
first coordinate\footnote{The \textit{coordinates} of a pair $\left(
u,v\right)  $ mean the entries $u$ and $v$. Thus, the first coordinate of
$\left(  u,v\right)  $ is $u$.} equal to $k$.

\item For every $\ell\in\left[  j\right]  $, at least one element of $S$ has
its second coordinate equal to $\ell$.
\end{enumerate}

We can visualize subsets $S$ of $\left[  i\right]  \times\left[  j\right]  $
as selections of boxes in a rectangular table that has $i$ rows and $j$
columns\footnote{Namely, for every $\left(  u,v\right)  \in S$, we select the
box in row $u$ and column $v$.}. For instance, for $i=3$ and $j=4$, we can
represent the subset $S=\left\{  \left(  1,1\right)  ,\left(  1,3\right)
,\left(  2,2\right)  ,\left(  3,1\right)  ,\left(  3,3\right)  ,\left(
3,4\right)  \right\}  $ of $\left[  i\right]  \times\left[  j\right]  $ as the
selection%
\begin{equation}%
\begin{tabular}
[c]{|c|c|c|c|}\hline
X &  & X & \\\hline
& X &  & \\\hline
X &  & X & X\\\hline
\end{tabular}
\ \ \label{sol.ps1.1.2.exa1}%
\end{equation}
(where the rows are labelled $1,2,3$ from top to bottom, the columns are
labelled $1,2,3,4$ from left to right, as in a matrix, and where the elements
of $S$ are marked with X'es). Condition 1 then says that every row contains at
least one selected box (i.e., at least one X); and Condition 2 says that every
column contains at least one selected box. Our example (\ref{sol.ps1.1.2.exa1}%
) satisfies these two conditions, but (for instance) the subset%
\[%
\begin{tabular}
[c]{|c|c|c|c|}\hline
X & X &  & X\\\hline
& X & \phantom{X} & \\\hline
X &  &  & X\\\hline
\end{tabular}
\ \
\]
does not (it fails Condition 2).

For every $i\in\mathbb{N}$ and $j\in\mathbb{N}$, we define a nonnegative
integer $c_{i,j}$ as the number of all filled $\left(  i,j\right)  $-sets
which have $n$ elements\footnote{When we say \textquotedblleft have $n$
elements\textquotedblright, we mean \textquotedblleft have exactly $n$
elements\textquotedblright, not \textquotedblleft have at least $n$
elements\textquotedblright.}. We shall now show that (\ref{eq.exe.1.2.claim})
is satisfied.

Indeed, let us first prove that any $x\in\mathbb{N}$ and $y\in\mathbb{N}$
satisfy%
\begin{equation}
\dbinom{xy}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}{i}\dbinom{y}{j}.
\label{sol.ps1.1.2.xyclaim}%
\end{equation}


Keep in mind that (\ref{sol.ps1.1.2.xyclaim}) and (\ref{eq.exe.1.2.claim}) are
different claims: The $x$ and $y$ in (\ref{sol.ps1.1.2.xyclaim}) are
nonnegative integers, while the $X$ and $Y$ in (\ref{eq.exe.1.2.claim}) are indeterminates!

\textit{Proof of (\ref{sol.ps1.1.2.xyclaim}):} Let $x\in\mathbb{N}$ and
$y\in\mathbb{N}$. Recall that $\dbinom{xy}{n}$ is the number of all
$n$-element subsets of a given $xy$-element set. Since $\left[  x\right]
\times\left[  y\right]  $ is an $xy$-element set, we thus conclude that
$\dbinom{xy}{n}$ is the number of all $n$-element subsets of $\left[
x\right]  \times\left[  y\right]  $.

Now, let us find a different way to count all $n$-element subsets of $\left[
x\right]  \times\left[  y\right]  $. As above, we can visualize such subsets
as selections of boxes in a rectangular table that has $x$ rows and $y$
columns; we again mark the selected boxes by X'es. We want to count all ways
to select $n$ boxes in this table, i.e., to place $n$ X'es in the table. We
can place $n$ X'es in the table by means of the following process:

\begin{enumerate}
\item We choose how many rows of the table will have at least one X. This can
be a number from $0$ to $n$ (inclusive)\footnote{I am not saying that any
number from $0$ to $n$ (inclusive) is possible; I am just saying that this
will always be a number from $0$ to $n$ (inclusive). Here is why:
\par
Clearly, the number of rows of the table that will have at least one X is
$\geq0$. But it is also $\leq n$, because we want to place only $n$ X'es in
the table, and these $n$ X'es will clearly occupy at most $n$ rows.}; we
denote it by $i$.

\item We choose how many columns of the table will have at least one X. This
can be a number from $0$ to $n$ (inclusive)\footnote{This follows by a similar
argument as the analogous statement in Step 1.}; we denote it by $j$.

\item We choose the $i$ rows of the table that will have at least one X. This
can be done in $\dbinom{x}{i}$ ways (since there are $x$ rows to choose from).

\item We choose the $j$ columns of the table that will have at least one X.
This can be done in $\dbinom{y}{j}$ ways (since there are $y$ columns to
choose from).

\item It remains to place $n$ X'es in the table in such a way that the rows
that contain at least one X are precisely the $i$ chosen rows, and the columns
that contain at least one X are precisely the $j$ chosen columns. To do so, we
can temporarily remove all the remaining $x-i$ rows and $y-j$ columns. We are
then left with a rectangular table that has $i$ rows and $j$ columns, and now
we need to place $n$ X'es in it in such a way that every row contains at least
one X and every column contains at least one X. As we know, the number of ways
to do this is $c_{i,j}$ (because this is how $c_{i,j}$ was defined).
\end{enumerate}

This process makes it clear that the total number of ways to place $n$ X'es in
the (original) table is $\sum_{i=0}^{n}\sum_{j=0}^{n}\dbinom{x}{i}\dbinom
{y}{j}c_{i,j}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}{i}\dbinom{y}{j}$.
In other words, the number of all $n$-element subsets of $\left[  x\right]
\times\left[  y\right]  $ is $\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}%
{i}\dbinom{y}{j}$.

So we know that this number equals both $\dbinom{xy}{n}$ and $\sum_{i=0}%
^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}{i}\dbinom{y}{j}$ at the same time.
Comparing these values, we obtain (\ref{sol.ps1.1.2.xyclaim}).

Now that (\ref{sol.ps1.1.2.xyclaim}) is proven, we can finally solve the
exercise. We define two polynomials $P$ and $Q$ in the indeterminates $X$ and
$Y$ with rational coefficients by setting%
\begin{align*}
P  &  =\dbinom{XY}{n};\\
Q  &  =\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{X}{i}\dbinom{Y}{j}%
\end{align*}
\footnote{These are both polynomials since $\dbinom{XY}{n}$, $\dbinom{X}{i}$
and $\dbinom{Y}{j}$ are polynomials in $X$ and $Y$.}. The equality
(\ref{sol.ps1.1.2.xyclaim}) (which we have proven) states that $P\left(
x,y\right)  =Q\left(  x,y\right)  $ for all $x\in\mathbb{N}$ and
$y\in\mathbb{N}$. Thus, Lemma \ref{lem.polyeq} \textbf{(d)} yields that $P=Q$.
Recalling how $P$ and $Q$ are defined, we see that this is precisely the
equality (\ref{eq.exe.1.2.claim}).

Hence, (\ref{eq.exe.1.2.claim}) is proven, and Exercise \ref{exe.ps1.1.2} solved.
\end{proof}

\begin{remark}
I learnt the above solution to Exercise \ref{exe.ps1.1.2}
\href{http://www.artofproblemsolving.com/community/c6h299793p1623722}{from
Gjergji Zaimi on AoPS}.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps1.1.3}}

\begin{proof}
[Solution to Exercise \ref{exe.ps1.1.3}.]Here is one possible solution:

Exercise \ref{exe.ps1.1.2} shows that, for every $n\in\mathbb{N}$, there exist
\textbf{nonnegative} integers $c_{i,j}$ for all $0\leq i\leq n$ and $0\leq
j\leq n$ such that (\ref{eq.exe.1.2.claim}) holds. We denote these integers
$c_{i,j}$ by $c_{i,j,n}$ (in order to make their dependence on $n$ explicit).
Thus, for every $n\in\mathbb{N}$, the nonnegative integers $c_{i,j,n}$ defined
for all $0\leq i\leq n$ and $0\leq j\leq n$ satisfy%
\begin{equation}
\dbinom{XY}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j,n}\dbinom{X}{i}\dbinom{Y}%
{j}. \label{pf.exe.1.3.1}%
\end{equation}
Substituting $a$ and $X$ for $X$ and $Y$ in this equality, we obtain%
\begin{equation}
\dbinom{aX}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j,n}\dbinom{a}{i}\dbinom{X}%
{j}. \label{pf.exe.1.3.2}%
\end{equation}


Now, Theorem \ref{thm.vandermonde} (applied to $n=c$) yields%
\[
\dbinom{X+Y}{c}=\sum_{k=0}^{c}\dbinom{X}{k}\dbinom{Y}{c-k}.
\]
Substituting $aX$ and $b$ for $X$ and $Y$ in this equality, we obtain%
\begin{align}
\dbinom{aX+b}{c}  &  =\sum_{k=0}^{c}\underbrace{\dbinom{aX}{k}}%
_{\substack{=\sum_{i=0}^{k}\sum_{j=0}^{k}c_{i,j,k}\dbinom{a}{i}\dbinom{X}%
{j}\\\text{(by (\ref{pf.exe.1.3.2}), applied to }n=k\text{)}}}\dbinom{b}%
{c-k}\nonumber\\
&  =\sum_{k=0}^{c}\left(  \sum_{i=0}^{k}\sum_{j=0}^{k}c_{i,j,k}\dbinom{a}%
{i}\dbinom{X}{j}\right)  \dbinom{b}{c-k}\nonumber\\
&  =\underbrace{\sum_{k=0}^{c}\sum_{i=0}^{k}\sum_{j=0}^{k}}_{=\sum_{j=0}%
^{c}\sum_{k=j}^{c}\sum_{i=0}^{k}}c_{i,j,k}\dbinom{a}{i}\underbrace{\dbinom
{X}{j}\dbinom{b}{c-k}}_{=\dbinom{b}{c-k}\dbinom{X}{j}}\nonumber\\
&  =\sum_{j=0}^{c}\sum_{k=j}^{c}\sum_{i=0}^{k}c_{i,j,k}\dbinom{a}{i}\dbinom
{b}{c-k}\dbinom{X}{j}. \label{pf.exe.1.3.5}%
\end{align}


Now, for every $j\in\left\{  0,1,\ldots,c\right\}  $, define an integer
$d_{j}$ by $d_{j}=\sum_{k=j}^{c}\sum_{i=0}^{k}c_{i,j,k}\dbinom{a}{i}\dbinom
{b}{c-k}$. This $d_{j}$ is clearly a nonnegative integer (since the
$c_{i,j,n}$ are nonnegative, and so are the binomial coefficients $\dbinom
{a}{i}$ and $\dbinom{b}{c-k}$ (due to $a$ and $b$ being nonnegative)). Then,
(\ref{pf.exe.1.3.5}) becomes%
\[
\dbinom{aX+b}{c}=\sum_{j=0}^{c}\underbrace{\sum_{k=j}^{c}\sum_{i=0}%
^{k}c_{i,j,k}\dbinom{a}{i}\dbinom{b}{c-k}}_{=d_{j}}\dbinom{X}{j}=\sum
_{j=0}^{c}d_{j}\dbinom{X}{j}=\sum_{i=0}^{c}d_{i}\dbinom{X}{i}.
\]
Exercise \ref{exe.ps1.1.3} is thus solved.
\end{proof}

\subsection{Solution to Additional exercise \ref{exeadd.AoPS333199}}

We shall prove the following generalization of Additional exercise
\ref{exeadd.AoPS333199}:

\begin{proposition}
\label{prop.AoPS333199.gen}Let $\mathbb{K}$ be a commutative ring. (See
Definition \ref{def.commring} for the definition of a \textquotedblleft
commutative ring\textquotedblright. For example, we can set $\mathbb{K}%
=\mathbb{Z}$ or $\mathbb{K}=\mathbb{R}$ or $\mathbb{K}=\mathbb{Q}\left[
X\right]  $.) Let $x$ and $y$ be two elements of $\mathbb{K}$. For any
$n\in\mathbb{N}$ and $m\in\mathbb{N}$, define $Y_{m,n}\in\mathbb{K}$ by%
\[
Y_{m,n}=\sum_{k=0}^{n}y^{k}\dbinom{n}{k}\left(  x^{n-k}+y\right)  ^{m}.
\]
Then, $Y_{m,n}=Y_{n,m}$ for any $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.AoPS333199.gen}.]For any $n\in\mathbb{N}$ and
$m\in\mathbb{N}$, we have%
\begin{align}
Y_{m,n}  &  =\sum_{k=0}^{n}y^{k}\dbinom{n}{k}\left(  x^{n-k}+y\right)
^{m}=\sum_{\ell=0}^{n}y^{\ell}\dbinom{n}{\ell}\left(  \underbrace{x^{n-\ell
}+y}_{=y+x^{n-\ell}}\right)  ^{m}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}k\text{ as }\ell\right) \nonumber\\
&  =\sum_{\ell=0}^{n}y^{\ell}\dbinom{n}{\ell}\underbrace{\left(  y+x^{n-\ell
}\right)  ^{m}}_{\substack{=\sum_{k=0}^{m}\dbinom{m}{k}y^{k}\left(  x^{n-\ell
}\right)  ^{m-k}\\\text{(by the binomial formula)}}}=\sum_{\ell=0}^{n}y^{\ell
}\dbinom{n}{\ell}\left(  \sum_{k=0}^{m}\dbinom{m}{k}y^{k}\left(  x^{n-\ell
}\right)  ^{m-k}\right) \nonumber\\
&  =\sum_{\ell=0}^{n}\sum_{k=0}^{m}\underbrace{y^{\ell}\dbinom{n}{\ell}%
\dbinom{m}{k}}_{=\dbinom{n}{\ell}\dbinom{m}{k}y^{\ell}}y^{k}\left(  x^{n-\ell
}\right)  ^{m-k}=\sum_{\ell=0}^{n}\sum_{k=0}^{m}\dbinom{n}{\ell}\dbinom{m}%
{k}\underbrace{y^{\ell}y^{k}}_{=y^{\ell+k}}\underbrace{\left(  x^{n-\ell
}\right)  ^{m-k}}_{=x^{\left(  n-\ell\right)  \left(  m-k\right)  }%
}\nonumber\\
&  =\sum_{\ell=0}^{n}\sum_{k=0}^{m}\dbinom{n}{\ell}\dbinom{m}{k}y^{\ell
+k}x^{\left(  n-\ell\right)  \left(  m-k\right)  }.
\label{pf.prop.AoPS333199.gen.1}%
\end{align}
Now, let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Then,
(\ref{pf.prop.AoPS333199.gen.1}) (applied to $m$ and $n$ instead of $n$ and
$m$) shows that%
\begin{align*}
Y_{n,m}  &  =\underbrace{\sum_{\ell=0}^{m}\sum_{k=0}^{n}}_{=\sum_{k=0}^{n}%
\sum_{\ell=0}^{m}}\underbrace{\dbinom{m}{\ell}\dbinom{n}{k}}_{=\dbinom{n}%
{k}\dbinom{m}{\ell}}\underbrace{y^{\ell+k}}_{=y^{k+\ell}}%
\underbrace{x^{\left(  m-\ell\right)  \left(  n-k\right)  }}_{=x^{\left(
n-k\right)  \left(  m-\ell\right)  }}\\
&  =\sum_{k=0}^{n}\sum_{\ell=0}^{m}\dbinom{n}{k}\dbinom{m}{\ell}y^{k+\ell
}x^{\left(  n-k\right)  \left(  m-\ell\right)  }=\sum_{k=0}^{n}\sum_{g=0}%
^{m}\dbinom{n}{k}\dbinom{m}{g}y^{k+g}x^{\left(  n-k\right)  \left(
m-g\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}\ell\text{ as }g\right) \\
&  =\sum_{\ell=0}^{n}\sum_{g=0}^{m}\dbinom{n}{\ell}\dbinom{m}{g}y^{\ell
+g}x^{\left(  n-\ell\right)  \left(  m-g\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}k\text{ as }\ell\right) \\
&  =\sum_{\ell=0}^{n}\sum_{k=0}^{m}\dbinom{n}{\ell}\dbinom{m}{k}y^{\ell
+k}x^{\left(  n-\ell\right)  \left(  m-k\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}g\text{ as }k\right) \\
&  =Y_{m,n}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.AoPS333199.gen.1})}\right)  .
\end{align*}
This proves Proposition \ref{prop.AoPS333199.gen}.
\end{proof}

\begin{proof}
[Solution to Additional exercise \ref{exeadd.AoPS333199}.]Set $\mathbb{K}%
=\mathbb{Z}\left[  X\right]  $, and define two elements $x$ and $y$ of
$\mathbb{K}$ by $x=X$ and $y=-1$. For any $n\in\mathbb{N}$ and $m\in
\mathbb{N}$, define $Y_{m,n}\in\mathbb{K}$ as in Proposition
\ref{prop.AoPS333199.gen}. Then, for any $n\in\mathbb{N}$ and $m\in\mathbb{N}%
$, we have%
\begin{align}
Y_{m,n}  &  =\sum_{k=0}^{n}\underbrace{y^{k}}_{\substack{=\left(  -1\right)
^{k}\\\text{(since }y=-1\text{)}}}\dbinom{n}{k}\left(  \underbrace{x^{n-k}%
}_{\substack{=X^{n-k}\\\text{(since }x=X\text{)}}}+\underbrace{y}%
_{=-1}\right)  ^{m}\nonumber\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\left(
\underbrace{X^{n-k}+\left(  -1\right)  }_{=X^{n-k}-1}\right)  ^{m}=\sum
_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\left(  X^{n-k}-1\right)
^{m}=Z_{m,n}. \label{sol.exeadd.AoPS333199.1}%
\end{align}


Now, fix $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Applying
(\ref{sol.exeadd.AoPS333199.1}) to $m$ and $n$ instead of $n$ and $m$, we
obtain $Y_{n,m}=Z_{n,m}$. But Proposition \ref{prop.AoPS333199.gen} shows that
$Y_{m,n}=Y_{n,m}$. Comparing this with (\ref{sol.exeadd.AoPS333199.1}), we
obtain $Y_{n,m}=Z_{m,n}$. Comparing this with $Y_{n,m}=Z_{n,m}$, we obtain
$Z_{m,n}=Z_{n,m}$. This solves Exercise \ref{exeadd.AoPS333199}.
\end{proof}

\begin{remark}
Two solutions to Exercise \ref{exeadd.AoPS333199} are sketched in
\href{http://www.artofproblemsolving.com/community/c6h333199p1782800}{http://www.artofproblemsolving.com/community/c6h333199p1782800}
. One is essentially the solution given above (except in lesser generality);
the other is combinatorial.
\end{remark}

\subsection{Solution to Additional exercise \ref{exeadd.AoPS262752}}

We shall give two solutions to Additional exercise \ref{exeadd.AoPS262752}.
The first solution follows the Hint given in the exercise, and illustrates
both the use of Lemma \ref{lem.polyeq} \textbf{(b)} and of \textquotedblleft
generating functions\textquotedblright\ (the strategy of proving identities by
identifying both sides as coefficients in polynomials or power series). The
second solution is of a more classical nature, using no new methods but a
tricky application of Theorem \ref{thm.vandermonde}.

The crux of the first solution is the proof of the following lemma (which
appears in \cite[(5.55)]{GKP}):

\begin{lemma}
\label{lem.AoPS262752.r}Let $n\in\mathbb{N}$ and $x\in\mathbb{N}$. Then,%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{k}\dbinom{x}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]

\end{lemma}

Thus, Lemma \ref{lem.AoPS262752.r} is obtained from Additional exercise
\ref{exeadd.AoPS262752} by substituting a nonnegative integer $x$ for the
indeterminate $X$. It thus is clear that Lemma \ref{lem.AoPS262752.r} follows
from Additional exercise \ref{exeadd.AoPS262752}. However, for us, the
interest lies in the opposite implication: We shall derive Additional exercise
\ref{exeadd.AoPS262752} from Lemma \ref{lem.AoPS262752.r}. Let us, however,
prove Lemma \ref{lem.AoPS262752.r} first. But before we do this, let us state
a version of the binomial theorem:

\begin{proposition}
\label{prop.AoPS262752.binom}Let $x\in\mathbb{N}$. Then,%
\[
\left(  1+X\right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}%
\]
(an equality between polynomials in $\mathbb{Z}\left[  X\right]  $). (The sum
$\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}$ is an infinite sum, but only
finitely many of its addends are nonzero, so it is well-defined.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.AoPS262752.binom}.]We have%
\begin{align*}
\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}  &  =\underbrace{\sum_{\substack{k\in
\mathbb{N};\\k\leq x}}}_{=\sum_{k=0}^{x}}\dbinom{x}{k}X^{k}+\sum
_{\substack{k\in\mathbb{N};\\k>x}}\underbrace{\dbinom{x}{k}}%
_{\substack{=0\\\text{(by (\ref{eq.binom.0}) (applied to }x\text{ and
}k\\\text{instead of }m\text{ and }n\text{) (since }x<k\text{ (since
}k>x\text{)))}}}X^{k}\\
&  =\sum_{k=0}^{x}\dbinom{x}{k}X^{k}+\underbrace{\sum_{\substack{k\in
\mathbb{N};\\k>x}}0X^{k}}_{=0}=\sum_{k=0}^{x}\dbinom{x}{k}X^{k}.
\end{align*}
Comparing this with
\begin{align*}
\left(  \underbrace{1+X}_{=X+1}\right)  ^{x}  &  =\left(  X+1\right)
^{x}=\sum_{k=0}^{x}\dbinom{x}{k}X^{k}\underbrace{1^{x-k}}_{=1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the binomial formula}\right) \\
&  =\sum_{k=0}^{x}\dbinom{x}{k}X^{k},
\end{align*}
we obtain $\left(  1+X\right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}$.
This proves Proposition \ref{prop.AoPS262752.binom}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.AoPS262752.r}.]Proposition
\ref{prop.AoPS262752.binom} yields
\begin{equation}
\left(  1+X\right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}.
\label{pf.lem.AoPS262752.r.1}%
\end{equation}
Substituting $-X$ for $X$ in this equality, we obtain%
\[
\left(  1+\left(  -X\right)  \right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}%
{k}\underbrace{\left(  -X\right)  ^{k}}_{=\left(  -1\right)  ^{k}X^{k}}%
=\sum_{k\in\mathbb{N}}\dbinom{x}{k}\left(  -1\right)  ^{k}X^{k}=\sum
_{k\in\mathbb{N}}\left(  -1\right)  ^{k}\dbinom{x}{k}X^{k}.
\]
Since $1+\left(  -X\right)  =1-X$, this rewrites as%
\[
\left(  1-X\right)  ^{x}=\sum_{k\in\mathbb{N}}\left(  -1\right)  ^{k}%
\dbinom{x}{k}X^{k}.
\]
Multiplying this equality with (\ref{pf.lem.AoPS262752.r.1}), we obtain%
\begin{align*}
\left(  1-X\right)  ^{x}\left(  1+X\right)  ^{x}  &  =\left(  \sum
_{k\in\mathbb{N}}\left(  -1\right)  ^{k}\dbinom{x}{k}X^{k}\right)  \left(
\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}\right) \\
&  =\sum_{k\in\mathbb{N}}\left(  \sum_{i=0}^{k}\left(  -1\right)  ^{i}%
\dbinom{x}{i}\dbinom{x}{k-i}\right)  X^{k}%
\end{align*}
(according to the definition of the product of two polynomials). Hence,
\begin{align}
&  \left(  \text{the coefficient of }X^{n}\text{ in }\left(  1-X\right)
^{x}\left(  1+X\right)  ^{x}\right) \nonumber\\
&  =\sum_{i=0}^{n}\left(  -1\right)  ^{i}\dbinom{x}{i}\dbinom{x}{n-i}%
=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{k}\dbinom{x}{n-k}
\label{pf.lem.AoPS262752.r.6}%
\end{align}
(here, we have renamed the summation index $i$ as $k$).

On the other hand,%
\begin{align*}
\left(  1-X\right)  ^{x}\left(  1+X\right)  ^{x}  &  =\left(
\underbrace{\left(  1-X\right)  \left(  1+X\right)  }_{=1-X^{2}=1+\left(
-X\right)  ^{2}}\right)  ^{x}=\left(  1+\left(  -X\right)  ^{2}\right)
^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}\underbrace{\left(  -X^{2}\right)
^{k}}_{=\left(  -1\right)  ^{k}\left(  X^{2}\right)  ^{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{this follows from substituting }%
-X^{2}\text{ for }X\text{ in (\ref{pf.lem.AoPS262752.r.1})}\right) \\
&  =\sum_{k\in\mathbb{N}}\underbrace{\dbinom{x}{k}\left(  -1\right)  ^{k}%
}_{=\left(  -1\right)  ^{k}\dbinom{x}{k}}\underbrace{\left(  X^{2}\right)
^{k}}_{=X^{2k}}=\sum_{k\in\mathbb{N}}\left(  -1\right)  ^{k}\dbinom{x}%
{k}X^{2k}.
\end{align*}
Hence,%
\begin{align*}
&  \left(  \text{the coefficient of }X^{n}\text{ in }\left(  1-X\right)
^{x}\left(  1+X\right)  ^{x}\right) \\
&  =%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\end{align*}
Comparing this with (\ref{pf.lem.AoPS262752.r.6}), we obtain%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{k}\dbinom{x}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
This proves Lemma \ref{lem.AoPS262752.r}.
\end{proof}

\begin{proof}
[First solution to Additional exercise \ref{exeadd.AoPS262752}.]Define two
polynomials $P$ and $Q$ (with rational coefficients) by%
\begin{equation}
P=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}
\label{sol.exeadd.AoPS262752.sol1.P}%
\end{equation}
and%
\begin{equation}
Q=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
. \label{sol.exeadd.AoPS262752.sol1.Q}%
\end{equation}
For every $x\in\mathbb{N}$, we have%
\begin{align*}
P\left(  x\right)   &  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}%
{k}\dbinom{x}{n-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}P\right) \\
&  =%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.AoPS262752.r}}\right) \\
&  =Q\left(  x\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}Q\right)  .
\end{align*}
Hence, Lemma \ref{lem.polyeq} \textbf{(b)} shows that $P=Q$. In light of
(\ref{sol.exeadd.AoPS262752.sol1.P}) and (\ref{sol.exeadd.AoPS262752.sol1.Q}),
this rewrites as
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
This solves Additional exercise \ref{exeadd.AoPS262752}.
\end{proof}

Now let us prepare for the second solution to Additional exercise
\ref{exeadd.AoPS262752}. We shall use the following notation (known as
\href{https://en.wikipedia.org/wiki/Iverson_bracket}{the \textit{Iverson
bracket}}): If $\mathcal{A}$ is any logical statement, then $\left[
\mathcal{A}\right]  $ will mean the integer $%
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}%
\end{cases}
$. For example, $\left[  1+1=2\right]  =1$ (since $1+1=2$ is true), whereas
$\left[  1+1=1\right]  =0$ (since $1+1=1$ is false). Clearly, if $\mathcal{A}$
and $\mathcal{B}$ are two equivalent logical statements, then $\left[
\mathcal{A}\right]  =\left[  \mathcal{B}\right]  $.

We notice that every $n\in\mathbb{N}$ satisfies%
\[
\left[  n=0\right]  =%
\begin{cases}
1, & \text{if }n=0\text{ is true};\\
0, & \text{if }n=0\text{ is false}%
\end{cases}
=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n\neq0
\end{cases}
=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}%
\]
(by Proposition \ref{prop.binom.bin-id} \textbf{(c)}). In other words, every
$n\in\mathbb{N}$ satisfies%
\begin{equation}
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}=\left[  n=0\right]  .
\label{sol.exeadd.AoPS262752.n=0}%
\end{equation}


Next, we state a simple fact:

\begin{lemma}
\label{lem.exeadd.AoPS262752.1}Let $m\in\mathbb{N}$ and $i\in\mathbb{N}$.
Then,%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\left[  m=i\right]  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.exeadd.AoPS262752.1}.]If $m<i$, then Lemma
\ref{lem.exeadd.AoPS262752.1} holds\footnote{\textit{Proof.} Assume that
$m<i$. Then, $m\neq i$. Thus, $\left[  m=i\right]  =0$.
\par
But every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies $k\leq m<i$. Hence,
every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies $\dbinom{k}{i}=0$ (by
(\ref{eq.binom.0}), applied to $k$ and $i$ instead of $m$ and $n$). Now,
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}}_{=0}\dbinom
{m}{k}=\sum_{k=0}^{m}\left(  -1\right)  ^{k}0\dbinom{m}{k}=0=\left[
m=i\right]
\]
(since $\left[  m=i\right]  =0$). In other words, Lemma
\ref{lem.exeadd.AoPS262752.1} holds, qed.}. Hence, for the rest of this proof
of Lemma \ref{lem.exeadd.AoPS262752.1}, we can WLOG assume that we don't have
$m<i$. Assume this.

We have $m\geq i$ (since we don't have $m<i$). Hence, $m\geq i\geq0$, so that%
\begin{align}
&  \sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}\dbinom{m}%
{k}}_{=\dbinom{m}{k}\dbinom{k}{i}}\nonumber\\
&  =\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{m}{k}\dbinom{k}{i}\nonumber\\
&  =\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}\underbrace{\dbinom
{k}{i}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}) (applied to }k\text{ and
}i\\\text{instead of }m\text{ and }n\text{) (since }k<i\text{))}}}+\sum
_{k=i}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{m}{k}\dbinom{k}{i}%
}_{\substack{=\dbinom{m}{i}\dbinom{m-i}{k-i}\\\text{(by
(\ref{eq.binom.trinom-rev.m}) (applied to }k\text{ and }i\\\text{instead of
}i\text{ and }a\text{) (since }k\geq i\text{))}}}\nonumber\\
&  =\underbrace{\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}0}%
_{=0}+\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
=\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
\nonumber\\
&  =\sum_{k=0}^{m-i}\underbrace{\left(  -1\right)  ^{k+i}}_{=\left(
-1\right)  ^{k}\left(  -1\right)  ^{i}}\dbinom{m}{i}\dbinom{m-i}{k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+i\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=0}^{m-i}\left(  -1\right)  ^{k}\left(  -1\right)  ^{i}\dbinom
{m}{i}\dbinom{m-i}{k}=\left(  -1\right)  ^{i}\dbinom{m}{i}\underbrace{\sum
_{k=0}^{m-i}\left(  -1\right)  ^{k}\dbinom{m-i}{k}}_{\substack{=\left[
m-i=0\right]  \\\text{(by (\ref{sol.exeadd.AoPS262752.n=0}) (applied to
}n=m-i\text{))}}}\nonumber\\
&  =\left(  -1\right)  ^{i}\dbinom{m}{i}\left[  \underbrace{m-i=0}%
_{\substack{\text{this is equivalent to}\\m=i}}\right]  =\left(  -1\right)
^{i}\dbinom{m}{i}\left[  m=i\right]  .
\label{pf.lem.exeadd.AoPS262752.1.short.main}%
\end{align}
But it is easy to see that $\dbinom{m}{i}\left[  m=i\right]  =\left[
m=i\right]  $\ \ \ \ \footnote{\textit{Proof.} We have $\underbrace{\dbinom
{i}{i}}_{=1}\left[  i=i\right]  =\left[  i=i\right]  $. In other words, the
equality $\dbinom{m}{i}\left[  m=i\right]  =\left[  m=i\right]  $ holds in the
case when $m=i$. Therefore, in order to prove this equality, we only need to
consider the case when $m\neq i$. So assume that $m\neq i$. Then, $\left[
m=i\right]  =0$, and thus $\dbinom{m}{i}\underbrace{\left[  m=i\right]  }%
_{=0}=0=\left[  m=i\right]  $, qed.}. Hence,
(\ref{pf.lem.exeadd.AoPS262752.1.short.main}) becomes%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\underbrace{\dbinom{m}{i}\left[  m=i\right]  }_{=\left[
m=i\right]  }=\left(  -1\right)  ^{i}\left[  m=i\right]  .
\]
This proves Lemma \ref{lem.exeadd.AoPS262752.1}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.exeadd.AoPS262752.1}.]If $m<i$, then Lemma
\ref{lem.exeadd.AoPS262752.1} holds\footnote{\textit{Proof.} Assume that
$m<i$. Then, $m\neq i$. Hence, the statement $m=i$ is false. Thus, $\left[
m=i\right]  =0$.
\par
But every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies $k\leq m<i$. Hence,
every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies
\begin{equation}
\dbinom{k}{i}=0 \label{pf.lem.exeadd.AoPS262752.1.fn1.1}%
\end{equation}
(by (\ref{eq.binom.0}), applied to $k$ and $i$ instead of $m$ and $n$). Now,
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}}%
_{\substack{=0\\\text{(by (\ref{pf.lem.exeadd.AoPS262752.1.fn1.1}))}}%
}\dbinom{m}{k}=\sum_{k=0}^{m}\left(  -1\right)  ^{k}0\dbinom{m}{k}=0=\left[
m=i\right]
\]
(since $\left[  m=i\right]  =0$). In other words, Lemma
\ref{lem.exeadd.AoPS262752.1} holds, qed.}. Hence, for the rest of this proof
of Lemma \ref{lem.exeadd.AoPS262752.1}, we can WLOG assume that we don't have
$m<i$. Assume this.

We have $m\geq i$ (since we don't have $m<i$). Hence, $m\geq i\geq0$, so that%
\begin{align}
&  \sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}\dbinom{m}%
{k}}_{=\dbinom{m}{k}\dbinom{k}{i}}\nonumber\\
&  =\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{m}{k}\dbinom{k}{i}\nonumber\\
&  =\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}\underbrace{\dbinom
{k}{i}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}) (applied to }k\text{ and
}i\\\text{instead of }m\text{ and }n\text{) (since }k<i\text{))}}}+\sum
_{k=i}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{m}{k}\dbinom{k}{i}%
}_{\substack{=\dbinom{m}{i}\dbinom{m-i}{k-i}\\\text{(by
(\ref{eq.binom.trinom-rev.m}) (applied to }k\text{ and }i\\\text{instead of
}i\text{ and }a\text{) (since }k\geq i\text{))}}}\nonumber\\
&  =\underbrace{\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}0}%
_{=0}+\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
=\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
\nonumber\\
&  =\sum_{k=0}^{m-i}\underbrace{\left(  -1\right)  ^{k+i}}_{=\left(
-1\right)  ^{k}\left(  -1\right)  ^{i}}\dbinom{m}{i}\underbrace{\dbinom
{m-i}{k+i-i}}_{\substack{=\dbinom{m-i}{k}\\\text{(since }k+i-i=k\text{)}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+i\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=0}^{m-i}\left(  -1\right)  ^{k}\left(  -1\right)  ^{i}\dbinom
{m}{i}\dbinom{m-i}{k}=\left(  -1\right)  ^{i}\dbinom{m}{i}\underbrace{\sum
_{k=0}^{m-i}\left(  -1\right)  ^{k}\dbinom{m-i}{k}}_{\substack{=\left[
m-i=0\right]  \\\text{(by (\ref{sol.exeadd.AoPS262752.n=0}) (applied to
}n=m-i\text{))}}}\nonumber\\
&  =\left(  -1\right)  ^{i}\dbinom{m}{i}\left[  m-i=0\right]  .
\label{pf.lem.exeadd.AoPS262752.1.main}%
\end{align}


If $m=i$, then Lemma \ref{lem.exeadd.AoPS262752.1}
holds\footnote{\textit{Proof.} Assume that $m=i$. Hence, $m-i=0$, so that
$\left[  m-i=0\right]  =1$. Also, $\left[  m=i\right]  =1$ (since $m=i$).
Also, from $m=i$, we obtain $\dbinom{m}{i}=\dbinom{i}{i}=1$ (by
(\ref{eq.binom.mm}), applied to $m=i$). Hence,
(\ref{pf.lem.exeadd.AoPS262752.1.main}) becomes%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\underbrace{\dbinom{m}{i}}_{=1}\underbrace{\left[
m-i=0\right]  }_{=1=\left[  m=i\right]  }=\left(  -1\right)  ^{i}\left[
m=i\right]  .
\]
In other words, Lemma \ref{lem.exeadd.AoPS262752.1} holds, qed.}. Hence, for
the rest of this proof of Lemma \ref{lem.exeadd.AoPS262752.1}, we can WLOG
assume that we don't have $m=i$. Assume this.

We have $m\neq i$ (since we don't have $m=i$). Hence, $\left[  m=i\right]
=0$, so that $\left(  -1\right)  ^{i}\underbrace{\left[  m=i\right]  }_{=0}%
=0$. Also, $m-i\neq0$ (since $m\neq i$) and thus $\left[  m-i=0\right]  =0$.
Now, (\ref{pf.lem.exeadd.AoPS262752.1.main}) becomes%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\dbinom{m}{i}\underbrace{\left[  m-i=0\right]  }_{=0}=0=\left(
-1\right)  ^{i}\left[  m=i\right]
\]
(since $\left(  -1\right)  ^{i}\left[  m=i\right]  =0$). This proves Lemma
\ref{lem.exeadd.AoPS262752.1}.
\end{proof}
\end{verlong}

Here comes one more simple lemma:

\begin{lemma}
\label{lem.exeadd.AoPS262752.2}Let $n\in\mathbb{N}$. Let $a_{0},a_{1}%
,\ldots,a_{n}$ be $n+1$ polynomials in the indeterminate $X$ with rational
coefficients (that is, $n+1$ elements of $\mathbb{Q}\left[  X\right]  $).
Then,%
\[
\sum_{i=0}^{n}a_{i}\left[  n-i=i\right]  =%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.exeadd.AoPS262752.2}.]We have $n\in\mathbb{N}$, so
that $n\geq0$. For every $i\in\left\{  0,1,\ldots,n\right\}  $, we have%
\begin{equation}
\left[  \underbrace{n-i=i}_{\text{this is equivalent to }n=2i}\right]
=\left[  \underbrace{n=2i}_{\text{this is equivalent to }i=n/2}\right]
=\left[  i=n/2\right]  . \label{pf.lem.exeadd.AoPS262752.2.1}%
\end{equation}
Thus,%
\begin{align}
&  \underbrace{\sum_{i=0}^{n}}_{=\sum_{i\in\left\{  0,1,\ldots,n\right\}  }%
}a_{i}\underbrace{\left[  n-i=i\right]  }_{\substack{=\left[  i=n/2\right]
\\\text{(by (\ref{pf.lem.exeadd.AoPS262752.2.1}))}}}\nonumber\\
&  =\sum_{i\in\left\{  0,1,\ldots,n\right\}  }a_{i}\left[  i=n/2\right]
=\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}}a_{i}%
\underbrace{\left[  i=n/2\right]  }_{\substack{=1\\\text{(since }%
i=n/2\text{)}}}+\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i\neq
n/2}}a_{i}\underbrace{\left[  i=n/2\right]  }_{\substack{=0\\\text{(since
}i\neq n/2\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every }i\in\left\{  0,1,\ldots
,n\right\}  \text{ satisfies either }i=n/2\text{ or }i\neq n/2\text{ (but not
both)}\right) \nonumber\\
&  =\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}%
}a_{i}+\underbrace{\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i\neq
n/2}}a_{i}0}_{=0}=\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}
;\\i=n/2}}a_{i}. \label{pf.lem.exeadd.AoPS262752.2.2}%
\end{align}


We must be in one of the following two cases:

\textit{Case 1:} The number $n$ is even.

\textit{Case 2:} The number $n$ is odd.

Let us first consider Case 1. In this case, the number $n$ is even. Hence,
\begin{equation}%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
=a_{n/2}. \label{pf.lem.exeadd.AoPS262752.2.c1.1}%
\end{equation}


On the other hand, $n/2\in\mathbb{Z}$ (since $n$ is even). Combined with
$n/2\geq0$ (since $n\geq0$) and $n/2\leq n$ (for the same reason), this shows
that $n/2\in\left\{  0,1,\ldots,n\right\}  $. Now,
(\ref{pf.lem.exeadd.AoPS262752.2.2}) becomes%
\begin{align*}
&  \sum_{i=0}^{n}a_{i}\left[  n-i=i\right] \\
&  =\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}}a_{i}%
=a_{n/2}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n/2\in\left\{  0,1,\ldots
,n\right\}  \right) \\
&  =%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.exeadd.AoPS262752.2.c1.1}%
)}\right)  .
\end{align*}
Thus, Lemma \ref{lem.exeadd.AoPS262752.2} is proven in Case 1.

Let us now consider Case 2. In this case, the number $n$ is odd. Hence,
\begin{equation}%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
=0. \label{pf.lem.exeadd.AoPS262752.2.c2.1}%
\end{equation}


On the other hand, $n/2\notin\mathbb{Z}$ (since $n$ is odd). Hence,
$n/2\notin\left\{  0,1,\ldots,n\right\}  $ (since $\left\{  0,1,\ldots
,n\right\}  \subseteq\mathbb{Z}$). Now, (\ref{pf.lem.exeadd.AoPS262752.2.2})
becomes%
\begin{align*}
&  \sum_{i=0}^{n}a_{i}\left[  n-i=i\right] \\
&  =\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}}a_{i}=\left(
\text{empty sum}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
n/2\notin\left\{  0,1,\ldots,n\right\}  \right) \\
&  =0=%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.exeadd.AoPS262752.2.c2.1}%
)}\right)  .
\end{align*}
Thus, Lemma \ref{lem.exeadd.AoPS262752.2} is proven in Case 2.

We have now proved Lemma \ref{lem.exeadd.AoPS262752.2} in both Cases 1 and 2.
Since these two Cases cover all possibilities, this shows that Lemma
\ref{lem.exeadd.AoPS262752.2} always holds.
\end{proof}

Now, we are ready to solve Additional exercise \ref{exeadd.AoPS262752} again:

\begin{proof}
[Second solution to Additional exercise \ref{exeadd.AoPS262752}.]Let
$g\in\left\{  0,1,\ldots,n\right\}  $ be arbitrary. Then, Theorem
\ref{thm.vandermonde} (applied to $n-g$ instead of $n$) yields%
\[
\dbinom{X+Y}{n-g}=\sum_{k=0}^{n-g}\dbinom{X}{k}\dbinom{Y}{n-g-k}%
\]
(an equality between two polynomials in $X$ and $Y$). Substituting $g$ and
$X-g$ for $X$ and $Y$ in this equality, we obtain%
\[
\dbinom{g+\left(  X-g\right)  }{n-g}=\sum_{k=0}^{n-g}\dbinom{g}{k}\dbinom
{X-g}{n-g-k}=\sum_{i=0}^{n-g}\dbinom{g}{i}\dbinom{X-g}{n-g-i}%
\]
(here, we have renamed the summation index $k$ as $i$). Since $g+\left(
X-g\right)  =X$, this rewrites as
\begin{equation}
\dbinom{X}{n-g}=\sum_{i=0}^{n-g}\dbinom{g}{i}\dbinom{X-g}{n-g-i}.
\label{sol.exeadd.AoPS262752.sol2.1}%
\end{equation}


Now, let us forget that we fixed $g$. We thus have shown that
(\ref{sol.exeadd.AoPS262752.sol2.1}) holds for every $g\in\left\{
0,1,\ldots,n\right\}  $.

On the other hand, for every $k\in\mathbb{N}$ and $i\in\mathbb{N}$ satisfying
$k+i\leq n$, we have%
\begin{equation}
\dbinom{X}{k}\dbinom{X-k}{n-k-i}=\dbinom{X}{n-i}\dbinom{n-i}{k}
\label{sol.exeadd.AoPS262752.sol2.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exeadd.AoPS262752.sol2.3}):} Let
$k\in\mathbb{N}$ and $i\in\mathbb{N}$ be such that $k+i\leq n$. From $k+i\leq
n$, we obtain $n-i\geq k$. Thus, $n-i\geq k\geq0$, so that $n-i\in\mathbb{N}$.
Hence, (\ref{eq.binom.trinom-rev}) (applied to $n-i$ and $k$ instead of $i$
and $a$) shows that
\[
\dbinom{X}{n-i}\dbinom{n-i}{k}=\dbinom{X}{k}\dbinom{X-k}{\left(  n-i\right)
-k}=\dbinom{X}{k}\dbinom{X-k}{n-k-i}%
\]
(since $\left(  n-i\right)  -k=n-k-i$). This proves
(\ref{sol.exeadd.AoPS262752.sol2.3}).}.

Now,%
\begin{align}
&  \sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\underbrace{\dbinom
{X}{n-k}}_{\substack{=\sum_{i=0}^{n-k}\dbinom{k}{i}\dbinom{X-k}{n-k-i}%
\\\text{(by (\ref{sol.exeadd.AoPS262752.sol2.1}) (applied to }g=k\text{))}%
}}\nonumber\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\left(  \sum_{i=0}%
^{n-k}\dbinom{k}{i}\dbinom{X-k}{n-k-i}\right) \nonumber\\
&  =\underbrace{\sum_{k=0}^{n}\sum_{i=0}^{n-k}}_{\substack{=\sum
_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\k\leq n;\ i\leq n-k}%
}=\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\k\leq n;\ k+i\leq
n}}\\\text{(since the condition }i\leq n-k\text{ is equivalent to }k+i\leq
n\text{)}}}\left(  -1\right)  ^{k}\underbrace{\dbinom{X}{k}\dbinom{k}{i}%
}_{=\dbinom{k}{i}\dbinom{X}{k}}\dbinom{X-k}{n-k-i}\nonumber\\
&  =\underbrace{\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\k\leq
n;\ k+i\leq n}}}_{\substack{=\sum_{\substack{\left(  k,i\right)  \in
\mathbb{N}^{2};\\k+i\leq n}}\\\text{(since the condition }\left(  k\leq
n\text{ and }k+i\leq n\right)  \text{ is equivalent to }k+i\leq
n\\\text{(because the condition }k\leq n\text{ follows from }k+i\leq
n\text{))}}}\left(  -1\right)  ^{k}\dbinom{k}{i}\underbrace{\dbinom{X}%
{k}\dbinom{X-k}{n-k-i}}_{\substack{=\dbinom{X}{n-i}\dbinom{n-i}{k}\\\text{(by
(\ref{sol.exeadd.AoPS262752.sol2.3}))}}}\nonumber\\
&  =\underbrace{\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}%
^{2};\\k+i\leq n}}}_{\substack{=\sum_{\substack{\left(  k,i\right)
\in\mathbb{N}^{2};\\i\leq n;\ k+i\leq n}}\\\text{(since the condition }k+i\leq
n\text{ is equivalent to }\left(  i\leq n\text{ and }k+i\leq n\right)
\\\text{(because the condition }i\leq n\text{ follows from }k+i\leq
n\text{))}}}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{X}{n-i}\dbinom{n-i}%
{k}\nonumber
\end{align}%
\begin{align*}
&  =\underbrace{\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\i\leq
n;\ k+i\leq n}}}_{\substack{=\sum_{\substack{\left(  i,k\right)  \in
\mathbb{N}^{2};\\i\leq n;\ k+i\leq n}}=\sum_{\substack{\left(  i,k\right)
\in\mathbb{N}^{2};\\i\leq n;\ k\leq n-i}}\\\text{(since the condition }k+i\leq
n\text{ is equivalent to }k\leq n-i\text{)}}}\left(  -1\right)  ^{k}\dbinom
{k}{i}\dbinom{X}{n-i}\dbinom{n-i}{k}\\
&  =\underbrace{\sum_{\substack{\left(  i,k\right)  \in\mathbb{N}^{2};\\i\leq
n;\ k\leq n-i}}}_{=\sum_{i=0}^{n}\sum_{k=0}^{n-i}}\left(  -1\right)
^{k}\dbinom{k}{i}\dbinom{X}{n-i}\dbinom{n-i}{k}\\
&  =\sum_{i=0}^{n}\sum_{k=0}^{n-i}\left(  -1\right)  ^{k}\dbinom{k}{i}%
\dbinom{X}{n-i}\dbinom{n-i}{k}=\sum_{i=0}^{n}\dbinom{X}{n-i}\underbrace{\sum
_{k=0}^{n-i}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{n-i}{k}}%
_{\substack{=\left(  -1\right)  ^{i}\left[  n-i=i\right]  \\\text{(by Lemma
\ref{lem.exeadd.AoPS262752.1}}\\\text{(applied to }m=n-i\text{))}}}\\
&  =\sum_{i=0}^{n}\underbrace{\dbinom{X}{n-i}\left(  -1\right)  ^{i}%
}_{=\left(  -1\right)  ^{i}\dbinom{X}{n-i}}\left[  n-i=i\right] \\
&  =\sum_{i=0}^{n}\left(  -1\right)  ^{i}\dbinom{X}{n-i}\left[  n-i=i\right]
=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n-n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.exeadd.AoPS262752.2},
applied to }a_{i}=\left(  -1\right)  ^{i}\dbinom{X}{n-i}\right) \\
&  =%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n-n/2=n/2\right)  .
\end{align*}
This solves Additional exercise \ref{exeadd.AoPS262752}.
\end{proof}

Let us record a classical result which follows from Additional exercise
\ref{exeadd.AoPS262752}:

\begin{corollary}
\label{cor.AoPS262752.X=n}Let $n\in\mathbb{N}$. Then,%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}^{2}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{n}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]

\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.AoPS262752.X=n}.]Additional exercise
\ref{exeadd.AoPS262752} shows that%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
Substituting $n$ for $X$ in this equality, we obtain%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\dbinom{n}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{n}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
Now,%
\begin{align*}
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\underbrace{\dbinom{n}{k}^{2}}%
_{=\dbinom{n}{k}\dbinom{n}{k}}  &  =\sum_{k=0}^{n}\left(  -1\right)
^{k}\dbinom{n}{k}\underbrace{\dbinom{n}{k}}_{\substack{=\dbinom{n}%
{n-k}\\\text{(by (\ref{eq.binom.symm}) (applied to }n\text{ and }%
k\\\text{instead of }m\text{ and }n\text{))}}}\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\dbinom{n}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{n}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\end{align*}
This proves Corollary \ref{cor.AoPS262752.X=n}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.1}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.1}.]We claim that every $n\in\mathbb{N}$
satisfies%
\begin{equation}
x_{n}=\dfrac{1}{2^{n}}\left(  2na^{n-1}x_{1}-\left(  n-1\right)  a^{n}%
x_{0}\right)  \label{sol.ps2.2.1.claim}%
\end{equation}
(where $na^{n-1}$ is to be understood as $0$ when $n=0$\ \ \ \ \footnote{This
needs to be said, because $a^{n-1}$ alone can be undefined for $n=0$ (if
$a=0$).}).

\textit{Proof of (\ref{sol.ps2.2.1.claim}):} We shall prove
(\ref{sol.ps2.2.1.claim}) by strong induction on $n$. So we fix some
$N\in\mathbb{N}$, and we assume that (\ref{sol.ps2.2.1.claim}) is already
proven for every $n<N$. (This is our induction hypothesis.) We now need to
show that (\ref{sol.ps2.2.1.claim}) holds for $n=N$ as well.\footnote{If you
are wondering \textquotedblleft where is the induction base?\textquotedblright%
: It isn't missing. A strong induction needs no induction base. Strong
induction lets you prove that some statement $\mathcal{A}_{n}$ holds for every
$n\in\mathbb{N}$ by means of proving that for every $N\in\mathbb{N}$,%
\begin{equation}
\text{if }\mathcal{A}_{n}\text{ holds for every }n<N\text{, then }%
\mathcal{A}_{N}\text{ holds.} \label{sol.ps2.2.1.strind}%
\end{equation}
This immediately shows that $\mathcal{A}_{0}$ holds: Namely, it is clear that
$\mathcal{A}_{n}$ holds for every $n<0$ (because there exists no $n<0$), and
thus (\ref{sol.ps2.2.1.strind}) (applied to $N=0$) shows that $\mathcal{A}%
_{0}$ holds.
\par
Of course, the proof of (\ref{sol.ps2.2.1.strind}) might involve some case
analysis; in particular, it might argue differently depending on whether $N=0$
or $N\geq1$. (Indeed, our proof will be something like this: it will treat the
cases $N=0$, $N=1$ and $N\geq2$ separately.) So there can be a
\textquotedblleft de-facto induction base\textquotedblright\ (or two, or many)
hidden in the proof of (\ref{sol.ps2.2.1.strind}).} In other words, we need to
prove that
\begin{equation}
x_{N}=\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(  N-1\right)  a^{N}%
x_{0}\right)  . \label{sol.ps2.2.1.goal}%
\end{equation}


We must be in one of the following three cases:

\textit{Case 1:} We have $N=0$.

\textit{Case 2:} We have $N=1$.

\textit{Case 3:} We have $N\geq2$.

Let us first consider Case 1. In this case, we have $N=0$. Hence,%
\begin{align*}
\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(  N-1\right)  a^{N}x_{0}\right)
&  =\underbrace{\dfrac{1}{2^{0}}}_{=1}\left(  \underbrace{2\cdot0a^{0-1}x_{1}%
}_{=0}-\underbrace{\left(  0-1\right)  a^{0}}_{=-1}x_{0}\right) \\
&  =1\left(  0-\left(  -1\right)  x_{0}\right)  =1x_{0}=x_{0}=x_{N}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0=N\right)  .
\end{align*}
In other words, $x_{N}=\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(
N-1\right)  a^{N}x_{0}\right)  $. Hence, (\ref{sol.ps2.2.1.goal}) is proven in
Case 1.

The proof of (\ref{sol.ps2.2.1.goal}) in Case 2 is similarly straightforward,
and is left to the reader.

Let us now consider Case 3. In this case, we have $N\geq2$. Hence, both $N-1$
and $N-2$ are nonnegative integers. Moreover, $N-1<N$, so that
(\ref{sol.ps2.2.1.claim}) is already proven for $n=N-1$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{N-1}=\dfrac{1}{2^{N-1}}\left(  2\left(  N-1\right)  a^{N-2}x_{1}-\left(
N-2\right)  a^{N-1}x_{0}\right)  . \label{sol.ps2.2.1.hyp1}%
\end{equation}
Also, $N-2$ is a nonnegative integer such that $N-2<N$. Hence,
(\ref{sol.ps2.2.1.claim}) is already proven for $n=N-2$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{N-2}=\dfrac{1}{2^{N-2}}\left(  2\left(  N-2\right)  a^{N-3}x_{1}-\left(
N-3\right)  a^{N-2}x_{0}\right)  . \label{sol.ps2.2.1.hyp2}%
\end{equation}


Now, recall that $a^{2}+4b=0$, so that $4b=-a^{2}$. Hence, $4b\cdot\left(
N-2\right)  a^{N-3}=\left(  -a^{2}\right)  \cdot\left(  N-2\right)
a^{N-3}=-\left(  N-2\right)  a^{N-1}$. (Don't forget to check that this latter
equality holds also when $N-2=0$; keep in mind that $\left(  N-2\right)
a^{N-3}$ was defined to be $0$ in this case, although $a^{N-3}$ might be
undefined.) From $4b=-a^{2}$, we also deduce $b=-\dfrac{a^{2}}{4}$, so that
$b\left(  N-3\right)  a^{N-2}=\dfrac{-a^{2}}{4}\left(  N-3\right)
a^{N-2}=-\dfrac{1}{4}\left(  N-3\right)  a^{N}$.

But the sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent. Hence,%
\begin{align*}
x_{N}  &  =ax_{N-1}+bx_{N-2}\\
&  =a\cdot\dfrac{1}{2^{N-1}}\left(  2\left(  N-1\right)  a^{N-2}x_{1}-\left(
N-2\right)  a^{N-1}x_{0}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +b\cdot\dfrac{1}{2^{N-2}}\left(  2\left(  N-2\right)
a^{N-3}x_{1}-\left(  N-3\right)  a^{N-2}x_{0}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.1.hyp1}) and
(\ref{sol.ps2.2.1.hyp2})}\right) \\
&  =a\cdot\dfrac{1}{2^{N-1}}2\left(  N-1\right)  a^{N-2}x_{1}-a\cdot\dfrac
{1}{2^{N-1}}\left(  N-2\right)  a^{N-1}x_{0}\\
&  \ \ \ \ \ \ \ \ \ \ +b\cdot\dfrac{1}{2^{N-2}}2\left(  N-2\right)
a^{N-3}x_{1}-b\cdot\dfrac{1}{2^{N-2}}\left(  N-3\right)  a^{N-2}x_{0}\\
&  =\underbrace{\left(  a\cdot\dfrac{1}{2^{N-1}}2\left(  N-1\right)
a^{N-2}+b\cdot\dfrac{1}{2^{N-2}}2\left(  N-2\right)  a^{N-3}\right)
}_{=\dfrac{1}{2^{N-1}}\left(  a\cdot2\left(  N-1\right)  a^{N-2}%
+4b\cdot\left(  N-2\right)  a^{N-3}\right)  }x_{1}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  a\cdot\dfrac{1}{2^{N-1}}\left(
N-2\right)  a^{N-1}+b\cdot\dfrac{1}{2^{N-2}}\left(  N-3\right)  a^{N-2}%
\right)  }_{=\dfrac{1}{2^{N-1}}\left(  a\cdot\left(  N-2\right)
a^{N-1}+2b\left(  N-3\right)  a^{N-2}\right)  }x_{0}\\
&  =\dfrac{1}{2^{N-1}}\left(  \underbrace{a\cdot2\left(  N-1\right)  a^{N-2}%
}_{=2\left(  N-1\right)  a^{N-1}}+\underbrace{4b\cdot\left(  N-2\right)
a^{N-3}}_{=-\left(  N-2\right)  a^{N-1}}\right)  x_{1}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2^{N-1}}\left(  \underbrace{a\cdot\left(
N-2\right)  a^{N-1}}_{=\left(  N-2\right)  a^{N}}+2\underbrace{b\left(
N-3\right)  a^{N-2}}_{=-\dfrac{1}{4}\left(  N-3\right)  a^{N}}\right)  x_{0}\\
&  =\dfrac{1}{2^{N-1}}\underbrace{\left(  2\left(  N-1\right)  a^{N-1}+\left(
-\left(  N-2\right)  a^{N-1}\right)  \right)  }_{=Na^{N-1}}x_{1}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2^{N-1}}\underbrace{\left(  \left(
N-2\right)  a^{N}+2\left(  -\dfrac{1}{4}\left(  N-3\right)  \right)
a^{N}\right)  }_{=\dfrac{1}{2}\left(  N-1\right)  a^{N}}x_{0}\\
&  =\dfrac{1}{2^{N-1}}Na^{N-1}x_{1}-\dfrac{1}{2^{N-1}}\cdot\dfrac{1}{2}\left(
N-1\right)  a^{N}x_{0}=\dfrac{1}{2^{N-1}}\left(  Na^{N-1}x_{1}-\dfrac{1}%
{2}\left(  N-1\right)  a^{N}x_{0}\right) \\
&  =\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(  N-1\right)  a^{N}%
x_{0}\right)  .
\end{align*}
In other words, (\ref{sol.ps2.2.1.goal}) is proven in Case 3.

Thus we have seen that (\ref{sol.ps2.2.1.goal}) holds in each of the three
Cases 1, 2 and 3. Since these three cases cover all possibilities, this
finishes the proof of (\ref{sol.ps2.2.1.goal}). Hence, we have finished our
proof of (\ref{sol.ps2.2.1.claim}) by strong induction.
\end{proof}

\begin{remark}
Proving (\ref{sol.ps2.2.1.claim}) by strong induction is a completely
straightforward task. The main difficulty of the exercise is finding this
identity. Linear algebra (specifically, the theory of the Jordan normal form)
gives a \textquotedblleft conceptual\textquotedblright\ way to derive it, but
it can also be experimentally found by computing $x_{2},x_{3},x_{4}%
,x_{5},x_{6}$ directly (using $a^{2}+4b=0$ to rewrite $b$ as $-\dfrac{a^{2}%
}{4}$, so that only the variable $a$ appears in the expressions) and guessing
the pattern.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps2.2.2}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.2}.]We shall only solve part
\textbf{(c)}, since the other two parts are its particular cases (for $N=2$
and for $N=3$, respectively).

\textbf{(c)} We define a new sequence $\left(  c_{0},c_{1},c_{2}%
,\ldots\right)  $ recursively by
\begin{align*}
c_{0}  &  =2,\\
c_{1}  &  =a,\ \ \ \ \ \ \ \ \ \ \text{and}\\
c_{n}  &  =ac_{n-1}+bc_{n-2}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2.
\end{align*}
(So this sequence $\left(  c_{0},c_{1},c_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent. Its first values are $c_{0}=2$, $c_{1}=a$,
$c_{2}=a^{2}+2b$, $c_{3}=a\left(  a^{2}+3b\right)  $ and $c_{4}=a^{4}%
+4a^{2}b+2b^{2}$. We notice that this sequence depends only on $a$ and $b$.)

We now claim that every $N\in\mathbb{N}$ and $m\in\mathbb{N}$ satisfy%
\begin{equation}
x_{m+2N}=c_{N}x_{m+N}+\left(  -1\right)  ^{N-1}b^{N}x_{m}.
\label{sol.ps2.2.2.c.claim}%
\end{equation}
Once this is proven, we will be done: In fact, (\ref{sol.ps2.2.2.c.claim})
shows that, for every nonnegative integers $N$ and $K$, the sequence $\left(
x_{K},x_{N+K},x_{2N+K},x_{3N+K},\ldots\right)  $ is $\left(  c_{N},\left(
-1\right)  ^{N-1}b^{N}\right)  $-recurrent\footnote{\textit{Proof.} Assume
that we have already proven (\ref{sol.ps2.2.2.c.claim}). Now, for every
nonnegative integers $N$ and $K$, for every $u\geq2$, we have%
\begin{align*}
x_{uN+K}  &  =x_{\left(  u-2\right)  N+K+2N}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }uN+K=\left(  u-2\right)  N+K+2N\right) \\
&  =c_{N}\underbrace{x_{\left(  u-2\right)  N+K+N}}_{=x_{\left(  u-1\right)
N+K}}+\left(  -1\right)  ^{N-1}b^{N}x_{\left(  u-2\right)  N+K}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.2.c.claim}), applied to
}m=\left(  u-2\right)  N+K\right) \\
&  =c_{N}x_{\left(  u-1\right)  N+K}+\left(  -1\right)  ^{N-1}b^{N}x_{\left(
u-2\right)  N+K}.
\end{align*}
In other words, for every nonnegative integers $N$ and $K$, the sequence
$\left(  x_{K},x_{N+K},x_{2N+K},x_{3N+K},\ldots\right)  $ is $\left(
c_{N},\left(  -1\right)  ^{N-1}b^{N}\right)  $-recurrent. Qed.}. Thus, in
order to solve Exercise \ref{exe.ps2.2.2} \textbf{(c)}, we only need to prove
(\ref{sol.ps2.2.2.c.claim}).

\textit{Proof of (\ref{sol.ps2.2.2.c.claim}):} We shall prove
(\ref{sol.ps2.2.2.c.claim}) by strong induction over $N$. Thus, we fix some
$n\in\mathbb{N}$, and we assume (as our induction hypothesis) that
(\ref{sol.ps2.2.2.c.claim}) holds for every $N<n$ (and, of course, every
$m\in\mathbb{N}$). We need to prove that (\ref{sol.ps2.2.2.c.claim}) holds for
$N=n$ (and every $m\in\mathbb{N}$). In other words, we need to prove that%
\begin{equation}
x_{m+2n}=c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}
\label{sol.ps2.2.2.c.goal}%
\end{equation}
for every $m\in\mathbb{N}$.

We must be in one of the following three cases:

\textit{Case 1:} We have $n=0$.

\textit{Case 2:} We have $n=1$.

\textit{Case 3:} We have $n\geq2$.

Let us first consider Case 1. In this case, we have $n=0$. Now, let
$m\in\mathbb{N}$. Since $n=0$, we have%
\[
c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}=\underbrace{c_{0}}%
_{=2}\underbrace{x_{m+0}}_{=x_{m}}+\underbrace{\left(  -1\right)  ^{0-1}%
}_{=-1}\underbrace{b^{0}}_{=1}x_{m}=2x_{m}+\left(  -1\right)  x_{m}=x_{m}.
\]
Compared with%
\begin{align*}
x_{m+2n}  &  =x_{m+2\cdot0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=0\right)
\\
&  =x_{m},
\end{align*}
this yields $x_{m+2n}=c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}$.
Hence, (\ref{sol.ps2.2.2.c.goal}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $n=1$. Now, let
$m\in\mathbb{N}$. Since $n=1$, we have%
\[
c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}=\underbrace{c_{1}}%
_{=a}x_{m+1}+\underbrace{\left(  -1\right)  ^{1-1}}_{=1}\underbrace{b^{1}%
}_{=b}x_{m}=ax_{m+1}+bx_{m}.
\]
Compared with%
\begin{align*}
x_{m+2n}  &  =x_{m+2\cdot1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=1\right)
\\
&  =x_{m+2}=ax_{\left(  m+2\right)  -1}+bx_{\left(  m+2\right)  -2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the sequence }\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  \text{ is }\left(  a,b\right)  \text{-recurrent}%
\right) \\
&  =ax_{m+1}+bx_{m},
\end{align*}
this yields $x_{m+2n}=c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}$.
Hence, (\ref{sol.ps2.2.2.c.goal}) is proven in Case 2.

Let us finally consider Case 3. In this case, we have $n\geq2$. Hence, both
$n-1$ and $n-2$ are nonnegative integers. Moreover, $n-1<n$, so that
(\ref{sol.ps2.2.2.c.claim}) is already proven for $N=n-1$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{m+2\left(  n-1\right)  }=c_{n-1}x_{m+\left(  n-1\right)  }+\left(
-1\right)  ^{\left(  n-1\right)  -1}b^{n-1}x_{m} \label{sol.ps2.2.2.c.hyp1}%
\end{equation}
for every $m\in\mathbb{N}$.

Also, $n-2$ is a nonnegative integer such that $n-2<n$. Hence,
(\ref{sol.ps2.2.1.claim}) is already proven for $N=n-2$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{m+2\left(  n-2\right)  }=c_{n-2}x_{m+\left(  n-2\right)  }+\left(
-1\right)  ^{\left(  n-2\right)  -1}b^{n-2}x_{m} \label{sol.ps2.2.2.c.hyp2}%
\end{equation}
for every $m\in\mathbb{N}$.

Now, fix $m\in\mathbb{N}$. We want to prove (\ref{sol.ps2.2.2.c.goal}). The
recursive definition of the sequence $\left(  c_{0},c_{1},c_{2},\ldots\right)
$ yields
\begin{equation}
c_{n}=ac_{n-1}+bc_{n-2}. \label{sol.ps2.2.2.c.1}%
\end{equation}
Since the sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent, we have%
\[
x_{m+2}=a\underbrace{x_{\left(  m+2\right)  -1}}_{=x_{m+1}}%
+b\underbrace{x_{\left(  m+2\right)  -2}}_{=x_{m}}=ax_{m+1}+bx_{m},
\]
so that%
\begin{equation}
ax_{m+1}-x_{m+2}=-bx_{m}. \label{sol.ps2.2.2.c.3}%
\end{equation}
But since the sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent, we also have
\begin{align*}
x_{m+2n}  &  =a\underbrace{x_{\left(  m+2n\right)  -1}}_{\substack{=x_{\left(
m+1\right)  +2\left(  n-1\right)  }\\\text{(since }\left(  m+2n\right)
-1=\left(  m+1\right)  +2\left(  n-1\right)  \text{)}}%
}+b\underbrace{x_{\left(  m+2n\right)  -2}}_{\substack{=x_{\left(  m+2\right)
+2\left(  n-2\right)  }\\\text{(since }\left(  m+2n\right)  -2=\left(
m+2\right)  +2\left(  n-2\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{m}_{\geq
0}+2\underbrace{n}_{\geq2}\geq0+2\cdot2=4\geq2\right) \\
&  =a\underbrace{x_{\left(  m+1\right)  +2\left(  n-1\right)  }}%
_{\substack{=c_{n-1}x_{\left(  m+1\right)  +\left(  n-1\right)  }+\left(
-1\right)  ^{\left(  n-1\right)  -1}b^{n-1}x_{m+1}\\\text{(by
(\ref{sol.ps2.2.2.c.hyp1}), applied to }m+1\text{ instead of }m\text{)}%
}}+b\underbrace{x_{\left(  m+2\right)  +2\left(  n-2\right)  }}%
_{\substack{=c_{n-2}x_{\left(  m+2\right)  +\left(  n-2\right)  }+\left(
-1\right)  ^{\left(  n-2\right)  -1}b^{n-2}x_{m+2}\\\text{(by
(\ref{sol.ps2.2.2.c.hyp2}), applied to }m+2\text{ instead of }m\text{)}}}\\
&  =a\left(  c_{n-1}\underbrace{x_{\left(  m+1\right)  +\left(  n-1\right)  }%
}_{=x_{m+n}}+\underbrace{\left(  -1\right)  ^{\left(  n-1\right)  -1}%
}_{=\left(  -1\right)  ^{n-2}=\left(  -1\right)  ^{n}}b^{n-1}x_{m+1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +b\left(  c_{n-2}\underbrace{x_{\left(  m+2\right)
+\left(  n-2\right)  }}_{=x_{m+n}}+\underbrace{\left(  -1\right)  ^{\left(
n-2\right)  -1}}_{=\left(  -1\right)  ^{n-3}=\left(  -1\right)  ^{n-1}%
=-\left(  -1\right)  ^{n}}b^{n-2}x_{m+2}\right) \\
&  =a\left(  c_{n-1}x_{m+n}+\left(  -1\right)  ^{n}b^{n-1}x_{m+1}\right)
+b\left(  c_{n-2}x_{m+n}+\left(  -\left(  -1\right)  ^{n}\right)
b^{n-2}x_{m+2}\right) \\
&  =ac_{n-1}x_{m+n}+\left(  -1\right)  ^{n}ab^{n-1}x_{m+1}+bc_{n-2}%
x_{m+n}-\left(  -1\right)  ^{n}\underbrace{bb^{n-2}}_{=b^{n-1}}x_{m+2}\\
&  =ac_{n-1}x_{m+n}+\left(  -1\right)  ^{n}ab^{n-1}x_{m+1}+bc_{n-2}%
x_{m+n}-\left(  -1\right)  ^{n}b^{n-1}x_{m+2}\\
&  =\underbrace{\left(  ac_{n-1}x_{m+n}+bc_{n-2}x_{m+n}\right)  }_{=\left(
ac_{n-1}+bc_{n-2}\right)  x_{m+n}}+\underbrace{\left(  -1\right)  ^{n}%
ab^{n-1}x_{m+1}-\left(  -1\right)  ^{n}b^{n-1}x_{m+2}}_{=\left(  -1\right)
^{n}b^{n-1}\left(  ax_{m+1}-x_{m+2}\right)  }\\
&  =\underbrace{\left(  ac_{n-1}+bc_{n-2}\right)  }_{\substack{=c_{n}%
\\\text{(by (\ref{sol.ps2.2.2.c.1}))}}}x_{m+n}+\left(  -1\right)  ^{n}%
b^{n-1}\underbrace{\left(  ax_{m+1}-x_{m+2}\right)  }_{\substack{=-bx_{m}%
\\\text{(by (\ref{sol.ps2.2.2.c.3}))}}}\\
&  =c_{n}x_{m+n}+\underbrace{\left(  -1\right)  ^{n}b^{n-1}\left(
-bx_{m}\right)  }_{=-\left(  -1\right)  ^{n}b^{n-1}bx_{m}}=c_{n}%
x_{m+n}+\left(  \underbrace{-\left(  -1\right)  ^{n}}_{=\left(  -1\right)
^{n-1}}\underbrace{b^{n-1}b}_{=b^{n}}x_{m}\right) \\
&  =c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}.
\end{align*}


In other words, (\ref{sol.ps2.2.2.c.goal}) is proven in Case 3.

Thus we have seen that (\ref{sol.ps2.2.2.c.goal}) holds in each of the three
Cases 1, 2 and 3. Since these three cases cover all possibilities, this
finishes the proof of (\ref{sol.ps2.2.2.c.goal}). This finishes our
(inductive) proof of (\ref{sol.ps2.2.2.c.claim}). As we know, this solves
Exercise \ref{exe.ps2.2.2}.
\end{proof}

\begin{remark}
How on earth could one have come up with my definition of the sequence
$\left(  c_{0},c_{1},c_{2},\ldots\right)  $ in the solution above? One way is
to solve parts \textbf{(a)} and \textbf{(b)} of the exercise first (which can
be solved by applying the equation $x_{n}=ax_{n-1}+bx_{n-2}$ several times),
and then guess that the answer to \textbf{(c)} is a pair of the form $\left(
c,d\right)  =\left(  c_{N},\left(  -1\right)  ^{N-1}b^{N}\right)  $ for some
sequence $\left(  c_{0},c_{1},c_{2},\ldots\right)  $. What remains is finding
this sequence. I believe its entry $c_{3}=a\left(  a^{2}+3b\right)  $ is
particularly telltale.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps2.2.3}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.3}.]A set $I$ of integers is said to be
\textit{lacunar} if no two elements of $I$ are consecutive (i.e., there exists
no $i\in\mathbb{Z}$ such that both $i$ and $i+1$ belong to $I$). Then,
Exercise \ref{exe.ps2.2.3} asks us to prove that, for every positive integer
$n$,%
\begin{equation}
\text{the number }f_{n}\text{ is the number of lacunar subsets of }\left\{
1,2,\ldots,n-2\right\}  . \label{sol.ps2.2.3.goal}%
\end{equation}


We shall prove (\ref{sol.ps2.2.3.goal}) by strong induction over $n$. Thus, we
let $N$ be a positive integer, and we assume (as the induction hypothesis)
that (\ref{sol.ps2.2.3.goal}) is proven for every $n<N$. We need to prove
(\ref{sol.ps2.2.3.goal}) for $n=N$. In other words, we need to prove that
\begin{equation}
\text{the number }f_{N}\text{ is the number of lacunar subsets of }\left\{
1,2,\ldots,N-2\right\}  . \label{sol.ps2.2.3.goal2}%
\end{equation}


Recall that $N$ is a positive integer. Hence, we are in one of the following
three cases:

\textit{Case 1:} We have $N=1$.

\textit{Case 2:} We have $N=2$.

\textit{Case 3:} We have $N\geq3$.

Let us first consider Case 1. In this case, we have $N=1$. Thus, $f_{N}%
=f_{1}=1$. On the other hand, the number of lacunar subsets of $\left\{
1,2,\ldots,N-2\right\}  $ is $1$ (since the set $\left\{  1,2,\ldots
,\underbrace{N}_{=1}-2\right\}  =\left\{  1,2,\ldots,1-2\right\}
=\varnothing$ has only one subset, and this subset is lacunar). Thus, $f_{N}$
is the number of lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $.
Hence, (\ref{sol.ps2.2.3.goal2}) is proven in Case 1.

Case 2 can be dealt with similarly (in this case, the set $\left\{
1,2,\ldots,N-2\right\}  $ is still empty, and $f_{N}$ is still $1$), and is
left to the reader.

We now consider Case 3. In this case, we have $N\geq3$. Hence, $N-1$ and $N-2$
are positive integers. Since $N-1$ is a positive integer and $<N$, we know
that (\ref{sol.ps2.2.3.goal2}) is proven for $n=N-1$ (due to our induction
hypothesis). In other words, the number $f_{N-1}$ is the number of lacunar
subsets of $\left\{  1,2,\ldots,\left(  N-1\right)  -2\right\}  $. In other
words, $f_{N-1}$ is the number of lacunar subsets of $\left\{  1,2,\ldots
,N-3\right\}  $.

Also, since $N-2$ is a positive integer and $<N$, we know that
(\ref{sol.ps2.2.3.goal2}) is proven for $n=N-2$ (due to our induction
hypothesis). In other words, the number $f_{N-2}$ is the number of lacunar
subsets of $\left\{  1,2,\ldots,\left(  N-2\right)  -2\right\}  $. In other
words, $f_{N-2}$ is the number of lacunar subsets of $\left\{  1,2,\ldots
,N-4\right\}  $.

Now, how do the lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ look
like? We say that a lacunar subset of $\left\{  1,2,\ldots,N-2\right\}  $ has
\textit{type 1} if it contains $N-2$, and has \textit{type 2} if it does not.
Now, let us count the lacunar subsets having type 1 and those having type 2:

\begin{itemize}
\item If $I$ is a lacunar subset of $\left\{  1,2,\ldots,N-2\right\}  $ which
has type 1, then it contains $N-2$, and thus cannot contain $N-3$ (because it
is lacunar, i.e., contains no two consecutive integers, but $N-3$ and $N-2$
are two consecutive integers); moreover, $I\setminus\left\{  N-2\right\}  $ is
a lacunar subset of $\left\{  1,2,\ldots,N-4\right\}  $%
\ \ \ \ \footnote{Indeed, it is lacunar because $I$ is lacunar; and it is a
subset of $\left\{  1,2,\ldots,N-4\right\}  $ because $I$ cannot contain
$N-3$.}. Thus, to every lacunar subset $I$ of $\left\{  1,2,\ldots
,N-2\right\}  $ which has type 1, we have assigned a lacunar subset of
$\left\{  1,2,\ldots,N-4\right\}  $ (namely, $I\setminus\left\{  N-2\right\}
$). It is easy to see that this assignment is injective (indeed, if $I$ and
$J$ are two lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ which have
type 1, and if $I\setminus\left\{  N-2\right\}  =J\setminus\left\{
N-2\right\}  $, then $I=J$) and surjective (because whenever $K$ is a lacunar
subset of $\left\{  1,2,\ldots,N-4\right\}  $, the set $K\cup\left\{
N-2\right\}  $ is a lacunar subset of $\left\{  1,2,\ldots,N-2\right\}  $
which has type 1, and this set $K\cup\left\{  N-2\right\}  $ is sent back to
$K$ by our assignment); thus, it is bijective. Hence, we have found a
bijection between the lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $
which have type 1 and the lacunar subsets of $\left\{  1,2,\ldots,N-4\right\}
$. Therefore, the number of lacunar subsets of $\left\{  1,2,\ldots
,N-2\right\}  $ which have type 1 equals the number of lacunar subsets of
$\left\{  1,2,\ldots,N-4\right\}  $. But the latter number is $f_{N-2}$.
Hence, the number of lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $
which have type 1 is $f_{N-2}$.

\item The lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ which have
type 2 are precisely the lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}
$ which do not contain $N-2$; in other words, they are precisely the lacunar
subsets of $\left\{  1,2,\ldots,N-3\right\}  $. Hence, the number of lacunar
subsets of $\left\{  1,2,\ldots,N-2\right\}  $ which have type 2 is $f_{N-1}$
(since we know that the number of lacunar subsets of $\left\{  1,2,\ldots
,N-3\right\}  $ is $f_{N-1}$).
\end{itemize}

Now, let us summarize. Each of the lacunar subsets of $\left\{  1,2,\ldots
,N-2\right\}  $ either has type 1 or has type 2 (but not both). Hence, the
number of lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ equals%
\begin{align*}
&  \underbrace{\left(  \text{the number of lacunar subsets of }\left\{
1,2,\ldots,N-2\right\}  \text{ which have type 1}\right)  }_{=f_{N-2}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the number of lacunar
subsets of }\left\{  1,2,\ldots,N-2\right\}  \text{ which have type 2}\right)
}_{=f_{N-1}}\\
&  =f_{N-2}+f_{N-1}=f_{N-1}+f_{N-2}=f_{N}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the recursion of the Fibonacci numbers}\right)  .
\end{align*}
This proves (\ref{sol.ps2.2.3.goal2}) in Case 3.

Now, (\ref{sol.ps2.2.3.goal2}) is proven in each of the three Cases 1, 2 and
3. Hence, (\ref{sol.ps2.2.3.goal2}) always holds. This completes our
(inductive) proof of (\ref{sol.ps2.2.3.goal}). Exercise \ref{exe.ps2.2.3} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.S}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.S}.]Let us first explain why the right
hand side of (\ref{eq.exe.2.S}) is well-defined. In fact, this is not obvious,
because if $r=0$, then $r^{n-1-2i}$ might not always make sense (because
$n-1-2i$ can be negative). However, it turns out that $\dbinom{n-1-i}{i}=0$
for every $i\in\left\{  0,1,\ldots,n-1\right\}  $ satisfying $n-1-2i<0$%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  0,1,\ldots,n-1\right\}  $
be such that $n-1-2i<0$. Then, $n-1-i\geq0$ (since $i\in\left\{
0,1,\ldots,n-1\right\}  $) but $n-1-i<i$ (since $\left(  n-1-i\right)
-i=n-1-2i<0$), and thus the binomial coefficient $\dbinom{n-1-i}{i}$ is $0$
(because any binomial coefficient $\dbinom{a}{b}$ with $a\geq0$ and $a<b$ must
be $0$), qed.}. Hence, we interpret $\dbinom{n-1-i}{i}r^{n-1-2i}$ as $0$
whenever $n-1-2i<0$ (even if the term $r^{n-1-2i}$ by itself is not
well-defined), following our convention that any expression of the form
$a\cdot b$, where $a$ is $0$, has to be interpreted as $0$. Thus, the right
hand side of (\ref{eq.exe.2.S}) is well-defined.

Now, we shall prove (\ref{eq.exe.2.S}) by strong induction over $n$. Thus, we
let $N\in\mathbb{N}$, and we assume (as the induction hypothesis) that
(\ref{eq.exe.2.S}) is proven for every $n<N$. We need to prove
(\ref{eq.exe.2.S}) for $n=N$. In other words, we need to prove that
\begin{equation}
c_{N}=\sum_{i=0}^{N-1}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}.
\label{sol.ps2.2.S.goal}%
\end{equation}


Recall that $N\in\mathbb{N}$. Hence, we are in one of the following three cases:

\textit{Case 1:} We have $N=0$.

\textit{Case 2:} We have $N=1$.

\textit{Case 3:} We have $N\geq2$.

Let us first consider Case 1. In this case, we have $N=0$. Hence, $c_{N}%
=c_{0}=0$. But also, the sum $\sum_{i=0}^{N-1}\left(  -1\right)  ^{i}%
\dbinom{N-1-i}{i}r^{N-1-2i}$ is an empty sum (since $N=0$) and thus equals
$0$. Therefore, both sides of the equality (\ref{sol.ps2.2.S.goal}) equal $0$.
Hence, the equality (\ref{sol.ps2.2.S.goal}) holds. We thus have proven
(\ref{sol.ps2.2.S.goal}) in Case 1.

Let us now consider Case 2. In this case, we have $N=1$. Thus, $c_{N}=c_{1}%
=1$. On the other hand, from $N=1$, we obtain%
\begin{align*}
\sum_{i=0}^{N-1}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}  &
=\sum_{i=0}^{1-1}\left(  -1\right)  ^{i}\dbinom{1-1-i}{i}r^{1-1-2i}\\
&  =\underbrace{\left(  -1\right)  ^{0}}_{=1}\underbrace{\dbinom{1-1-0}{0}%
}_{=1}\underbrace{r^{1-1-2\cdot0}}_{=r^{0}=1}=1=c_{N}.
\end{align*}
Hence, (\ref{sol.ps2.2.S.goal}) is proven in Case 2.

Let us now consider Case 3. In this case, we have $N\geq2$. Therefore, both
$N-1$ and $N-2$ belong to $\mathbb{N}$.

So we know that $N-1$ is an element of $\mathbb{N}$ satisfying $N-1<N$. Hence,
(\ref{eq.exe.2.S}) is proven for $n=N-1$ (by our induction hypothesis). In
other words,%
\begin{equation}
c_{N-1}=\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}{i}r^{N-2-2i}.
\label{sol.ps2.2.S.hyp1}%
\end{equation}


Also, $N-2$ is an element of $\mathbb{N}$ satisfying $N-2<N$. Hence,
(\ref{eq.exe.2.S}) is proven for $n=N-2$ (by our induction hypothesis). In
other words,%
\begin{equation}
c_{N-2}=\sum_{i=0}^{N-3}\left(  -1\right)  ^{i}\dbinom{N-3-i}{i}r^{N-3-2i}.
\label{sol.ps2.2.S.hyp2}%
\end{equation}


Let us further recall that every positive integer $i$ and every $a\in
\mathbb{Z}$ satisfy%
\begin{equation}
\dbinom{a-1}{i}+\dbinom{a-1}{i-1}=\dbinom{a}{i} \label{sol.ps2.2.S.rec1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps2.2.S.rec1}):} Let $i$ be a positive
integer. Let $a\in\mathbb{Z}$. Then, (\ref{eq.binom.rec.m}) (applied to $m=a$
and $n=i$) shows that $\dbinom{a}{i}=\dbinom{a-1}{i-1}+\dbinom{a-1}{i}%
=\dbinom{a-1}{i}+\dbinom{a-1}{i-1}$. This proves (\ref{sol.ps2.2.S.rec1}).}.

\begin{noncompile}
Here is a proof of (\ref{sol.ps2.2.S.rec1}) that I have written back before I
had written up the section about binomial coefficients:

\textit{Proof of (\ref{sol.ps2.2.S.rec1}):} This is the famous recurrence
relation of Pascal's triangle, saying that every number in Pascal's triangle
equals the sum of the two numbers above. However, Pascal's triangle is usually
only drawn to contain the binomial coefficients $\dbinom{a}{i}$ with $0\leq
i\leq a$ (or else it would not be a triangle), so you might not be aware that
it holds for all $a\in\mathbb{Z}$. Either way, the proof is simple:%
\begin{align*}
&  \underbrace{\dbinom{a-1}{i}}_{\substack{=\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i\right)  }{i!}=\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i+1\right)  }{\left(  i-1\right)  !}\cdot
\dfrac{a-i}{i}\\\text{(since }i!=\left(  i-1\right)  !\cdot i\text{)}%
}}+\underbrace{\dbinom{a-1}{i-1}}_{=\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i+1\right)  }{\left(  i-1\right)  !}}\\
&  =\dfrac{\left(  a-1\right)  \left(  a-2\right)  \cdots\left(  a-i+1\right)
}{\left(  i-1\right)  !}\cdot\dfrac{a-i}{i}+\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i+1\right)  }{\left(  i-1\right)  !}\\
&  =\dfrac{\left(  a-1\right)  \left(  a-2\right)  \cdots\left(  a-i+1\right)
}{\left(  i-1\right)  !}\underbrace{\left(  \dfrac{a-i}{i}+1\right)
}_{=\dfrac{a}{i}}=\dfrac{\left(  a-1\right)  \left(  a-2\right)  \cdots\left(
a-i+1\right)  }{\left(  i-1\right)  !}\cdot\dfrac{a}{i}\\
&  =\dfrac{a\left(  a-1\right)  \left(  a-2\right)  \cdots\left(
a-i+1\right)  }{i!}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i-1\right)  !\cdot i=i!\right) \\
&  =\dbinom{a}{i}.
\end{align*}

\end{noncompile}

Now, $N\geq2$, so that the recursive definition of the sequence $\left(
c_{0},c_{1},c_{2},\ldots\right)  $ yields%
\begin{align*}
c_{N}  &  =r\underbrace{c_{N-1}}_{\substack{=\sum_{i=0}^{N-2}\left(
-1\right)  ^{i}\dbinom{N-2-i}{i}r^{N-2-2i}\\\text{(by (\ref{sol.ps2.2.S.hyp1}%
))}}}-\underbrace{c_{N-2}}_{\substack{=\sum_{i=0}^{N-3}\left(  -1\right)
^{i}\dbinom{N-3-i}{i}r^{N-3-2i}\\\text{(by (\ref{sol.ps2.2.S.hyp2}))}}}\\
&  =r\left(  \sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}%
{i}r^{N-2-2i}\right)  -\sum_{i=0}^{N-3}\left(  -1\right)  ^{i}\dbinom
{N-3-i}{i}r^{N-3-2i}\\
&  =\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}{i}%
\underbrace{rr^{N-2-2i}}_{=r^{N-1-2i}}-\underbrace{\sum_{i=0}^{N-3}\left(
-1\right)  ^{i}\dbinom{N-3-i}{i}r^{N-3-2i}}_{\substack{=\sum_{i=1}%
^{N-2}\left(  -1\right)  ^{i-1}\dbinom{N-3-\left(  i-1\right)  }%
{i-1}r^{N-3-2\left(  i-1\right)  }\\\text{(here, we substituted }i-1\text{ for
}i\text{ in the sum)}}}\\
&  =\underbrace{\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}%
{i}r^{N-1-2i}}_{=\left(  -1\right)  ^{0}\dbinom{N-2-0}{0}r^{N-1-2\cdot0}%
+\sum_{i=1}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}{i}r^{N-1-2i}}\\
&  \ \ \ \ \ \ \ \ \ \ -\sum_{i=1}^{N-2}\underbrace{\left(  -1\right)  ^{i-1}%
}_{=-\left(  -1\right)  ^{i}}\underbrace{\dbinom{N-3-\left(  i-1\right)
}{i-1}}_{=\dbinom{N-2-i}{i-1}}\underbrace{r^{N-3-2\left(  i-1\right)  }%
}_{=r^{N-1-2i}}%
\end{align*}%
\begin{align*}
&  =\left(  -1\right)  ^{0}\underbrace{\dbinom{N-2-0}{0}}_{=1=\dbinom
{N-1-0}{0}}r^{N-1-2\cdot0}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{i=1}^{N-2}\left(  -1\right)
^{i}\dbinom{N-2-i}{i}r^{N-1-2i}-\sum_{i=1}^{N-2}\left(  -\left(  -1\right)
^{i}\right)  \dbinom{N-2-i}{i-1}r^{N-1-2i}}_{=\sum_{i=1}^{N-2}\left(
-1\right)  ^{i}\left(  \dbinom{N-2-i}{i}+\dbinom{N-2-i}{i-1}\right)
r^{N-1-2i}}\\
&  =\left(  -1\right)  ^{0}\dbinom{N-1-0}{0}r^{N-1-2\cdot0}+\sum_{i=1}%
^{N-2}\left(  -1\right)  ^{i}\underbrace{\left(  \dbinom{N-2-i}{i}%
+\dbinom{N-2-i}{i-1}\right)  }_{\substack{=\dbinom{\left(  N-1-i\right)
-1}{i}+\dbinom{\left(  N-1-i\right)  -1}{i-1}\\=\dbinom{N-1-i}{i}\\\text{(by
(\ref{sol.ps2.2.S.rec1}), applied to }a=N-1-i\text{)}}}r^{N-1-2i}\\
&  =\left(  -1\right)  ^{0}\dbinom{N-1-0}{0}r^{N-1-2\cdot0}+\sum_{i=1}%
^{N-2}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}\\
&  =\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}.
\end{align*}
Hence, (\ref{sol.ps2.2.S.goal}) is proven in Case 3. We thus have proven
(\ref{sol.ps2.2.S.goal}) in all three Cases 1, 2 and 3, so that we conclude
that (\ref{sol.ps2.2.S.goal}) always holds. This completes our proof of
(\ref{eq.exe.2.S}).
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.4}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.4}.]\textbf{(a)} This proof is completely
straightforward, and would be left to the reader in any research paper; we
give a few details only:

Let $i\in\left\{  1,2,\ldots,n-2\right\}  $. We need to prove that $s_{i}\circ
s_{i+1}\circ s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}$. In order to do so, it is
clearly enough to show that $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)
\left(  h\right)  =\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(
h\right)  $ for every $h\in\left\{  1,2,\ldots,n\right\}  $. So let us fix
$h\in\left\{  1,2,\ldots,n\right\}  $. We must be in one of the following four cases:

\textit{Case 1:} We have $h=i$.

\textit{Case 2:} We have $h=i+1$.

\textit{Case 3:} We have $h=i+2$.

\textit{Case 4:} We have $h\notin\left\{  i,i+1,i+2\right\}  $.

Let us first consider Case 1. In this case, we have $h=i$ and thus
\begin{align*}
\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  \underbrace{h}%
_{=i}\right)   &  =\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(
i\right)  =s_{i}\left(  s_{i+1}\left(  \underbrace{s_{i}\left(  i\right)
}_{=i+1}\right)  \right) \\
&  =s_{i}\left(  \underbrace{s_{i+1}\left(  i+1\right)  }_{=i+2}\right)
=s_{i}\left(  i+2\right)  =i+2.
\end{align*}
A similar computation shows $\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)
\left(  h\right)  =i+2$. Thus, $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)
\left(  h\right)  =i+2=\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(
h\right)  $. Hence, we have proven the equality $\left(  s_{i}\circ
s_{i+1}\circ s_{i}\right)  \left(  h\right)  =\left(  s_{i+1}\circ s_{i}\circ
s_{i+1}\right)  \left(  h\right)  $ in Case 1.

Similarly, we can prove the same equality in Cases 2 and 3.

Now, let us consider Case 4. In this case, we have $h\notin\left\{
i,i+1,i+2\right\}  $. Thus, $h$ is neither $i$ nor $i+1$, so that we have
$s_{i}\left(  h\right)  =h$. Also, $h$ is neither $i+1$ nor $i+2$ (since
$h\notin\left\{  i,i+1,i+2\right\}  $), and thus we have $s_{i+1}\left(
h\right)  =h$. Hence,
\[
\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  h\right)  =s_{i}\left(
s_{i+1}\left(  \underbrace{s_{i}\left(  h\right)  }_{=h}\right)  \right)
=s_{i}\left(  \underbrace{s_{i+1}\left(  h\right)  }_{=h}\right)
=s_{i}\left(  h\right)  =h.
\]
Similarly, $\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)
=h$. Thus, $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  h\right)
=h=\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)  $. Hence,
we have proven $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(
h\right)  =\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)  $
in Case 4.

Altogether, we have proven the equality $\left(  s_{i}\circ s_{i+1}\circ
s_{i}\right)  \left(  h\right)  =\left(  s_{i+1}\circ s_{i}\circ
s_{i+1}\right)  \left(  h\right)  $ in each of the four Cases 1, 2, 3, and 4.
Thus, $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  h\right)
=\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)  $ always
holds. Exercise \ref{exe.ps2.2.4} \textbf{(a)} is thus solved.

\textbf{(b)} We follow the hint and delay the solution of this part until
later (see Exercise \ref{exe.ps2.2.5} \textbf{(e)}).

\textbf{(c)} For every $i\in\left\{  1,2,\ldots,n\right\}  $, we let $a_{i}$
be the permutation $s_{i-1}\circ s_{i-2}\circ\cdots\circ s_{1}\in S_{n}%
$\ \ \ \ \footnote{In particular, $a_{1}=s_{1-1}\circ s_{1-2}\circ\cdots\circ
s_{1}$ is the composition of $0$ permutations. What does this mean? Just as a
sum of $0$ terms is defined to be $0$ (because $0$ is the neutral element of
addition), and a product of $0$ terms is $1$ (since $1$ is the neutral element
of multiplication), the composition of $0$ permutations is defined to be the
identity permutation (since the identity permutation is the neutral element of
composition). Hence, $a_{1}$ is the identity permutation, i.e., we have
$a_{1}=\operatorname*{id}$.}. Now, we claim that%
\begin{equation}
w_{0}=a_{1}\circ a_{2}\circ\cdots\circ a_{n}. \label{sol.ps2.2.4.c.claim}%
\end{equation}
This is essentially an explicit way to write $w_{0}$ as a composition of
several permutations of the form $s_{i}$ (because each $a_{i}$ on the right
hand side is the composition $s_{i-1}\circ s_{i-2}\circ\cdots\circ s_{1}$).
Thus, once (\ref{sol.ps2.2.4.c.claim}) is proven, the exercise will be solved.

Before we prove (\ref{sol.ps2.2.4.c.claim}), let us first understand how
$a_{i}$ operates. We claim that%
\begin{equation}
a_{i}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq i;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>i
\end{array}
\right.  \label{sol.ps2.2.4.c.aik}%
\end{equation}
for each $i\in\left\{  1,2,\ldots,n\right\}  $ and $k\in\left\{
1,2,\ldots,n\right\}  $. (In other words, $a_{i}$ is the permutation which
cycles $i\rightarrow i-1\rightarrow\cdots\rightarrow1\rightarrow i$ and leaves
all numbers $>i$ untouched.)

\textit{Proof of (\ref{sol.ps2.2.4.c.aik}):} Let me give an informal proof of
(\ref{sol.ps2.2.4.c.aik}) which, I trust, you can turn into a formal proof if
you so desire.

Let $i\in\left\{  1,2,\ldots,n\right\}  $. We have $a_{i}=s_{i-1}\circ
s_{i-2}\circ\cdots\circ s_{1}$. Therefore, $a_{i}$ is the permutation which
first switches $1$ with $2$, then switches $2$ with $3$, etc., until it
finally switches $i-1$ with $i$. Thus:

\begin{itemize}
\item When we apply $a_{i}$ to $1$, we arrive at $i$ at the end (since the $1$
is carried to $2$ by the first switch, which is then carried to $3$ by the
next switch, and so on, until it finally becomes $i$).

\item When we apply $a_{i}$ to some $k\in\left\{  2,3,\ldots,i\right\}  $, we
arrive at $k-1$ at the end (since the first switch to move $k$ is the $\left(
k-1\right)  $-st switch, which changes it into $k-1$, and from then on all the
following switches leave $k-1$ untouched).

\item When we apply $a_{i}$ to some $k\in\left\{  i+1,i+2,\ldots,n\right\}  $,
we arrive at $k$ at the end (since none of our switches changes $k$).
\end{itemize}

Expressing this in a formula instead of in words, we obtain precisely
(\ref{sol.ps2.2.4.c.aik}).

\begin{verlong}
\textit{Formal proof of (\ref{sol.ps2.2.4.c.aik}):} For the sake of
completeness, let me show how to prove (\ref{sol.ps2.2.4.c.aik}) formally.

We shall prove (\ref{sol.ps2.2.4.c.aik}) by induction on $i$:

\textit{Induction base:} We have $a_{1}=s_{0}\circ s_{1}\circ\cdots\circ
s_{1}=\operatorname*{id}$. Thus, every $k\in\left\{  1,2,\ldots,n\right\}  $
satisfies%
\begin{align*}
\underbrace{a_{1}}_{=\operatorname*{id}}\left(  k\right)   &
=\operatorname*{id}\left(  k\right)  =k=\left\{
\begin{array}
[c]{c}%
k,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>1
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>1
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k=1\text{ when }k=1\right) \\
&  =\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>1
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we added a \textquotedblleft}1<k\leq1\text{\textquotedblright%
\ case, which does not}\\
\text{change the result because this case never happens}%
\end{array}
\right)  .
\end{align*}
In other words, (\ref{sol.ps2.2.4.c.aik}) holds for $i=1$. This completes the
induction base.

\textit{Induction step:} Let $I\in\left\{  1,2,\ldots,n\right\}  $ be such
that $I>1$. Assume that (\ref{sol.ps2.2.4.c.aik}) holds for $i=I-1$. We need
to show that (\ref{sol.ps2.2.4.c.aik}) holds for $i=I$.

We have assumed that (\ref{sol.ps2.2.4.c.aik}) holds for $i=I-1$. In other
words,%
\begin{equation}
a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  \label{sol.ps2.2.4.c.aik.pf.hyp}%
\end{equation}
for each $k\in\left\{  1,2,\ldots,n\right\}  $.

The definition of $a_{I-1}$ yields $a_{I-1}=s_{I-2}\circ s_{I-3}\circ
\cdots\circ s_{1}$. The definition of $a_{I}$ yields $a_{I}=s_{I-1}\circ
s_{I-2}\circ\cdots\circ s_{1}=s_{I-1}\circ\underbrace{\left(  s_{I-2}\circ
s_{I-3}\circ\cdots\circ s_{1}\right)  }_{=a_{I-1}}=s_{I-1}\circ a_{I-1}$.
Hence, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{equation}
a_{I}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  \label{sol.ps2.2.4.c.aik.pf.step}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps2.2.4.c.aik.pf.step}):} Let
$k\in\left\{  1,2,\ldots,n\right\}  $. We need to prove
(\ref{sol.ps2.2.4.c.aik.pf.step}). We are in one of the following four cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $1<k\leq I$ and $k<I$.
\par
\textit{Case 3:} We have $1<k\leq I$ and $k\geq I$.
\par
\textit{Case 4:} We have $k>I$.
\par
Let us first consider Case 1. In this case, we have $k=1$. Thus,
(\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =I-1$ (since $k=1$) and thus $\underbrace{a_{I}}_{=s_{I-1}\circ
a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}\right)  \left(
k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(  k\right)  }%
_{=I-1}\right)  =s_{I-1}\left(  I-1\right)  =I$. Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =I$ (since $k=1$), this yields $a_{I}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $1<k\leq I$ and $k<I$. From
$k<I$, we obtain $k\leq I-1$ (since $k$ and $I$ are integers). Thus,
(\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =k-1$ (since $1<k\leq I-1$) and thus $\underbrace{a_{I}}%
_{=s_{I-1}\circ a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}%
\right)  \left(  k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(
k\right)  }_{=k-1}\right)  =s_{I-1}\left(  k-1\right)  =k-1$ (since $k-1<k\leq
I-1$). Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =k-1$ (since $1<k\leq I$), this yields $a_{I}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 2.
\par
Let us now consider Case 3. In this case, we have $1<k\leq I$ and $k\geq I$.
Combining $k\leq I$ with $k\geq I$, we obtain $k=I>I-1$. Thus,
(\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =k$ (since $k>I-1$) and thus $\underbrace{a_{I}}_{=s_{I-1}\circ
a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}\right)  \left(
k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(  k\right)  }%
_{=k=I}\right)  =s_{I-1}\left(  I\right)  =\underbrace{I}_{=k}-1=k-1$.
Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =k-1$ (since $1<k\leq I$), this yields $a_{I}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 3.
\par
Let us finally consider Case 4. In this case, we have $k>I$. Hence, $k>I>I-1$.
Thus, (\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =k$ (since $k>I-1$) and thus $\underbrace{a_{I}}_{=s_{I-1}\circ
a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}\right)  \left(
k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(  k\right)  }_{=k}\right)
=s_{I-1}\left(  k\right)  =k$ (since $k>I$). Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =k$ (since $k>I$), this yields $a_{I}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 4.
\par
Hence, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in each of the four Cases
1, 2, 3 and 4. Since these four Cases are the only possible cases, this shows
that (\ref{sol.ps2.2.4.c.aik.pf.step}) always holds, qed.}. In other words,
(\ref{sol.ps2.2.4.c.aik}) holds for $i=I$. This completes the induction step.
The induction proof of (\ref{sol.ps2.2.4.c.aik}) is thus finished.
\end{verlong}

Next, for every $m\in\left\{  0,1,\ldots,n\right\}  $, set%
\[
b_{m}=a_{1}\circ a_{2}\circ\cdots\circ a_{m}\in S_{n}.
\]
As a consequence, $b_{0}=a_{0}\circ a_{1}\circ\cdots\circ a_{0}=\left(
\text{a composition of }0\text{ maps}\right)  =\operatorname*{id}$ and
$b_{n}=a_{1}\circ a_{2}\circ\cdots\circ a_{n}$. We claim that%
\begin{equation}
b_{m}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
m+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq m;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>m
\end{array}
\right.  \label{sol.ps2.2.4.c.bik}%
\end{equation}
for every $m\in\left\{  0,1,\ldots,n\right\}  $ and $k\in\left\{
1,2,\ldots,n\right\}  $.

Again, one can prove (\ref{sol.ps2.2.4.c.bik}) either formally by induction on
$m$, or more intuitively by tracking what happens to $k$ under the maps
$a_{m}$, $a_{m-1}$, $\ldots$, $a_{1}$ when these maps are applied one after
the other. This time the informal way is a bit tricky, so let us show the
formal one in all its glory. (You do not ever need to write a proof in this
level of detail unless you are talking to a computer.)

\textit{Proof of (\ref{sol.ps2.2.4.c.bik}):} We shall prove
(\ref{sol.ps2.2.4.c.bik}) by induction on $m$:

\textit{Induction base:} For every $k\in\left\{  1,2,\ldots,n\right\}  $, we
have $\underbrace{b_{0}}_{=\operatorname*{id}}\left(  k\right)
=\operatorname*{id}\left(  k\right)  =k=\left\{
\begin{array}
[c]{c}%
0+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq0;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>0
\end{array}
\right.  $ (since $\left\{
\begin{array}
[c]{c}%
0+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq0;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>0
\end{array}
\right.  =k$ (since $k>0$)). In other words, (\ref{sol.ps2.2.4.c.bik}) holds
for $m=0$. The induction base is thus complete.

\textit{Induction step:} Let $M\in\left\{  0,1,\ldots,n\right\}  $ be such
that $M>0$. Assume that (\ref{sol.ps2.2.4.c.bik}) holds for $m=M-1$. We need
to show that (\ref{sol.ps2.2.4.c.bik}) holds for $m=M$.

We have assumed that (\ref{sol.ps2.2.4.c.bik}) holds for $m=M-1$. In other
words,
\begin{equation}
b_{M-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
\left(  M-1\right)  +1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
M-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right.  \label{sol.ps2.2.4.c.bik.pf.hyp}%
\end{equation}
for every $k\in\left\{  1,2,\ldots,n\right\}  $.

The definition of $b_{M-1}$ yields $b_{M-1}=a_{1}\circ a_{2}\circ\cdots\circ
a_{M-1}$. The definition of $b_{M}$ yields $b_{M}=a_{1}\circ a_{2}\circ
\cdots\circ a_{M}=\underbrace{\left(  a_{1}\circ a_{2}\circ\cdots\circ
a_{M-1}\right)  }_{=b_{M-1}}\circ a_{M}=b_{M-1}\circ a_{M}$. Thus, we obtain%
\begin{equation}
b_{M}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  \label{sol.ps2.2.4.c.bik.pf.step}%
\end{equation}
for every $k\in\left\{  1,2,\ldots,n\right\}  $\ \ \ \ \footnote{\textit{Proof
of (\ref{sol.ps2.2.4.c.bik.pf.step}):} Let $k\in\left\{  1,2,\ldots,n\right\}
$. We need to prove (\ref{sol.ps2.2.4.c.bik.pf.step}). We are in one of the
following three cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $1<k\leq M$.
\par
\textit{Case 3:} We have $k>M$.
\par
Let us first consider Case 1. In this case, we have $k=1$. Thus, $k=1\leq M$
(since $M\geq1$ (since $M>0$)). But (\ref{sol.ps2.2.4.c.aik}) (applied to
$i=M$) yields $a_{M}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
M,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =M$ (since $k=1$), so that
\begin{align*}
\underbrace{b_{M}}_{=b_{M-1}\circ a_{M}}\left(  k\right)   &  =\left(
b_{M-1}\circ a_{M}\right)  \left(  k\right)  =b_{M-1}\left(  \underbrace{a_{M}%
\left(  k\right)  }_{=M}\right)  =b_{M-1}\left(  M\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M-M,\ \ \ \ \ \ \ \ \ \ \text{if }M\leq M-1;\\
M,\ \ \ \ \ \ \ \ \ \ \text{if }M>M-1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.4.c.bik.pf.hyp}%
), applied to }k=M\right) \\
&  =M\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M>M-1\right) \\
&  =M+1-\underbrace{1}_{=k}=M+1-k=\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =M+1-k\text{ (because }k\leq M\text{)}\right)  .
\end{align*}
Thus, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $1<k\leq M$. Now,
(\ref{sol.ps2.2.4.c.aik}) (applied to $i=M$) yields $a_{M}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
M,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =k-1$ (since $1<k\leq M$), so that%
\begin{align*}
\underbrace{b_{M}}_{=b_{M-1}\circ a_{M}}\left(  k\right)   &  =\left(
b_{M-1}\circ a_{M}\right)  \left(  k\right)  =b_{M-1}\left(  \underbrace{a_{M}%
\left(  k\right)  }_{=k-1}\right)  =b_{M-1}\left(  k-1\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M-\left(  k-1\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k-1\leq M-1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }k-1>M-1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{sol.ps2.2.4.c.bik.pf.hyp}), applied to }k-1\\
\text{instead of }k
\end{array}
\right) \\
&  =M-\left(  k-1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k-1\leq
M-1\text{ (since }k\leq M\text{)}\right) \\
&  =M+1-k=\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =M+1-k\text{ (because }k\leq M\text{)}\right)  .
\end{align*}
Thus, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in Case 2.
\par
Let us finally consider Case 3. In this case, we have $k>M$. Now,
(\ref{sol.ps2.2.4.c.aik}) (applied to $i=M$) yields $a_{M}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
M,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =k$ (since $k>M$), so that%
\begin{align*}
\underbrace{b_{M}}_{=b_{M-1}\circ a_{M}}\left(  k\right)   &  =\left(
b_{M-1}\circ a_{M}\right)  \left(  k\right)  =b_{M-1}\left(  \underbrace{a_{M}%
\left(  k\right)  }_{=k}\right)  =b_{M-1}\left(  k\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.4.c.bik.pf.hyp}%
)}\right) \\
&  =k\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k>M>M-1\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =k\text{ (because }k>M\text{)}\right)  .
\end{align*}
Thus, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in Case 3.
\par
Hence, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in each of the three Cases
1, 2 and 3. Since these three Cases are the only possible cases, this shows
that (\ref{sol.ps2.2.4.c.bik.pf.step}) always holds, qed.}. In other words,
(\ref{sol.ps2.2.4.c.bik}) holds for $m=M$. This completes the induction step.
Thus, the induction proof of (\ref{sol.ps2.2.4.c.bik}) is complete.

Now, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align*}
b_{n}\left(  k\right)   &  =\left\{
\begin{array}
[c]{c}%
n+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq n;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>n
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.4.c.bik}),
applied to }m=n\right) \\
&  =n+1-k\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\leq n\right) \\
&  =w_{0}\left(  k\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
w_{0}\left(  k\right)  =n+1-k\text{ (by the definition of }w_{0}%
\text{)}\right)  .
\end{align*}
In other words, $b_{n}=w_{0}$, so that $w_{0}=b_{n}=a_{1}\circ a_{2}%
\circ\cdots\circ a_{n}$. This proves (\ref{sol.ps2.2.4.c.claim}). As we know,
this solves Exercise \ref{exe.ps2.2.4} \textbf{(c)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.5}}

Let us first state Exercise \ref{exe.ps2.2.5} \textbf{(d)} as a separate result:

\begin{proposition}
\label{prop.sol.exe.ps2.2.5.d}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ be a
permutation satisfying $\sigma\left(  1\right)  \leq\sigma\left(  2\right)
\leq\cdots\leq\sigma\left(  n\right)  $. Then, $\sigma=\operatorname*{id}$.
\end{proposition}

Proposition \ref{prop.sol.exe.ps2.2.5.d} essentially says that the only way to
list the numbers $1,2,\ldots,n$ in increasing order is $\left(  1,2,\ldots
,n\right)  $. If you think this is intuitively obvious, you are right. Let me
nevertheless give two proofs (the second of which is completely formal):

\begin{proof}
[First proof of Proposition \ref{prop.sol.exe.ps2.2.5.d}.]First of all, every
$k\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies $\sigma\left(  k\right)
<\sigma\left(  k+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $\sigma$ is a permutation, thus
injective. Hence, $\sigma\left(  k\right)  \neq\sigma\left(  k+1\right)  $
(since $k\neq k+1$). Combined with $\sigma\left(  k\right)  \leq\sigma\left(
k+1\right)  $ (since $\sigma\left(  1\right)  \leq\sigma\left(  2\right)
\leq\cdots\leq\sigma\left(  n\right)  $), this yields $\sigma\left(  k\right)
<\sigma\left(  k+1\right)  $, qed.}. In other words, $\sigma\left(  1\right)
<\sigma\left(  2\right)  <\cdots<\sigma\left(  n\right)  $.

Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\sigma\left(  i\right)  $ is
the $i$-th smallest among the numbers $\sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  n\right)  $ (because $\sigma\left(  1\right)
<\sigma\left(  2\right)  <\cdots<\sigma\left(  n\right)  $). But since the
numbers $\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots
,\sigma\left(  n\right)  $ are just the numbers $1,2,\ldots,n$ (possibly in a
different order)\footnote{since $\sigma$ is a permutation of $\left\{
1,2,\ldots,n\right\}  $}, it is clear that the $i$-th smallest among these
numbers is $i$. Thus, $\sigma\left(  i\right)  =i$ (since $\sigma\left(
i\right)  $ is the $i$-th smallest among the numbers $\sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  $). Hence,
$\sigma\left(  i\right)  =i=\operatorname*{id}\left(  i\right)  $.

Let us now forget that we fixed $i$. Thus, we have shown that $\sigma\left(
i\right)  =\operatorname*{id}\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,n\right\}  $. In other words, $\sigma=\operatorname*{id}$.
Proposition \ref{prop.sol.exe.ps2.2.5.d} is thus proven.
\end{proof}

\begin{proof}
[Second proof of Proposition \ref{prop.sol.exe.ps2.2.5.d}.]We shall show that
\begin{equation}
\sigma\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,n\right\}  . \label{sol.ps2.2.5.d.1}%
\end{equation}


\textit{Proof of (\ref{sol.ps2.2.5.d.1}):} We shall prove
(\ref{sol.ps2.2.5.d.1}) by strong induction over $i$. Thus, we fix some
$I\in\left\{  1,2,\ldots,n\right\}  $, and we assume that
(\ref{sol.ps2.2.5.d.1}) is proven for every $i<I$. We then have to prove that
(\ref{sol.ps2.2.5.d.1}) holds for $i=I$.

We have assumed that (\ref{sol.ps2.2.5.d.1}) is proven for every $i<I$. In
other words,%
\begin{equation}
\sigma\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }i<I. \label{sol.ps2.2.5.d.1.pf.1}%
\end{equation}


We assume (for the sake of contradiction) that $\sigma\left(  I\right)  \neq
I$. But $\sigma$ is a permutation, and thus injective. Hence, from
$\sigma\left(  I\right)  \neq I$, we obtain $\sigma\left(  \sigma\left(
I\right)  \right)  \neq\sigma\left(  I\right)  $. But if $\sigma\left(
I\right)  <I$, then $\sigma\left(  \sigma\left(  I\right)  \right)
=\sigma\left(  I\right)  $ (by (\ref{sol.ps2.2.5.d.1.pf.1}), applied to
$i=\sigma\left(  I\right)  $), which contradicts $\sigma\left(  \sigma\left(
I\right)  \right)  \neq\sigma\left(  I\right)  $. Hence, we cannot have
$\sigma\left(  I\right)  <I$. Thus, we have $\sigma\left(  I\right)  \geq I$.
Combined with $\sigma\left(  I\right)  \neq I$, this yields $\sigma\left(
I\right)  >I$.

Now, let $K=\sigma^{-1}\left(  I\right)  $. Then, $I=\sigma\left(  K\right)
$, so that $\sigma\left(  K\right)  =I\neq\sigma\left(  I\right)  $ and
therefore $K\neq I$. If $K<I$, then $\sigma\left(  K\right)  =K$ (by
(\ref{sol.ps2.2.5.d.1.pf.1}), applied to $i=K$), which contradicts
$\sigma\left(  K\right)  =I\neq K$. Hence, we cannot have $K<I$. We thus have
$I\leq K$.

Now, recall that $\sigma\left(  1\right)  \leq\sigma\left(  2\right)
\leq\cdots\leq\sigma\left(  n\right)  $. In other words, $\sigma\left(
a\right)  \leq\sigma\left(  b\right)  $ for every two elements $a$ and $b$ of
$\left\{  1,2,\ldots,n\right\}  $ satisfying $a\leq b$. Applying this to $a=I$
and $b=K$, we obtain $\sigma\left(  I\right)  \leq\sigma\left(  K\right)  $.
This contradicts $\sigma\left(  I\right)  >I=\sigma\left(  K\right)  $. This
contradiction proves that our assumption (that $\sigma\left(  I\right)  \neq
I$) was wrong. Hence, we must have $\sigma\left(  I\right)  =I$. In other
words, (\ref{sol.ps2.2.5.d.1}) holds for $i=I$. This completes our inductive
proof of (\ref{sol.ps2.2.5.d.1}).

Now, (\ref{sol.ps2.2.5.d.1}) shows that every $i\in\left\{  1,2,\ldots
,n\right\}  $ satisfies $\sigma\left(  i\right)  =i=\operatorname*{id}\left(
i\right)  $. In other words, $\sigma=\operatorname*{id}$. Proposition
\ref{prop.sol.exe.ps2.2.5.d} is solved again.
\end{proof}

For future use, let us record an easy consequence of Proposition
\ref{prop.sol.exe.ps2.2.5.d}:

\begin{corollary}
\label{cor.sol.exe.ps2.2.5.d2}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ be a
permutation satisfying $\ell\left(  \sigma\right)  =0$. Then, $\sigma
=\operatorname*{id}$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.sol.exe.ps2.2.5.d2}.]Let $k\in\left\{
1,2,\ldots,n-1\right\}  $. Assume (for the sake of contradiction) that
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $. Then, $\left(
k,k+1\right)  $ is a pair of integers satisfying $1\leq k<k+1\leq n$ and
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $. In other words,
$\left(  k,k+1\right)  $ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. In other words, $\left(  k,k+1\right)  $ is an inversion of
$\sigma$ (by the definition of an \textquotedblleft
inversion\textquotedblright). Thus, the permutation $\sigma$ has at least one
inversion (namely, $\left(  k,k+1\right)  $).

But the number of inversions of $\sigma$ is $\ell\left(  \sigma\right)  =0$;
in other words, $\sigma$ has no inversions. This contradicts the fact that
$\sigma$ has at least one inversion. This contradiction proves that our
assumption (that $\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $) was
wrong. Hence, we have $\sigma\left(  k\right)  \leq\sigma\left(  k+1\right)  $.

Now, let us forget that we fixed $k$. We thus have shown that $\sigma\left(
k\right)  \leq\sigma\left(  k+1\right)  $ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. Thus, $\sigma\left(  1\right)  \leq\sigma\left(
2\right)  \leq\cdots\leq\sigma\left(  n\right)  $. Therefore, Proposition
\ref{prop.sol.exe.ps2.2.5.d} shows that $\sigma=\operatorname*{id}$. This
proves Corollary \ref{cor.sol.exe.ps2.2.5.d2}.
\end{proof}

Now, we come to the actual solution of Exercise \ref{exe.ps2.2.5}.

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.5}.]Exercise \ref{exe.ps2.2.5}
\textbf{(d)} follows immediately from Proposition \ref{prop.sol.exe.ps2.2.5.d}%
. We shall next prove part \textbf{(f)} of the exercise, then part
\textbf{(a)}, then part \textbf{(e)}, and then the remaining three parts.

Before we come to the actual solution, let us introduce one more notation.

For every $\sigma\in S_{n}$, let $\operatorname*{Inv}\left(  \sigma\right)  $
be the set of all inversions of the permutation $\sigma$. Thus, for every
$\sigma\in S_{n}$, we have%
\begin{align}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \nonumber\\
&  =\left(  \text{the number of elements of }\operatorname*{Inv}\left(
\sigma\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\sigma\right)  \text{ is the set of all inversions of }\sigma\right)
\nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\label{sol.ps2.2.5.f.1}%
\end{align}


\textbf{(d)} Let $\sigma\in S_{n}$ be a permutation satisfying $\sigma\left(
1\right)  \leq\sigma\left(  2\right)  \leq\cdots\leq\sigma\left(  n\right)  $.
Then, Proposition \ref{prop.sol.exe.ps2.2.5.d} shows that $\sigma
=\operatorname*{id}$. This solves Exercise \ref{exe.ps2.2.5} \textbf{(d)}.

\textbf{(f)} Let $\sigma\in S_{n}$. For every $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  $, we have $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  \in\operatorname*{Inv}\left(
\sigma^{-1}\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $\left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $. Then, $\left(
i,j\right)  $ is an inversion of $\sigma$ (since $\operatorname*{Inv}\left(
\sigma\right)  $ is the set of all inversions of $\sigma$). In other words,
$\left(  i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright). Hence,
$\sigma\left(  j\right)  <\sigma\left(  i\right)  $, so that $1\leq
\sigma\left(  j\right)  <\sigma\left(  i\right)  \leq n$; also, $\sigma
^{-1}\left(  \sigma\left(  i\right)  \right)  =i<j=\sigma^{-1}\left(
\sigma\left(  j\right)  \right)  $, so that $\sigma^{-1}\left(  \sigma\left(
j\right)  \right)  >\sigma^{-1}\left(  \sigma\left(  i\right)  \right)  $.
Therefore, $\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)
$ is a pair of integers $\left(  u,v\right)  $ satisfying $1\leq u<v\leq n$
and $\sigma^{-1}\left(  u\right)  >\sigma^{-1}\left(  v\right)  $ (since
$1\leq\sigma\left(  j\right)  <\sigma\left(  i\right)  \leq n$ and
$\sigma^{-1}\left(  \sigma\left(  j\right)  \right)  >\sigma^{-1}\left(
\sigma\left(  i\right)  \right)  $). In other words, $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  $ is an inversion of $\sigma^{-1}$
(because inversions of $\sigma^{-1}$ are defined as pairs of integers $\left(
u,v\right)  $ satisfying $1\leq u<v\leq n$ and $\sigma^{-1}\left(  u\right)
>\sigma^{-1}\left(  v\right)  $). In other words, $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  \in\operatorname*{Inv}\left(
\sigma^{-1}\right)  $ (since $\operatorname*{Inv}\left(  \sigma^{-1}\right)  $
is the set of all inversions of $\sigma^{-1}$), qed.}. Hence, we can define a
map%
\begin{align*}
\Phi:\operatorname*{Inv}\left(  \sigma\right)   &  \rightarrow
\operatorname*{Inv}\left(  \sigma^{-1}\right)  ,\\
\left(  i,j\right)   &  \mapsto\left(  \sigma\left(  j\right)  ,\sigma\left(
i\right)  \right)  .
\end{align*}
This map $\Phi$ is injective\footnote{\textit{Proof.} We simply need to prove
that an element $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  $ can be reconstructed from its image $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  $. But this is easy: If you know
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  $, then you
know $\sigma\left(  j\right)  $ and $\sigma\left(  i\right)  $, and therefore
also $i$ (since $i=\sigma^{-1}\left(  \sigma\left(  i\right)  \right)  $) and
$j$ (since $j=\sigma^{-1}\left(  \sigma\left(  j\right)  \right)  $), and thus
also $\left(  i,j\right)  $.}. Thus, we have found an injective map from
$\operatorname*{Inv}\left(  \sigma\right)  $ to $\operatorname*{Inv}\left(
\sigma^{-1}\right)  $. Conversely, $\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert \leq\left\vert \operatorname*{Inv}\left(
\sigma^{-1}\right)  \right\vert $. But $\ell\left(  \sigma^{-1}\right)
=\left\vert \operatorname*{Inv}\left(  \sigma^{-1}\right)  \right\vert $ (by
(\ref{sol.ps2.2.5.f.1}), applied to $\sigma^{-1}$ instead of $\sigma$). Now,
(\ref{sol.ps2.2.5.f.1}) yields $\ell\left(  \sigma\right)  =\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert \leq\left\vert
\operatorname*{Inv}\left(  \sigma^{-1}\right)  \right\vert =\ell\left(
\sigma^{-1}\right)  $.

Now, let us forget that we fixed $\sigma$. We thus have proven that
\begin{equation}
\ell\left(  \sigma\right)  \leq\ell\left(  \sigma^{-1}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}. \label{sol.ps2.2.5.f.6}%
\end{equation}


Now, let $\sigma\in S_{n}$ again. We can apply (\ref{sol.ps2.2.5.f.6}) to
$\sigma^{-1}$ instead of $\sigma$, and thus obtain $\ell\left(  \sigma
^{-1}\right)  \leq\ell\left(  \underbrace{\left(  \sigma^{-1}\right)  ^{-1}%
}_{=\sigma}\right)  =\ell\left(  \sigma\right)  $. Combined with
(\ref{sol.ps2.2.5.f.6}), this yields $\ell\left(  \sigma\right)  =\ell\left(
\sigma^{-1}\right)  $. This solves Exercise \ref{exe.ps2.2.5} \textbf{(f)}.

\textbf{(a)} As I warned above, this solution will be a tedious formalization
of the argument sketched in Example \ref{exa.2.5}.

Let us first show a very simple fact: If $u$ and $v$ are two integers such
that $1\leq u<v\leq n$, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $ is such
that $\left(  u,v\right)  \neq\left(  k,k+1\right)  $, then%
\begin{equation}
s_{k}\left(  u\right)  <s_{k}\left(  v\right)  \label{sol.ps2.2.5.a.sk-inc}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps2.2.5.a.sk-inc}):} We can prove
(\ref{sol.ps2.2.5.a.sk-inc}) by analyzing three cases (Case 1 is when $u=k$,
Case 2 is when $u=k+1$, and Case 3 is when $u\notin\left\{  k,k+1\right\}  $),
each of which can be split into three subcases (Subcase 1 is when $v=k$,
Subcase 2 is when $v=k+1$, and Subcase 3 is when $v\notin\left\{
k,k+1\right\}  $). These are (altogether) nine subcases, but four of them
(namely, Subcases 1 and 2 in Case 1, and Subcases 1 and 2 in Case 2) are
impossible (because $u<v$ and $\left(  u,v\right)  \neq\left(  k,k+1\right)
$), and the proof of (\ref{sol.ps2.2.5.a.sk-inc}) is easy in the remaining
five subcases.
\par
Here is a smarter way to prove (\ref{sol.ps2.2.5.a.sk-inc}): Let $u$ and $v$
be two integers such that $1\leq u<v\leq n$. Let $k\in\left\{  1,2,\ldots
,n-1\right\}  $ be such that $\left(  u,v\right)  \neq\left(  k,k+1\right)  $.
We need to prove (\ref{sol.ps2.2.5.a.sk-inc}). Indeed, assume the contrary.
Thus, $s_{k}\left(  u\right)  \geq s_{k}\left(  v\right)  $.
\par
But $u<v$ and thus $u\neq v$. The map $s_{k}$ is a permutation, thus
bijective, and therefore injective. Hence, $s_{k}\left(  u\right)  \neq
s_{k}\left(  v\right)  $ (since $u\neq v$). Combined with $s_{k}\left(
u\right)  \geq s_{k}\left(  v\right)  $, this yields $s_{k}\left(  u\right)
>s_{k}\left(  v\right)  $. Thus, $s_{k}\left(  u\right)  \geq s_{k}\left(
v\right)  +1$ (since $s_{k}\left(  u\right)  $ and $s_{k}\left(  v\right)  $
are integers), so that $s_{k}\left(  v\right)  +1\leq s_{k}\left(  u\right)
$.
\par
We have $u<v$ and thus $u+1\leq v$ (since $u$ and $v$ are integers).
\par
Recall that $s_{k}$ is the permutation in $S_{n}$ which switches $k$ and
$k+1$, while leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $
unchanged. Hence,
\begin{equation}
s_{k}\left(  p\right)  \leq p+1\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  , \label{sol.ps2.2.5.a.sk-inc.pf.1}%
\end{equation}
and this inequality becomes an equality only for $p=k$. For the same reason,
we have%
\begin{equation}
s_{k}\left(  p\right)  \geq p-1\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  , \label{sol.ps2.2.5.a.sk-inc.pf.2}%
\end{equation}
and this inequality becomes an equality only for $p=k+1$.
\par
Applying (\ref{sol.ps2.2.5.a.sk-inc.pf.1}) to $p=u$, we obtain $s_{k}\left(
u\right)  \leq u+1$. Applying (\ref{sol.ps2.2.5.a.sk-inc.pf.2}) to $p=v$, we
obtain $s_{k}\left(  v\right)  \geq v-1$, so that $v-1\leq s_{k}\left(
v\right)  $ and thus $v\leq s_{k}\left(  v\right)  +1\leq s_{k}\left(
u\right)  \leq u+1\leq v$.
\par
Combining $v\leq s_{k}\left(  v\right)  +1$ with $s_{k}\left(  v\right)
+1\leq v$, we obtain $v=s_{k}\left(  v\right)  +1$, so that $s_{k}\left(
v\right)  =v-1$. In other words, $s_{k}\left(  p\right)  =p-1$ holds for
$p=v$. But recall that the inequality (\ref{sol.ps2.2.5.a.sk-inc.pf.2})
becomes an equality only for $p=k+1$. In other words, $s_{k}\left(  p\right)
=p-1$ holds only for $p=k+1$. Applying this to $p=v$, we obtain $v=k+1$ (since
$s_{k}\left(  p\right)  =p-1$ holds for $p=v$).
\par
Combining $s_{k}\left(  u\right)  \leq u+1$ with $u+1\leq v\leq s_{k}\left(
u\right)  $, we obtain $s_{k}\left(  u\right)  =u+1$. In other words,
$s_{k}\left(  p\right)  =p+1$ holds for $p=u$. But recall that the inequality
(\ref{sol.ps2.2.5.a.sk-inc.pf.1}) becomes an equality only for $p=k$. In other
words, $s_{k}\left(  p\right)  =p+1$ holds only for $p=k$. Applying this to
$p=u$, we obtain $u=k$ (since $s_{k}\left(  p\right)  =p+1$ holds for $p=u$).
\par
Now, $\left(  \underbrace{u}_{=k},\underbrace{v}_{=k+1}\right)  =\left(
k,k+1\right)  $ contradicts $\left(  u,v\right)  \neq\left(  k,k+1\right)  $.
This contradiction proves that our assumption was wrong. Hence,
(\ref{sol.ps2.2.5.a.sk-inc}) is proven.}.

We shall now show that%
\begin{align}
&  \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\} \nonumber\\
&  =\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  \label{sol.ps2.2.5.a.equalsets}%
\end{align}
for every $\sigma\in S_{n}$ and $k\in\left\{  1,2,\ldots,n-1\right\}  $.

[Notice that we do not necessarily have $\left(  \sigma^{-1}\left(
k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \in\operatorname*{Inv}%
\left(  \sigma\right)  $; nor do we always have $\left(  \sigma^{-1}\left(
k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)  \in\operatorname*{Inv}%
\left(  s_{k}\circ\sigma\right)  $. In fact, for each given $\sigma$ and $k$,
exactly one of these two statements holds. But we can form the difference
$A\setminus B$ of two sets $A$ and $B$ even if $B$ is not a subset of $A$, so
the statement (\ref{sol.ps2.2.5.a.equalsets}) still makes sense.]

\textit{Proof of (\ref{sol.ps2.2.5.a.equalsets}):} Let $\sigma\in S_{n}$ and
$k\in\left\{  1,2,\ldots,n-1\right\}  $. Let $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  $. Thus, $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  $ and $\left(  i,j\right)  \neq\left(  \sigma^{-1}\left(
k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  $. Therefore, $\left(
\sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  \neq\left(
k,k+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  =\left(
k,k+1\right)  $. Hence, $\sigma\left(  j\right)  =k$ and $\sigma\left(
i\right)  =k+1$. Hence, $\left(  \underbrace{\sigma^{-1}\left(  k+1\right)
}_{\substack{=i\\\text{(since }\sigma\left(  i\right)  =k+1\text{)}%
}},\underbrace{\sigma^{-1}\left(  k\right)  }_{\substack{=j\\\text{(since
}\sigma\left(  j\right)  =k\text{)}}}\right)  =\left(  i,j\right)  \neq\left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  $, which
is absurd. Hence, we have found a contradiction, so that our assumption was
wrong, qed.}.

We have $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $.
In other words, $\left(  i,j\right)  $ is an inversion of $\sigma$. In other
words, $\left(  i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq
n$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. Now,
$\sigma\left(  j\right)  <\sigma\left(  i\right)  $, so that $1\leq
\sigma\left(  j\right)  <\sigma\left(  i\right)  \leq n$. Also, as we know,
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  \neq\left(
k,k+1\right)  $. Hence, $s_{k}\left(  \sigma\left(  j\right)  \right)
<s_{k}\left(  \sigma\left(  i\right)  \right)  $ (by
(\ref{sol.ps2.2.5.a.sk-inc}), applied to $u=\sigma\left(  j\right)  $ and
$v=\sigma\left(  i\right)  $). Thus, $\left(  s_{k}\circ\sigma\right)  \left(
j\right)  =s_{k}\left(  \sigma\left(  j\right)  \right)  <s_{k}\left(
\sigma\left(  i\right)  \right)  =\left(  s_{k}\circ\sigma\right)  \left(
i\right)  $, hence $\left(  s_{k}\circ\sigma\right)  \left(  i\right)
>\left(  s_{k}\circ\sigma\right)  \left(  j\right)  $. Hence, $\left(
i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and $\left(
s_{k}\circ\sigma\right)  \left(  i\right)  >\left(  s_{k}\circ\sigma\right)
\left(  j\right)  $. In other words, $\left(  i,j\right)  $ is an inversion of
$s_{k}\circ\sigma$ (by the definition of an inversion). In other words,
$\left(  i,j\right)  \in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $
(since $\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $ is defined as
the set of all inversions of $s_{k}\circ\sigma$). Furthermore, $\left(
i,j\right)  \neq\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(
k+1\right)  \right)  $\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, $\left(  i,j\right)  =\left(  \sigma^{-1}\left(  k\right)  ,\sigma
^{-1}\left(  k+1\right)  \right)  $. Thus, $i=\sigma^{-1}\left(  k\right)  $
and $j=\sigma^{-1}\left(  k+1\right)  $. Hence, $\sigma\left(  i\right)  =k$
(since $i=\sigma^{-1}\left(  k\right)  $) and $\sigma\left(  j\right)  =k+1$
(since $j=\sigma^{-1}\left(  k+1\right)  $). Now, $k+1=\sigma\left(  j\right)
<\sigma\left(  i\right)  =k<k+1$, which is absurd. Thus, we have found a
contradiction, so that our assumption must have been wrong, qed.}. Thus,
$\left(  i,j\right)  \in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)
\setminus\left\{  \left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(
k+1\right)  \right)  \right\}  $ (since $\left(  i,j\right)  \in
\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $ and $\left(  i,j\right)
\neq\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  $).

Now, let us forget that we fixed $\left(  i,j\right)  $. We thus have shown
that every $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\setminus\left\{  \left(  \sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(
k\right)  \right)  \right\}  $ satisfies $\left(  i,j\right)  \in
\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  $. In other words,%
\begin{align}
&  \operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \right\}
\nonumber\\
&  \subseteq\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus
\left\{  \left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  \right\}  . \label{sol.ps2.2.5.a.equalsets.pf.1}%
\end{align}


Now, let $\tau=s_{k}\circ\sigma$. Then, $s_{k}\circ\underbrace{\tau}%
_{=s_{k}\circ\sigma}=\underbrace{s_{k}\circ s_{k}}_{=s_{k}^{2}%
=\operatorname*{id}}\circ\sigma=\operatorname*{id}\circ\sigma=\sigma$.
Moreover, $\tau^{-1}\left(  k\right)  =\sigma^{-1}\left(  k+1\right)
$\ \ \ \ \footnote{\textit{Proof.} We have $\underbrace{\tau}_{=s_{k}%
\circ\sigma}\left(  \sigma^{-1}\left(  k+1\right)  \right)  =\left(
s_{k}\circ\sigma\right)  \left(  \sigma^{-1}\left(  k+1\right)  \right)
=s_{k}\left(  \underbrace{\sigma\left(  \sigma^{-1}\left(  k+1\right)
\right)  }_{=k+1}\right)  =s_{k}\left(  k+1\right)  =k$ (by the definition of
$s_{k}$). Thus, $\tau^{-1}\left(  k\right)  =\sigma^{-1}\left(  k+1\right)  $,
qed.} and $\tau^{-1}\left(  k+1\right)  =\sigma^{-1}\left(  k\right)
$\ \ \ \ \footnote{for similar reasons}.

But recall that we have proven (\ref{sol.ps2.2.5.a.equalsets.pf.1}). The same
arguments, but carried out for $\tau$ instead of $\sigma$, show that%
\begin{align*}
&  \operatorname*{Inv}\left(  \tau\right)  \setminus\left\{  \left(  \tau
^{-1}\left(  k+1\right)  ,\tau^{-1}\left(  k\right)  \right)  \right\} \\
&  \subseteq\operatorname*{Inv}\left(  s_{k}\circ\tau\right)  \setminus
\left\{  \left(  \tau^{-1}\left(  k\right)  ,\tau^{-1}\left(  k+1\right)
\right)  \right\}  .
\end{align*}
Using the identities $s_{k}\circ\tau=\sigma$, $\tau^{-1}\left(  k\right)
=\sigma^{-1}\left(  k+1\right)  $ and $\tau^{-1}\left(  k+1\right)
=\sigma^{-1}\left(  k\right)  $, we can rewrite this as follows:%
\begin{align*}
&  \operatorname*{Inv}\left(  \tau\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \right\}
\\
&  \subseteq\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  .
\end{align*}
Since $\tau=s_{k}\circ\sigma$, this further rewrites as follows:%
\begin{align*}
&  \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\} \\
&  \subseteq\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  .
\end{align*}
Combining this with (\ref{sol.ps2.2.5.a.equalsets.pf.1}), we obtain%
\begin{align*}
&  \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\} \\
&  =\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  .
\end{align*}
This proves (\ref{sol.ps2.2.5.a.equalsets}).

Now, let $k\in\left\{  1,2,\ldots,n-1\right\}  $.

We shall first show that for every $\sigma\in S_{n}$, we have%
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)
+1\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(  k\right)  <\sigma
^{-1}\left(  k+1\right)  . \label{sol.ps2.2.5.a.part1}%
\end{equation}


\textit{Proof of (\ref{sol.ps2.2.5.a.part1}):} Let $\sigma\in S_{n}$. Assume
that $\sigma^{-1}\left(  k\right)  <\sigma^{-1}\left(  k+1\right)  $. Then,
$\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
$ is a pair of integers satisfying $1\leq\sigma^{-1}\left(  k\right)
<\sigma^{-1}\left(  k+1\right)  \leq n$ and $\left(  s_{k}\circ\sigma\right)
\left(  \sigma^{-1}\left(  k\right)  \right)  >\left(  s_{k}\circ
\sigma\right)  \left(  \sigma^{-1}\left(  k+1\right)  \right)  $%
\ \ \ \ \footnote{because $\left(  s_{k}\circ\sigma\right)  \left(
\sigma^{-1}\left(  k\right)  \right)  =s_{k}\left(  \underbrace{\sigma\left(
\sigma^{-1}\left(  k\right)  \right)  }_{=k}\right)  =s_{k}\left(  k\right)
=k+1$ and similarly $\left(  s_{k}\circ\sigma\right)  \left(  \sigma
^{-1}\left(  k+1\right)  \right)  =k$, so that $\left(  s_{k}\circ
\sigma\right)  \left(  \sigma^{-1}\left(  k\right)  \right)  =k+1>k=\left(
s_{k}\circ\sigma\right)  \left(  \sigma^{-1}\left(  k+1\right)  \right)  $}.
In other words, $\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(
k+1\right)  \right)  $ is an inversion of $s_{k}\circ\sigma$. In other words,
$\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $. Hence,%
\begin{equation}
\left\vert \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus
\left\{  \left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  \right\}  \right\vert =\left\vert \operatorname*{Inv}\left(
s_{k}\circ\sigma\right)  \right\vert -1 \label{sol.ps2.2.5.a.part1.pf.1}%
\end{equation}


On the other hand, $\left(  \sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(
k\right)  \right)  $ is not an inversion of $\sigma$ (because if it was an
inversion of $\sigma$, then we would have $1\leq\sigma^{-1}\left(  k+1\right)
<\sigma^{-1}\left(  k\right)  \leq n$ and therefore $\sigma^{-1}\left(
k+1\right)  <\sigma^{-1}\left(  k\right)  <\sigma^{-1}\left(  k+1\right)  $,
which would be absurd). In other words, $\left(  \sigma^{-1}\left(
k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \notin\operatorname*{Inv}%
\left(  \sigma\right)  $. Thus,%
\[
\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  =\operatorname*{Inv}\left(  \sigma\right)  ,
\]
so that%
\begin{align}
\operatorname*{Inv}\left(  \sigma\right)   &  =\operatorname*{Inv}\left(
\sigma\right)  \setminus\left\{  \left(  \sigma^{-1}\left(  k+1\right)
,\sigma^{-1}\left(  k\right)  \right)  \right\} \nonumber\\
&  =\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.5.a.equalsets}%
)}\right)  . \label{sol.ps2.2.5.a.part1.pf.4}%
\end{align}
Now, (\ref{sol.ps2.2.5.f.1}) yields%
\begin{align}
\ell\left(  \sigma\right)   &  =\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert =\left\vert \operatorname*{Inv}\left(  s_{k}%
\circ\sigma\right)  \setminus\left\{  \left(  \sigma^{-1}\left(  k\right)
,\sigma^{-1}\left(  k+1\right)  \right)  \right\}  \right\vert
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.5.a.part1.pf.4})}\right)
\nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \right\vert
-1\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.5.a.part1.pf.1}%
)}\right)  . \label{sol.ps2.2.5.a.part1.pf.6}%
\end{align}
But (\ref{sol.ps2.2.5.f.1}) (applied to $s_{k}\circ\sigma$ instead of $\sigma
$) yields $\ell\left(  s_{k}\circ\sigma\right)  =\left\vert
\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \right\vert $. Hence,
(\ref{sol.ps2.2.5.a.part1.pf.6}) becomes%
\[
\ell\left(  \sigma\right)  =\underbrace{\left\vert \operatorname*{Inv}\left(
s_{k}\circ\sigma\right)  \right\vert }_{=\ell\left(  s_{k}\circ\sigma\right)
}-1=\ell\left(  s_{k}\circ\sigma\right)  -1,
\]
so that $\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)  +1$.
This proves (\ref{sol.ps2.2.5.a.part1}).

Next, we will show that for every $\sigma\in S_{n}$, we have
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)
-1\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(  k\right)  >\sigma
^{-1}\left(  k+1\right)  . \label{sol.ps2.2.5.a.part2}%
\end{equation}


\textit{Proof of (\ref{sol.ps2.2.5.a.part2}):} Let $\sigma\in S_{n}$. Assume
that $\sigma^{-1}\left(  k\right)  >\sigma^{-1}\left(  k+1\right)  $. But
$\sigma^{-1}\left(  k\right)  =\left(  s_{k}\circ\sigma\right)  ^{-1}\left(
k+1\right)  $\ \ \ \ \footnote{since $\left(  s_{k}\circ\sigma\right)  \left(
\sigma^{-1}\left(  k\right)  \right)  =s_{k}\left(  \underbrace{\sigma\left(
\sigma^{-1}\left(  k\right)  \right)  }_{=k}\right)  =s_{k}\left(  k\right)
=k+1$} and $\sigma^{-1}\left(  k+1\right)  =\left(  s_{k}\circ\sigma\right)
^{-1}\left(  k\right)  $\ \ \ \ \footnote{for similar reasons}. Thus, $\left(
s_{k}\circ\sigma\right)  ^{-1}\left(  k\right)  =\sigma^{-1}\left(
k+1\right)  <\sigma^{-1}\left(  k\right)  =\left(  s_{k}\circ\sigma\right)
^{-1}\left(  k+1\right)  $. Hence, we can apply (\ref{sol.ps2.2.5.a.part1}) to
$s_{k}\circ\sigma$ instead of $\sigma$. As a result, we obtain%
\[
\ell\left(  s_{k}\circ s_{k}\circ\sigma\right)  =\ell\left(  s_{k}\circ
\sigma\right)  +1.
\]
Since $\underbrace{s_{k}\circ s_{k}}_{=s_{k}^{2}=\operatorname*{id}}%
\circ\sigma=\operatorname*{id}\circ\sigma=\sigma$, this rewrites as
$\ell\left(  \sigma\right)  =\ell\left(  s_{k}\circ\sigma\right)  +1$, so that
$\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)  -1$. This
proves (\ref{sol.ps2.2.5.a.part2}).

\begin{vershort}
Now, (\ref{eq.exe.2.5.a.2}) follows immediately by combining
(\ref{sol.ps2.2.5.a.part1}) with (\ref{sol.ps2.2.5.a.part2}).\footnote{The
term \textquotedblleft$\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  $\textquotedblright\ in (\ref{eq.exe.2.5.a.2}) makes sense because
every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies exactly one of the conditions $\sigma\left(  k\right)
<\sigma\left(  k+1\right)  $ and $\sigma\left(  k\right)  >\sigma\left(
k+1\right)  $. (Indeed, $\sigma\left(  k\right)  =\sigma\left(  k+1\right)  $
is impossible, because every permutation $\sigma\in S_{n}$ is injective.)}
\end{vershort}

\begin{verlong}
We will soon prove (\ref{eq.exe.2.5.a.1}) and (\ref{eq.exe.2.5.a.2}). Let us
first show that the right-hand sides of (\ref{eq.exe.2.5.a.1}) and
(\ref{eq.exe.2.5.a.2}) are always well-defined. Indeed, for every $\sigma\in
S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the right-hand side
of (\ref{eq.exe.2.5.a.1}) is well-defined\footnote{\textit{Proof.} Let
$\sigma\in S_{n}$ and $k\in\left\{  1,2,\ldots,n-1\right\}  $. The map
$\sigma$ is a permutation and thus injective. Hence, $\sigma\left(  k\right)
\neq\sigma\left(  k+1\right)  $ (since $k\neq k+1$). Thus, either
$\sigma\left(  k\right)  <\sigma\left(  k+1\right)  $ or $\sigma\left(
k\right)  >\sigma\left(  k+1\right)  $. More precisely, exactly one of the
conditions $\sigma\left(  k\right)  <\sigma\left(  k+1\right)  $ and
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $ is satisfied. Hence, the
right-hand side of (\ref{eq.exe.2.5.a.1}) is well-defined.}, and the
right-hand side of (\ref{eq.exe.2.5.a.2}) is
well-defined\footnote{\textit{Proof.} Let $\sigma\in S_{n}$ and $k\in\left\{
1,2,\ldots,n-1\right\}  $. The map $\sigma^{-1}$ is a permutation and thus
injective. Hence, $\sigma^{-1}\left(  k\right)  \neq\sigma^{-1}\left(
k+1\right)  $ (since $k\neq k+1$). Thus, either $\sigma^{-1}\left(  k\right)
<\sigma^{-1}\left(  k+1\right)  $ or $\sigma^{-1}\left(  k\right)
>\sigma^{-1}\left(  k+1\right)  $. More precisely, exactly one of the
conditions $\sigma^{-1}\left(  k\right)  <\sigma^{-1}\left(  k+1\right)  $ and
$\sigma^{-1}\left(  k\right)  >\sigma^{-1}\left(  k+1\right)  $ is satisfied.
Hence, the right-hand side of (\ref{eq.exe.2.5.a.2}) is well-defined.}.

Now, for every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots
,n-1\right\}  $, the equality (\ref{eq.exe.2.5.a.2}) holds (since it follows
immediately by combining (\ref{sol.ps2.2.5.a.part1}) with
(\ref{sol.ps2.2.5.a.part2})).
\end{verlong}

It remains to prove (\ref{eq.exe.2.5.a.1}). Indeed, let $\sigma\in S_{n}$. Let
us recall that $\left(  \alpha\circ\beta\right)  ^{-1}=\beta^{-1}\circ
\alpha^{-1}$ for any two permutations $\alpha$ and $\beta$ in $S_{n}$.
Applying this to $\alpha=s_{k}$ and $\beta=\sigma^{-1}$, we obtain $\left(
s_{k}\circ\sigma^{-1}\right)  ^{-1}=\underbrace{\left(  \sigma^{-1}\right)
^{-1}}_{=\sigma}\circ\underbrace{s_{k}^{-1}}_{=s_{k}}=\sigma\circ s_{k}$. But
Exercise \ref{exe.ps2.2.5} \textbf{(f)} yields $\ell\left(  \sigma\right)
=\ell\left(  \sigma^{-1}\right)  $. Also, Exercise \ref{exe.ps2.2.5}
\textbf{(f)} (applied to $s_{k}\circ\sigma^{-1}$ instead of $\sigma$) yields
$\ell\left(  s_{k}\circ\sigma^{-1}\right)  =\ell\left(  \underbrace{\left(
s_{k}\circ\sigma^{-1}\right)  ^{-1}}_{=\sigma\circ s_{k}}\right)  =\ell\left(
\sigma\circ s_{k}\right)  $. But applying (\ref{eq.exe.2.5.a.2}) to
$\sigma^{-1}$ instead of $\sigma$, we obtain%
\[
\ell\left(  s_{k}\circ\sigma^{-1}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma^{-1}\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\left(
\sigma^{-1}\right)  ^{-1}\left(  k\right)  <\left(  \sigma^{-1}\right)
^{-1}\left(  k+1\right)  ;\\
\ell\left(  \sigma^{-1}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\left(
\sigma^{-1}\right)  ^{-1}\left(  k\right)  >\left(  \sigma^{-1}\right)
^{-1}\left(  k+1\right)
\end{array}
\right.  .
\]
Since $\ell\left(  s_{k}\circ\sigma^{-1}\right)  =\ell\left(  \sigma\circ
s_{k}\right)  $, $\ell\left(  \sigma^{-1}\right)  =\ell\left(  \sigma\right)
$ and $\left(  \sigma^{-1}\right)  ^{-1}=\sigma$, this equality rewrites as
follows:%
\[
\ell\left(  \sigma\circ s_{k}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  .
\]
This proves (\ref{eq.exe.2.5.a.1}), and thus completes the solution of
Exercise \ref{exe.ps2.2.5} \textbf{(a)}.

\textbf{(e)} We shall solve Exercise \ref{exe.ps2.2.5} \textbf{(e)} by
induction over $\ell\left(  \sigma\right)  $:

\textit{Induction base:} Exercise \ref{exe.ps2.2.5} \textbf{(e)} holds in the
case when $\ell\left(  \sigma\right)  =0$\ \ \ \ \footnote{\textit{Proof.} Let
$\sigma\in S_{n}$ be such that $\ell\left(  \sigma\right)  =0$. We need to
show that $\sigma$ can be written as a composition of $\ell\left(
\sigma\right)  $ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $).
\par
Recall that the composition of $0$ permutations in $S_{n}$ is
$\operatorname*{id}$ (by definition).
\par
We have $\ell\left(  \sigma\right)  =0$, and thus $\sigma=\operatorname*{id}$
(by Corollary \ref{cor.sol.exe.ps2.2.5.d2}). Therefore, $\sigma$ is a
composition of $0$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $) (because the composition of $0$ permutations in
$S_{n}$ is $\operatorname*{id}$). In other words, $\sigma$ is a composition of
$\ell\left(  \sigma\right)  $ permutations of the form $s_{k}$ (with
$k\in\left\{  1,2,\ldots,n-1\right\}  $) (since $\ell\left(  \sigma\right)
=0$). Thus, Exercise \ref{exe.ps2.2.5} \textbf{(e)} is solved in the case when
$\ell\left(  \sigma\right)  =0$, qed.}. This completes the induction base.

\textit{Induction step:} Let $L$ be a positive integer. Assume that Exercise
\ref{exe.ps2.2.5} \textbf{(e)} is solved in the case when $\ell\left(
\sigma\right)  =L-1$. We need to solve Exercise \ref{exe.ps2.2.5} \textbf{(e)}
in the case when $\ell\left(  \sigma\right)  =L$.

So let $\sigma\in S_{n}$ be such that $\ell\left(  \sigma\right)  =L$. We need
to show that $\sigma$ can be written as a composition of $\ell\left(
\sigma\right)  $ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $).

There exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, every
$k\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies $\sigma\left(  k\right)
\leq\sigma\left(  k+1\right)  $. In other words, $\sigma\left(  1\right)
\leq\sigma\left(  2\right)  \leq\cdots\leq\sigma\left(  n\right)  $. Exercise
\ref{exe.ps2.2.5} \textbf{(d)} yields $\sigma=\operatorname*{id}$. Hence,
$\ell\left(  \sigma\right)  =\ell\left(  \operatorname*{id}\right)  =0$, so
that $0=\ell\left(  \sigma\right)  =L$. This contradicts the fact that $L$ is
a positive integer. This contradiction shows that our assumption was wrong,
qed.}. Let $j$ be such a $j$. From (\ref{eq.exe.2.5.a.1}) (applied to $k=j$),
we obtain%
\begin{align*}
\ell\left(  \sigma\circ s_{j}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
j\right)  <\sigma\left(  j+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
j\right)  >\sigma\left(  j+1\right)
\end{array}
\right. \\
&  =\underbrace{\ell\left(  \sigma\right)  }_{=L}-1\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\sigma\left(  j\right)  >\sigma\left(  j+1\right)  \right) \\
&  =L-1.
\end{align*}
Hence, we can apply Exercise \ref{exe.ps2.2.5} \textbf{(e)} to $\sigma\circ
s_{j}$ instead of $\sigma$ (because we assumed that Exercise \ref{exe.ps2.2.5}
\textbf{(e)} is solved in the case when $\ell\left(  \sigma\right)  =L-1$). As
a result, we conclude that $\sigma\circ s_{j}$ can be written as a composition
of $\ell\left(  \sigma\circ s_{j}\right)  $ permutations of the form $s_{k}$
(with $k\in\left\{  1,2,\ldots,n-1\right\}  $). In other words, $\sigma\circ
s_{j}$ can be written as a composition of $L-1$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $) (since $\ell\left(
\sigma\circ s_{j}\right)  =L-1$). In other words, there exists an $\left(
L-1\right)  $-tuple $\left(  k_{1},k_{2},\ldots,k_{L-1}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{L-1}$ such that $\sigma\circ s_{j}=s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{L-1}}$. Consider this $\left(  k_{1}%
,k_{2},\ldots,k_{L-1}\right)  $.

We have $\sigma\circ\underbrace{s_{j}\circ s_{j}}_{=s_{j}^{2}%
=\operatorname*{id}}=\sigma$ and thus%
\[
\sigma=\underbrace{\sigma\circ s_{j}}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{L-1}}}\circ s_{j}=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{L-1}}\circ s_{j}.
\]
The right hand side of this equality is a composition of $L$ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). Thus,
$\sigma$ can be written as a composition of $L$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). In other words,
$\sigma$ can be written as a composition of $\ell\left(  \sigma\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$) (since $\ell\left(  \sigma\right)  =L$). This solves Exercise
\ref{exe.ps2.2.5} \textbf{(e)} in the case when $\ell\left(  \sigma\right)
=L$. The induction step is thus complete, and Exercise \ref{exe.ps2.2.5}
\textbf{(e)} is solved by induction.

\textbf{(b)} From (\ref{eq.exe.2.5.a.1}), we can easily conclude that%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  \equiv\ell\left(  \sigma\right)
+1\operatorname{mod}2 \label{sol.ps2.2.5.b.stepper}%
\end{equation}
for every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.b.stepper}):} Let $\sigma\in S_{n}$ and
$k\in\left\{  1,2,\ldots,n-1\right\}  $. From (\ref{eq.exe.2.5.a.1}), we
obtain%
\begin{align*}
\ell\left(  \sigma\circ s_{k}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \equiv\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\right)
-1\equiv\ell\left(  \sigma\right)  +1\operatorname{mod}2\text{ in the case
when }\sigma\left(  k\right)  >\sigma\left(  k+1\right)  \right) \\
&  =\ell\left(  \sigma\right)  +1\operatorname{mod}2.
\end{align*}
This proves (\ref{sol.ps2.2.5.b.stepper}).
\end{verlong}

Thus, using induction, it is easy to prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \equiv\ell\left(  \sigma\right)
+p\operatorname{mod}2 \label{sol.ps2.2.5.b.snake}%
\end{equation}
for every $\sigma\in S_{n}$, every $p\in\mathbb{N}$ and every $\left(
k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{p}$.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.b.snake}):} Let $\sigma\in S_{n}$, let
$p\in\mathbb{N}$ and let $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$. We shall prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{q}}\right)  \right)  \equiv\ell\left(  \sigma\right)
+q\operatorname{mod}2 \label{sol.ps2.2.5.b.snake.pf.1}%
\end{equation}
for all $q\in\left\{  0,1,\ldots,p\right\}  $.

Indeed, let us prove (\ref{sol.ps2.2.5.b.snake.pf.1}) by induction over $q$.

\textit{Induction base:} We have $\ell\left(  \sigma\circ\underbrace{\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  }_{=\left(
\text{empty composition}\right)  =\operatorname*{id}}\right)  =\ell\left(
\sigma\circ\operatorname*{id}\right)  =\ell\left(  \sigma\right)  =\ell\left(
\sigma\right)  +0\equiv\ell\left(  \sigma\right)  +0\operatorname{mod}2$. In
other words, (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=0$. This completes
the induction base.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,p\right\}  $ be
positive. Assume that (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q-1$. We
need to show that (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q$.

We have assumed that (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q-1$. In
other words, we have%
\[
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \right)  \equiv\ell\left(  \sigma\right)  +\left(
Q-1\right)  \operatorname{mod}2.
\]
Now,%
\begin{align*}
&  \ell\left(  \sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q}}\right)  }_{=\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q-1}}\right)  \circ s_{k_{Q}}}\right) \\
&  =\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \circ s_{k_{Q}}\right) \\
&  \equiv\underbrace{\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{Q-1}}\right)  \right)  }_{\equiv\ell\left(
\sigma\right)  +\left(  Q-1\right)  \operatorname{mod}2}+1\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{sol.ps2.2.5.b.stepper}), applied to }\sigma\circ\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{Q-1}}\right)  \text{ and }%
k_{Q}\\
\text{instead of }\sigma\text{ and }k
\end{array}
\right) \\
&  \equiv\ell\left(  \sigma\right)  +\left(  Q-1\right)  +1=\ell\left(
\sigma\right)  +Q\operatorname{mod}2.
\end{align*}
In other words, (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q$. This
completes the induction step. Thus, (\ref{sol.ps2.2.5.b.snake.pf.1}) is proven
by induction.

Now, applying (\ref{sol.ps2.2.5.b.snake.pf.1}) to $q=p$, we obtain
$\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \equiv\ell\left(  \sigma\right)
+p\operatorname{mod}2$. This proves (\ref{sol.ps2.2.5.b.snake}).
\end{verlong}

Now, let $\sigma$ and $\tau$ be two permutations in $S_{n}$. Exercise
\ref{exe.ps2.2.5} \textbf{(e)} (applied to $\tau$ instead of $\sigma$) yields
that $\tau$ can be written as a composition of $\ell\left(  \tau\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). In other words, there exists an $\ell\left(  \tau\right)  $-tuple $\left(
k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)  }\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{\ell\left(  \tau\right)  }$ such that $\tau
=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(  \tau\right)  }}$.
Consider this $\left(  k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)
}\right)  $. Then,%
\[
\ell\left(  \sigma\circ\underbrace{\tau}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{\ell\left(  \tau\right)  }}}\right)  =\ell\left(
\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(
\tau\right)  }}\right)  \right)  \equiv\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  \operatorname{mod}2
\]
(by (\ref{sol.ps2.2.5.b.snake}), applied to $p=\ell\left(  \tau\right)  $).
This solves Exercise \ref{exe.ps2.2.5} \textbf{(b)}.

\textbf{(c)} The solution of Exercise \ref{exe.ps2.2.5} \textbf{(c)} is mostly
parallel to our above solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}.

From (\ref{eq.exe.2.5.a.1}), we can easily conclude that%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  \leq\ell\left(  \sigma\right)  +1
\label{sol.ps2.2.5.c.stepper}%
\end{equation}
for every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.c.stepper}):} Let $\sigma\in S_{n}$ and
$k\in\left\{  1,2,\ldots,n-1\right\}  $. From (\ref{eq.exe.2.5.a.1}), we
obtain%
\begin{align*}
\ell\left(  \sigma\circ s_{k}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \leq\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\right)
-1\leq\ell\left(  \sigma\right)  +1\text{ in the case when }\sigma\left(
k\right)  >\sigma\left(  k+1\right)  \right) \\
&  =\ell\left(  \sigma\right)  +1.
\end{align*}
This proves (\ref{sol.ps2.2.5.c.stepper}).
\end{verlong}

Thus, using induction, it is easy to prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \leq\ell\left(  \sigma\right)  +p
\label{sol.ps2.2.5.c.snake}%
\end{equation}
for every $\sigma\in S_{n}$, every $p\in\mathbb{N}$ and every $\left(
k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{p}$.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.c.snake}):} Let $\sigma\in S_{n}$, let
$p\in\mathbb{N}$ and let $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$. We shall prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{q}}\right)  \right)  \leq\ell\left(  \sigma\right)  +q
\label{sol.ps2.2.5.c.snake.pf.1}%
\end{equation}
for all $q\in\left\{  0,1,\ldots,p\right\}  $.

Indeed, let us prove (\ref{sol.ps2.2.5.c.snake.pf.1}) by induction over $q$.

\textit{Induction base:} We have $\ell\left(  \sigma\circ\underbrace{\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  }_{=\left(
\text{empty composition}\right)  =\operatorname*{id}}\right)  =\ell\left(
\sigma\circ\operatorname*{id}\right)  =\ell\left(  \sigma\right)  =\ell\left(
\sigma\right)  +0\leq\ell\left(  \sigma\right)  +0$. In other words,
(\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=0$. This completes the induction base.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,p\right\}  $ be
positive. Assume that (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q-1$. We
need to show that (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q$.

We have assumed that (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q-1$. In
other words, we have%
\[
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \right)  \leq\ell\left(  \sigma\right)  +\left(
Q-1\right)  .
\]
Now,%
\begin{align*}
&  \ell\left(  \sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q}}\right)  }_{=\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q-1}}\right)  \circ s_{k_{Q}}}\right) \\
&  =\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \circ s_{k_{Q}}\right) \\
&  \leq\underbrace{\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{Q-1}}\right)  \right)  }_{\leq\ell\left(
\sigma\right)  +\left(  Q-1\right)  }+1\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{sol.ps2.2.5.c.stepper}), applied to }\sigma\circ\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{Q-1}}\right)  \text{ and }%
k_{Q}\\
\text{instead of }\sigma\text{ and }k
\end{array}
\right) \\
&  \leq\ell\left(  \sigma\right)  +\left(  Q-1\right)  +1=\ell\left(
\sigma\right)  +Q.
\end{align*}
In other words, (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q$. This
completes the induction step. Thus, (\ref{sol.ps2.2.5.c.snake.pf.1}) is proven
by induction.

Now, applying (\ref{sol.ps2.2.5.c.snake.pf.1}) to $q=p$, we obtain
$\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \leq\ell\left(  \sigma\right)  +p$. This proves
(\ref{sol.ps2.2.5.c.snake}).
\end{verlong}

Now, let $\sigma$ and $\tau$ be two permutations in $S_{n}$. Exercise
\ref{exe.ps2.2.5} \textbf{(e)} (applied to $\tau$ instead of $\sigma$) yields
that $\tau$ can be written as a composition of $\ell\left(  \tau\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). In other words, there exists an $\ell\left(  \tau\right)  $-tuple $\left(
k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)  }\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{\ell\left(  \tau\right)  }$ such that $\tau
=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(  \tau\right)  }}$.
Consider this $\left(  k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)
}\right)  $. Then,%
\[
\ell\left(  \sigma\circ\underbrace{\tau}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{\ell\left(  \tau\right)  }}}\right)  =\ell\left(
\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(
\tau\right)  }}\right)  \right)  \leq\ell\left(  \sigma\right)  +\ell\left(
\tau\right)
\]
(by (\ref{sol.ps2.2.5.c.snake}), applied to $p=\ell\left(  \tau\right)  $).
This solves Exercise \ref{exe.ps2.2.5} \textbf{(c)}.

\textbf{(g)} Exercise \ref{exe.ps2.2.5} \textbf{(e)} shows that $\sigma$ can
be written as a composition of $\ell\left(  \sigma\right)  $ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). In other
words, $\ell\left(  \sigma\right)  $ is an $N\in\mathbb{N}$ such that $\sigma$
can be written as a composition of $N$ permutations of the form $s_{k}$ (with
$k\in\left\{  1,2,\ldots,n-1\right\}  $). In order to solve Exercise
\ref{exe.ps2.2.5} \textbf{(g)}, it only remains to show that $\ell\left(
\sigma\right)  $ is the \textbf{smallest} such $N$. In other words, it remains
to show that if $N\in\mathbb{N}$ is such that $\sigma$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $), then $N\geq\ell\left(  \sigma\right)  $.

So let $N\in\mathbb{N}$ be such that $\sigma$ can be written as a composition
of $N$ permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots
,n-1\right\}  $). In other words, there exists an $N$-tuple $\left(
k_{1},k_{2},\ldots,k_{N}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{N}$
such that $\sigma=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{N}}$.
Applying (\ref{sol.ps2.2.5.c.snake}) to $\operatorname*{id}$ and $N$ instead
of $\sigma$ and $p$, we obtain%
\[
\ell\left(  \operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{N}}\right)  \right)  \leq\underbrace{\ell\left(
\operatorname*{id}\right)  }_{=0}+N=N.
\]
Since $\operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{N}}\right)  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{N}}=\sigma$,
this rewrites as $\ell\left(  \sigma\right)  \leq N$. In other words,
$N\geq\ell\left(  \sigma\right)  $. This completes our solution to Exercise
\ref{exe.ps2.2.5} \textbf{(g)}.
\end{proof}

\begin{remark}
The above solution to Exercise \ref{exe.ps2.2.5} owes most of its length to my
attempts at being precise. As Pascal said, \textquotedblleft I have made this
longer than usual because I have not had time to make it
shorter\textquotedblright. The proofs are not, per se, difficult, but this is
combinatorics, and proofs in combinatorics often have to walk a tightrope
between being unreadably long and unreadably terse.

Most parts of Exercise \ref{exe.ps2.2.5} can be proven in more than just one
way. Let me briefly mention an alternative proof for parts \textbf{(b)} and
\textbf{(c)}. Namely, let us use the notation $\operatorname*{Inv}\left(
\sigma\right)  $ for the set of all inversions of a permutation $\sigma$. Let
$\sigma\in S_{n}$ and $\tau\in S_{n}$. Then, it is not hard to convince
oneself that%
\[
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  =A\cup B,
\]
where $A$ and $B$ are the sets defined by%
\begin{align*}
A  &  =\left\{  \left(  i,j\right)  \ \mid\ \left(  i,j\right)  \in
\operatorname*{Inv}\left(  \tau\right)  \text{ and }\left(  \tau\left(
j\right)  ,\tau\left(  i\right)  \right)  \notin\operatorname*{Inv}\left(
\sigma\right)  \right\}  ;\\
B  &  =\left\{  \left(  i,j\right)  \ \mid\ \left(  i,j\right)  \notin%
\operatorname*{Inv}\left(  \tau\right)  \text{ and }\left(  \tau\left(
i\right)  ,\tau\left(  j\right)  \right)  \in\operatorname*{Inv}\left(
\sigma\right)  \right\}
\end{align*}
(where $\left(  i,j\right)  $ are subject to the condition $1\leq i<j\leq n$
both times). (Essentially, this is because an $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ either satisfies
$\tau\left(  i\right)  >\tau\left(  j\right)  $ or satisfies $\tau\left(
i\right)  <\tau\left(  j\right)  $. In the first case, this $\left(
i,j\right)  $ belongs to $A$; in the second case, it belongs to $B$.) It is
furthermore clear that $\left\vert A\right\vert \leq\left\vert
\operatorname*{Inv}\left(  \tau\right)  \right\vert =\ell\left(  \tau\right)
$ (by (\ref{sol.ps2.2.5.f.1})) and $\left\vert B\right\vert \leq\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert =\ell\left(
\sigma\right)  $ (by (\ref{sol.ps2.2.5.f.1})). Hence, (\ref{sol.ps2.2.5.f.1})
yields $\ell\left(  \sigma\circ\tau\right)  =\left\vert
\underbrace{\operatorname*{Inv}\left(  \sigma\circ\tau\right)  }_{=A\cup
B}\right\vert =\left\vert A\cup B\right\vert \leq\underbrace{\left\vert
A\right\vert }_{\leq\ell\left(  \tau\right)  }+\underbrace{\left\vert
B\right\vert }_{\leq\ell\left(  \sigma\right)  }\leq\ell\left(  \tau\right)
+\ell\left(  \sigma\right)  $. This solves Exercise \ref{exe.ps2.2.5}
\textbf{(c)}. To solve part \textbf{(b)}, we need to take a few more steps.
First, it is clear that $A\cap B=\varnothing$, so that $\left\vert A\cup
B\right\vert =\left\vert A\right\vert +\left\vert B\right\vert $. Second, let
us set%
\[
C=\left\{  \left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}%
\ \mid\ i<j\text{, }\tau\left(  i\right)  >\tau\left(  j\right)  \text{ and
}\sigma\left(  \tau\left(  i\right)  \right)  <\sigma\left(  \tau\left(
j\right)  \right)  \right\}  .
\]
Then, it is easy to see that $C\subseteq\operatorname*{Inv}\left(
\tau\right)  $ and $A=\operatorname*{Inv}\left(  \tau\right)  \setminus C$, so
that $\left\vert A\right\vert =\left\vert \operatorname*{Inv}\left(
\tau\right)  \right\vert -\left\vert C\right\vert $. Moreover, if $\mathbf{t}$
denotes the permutation of $\left\{  1,2,\ldots,n\right\}  ^{2}$ which sends
every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ to $\left(
\tau\left(  i\right)  ,\tau\left(  j\right)  \right)  $, and if $\mathbf{f}$
denotes the permutation of $\left\{  1,2,\ldots,n\right\}  ^{2}$ which sends
every $\left(  i,j\right)  $ to $\left(  j,i\right)  $, then $\mathbf{f}%
\left(  C\right)  \subseteq\mathbf{t}^{-1}\left(  \operatorname*{Inv}\left(
\sigma\right)  \right)  $ and $B=\mathbf{t}^{-1}\left(  \operatorname*{Inv}%
\left(  \sigma\right)  \right)  \setminus\mathbf{f}\left(  C\right)  $, so
that $\left\vert B\right\vert =\underbrace{\left\vert \mathbf{t}^{-1}\left(
\operatorname*{Inv}\left(  \sigma\right)  \right)  \right\vert }_{=\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }-\underbrace{\left\vert
\mathbf{f}\left(  C\right)  \right\vert }_{=\left\vert C\right\vert
}=\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert -\left\vert
C\right\vert $. Thus,%
\begin{align*}
\ell\left(  \sigma\circ\tau\right)   &  =\left\vert
\underbrace{\operatorname*{Inv}\left(  \sigma\circ\tau\right)  }_{=A\cup
B}\right\vert =\left\vert A\cup B\right\vert =\underbrace{\left\vert
A\right\vert }_{=\left\vert \operatorname*{Inv}\left(  \tau\right)
\right\vert -\left\vert C\right\vert }+\underbrace{\left\vert B\right\vert
}_{=\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert
-\left\vert C\right\vert }\\
&  =\left(  \underbrace{\left\vert \operatorname*{Inv}\left(  \tau\right)
\right\vert }_{=\ell\left(  \tau\right)  }-\left\vert C\right\vert \right)
+\left(  \underbrace{\left\vert \operatorname*{Inv}\left(  \sigma\right)
\right\vert }_{=\ell\left(  \sigma\right)  }-\left\vert C\right\vert \right)
=\left(  \ell\left(  \tau\right)  -\left\vert C\right\vert \right)  +\left(
\ell\left(  \sigma\right)  -\left\vert C\right\vert \right) \\
&  =\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  -2\left\vert
C\right\vert \equiv\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
\operatorname{mod}2.
\end{align*}
This solves Exercise \ref{exe.ps2.2.5} \textbf{(b)}.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps2.2.6}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.6}.]We need to show that if
$p\in\mathbb{N}$ and $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$ are such that $\sigma=s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{p}}$, then $p\equiv\ell\left(  \sigma\right)
\operatorname{mod}2$.

Let $p\in\mathbb{N}$ and $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$ be such that $\sigma=s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{p}}$. We must prove that $p\equiv\ell\left(
\sigma\right)  \operatorname{mod}2$.

Applying (\ref{sol.ps2.2.5.b.snake}) to $\operatorname*{id}$ instead of
$\sigma$, we see that%
\[
\ell\left(  \operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{p}}\right)  \right)  \equiv\underbrace{\ell\left(
\operatorname*{id}\right)  }_{=0}+p=p\operatorname{mod}2.
\]
Since $\operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{p}}=\sigma$,
this rewrites as $\ell\left(  \sigma\right)  \equiv p\operatorname{mod}2$. In
other words, $p\equiv\ell\left(  \sigma\right)  \operatorname{mod}2$. This
completes the solution of Exercise \ref{exe.ps2.2.6}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.7}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.7}.]We have $n\geq2$. Thus, the
permutation $s_{1}$ is well-defined. (This is the permutation which switches
$1$ with $2$ while leaving all other elements of $\left\{  1,2,\ldots
,n\right\}  $ unchanged.)

Let $A_{n}$ denote the set of all even permutations in $S_{n}$. Let $C_{n}$
denote the set of all odd permutations in $S_{n}$. The sign of a permutation
in $S_{n}$ is either $1$ or $-1$ (because it is defined to be an integer power
of $-1$), but not both. Hence, every permutation in $S_{n}$ is either even or
odd, but not both. In other words, we have $S_{n}=A_{n}\cup C_{n}$ and
$A_{n}\cap C_{n}=\varnothing$. Therefore, $\left\vert S_{n}\right\vert
=\left\vert A_{n}\right\vert +\left\vert C_{n}\right\vert $.

Now, for every $\sigma\in A_{n}$, we have $\sigma\circ s_{k}\in C_{n}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in A_{n}$. Thus, $\sigma$ is an
even permutation in $S_{n}$ (since $A_{n}$ is the set of all even permutations
in $S_{n}$). Since $\sigma$ is even, we have $\left(  -1\right)  ^{\sigma}=1$,
so that $1=\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }$. Therefore, $\ell\left(  \sigma\right)  \equiv
0\operatorname{mod}2$. Now, (\ref{sol.ps2.2.5.b.stepper}) yields $\ell\left(
\sigma\circ s_{k}\right)  \equiv\underbrace{\ell\left(  \sigma\right)
}_{\equiv0\operatorname{mod}2}+1\equiv1\operatorname{mod}2$, so that $\left(
-1\right)  ^{\ell\left(  \sigma\circ s_{k}\right)  }=-1$. But now, $\left(
-1\right)  ^{\sigma\circ s_{k}}=\left(  -1\right)  ^{\ell\left(  \sigma\circ
s_{k}\right)  }=-1$, so that the permutation $\sigma\circ s_{k}$ is odd. In
other words, $\sigma\circ s_{k}\in C_{n}$ (since $C_{n}$ is the set of all odd
permutations in $S_{n}$), qed.}. Hence, we can define a map $\Phi
:A_{n}\rightarrow C_{n}$ by
\[
\Phi\left(  \sigma\right)  =\sigma\circ s_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in A_{n}.
\]
Similarly, we can define a map $\Psi:C_{n}\rightarrow A_{n}$ by
\[
\Psi\left(  \sigma\right)  =\sigma\circ s_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in C_{n}.
\]
These two maps $\Phi$ and $\Psi$ are mutually inverse (since every $\sigma\in
S_{n}$ satisfies $\sigma\circ\underbrace{s_{k}\circ s_{k}}_{=s_{k}%
^{2}=\operatorname*{id}}=\sigma$). Therefore, the map $\Phi$ is a bijection.
Thus, there exists a bijection form $A_{n}$ to $C_{n}$ (namely, $\Phi$), so
that we obtain $\left\vert A_{n}\right\vert =\left\vert C_{n}\right\vert $.
Hence, $\left\vert S_{n}\right\vert =\underbrace{\left\vert A_{n}\right\vert
}_{=\left\vert C_{n}\right\vert }+\left\vert C_{n}\right\vert =\left\vert
C_{n}\right\vert +\left\vert C_{n}\right\vert =2\left\vert C_{n}\right\vert $
and therefore $\left\vert C_{n}\right\vert =\dfrac{1}{2}\underbrace{\left\vert
S_{n}\right\vert }_{=n!}=\dfrac{1}{2}n!=n!/2$. In other words, the number of
odd permutations in $S_{n}$ is $n!/2$. Similarly, the number of even
permutations in $S_{n}$ is $n!/2$. Exercise \ref{exe.ps2.2.7} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.4'}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.4'}.]The solution of Exercise
\ref{exe.ps2.2.4'} \textbf{(a)} is completely analogous to the solution of
Exercise \ref{exe.ps2.2.4} \textbf{(a)}; it can be obtained from the latter by
replacing $S_{n}$ by $S_{\infty}$, replacing $\left\{  1,2,\ldots,n\right\}  $
by $\left\{  1,2,3,\ldots\right\}  $, and replacing $\left\{  1,2,\ldots
,n-2\right\}  $ by $\left\{  1,2,3,\ldots\right\}  $.

As for Exercise \ref{exe.ps2.2.4'} \textbf{(b)}, we again omit the solution,
because it follows from an exercise that will be solved below (Exercise
\ref{exe.ps2.2.5'} \textbf{(e)}).
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.5'}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.5'}.]A solution of Exercise
\ref{exe.ps2.2.5'} can be obtained by copying our above solution of Exercise
\ref{exe.ps2.2.5} almost verbatim, occasionally doing some replacements (e.g.,
we have to replace $S_{n}$ by $S_{\left(  \infty\right)  }$, to replace $1\leq
i<j\leq n$ by $1\leq i<j$, to replace $\left\{  1,2,\ldots,n-1\right\}  $ by
$\left\{  1,2,3,\ldots\right\}  $, and to replace $\left\{  1,2,\ldots
,n\right\}  $ by $\left\{  1,2,3,\ldots\right\}  $). We leave the
straightforward changes to the reader.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.6'}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.6'}.]A solution of Exercise
\ref{exe.ps2.2.6'} can be obtained by copying our above solution of Exercise
\ref{exe.ps2.2.6} almost verbatim, occasionally doing some replacements (e.g.,
we have to replace $S_{n}$ by $S_{\left(  \infty\right)  }$, to replace
$\left\{  1,2,\ldots,n-1\right\}  $ by $\left\{  1,2,3,\ldots\right\}  $, and
to replace $\left\{  1,2,\ldots,n\right\}  $ by $\left\{  1,2,3,\ldots
\right\}  $). We leave the straightforward changes to the reader.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.0}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.0}.]In the following, \textquotedblleft
path\textquotedblright\ will always mean \textquotedblleft path on the
(undirected) $n$-th right Bruhat graph\textquotedblright. Hence, we need to
prove that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest length
of a path between $\sigma$ and $\tau$.

We write any path as the tuple consisting of its vertices (from its beginning
to its end).\footnote{This is legitimate, because the (undirected) $n$-th
right Bruhat graph does not have multiple edges.}

Let $L=\ell\left(  \sigma^{-1}\circ\tau\right)  $. We shall first show that
there exists a path of length $L$ between $\sigma$ and $\tau$.

Indeed, Exercise \ref{exe.ps2.2.5} \textbf{(g)} (applied to $\sigma^{-1}%
\circ\tau$ instead of $\sigma$) yields that $\ell\left(  \sigma^{-1}\circ
\tau\right)  $ is the smallest $N\in\mathbb{N}$ such that $\sigma^{-1}%
\circ\tau$ can be written as a composition of $N$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). Since $L=\ell\left(
\sigma^{-1}\circ\tau\right)  $, we can rewrite this as follows: $L$ is the
smallest $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). Hence, $\sigma^{-1}\circ\tau$ can be written as a
composition of $L$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). In other words, there exists an $L$-tuple $\left(
j_{1},j_{2},\ldots,j_{L}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{L}$
such that $\sigma^{-1}\circ\tau=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ
s_{j_{L}}$. Consider this $\left(  j_{1},j_{2},\ldots,j_{L}\right)  $.

We have $\underbrace{\sigma\circ\sigma^{-1}}_{=\operatorname*{id}}\circ
\tau=\tau$, so that $\tau=\sigma\circ\underbrace{\sigma^{-1}\circ\tau
}_{=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  $. Now, for every
$p\in\left\{  0,1,\ldots,L\right\}  $, define a permutation $\gamma_{p}\in
S_{n}$ by%
\[
\gamma_{p}=\sigma\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ
s_{j_{p}}\right)  .
\]
Thus, $\gamma_{0}=\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{0}}\right)  }_{=\left(  \text{a composition of }0\text{
permutations}\right)  =\operatorname*{id}}=\sigma$ and $\gamma_{L}=\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  =\tau$.

Now, for every $i\in\left\{  1,2,\ldots,L\right\}  $, the vertices $\gamma
_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right Bruhat graph are
adjacent\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,L\right\}  $.
Then, the definition of $\gamma_{i-1}$ yields $\gamma_{i-1}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)  $, whereas the
definition of $\gamma_{i}$ yields%
\[
\gamma_{i}=\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}\circ
\cdots\circ s_{j_{i}}\right)  }_{=\left(  s_{j_{1}}\circ s_{j_{2}}\circ
\cdots\circ s_{j_{i-1}}\right)  \circ s_{j_{i}}}=\underbrace{\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)
}_{=\gamma_{i-1}}\circ s_{j_{i}}=\gamma_{i-1}\circ s_{j_{i}}.
\]
Therefore, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$\gamma_{i}=\gamma_{i-1}\circ s_{k}$ (namely, $k=j_{i}$). In other words, the
vertices $\gamma_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right
Bruhat graph are adjacent (by the definition of the edges of this graph).
Qed.}. Hence, the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$ form a
path. This path connects $\sigma$ to $\tau$ (since it begins at $\gamma
_{0}=\sigma$ and ends at $\gamma_{L}=\tau$), and has length $L$. Thus, there
exists a path between $\sigma$ and $\tau$ which has length $L$ (namely, the
path formed by the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$).

We shall now show that $L$ is the smallest length of a path between $\sigma$
and $\tau$. Indeed, let $\mathbf{d}$ be any path between $\sigma$ and $\tau$.
We shall show that the length of $\mathbf{d}$ is $\geq L$.

The path $\mathbf{d}$ is a path between $\sigma$ and $\tau$. Hence, we can
write the path $\mathbf{d}$ in the form $\mathbf{d}=\left(  \delta_{0}%
,\delta_{1},\ldots,\delta_{M}\right)  $ for some $\delta_{0},\delta_{1}%
,\ldots,\delta_{M}\in S_{n}$ with $\delta_{0}=\sigma$ and $\delta_{M}=\tau$.
Consider these $\delta_{0},\delta_{1},\ldots,\delta_{M}\in S_{n}$.

For every $i\in\left\{  1,2,\ldots,M\right\}  $, there exists a $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k}%
$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,M\right\}  $.
Then, the vertices $\delta_{i}$ and $\delta_{i-1}$ of the (undirected) $n$-th
right Bruhat graph are adjacent (because they are two consecutive vertices on
the path $\left(  \delta_{0},\delta_{1},\ldots,\delta_{M}\right)  =\mathbf{d}%
$). In other words, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $
such that $\delta_{i}=\delta_{i-1}\circ s_{k}$ (by the definition of the edges
of this graph). Qed.}. We denote this $k$ by $k_{i}$. Thus, for every
$i\in\left\{  1,2,\ldots,M\right\}  $, we have defined a $k_{i}\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$.

Now, every $j\in\left\{  0,1,\ldots,M\right\}  $ satisfies%
\begin{equation}
\delta_{j}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{j}}\right)  \label{sol.ps4.short.0.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.short.0.5}):} We shall prove
(\ref{sol.ps4.short.0.5}) by induction over $j$:
\par
\textit{Induction base:} We have $\delta_{0}=\sigma$. Compared with
$\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{0}}\right)  }_{=\left(  \text{a composition of }0\text{ permutations}%
\right)  =\operatorname*{id}}=\sigma$, this yields $\delta_{0}=\sigma
\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  $. In
other words, (\ref{sol.ps4.short.0.5}) holds for $j=0$. This completes the
induction base.
\par
\textit{Induction step:} Let $J\in\left\{  0,1,\ldots,M\right\}  $ be
positive. Assume that (\ref{sol.ps4.short.0.5}) holds for $j=J-1$. We need to
show that (\ref{sol.ps4.short.0.5}) holds for $j=J$.
\par
We have $\delta_{J}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J-1}}\right)  $ (since (\ref{sol.ps4.short.0.5}) holds for
$j=J-1$). Now, recall that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$ for every
$i\in\left\{  1,2,\ldots,M\right\}  $. Applying this to $i=J$, we obtain%
\begin{align*}
\delta_{J}  &  =\underbrace{\delta_{J-1}}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{J-1}}\right)  }\circ s_{k_{J}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }J\in\left\{  1,2,\ldots,M\right\}
\text{ (since }J\in\left\{  0,1,\ldots,M\right\}  \text{ is positive)}\right)
\\
&  =\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{J-1}}\right)  \circ s_{k_{J}}}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}\right)  .
\end{align*}
In other words, (\ref{sol.ps4.short.0.5}) holds for $j=J$. This completes the
induction step. Thus, (\ref{sol.ps4.short.0.5}) is proven by induction.}.
Applying this to $j=M$, we obtain%
\[
\delta_{M}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{M}}\right)  .
\]
Compared with $\delta_{M}=\tau$, this yields%
\[
\tau=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{M}%
}\right)  ,
\]
so that
\[
\sigma^{-1}\circ\underbrace{\tau}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{M}}\right)  }=\underbrace{\sigma^{-1}%
\circ\sigma}_{=\operatorname*{id}}\circ\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{M}}\right)  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{M}}.
\]
Therefore, the permutation $\sigma^{-1}\circ\tau$ can be written as a
composition of $M$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $) (namely, of the $M$ permutations $s_{k_{1}}$,
$s_{k_{2}}$, $\ldots$, $s_{k_{M}}$).

Now, we recall that $L$ is the smallest $N\in\mathbb{N}$ such that
$\sigma^{-1}\circ\tau$ can be written as a composition of $N$ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). Hence, if
$N\in\mathbb{N}$ is such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $), then $N\geq L$. We can apply this to $N=M$
(because $\sigma^{-1}\circ\tau$ can be written as a composition of $M$
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$)), and thus obtain $M\geq L$.

But the length of the path $\mathbf{d}$ is $M$ (since $\mathbf{d}=\left(
\delta_{0},\delta_{1},\ldots,\delta_{M}\right)  $). Hence, the length of the
path $\mathbf{d}$ is $\geq L$ (since $M\geq L$).

Let us now forget that we fixed $\mathbf{d}$. We thus have shown that if
$\mathbf{d}$ is any path between $\sigma$ and $\tau$, then the length of the
path $\mathbf{d}$ is $\geq L$. In other words, every path between $\sigma$ and
$\tau$ has length $\geq L$.

Altogether, we have proven the following two statements:

\begin{itemize}
\item There exists a path of length $L$ between $\sigma$ and $\tau$.

\item Every path between $\sigma$ and $\tau$ has length $\geq L$.
\end{itemize}

Therefore, $L$ is the smallest length of a path between $\sigma$ and $\tau$.
In other words, $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest
length of a path between $\sigma$ and $\tau$ (since $L=\ell\left(  \sigma
^{-1}\circ\tau\right)  $). Exercise \ref{exe.ps4.0} is solved.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.0}.]Let us first forget about $\sigma$ and
$\tau$, and make a few general remarks.

Let $\sigma\in S_{n}$. Then, Exercise \ref{exe.ps2.2.5} \textbf{(g)} shows
that
\begin{equation}
\left(
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  \text{ is the smallest }N\in\mathbb{N}\text{ such
that }\sigma\text{ can be written as a composition}\\
\text{of }N\text{ permutations of the form }s_{k}\text{ (with }k\in\left\{
1,2,\ldots,n-1\right\}  \text{)}%
\end{array}
\right)  . \label{sol.ps4.0.lem}%
\end{equation}


Let us now forget that we fixed $\sigma$. We thus have proven
(\ref{sol.ps4.0.lem}) for every $\sigma\in S_{n}$.

Now, let $\sigma\in S_{n}$ and $\tau\in S_{n}$. In the following,
\textquotedblleft path\textquotedblright\ will always mean \textquotedblleft
path on the (undirected) $n$-th right Bruhat graph\textquotedblright. Hence,
we need to prove that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the
smallest length of a path between $\sigma$ and $\tau$.

We write any path as the tuple consisting of its vertices (from its beginning
to its end).\footnote{This is legitimate, because the (undirected) $n$-th
right Bruhat graph does not have multiple edges.}

Let $L=\ell\left(  \sigma^{-1}\circ\tau\right)  $. We shall first show that
there exists a path of length $L$ between $\sigma$ and $\tau$.

Indeed, (\ref{sol.ps4.0.lem}) (applied to $\sigma^{-1}\circ\tau$ instead of
$\sigma$) yields that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the
smallest $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). Hence, $\ell\left(  \sigma^{-1}\circ\tau\right)  $
is an $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). In other words, $\sigma^{-1}\circ\tau$ can be
written as a composition of $\ell\left(  \sigma^{-1}\circ\tau\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). In other words, $\sigma^{-1}\circ\tau$ can be written as a composition of
$L$ permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots
,n-1\right\}  $) (since $L=\ell\left(  \sigma^{-1}\circ\tau\right)  $). In
other words, there exists an $L$-tuple $\left(  j_{1},j_{2},\ldots
,j_{L}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{L}$ such that
$\sigma^{-1}\circ\tau=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}$.
Consider this $\left(  j_{1},j_{2},\ldots,j_{L}\right)  $.

We have $\underbrace{\sigma\circ\sigma^{-1}}_{=\operatorname*{id}}\circ
\tau=\tau$, so that $\tau=\sigma\circ\underbrace{\sigma^{-1}\circ\tau
}_{=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  $. Now, for every
$p\in\left\{  0,1,\ldots,L\right\}  $, define a permutation $\gamma_{p}\in
S_{n}$ by%
\[
\gamma_{p}=\sigma\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ
s_{j_{p}}\right)  .
\]
Thus, $\gamma_{0}=\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{0}}\right)  }_{=\left(  \text{a composition of }0\text{
permutations}\right)  =\operatorname*{id}}=\sigma$ and $\gamma_{L}=\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  =\tau$.

Now, for every $i\in\left\{  1,2,\ldots,L\right\}  $, the vertices $\gamma
_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right Bruhat graph are
adjacent\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,L\right\}  $.
Then, the definition of $\gamma_{i-1}$ yields $\gamma_{i-1}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)  $, whereas the
definition of $\gamma_{i}$ yields%
\begin{align*}
\gamma_{i}  &  =\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{i}}\right)  }_{=\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{i-1}}\right)  \circ s_{j_{i}}}=\underbrace{\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)
}_{=\gamma_{i-1}}\circ s_{j_{i}}\\
&  =\gamma_{i-1}\circ s_{j_{i}}.
\end{align*}
Therefore, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$\gamma_{i}=\gamma_{i-1}\circ s_{k}$ (namely, $k=j_{i}$). In other words, the
vertices $\gamma_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right
Bruhat graph are adjacent (by the definition of the edges of this graph).
Qed.}. Hence, the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$ form a
path. This path connects $\sigma$ to $\tau$ (since it begins at $\gamma
_{0}=\sigma$ and ends at $\gamma_{L}=\tau$), and has length $L$. Thus, there
exists a path between $\sigma$ and $\tau$ which has length $L$ (namely, the
path formed by the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$).

We shall now show that $L$ is the smallest length of a path between $\sigma$
and $\tau$. Indeed, let $\mathbf{d}$ be any path between $\sigma$ and $\tau$.
We shall show that the length of $\mathbf{d}$ is $\geq L$.

The path $\mathbf{d}$ is a path between $\sigma$ and $\tau$. Hence, we can
write the path $\mathbf{d}$ in the form $\mathbf{d}=\left(  \delta_{0}%
,\delta_{1},\ldots,\delta_{M}\right)  $ for some $\delta_{0},\delta_{1}%
,\ldots,\delta_{M}\in S_{n}$ with $\delta_{0}=\sigma$ and $\delta_{M}=\tau$.
Consider these $\delta_{0},\delta_{1},\ldots,\delta_{M}\in S_{n}$.

For every $i\in\left\{  1,2,\ldots,M\right\}  $, there exists a $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k}%
$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,M\right\}  $.
Then, the vertices $\delta_{i}$ and $\delta_{i-1}$ of the (undirected) $n$-th
right Bruhat graph are adjacent (because they are two consecutive vertices on
the path $\left(  \delta_{0},\delta_{1},\ldots,\delta_{M}\right)  =\mathbf{d}%
$). In other words, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $
such that $\delta_{i}=\delta_{i-1}\circ s_{k}$ (by the definition of the edges
of this graph). Qed.}. We denote this $k$ by $k_{i}$. Thus, for every
$i\in\left\{  1,2,\ldots,M\right\}  $, we have defined a $k_{i}\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$.

Now, every $j\in\left\{  0,1,\ldots,M\right\}  $ satisfies%
\begin{equation}
\delta_{j}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{j}}\right)  \label{sol.ps4.0.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.0.5}):} We shall prove
(\ref{sol.ps4.0.5}) by induction over $j$:
\par
\textit{Induction base:} We have $\delta_{0}=\sigma$. Compared with
$\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{0}}\right)  }_{=\left(  \text{a composition of }0\text{ permutations}%
\right)  =\operatorname*{id}}=\sigma$, this yields $\delta_{0}=\sigma
\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  $. In
other words, (\ref{sol.ps4.0.5}) holds for $j=0$. This completes the induction
base.
\par
\textit{Induction step:} Let $J\in\left\{  0,1,\ldots,M\right\}  $ be
positive. Assume that (\ref{sol.ps4.0.5}) holds for $j=J-1$. We need to show
that (\ref{sol.ps4.0.5}) holds for $j=J$.
\par
We have $\delta_{J}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J-1}}\right)  $ (since (\ref{sol.ps4.0.5}) holds for
$j=J-1$). Now, recall that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$ for every
$i\in\left\{  1,2,\ldots,M\right\}  $. Applying this to $i=J$, we obtain%
\begin{align*}
\delta_{J}  &  =\underbrace{\delta_{J-1}}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{J-1}}\right)  }\circ s_{k_{J}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }J\in\left\{  1,2,\ldots,M\right\}
\text{ (since }J\in\left\{  0,1,\ldots,M\right\}  \text{ is positive)}\right)
\\
&  =\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{J-1}}\right)  \circ s_{k_{J}}}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}\right)  .
\end{align*}
In other words, (\ref{sol.ps4.0.5}) holds for $j=J$. This completes the
induction step. Thus, (\ref{sol.ps4.0.5}) is proven by induction.}. Applying
this to $j=M$, we obtain%
\[
\delta_{M}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{M}}\right)  .
\]
Compared with $\delta_{M}=\tau$, this yields%
\[
\tau=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{M}%
}\right)  ,
\]
so that
\begin{align*}
\sigma^{-1}\circ\underbrace{\tau}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{M}}\right)  }  &  =\underbrace{\sigma
^{-1}\circ\sigma}_{=\operatorname*{id}}\circ\left(  s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{M}}\right) \\
&  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{M}}.
\end{align*}
Therefore, the permutation $\sigma^{-1}\circ\tau$ can be written as a
composition of $M$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $) (namely, of the $M$ permutations $s_{k_{1}}$,
$s_{k_{2}}$, $\ldots$, $s_{k_{M}}$).

Now, we recall that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the
smallest $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). Hence, if $N\in\mathbb{N}$ is such that
$\sigma^{-1}\circ\tau$ can be written as a composition of $N$ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $), then
$N\geq\ell\left(  \sigma^{-1}\circ\tau\right)  $. We can apply this to $N=M$
(because $\sigma^{-1}\circ\tau$ can be written as a composition of $M$
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$)), and thus obtain $M\geq\ell\left(  \sigma^{-1}\circ\tau\right)  =L$.

But the length of the path $\mathbf{d}$ is $M$ (since $\mathbf{d}=\left(
\delta_{0},\delta_{1},\ldots,\delta_{M}\right)  $). Hence, the length of the
path $\mathbf{d}$ is $\geq L$ (since $M\geq L$).

Let us now forget that we fixed $\mathbf{d}$. We thus have shown that if
$\mathbf{d}$ is any path between $\sigma$ and $\tau$, then the length of the
path $\mathbf{d}$ is $\geq L$. In other words, every path between $\sigma$ and
$\tau$ has length $\geq L$.

Altogether, we have proven the following two statements:

\begin{itemize}
\item There exists a path of length $L$ between $\sigma$ and $\tau$.

\item Every path between $\sigma$ and $\tau$ has length $\geq L$.
\end{itemize}

Therefore, $L$ is the smallest length of a path between $\sigma$ and $\tau$.
In other words, $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest
length of a path between $\sigma$ and $\tau$ (since $L=\ell\left(  \sigma
^{-1}\circ\tau\right)  $). Exercise \ref{exe.ps4.0} is solved.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.ps4.1ab}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.1ab}.]\textbf{(a)} We shall prove that%
\begin{equation}
\ell\left(  t_{i,j}\right)  =2\left\vert j-i\right\vert -1.
\label{sol.ps4.1ab.a.claim}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.1ab.a.claim}):} We know that $t_{i,j}$ is the
permutation in $S_{n}$ which switches $i$ with $j$ while leaving all other
elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged; on the other hand,
$t_{j,i}$ is the permutation in $S_{n}$ which switches $j$ with $i$ while
leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged.
Comparing these two descriptions of $t_{i,j}$ and $t_{j,i}$, we immediately
see that they are identical (since switching $i$ with $j$ is the same thing as
switching $j$ with $i$). Thus, $t_{i,j}=t_{j,i}$. Also, clearly, $\left\vert
j-i\right\vert =\left\vert i-j\right\vert $. Hence, the claim
(\ref{sol.ps4.1ab.a.claim}) does not change if we switch $i$ with $j$. Thus,
we can WLOG assume that $i\leq j$ (because otherwise, we can just switch $i$
with $j$). Assume this. Now, $i\leq j$, so that $i<j$ (since $i$ and $j$ are
distinct). Hence, $j>i$, so that $j-i>0$, so that $\left\vert j-i\right\vert
=j-i$.

\begin{vershort}
Set%
\begin{align*}
A  &  =\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  \ \ \ \ \ \ \ \ \ \ \text{and}\\
B  &  =\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right\}  .
\end{align*}
These two sets $A$ and $B$ satisfy $\left\vert A\right\vert =j-i$ and
$\left\vert B\right\vert =j-i-1$. Also, it is easy to see that these sets $A$
and $B$ are disjoint\footnote{\textit{Proof.} Every element of $B$ is a pair
$\left(  k,j\right)  $ whose first entry is $>i$, whereas every element of $A$
is a pair $\left(  i,k\right)  $ whose first entry equals $i$. Thus, if the
sets $A$ and $B$ had an element $e$ in common, then $e$ would be a pair whose
first entry is $>i$ (since $e\in B$) and equals $i$ (since $e\in A$) at the
same time, which of course is impossible. Hence, $A$ and $B$ are disjoint.}.
Thus,%
\[
\left\vert A\cup B\right\vert =\underbrace{\left\vert A\right\vert }%
_{=j-i}+\underbrace{\left\vert B\right\vert }_{=j-i-1}=\left(  j-i\right)
+\left(  j-i-1\right)  =2\underbrace{\left(  j-i\right)  }_{=\left\vert
j-i\right\vert }-1=2\left\vert j-i\right\vert -1.
\]


Now, let $\operatorname*{Inv}\left(  t_{i,j}\right)  $ denote the set of all
inversions of $t_{i,j}$. Then, $\ell\left(  t_{i,j}\right)  =\left\vert
\operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert $ (because $\ell\left(
t_{i,j}\right)  $ was defined as the number of inversions of $t_{i,j}$, which
number is obviously $\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)
\right\vert $).

We shall now show that $\operatorname*{Inv}\left(  t_{i,j}\right)  =A\cup B$.
Indeed, it is clearly enough to prove $A\cup B\subseteq\operatorname*{Inv}%
\left(  t_{i,j}\right)  $ and $\operatorname*{Inv}\left(  t_{i,j}\right)
\subseteq A\cup B$. Proving that $A\cup B\subseteq\operatorname*{Inv}\left(
t_{i,j}\right)  $ means proving that every element of $A\cup B$ is an
inversion of $t_{i,j}$; this is straightforward\footnote{\textit{Proof.} We
want to show that $A\cup B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)
$. In other words, we want to prove that $e\in\operatorname*{Inv}\left(
t_{i,j}\right)  $ for every $e\in A\cup B$.
\par
So let $e\in A\cup B$. Thus, either $e\in A$ or $e\in B$.
\par
Let us first consider the case when $e\in A$. Thus, $e\in A=\left\{  \left(
i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\}  $. In
other words, $e$ has the form $e=\left(  i,k\right)  $ for some $k\in\left\{
i+1,i+2,\ldots,j\right\}  $. Consider this $k$. The permutation $t_{i,j}$
switches $i$ with $j$ while leaving all other numbers fixed. Thus, the
permutation $t_{i,j}$ leaves the numbers $i+1,i+2,\ldots,j-1$ fixed, while
sending the number $j$ to $i$. Consequently, $t_{i,j}$ sends the numbers
$i+1,i+2,\ldots,j-1,j$ to $i+1,i+2,\ldots,j-1,i$, respectively. Notice that
all of the latter numbers $i+1,i+2,\ldots,j-1,i$ are smaller than $j$. Thus,
$t_{i,j}\left(  p\right)  <j$ for every $p\in\left\{  i+1,i+2,\ldots
,j\right\}  $. Applying this to $p=k$, we conclude that $t_{i,j}\left(
k\right)  <j$.
\par
We have $i<k$ (since $k\in\left\{  i+1,i+2,\ldots,j\right\}  $) but
$t_{i,j}\left(  i\right)  =j>t_{i,j}\left(  k\right)  $ (since we have just
showed that $t_{i,j}\left(  k\right)  <j$). Thus, $\left(  i,k\right)  $ is an
inversion of $t_{i,j}$. In other words, $\left(  i,k\right)  \in
\operatorname*{Inv}\left(  t_{i,j}\right)  $. Thus, $e=\left(  i,k\right)
\in\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
Thus, we have proven that $e\in\operatorname*{Inv}\left(  t_{i,j}\right)  $ in
the case when $e\in A$. A similar argument (but now using $t_{i,j}\left(
k\right)  >i$ instead of $t_{i,j}\left(  k\right)  <j$) shows that
$e\in\operatorname*{Inv}\left(  t_{i,j}\right)  $ in the case when $e\in B$.
Since either of these two cases must hold (because we have either $e\in A$ or
$e\in B$), we thus conclude that $e\in\operatorname*{Inv}\left(
t_{i,j}\right)  $. This concludes the proof.}. Proving that
$\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$ means proving
that every inversion of $t_{i,j}$ belongs to $A\cup B$; this is equally
straightforward (although more tiresome)\footnote{\textit{Proof.} We want to
show that $\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$. In
other words, we want to prove that $c\in A\cup B$ for every $c\in
\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
So let $c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $. Thus, $c$ is an
inversion of $t_{i,j}$. In other words, $c$ is a pair $\left(  u,v\right)  $
of integers satisfying $1\leq u<v\leq n$ and $t_{i,j}\left(  u\right)
>t_{i,j}\left(  v\right)  $. Consider this $\left(  u,v\right)  $. Thus,
$c=\left(  u,v\right)  $. Our goal is to show that $c\in A\cup B$.
\par
The permutation $t_{i,j}$ switches $i$ with $j$ while leaving all other
numbers fixed. It thus makes sense to analyze several cases separately,
depending on which of the numbers $u$ and $v$ belongs to $\left\{
i,j\right\}  $. Four cases are possible:
\par
\textit{Case 1:} We have $u\in\left\{  i,j\right\}  $ and $v\in\left\{
i,j\right\}  $.
\par
\textit{Case 2:} We have $u\in\left\{  i,j\right\}  $ and $v\notin\left\{
i,j\right\}  $.
\par
\textit{Case 3:} We have $u\notin\left\{  i,j\right\}  $ and $v\in\left\{
i,j\right\}  $.
\par
\textit{Case 4:} We have $u\notin\left\{  i,j\right\}  $ and $v\notin\left\{
i,j\right\}  $.
\par
Let us first consider Case 1. In this case, we have $u\in\left\{  i,j\right\}
$ and $v\in\left\{  i,j\right\}  $. Thus, $u$ and $v$ are two elements of
$\left\{  i,j\right\}  $. Since $u<v$, this leaves only one possibility for
the pair $\left(  u,v\right)  $: namely, $\left(  u,v\right)  =\left(
i,j\right)  $. Thus,
\begin{align*}
c  &  =\left(  u,v\right)  =\left(  i,j\right)  \in\left\{  \left(
i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j\in\left\{  i+1,i+2,\ldots
,j\right\}  \right) \\
&  =A\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $u\in\left\{  i,j\right\}
$ and $v\notin\left\{  i,j\right\}  $. Since $v\notin\left\{  i,j\right\}  $,
we have $t_{i,j}\left(  v\right)  =v$ (since $t_{i,j}$ leaves all numbers
other than $i$ and $j$ unchanged). Thus, $t_{i,j}\left(  u\right)
>t_{i,j}\left(  v\right)  =v>u$ (since $u<v$). If we had $u=j$, then this
would rewrite as $t_{i,j}\left(  j\right)  >j$, which would contradict
$t_{i,j}\left(  j\right)  =i<j$. Thus, we cannot have $u=j$. Hence, we must
have $u=i$ (since $u\in\left\{  i,j\right\}  $ forces $u$ to be either $i$ or
$j$). But we have shown that $t_{i,j}\left(  u\right)  >v$, so that
$v<t_{i,j}\left(  \underbrace{u}_{=i}\right)  =t_{i,j}\left(  i\right)  =j$
(since $t_{i,j}$ switches $i$ with $j$). Combined with $v>u=i$, this yields
$i<v<j$, so that $v\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, so that%
\begin{align*}
c  &  =\left(  \underbrace{u}_{=i},v\right)  =\left(  i,v\right)  \in\left\{
\left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right) \\
&  \subseteq\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  =A\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 2.
\par
Let us next consider Case 3. In this case, we have $u\notin\left\{
i,j\right\}  $ and $v\in\left\{  i,j\right\}  $. Since $u\notin\left\{
i,j\right\}  $, we have $t_{i,j}\left(  u\right)  =u$ (since $t_{i,j}$ leaves
all numbers other than $i$ and $j$ unchanged). Thus, from $t_{i,j}\left(
u\right)  >t_{i,j}\left(  v\right)  $, we obtain $t_{i,j}\left(  v\right)
<t_{i,j}\left(  u\right)  =u<v$. If we had $v=i$, then this would rewrite as
$t_{i,j}\left(  i\right)  <i$, which would contradict $t_{i,j}\left(
i\right)  =j>i$. Thus, we cannot have $v=i$. Hence, we must have $v=j$ (since
$v\in\left\{  i,j\right\}  $ forces $v$ to be either $i$ or $j$). But we have
shown that $t_{i,j}\left(  v\right)  <u$, so that $u>t_{i,j}\left(
\underbrace{v}_{=j}\right)  =t_{i,j}\left(  j\right)  =i$ (since $t_{i,j}$
switches $i$ with $j$). Combined with $u<v=j$, this yields $i<u<j$, so that
$u\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, so that%
\begin{align*}
c  &  =\left(  u,\underbrace{v}_{=j}\right)  =\left(  u,j\right)  \in\left\{
\left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }u\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right) \\
&  =B\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 3.
\par
(Notice that Case 3 was very similar to Case 2 -- almost like a mirror version
of that case, if not for a slight asymmetry in our definition of the sets $A$
and $B$.)
\par
Let us finally consider Case 4. In this case, we have $u\notin\left\{
i,j\right\}  $ and $v\notin\left\{  i,j\right\}  $. Thus, $t_{i,j}\left(
u\right)  =u$ (as in Case 3) and $t_{i,j}\left(  v\right)  =v$ (as in Case 2),
so that $t_{i,j}\left(  u\right)  =u<v=t_{i,j}\left(  v\right)  $. This
contradicts $t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)  $. This
contradiction shows that Case 4 cannot happen; thus, we can ignore this case
completely. (Or we can argue that because \textquotedblleft ex falso
quodlibet\textquotedblright, we have $c\in A\cup B$ in Case 4.
\par
[The principle of
\href{https://en.wikipedia.org/wiki/Principle_of_explosion}{``ex falso
quodlibet''} says that from a false assertion, any arbitrary assertion
follows. (For example, if $1 = 0$, then anything is true.) This is one of the
basic principles in logic, and we could use it here to prove $c \in A \cup B$
in Case 4: Namely, since we have derived a contradiction (i.e., proven a false
assertion) in Case 4, we see that any arbitrary assertion holds in Case 4; in
particular, $c \in A \cup B$ holds in Case 4.])
\par
We have now checked that $c\in A\cup B$ in each of the four cases 1, 2, 3 and
4 (or in each of the three cases 1, 2 and 3, and Case 4 never happens). Thus,
$c\in A\cup B$ always holds. This completes our proof.}. Hence, both $A\cup
B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $ and
$\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$ are proven, and
we conclude that $\operatorname*{Inv}\left(  t_{i,j}\right)  =A\cup B$. Thus,
$\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert =\left\vert
A\cup B\right\vert =2\left\vert j-i\right\vert -1$. Thus, $\ell\left(
t_{i,j}\right)  =\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)
\right\vert =2\left\vert j-i\right\vert -1$. This solves Exercise
\ref{exe.ps4.1ab} \textbf{(a)}.
\end{vershort}

\begin{verlong}
The permutation $t_{i,j}$ switches $i$ with $j$ while leaving all other
elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged. In other words, we
have $t_{i,j}\left(  i\right)  =j$, $t_{i,j}\left(  j\right)  =i$ and%
\begin{equation}
t_{i,j}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{  i,j\right\}  .
\label{sol.ps4.1ab.a.tij}%
\end{equation}


Let%
\begin{align*}
A  &  =\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  \ \ \ \ \ \ \ \ \ \ \text{and}\\
B  &  =\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right\}  .
\end{align*}
These two sets $A$ and $B$ satisfy $\left\vert A\right\vert =j-i$%
\ \ \ \ \footnote{\textit{Proof.} The elements $\left(  i,k\right)  $ for all
$k\in\left\{  i+1,i+2,\ldots,j\right\}  $ are pairwise distinct (because $k$
can be reconstructed from $\left(  i,k\right)  $). Therefore, the number of
these elements (counted without multiplicities) is $j-i$ (since the number of
all $k\in\left\{  i+1,i+2,\ldots,j\right\}  $ is $j-i$). In other words,
$\left\vert \left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  \right\vert =j-i$. Now,%
\[
\left\vert \underbrace{A}_{=\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j\right\}  \right\}  }\right\vert =\left\vert \left\{  \left(
i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\}
\right\vert =j-i,
\]
qed.} and $\left\vert B\right\vert =j-i-1$\ \ \ \ \footnote{\textit{Proof.}
The elements $\left(  k,j\right)  $ for all $k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  $ are pairwise distinct (because $k$ can be reconstructed from
$\left(  k,j\right)  $). Therefore, the number of these elements (counted
without multiplicities) is $j-i-1$ (since the number of all $k\in\left\{
i+1,i+2,\ldots,j-1\right\}  $ is $\left(  j-1\right)  -i=j-i-1$). In other
words, $\left\vert \left\{  \left(  k,j\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j-1\right\}  \right\}  \right\vert =j-i-1$. Now,%
\[
\left\vert \underbrace{B}_{=\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j-1\right\}  \right\}  }\right\vert =\left\vert \left\{
\left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\right\vert =j-i-1,
\]
qed.}. Also, these sets $A$ and $B$ are disjoint\footnote{\textit{Proof.}
Assume the contrary. Then, $A\cap B\neq\varnothing$. Hence, there exists some
element $c$ of the set $A\cap B$. Consider such a $c$. We have%
\[
c\in A\cap B\subseteq A=\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j\right\}  \right\}  .
\]
In other words, we can write $c$ in the form $c=\left(  i,k\right)  $ for some
$k\in\left\{  i+1,i+2,\ldots,j\right\}  $. Let us denote this $k$ by $\ell$.
Then, $\ell$ is an element of $\left\{  i+1,i+2,\ldots,j\right\}  $ and
satisfies $c=\left(  i,\ell\right)  $.
\par
Also, $c\in A\cap B\subseteq B=\left\{  \left(  k,j\right)  \ \mid
\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}  $. Hence, we can write
$c$ in the form $c=\left(  k,j\right)  $ for some $k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  $. Consider this $k$. Now, $c=\left(  k,j\right)  $, so that
$\left(  k,j\right)  =c=\left(  i,\ell\right)  $. Hence, $k=i$ and $j=\ell$.
Thus, $i=k\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, which contradicts
$i\notin\left\{  i+1,i+2,\ldots,j-1\right\}  $. This contradiction shows that
our assumption was wrong, qed.}. Hence,
\[
\left\vert A\cup B\right\vert =\underbrace{\left\vert A\right\vert }%
_{=j-i}+\underbrace{\left\vert B\right\vert }_{=j-i-1}=\left(  j-i\right)
+\left(  j-i-1\right)  =2\underbrace{\left(  j-i\right)  }_{=\left\vert
j-i\right\vert }-1=2\left\vert j-i\right\vert -1.
\]


Now, let $\operatorname*{Inv}\left(  t_{i,j}\right)  $ denote the set of all
inversions of $t_{i,j}$. Then, $\ell\left(  t_{i,j}\right)  =\left\vert
\operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert $%
\ \ \ \ \footnote{\textit{Proof.} Recall that $\ell\left(  t_{i,j}\right)  $
is defined as the number of inversions of $t_{i,j}$. Thus,%
\begin{align*}
\ell\left(  t_{i,j}\right)   &  =\left(  \text{the number of inversions of
}t_{i,j}\right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
t_{i,j}\right)  }_{=\operatorname*{Inv}\left(  t_{i,j}\right)  }\right\vert
=\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert ,
\end{align*}
qed.}.

But $A\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in A$. We shall show that
$c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
We have $c\in A=\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  i,k\right)  $ for some $k\in\left\{  i+1,i+2,\ldots,j\right\}  $.
Consider this $k$. From $k\in\left\{  i+1,i+2,\ldots,j\right\}  $, we obtain
$i<k\leq j$, so that $k\leq j\leq n$ and thus $1\leq i<k\leq n$.
\par
We shall now show that $j>t_{i,j}\left(  k\right)  $. In fact, we must be in
one of the following two cases:
\par
\textit{Case 1:} We have $k=j$.
\par
\textit{Case 2:} We have $k\neq j$.
\par
Let us first consider Case 1. In this case, we have $k=j$. Thus,
$t_{i,j}\left(  \underbrace{k}_{=j}\right)  =t_{i,j}\left(  j\right)  =i$.
Thus, $j>i=t_{i,j}\left(  k\right)  $. Hence, $j>t_{i,j}\left(  k\right)  $ is
proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $k\neq j$. Combined with
$k\neq i$ (since $i<k$), this yields $k\notin\left\{  i,j\right\}  $. Hence,
$t_{i,j}\left(  k\right)  =k$ (by (\ref{sol.ps4.1ab.a.tij})). Also, combining
$k\leq j$ with $k\neq j$, we obtain $k<j$ and thus $j>k=t_{i,j}\left(
k\right)  $. Hence, $j>t_{i,j}\left(  k\right)  $ is proven in Case 2.
\par
Now, we have proven $j>t_{i,j}\left(  k\right)  $ in each of the two Cases 1
and 2. Thus, $j>t_{i,j}\left(  k\right)  $ always holds.
\par
Now, $t_{i,j}\left(  i\right)  =j>t_{i,j}\left(  k\right)  $. Hence, $\left(
i,k\right)  $ is a pair of integers satisfying $1\leq i<k\leq n$ and
$t_{i,j}\left(  i\right)  >t_{i,j}\left(  k\right)  $. In other words,
$\left(  i,k\right)  $ is an inversion of $t_{i,j}$ (by the definition of
\textquotedblleft inversion of $t_{i,j}$\textquotedblright). In other words,
$\left(  i,k\right)  \in\operatorname*{Inv}\left(  t_{i,j}\right)  $ (since
$\operatorname*{Inv}\left(  t_{i,j}\right)  $ is the set of all inversions of
$t_{i,j}$). Thus, $c=\left(  i,k\right)  \in\operatorname*{Inv}\left(
t_{i,j}\right)  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  t_{i,j}\right)  $ for every $c\in A$. In other
words, $A\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $, qed.} and
$B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in B$. We shall show that
$c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
We have $c\in B=\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j-1\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  k,j\right)  $ for some $k\in\left\{  i+1,i+2,\ldots,j-1\right\}  $.
Consider this $k$. From $k\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, we obtain
$i<k<j$, so that $1\leq i<k$ and thus $1\leq k<j\leq n$. Also, combining
$k\neq i$ (since $i<k$) and $k\neq j$ (since $k<j$), we obtain $k\notin%
\left\{  i,j\right\}  $. Hence, $t_{i,j}\left(  k\right)  =k$ (by
(\ref{sol.ps4.1ab.a.tij})). But $t_{i,j}\left(  j\right)  =i<k=t_{i,j}\left(
k\right)  $, so that $t_{i,j}\left(  k\right)  >t_{i,j}\left(  j\right)  $.
\par
Hence, $\left(  k,j\right)  $ is a pair of integers satisfying $1\leq k<j\leq
n$ and $t_{i,j}\left(  k\right)  >t_{i,j}\left(  j\right)  $. In other words,
$\left(  k,j\right)  $ is an inversion of $t_{i,j}$ (by the definition of
\textquotedblleft inversion of $t_{i,j}$\textquotedblright). In other words,
$\left(  k,j\right)  \in\operatorname*{Inv}\left(  t_{i,j}\right)  $ (since
$\operatorname*{Inv}\left(  t_{i,j}\right)  $ is the set of all inversions of
$t_{i,j}$). Thus, $c=\left(  k,j\right)  \in\operatorname*{Inv}\left(
t_{i,j}\right)  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  t_{i,j}\right)  $ for every $c\in B$. In other
words, $B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $, qed.}. Hence,%
\[
\underbrace{A}_{\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  }%
\cup\underbrace{B}_{\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)
}\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  \cup\operatorname*{Inv}%
\left(  t_{i,j}\right)  =\operatorname*{Inv}\left(  t_{i,j}\right)  .
\]


On the other hand, $\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup
B$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
t_{i,j}\right)  $. We shall show that $c\in A\cup B$.
\par
We have $c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $. In other words, $c$
is an inversion of $t_{i,j}$ (since $\operatorname*{Inv}\left(  t_{i,j}%
\right)  $ is the set of all inversions of $t_{i,j}$). In other words, $c$ is
a pair $\left(  u,v\right)  $ of integers satisfying $1\leq u<v\leq n$ and
$t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)  $. Consider this $\left(
u,v\right)  $. Thus, $c=\left(  u,v\right)  $.
\par
We must be in one of the following two cases:
\par
\textit{Case 1:} We have $t_{i,j}\left(  u\right)  >u$.
\par
\textit{Case 2:} We don't have $t_{i,j}\left(  u\right)  >u$.
\par
Let us first consider Case 1. In this case, we have $t_{i,j}\left(  u\right)
>u$. If we had $u\notin\left\{  i,j\right\}  $, then we would have
$t_{i,j}\left(  u\right)  =u$ (by (\ref{sol.ps4.1ab.a.tij}), applied to
$k=u$), which would contradict $t_{i,j}\left(  u\right)  >u$. Hence, we cannot
have $u\notin\left\{  i,j\right\}  $. Thus, we have $u\in\left\{  i,j\right\}
$. If we had $u=j$, then we would have $t_{i,j}\left(  \underbrace{u}%
_{=j}\right)  =t_{i,j}\left(  j\right)  =i<j$, which would contradict
$t_{i,j}\left(  u\right)  >u=j$. Hence, we cannot have $u=j$. We thus have
$u\neq j$. Since $u\in\left\{  i,j\right\}  $ but $u\neq j$, we must have
$u=i$. Hence, $t_{i,j}\left(  \underbrace{u}_{=i}\right)  =t_{i,j}\left(
i\right)  =j$, so that $j=t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)
$.
\par
Now, we assume (for the sake of contradiction) that $v>j$. Combining $v\neq i$
(since $v>j>i$) and $v\neq j$ (since $v>j$), we obtain $v\notin\left\{
i,j\right\}  $, so that $t_{i,j}\left(  v\right)  =v$ (by
(\ref{sol.ps4.1ab.a.tij}), applied to $k=v$). Hence, $j>t_{i,j}\left(
v\right)  =v>j$, which is absurd. Thus, we have obtained a contradiction.
Therefore, our assumption (that $v>j$) was wrong. We thus must have $v\leq j$.
Combined with $i=u<v$, this yields $i<v\leq j$, so that $v\in\left\{
i+1,i+2,\ldots,j\right\}  $. Now,%
\begin{align*}
c  &  =\left(  \underbrace{u}_{=i},v\right)  =\left(  i,v\right)  \in\left\{
\left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\in\left\{  i+1,i+2,\ldots
,j\right\}  \right) \\
&  =A\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 1.
\par
Let us next consider Case 2. In this case, we don't have $t_{i,j}\left(
u\right)  >u$. Hence, we have $t_{i,j}\left(  u\right)  \leq u$, so that
$u\geq t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)  $ and thus
$t_{i,j}\left(  v\right)  <u<v$. If we had $v\notin\left\{  i,j\right\}  $,
then we would have $t_{i,j}\left(  v\right)  =v$ (by (\ref{sol.ps4.1ab.a.tij}%
), applied to $k=v$), which would contradict $t_{i,j}\left(  v\right)  <v$.
Hence, we cannot have $v\notin\left\{  i,j\right\}  $. Thus, we have
$v\in\left\{  i,j\right\}  $. If we had $v=i$, then we would have
$t_{i,j}\left(  \underbrace{v}_{=i}\right)  =t_{i,j}\left(  i\right)  =j>i$,
which would contradict $t_{i,j}\left(  v\right)  <v=i$. Hence, we cannot have
$v=i$. We thus have $v\neq i$. Since $v\in\left\{  i,j\right\}  $ but $v\neq
i$, we must have $v=j$. Hence, $t_{i,j}\left(  \underbrace{v}_{=j}\right)
=t_{i,j}\left(  j\right)  =i$, so that $t_{i,j}\left(  u\right)
>t_{i,j}\left(  v\right)  =i$. Hence, $i<t_{i,j}\left(  u\right)  $.
\par
If we had $u=i$, then we would have $t_{i,j}\left(  \underbrace{u}%
_{=i}\right)  =t_{i,j}\left(  i\right)  =j>i=u$, which would contradict
$t_{i,j}\left(  u\right)  \leq u$. Thus, we cannot have $u=i$. Hence, $u\neq
i$.
\par
Now, we assume (for the sake of contradiction) that $u\leq i$. Combining
$u\neq i$ and $u\neq j$ (since $u\leq i<j$), we obtain $u\notin\left\{
i,j\right\}  $, so that $t_{i,j}\left(  u\right)  =u$ (by
(\ref{sol.ps4.1ab.a.tij}), applied to $k=u$). Hence, $i<t_{i,j}\left(
u\right)  =u\leq i$, which is absurd. Thus, we have obtained a contradiction.
Therefore, our assumption (that $u\leq i$) was wrong. We thus must have $u>i$.
In other words, $i<u$. Combined with $u<v\leq j$, this yields $i<u<j$, so that
$u\in\left\{  i+1,i+2,\ldots,j-1\right\}  $. Now,%
\begin{align*}
c  &  =\left(  u,\underbrace{v}_{=j}\right)  =\left(  u,j\right)  \in\left\{
\left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right) \\
&  =B\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 2.
\par
We therefore have shown that $c\in A\cup B$ in each of the two Cases 1 and 2.
Hence, $c\in A\cup B$ always holds.
\par
Now, let us forget that we fixed $c$. We thus have proven that $c\in A\cup B$
for every $c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $. In other words,
$\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$, qed.}.
Combined with $A\cup B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $,
this yields $A\cup B=\operatorname*{Inv}\left(  t_{i,j}\right)  $. Thus,
$\left\vert A\cup B\right\vert =\left\vert \operatorname*{Inv}\left(
t_{i,j}\right)  \right\vert $. Compared with $\ell\left(  t_{i,j}\right)
=\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert $, this
yields $\ell\left(  t_{i,j}\right)  =\left\vert A\cup B\right\vert
=2\left\vert j-i\right\vert -1$. This proves (\ref{sol.ps4.1ab.a.claim}).
Exercise \ref{exe.ps4.1ab} \textbf{(a)} is thus solved.
\end{verlong}

\textbf{(b)} \textit{First solution to Exercise \ref{exe.ps4.1ab}
\textbf{(b)}:} From (\ref{sol.ps4.1ab.a.claim}), we have $\ell\left(
t_{i,j}\right)  =2\left\vert j-i\right\vert -1$.

But the integer $2\left\vert j-i\right\vert -1$ is odd. Thus, $\left(
-1\right)  ^{2\left\vert j-i\right\vert -1}=-1$. But the definition of
$\left(  -1\right)  ^{t_{i,j}}$ yields%
\begin{align*}
\left(  -1\right)  ^{t_{i,j}}  &  =\left(  -1\right)  ^{\ell\left(
t_{i,j}\right)  }=\left(  -1\right)  ^{2\left\vert j-i\right\vert
-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  t_{i,j}\right)
=2\left\vert j-i\right\vert -1\right) \\
&  =-1.
\end{align*}
This solves Exercise \ref{exe.ps4.1ab} \textbf{(b)}.

\textit{Second solution to Exercise \ref{exe.ps4.1ab} \textbf{(b)}:} Here is
an alternative solution of Exercise \ref{exe.ps4.1ab} \textbf{(b)} which makes
no use of part \textbf{(a)}.

The set $\left\{  1,2,\ldots,n\right\}  $ has at least two distinct elements
(namely, $i$ and $j$). Hence, $n\geq2$.

There exists a permutation $\sigma\in S_{n}$ such that $\left(  i,j\right)
=\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  \right)
$\ \ \ \ \footnote{\textit{Proof.} Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Notice that $n\geq2$ and thus $2\in\left\{
0,1,\ldots,n\right\}  $.
\par
The integers $i$ and $j$ are distinct. Hence, $\left(  i,j\right)  $ is a list
of some elements of $\left[  n\right]  $ such that $i$ and $j$ are distinct.
Therefore, Proposition \ref{prop.perms.lists} \textbf{(c)} (applied to $k=2$
and $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =\left(  i,j\right)  $) yields
that there exists a permutation $\sigma\in S_{n}$ such that $\left(
i,j\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  \right)
$. Qed.}. Consider such a $\sigma$.

We have $\left(  i,j\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  \right)  $, thus $\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  \right)  =\left(  i,j\right)  $. In other words, $\sigma\left(
1\right)  =i$ and $\sigma\left(  2\right)  =j$.

We have $n\geq2$. Hence, the permutation $s_{1}$ in $S_{n}$ is well-defined.
According to its definition, this permutation $s_{1}$ switches $1$ with $2$
but leaves all other numbers unchanged. In other words, we have $s_{1}\left(
1\right)  =2$, $s_{1}\left(  2\right)  =1$, and%
\begin{equation}
s_{1}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{  1,2\right\}  .
\label{sol.ps4.1ab.b.s1}%
\end{equation}


On the other hand, the permutation $t_{i,j}$ switches $i$ with $j$ while
leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged. In
other words, we have $t_{i,j}\left(  i\right)  =j$, $t_{i,j}\left(  j\right)
=i$ and%
\begin{equation}
t_{i,j}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{  i,j\right\}  .
\label{sol.ps4.1ab.b.tij}%
\end{equation}


\begin{vershort}
Now, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots
,n\right\}  $. We need to show that $t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  $.
\par
We are in one of the following three cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $k=2$.
\par
\textit{Case 3:} We have $k\notin\left\{  1,2\right\}  $.
\par
Let us first consider Case 1. In this case, we have $k=1$. Hence,
$t_{i,j}\left(  \sigma\left(  \underbrace{k}_{=1}\right)  \right)
=t_{i,j}\left(  \underbrace{\sigma\left(  1\right)  }_{=i}\right)
=t_{i,j}\left(  i\right)  =j$. Compared with $\sigma\left(  s_{1}\left(
\underbrace{k}_{=1}\right)  \right)  =\sigma\left(  \underbrace{s_{1}\left(
1\right)  }_{=2}\right)  =\sigma\left(  2\right)  =j$, this yields
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(
k\right)  \right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 1.
\par
The proof of $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ in Case 2 is similar and left to the reader.
\par
Let us first consider Case 3. In this case, we have $k\notin\left\{
1,2\right\}  $. Hence, $s_{1}\left(  k\right)  =k$ (by (\ref{sol.ps4.1ab.b.s1}%
)). On the other hand, the map $\sigma$ is a permutation (since $\sigma\in
S_{n}$), thus injective. Now, from $k\neq1$ (since $k\notin\left\{
1,2\right\}  $), we obtain $\sigma\left(  k\right)  \neq\sigma\left(
1\right)  $ (since the map $\sigma$ is injective), so that $\sigma\left(
k\right)  \neq\sigma\left(  1\right)  =i$. Similarly, from $k\neq2$, we can
obtain $\sigma\left(  k\right)  \neq j$. Combining $\sigma\left(  k\right)
\neq i$ with $\sigma\left(  k\right)  \neq j$, we obtain $\sigma\left(
k\right)  \notin\left\{  i,j\right\}  $, and therefore $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  k\right)  $ (by
(\ref{sol.ps4.1ab.b.tij}), applied to $\sigma\left(  k\right)  $ instead of
$k$). Compared with $\sigma\left(  \underbrace{s_{1}\left(  k\right)  }%
_{=k}\right)  =\sigma\left(  k\right)  $, this yields $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 3.
\par
Thus, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ is proven in each of the three Cases 1, 2
and 3. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ always holds, qed.}. Hence, every
$k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $\left(  t_{i,j}\circ
\sigma\right)  \left(  k\right)  =t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  =\left(  \sigma\circ
s_{1}\right)  \left(  k\right)  $. In other words, $t_{i,j}\circ\sigma
=\sigma\circ s_{1}$.
\end{vershort}

\begin{verlong}
Now, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots
,n\right\}  $. We need to show that $t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  $.
\par
We are in one of the following three cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $k=2$.
\par
\textit{Case 3:} We have $k\notin\left\{  1,2\right\}  $.
\par
Let us first consider Case 1. In this case, we have $k=1$. Hence,
$t_{i,j}\left(  \sigma\left(  \underbrace{k}_{=1}\right)  \right)
=t_{i,j}\left(  \underbrace{\sigma\left(  1\right)  }_{=i}\right)
=t_{i,j}\left(  i\right)  =j$. Compared with $\sigma\left(  s_{1}\left(
\underbrace{k}_{=1}\right)  \right)  =\sigma\left(  \underbrace{s_{1}\left(
1\right)  }_{=2}\right)  =\sigma\left(  2\right)  =j$, this yields
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(
k\right)  \right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $k=2$. Hence,
$t_{i,j}\left(  \sigma\left(  \underbrace{k}_{=2}\right)  \right)
=t_{i,j}\left(  \underbrace{\sigma\left(  2\right)  }_{=j}\right)
=t_{i,j}\left(  j\right)  =i$. Compared with $\sigma\left(  s_{1}\left(
\underbrace{k}_{=2}\right)  \right)  =\sigma\left(  \underbrace{s_{1}\left(
2\right)  }_{=1}\right)  =\sigma\left(  1\right)  =i$, this yields
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(
k\right)  \right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 2.
\par
Let us first consider Case 3. In this case, we have $k\notin\left\{
1,2\right\}  $. Hence, $s_{1}\left(  k\right)  =k$ (by (\ref{sol.ps4.1ab.b.s1}%
)). On the other hand, the map $\sigma$ is a permutation (since $\sigma\in
S_{n}$), thus injective. Now, from $k\neq1$ (since $k\notin\left\{
1,2\right\}  $), we obtain $\sigma\left(  k\right)  \neq\sigma\left(
1\right)  $ (since the map $\sigma$ is injective), so that $\sigma\left(
k\right)  \neq\sigma\left(  1\right)  =i$. Also, from $k\neq2$ (since
$k\notin\left\{  1,2\right\}  $), we obtain $\sigma\left(  k\right)
\neq\sigma\left(  2\right)  $ (since the map $\sigma$ is injective), so that
$\sigma\left(  k\right)  \neq\sigma\left(  2\right)  =j$. Combining
$\sigma\left(  k\right)  \neq i$ with $\sigma\left(  k\right)  \neq j$, we
obtain $\sigma\left(  k\right)  \notin\left\{  i,j\right\}  $, and therefore
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  k\right)  $
(by (\ref{sol.ps4.1ab.b.tij}), applied to $\sigma\left(  k\right)  $ instead
of $k$). Compared with $\sigma\left(  \underbrace{s_{1}\left(  k\right)
}_{=k}\right)  =\sigma\left(  k\right)  $, this yields $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 3.
\par
Thus, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ is proven in each of the three Cases 1, 2
and 3. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ always holds, qed.}. Hence, every
$k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $\left(  t_{i,j}\circ
\sigma\right)  \left(  k\right)  =t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  =\left(  \sigma\circ
s_{1}\right)  \left(  k\right)  $. In other words, $t_{i,j}\circ\sigma
=\sigma\circ s_{1}$.
\end{verlong}

Recall that $\left(  -1\right)  ^{s_{k}}=-1$ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. Applied to $k=1$, this yields $\left(  -1\right)
^{s_{1}}=-1$.

On the other hand, (\ref{eq.sign.prod}) (applied to $\tau=s_{1}$) yields
$\left(  -1\right)  ^{\sigma\circ s_{1}}=\left(  -1\right)  ^{\sigma}%
\cdot\underbrace{\left(  -1\right)  ^{s_{1}}}_{=-1}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  $.

But (\ref{eq.sign.prod}) (applied to $t_{i,j}$ and $\sigma$ instead of
$\sigma$ and $\tau$) yields $\left(  -1\right)  ^{t_{i,j}\circ\sigma}=\left(
-1\right)  ^{t_{i,j}}\cdot\left(  -1\right)  ^{\sigma}$, so that%
\begin{align*}
\left(  -1\right)  ^{t_{i,j}}\cdot\left(  -1\right)  ^{\sigma}  &  =\left(
-1\right)  ^{t_{i,j}\circ\sigma}=\left(  -1\right)  ^{\sigma\circ s_{1}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }t_{i,j}\circ\sigma=\sigma\circ
s_{1}\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  .
\end{align*}
We can cancel $\left(  -1\right)  ^{\sigma}$ from this equality (since
$\left(  -1\right)  ^{\sigma}\in\left\{  1,-1\right\}  $ is a nonzero
integer), and thus obtain $\left(  -1\right)  ^{t_{i,j}}=-1$. This solves
Exercise \ref{exe.ps4.1ab} \textbf{(b)} again.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.1c}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.1c}.]The inversions of $w_{0}$ are the
pairs $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$w_{0}\left(  i\right)  >w_{0}\left(  j\right)  $ (because this is how we
defined inversions). Since \textbf{every} pair of integers $\left(
i,j\right)  $ satisfying $1\leq i<j\leq n$ automatically satisfies
$w_{0}\left(  i\right)  >w_{0}\left(  j\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  $ be a pair of
integers satisfying $1\leq i<j\leq n$. The definition of $w_{0}$ yields
$w_{0}\left(  i\right)  =n+1-i$ and $w_{0}\left(  j\right)  =n+1-j$. Hence,
$w_{0}\left(  i\right)  =n+1-\underbrace{i}_{<j}>n+1-j=w_{0}\left(  j\right)
$, qed.}, we can simplify this statement as follows: The inversions of $w_{0}$
are the pairs $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$.
But the number of such pairs is $n\left(  n-1\right)  /2$%
\ \ \ \ \footnote{There are two ways to prove this:
\par
\begin{itemize}
\item Either we can argue that these pairs are in a one-to-one correspondence
with the $2$-element subsets of $\left\{  1,2,\ldots,n\right\}  $. (Namely,
any pair $\left(  i,j\right)  $ corresponds to the subset $\left\{
i,j\right\}  $, and conversely, any subset $S$ corresponds to the pair
$\left(  \min S,\max S\right)  $.) Therefore, the number of such pairs equals
the number of all $2$-element subsets of $\left\{  1,2,\ldots,n\right\}  $;
but the latter number is known to be $\dbinom{n}{2}=n\left(  n-1\right)  /2$.
\par
\item Alternatively, we can compute this number as follows: For every pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$, we have
$j\in\left\{  2,3,\ldots,n\right\}  $ (since $1<j\leq n$). Hence,
\begin{align*}
&  \left(  \text{the number of pairs }\left(  i,j\right)  \text{ of integers
satisfying }1\leq i<j\leq n\right) \\
&  =\sum_{k=2}^{n}\underbrace{\left(  \text{the number of pairs } \left(  i,
j\right)  \text{ satisfying }1\leq i<j\leq n\text{ and } j=k\right)
}_{=\left(  \text{the number of integers }i\text{ satisfying }1\leq
i<k\right)  = k-1 }\\
&  =\sum_{k=2}^{n}\left(  k-1\right)  =\sum_{j=1}^{n-1}%
j\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }j\text{ for
}k-1\text{ in the sum}\right) \\
&  = 1+2+\cdots+\left(  n-1\right)  =\left(  n-1\right)  n/2\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by a famous formula commonly ascribed to
Gauss}\right) \\
&  =n\left(  n-1\right)  /2.
\end{align*}
\end{itemize}
}. Thus, the number of inversions of $w_{0}$ is $n\left(  n-1\right)  /2$. In
other words, $\ell\left(  w_{0}\right)  =n\left(  n-1\right)  /2$ (since
$\ell\left(  w_{0}\right)  $ is defined as the number of inversions of $w_{0}%
$). Therefore, the definition of $\left(  -1\right)  ^{w_{0}}$ yields $\left(
-1\right)  ^{w_{0}}=\left(  -1\right)  ^{\ell\left(  w_{0}\right)  }=\left(
-1\right)  ^{n\left(  n-1\right)  /2}$ (since $\ell\left(  w_{0}\right)
=n\left(  n-1\right)  /2$).

At this point, we could declare Exercise \ref{exe.ps4.1c} to be solved, since
we have found formulas for both $\ell\left(  w_{0}\right)  $ and $\left(
-1\right)  ^{w_{0}}$. Nevertheless, let us give a different expression for
$\left(  -1\right)  ^{w_{0}}$, which can be evaluated faster. Namely, we claim
that%
\begin{equation}
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  . \label{sol.ps4.1c.short.extra}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.1c.short.extra}):} We must be in one of the
following four cases:

\textit{Case 1:} We have $n\equiv0\operatorname{mod}4$.

\textit{Case 2:} We have $n\equiv1\operatorname{mod}4$.

\textit{Case 3:} We have $n\equiv2\operatorname{mod}4$.

\textit{Case 4:} We have $n\equiv3\operatorname{mod}4$.

The proofs of (\ref{sol.ps4.1c.short.extra}) in these four cases are more or
less analogous. Let us only show the proof in Case 4. In this case, we have
$n\equiv3\operatorname{mod}4$. Thus, $n=4m+3$ for some $m\in\mathbb{Z}$.
Consider this $m$. We have%
\begin{align*}
\underbrace{n}_{=4m+3}\underbrace{\left(  n-1\right)  }%
_{\substack{=4m+2\\\text{(since }n=4m+3\text{)}}}/2  &  =\left(  4m+3\right)
\underbrace{\left(  4m+2\right)  /2}_{=2m+1}=\left(  4m+3\right)  \left(
2m+1\right) \\
&  =8m^{2}+10m+3=2\left(  4m^{2}+5m+1\right)  +1.
\end{align*}
Thus, the integer $n\left(  n-1\right)  /2$ is odd, so that $\left(
-1\right)  ^{n\left(  n-1\right)  /2}=-1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=-1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =-1$ (since $n\equiv3\operatorname{mod}4$), this yields $\left(
-1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  $. Thus, (\ref{sol.ps4.1c.short.extra}) is proven in Case 4. The
other three cases are analogous, and so we conclude that
(\ref{sol.ps4.1c.short.extra}) holds.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.1c}.]Let $G=\left\{  \left(  i,j\right)
\in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  $. Then, $\left\vert
G\right\vert =n\left(  n-1\right)  /2$\ \ \ \ \footnote{\textit{Proof.} This
is fairly obvious, but let us nevertheless give a proof for the sake of
completeness.
\par
It is well-known that $\sum_{i=1}^{m}i=m\left(  m+1\right)  /2$ for every
$m\in\mathbb{N}$. Applying this to $m=n-1$, we obtain $\sum_{i=1}%
^{n-1}i=\left(  n-1\right)  \underbrace{\left(  \left(  n-1\right)  +1\right)
}_{=n}/2=\left(  n-1\right)  n/2=n\left(  n-1\right)  /2$.
\par
We have%
\[
\sum_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\1\leq i<j\leq
n}}1=\left\vert \underbrace{\left\{  \left(  i,j\right)  \in\mathbb{Z}%
^{2}\ \mid\ 1\leq i<j\leq n\right\}  }_{=G}\right\vert \cdot1=\left\vert
G\right\vert \cdot1=\left\vert G\right\vert ,
\]
so that%
\begin{align*}
\left\vert G\right\vert  &  =\underbrace{\sum_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\1\leq i<j\leq n}}}_{\substack{=\sum_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\1\leq i\leq n;\ i<j\leq n}}\\\text{(since the
condition }1\leq i<j\leq n\\\text{is equivalent to }\left(  1\leq i\leq
n\text{ and }i<j\leq n\right)  \text{)}}}1=\underbrace{\sum_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\1\leq i\leq n;\ i<j\leq n}}}_{=\sum
_{\substack{i\in\mathbb{Z};\\1\leq i\leq n}}\sum_{\substack{j\in
\mathbb{Z};\\i<j\leq n}}}1\\
&  =\sum_{\substack{i\in\mathbb{Z};\\1\leq i\leq n}}\underbrace{\sum
_{\substack{j\in\mathbb{Z};\\i<j\leq n}}1}_{=\left\vert \left\{
j\in\mathbb{Z}\ \mid\ i<j\leq n\right\}  \right\vert }=\sum_{\substack{i\in
\mathbb{Z};\\1\leq i\leq n}}\left\vert \underbrace{\left\{  j\in
\mathbb{Z}\ \mid\ i<j\leq n\right\}  }_{=\left\{  i+1,i+2,\ldots,n\right\}
}\right\vert \\
&  =\underbrace{\sum_{\substack{i\in\mathbb{Z};\\1\leq i\leq n}}}_{=\sum
_{i=1}^{n}}\underbrace{\left\vert \left\{  i+1,i+2,\ldots,n\right\}
\right\vert }_{\substack{=n-i\\\text{(since }i\leq n\text{)}}}=\sum_{i=1}%
^{n}\left(  n-i\right)  =\sum_{i=0}^{n-1}i\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}n-i\text{ in the sum}\right) \\
&  =0+\sum_{i=1}^{n-1}i=\sum_{i=1}^{n-1}i=n\left(  n-1\right)  /2,
\end{align*}
qed.}.

Now, let $\operatorname*{Inv}\left(  w_{0}\right)  $ denote the set of all
inversions of $w_{0}$. Then, $\ell\left(  w_{0}\right)  =\left\vert
\operatorname*{Inv}\left(  w_{0}\right)  \right\vert $%
\ \ \ \ \footnote{\textit{Proof.} Recall that $\ell\left(  w_{0}\right)  $ is
defined as the number of inversions of $w_{0}$. Thus,%
\begin{align*}
\ell\left(  w_{0}\right)   &  =\left(  \text{the number of inversions of
}w_{0}\right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
w_{0}\right)  }_{=\operatorname*{Inv}\left(  w_{0}\right)  }\right\vert
=\left\vert \operatorname*{Inv}\left(  w_{0}\right)  \right\vert ,
\end{align*}
qed.}.

On the other hand, $\operatorname*{Inv}\left(  w_{0}\right)  \subseteq
G$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
w_{0}\right)  $. We shall show that $c\in G$.
\par
We have $c\in\operatorname*{Inv}\left(  w_{0}\right)  $. In other words, $c$
is an inversion of $w_{0}$ (since $\operatorname*{Inv}\left(  w_{0}\right)  $
is the set of all inversions of $w_{0}$). In other words, $c$ is a pair
$\left(  u,v\right)  $ of integers satisfying $1\leq u<v\leq n$ and
$w_{0}\left(  u\right)  >w_{0}\left(  v\right)  $. Consider this $\left(
u,v\right)  $. We have $c=\left(  u,v\right)  $ and $1\leq u<v\leq n$. Thus,
$c$ has the form $c=\left(  i,j\right)  $ for a pair $\left(  i,j\right)
\in\mathbb{Z}^{2}$ satisfying $1\leq i<j\leq n$ (namely, $\left(  i,j\right)
=\left(  u,v\right)  $). In other words, $c\in\left\{  \left(  i,j\right)
\in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  =G$.
\par
Let us now forget that we fixed $c$. We thus have shown that $c\in G$ for
every $c\in\operatorname*{Inv}\left(  w_{0}\right)  $. In other words,
$\operatorname*{Inv}\left(  w_{0}\right)  \subseteq G$, qed.} and
$G\subseteq\operatorname*{Inv}\left(  w_{0}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in G$. We shall prove that
$c\in\operatorname*{Inv}\left(  w_{0}\right)  $.
\par
We have $c\in G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq
i<j\leq n\right\}  $. In other words, $c$ can be written in the form
$c=\left(  i,j\right)  $ for some $\left(  i,j\right)  \in\mathbb{Z}^{2}$
satisfying $1\leq i<j\leq n$. Consider this $\left(  i,j\right)  $.
\par
The definition of $w_{0}$ yields $w_{0}\left(  j\right)  =n+1-j$ and
$w_{0}\left(  i\right)  =n+1-\underbrace{i}_{<j}>n+1-j=w_{0}\left(  j\right)
$. Hence, $\left(  i,j\right)  $ is a pair of integers satisfying $1\leq
i<j\leq n$ and $w_{0}\left(  i\right)  >w_{0}\left(  j\right)  $. In other
words, $\left(  i,j\right)  $ is an inversion of $w_{0}$ (by the definition of
an \textquotedblleft inversion of $w_{0}$\textquotedblright). In other words,
$\left(  i,j\right)  \in\operatorname*{Inv}\left(  w_{0}\right)  $ (since
$\operatorname*{Inv}\left(  w_{0}\right)  $ is the set of all inversions of
$w_{0}$). Thus, $c=\left(  i,j\right)  \in\operatorname*{Inv}\left(
w_{0}\right)  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  w_{0}\right)  $ for every $c\in G$. In other words,
$G\subseteq\operatorname*{Inv}\left(  w_{0}\right)  $, qed.}. Combining these
two relations, we obtain $\operatorname*{Inv}\left(  w_{0}\right)  =G$. Thus,
$\left\vert \underbrace{\operatorname*{Inv}\left(  w_{0}\right)  }%
_{=G}\right\vert =\left\vert G\right\vert =n\left(  n-1\right)  /2$, so that
$\ell\left(  w_{0}\right)  =\left\vert \operatorname*{Inv}\left(
w_{0}\right)  \right\vert =n\left(  n-1\right)  /2$.

Now, the definition of $\left(  -1\right)  ^{w_{0}}$ yields $\left(
-1\right)  ^{w_{0}}=\left(  -1\right)  ^{\ell\left(  w_{0}\right)  }=\left(
-1\right)  ^{n\left(  n-1\right)  /2}$ (since $\ell\left(  w_{0}\right)
=n\left(  n-1\right)  /2$).

At this point, we could declare Exercise \ref{exe.ps4.1c} to be solved, since
we have found formulas for both $\ell\left(  w_{0}\right)  $ and $\left(
-1\right)  ^{w_{0}}$. Nevertheless, let us give a different expression for
$\left(  -1\right)  ^{w_{0}}$, which can be evaluated faster. Namely, we claim
that%
\begin{equation}
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  . \label{sol.ps4.1c.extra}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.1c.extra}):} We must be in one of the following
four cases:

\textit{Case 1:} We have $n\equiv0\operatorname{mod}4$.

\textit{Case 2:} We have $n\equiv1\operatorname{mod}4$.

\textit{Case 3:} We have $n\equiv2\operatorname{mod}4$.

\textit{Case 4:} We have $n\equiv3\operatorname{mod}4$.

Let us first consider Case 1. In this case, we have $n\equiv
0\operatorname{mod}4$. Thus, $n=4m$ for some $m\in\mathbb{Z}$. Consider this
$m$. We have $\underbrace{n}_{=4m}\left(  n-1\right)  /2=4m\left(  n-1\right)
/2=2m\left(  n-1\right)  $. Thus, the integer $n\left(  n-1\right)  /2$ is
even, so that $\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =1$ (since $n\equiv0\operatorname{mod}4$ or $n\equiv
1\operatorname{mod}4$ (since $n\equiv0\operatorname{mod}4$)), this yields
$\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  $. Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $n\equiv1\operatorname{mod}%
4$. Thus, $n=4m+1$ for some $m\in\mathbb{Z}$. Consider this $m$. We have
$n\underbrace{\left(  n-1\right)  }_{\substack{=4m\\\text{(since
}n=4m+1\text{)}}}/2=n\cdot4m/2=2nm$. Thus, the integer $n\left(  n-1\right)
/2$ is even, so that $\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =1$ (since $n\equiv0\operatorname{mod}4$ or $n\equiv
1\operatorname{mod}4$ (since $n\equiv1\operatorname{mod}4$)), this yields
$\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  $. Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 2.

Let us next consider Case 3. In this case, we have $n\equiv2\operatorname{mod}%
4$. Thus, $n=4m+2$ for some $m\in\mathbb{Z}$. Consider this $m$. We have%
\begin{align*}
\underbrace{n}_{\substack{=4m+2\\=2\left(  2m+1\right)  }}\underbrace{\left(
n-1\right)  }_{\substack{=4m+1\\\text{(since }n=4m+2\text{)}}}/2  &  =2\left(
2m+1\right)  \left(  4m+1\right)  /2=\left(  2m+1\right)  \left(  4m+1\right)
\\
&  =8m^{2}+6m+1=2\left(  4m^{2}+3m\right)  +1.
\end{align*}
Thus, the integer $n\left(  n-1\right)  /2$ is odd, so that $\left(
-1\right)  ^{n\left(  n-1\right)  /2}=-1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=-1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =-1$ (since $n\equiv2\operatorname{mod}4$ or $n\equiv
3\operatorname{mod}4$ (since $n\equiv2\operatorname{mod}4$)), this yields
$\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  $. Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 3.

Let us finally consider Case 4. In this case, we have $n\equiv
3\operatorname{mod}4$. Thus, $n=4m+3$ for some $m\in\mathbb{Z}$. Consider this
$m$. We have%
\begin{align*}
\underbrace{n}_{=4m+3}\underbrace{\left(  n-1\right)  }%
_{\substack{=4m+2\\\text{(since }n=4m+3\text{)}}}/2  &  =\left(  4m+3\right)
\underbrace{\left(  4m+2\right)  /2}_{=2m+1}=\left(  4m+3\right)  \left(
2m+1\right) \\
&  =8m^{2}+10m+3=2\left(  4m^{2}+5m+1\right)  +1.
\end{align*}
Thus, the integer $n\left(  n-1\right)  /2$ is odd, so that $\left(
-1\right)  ^{n\left(  n-1\right)  /2}=-1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=-1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =-1$ (since $n\equiv2\operatorname{mod}4$ or $n\equiv
3\operatorname{mod}4$ (since $n\equiv3\operatorname{mod}4$)), this yields
$\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  $. Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 4.

We now have proven (\ref{sol.ps4.1c.extra}) in each of the four Cases 1, 2, 3
and 4. Thus, (\ref{sol.ps4.1c.extra}) always holds. This finishes our solution
of Exercise \ref{exe.ps4.1c}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.ps4.2}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.2}.]\textbf{(a)} Let $\sigma$ be a
permutation of $X$. We need to prove that $\left(  -1\right)  _{\phi}^{\sigma
}$ depends only on the permutation $\sigma$ of $X$, but not on the bijection
$\phi$. In other words, we need to prove that any two different choices of
$\phi$ will lead to the same $\left(  -1\right)  _{\phi}^{\sigma}$. In other
words, we need to prove that if $\phi_{1}$ and $\phi_{2}$ are two bijections
$\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$
(possibly distinct), then $\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(
-1\right)  _{\phi_{2}}^{\sigma}$.

So let $\phi_{1}$ and $\phi_{2}$ be two bijections $\phi:X\rightarrow\left\{
1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$ (possibly distinct). We must
show that $\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(  -1\right)
_{\phi_{2}}^{\sigma}$.

The map $\sigma$ is a permutation of $X$, thus a bijection $X\rightarrow X$.

\begin{vershort}
We know that $\phi_{1}$ is a bijection $X\rightarrow\left\{  1,2,\ldots
,n\right\}  $ for some $n\in\mathbb{N}$. In other words, there exist some
$n\in\mathbb{N}$ such that $\phi_{1}$ is a bijection $X\rightarrow\left\{
1,2,\ldots,n\right\}  $. Denote this $n$ by $n_{1}$. Thus, $\phi_{1}$ is a
bijection $X\rightarrow\left\{  1,2,\ldots,n_{1}\right\}  $. The definition of
$\left(  -1\right)  _{\phi_{1}}^{\sigma}$ yields $\left(  -1\right)
_{\phi_{1}}^{\sigma}=\left(  -1\right)  ^{\phi_{1}\circ\sigma\circ\phi
_{1}^{-1}}$.
\end{vershort}

\begin{verlong}
We know that $\phi_{1}$ is a bijection $\phi:X\rightarrow\left\{
1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$. In other words, $\phi_{1}$
is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. In other words, there exist some $n\in\mathbb{N}$ such that
$\phi_{1}$ is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $.
Denote this $n$ by $n_{1}$. Thus, $\phi_{1}$ is a bijection $X\rightarrow
\left\{  1,2,\ldots,n_{1}\right\}  $. The definition of $\left(  -1\right)
_{\phi_{1}}^{\sigma}$ yields $\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(
-1\right)  ^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}$.
\end{verlong}

The map $\phi_{1}$ is a bijection. Thus, its inverse $\phi_{1}^{-1}$ is
well-defined and also a bijection.

The map $\phi_{1}\circ\sigma\circ\phi_{1}^{-1}:\left\{  1,2,\ldots
,n_{1}\right\}  \rightarrow\left\{  1,2,\ldots,n_{1}\right\}  $ is a bijection
(since it is a composition of the three bijections $\phi_{1}$, $\sigma$ and
$\phi_{1}^{-1}$). In other words, the map $\phi_{1}\circ\sigma\circ\phi
_{1}^{-1}$ is a permutation of $\left\{  1,2,\ldots,n_{1}\right\}  $. Thus,
$\phi_{1}\circ\sigma\circ\phi_{1}^{-1}\in S_{n_{1}}$.

\begin{vershort}
We thus have shown that $\phi_{1}^{-1}$ is well-defined and a bijection, and
found an $n_{1}\in\mathbb{N}$ such that $\phi_{1}\circ\sigma\circ\phi_{1}%
^{-1}\in S_{n_{1}}$. Similarly, we can show that $\phi_{2}^{-1}$ is
well-defined and a bijection, and find an $n_{2}\in\mathbb{N}$ such that
$\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\in S_{n_{2}}$. Consider this $n_{2}$.
(We shall soon see that $n_{1}=n_{2}$.) We have $\left(  -1\right)  _{\phi
_{2}}^{\sigma}=\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}}$
(by the definition of $\left(  -1\right)  _{\phi_{2}}^{\sigma}$).
\end{vershort}

\begin{verlong}
We know that $\phi_{2}$ is a bijection $\phi:X\rightarrow\left\{
1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$. In other words, $\phi_{2}$
is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. In other words, there exist some $n\in\mathbb{N}$ such that
$\phi_{2}$ is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $.
Denote this $n$ by $n_{2}$. Thus, $\phi_{2}$ is a bijection $X\rightarrow
\left\{  1,2,\ldots,n_{2}\right\}  $. The definition of $\left(  -1\right)
_{\phi_{2}}^{\sigma}$ yields $\left(  -1\right)  _{\phi_{2}}^{\sigma}=\left(
-1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}}$. (We will soon see that
$n_{1}=n_{2}$.)

The map $\phi_{2}$ is a bijection. Thus, its inverse $\phi_{2}^{-1}$ is
well-defined and also a bijection.

The map $\phi_{2}\circ\sigma\circ\phi_{2}^{-1}:\left\{  1,2,\ldots
,n_{2}\right\}  \rightarrow\left\{  1,2,\ldots,n_{2}\right\}  $ is a bijection
(since it is a composition of the three bijections $\phi_{2}$, $\sigma$ and
$\phi_{2}^{-1}$). In other words, the map $\phi_{2}\circ\sigma\circ\phi
_{2}^{-1}$ is a permutation of $\left\{  1,2,\ldots,n_{2}\right\}  $. Thus,
$\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\in S_{n_{2}}$.
\end{verlong}

There exists a bijection from $X$ to $\left\{  1,2,\ldots,n_{1}\right\}  $
(namely, $\phi_{1}$). Hence, $\left\vert X\right\vert =\left\vert \left\{
1,2,\ldots,n_{1}\right\}  \right\vert =n_{1}$. Similarly, $\left\vert
X\right\vert =n_{2}$. Thus, $n_{1}=\left\vert X\right\vert =n_{2}$. Thus, we
can define an $n\in\mathbb{N}$ by $n=n_{1}=n_{2}$. Consider this $n$.

The map $\phi_{2}\circ\phi_{1}^{-1}:\left\{  1,2,\ldots,n_{1}\right\}
\rightarrow\left\{  1,2,\ldots,n_{2}\right\}  $ is a bijection (since it is a
composition of two bijections). Since $n_{1}=n$ and $n_{2}=n$, this rewrites
as follows: The map $\phi_{2}\circ\phi_{1}^{-1}:\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}  $ is a bijection. In other words,
the map $\phi_{2}\circ\phi_{1}^{-1}$ is a permutation of $\left\{
1,2,\ldots,n\right\}  $. Thus, $\phi_{2}\circ\phi_{1}^{-1}\in S_{n}$.

We have $\phi_{1}\circ\sigma\circ\phi_{1}^{-1}\in S_{n_{1}}=S_{n}$ (since
$n_{1}=n$) and $\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\in S_{n_{2}}=S_{n}$
(since $n_{2}=n$).

Now, (\ref{eq.sign.prod}) (applied to $\phi_{2}\circ\phi_{1}^{-1}$ and
$\phi_{1}\circ\sigma\circ\phi_{1}^{-1}$ instead of $\sigma$ and $\tau$) yields%
\[
\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}\circ\phi_{1}\circ\sigma
\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}%
\cdot\left(  -1\right)  ^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}.
\]
Since $\phi_{2}\circ\underbrace{\phi_{1}^{-1}\circ\phi_{1}}%
_{=\operatorname*{id}}\circ\sigma\circ\phi_{1}^{-1}=\phi_{2}\circ\sigma
\circ\phi_{1}^{-1}$, this rewrites as
\begin{equation}
\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)
^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{1}\circ
\sigma\circ\phi_{1}^{-1}}. \label{sol.ps4.2.1}%
\end{equation}


On the other hand, (\ref{eq.sign.prod}) (applied to $\phi_{2}\circ\sigma
\circ\phi_{2}^{-1}$ and $\phi_{2}\circ\phi_{1}^{-1}$ instead of $\sigma$ and
$\tau$) yields%
\[
\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\circ\phi_{2}%
\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}%
^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}=\left(  -1\right)
^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ
\sigma\circ\phi_{2}^{-1}}.
\]
Since $\phi_{2}\circ\sigma\circ\underbrace{\phi_{2}^{-1}\circ\phi_{2}%
}_{=\operatorname*{id}}\circ\phi_{1}^{-1}=\phi_{2}\circ\sigma\circ\phi
_{1}^{-1}$, this rewrites as
\[
\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)
^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ
\sigma\circ\phi_{2}^{-1}}.
\]
Comparing this with (\ref{sol.ps4.2.1}), we obtain
\[
\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)
^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}%
\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi
_{2}^{-1}}.
\]
We can cancel $\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}$ from this
equality (since $\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}\in\left\{
1,-1\right\}  $ is a nonzero integer), and thus obtain $\left(  -1\right)
^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}%
\circ\sigma\circ\phi_{2}^{-1}}$. Hence, $\left(  -1\right)  _{\phi_{1}%
}^{\sigma}=\left(  -1\right)  ^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}=\left(
-1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}}=\left(  -1\right)
_{\phi_{2}}^{\sigma}$. As we know, this completes the solution of Exercise
\ref{exe.ps4.2} \textbf{(a)}.

\textbf{(b)} Fix a bijection $\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}
$ for some $n\in\mathbb{N}$. (Such a bijection always exists.) We shall denote
the identity permutation of $X$ by $\operatorname*{id}\nolimits_{X}$, so as to
distinguish it from the identity permutation of $\left\{  1,2,\ldots
,n\right\}  $ (which we keep denoting by $\operatorname*{id}$ without a
subscript). The definition of $\left(  -1\right)  ^{\operatorname*{id}%
\nolimits_{X}}$ now yields
\begin{align*}
\left(  -1\right)  ^{\operatorname*{id}\nolimits_{X}}  &  =\left(  -1\right)
_{\phi}^{\operatorname*{id}\nolimits_{X}}=\left(  -1\right)  ^{\phi
\circ\operatorname*{id}\nolimits_{X}\circ\phi^{-1}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\left(  -1\right)  _{\phi}^{\operatorname*{id}%
\nolimits_{X}}\right) \\
&  =\left(  -1\right)  ^{\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\phi\circ\operatorname*{id}\nolimits_{X}\circ\phi^{-1}=\phi
\circ\phi^{-1}=\operatorname*{id}\right) \\
&  =1.
\end{align*}
In other words, $\left(  -1\right)  ^{\operatorname*{id}}=1$ for the identity
permutation $\operatorname*{id}:X\rightarrow X$ of $X$. Exercise
\ref{exe.ps4.2} \textbf{(b)} is thus solved.

\textbf{(c)} Let $\sigma$ and $\tau$ be two permutations of $X$. Fix a
bijection $\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. (Such a bijection always exists.) The definition of $\left(
-1\right)  ^{\sigma}$ yields%
\begin{equation}
\left(  -1\right)  ^{\sigma}=\left(  -1\right)  _{\phi}^{\sigma}=\left(
-1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\left(  -1\right)  _{\phi}^{\sigma}\right)  .
\label{sol.ps4.2.c.1}%
\end{equation}
The definition of $\left(  -1\right)  ^{\tau}$ yields%
\begin{equation}
\left(  -1\right)  ^{\tau}=\left(  -1\right)  _{\phi}^{\tau}=\left(
-1\right)  ^{\phi\circ\tau\circ\phi^{-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\left(  -1\right)  _{\phi}^{\tau}\right)  .
\label{sol.ps4.2.c.2}%
\end{equation}
The maps $\phi\circ\sigma\circ\phi^{-1}$ and $\phi\circ\tau\circ\phi^{-1}$ are
permutations in $S_{n}$. Therefore, (\ref{eq.sign.prod}) (applied to
$\phi\circ\sigma\circ\phi^{-1}$ and $\phi\circ\tau\circ\phi^{-1}$ instead of
$\sigma$ and $\tau$) yields%
\[
\left(  -1\right)  ^{\phi\circ\sigma\circ\phi^{-1}\circ\phi\circ\tau\circ
\phi^{-1}}=\left(  -1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}\cdot\left(
-1\right)  ^{\phi\circ\tau\circ\phi^{-1}}.
\]
Since $\phi\circ\sigma\circ\underbrace{\phi^{-1}\circ\phi}%
_{=\operatorname*{id}}\circ\tau\circ\phi^{-1}=\phi\circ\sigma\circ\tau
\circ\phi^{-1}$, this rewrites as
\[
\left(  -1\right)  ^{\phi\circ\sigma\circ\tau\circ\phi^{-1}}=\left(
-1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}\cdot\left(  -1\right)  ^{\phi
\circ\tau\circ\phi^{-1}}.
\]
But the definition of $\left(  -1\right)  ^{\sigma\circ\tau}$ yields%
\begin{align*}
\left(  -1\right)  ^{\sigma\circ\tau}  &  =\left(  -1\right)  _{\phi}%
^{\sigma\circ\tau}=\left(  -1\right)  ^{\phi\circ\sigma\circ\tau\circ\phi
^{-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(
-1\right)  _{\phi}^{\sigma\circ\tau}\right) \\
&  =\underbrace{\left(  -1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}%
}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by (\ref{sol.ps4.2.c.1}))}%
}}\cdot\underbrace{\left(  -1\right)  ^{\phi\circ\tau\circ\phi^{-1}}%
}_{\substack{=\left(  -1\right)  ^{\tau}\\\text{(by (\ref{sol.ps4.2.c.2}))}%
}}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}.
\end{align*}
This solves Exercise \ref{exe.ps4.2} \textbf{(c)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.perm.sign.pseudoexplicit}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.perm.sign.pseudoexplicit}.]\textbf{(b)} We
start with some trivia on sets and subsets.

If $A$ is any set, then $\mathcal{P}_{2}\left(  A\right)  $ shall denote the
set of all $2$-element subsets of $A$. In other words, $\mathcal{P}_{2}\left(
A\right)  $ is defined to be $\left\{  S\subseteq A\ \mid\ \left\vert
S\right\vert =2\right\}  $. For instance,
\begin{align*}
\mathcal{P}_{2}\left(  \left\{  3,6,7\right\}  \right)   &  =\left\{  \left\{
3,6\right\}  ,\left\{  3,7\right\}  ,\left\{  6,7\right\}  \right\}  ;\\
\mathcal{P}_{2}\left(  \left\{  1,2,3,4\right\}  \right)   &  =\left\{
\left\{  1,2\right\}  ,\left\{  1,3\right\}  ,\left\{  1,4\right\}  ,\left\{
2,3\right\}  ,\left\{  2,4\right\}  ,\left\{  3,4\right\}  \right\}  ;\\
\mathcal{P}_{2}\left(  \left\{  3\right\}  \right)   &  =\varnothing;\\
\mathcal{P}_{2}\left(  \varnothing\right)   &  =\varnothing.
\end{align*}
\footnote{Several authors write $\dbinom{A}{2}$ for $\mathcal{P}_{2}\left(
A\right)  $. This notation looks like a binomial coefficient, but with $A$ at
the top. Of course, this notation is chosen for its suggestiveness: When $A$
is finite, the set $\dbinom{A}{2}$ satisfies $\left\vert \dbinom{A}%
{2}\right\vert =\dbinom{\left\vert A\right\vert }{2}$.}

If $A$ and $B$ are two sets, and if $f:A\rightarrow B$ is an injective map,
then we can define a map $f_{\ast}:\mathcal{P}_{2}\left(  A\right)
\rightarrow\mathcal{P}_{2}\left(  B\right)  $ by%
\[
\left(  f_{\ast}\left(  S\right)  =f\left(  S\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }S\in\mathcal{P}_{2}\left(  A\right)
\right)  .
\]
\footnote{At this point, we need to check that this map $f_{\ast}$ is
well-defined. Before I do this, let me rewrite the definition of $f_{\ast}$ in
a more intuitive way: An element of $\mathcal{P}_{2}\left(  A\right)  $ is a
$2$-element subset $\left\{  a,a^{\prime}\right\}  $ of $A$. The map $f_{\ast
}$ takes this subset to $\left\{  f\left(  a\right)  ,f\left(  a^{\prime
}\right)  \right\}  $ (in other words, it applies $f$ to each of its
elements).
\par
So why is the map $f_{\ast}$ well-defined? It is supposed to send every
$S\in\mathcal{P}_{2}\left(  A\right)  $ to $f\left(  S\right)  $. Thus, in
order to prove that it is well-defined, we need to show that $f\left(
S\right)  \in\mathcal{P}_{2}\left(  B\right)  $ for every $S\in\mathcal{P}%
_{2}\left(  A\right)  $.
\par
Let $S\in\mathcal{P}_{2}\left(  A\right)  $. Thus, the set $S$ is a
$2$-element subset of $A$. The map $f$ sends its two elements to two
\textbf{distinct} elements of $B$ (they are distinct because $f$ is
injective). In other words, the set $f\left(  S\right)  $ has $2$ elements.
Thus, $f\left(  S\right)  $ is a $2$-element subset of $B$; in other words,
$f\left(  S\right)  \in\mathcal{P}_{2}\left(  B\right)  $. This proves that
the map $f_{\ast}$ is well-defined.
\par
Notice that we have used the injectivity of $f$ in this argument.} This
construction has the following three basic properties:

\begin{enumerate}
\item If $A$ is a set, then $\left(  \operatorname*{id}\nolimits_{A}\right)
_{\ast}=\operatorname*{id}\nolimits_{\mathcal{P}_{2}\left(  A\right)  }$.

\item If $A$, $B$ and $C$ are three sets, and if $f:A\rightarrow B$ and
$g:B\rightarrow C$ are two injective maps, then $\left(  g\circ f\right)
_{\ast}=g_{\ast}\circ f_{\ast}$. (Of course, $g\circ f$ is injective here, so
$\left(  g\circ f\right)  _{\ast}$ makes sense.)

\item If $A$ and $B$ are two sets, and if $f:A\rightarrow B$ is an invertible
map, then $f_{\ast}$ is invertible as well and satisfies $\left(
f^{-1}\right)  _{\ast}=\left(  f_{\ast}\right)  ^{-1}$.
\end{enumerate}

(The first of these three properties is obvious; the second follows by
observing that $\left(  g\circ f\right)  \left(  S\right)  =g\left(  f\left(
S\right)  \right)  $ for every $S\in\mathcal{P}_{2}\left(  A\right)  $; the
third can be proven using the second or directly.)

Let $\left[  n\right]  $ be the set $\left\{  1,2,\ldots,n\right\}  $. Recall
that $S_{n}$ is the set of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $. In other words, $S_{n}$ is the set of all
permutations of the set $\left[  n\right]  $ (since $\left\{  1,2,\ldots
,n\right\}  =\left[  n\right]  $).

We have $\sigma\in S_{n}$. In other words, $\sigma$ is a permutation of the
set $\left[  n\right]  $ (since $S_{n}$ is the set of all such permutations).
Thus, $\sigma$ is a bijective map $\left[  n\right]  \rightarrow\left[
n\right]  $. In particular, the map $\sigma_{\ast}:\mathcal{P}_{2}\left(
\left[  n\right]  \right)  \rightarrow\mathcal{P}_{2}\left(  \left[  n\right]
\right)  $ is well-defined.

Recall that if $A$ and $B$ are two sets, and if $f:A\rightarrow B$ is an
invertible map, then $f_{\ast}$ is invertible as well. Applying this to
$A=\left[  n\right]  $, $B=\left[  n\right]  $ and $f=\sigma$, we conclude
that $\sigma_{\ast}$ is invertible (since $\sigma$ is invertible).

Let $G$ be the subset%
\[
\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}
\]
of $\left[  n\right]  ^{2}$.

For example, if $n=4$, then%
\[
G=\left\{  \left(  1,2\right)  ,\left(  1,3\right)  ,\left(  1,4\right)
,\left(  2,3\right)  ,\left(  2,4\right)  ,\left(  3,4\right)  \right\}  .
\]
Comparing this with%
\[
\mathcal{P}_{2}\left(  \left[  4\right]  \right)  =\left\{  \left\{
1,2\right\}  ,\left\{  1,3\right\}  ,\left\{  1,4\right\}  ,\left\{
2,3\right\}  ,\left\{  2,4\right\}  ,\left\{  3,4\right\}  \right\}  ,
\]
we observe that the set $\mathcal{P}_{2}\left(  \left[  n\right]  \right)  $
is obtained from $G$ by \textquotedblleft replacing all parentheses by
brackets\textquotedblright\ (i.e., replacing each $\left(  i,j\right)  \in G$
by $\left\{  i,j\right\}  $). This holds for all $n$, not just for $n=4$. Let
us make this observation somewhat more bulletproof: We can define a map
$\rho:G\rightarrow\mathcal{P}_{2}\left(  \left[  n\right]  \right)  $ by
setting%
\[
\left(  \rho\left(  \left(  i,j\right)  \right)  =\left\{  i,j\right\}
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)  \in G\right)  .
\]
This map $\rho$ is injective (indeed, we can reconstruct every $\left(
i,j\right)  \in G$ from its image $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $, because $\left(  i,j\right)  \in G$ entails $i<j$)
and surjective (since every two-element subset $S$ of $\left[  n\right]  $ has
the form $\left\{  i,j\right\}  $ for some $\left(  i,j\right)  \in\left[
n\right]  ^{2}$ satisfying $i<j$). Hence, the map $\rho$ is bijective.

For every $S\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  $, we define
an element $a_{S}$ of $\mathbb{K}$ by%
\[
a_{S}=a_{\left(  \min S,\max S\right)  }.
\]
(In other words, for every $S\in\mathcal{P}_{2}\left(  \left[  n\right]
\right)  $, we define an element $a_{S}$ of $\mathbb{K}$ by $a_{S}=a_{\left(
i,j\right)  }$, where $i$ and $j$ are the two elements of $S$ in increasing order.)

Let $\operatorname*{Inv}\left(  \sigma\right)  $ be the set of inversions of
$\sigma$. Then, $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }$%
\ \ \ \ \footnote{\textit{Proof.} The definition of $\ell\left(
\sigma\right)  $ shows that%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  =\left(  \text{the number of elements of }\operatorname*{Inv}%
\left(  \sigma\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\sigma\right)  \text{ is the set of all inversions of }\sigma\right) \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\end{align*}
\par
But the definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }=\left(
-1\right)  ^{\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert
}$ (since $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert $), qed.}. Furthermore, $\operatorname*{Inv}\left(
\sigma\right)  \subseteq G$\ \ \ \ \footnote{\textit{Proof.} Let
$c\in\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $c$ is an inversion of
$\sigma$ (since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of
inversions of $\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $
of integers satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. Consider this $\left(  i,j\right)  $. We have
$i\in\left[  n\right]  $ (since $1\leq i\leq n$) and $j\in\left[  n\right]  $
(since $1\leq j\leq n$). Thus, $\left(  i,j\right)  \in\left[  n\right]  ^{2}%
$. Thus, $\left(  i,j\right)  $ is an element of $\left[  n\right]  ^{2}$ and
satisfies $i<j$. In other words, $\left(  i,j\right)  \in\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}  =G$. Hence,
$c=\left(  i,j\right)  \in G$.
\par
Now, let us forget that we fixed $c$. We thus have proven that every
$c\in\operatorname*{Inv}\left(  \sigma\right)  $ satisfies $c\in G$. In other
words, $\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$, qed.}.

Now, we notice the following facts:

\begin{itemize}
\item For every $\left(  i,j\right)  \in G$, we have%
\begin{equation}
a_{\left(  i,j\right)  }=a_{\rho\left(  \left(  i,j\right)  \right)  }.
\label{sol.perm.sign.pseudoexplicit.short.b.a1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.short.b.a1}):}
Let $\left(  i,j\right)  \in G$. Thus, $\left(  i,j\right)  \in G=\left\{
\left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}  $. In other
words, $\left(  i,j\right)  \in\left[  n\right]  ^{2}$ and $i<j$.
\par
The definition of $\rho$ shows that $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $. Since $i<j$, this shows that the elements of the set
$\rho\left(  \left(  i,j\right)  \right)  $ listed in increasing order are $i$
and $j$. Hence, $\min\left(  \rho\left(  \left(  i,j\right)  \right)  \right)
=i$ and $\max\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  =j$.
\par
Now, the definition of $a_{\rho\left(  \left(  i,j\right)  \right)  }$ shows
that $a_{\rho\left(  \left(  i,j\right)  \right)  }=a_{\left(  \min\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  ,\max\left(  \rho\left(
\left(  i,j\right)  \right)  \right)  \right)  }=a_{\left(  i,j\right)  }$
(since $\min\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  =i$ and
$\max\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  =j$). This
proves (\ref{sol.perm.sign.pseudoexplicit.short.b.a1}).}

\item For every $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  $, we have%
\begin{equation}
a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=-a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }.
\label{sol.perm.sign.pseudoexplicit.short.b.a2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.short.b.a2}):}
Let $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $. Thus,
$\left(  i,j\right)  $ is an inversion of $\sigma$. In other words, $\left(
i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $.
\par
The definition of $\rho$ shows that $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $. Hence, $\sigma\left(  \underbrace{\rho\left(
\left(  i,j\right)  \right)  }_{=\left\{  i,j\right\}  }\right)
=\sigma\left(  \left\{  i,j\right\}  \right)  =\left\{  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right\}  $. Since $\sigma\left(  i\right)
>\sigma\left(  j\right)  $, this shows that the elements of the set
$\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  $ listed in
increasing order are $\sigma\left(  j\right)  $ and $\sigma\left(  i\right)
$. Hence, $\min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  =\sigma\left(  j\right)  $ and $\max\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
i\right)  $.
\par
Now, the definition of $a_{\sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  }$ shows that%
\begin{align*}
a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }  &
=a_{\left(  \min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  ,\max\left(  \sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  \right)  \right)  }=a_{\left(  \sigma\left(  j\right)
,\sigma\left(  i\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\min\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
j\right)  \text{ and }\max\left(  \sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  \right)  =\sigma\left(  i\right)  \right) \\
&  =-a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.exe.perm.sign.pseudoexplicit.b.skew}), applied to }\sigma\left(
i\right)  \text{ and }\sigma\left(  j\right)  \text{ instead of }i\text{ and
}j\right)  .
\end{align*}
Hence, $a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=-a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }$. This
proves (\ref{sol.perm.sign.pseudoexplicit.short.b.a2}).}

\item For every $\left(  i,j\right)  \in G$ satisfying $\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  $, we have%
\begin{equation}
a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }.
\label{sol.perm.sign.pseudoexplicit.short.b.a3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.short.b.a3}):}
Let $\left(  i,j\right)  \in G$ be such that $\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $\left(  i,j\right)
$ is an element of $\left[  n\right]  ^{2}$ satisfying $i<j$ (since $\left(
i,j\right)  \in G=\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\right\}  $).
\par
If we had $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, then $\left(
i,j\right)  $ would be an inversion of $\sigma$ (since $\left(  i,j\right)  $
is a pair of integers satisfying $1\leq i<j\leq n$), and thus would belong to
$\operatorname*{Inv}\left(  \sigma\right)  $; this would contradict $\left(
i,j\right)  \notin\operatorname*{Inv}\left(  \sigma\right)  $. Hence, we
cannot have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. Thus, we have
$\sigma\left(  i\right)  \leq\sigma\left(  j\right)  $. Since $\sigma\left(
i\right)  \neq\sigma\left(  j\right)  $ (because $i\neq j$ and because
$\sigma$ is injective), this shows that $\sigma\left(  i\right)
<\sigma\left(  j\right)  $.
\par
The definition of $\rho$ shows that $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $. Hence, $\sigma\left(  \underbrace{\rho\left(
\left(  i,j\right)  \right)  }_{=\left\{  i,j\right\}  }\right)
=\sigma\left(  \left\{  i,j\right\}  \right)  =\left\{  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right\}  $. Since $\sigma\left(  i\right)
<\sigma\left(  j\right)  $, this shows that the elements of the set
$\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  $ listed in
increasing order are $\sigma\left(  i\right)  $ and $\sigma\left(  j\right)
$. Hence, $\min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  =\sigma\left(  i\right)  $ and $\max\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
j\right)  $.
\par
Now, the definition of $a_{\sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  }$ shows that%
\begin{align*}
a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }  &
=a_{\left(  \min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  ,\max\left(  \sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  \right)  \right)  }=a_{\left(  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\min\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
i\right)  \text{ and }\max\left(  \sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  \right)  =\sigma\left(  j\right)  \right)  .
\end{align*}
This proves (\ref{sol.perm.sign.pseudoexplicit.short.b.a3}).}.
\end{itemize}

Now, recall that $G=\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\right\}  $. Hence, the product sign $\prod_{1\leq i<j\leq n}$
in $\prod_{1\leq i<j\leq n}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }$ can be replaced by the product sign $\prod_{\left(
i,j\right)  \in G}$. Thus, we have%
\begin{align}
&  \underbrace{\prod_{1\leq i<j\leq n}}_{=\prod_{\left(  i,j\right)  \in G}%
}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}\nonumber\\
&  =\prod_{\left(  i,j\right)  \in G}a_{\left(  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right)  }=\left(  \prod_{\substack{\left(
i,j\right)  \in G;\\\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  }}\underbrace{a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }}_{\substack{=-a_{\sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  }\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.short.b.a2}))}}}\right)  \cdot\left(
\prod_{\substack{\left(  i,j\right)  \in G;\\\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  }}\underbrace{a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }}%
_{\substack{=a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  }\\\text{(by (\ref{sol.perm.sign.pseudoexplicit.short.b.a3}))}%
}}\right) \nonumber\\
&  =\underbrace{\left(  \prod_{\substack{\left(  i,j\right)  \in G;\\\left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  }}\left(
-a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }\right)
\right)  }_{=\left(  -1\right)  ^{\left\vert \left\{  \left(  i,j\right)  \in
G\ \mid\ \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right\}  \right\vert }\left(  \prod_{\substack{\left(  i,j\right)  \in
G;\\\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
}}a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }\right)
}\cdot\left(  \prod_{\substack{\left(  i,j\right)  \in G;\\\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  }}a_{\sigma\left(  \rho\left(
\left(  i,j\right)  \right)  \right)  }\right) \nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\left\vert \left\{  \left(  i,j\right)
\in G\ \mid\ \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right\}  \right\vert }}_{\substack{=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }\\\text{(since
}\operatorname*{Inv}\left(  \sigma\right)  \subseteq G\text{, and
thus}\\\left\{  \left(  i,j\right)  \in G\ \mid\ \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right\}  =\operatorname*{Inv}%
\left(  \sigma\right)  \text{)}}}\underbrace{\left(  \prod_{\substack{\left(
i,j\right)  \in G;\\\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  }}a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  }\right)  \cdot\left(  \prod_{\substack{\left(  i,j\right)  \in
G;\\\left(  i,j\right)  \notin\operatorname*{Inv}\left(  \sigma\right)
}}a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }\right)
}_{=\prod_{\left(  i,j\right)  \in G}a_{\sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  }}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert }}_{=\left(  -1\right)  ^{\sigma}}\underbrace{\prod
_{\left(  i,j\right)  \in G}a_{\sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  }}_{\substack{=\prod_{T\in\mathcal{P}_{2}\left(  \left[
n\right]  \right)  }a_{\sigma\left(  T\right)  }\\\text{(here, we have
substituted }T\\\text{for }\rho\left(  \left(  i,j\right)  \right)  \text{ in
the product,}\\\text{since the map }\rho:G\rightarrow\mathcal{P}_{2}\left(
\left[  n\right]  \right)  \\\text{is bijective)}}}=\left(  -1\right)
^{\sigma}\cdot\prod_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)
}a_{\sigma\left(  T\right)  }. \label{sol.perm.sign.pseudoexplicit.short.b.u}%
\end{align}
On the other hand,
\begin{align}
&  \underbrace{\prod_{1\leq i<j\leq n}}_{=\prod_{\left(  i,j\right)  \in G}%
}\underbrace{a_{\left(  i,j\right)  }}_{\substack{=a_{\rho\left(  \left(
i,j\right)  \right)  }\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.short.b.a1}))}}}\nonumber\\
&  =\prod_{\left(  i,j\right)  \in G}a_{\rho\left(  \left(  i,j\right)
\right)  }=\prod_{S\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  }%
a_{S}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }S\text{ for }\rho\left(  \left(  i,j\right)
\right)  \text{ in the product,}\\
\text{since the map }\rho:G\rightarrow\mathcal{P}_{2}\left(  \left[  n\right]
\right)  \text{ is bijective}%
\end{array}
\right) \nonumber\\
&  =\prod_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)
}\underbrace{a_{\sigma_{\ast}\left(  T\right)  }}_{\substack{=a_{\sigma\left(
T\right)  }\\\text{(since }\sigma_{\ast}\left(  T\right)  =\sigma\left(
T\right)  \\\text{(by the definition of }\sigma_{\ast}\text{))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma_{\ast}\left(  T\right)  \text{ for
}S\text{ in the product,}\\
\text{since the map }\sigma_{\ast}:\mathcal{P}_{2}\left(  \left[  n\right]
\right)  \rightarrow\mathcal{P}_{2}\left(  \left[  n\right]  \right)  \text{
is bijective}%
\end{array}
\right) \nonumber\\
&  =\prod_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  }%
a_{\sigma\left(  T\right)  }. \label{sol.perm.sign.pseudoexplicit.short.b.v}%
\end{align}
Hence, (\ref{sol.perm.sign.pseudoexplicit.short.b.u}) becomes%
\[
\prod_{1\leq i<j\leq n}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }=\left(  -1\right)  ^{\sigma}\cdot\underbrace{\prod
_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  }a_{\sigma\left(
T\right)  }}_{\substack{=\prod_{1\leq i<j\leq n}a_{\left(  i,j\right)
}\\\text{(by (\ref{sol.perm.sign.pseudoexplicit.short.b.v}))}}}=\left(
-1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}a_{\left(  i,j\right)  }.
\]
This solves Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)}.

\textbf{(a)} We have $x_{j}-x_{i}=-\left(  x_{i}-x_{j}\right)  $ for every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence, we can
apply Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)} to $a_{\left(
i,j\right)  }=x_{i}-x_{j}$. As a result, we obtain $\prod_{1\leq i<j\leq
n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(  j\right)  }\right)
=\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  $. This solves Exercise \ref{exe.perm.sign.pseudoexplicit}
\textbf{(a)}.

\textbf{(c)} Applying Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)}
to $x_{i}=i$, we obtain $\prod_{1\leq i<j\leq n}\left(  \sigma\left(
i\right)  -\sigma\left(  j\right)  \right)  =\left(  -1\right)  ^{\sigma}%
\cdot\prod_{1\leq i<j\leq n}\left(  i-j\right)  $. We can divide both sides of
this equality by $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ (because
$\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is a product of nonzero
integers, and thus nonzero). As a result, we obtain $\dfrac{\prod_{1\leq
i<j\leq n}\left(  \sigma\left(  i\right)  -\sigma\left(  j\right)  \right)
}{\prod_{1\leq i<j\leq n}\left(  i-j\right)  }=\left(  -1\right)  ^{\sigma}$.
Thus,%
\[
\left(  -1\right)  ^{\sigma}=\dfrac{\prod_{1\leq i<j\leq n}\left(
\sigma\left(  i\right)  -\sigma\left(  j\right)  \right)  }{\prod_{1\leq
i<j\leq n}\left(  i-j\right)  }=\prod_{1\leq i<j\leq n}\dfrac{\sigma\left(
i\right)  -\sigma\left(  j\right)  }{i-j}.
\]
Thus, (\ref{eq.sign.pseudoexplicit}) is proven. This solves Exercise
\ref{exe.perm.sign.pseudoexplicit} \textbf{(c)}.

\textbf{(d)} See below.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.perm.sign.pseudoexplicit}.]\textbf{(b)} Let
$\left[  n\right]  $ be the set $\left\{  1,2,\ldots,n\right\}  $. Recall that
$S_{n}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $. In other words, $S_{n}$ is the set of all permutations of the
set $\left[  n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $).

Let $G$ be the subset%
\[
\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\right\}
\]
of $\left[  n\right]  ^{2}$. Thus,%
\[
G=\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\right\}
=\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}
\]
(here, we renamed the index $\left(  i,j\right)  $ as $\left(  u,v\right)  $).
Clearly, the set $G$ is finite (since it is a subset of the finite set
$\left[  n\right]  ^{2}$).

Let $\operatorname*{Inv}\left(  \sigma\right)  $ be the set of inversions of
$\sigma$. Then, $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }$%
\ \ \ \ \footnote{\textit{Proof.} We have%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \\
&  =\left(  \text{the number of elements of }\operatorname*{Inv}\left(
\sigma\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\sigma\right)  \text{ is the set of all inversions of }\sigma\right) \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\end{align*}
\par
But the definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }=\left(
-1\right)  ^{\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert
}$ (since $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert $), qed.}. We notice furthermore that
$\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
\sigma\right)  $. Thus, $c$ is an inversion of $\sigma$ (since
$\operatorname*{Inv}\left(  \sigma\right)  $ is the set of inversions of
$\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. Consider this $\left(  i,j\right)  $. We have $i\in\left[
n\right]  $ (since $1\leq i\leq n$) and $j\in\left[  n\right]  $ (since $1\leq
j\leq n$). Thus, $\left(  i,j\right)  \in\left[  n\right]  ^{2}$ (since
$i\in\left[  n\right]  $ and $j\in\left[  n\right]  $). Hence, $\left(
i,j\right)  $ is an element of $\left[  n\right]  ^{2}$ and satisfies $i<j$.
In other words, $\left(  i,j\right)  $ is an element $\left(  u,v\right)  $ of
$\left[  n\right]  ^{2}$ and satisfies $u<v$. In other words, $\left(
i,j\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\right\}  =G$. Hence, $c=\left(  i,j\right)  \in G$.
\par
Now, let us forget that we fixed $c$. We thus have proven that every
$c\in\operatorname*{Inv}\left(  \sigma\right)  $ satisfies $c\in G$. In other
words, $\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$, qed.}.

We shall use the following notation (known as
\href{https://en.wikipedia.org/wiki/Iverson_bracket}{the \textit{Iverson
bracket}}): If $\mathcal{A}$ is any logical statement, then $\left[
\mathcal{A}\right]  $ will mean the integer $%
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}%
\end{cases}
$. For example, $\left[  1+1=2\right]  =1$ (since $1+1=2$ is true), whereas
$\left[  1+1=1\right]  =0$ (since $1+1=1$ is false). Clearly, if $\mathcal{A}$
and $\mathcal{B}$ are two equivalent logical statements, then $\left[
\mathcal{A}\right]  =\left[  \mathcal{B}\right]  $.

A useful property of the Iverson bracket is that it turns cardinalities of
sets into sums: Namely, if $S$ is a finite set, and if $T$ is a subset of $S$,
then%
\begin{equation}
\left\vert T\right\vert =\sum_{s\in S}\left[  s\in T\right]
\label{sol.perm.sign.pseudoexplicit.b.iverson.sum}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.iverson.sum}%
):} Let $S$ be a finite set. Let $T$ be a subset of $S$. Then,%
\begin{align*}
\sum_{s\in S}\left[  s\in T\right]   &  =\underbrace{\sum_{\substack{s\in
S;\\s\in T}}}_{\substack{=\sum_{s\in T}\\\text{(since }T\text{ is a subset of
}S\text{)}}}\underbrace{\left[  s\in T\right]  }_{\substack{=1\\\text{(since
}s\in T\text{ is true)}}}+\sum_{\substack{s\in S;\\s\notin T}%
}\underbrace{\left[  s\in T\right]  }_{\substack{=0\\\text{(since }s\in
T\text{ is false}\\\text{(since }s\notin T\text{))}}}\\
&  =\underbrace{\sum_{s\in T}1}_{=\left\vert T\right\vert \cdot1}%
+\underbrace{\sum_{\substack{s\in S;\\s\notin T}}0}_{=0}=\left\vert
T\right\vert \cdot1+0=\left\vert T\right\vert ,
\end{align*}
qed.}. Consequently, if $S$ is a finite set, and if $T$ is a subset of $S$,
then%
\begin{equation}
\left(  -1\right)  ^{\left\vert T\right\vert }=\prod_{s\in S}\left(
-1\right)  ^{\left[  s\in T\right]  }
\label{sol.perm.sign.pseudoexplicit.b.iverson.prod}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.iverson.prod}%
):} Let $S$ be a finite set. Let $T$ be a subset of $S$. Then,
(\ref{sol.perm.sign.pseudoexplicit.b.iverson.sum}) yields $\left\vert
T\right\vert =\sum_{s\in S}\left[  s\in T\right]  $. Hence,
\[
\left(  -1\right)  ^{\left\vert T\right\vert }=\left(  -1\right)  ^{\sum_{s\in
S}\left[  s\in T\right]  }=\prod_{s\in S}\left(  -1\right)  ^{\left[  s\in
T\right]  }%
\]
(because $a^{\sum_{s\in S}b_{s}}=\prod_{s\in S}a^{b_{s}}$ whenever $a$ is an
integer and $b_{s}$ is an integer for every $s\in S$). This proves
(\ref{sol.perm.sign.pseudoexplicit.b.iverson.prod}).}. We can apply this to
$S=G$ and $T=\operatorname*{Inv}\left(  \sigma\right)  $ (since
$\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$). As a result, we
obtain%
\[
\left(  -1\right)  ^{\left\vert \operatorname*{Inv}\left(  \sigma\right)
\right\vert }=\prod_{s\in G}\left(  -1\right)  ^{\left[  s\in
\operatorname*{Inv}\left(  \sigma\right)  \right]  }=\prod_{\left(
i,j\right)  \in G}\left(  -1\right)  ^{\left[  \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right]  }%
\]
(here, we renamed the index $s$ as $\left(  i,j\right)  $, because all
elements of $G$ are pairs). Thus,%
\begin{equation}
\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }=\prod_{\left(
i,j\right)  \in G}\left(  -1\right)  ^{\left[  \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right]  }.
\label{sol.perm.sign.pseudoexplicit.b.2}%
\end{equation}


On the other hand, for every $\tau\in S_{n}$, we define a map $\tau^{\left[
2\right]  }:G\rightarrow G$ by setting%
\begin{equation}
\left(  \tau^{\left[  2\right]  }\left(  i,j\right)  =\left(  \min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  ,\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)  \in G\right)  .
\label{sol.perm.sign.pseudoexplicit.b.tau}%
\end{equation}
This map $\tau^{\left[  2\right]  }$ is well-defined\footnote{\textit{Proof.}
In order to prove this, we need to show that every element of $G$ can be
written in the form $\left(  i,j\right)  $, and that every $\left(
i,j\right)  \in G$ satisfies $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  \in G$. The first of these two
claims (i.e., that every element of $G$ can be written in the form $\left(
i,j\right)  $) is obvious. It thus remains to prove the second of these two
claims. In other words, it remains to prove that every $\left(  i,j\right)
\in G$ satisfies $\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  \right)  \in G$.
\par
So let $\left(  i,j\right)  \in G$. We must prove that $\left(  \min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  ,\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)  \in G$.
\par
We have $\left(  i,j\right)  \in G=\left\{  \left(  u,v\right)  \in\left[
n\right]  ^{2}\ \mid\ u<v\right\}  $. In other words, $\left(  i,j\right)  $
is an element $\left(  u,v\right)  $ of $\left[  n\right]  ^{2}$ satisfying
$u<v$. In other words, $\left(  i,j\right)  $ is an element of $\left[
n\right]  ^{2}$ satisfying $i<j$.
\par
Now, $\tau\in S_{n}$. Hence, $\tau$ is a permutation of the set $\left[
n\right]  $ (since $S_{n}$ is the set of all permutations of the set $\left[
n\right]  $). In other words, $\tau$ is a bijective map $\left[  n\right]
\rightarrow\left[  n\right]  $. Hence, $\tau$ is both surjective and
injective. We have $i\neq j$ (since $i<j$) and thus $\tau\left(  i\right)
\neq\tau\left(  j\right)  $ (since $\tau$ is injective). Clearly, both
$\tau\left(  i\right)  $ and $\tau\left(  j\right)  $ are elements of $\left[
n\right]  $. Thus, $\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  \subseteq\left[  n\right]  $, so that $\min\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  \in\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \subseteq\left[  n\right]  $ and
$\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
\in\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
\subseteq\left[  n\right]  $. Hence,
\[
\left(  \underbrace{\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  }_{\in\left[  n\right]  },\underbrace{\max\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  }_{\in\left[  n\right]  }\right)
\in\left[  n\right]  \times\left[  n\right]  =\left[  n\right]  ^{2}.
\]
\par
Now, let us show that $\min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  <\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  $. Indeed, we assume the contrary (for the sake of contradiction).
Thus, $\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
\geq\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Now, $\tau\left(  i\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus greater or equal to the
minimum of this set. In other words, we have $\tau\left(  i\right)  \geq
\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Also, $\tau\left(  i\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus less or equal to the
maximum of this set. In other words, we have $\tau\left(  i\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Combining $\tau\left(  i\right)  \geq\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \geq\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  $ with $\tau\left(  i\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $, we
obtain $\tau\left(  i\right)  =\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  $.
\par
Also, $\tau\left(  j\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus greater or equal to the
minimum of this set. In other words, we have $\tau\left(  j\right)  \geq
\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Also, $\tau\left(  j\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus less or equal to the
maximum of this set. In other words, we have $\tau\left(  j\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Combining $\tau\left(  j\right)  \geq\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \geq\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  $ with $\tau\left(  j\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $, we
obtain $\tau\left(  j\right)  =\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  $. Compared with $\tau\left(  i\right)  =\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $, this yields
$\tau\left(  i\right)  =\tau\left(  j\right)  $, which contradicts
$\tau\left(  i\right)  \neq\tau\left(  j\right)  $. This contradiction proves
that our assumption was wrong.
\par
Hence, $\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
<\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $ is
proven. Now, we know that $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  $ is an element of $\left[
n\right]  ^{2}$ satisfying $\min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  <\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  $. In other words, $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  $ is an element $\left(  u,v\right)
\in\left[  n\right]  ^{2}$ satisfying $u<v$. In other words,%
\[
\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)
\in\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}
=G.
\]
Thus, we have proven that $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  \in G$. Qed.}. Moreover, we have%
\begin{equation}
\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[  2\right]
}=\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for every }\tau\in S_{n}
\label{sol.perm.sign.pseudoexplicit.b.tautau}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.tautau}):} Let
$\tau\in S_{n}$. Let $c\in G$. We are going to prove that $\left(  \left(
\tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[  2\right]  }\right)
\left(  c\right)  =c$.
\par
We have $c\in G=\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}%
\ \mid\ i<j\right\}  $. In other words, $c$ can be written in the form
$c=\left(  i,j\right)  $ for some $\left(  i,j\right)  \in\left[  n\right]
^{2}$ satisfying $i<j$. Consider this $\left(  i,j\right)  $.
\par
The definition of $\tau^{\left[  2\right]  }$ yields $\tau^{\left[  2\right]
}\left(  i,j\right)  =\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  \right)  $. Thus, $\tau^{\left[  2\right]  }\left(  \underbrace{c}%
_{=\left(  i,j\right)  }\right)  =\tau^{\left[  2\right]  }\left(  i,j\right)
=\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)
$.
\par
Now, $\tau\in S_{n}$. Hence, $\tau$ is a permutation of the set $\left[
n\right]  $ (since $S_{n}$ is the set of all permutations of the set $\left[
n\right]  $). In other words, $\tau$ is a bijective map $\left[  n\right]
\rightarrow\left[  n\right]  $. Hence, $\tau$ is both surjective and
injective. We have $i\neq j$ (since $i<j$) and thus $\tau\left(  i\right)
\neq\tau\left(  j\right)  $ (since $\tau$ is injective). Hence, we have either
$\tau\left(  i\right)  <\tau\left(  j\right)  $ or $\tau\left(  i\right)
>\tau\left(  j\right)  $. In other words, we are in one of the following two
cases:
\par
\textit{Case 1:} We have $\tau\left(  i\right)  <\tau\left(  j\right)  $.
\par
\textit{Case 2:} We have $\tau\left(  i\right)  >\tau\left(  j\right)  $.
\par
Let us first consider Case 1. In this case, we have $\tau\left(  i\right)
<\tau\left(  j\right)  $. Thus, $\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  =\tau\left(  i\right)  $ and $\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  =\tau\left(  j\right)
$. Hence,%
\[
\tau^{\left[  2\right]  }\left(  c\right)  =\left(  \underbrace{\min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  }_{=\tau\left(
i\right)  },\underbrace{\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  }_{=\tau\left(  j\right)  }\right)  =\left(  \tau\left(
i\right)  ,\tau\left(  j\right)  \right)  .
\]
Now,%
\begin{align*}
\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[
2\right]  }\right)  \left(  c\right)   &  =\left(  \tau^{-1}\right)  ^{\left[
2\right]  }\left(  \underbrace{\tau^{\left[  2\right]  }\left(  c\right)
}_{=\left(  \tau\left(  i\right)  ,\tau\left(  j\right)  \right)  }\right)
=\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\left(  \tau\left(  i\right)
,\tau\left(  j\right)  \right) \\
&  =\left(  \min\left\{  \underbrace{\tau^{-1}\left(  \tau\left(  i\right)
\right)  }_{=i},\underbrace{\tau^{-1}\left(  \tau\left(  j\right)  \right)
}_{=j}\right\}  ,\max\left\{  \underbrace{\tau^{-1}\left(  \tau\left(
i\right)  \right)  }_{=i},\underbrace{\tau^{-1}\left(  \tau\left(  j\right)
\right)  }_{=j}\right\}  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  \tau
^{-1}\right)  ^{\left[  2\right]  }\right) \\
&  =\left(  \underbrace{\min\left\{  i,j\right\}  }%
_{\substack{=i\\\text{(since }i<j\text{)}}},\underbrace{\max\left\{
i,j\right\}  }_{\substack{=j\\\text{(since }i<j\text{)}}}\right)  =\left(
i,j\right)  =c.
\end{align*}
Hence, $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $\tau\left(  i\right)
>\tau\left(  j\right)  $. Thus, $\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  =\tau\left(  j\right)  $ and $\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  =\tau\left(  i\right)
$. Hence,%
\[
\tau^{\left[  2\right]  }\left(  c\right)  =\left(  \underbrace{\min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  }_{=\tau\left(
j\right)  },\underbrace{\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  }_{=\tau\left(  i\right)  }\right)  =\left(  \tau\left(
j\right)  ,\tau\left(  i\right)  \right)  .
\]
Now,%
\begin{align*}
\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[
2\right]  }\right)  \left(  c\right)   &  =\left(  \tau^{-1}\right)  ^{\left[
2\right]  }\left(  \underbrace{\tau^{\left[  2\right]  }\left(  c\right)
}_{=\left(  \tau\left(  j\right)  ,\tau\left(  i\right)  \right)  }\right)
=\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\left(  \tau\left(  j\right)
,\tau\left(  i\right)  \right) \\
&  =\left(  \min\left\{  \underbrace{\tau^{-1}\left(  \tau\left(  j\right)
\right)  }_{=j},\underbrace{\tau^{-1}\left(  \tau\left(  i\right)  \right)
}_{=i}\right\}  ,\max\left\{  \underbrace{\tau^{-1}\left(  \tau\left(
j\right)  \right)  }_{=j},\underbrace{\tau^{-1}\left(  \tau\left(  i\right)
\right)  }_{=i}\right\}  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  \tau
^{-1}\right)  ^{\left[  2\right]  }\right) \\
&  =\left(  \underbrace{\min\left\{  j,i\right\}  }%
_{\substack{=i\\\text{(since }i<j\text{)}}},\underbrace{\max\left\{
j,i\right\}  }_{\substack{=j\\\text{(since }i<j\text{)}}}\right)  =\left(
i,j\right)  =c.
\end{align*}
Hence, $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ is proven in Case 2.
\par
We have now proven $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]
}\circ\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ in each of the
two Cases 1 and 2. Thus, $\left(  \left(  \tau^{-1}\right)  ^{\left[
2\right]  }\circ\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ always
holds.
\par
So we have $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c=\operatorname*{id}%
\left(  c\right)  $.
\par
Let us now forget that we fixed $c$. We thus have proven that $\left(  \left(
\tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[  2\right]  }\right)
\left(  c\right)  =\operatorname*{id}\left(  c\right)  $ for every $c\in G$.
In other words, $\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }=\operatorname*{id}$. This proves
(\ref{sol.perm.sign.pseudoexplicit.b.tautau}).}. Thus, for every $\tau\in
S_{n}$, the map $\tau^{\left[  2\right]  }$ is a
bijection\footnote{\textit{Proof.} Let $\tau\in S_{n}$. Applying
(\ref{sol.perm.sign.pseudoexplicit.b.tautau}) to $\tau^{-1}$ instead of $\tau
$, we obtain $\left(  \left(  \tau^{-1}\right)  ^{-1}\right)  ^{\left[
2\right]  }\circ\left(  \tau^{-1}\right)  ^{\left[  2\right]  }%
=\operatorname*{id}$. Since $\left(  \tau^{-1}\right)  ^{-1}=\tau$, this
rewrites as $\tau^{\left[  2\right]  }\circ\left(  \tau^{-1}\right)  ^{\left[
2\right]  }=\operatorname*{id}$. Combined with $\left(  \tau^{-1}\right)
^{\left[  2\right]  }\circ\tau^{\left[  2\right]  }=\operatorname*{id}$ (which
follows from (\ref{sol.perm.sign.pseudoexplicit.b.tautau})), this yields that
the maps $\tau^{\left[  2\right]  }$ and $\left(  \tau^{-1}\right)  ^{\left[
2\right]  }$ are mutually inverse. Hence, the map $\tau^{\left[  2\right]  }$
is invertible, i.e., is a bijection. Qed.}. Applying this to $\tau=\sigma$, we
see that the map $\sigma^{\left[  2\right]  }$ is a bijection.

Now, every $\left(  i,j\right)  \in G$ satisfies%
\begin{equation}
\left(  -1\right)  ^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  \right]  }a_{\sigma^{\left[  2\right]  }\left(  i,j\right)
}=a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }
\label{sol.perm.sign.pseudoexplicit.b.a-inv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}):} Let
$\left(  i,j\right)  \in G$. Thus, $\left(  i,j\right)  \in G=\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}  $. In other words,
$\left(  i,j\right)  $ is an element $\left(  u,v\right)  $ of $\left[
n\right]  ^{2}$ satisfying $u<v$. In other words, $\left(  i,j\right)  $ is an
element of $\left[  n\right]  ^{2}$ satisfying $i<j$. From $\left(
i,j\right)  \in\left[  n\right]  ^{2}$, we obtain $i\in\left[  n\right]  $ and
$j\in\left[  n\right]  $. Thus, $1\leq i$ (since $i\in\left[  n\right]  $) and
$j\leq n$ (since $j\in\left[  n\right]  $), so that $1\leq i<j\leq n$.
\par
We must be in one of the following two cases:
\par
\textit{Case 1:} We have $\sigma\left(  i\right)  \leq\sigma\left(  j\right)
$.
\par
\textit{Case 2:} We have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $.
\par
Let us first consider Case 1. In this case, we have $\sigma\left(  i\right)
\leq\sigma\left(  j\right)  $. Thus, $\min\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  =\sigma\left(  i\right)  $ and
$\max\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}
=\sigma\left(  j\right)  $. Now, the definition of $\sigma^{\left[  2\right]
}$ yields%
\[
\sigma^{\left[  2\right]  }\left(  i,j\right)  =\left(  \underbrace{\min
\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}  }%
_{=\sigma\left(  i\right)  },\underbrace{\max\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  }_{=\sigma\left(  j\right)  }\right)
=\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  .
\]
Hence, $a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }=a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }$.
\par
On the other hand, let us show that $\left(  i,j\right)  \notin%
\operatorname*{Inv}\left(  \sigma\right)  $. Indeed, we assume the contrary
(for the sake of contradiction). Thus, $\left(  i,j\right)  \in
\operatorname*{Inv}\left(  \sigma\right)  $. In other words, $\left(
i,j\right)  $ is an inversion of $\sigma$ (since $\operatorname*{Inv}\left(
\sigma\right)  $ is the set of inversions of $\sigma$). In other words,
$\left(  i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $ (because of how an
\textquotedblleft inversion\textquotedblright\ is defined). But $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ contradicts $\sigma\left(  i\right)
\leq\sigma\left(  j\right)  $. Thus, we have obtained a contradiction.
Therefore, our assumption must have been false. This proves that $\left(
i,j\right)  \notin\operatorname*{Inv}\left(  \sigma\right)  $.
\par
Hence, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $ is
false. Thus, $\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  \right]  =0$, so that $\left(  -1\right)  ^{\left[  \left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]  }=1$ and
therefore $\underbrace{\left(  -1\right)  ^{\left[  \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right]  }}_{=1}%
\underbrace{a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }}_{=a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }}=1a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }=a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }$. Thus,
(\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. Thus, $\min\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  =\sigma\left(  j\right)  $ and
$\max\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}
=\sigma\left(  i\right)  $. Now, the definition of $\sigma^{\left[  2\right]
}$ yields%
\[
\sigma^{\left[  2\right]  }\left(  i,j\right)  =\left(  \underbrace{\min
\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}  }%
_{=\sigma\left(  j\right)  },\underbrace{\max\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  }_{=\sigma\left(  i\right)  }\right)
=\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  .
\]
Hence, $a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }=a_{\left(
\sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  }=-a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }$ (by
(\ref{eq.exe.perm.sign.pseudoexplicit.b.skew}), applied to $\sigma\left(
i\right)  $ and $\sigma\left(  j\right)  $ instead of $i$ and $j$).
\par
On the other hand, $\left(  i,j\right)  $ is a pair of integers satisfying
$1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. In
other words, $\left(  i,j\right)  $ is an inversion of $\sigma$ (because of
how an \textquotedblleft inversion\textquotedblright\ is defined). In other
words, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $
(since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of inversions
of $\sigma$). Hence, $\left[  \left(  i,j\right)  \in\operatorname*{Inv}%
\left(  \sigma\right)  \right]  =1$, so that $\left(  -1\right)  ^{\left[
\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]
}=-1$ and therefore $\underbrace{\left(  -1\right)  ^{\left[  \left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]  }}%
_{=-1}\underbrace{a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)
\right)  }}_{=-a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)
\right)  }}=\left(  -1\right)  \left(  -a_{\left(  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right)  }\right)  =a_{\left(  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right)  }$. Thus,
(\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) is proven in Case 2.
\par
We have now proven (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) in each of the
two Cases 1 and 2. Thus, (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) always
holds, qed.}.

Now,%
\begin{align*}
&  \underbrace{\prod_{1\leq i<j\leq n}}_{\substack{=\prod_{\substack{\left(
i,j\right)  \in\left[  n\right]  ^{2};\\i<j}}=\prod_{\left(  i,j\right)  \in
G}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\left[  n\right]
^{2}\ \mid\ i<j\right\}  \text{)}}}\underbrace{a_{\left(  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right)  }}_{\substack{=\left(  -1\right)
^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right]  }a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.b.a-inv}))}}}\\
&  =\prod_{\left(  i,j\right)  \in G}\left(  \left(  -1\right)  ^{\left[
\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]
}a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }\right)
=\underbrace{\left(  \prod_{\left(  i,j\right)  \in G}\left(  -1\right)
^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right]  }\right)  }_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.b.2}))}}}\left(  \prod_{\left(  i,j\right)
\in G}a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }\right) \\
&  =\left(  -1\right)  ^{\sigma}\prod_{\left(  i,j\right)  \in G}%
a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }=\left(  -1\right)
^{\sigma}\prod_{c\in G}a_{\sigma^{\left[  2\right]  }\left(  c\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
i,j\right)  \text{ as }c\text{ in the product}\right) \\
&  =\left(  -1\right)  ^{\sigma}\prod_{c\in G}a_{c}\ \ \ \ \ \ \ \ \ \ \left(
%
\begin{array}
[c]{c}%
\text{here, we have substituted }c\text{ for }\sigma^{\left[  2\right]
}\left(  c\right)  \text{ in the product,}\\
\text{since the map }\sigma^{\left[  2\right]  }:G\rightarrow G\text{ is a
bijection}%
\end{array}
\right) \\
&  =\left(  -1\right)  ^{\sigma}\underbrace{\prod_{\left(  i,j\right)  \in G}%
}_{\substack{=\prod_{\substack{\left(  i,j\right)  \in\left[  n\right]
^{2};\\i<j}}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\left[
n\right]  ^{2}\ \mid\ i<j\right\}  \text{)}}}a_{\left(  i,j\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the index }c\text{ as }\left(  i,j\right)  \text{
in the product,}\\
\text{since every element of }G\text{ has the form }\left(  i,j\right)
\end{array}
\right) \\
&  =\left(  -1\right)  ^{\sigma}\underbrace{\prod_{\substack{\left(
i,j\right)  \in\left[  n\right]  ^{2};\\i<j}}}_{=\prod_{1\leq i<j\leq n}%
}a_{\left(  i,j\right)  }=\left(  -1\right)  ^{\sigma}\prod_{1\leq i<j\leq
n}a_{\left(  i,j\right)  }.
\end{align*}
This solves Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)}.

\textbf{(a)} We have $x_{j}-x_{i}=-\left(  x_{i}-x_{j}\right)  $ for every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence, we can
apply Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)} to $a_{\left(
i,j\right)  }=x_{i}-x_{j}$. As a result, we obtain $\prod_{1\leq i<j\leq
n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(  j\right)  }\right)
=\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  $. This solves Exercise \ref{exe.perm.sign.pseudoexplicit}
\textbf{(a)}.

\textbf{(c)} Applying Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)}
to $x_{i}=i$, we obtain $\prod_{1\leq i<j\leq n}\left(  \sigma\left(
i\right)  -\sigma\left(  j\right)  \right)  =\left(  -1\right)  ^{\sigma}%
\cdot\prod_{1\leq i<j\leq n}\left(  i-j\right)  $. We can divide both sides of
this equality by $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ (because
$\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is a product of nonzero
integers, and thus nonzero). As a result, we obtain $\dfrac{\prod_{1\leq
i<j\leq n}\left(  \sigma\left(  i\right)  -\sigma\left(  j\right)  \right)
}{\prod_{1\leq i<j\leq n}\left(  i-j\right)  }=\left(  -1\right)  ^{\sigma}$.
Thus,%
\[
\left(  -1\right)  ^{\sigma}=\dfrac{\prod_{1\leq i<j\leq n}\left(
\sigma\left(  i\right)  -\sigma\left(  j\right)  \right)  }{\prod_{1\leq
i<j\leq n}\left(  i-j\right)  }=\prod_{1\leq i<j\leq n}\dfrac{\sigma\left(
i\right)  -\sigma\left(  j\right)  }{i-j}.
\]
Thus, (\ref{eq.sign.pseudoexplicit}) is proven. This solves Exercise
\ref{exe.perm.sign.pseudoexplicit} \textbf{(c)}.

\textbf{(d)} See below.
\end{proof}
\end{verlong}

Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(d)} asks us to give an
alternative solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}. Let us do
this now:

\begin{proof}
[Alternative solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}.]Let
$n\in\mathbb{N}$. Let $\sigma$ and $\tau$ be two permutations in $S_{n}$. We
need to show that $\ell\left(  \sigma\circ\tau\right)  \equiv\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  \operatorname{mod}2$.

We are going to prove that $\left(  -1\right)  ^{\sigma\circ\tau}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ first.

Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} (applied to $x_{i}%
=i$) yields%
\[
\prod_{1\leq i<j\leq n}\left(  \sigma\left(  i\right)  -\sigma\left(
j\right)  \right)  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}\left(  i-j\right)  .
\]
Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} (applied to
$\sigma\left(  i\right)  $ and $\tau$ instead of $x_{i}$ and $\sigma$) yields%
\begin{align}
\prod_{1\leq i<j\leq n}\left(  \sigma\left(  \tau\left(  i\right)  \right)
-\sigma\left(  \tau\left(  j\right)  \right)  \right)   &  =\left(  -1\right)
^{\tau}\cdot\underbrace{\prod_{1\leq i<j\leq n}\left(  \sigma\left(  i\right)
-\sigma\left(  j\right)  \right)  }_{=\left(  -1\right)  ^{\sigma}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right)  }\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\tau}\cdot\left(  -1\right)  ^{\sigma}%
}_{=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right) \nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right)  .
\label{sol.perm.sign.pseudoexplicit.d.1}%
\end{align}
Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} (applied to
$\sigma\circ\tau$ instead of $\sigma$) yields%
\[
\prod_{1\leq i<j\leq n}\left(  \left(  \sigma\circ\tau\right)  \left(
i\right)  -\left(  \sigma\circ\tau\right)  \left(  j\right)  \right)  =\left(
-1\right)  ^{\sigma\circ\tau}\cdot\prod_{1\leq i<j\leq n}\left(  i-j\right)
.
\]
Thus,%
\begin{align}
\left(  -1\right)  ^{\sigma\circ\tau}\cdot\prod_{1\leq i<j\leq n}\left(
i-j\right)   &  =\prod_{1\leq i<j\leq n}\left(  \underbrace{\left(
\sigma\circ\tau\right)  \left(  i\right)  }_{=\sigma\left(  \tau\left(
i\right)  \right)  }-\underbrace{\left(  \sigma\circ\tau\right)  \left(
j\right)  }_{=\sigma\left(  \tau\left(  j\right)  \right)  }\right)
=\prod_{1\leq i<j\leq n}\left(  \sigma\left(  \tau\left(  i\right)  \right)
-\sigma\left(  \tau\left(  j\right)  \right)  \right) \nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.perm.sign.pseudoexplicit.d.1})}\right)  .
\label{sol.perm.sign.pseudoexplicit.d.2}%
\end{align}


\begin{vershort}
But the integer $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is nonzero
(since it is a product of the nonzero integers $i-j$). Hence, we can divide
both sides of the equality (\ref{sol.perm.sign.pseudoexplicit.d.2}) by
$\prod_{1\leq i<j\leq n}\left(  i-j\right)  $. We thus obtain $\left(
-1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(
-1\right)  ^{\tau}$.
\end{vershort}

\begin{verlong}
But the integer $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is
nonzero\footnote{\textit{Proof.} If $i$ and $j$ are two integers satisfying
$1\leq i<j\leq n$, then the integer $i-j$ is negative (since $i<j$) and thus
nonzero. Hence, the product $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is a
product of nonzero integers, and therefore itself nonzero (since a product of
nonzero integers is always nonzero). Qed.}. Hence, we can divide both sides of
the equality (\ref{sol.perm.sign.pseudoexplicit.d.2}) by $\prod_{1\leq i<j\leq
n}\left(  i-j\right)  $. We thus obtain $\left(  -1\right)  ^{\sigma\circ\tau
}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$.
\end{verlong}

Now, the definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$. Also,
the definition of $\left(  -1\right)  ^{\tau}$ yields $\left(  -1\right)
^{\tau}=\left(  -1\right)  ^{\ell\left(  \tau\right)  }$. Hence,
\[
\underbrace{\left(  -1\right)  ^{\sigma}}_{=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }}\cdot\underbrace{\left(  -1\right)  ^{\tau}}_{=\left(
-1\right)  ^{\ell\left(  \tau\right)  }}=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }\cdot\left(  -1\right)  ^{\ell\left(  \tau\right)  }=\left(
-1\right)  ^{\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  }.
\]


Finally, the definition of $\left(  -1\right)  ^{\sigma\circ\tau}$ yields
$\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\ell\left(
\sigma\circ\tau\right)  }$. Thus,%
\[
\left(  -1\right)  ^{\ell\left(  \sigma\circ\tau\right)  }=\left(  -1\right)
^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau
}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
}.
\]
But it is obvious that if two integers $u$ and $v$ satisfy $\left(  -1\right)
^{u}=\left(  -1\right)  ^{v}$, then $u\equiv v\operatorname{mod}2$. Applying
this to $u=\ell\left(  \sigma\circ\tau\right)  $ and $v=\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $, we obtain $\ell\left(  \sigma
\circ\tau\right)  \equiv\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
\operatorname{mod}2$. Thus, Exercise \ref{exe.ps2.2.5} \textbf{(b)} is solved again.
\end{proof}

\subsection{Solution to Exercise \ref{exe.perm.cycles}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.perm.cycles}.]\textbf{(a)} This proof is going
to be long, but most of it will be spent unraveling the notations. If you find
Exercise \ref{exe.perm.cycles} \textbf{(a)} obvious, don't let this proof cast
doubt on your understanding.

Let $\sigma\in S_{n}$. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements
of $\left[  n\right]  $.

The map $\sigma$ is a permutation (since $\sigma\in S_{n}$), and therefore
bijective. Hence, in particular, $\sigma$ is injective.

For every $p\in\left\{  1,2,\ldots,k\right\}  $, let $j_{p}$ be the element
$\sigma\left(  i_{p}\right)  \in\left[  n\right]  $. Then, $\left(
j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $.
Furthermore, $j_{1},j_{2},\ldots,j_{k}$ are $k$ distinct elements of $\left[
n\right]  $\ \ \ \ \footnote{\textit{Proof.} The map $\sigma$ is injective.
Hence, the elements $\sigma\left(  i_{1}\right)  ,\sigma\left(  i_{2}\right)
,\ldots,\sigma\left(  i_{k}\right)  $ are distinct (since the elements
$i_{1},i_{2},\ldots,i_{k}$ are distinct). Thus, $\sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  $ are $k$
distinct elements of $\left[  n\right]  $. In other words, $j_{1},j_{2}%
,\ldots,j_{k}$ are $k$ distinct elements of $\left[  n\right]  $ (since
$\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $%
).}. Therefore, $\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ is a
well-defined permutation in $S_{n}$.

We have defined $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ to
be the permutation in $S_{n}$ which sends $i_{1},i_{2},\ldots,i_{k}$ to
$i_{2},i_{3},\ldots,i_{k},i_{1}$, respectively, while leaving all other
elements of $\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ sends $i_{1},i_{2},\ldots,i_{k}$ to $i_{2},i_{3},\ldots,i_{k},i_{1}$,
respectively. In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  i_{p}\right)
=i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.short.a.cyc-manifest.1}%
\end{equation}
where $i_{k+1}$ means $i_{1}$.

\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ leaves all other elements of $\left[  n\right]  $ fixed (where
\textquotedblleft other\textquotedblright\ means \textquotedblleft other than
$i_{1},i_{2},\ldots,i_{k}$\textquotedblright). In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
i_{1},i_{2},\ldots,i_{k}\right\}  .
\label{sol.perm.cycles.short.a.cyc-manifest.2}%
\end{equation}

\end{itemize}

Similarly, we can say the same about $\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}$:

\begin{itemize}
\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  j_{p}\right)
=j_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.short.a.cyc-manifest.1'}%
\end{equation}
where $j_{k+1}$ means $j_{1}$.

\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
j_{1},j_{2},\ldots,j_{k}\right\}  .
\label{sol.perm.cycles.short.a.cyc-manifest.2'}%
\end{equation}

\end{itemize}

In the following, we shall use the notation $i_{k+1}$ as a synonym for $i_{1}%
$, and the notation $j_{k+1}$ as a synonym for $j_{1}$. Then,%
\begin{equation}
j_{p}=\sigma\left(  i_{p}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,k+1\right\}  . \label{sol.perm.cycles.short.a.jp}%
\end{equation}
(Indeed, in the case when $p\in\left\{  1,2,\ldots,k\right\}  $, this follows
from the definition of $j_{p}$; but in the remaining case when $p=k+1$, it
follows from $j_{k+1}=j_{1}=\sigma\left(  \underbrace{i_{1}}_{=i_{k+1}%
}\right)  =\sigma\left(  i_{k+1}\right)  $.)

Now, let us show that
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
\label{sol.perm.cycles.short.a.qq}%
\end{equation}
for every $q\in\left[  n\right]  $.

\textit{Proof of (\ref{sol.perm.cycles.short.a.qq}):} Let $q\in\left[
n\right]  $. We must prove (\ref{sol.perm.cycles.short.a.qq}). We are in one
of the following two cases:

\textit{Case 1:} We have $q\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

\textit{Case 2:} We have $q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

Let us first consider Case 1. In this case, we have $q\in\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Thus, $q=j_{p}$ for some $p\in\left\{
1,2,\ldots,k\right\}  $. Consider this $p$. Clearly, $p+1\in\left\{
2,3,\ldots,k+1\right\}  \subseteq\left\{  1,2,\ldots,k+1\right\}  $. Hence,
applying (\ref{sol.perm.cycles.short.a.jp}) to $p+1$ instead of $p$, we obtain
$j_{p+1}=\sigma\left(  i_{p+1}\right)  $. But $q=j_{p}=\sigma\left(
i_{p}\right)  $ (by the definition of $j_{p}$) and thus $\sigma^{-1}\left(
q\right)  =i_{p}$. Hence,
\[
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\sigma\left(  \operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  \underbrace{\sigma^{-1}\left(
q\right)  }_{=i_{p}}\right)  \right)  =\sigma\left(
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
i_{p}\right)  }_{\substack{=i_{p+1}\\\text{(by
(\ref{sol.perm.cycles.short.a.cyc-manifest.1}))}}}\right)  =\sigma\left(
i_{p+1}\right)  .
\]
Compared with%
\begin{align*}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  \underbrace{q}%
_{=j_{p}}\right)   &  =\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}\left(  j_{p}\right)  =j_{p+1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.short.a.cyc-manifest.1'})}\right) \\
&  =\sigma\left(  i_{p+1}\right)  ,
\end{align*}
this yields $\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}\right)  \left(  q\right)
=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)  $.
Thus, (\ref{sol.perm.cycles.short.a.qq}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $q\notin\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Hence, $\sigma^{-1}\left(  q\right)
\notin\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $\sigma
^{-1}\left(  q\right)  \in\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $. In
other words, there exists a $p\in\left\{  1,2,\ldots,k\right\}  $ such that
$\sigma^{-1}\left(  q\right)  =i_{p}$. Consider this $p$. We have $\sigma
^{-1}\left(  q\right)  =i_{p}$, thus $q=\sigma\left(  i_{p}\right)  =j_{p}$
(since $j_{p}$ is defined as $\sigma\left(  i_{p}\right)  $). Thus,
$q=j_{p}\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $; but this contradicts
$q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $. This contradiction shows
that our assumption was wrong, qed.}. Thus, $\sigma^{-1}\left(  q\right)
\in\left[  n\right]  \setminus\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $.
Therefore, (\ref{sol.perm.cycles.short.a.cyc-manifest.2}) (applied to
$\sigma^{-1}\left(  q\right)  $ instead of $q$) yields $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  \sigma^{-1}\left(  q\right)
\right)  =\sigma^{-1}\left(  q\right)  $. Hence,%
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\sigma\left(
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
\sigma^{-1}\left(  q\right)  \right)  }_{=\sigma^{-1}\left(  q\right)
}\right)  =\sigma\left(  \sigma^{-1}\left(  q\right)  \right)  =q.
\label{sol.perm.cycles.short.a.qq.pf.5}%
\end{equation}
On the other hand, $q\in\left[  n\right]  \setminus\left\{  j_{1},j_{2}%
,\ldots,j_{k}\right\}  $ (since $q\notin\left\{  j_{1},j_{2},\ldots
,j_{k}\right\}  $) and thus $\operatorname*{cyc}\nolimits_{j_{1},j_{2}%
,\ldots,j_{k}}\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.short.a.cyc-manifest.2'})). Compared with
(\ref{sol.perm.cycles.short.a.qq.pf.5}), this yields $\left(  \sigma
\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma
^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}\left(  q\right)  $. Thus,
(\ref{sol.perm.cycles.short.a.qq}) is proven in Case 2.

We have now proven (\ref{sol.perm.cycles.short.a.qq}) in each of the two Cases
1 and 2. Therefore, (\ref{sol.perm.cycles.short.a.qq}) always holds.

From this, we conclude that $\sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}=\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}$ (because both $\sigma\circ\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma^{-1}$ and $\operatorname*{cyc}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ are maps from $\left[  n\right]  $ to
$\left[  n\right]  $). Hence,%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}=\operatorname*{cyc}\nolimits_{\sigma\left(  i_{1}\right)  ,\sigma\left(
i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  }%
\]
(since $\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(
i_{1}\right)  ,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)
\right)  $). This solves Exercise \ref{exe.perm.cycles} \textbf{(a)}.

\textbf{(b)} This is again a straightforward argument whose complexity stems
only from the number of cases that need to be considered. We shall try to
reduce the amount of brainless verification using some tricks, although at the
cost of making parts of the solution appear unmotivated.

Let $p\in\left\{  0,1,\ldots,n-k\right\}  $. Then, the elements
$p+1,p+2,\ldots,p+k$ belong to $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $. Hence, $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is
well-defined. We let $\sigma$ denote the permutation $\operatorname*{cyc}%
\nolimits_{p+1,p+2,\ldots,p+k}$.

The permutation $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is
defined to be the permutation in $S_{n}$ which sends $p+1,p+2,\ldots,p+k$ to
$p+2,p+3,\ldots,p+k,p+1$, respectively, while leaving all other elements of
$\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\sigma$ sends $p+1,p+2,\ldots,p+k$ to $p+2,p+3,\ldots
,p+k,p+1$, respectively. In other words, we have%
\begin{equation}
\left(  \sigma\left(  p+i\right)  =p+\left(  i+1\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,k-1\right\}
\right)  \label{sol.perm.cycles.short.b.sigma-manifest.1a}%
\end{equation}
and%
\begin{equation}
\sigma\left(  p+k\right)  =p+1.
\label{sol.perm.cycles.short.b.sigma-manifest.1b}%
\end{equation}


\item The permutation $\sigma$ leaves all other elements of $\left[  n\right]
$ fixed (where \textquotedblleft other\textquotedblright\ means
\textquotedblleft other than $p+1,p+2,\ldots,p+k$\textquotedblright). In other
words,
\begin{equation}
\sigma\left(  q\right)  =q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \setminus\left\{  p+1,p+2,\ldots,p+k\right\}  .
\label{sol.perm.cycles.short.b.sigma-manifest.2}%
\end{equation}

\end{itemize}

We can now observe that
\begin{equation}
\sigma\left(  q\right)  \geq q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \text{ satisfying }q\neq p+k \label{sol.perm.cycles.short.b.sigma.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.short.b.sigma.1}):} Let
$q\in\left[  n\right]  $ be such that $q\neq p+k$. We must prove that
$\sigma\left(  q\right)  \geq q$.
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $q\in\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
\textit{Case 2:} We have $q\notin\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
Let us first consider Case 1. In this case, we have $q\in\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q=p+i$ for some $i\in\left\{
1,2,\ldots,k\right\}  $. Consider this $i$. We have $q\neq p+k$, so that
$q-p\neq k$ and thus $k\neq q-p=i$ (since $q=p+i$). Therefore, $i\neq k$.
Combined with $i\in\left\{  1,2,\ldots,k\right\}  $, this shows that
$i\in\left\{  1,2,\ldots,k\right\}  \setminus\left\{  k\right\}  =\left\{
1,2,\ldots,k-1\right\}  $. Hence,
(\ref{sol.perm.cycles.short.b.sigma-manifest.1a}) yields $\sigma\left(
p+i\right)  =p+\underbrace{\left(  i+1\right)  }_{\geq i}\geq p+i=q$. Hence,
$\sigma\left(  \underbrace{q}_{=p+i}\right)  =\sigma\left(  p+i\right)  \geq
q$. We thus have proven $\sigma\left(  q\right)  \geq q$ in Case 1.
\par
Let us now consider Case 2. In this case, we have $q\notin\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $. Therefore, $\sigma\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.short.b.sigma-manifest.2})). Thus, $\sigma\left(
q\right)  \geq q$ is proven in Case 2.
\par
We have now proven $\sigma\left(  q\right)  \geq q$ in both Cases 1 and 2.
Therefore, $\sigma\left(  q\right)  \geq q$ always holds. This proves
(\ref{sol.perm.cycles.short.b.sigma.1}).}. Furthermore,%
\begin{equation}
\sigma\left(  q\right)  \leq q+1\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left[  n\right]  \label{sol.perm.cycles.short.b.sigma.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.short.b.sigma.2}):} Let
$q\in\left[  n\right]  $. We must prove that $\sigma\left(  q\right)  \leq
q+1$.
\par
Assume the contrary (for the sake of contradiction). Thus, $\sigma\left(
q\right)  >q+1$. If we had $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $, then we would have $\sigma\left(  q\right)  =q$
(by (\ref{sol.perm.cycles.short.b.sigma-manifest.2})), which would contradict
$\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have $q\in\left[  n\right]
\setminus\left\{  p+1,p+2,\ldots,p+k\right\}  $. We therefore have%
\[
q\in\left[  n\right]  \setminus\left(  \left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  \right)  \subseteq\left\{  p+1,p+2,\ldots
,p+k\right\}  .
\]
Hence, $q=p+i$ for some $i\in\left\{  1,2,\ldots,k\right\}  $. Consider this
$i$. Clearly, $i\geq1$ and $i\leq k$.
\par
If we had $i=k$, then we would have%
\begin{align*}
\sigma\left(  \underbrace{q}_{=p+i}\right)   &  =\sigma\left(
p+\underbrace{i}_{=k}\right)  =\sigma\left(  p+k\right)  =p+\underbrace{1}%
_{\leq i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.short.b.sigma-manifest.1b})}\right) \\
&  \leq p+i=q,
\end{align*}
which would contradict $\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have
$i=k$. Hence, $i\in\left\{  1,2,\ldots,k-1\right\}  $ (since $i\geq1$ and
$i\leq k$). Therefore, (\ref{sol.perm.cycles.short.b.sigma-manifest.1a})
yields $\sigma\left(  p+i\right)  =p+\left(  i+1\right)  =\underbrace{p+i}%
_{=1}+1=q+1$. This contradicts $\sigma\left(  \underbrace{p+i}_{=q}\right)
=\sigma\left(  q\right)  >q+1$. This contradiction proves that our assumption
was wrong. Hence, $\sigma\left(  q\right)  \leq q+1$ is proven.}.

Now, set%
\[
A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  .
\]
In other words,%
\[
A=\left\{  \left(  p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(
p+\left(  k-1\right)  ,p+k\right)  \right\}  .
\]
Thus, the set $A$ has $k-1$ elements (since the $k-1$ pairs \newline$\left(
p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(  p+\left(  k-1\right)
,p+k\right)  $ are clearly distinct). In other words, $\left\vert A\right\vert
=k-1$.

Now, let $\operatorname*{Inv}\sigma$ denote the set of all inversions of
$\sigma$. Recall that $\ell\left(  \sigma\right)  $ was defined as the number
of inversions of $\sigma$. In other words, $\ell\left(  \sigma\right)
=\left\vert \operatorname*{Inv}\sigma\right\vert $.

But $A\subseteq\operatorname*{Inv}\sigma$\ \ \ \ \footnote{\textit{Proof.} Let
$c\in A$. We shall show that $c\in\operatorname*{Inv}\sigma$.
\par
We have $c\in A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{
1,2,\ldots,k-1\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  p+h,p+k\right)  $ for some $h\in\left\{  1,2,\ldots,k-1\right\}  $.
Consider this $h$. From $h\in\left\{  1,2,\ldots,k-1\right\}  $, we obtain
$1\leq h\leq k-1$, so that $h\leq k-1<k$. Thus, $p+\underbrace{h}_{<k}<p+k$.
Moreover, $1\leq p+h$ (since $\underbrace{p}_{\geq0}+\underbrace{h}_{\geq
1}\geq0+1=1$) and $p+k\leq n$ (since $p\leq n-k$). Thus, $1\leq p+h<p+k\leq
n$.
\par
Applying (\ref{sol.perm.cycles.short.b.sigma-manifest.1a}) to $i=h$, we obtain
$\sigma\left(  p+h\right)  =p+\left(  \underbrace{h}_{\geq1>0}+1\right)
>p+\left(  0+1\right)  =p+1=\sigma\left(  p+k\right)  $ (by
(\ref{sol.perm.cycles.short.b.sigma-manifest.1b})).
\par
Now, $\left(  p+h,p+k\right)  $ is a pair of integers satisfying $1\leq
p+h<p+k\leq n$ and $\sigma\left(  p+h\right)  >\sigma\left(  p+k\right)  $. In
other words, $\left(  p+h,p+k\right)  $ is a pair of integers $\left(
i,j\right)  $ satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. In other words, $\left(  p+h,p+k\right)  $ is an
inversion of $\sigma$ (by the definition of \textquotedblleft inversion of
$\sigma$\textquotedblright). In other words, $\left(  p+h,p+k\right)
\in\operatorname*{Inv}\sigma$. Thus, $c=\left(  p+h,p+k\right)  \in
\operatorname*{Inv}\sigma$.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\sigma$ for every $c\in A$. In other words, $A\subseteq
\operatorname*{Inv}\sigma$, qed.} and $\operatorname*{Inv}\sigma\subseteq
A$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\sigma$. We
shall show that $c\in A$.
\par
We have $c\in\operatorname*{Inv}\sigma$. In other words, $c$ is an inversion
of $\sigma$. In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. Consider this $\left(  i,j\right)  $.
\par
We have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, so that
$\sigma\left(  i\right)  \geq\sigma\left(  j\right)  +1$ (since $\sigma\left(
i\right)  $ and $\sigma\left(  j\right)  $ are integers). Also, $i<j$, so that
$i\leq j-1$ (since $i$ and $j$ are integers). In other words, $i+1\leq j$. But
(\ref{sol.perm.cycles.short.b.sigma.2}) (applied to $q=i$) yields
$\sigma\left(  i\right)  \leq i+1\leq j$.
\par
Let us first show that $j=p+k$. Indeed, let us assume the contrary (for the
sake of contradiction). Thus, $j\neq p+k$. Hence, $\sigma\left(  j\right)
\geq j$ (by (\ref{sol.perm.cycles.short.b.sigma.1}), applied to $q=j$). Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  \geq j$. This contradicts
$\sigma\left(  i\right)  \leq j$. This contradiction shows that our assumption
was wrong. Hence, $j=p+k$ is proven.
\par
Now, $\sigma\left(  \underbrace{j}_{=p+k}\right)  =\sigma\left(  p+k\right)
=p+1$ (by (\ref{sol.perm.cycles.short.b.sigma-manifest.1b})). Hence,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  =p+1$. Therefore,
$p+1<\sigma\left(  i\right)  \leq i+1$. Subtracting $1$ from both sides of
this inequality, we obtain $p<i$. Hence, $i>p$, so that $i\geq p+1$ (since $i$
and $p$ are integers). Combined with $i<j=p+k$, this yields $i\in\left\{
p+1,p+2,\ldots,p+k-1\right\}  $. Thus, $i-p\in\left\{  1,2,\ldots,k-1\right\}
$.
\par
So we know that the element $i-p\in\left\{  1,2,\ldots,k-1\right\}  $
satisfies $c=\left(  \underbrace{i}_{=p+\left(  i-p\right)  },\underbrace{j}%
_{=p+k}\right)  =\left(  p+\left(  i-p\right)  ,p+k\right)  $. Hence, there
exists an $h\in\left\{  1,2,\ldots,k-1\right\}  $ such that $c=\left(
p+h,p+k\right)  $ (namely, $h=i-p$). Thus,%
\[
c\in\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  =A.
\]
\par
Now, let us forget that we fixed $c$. We thus have proven that $c\in A$ for
every $c\in\operatorname*{Inv}\sigma$. In other words, $\operatorname*{Inv}%
\sigma\subseteq A$, qed.}. Combining these two relations, we obtain
$A=\operatorname*{Inv}\sigma$. Hence, $\left\vert A\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert $. Compared with $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert $, this yields
$\ell\left(  \sigma\right)  =\left\vert A\right\vert =k-1$. This rewrites as
$\ell\left(  \operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\right)  =k-1$
(since $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$). This
solves Exercise \ref{exe.perm.cycles} \textbf{(b)}.

\textbf{(c)} This one is tricky. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$
distinct elements of $\left[  n\right]  $. We extend the $k$-tuple $\left(
i_{1},i_{2},\ldots,i_{k}\right)  $ to an infinite sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ of elements of $\left[  n\right]  $ by
setting%
\[
\left(  i_{u}=i_{\left(  \text{the element }u^{\prime}\in\left\{
1,2,\ldots,k\right\}  \text{ satisfying }u^{\prime}\equiv u\operatorname{mod}%
k\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1\right)  .
\]
This sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic with
period $k$. In other words,%
\begin{equation}
i_{u}=i_{u+k}\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1.
\label{sol.perm.cycles.short.c.period}%
\end{equation}
From this, it is easy to obtain that%
\[
i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}+\cdots
+i_{k+2}=\cdots
\]
\footnote{\textit{Proof.} Every $u\in\left\{  1,2,3,\ldots\right\}  $
satisfies%
\begin{align*}
i_{u}+i_{u+1}+\cdots+i_{u+k-1}  &  =\underbrace{i_{u}}_{\substack{=i_{u+k}%
\\\text{(by (\ref{sol.perm.cycles.short.c.period}))}}}+\left(  i_{u+1}%
+i_{u+2}+\cdots+i_{u+k-1}\right) \\
&  =i_{u+k}+\left(  i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  =\left(
i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  +i_{u+k}\\
&  =i_{u+1}+i_{u+2}+\cdots+i_{u+k}.
\end{align*}
Thus, $i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}%
+\cdots+i_{k+2}=\cdots$, qed.}. Thus,%
\begin{equation}
i_{r+1}+i_{r+2}+\cdots+i_{r+k}=i_{1}+i_{2}+\cdots+i_{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }r\in\mathbb{N}.
\label{sol.perm.cycles.short.c.periodsum}%
\end{equation}


Let $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$. Let
$\operatorname*{Inv}\sigma$ denote the set of all inversions of $\sigma$.
Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $. (This can be seen as in the solution to Exercise
\ref{exe.perm.cycles} \textbf{(b)}.) Moreover, the definitions of the sequence
$\left(  i_{1},i_{2},i_{3},\ldots\right)  $ and of $\sigma$ show that%
\begin{equation}
\sigma\left(  i_{p}\right)  =i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\geq1. \label{sol.perm.cycles.short.c.sigma}%
\end{equation}


Now, fix $r\in\left\{  1,2,\ldots,k-1\right\}  $. We shall prove that
\begin{equation}
\text{there exists some }u\geq1\text{ such that }\left(  i_{u},i_{u+r}\right)
\in\operatorname*{Inv}\sigma. \label{sol.perm.cycles.short.c.mainclaim}%
\end{equation}


\textit{Proof of (\ref{sol.perm.cycles.short.c.mainclaim}):} Let us (for the
sake of contradiction) assume the contrary. Thus, there exists no
$u\in\mathbb{Z}$ such that $\left(  i_{u},i_{u+r}\right)  \in
\operatorname*{Inv}\sigma$.

Consider the infinite sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2}%
,i_{3+r}-i_{3},\ldots\right)  $. This sequence is periodic with period $k$
(because the sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic
with period $k$) and all its entries are nonzero (because the $k$ elements
$i_{1},i_{2},\ldots,i_{k}$ are distinct, and thus every $u\geq1$ satisfies
$i_{u+r}-i_{u}\neq0$). Thus, each entry of this sequence is either positive or
negative. In other words, every $u\geq1$ satisfies%
\begin{equation}
\text{either }i_{\left(  u+1\right)  +r}-i_{u+1}>0\text{ or }i_{\left(
u+1\right)  +r}-i_{u+1}<0. \label{sol.perm.cycles.short.c.mainclaim.pf.4}%
\end{equation}
The sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2},i_{3+r}-i_{3},\ldots\right)
$ contains at least one positive entry\footnote{\textit{Proof.} Assume the
contrary. Thus, the sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2}%
,i_{3+r}-i_{3},\ldots\right)  $ contains no positive entries. Therefore, this
sequence must consist of negative entries only (because we know that each
entry of this sequence is either positive or negative). In other words,
$i_{u+r}-i_{u}<0$ for every $u\geq1$. Hence,%
\[
\sum_{u=1}^{k}\underbrace{\left(  i_{u+r}-i_{u}\right)  }_{<0}<\sum_{u=1}%
^{k}0=0.
\]
But this contradicts%
\begin{align*}
\sum_{u=1}^{k}\left(  \underbrace{i_{u+r}}_{=i_{r+u}}-i_{u}\right)   &
=\sum_{u=1}^{k}\left(  i_{r+u}-i_{u}\right)  =\left(  \sum_{u=1}^{k}%
i_{r+u}\right)  -\left(  \sum_{u=1}^{k}i_{u}\right) \\
&  =\left(  i_{r+1}+i_{r+2}+\cdots+i_{r+k}\right)  -\left(  i_{1}+i_{2}%
+\cdots+i_{k}\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.short.c.periodsum})}\right)  .
\end{align*}
This contradiction shows that our assumption was wrong, qed.}, and at least
one negative entry\footnote{This is proven similarly.}. Hence, there exists at
least one $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(  u+1\right)
+r}-i_{u+1}<0$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then,
there exists no $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(
u+1\right)  +r}-i_{u+1}<0$. Hence,%
\begin{equation}
\text{every }u\geq1\text{ satisfying }i_{u+r}-i_{u}>0\text{ must satisfy
}i_{\left(  u+1\right)  +r}-i_{u+r}>0
\label{sol.perm.cycles.short.c.mainclaim.pf.5}%
\end{equation}
(because of (\ref{sol.perm.cycles.short.c.mainclaim.pf.4})).
\par
But there exists some $v\geq1$ satisfying $i_{v+r}-i_{v}>0$ (since the
sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2},i_{3+r}-i_{3},\ldots\right)  $
contains at least one positive entry). Consider this $v$. Then, we have
$i_{v+r}-i_{v}>0$, therefore $i_{\left(  v+1\right)  +r}-i_{v+1}>0$ (by
(\ref{sol.perm.cycles.short.c.mainclaim.pf.5}), applied to $u=v$), therefore
$i_{\left(  v+2\right)  +r}-i_{v+2}>0$ (by
(\ref{sol.perm.cycles.short.c.mainclaim.pf.5}), applied to $u=v+1$), therefore
$i_{\left(  v+3\right)  +r}-i_{v+3}>0$ (by
(\ref{sol.perm.cycles.short.c.mainclaim.pf.5}), applied to $u=v+2$), and so
on. Altogether, we thus obtain
\[
i_{h+r}-i_{h}>0\ \ \ \ \ \ \ \ \ \ \text{for every }h\geq v.
\]
In other words, $i_{h}<i_{h+r}$ for every $h\geq v$. Hence, $i_{v}%
<i_{v+r}<i_{v+2r}<i_{v+3r}<\cdots$. Thus, the numbers $i_{v},i_{v+r}%
,i_{v+2r},i_{v+3r},\ldots$ are pairwise distinct; hence, the sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ contains infinitely many distinct entries.
But this contradicts the fact that this sequence is periodic. This
contradiction proves that our assumption was wrong, qed.}. Consider this $u$.
We have $i_{u}<i_{u+r}$ (since $i_{u+r}-i_{u}>0$), so that $1\leq
i_{u}<i_{u+r}\leq n$. Also, (\ref{sol.perm.cycles.short.c.sigma}) (applied to
$p=u$) yields $\sigma\left(  i_{u}\right)  =i_{u+1}$. Moreover,
(\ref{sol.perm.cycles.short.c.sigma}) (applied to $p=u+r$) yields
$\sigma\left(  i_{u+r}\right)  =i_{u+r+1}=i_{\left(  u+1\right)  +r}$. Hence,
\begin{align*}
\sigma\left(  i_{u}\right)   &  =i_{u+1}>i_{\left(  u+1\right)  +r}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i_{\left(  u+1\right)  +r}%
-i_{u+1}<0\right) \\
&  =\sigma\left(  i_{u+r}\right)  .
\end{align*}
So we know that $\left(  i_{u},i_{u+r}\right)  $ is a pair of integers
satisfying $1\leq i_{u}<i_{u+r}\leq n$ and $\sigma\left(  i_{u}\right)
>\sigma\left(  i_{u+r}\right)  $. In other words, $\left(  i_{u}%
,i_{u+r}\right)  $ is an inversion of $\sigma$. In other words, $\left(
i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Thus, we have found a
$u\geq1$ such that $\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}%
\sigma$. This proves (\ref{sol.perm.cycles.short.c.mainclaim}).

Now, let us forget that we fixed $r$. We have shown that, for every
$r\in\left\{  1,2,\ldots,k-1\right\}  $, there exists some $u\geq1$ such that
$\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Let us denote
this $u$ by $u_{r}$. Therefore, for every $r\in\left\{  1,2,\ldots
,k-1\right\}  $, we have found a $u_{r}\geq1$ such that $\left(  i_{u_{r}%
},i_{u_{r}+r}\right)  \in\operatorname*{Inv}\sigma$. The $k-1$ pairs%
\[
\left(  i_{u_{1}},i_{u_{1}+1}\right)  ,\ \left(  i_{u_{2}},i_{u_{2}+2}\right)
,\ \ldots,\ \left(  i_{u_{k-1}},i_{u_{k-1}+\left(  k-1\right)  }\right)
\]
are pairwise distinct\footnote{\textit{Proof.} Assume the contrary. Then,
there exist two distinct elements $x$ and $y$ of $\left\{  1,2,\ldots
,k-1\right\}  $ such that $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(
i_{u_{y}},i_{u_{y}+y}\right)  $. Consider these $x$ and $y$.
\par
We have $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(  i_{u_{y}},i_{u_{y}%
+y}\right)  $. In other words, $i_{u_{x}}=i_{u_{y}}$ and $i_{u_{x}+x}%
=i_{u_{y}+y}$. Since the numbers $i_{1},i_{2},\ldots,i_{k}$ are distinct (and
the sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ consists of these
numbers, repeated over and over), we obtain $u_{x}\equiv u_{y}%
\operatorname{mod}k$ from $i_{u_{x}}=i_{u_{y}}$, and we obtain $u_{x}+x\equiv
u_{y}+y\operatorname{mod}k$ from $i_{u_{x}+x}=i_{u_{y}+y}$. Subtracting the
congruence $u_{x}\equiv u_{y}\operatorname{mod}k$ from the congruence
$u_{x}+x\equiv u_{y}+y\operatorname{mod}k$, we obtain $x\equiv
y\operatorname{mod}k$. In light of $x,y\in\left\{  1,2,\ldots,k-1\right\}  $,
this shows that $x=y$. But this contradicts the fact that $x$ and $y$ are
distinct. This contradiction proves that our assumption was wrong, qed.}, and
all of them belong to $\operatorname*{Inv}\sigma$. Hence, the set
$\operatorname*{Inv}\sigma$ has at least $k-1$ elements. In other words,
$\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$. Thus, $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$.
Since $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$, this
rewrites as $\ell\left(  \operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}\right)  \geq k-1$. This solves Exercise \ref{exe.perm.cycles}
\textbf{(c)}.

\textbf{(d)} Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of
$\left[  n\right]  $. Hence, Proposition \ref{prop.perms.lists} \textbf{(c)}
(applied to $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  $) yields that there exists a permutation
$\sigma\in S_{n}$ such that $\left(  i_{1},i_{2},\ldots,i_{k}\right)  =\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
k\right)  \right)  $. Consider such a $\sigma$.

Exercise \ref{exe.perm.cycles} \textbf{(b)} yields $\ell\left(
\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)  =k-1$. But the definition
of $\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}$ yields
$\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}=\left(
-1\right)  ^{\ell\left(  \operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)
}=\left(  -1\right)  ^{k-1}$ (since $\ell\left(  \operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}\right)  =k-1$).

Exercise \ref{exe.perm.cycles} \textbf{(a)} (applied to $1,2,\ldots,k$ instead
of $i_{1},i_{2},\ldots,i_{k}$) yields%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma
^{-1}=\operatorname*{cyc}\nolimits_{\sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  k\right)  }=\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
\]
(since $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  k\right)  \right)  =\left(  i_{1},i_{2},\ldots
,i_{k}\right)  $). Hence,
\[
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}%
_{=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma^{-1}%
}\circ\sigma=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}%
\circ\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}}=\sigma
\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}.
\]
Thus,%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\sigma\circ\operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}}=\left(  -1\right)  ^{\sigma}\cdot\underbrace{\left(
-1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}}_{=\left(
-1\right)  ^{k-1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
\tau=\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{k-1}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}}\cdot\left(  -1\right)  ^{\sigma}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\text{ and }%
\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}},
\end{align*}
this yields $\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{k-1}$. We can cancel $\left(  -1\right)
^{\sigma}$ from this equality (since $\left(  -1\right)  ^{\sigma}\in\left\{
1,-1\right\}  $ is a nonzero integer), and thus obtain $\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{k-1}$. This solves Exercise \ref{exe.perm.cycles} \textbf{(d)}.

[\textit{Remark:} Exercise \ref{exe.perm.cycles} \textbf{(d)} can also be
solved in a different way, namely by arguing that%
\[
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}=t_{i_{1},i_{2}}\circ
t_{i_{2},i_{3}}\circ\cdots\circ t_{i_{k-1},i_{k}}%
\]
(using the notations of Definition \ref{def.transpos}) and then by applying
the equality $\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ multiple times. We leave the details
of this alternative proof to the curious reader. (That said, this alternative
proof is also the most popular proof, so it is easily found in textbooks.)]
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.perm.cycles}.]\textbf{(a)} Let $\sigma\in
S_{n}$. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of $\left[
n\right]  $.

The map $\sigma$ is a permutation (since $\sigma\in S_{n}$), and therefore
bijective. Hence, in particular, $\sigma$ is injective.

For every $p\in\left\{  1,2,\ldots,k\right\}  $, let $j_{p}$ be the element
$\sigma\left(  i_{p}\right)  \in\left[  n\right]  $. Then, $\left(
j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $.
Furthermore, $j_{1},j_{2},\ldots,j_{k}$ are $k$ distinct elements of $\left[
n\right]  $\ \ \ \ \footnote{\textit{Proof.} The map $\sigma$ is injective.
Hence, the elements $\sigma\left(  i_{1}\right)  ,\sigma\left(  i_{2}\right)
,\ldots,\sigma\left(  i_{k}\right)  $ are distinct (since the elements
$i_{1},i_{2},\ldots,i_{k}$ are distinct). Thus, $\sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  $ are $k$
distinct elements of $\left[  n\right]  $. In other words, $j_{1},j_{2}%
,\ldots,j_{k}$ are $k$ distinct elements of $\left[  n\right]  $ (since
$\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $%
).}. Therefore, $\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ is a
well-defined permutation in $S_{n}$.

We have defined $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ to
be the permutation in $S_{n}$ which sends $i_{1},i_{2},\ldots,i_{k}$ to
$i_{2},i_{3},\ldots,i_{k},i_{1}$, respectively, while leaving all other
elements of $\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ sends $i_{1},i_{2},\ldots,i_{k}$ to $i_{2},i_{3},\ldots,i_{k},i_{1}$,
respectively. In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  i_{p}\right)
=i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.a.cyc-manifest.1}%
\end{equation}
where $i_{k+1}$ means $i_{1}$.

\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ leaves all other elements of $\left[  n\right]  $ fixed (where
\textquotedblleft other\textquotedblright\ means \textquotedblleft other than
$i_{1},i_{2},\ldots,i_{k}$\textquotedblright). In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
i_{1},i_{2},\ldots,i_{k}\right\}  . \label{sol.perm.cycles.a.cyc-manifest.2}%
\end{equation}

\end{itemize}

Furthermore:

\begin{itemize}
\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  j_{p}\right)
=j_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.a.cyc-manifest.1'}%
\end{equation}
where $j_{k+1}$ means $j_{1}$. (This is proven in the same way as
(\ref{sol.perm.cycles.a.cyc-manifest.1}), except that every $i_{x}$ is
replaced by $j_{x}$.)

\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
j_{1},j_{2},\ldots,j_{k}\right\}  . \label{sol.perm.cycles.a.cyc-manifest.2'}%
\end{equation}
(This is proven in the same way as (\ref{sol.perm.cycles.a.cyc-manifest.2}),
except that every $i_{x}$ is replaced by $j_{x}$.)
\end{itemize}

In the following, we shall use the notation $i_{k+1}$ as a synonym for $i_{1}%
$, and the notation $j_{k+1}$ as a synonym for $j_{1}$. Then,%
\begin{equation}
j_{p}=\sigma\left(  i_{p}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,k+1\right\}  \label{sol.perm.cycles.a.jp}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.a.jp}):} Let $p\in\left\{
1,2,\ldots,k+1\right\}  $. We need to prove that $j_{p}=\sigma\left(
i_{p}\right)  $. If $p\in\left\{  1,2,\ldots,k\right\}  $, then this follows
from the definition of $j_{p}$. Hence, for the rest of this proof, we can WLOG
assume that we don't have $p\in\left\{  1,2,\ldots,k\right\}  $. Assume this.
\par
We have $p\in\left\{  1,2,\ldots,k+1\right\}  $, but we don't have
$p\in\left\{  1,2,\ldots,k\right\}  $. Hence, we have $p\in\left\{
1,2,\ldots,k+1\right\}  \setminus\left\{  1,2,\ldots,k\right\}  =\left\{
k+1\right\}  $. In other words, $p=k+1$, so that $i_{p}=i_{k+1}=i_{1}$ and
thus $\sigma\left(  i_{p}\right)  =\sigma\left(  i_{1}\right)  $. On the other
hand, from $p=k+1$, we obtain $j_{p}=j_{k+1}=j_{1}=\sigma\left(  i_{1}\right)
$ (by the definition of $j_{1}$). Compared with $\sigma\left(  i_{p}\right)
=\sigma\left(  i_{1}\right)  $, this yields $j_{p}=\sigma\left(  i_{p}\right)
$. Thus, $j_{p}=\sigma\left(  i_{p}\right)  $ is proven.}.

Now, let us show that
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
\label{sol.perm.cycles.a.qq}%
\end{equation}
for every $q\in\left[  n\right]  $.

\textit{Proof of (\ref{sol.perm.cycles.a.qq}):} Let $q\in\left[  n\right]  $.
We must prove (\ref{sol.perm.cycles.a.qq}). We are in one of the following two cases:

\textit{Case 1:} We have $q\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

\textit{Case 2:} We have $q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

Let us first consider Case 1. In this case, we have $q\in\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Thus, $q=j_{p}$ for some $p\in\left\{
1,2,\ldots,k\right\}  $. Consider this $p$. Clearly, $p+1\in\left\{
2,3,\ldots,k+1\right\}  \subseteq\left\{  1,2,\ldots,k+1\right\}  $. Hence,
applying (\ref{sol.perm.cycles.a.jp}) to $p+1$ instead of $p$, we obtain
$j_{p+1}=\sigma\left(  i_{p+1}\right)  $. But $q=j_{p}=\sigma\left(
i_{p}\right)  $ (by the definition of $j_{p}$) and thus $\sigma^{-1}\left(
q\right)  =i_{p}$. Hence,
\[
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\sigma\left(  \operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  \underbrace{\sigma^{-1}\left(
q\right)  }_{=i_{p}}\right)  \right)  =\sigma\left(
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
i_{p}\right)  }_{\substack{=i_{p+1}\\\text{(by
(\ref{sol.perm.cycles.a.cyc-manifest.1}))}}}\right)  =\sigma\left(
i_{p+1}\right)  .
\]
Compared with%
\begin{align*}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  \underbrace{q}%
_{=j_{p}}\right)   &  =\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}\left(  j_{p}\right)  =j_{p+1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.a.cyc-manifest.1'})}\right) \\
&  =\sigma\left(  i_{p+1}\right)  ,
\end{align*}
this yields $\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}\right)  \left(  q\right)
=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)  $.
Thus, (\ref{sol.perm.cycles.a.qq}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $q\notin\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Hence, $\sigma^{-1}\left(  q\right)
\notin\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $\sigma
^{-1}\left(  q\right)  \in\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $. In
other words, there exists a $p\in\left\{  1,2,\ldots,k\right\}  $ such that
$\sigma^{-1}\left(  q\right)  =i_{p}$. Consider this $p$. We have $\sigma
^{-1}\left(  q\right)  =i_{p}$, thus $q=\sigma\left(  i_{p}\right)  =j_{p}$
(since $j_{p}$ is defined as $\sigma\left(  i_{p}\right)  $). Thus,
$q=j_{p}\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $; but this contradicts
$q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $. This contradiction shows
that our assumption was wrong, qed.}. Thus, $\sigma^{-1}\left(  q\right)
\in\left[  n\right]  \setminus\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $.
Therefore, (\ref{sol.perm.cycles.a.cyc-manifest.2}) (applied to $\sigma
^{-1}\left(  q\right)  $ instead of $q$) yields $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  \sigma^{-1}\left(  q\right)
\right)  =\sigma^{-1}\left(  q\right)  $. Hence,%
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\sigma\left(
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
\sigma^{-1}\left(  q\right)  \right)  }_{=\sigma^{-1}\left(  q\right)
}\right)  =\sigma\left(  \sigma^{-1}\left(  q\right)  \right)  =q.
\label{sol.perm.cycles.a.qq.pf.5}%
\end{equation}
On the other hand, $q\in\left[  n\right]  \setminus\left\{  j_{1},j_{2}%
,\ldots,j_{k}\right\}  $ (since $q\notin\left\{  j_{1},j_{2},\ldots
,j_{k}\right\}  $) and thus $\operatorname*{cyc}\nolimits_{j_{1},j_{2}%
,\ldots,j_{k}}\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.a.cyc-manifest.2'})). Compared with
(\ref{sol.perm.cycles.a.qq.pf.5}), this yields $\left(  \sigma\circ
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma
^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}\left(  q\right)  $. Thus, (\ref{sol.perm.cycles.a.qq}) is
proven in Case 2.

We have now proven (\ref{sol.perm.cycles.a.qq}) in each of the two Cases 1 and
2. Therefore, (\ref{sol.perm.cycles.a.qq}) always holds.

Thus, we know that $\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}\right)  \left(  q\right)
=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)  $
for every $q\in\left[  n\right]  $. In other words, $\sigma\circ
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma
^{-1}=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ (because both
$\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}$ and $\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ are
maps from $\left[  n\right]  $ to $\left[  n\right]  $). Hence,%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}=\operatorname*{cyc}\nolimits_{\sigma\left(  i_{1}\right)  ,\sigma\left(
i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  }%
\]
(since $\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(
i_{1}\right)  ,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)
\right)  $). This solves Exercise \ref{exe.perm.cycles} \textbf{(a)}.

\textbf{(b)} Let $p\in\left\{  0,1,\ldots,n-k\right\}  $. Then, $p\geq0$ and
$p\leq n-k$. Hence, $\left\{  p+1,p+2,\ldots,p+k\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $ (since $\underbrace{p}_{\geq0}+1\geq1$ and
$\underbrace{p}_{\leq n-k}+k\leq n-k+k=n$). Thus, the elements $p+1,p+2,\ldots
,p+k$ belong to $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence,
$\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is well-defined. We let
$\sigma$ denote the permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots
,p+k}$.

The permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is defined
to be the permutation in $S_{n}$ which sends $p+1,p+2,\ldots,p+k$ to
$p+2,p+3,\ldots,p+k,p+1$, respectively, while leaving all other elements of
$\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$
sends $p+1,p+2,\ldots,p+k$ to $p+2,p+3,\ldots,p+k,p+1$, respectively. In other
words, the permutation $\sigma$ sends $p+1,p+2,\ldots,p+k$ to $p+2,p+3,\ldots
,p+k,p+1$, respectively (since $\sigma=\operatorname*{cyc}%
\nolimits_{p+1,p+2,\ldots,p+k}$). In other words, we have%
\begin{equation}
\left(  \sigma\left(  p+i\right)  =p+\left(  i+1\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,k-1\right\}
\right)  \label{sol.perm.cycles.b.sigma-manifest.1a}%
\end{equation}
and%
\begin{equation}
\sigma\left(  p+k\right)  =p+1. \label{sol.perm.cycles.b.sigma-manifest.1b}%
\end{equation}


\item The permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$
leaves all other elements of $\left[  n\right]  $ fixed (where
\textquotedblleft other\textquotedblright\ means \textquotedblleft other than
$p+1,p+2,\ldots,p+k$\textquotedblright). In other words, $\operatorname*{cyc}%
\nolimits_{p+1,p+2,\ldots,p+k}\left(  q\right)  =q$ for every $q\in\left[
n\right]  \setminus\left\{  p+1,p+2,\ldots,p+k\right\}  $. In other words,%
\begin{equation}
\sigma\left(  q\right)  =q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \setminus\left\{  p+1,p+2,\ldots,p+k\right\}
\label{sol.perm.cycles.b.sigma-manifest.2}%
\end{equation}
(since $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$).
\end{itemize}

We can now observe that
\begin{equation}
\sigma\left(  q\right)  \geq q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \text{ satisfying }q\neq p+k \label{sol.perm.cycles.b.sigma.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.b.sigma.1}):} Let
$q\in\left[  n\right]  $ be such that $q\neq p+k$. We must prove that
$\sigma\left(  q\right)  \geq q$.
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $q\in\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
\textit{Case 2:} We have $q\notin\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
Let us first consider Case 1. In this case, we have $q\in\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q=p+i$ for some $i\in\left\{
1,2,\ldots,k\right\}  $. Consider this $i$. We have $q\neq p+k$, so that
$q-p\neq k$ and thus $k\neq q-p=i$ (since $q=p+i$). Therefore, $i\neq k$.
Combined with $i\in\left\{  1,2,\ldots,k\right\}  $, this shows that
$i\in\left\{  1,2,\ldots,k\right\}  \setminus\left\{  k\right\}  =\left\{
1,2,\ldots,k-1\right\}  $. Hence, (\ref{sol.perm.cycles.b.sigma-manifest.1a})
yields $\sigma\left(  p+i\right)  =p+\underbrace{\left(  i+1\right)  }_{\geq
i}\geq p+i=q$. Hence, $\sigma\left(  \underbrace{q}_{=p+i}\right)
=\sigma\left(  p+i\right)  \geq q$. We thus have proven $\sigma\left(
q\right)  \geq q$ in Case 1.
\par
Let us now consider Case 2. In this case, we have $q\notin\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $. Therefore, $\sigma\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.b.sigma-manifest.2})). Thus, $\sigma\left(  q\right)
\geq q$ is proven in Case 2.
\par
We have now proven $\sigma\left(  q\right)  \geq q$ in both Cases 1 and 2.
Therefore, $\sigma\left(  q\right)  \geq q$ always holds. This proves
(\ref{sol.perm.cycles.b.sigma.1}).}. Furthermore,%
\begin{equation}
\sigma\left(  q\right)  \leq q+1\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left[  n\right]  \label{sol.perm.cycles.b.sigma.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.b.sigma.2}):} Let
$q\in\left[  n\right]  $. We must prove that $\sigma\left(  q\right)  \leq
q+1$.
\par
Assume the contrary (for the sake of contradiction). Thus, $\sigma\left(
q\right)  >q+1$. If we had $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $, then we would have $\sigma\left(  q\right)  =q$
(by (\ref{sol.perm.cycles.b.sigma-manifest.2})), which would contradict
$\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have $q\in\left[  n\right]
\setminus\left\{  p+1,p+2,\ldots,p+k\right\}  $. We therefore have%
\[
q\in\left[  n\right]  \setminus\left(  \left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  \right)  \subseteq\left\{  p+1,p+2,\ldots
,p+k\right\}  .
\]
Hence, $q=p+i$ for some $i\in\left\{  1,2,\ldots,k\right\}  $. Consider this
$i$. Clearly, $i\geq1$ and $i\leq k$.
\par
If we had $i=k$, then we would have%
\begin{align*}
\sigma\left(  \underbrace{q}_{=p+i}\right)   &  =\sigma\left(
p+\underbrace{i}_{=k}\right)  =\sigma\left(  p+k\right)  =p+\underbrace{1}%
_{\leq i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.b.sigma-manifest.1b})}\right) \\
&  \leq p+i=q,
\end{align*}
which would contradict $\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have
$i=k$. We thus have $i\neq k$. Combined with $i\in\left\{  1,2,\ldots
,k\right\}  $, this shows that $i\in\left\{  1,2,\ldots,k\right\}
\setminus\left\{  k\right\}  =\left\{  1,2,\ldots,k-1\right\}  $. Hence,
(\ref{sol.perm.cycles.b.sigma-manifest.1a}) yields $\sigma\left(  p+i\right)
=p+\left(  i+1\right)  =\underbrace{p+i}_{=q}+1=q+1$. This contradicts
$\sigma\left(  \underbrace{p+i}_{=q}\right)  =\sigma\left(  q\right)  >q+1$.
This contradiction proves that our assumption was wrong. Hence, $\sigma\left(
q\right)  \leq q+1$ is proven. We are thus done proving
(\ref{sol.perm.cycles.b.sigma.2}).}.

Now, set%
\[
A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  .
\]
In other words,%
\[
A=\left\{  \left(  p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(
p+\left(  k-1\right)  ,p+k\right)  \right\}  .
\]
Thus, the set $A$ has $k-1$ elements (since the $k-1$ pairs \newline$\left(
p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(  p+\left(  k-1\right)
,p+k\right)  $ are clearly distinct). In other words, $\left\vert A\right\vert
=k-1$.

Now, let $\operatorname*{Inv}\sigma$ denote the set of all inversions of
$\sigma$. Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $\ \ \ \ \footnote{\textit{Proof.} Recall that $\ell\left(
\sigma\right)  $ is defined as the number of inversions of $\sigma$. Thus,%
\[
\ell\left(  \sigma\right)  =\left(  \text{the number of inversions of }%
\sigma\right)  =\left\vert \underbrace{\left(  \text{the set of all inversions
of }\sigma\right)  }_{=\operatorname*{Inv}\sigma}\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert ,
\]
qed.}.

But $A\subseteq\operatorname*{Inv}\sigma$\ \ \ \ \footnote{\textit{Proof.} Let
$c\in A$. We shall show that $c\in\operatorname*{Inv}\sigma$.
\par
We have $c\in A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{
1,2,\ldots,k-1\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  p+h,p+k\right)  $ for some $h\in\left\{  1,2,\ldots,k-1\right\}  $.
Consider this $h$. From $h\in\left\{  1,2,\ldots,k-1\right\}  $, we obtain
$1\leq h\leq k-1$, so that $h\leq k-1<k$. Thus, $p+\underbrace{h}_{<k}<p+k$.
Moreover, $1\leq p+h$ (since $\underbrace{p}_{\geq0}+\underbrace{h}_{\geq
1}\geq0+1=1$) and $p+k\leq n$ (since $p\leq n-k$). Thus, $1\leq p+h<p+k\leq
n$.
\par
Applying (\ref{sol.perm.cycles.b.sigma-manifest.1a}) to $i=h$, we obtain
$\sigma\left(  p+h\right)  =p+\left(  \underbrace{h}_{\geq1>0}+1\right)
>p+\left(  0+1\right)  =p+1=\sigma\left(  p+k\right)  $ (by
(\ref{sol.perm.cycles.b.sigma-manifest.1b})).
\par
Now, $\left(  p+h,p+k\right)  $ is a pair of integers satisfying $1\leq
p+h<p+k\leq n$ and $\sigma\left(  p+h\right)  >\sigma\left(  p+k\right)  $. In
other words, $\left(  p+h,p+k\right)  $ is a pair of integers $\left(
i,j\right)  $ satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. In other words, $\left(  p+h,p+k\right)  $ is an
inversion of $\sigma$ (by the definition of \textquotedblleft inversion of
$\sigma$\textquotedblright). In other words, $\left(  p+h,p+k\right)
\in\operatorname*{Inv}\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of
all inversions of $\sigma$). Thus, $c=\left(  p+h,p+k\right)  \in
\operatorname*{Inv}\sigma$.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\sigma$ for every $c\in A$. In other words, $A\subseteq
\operatorname*{Inv}\sigma$, qed.} and $\operatorname*{Inv}\sigma\subseteq
A$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\sigma$. We
shall show that $c\in A$.
\par
We have $c\in\operatorname*{Inv}\sigma$. In other words, $c$ is an inversion
of $\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of all inversions of
$\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. Consider this $\left(  i,j\right)  $. Thus, $c=\left(
i,j\right)  $.
\par
We have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, so that
$\sigma\left(  i\right)  \geq\sigma\left(  j\right)  +1$ (since $\sigma\left(
i\right)  $ and $\sigma\left(  j\right)  $ are integers). Also, $i<j$, so that
$i\leq j-1$ (since $i$ and $j$ are integers). In other words, $i+1\leq j$. But
(\ref{sol.perm.cycles.b.sigma.2}) (applied to $q=i$) yields $\sigma\left(
i\right)  \leq i+1\leq j$.
\par
Let us first show that $j=p+k$. Indeed, let us assume the contrary (for the
sake of contradiction). Thus, $j\neq p+k$. Hence, $\sigma\left(  j\right)
\geq j$ (by (\ref{sol.perm.cycles.b.sigma.1}), applied to $q=j$). Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  \geq j$. This contradicts
$\sigma\left(  i\right)  \leq j$. This contradiction shows that our assumption
was wrong. Hence, $j=p+k$ is proven.
\par
Now, $\sigma\left(  \underbrace{j}_{=p+k}\right)  =\sigma\left(  p+k\right)
=p+1$ (by (\ref{sol.perm.cycles.b.sigma-manifest.1b})). Hence, $\sigma\left(
i\right)  >\sigma\left(  j\right)  =p+1$. Therefore, $p+1<\sigma\left(
i\right)  \leq i+1$. Subtracting $1$ from both sides of this inequality, we
obtain $p<i$. Hence, $i>p$, so that $i\geq p+1$ (since $i$ and $p$ are
integers). Combined with $i<j=p+k$, this yields $i\in\left\{  p+1,p+2,\ldots
,p+k-1\right\}  $. Thus, $i-p\in\left\{  1,2,\ldots,k-1\right\}  $.
\par
So we know that the element $i-p\in\left\{  1,2,\ldots,k-1\right\}  $
satisfies $c=\left(  \underbrace{i}_{=p+\left(  i-p\right)  },\underbrace{j}%
_{=p+k}\right)  =\left(  p+\left(  i-p\right)  ,p+k\right)  $. Hence, there
exists an $h\in\left\{  1,2,\ldots,k-1\right\}  $ such that $c=\left(
p+h,p+k\right)  $ (namely, $h=i-p$). Thus,%
\[
c\in\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  =A.
\]
\par
Now, let us forget that we fixed $c$. We thus have proven that $c\in A$ for
every $c\in\operatorname*{Inv}\sigma$. In other words, $\operatorname*{Inv}%
\sigma\subseteq A$, qed.}. Combining these two relations, we obtain
$A=\operatorname*{Inv}\sigma$. Hence, $\left\vert A\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert $. Compared with $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert $, this yields
$\ell\left(  \sigma\right)  =\left\vert A\right\vert =k-1$. This rewrites as
$\ell\left(  \operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\right)  =k-1$
(since $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$). This
solves Exercise \ref{exe.perm.cycles} \textbf{(b)}.

\textbf{(c)} We shall sketch this proof only briefly, since it is not
particularly useful. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements
of $\left[  n\right]  $. We extend the $k$-tuple $\left(  i_{1},i_{2}%
,\ldots,i_{k}\right)  $ to an infinite sequence $\left(  i_{1},i_{2}%
,i_{3},\ldots\right)  $ of elements of $\left[  n\right]  $ by setting%
\[
\left(  i_{u}=i_{\left(  \text{the element }u^{\prime}\in\left\{
1,2,\ldots,k\right\}  \text{ satisfying }u^{\prime}\equiv u\operatorname{mod}%
k\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1\right)  .
\]
This sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic with
period $k$. In other words,%
\begin{equation}
i_{u}=i_{u+k}\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1.
\label{sol.perm.cycles.c.period}%
\end{equation}
From this, it is easy to obtain that%
\[
i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}+\cdots
+i_{k+2}=\cdots
\]
\footnote{\textit{Proof.} Every $u\in\left\{  1,2,3,\ldots\right\}  $
satisfies%
\begin{align*}
i_{u}+i_{u+1}+\cdots+i_{u+k-1}  &  =\underbrace{i_{u}}_{\substack{=i_{u+k}%
\\\text{(by (\ref{sol.perm.cycles.c.period}))}}}+\left(  i_{u+1}%
+i_{u+2}+\cdots+i_{u+k-1}\right) \\
&  =i_{u+k}+\left(  i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  =\left(
i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  +i_{u+k}\\
&  =i_{u+1}+i_{u+2}+\cdots+i_{u+k}.
\end{align*}
Thus, $i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}%
+\cdots+i_{k+2}=\cdots$, qed.}. Thus,%
\begin{equation}
i_{r+1}+i_{r+2}+\cdots+i_{r+k}=i_{1}+i_{2}+\cdots+i_{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }r\in\mathbb{N}.
\label{sol.perm.cycles.c.periodsum}%
\end{equation}


Let $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$. Let
$\operatorname*{Inv}\sigma$ denote the set of all inversions of $\sigma$.
Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $. (This can be seen as in the solution to Exercise
\ref{exe.perm.cycles} \textbf{(b)}.) Moreover, the definitions of the sequence
$\left(  i_{1},i_{2},i_{3},\ldots\right)  $ and of $\sigma$ show that%
\begin{equation}
\sigma\left(  i_{p}\right)  =i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\geq1. \label{sol.perm.cycles.c.sigma}%
\end{equation}


Now, fix $r\in\left\{  1,2,\ldots,k-1\right\}  $. We shall prove that
\begin{equation}
\text{there exists some }u\geq1\text{ such that }\left(  i_{u},i_{u+r}\right)
\in\operatorname*{Inv}\sigma. \label{sol.perm.cycles.c.mainclaim}%
\end{equation}
\textit{Proof of (\ref{sol.perm.cycles.c.mainclaim}):} Let us (for the sake of
contradiction) assume the contrary. Thus, there exists no $u\in\mathbb{Z}$
such that $\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$.

Consider the infinite sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2}%
,i_{3+r}-i_{3},\ldots\right)  $. This sequence is periodic with period $k$
(because the sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic
with period $k$) and all its entries are nonzero (because the $k$ elements
$i_{1},i_{2},\ldots,i_{k}$ are distinct, and thus every $u\geq1$ satisfies
$i_{u+r}-i_{u}\neq0$). Thus, each entry of this sequence is either positive or
negative. In other words, every $u\geq1$ satisfies%
\begin{equation}
\text{either }i_{\left(  u+1\right)  +r}-i_{u+1}>0\text{ or }i_{\left(
u+1\right)  +r}-i_{u+1}<0. \label{sol.perm.cycles.c.mainclaim.pf.4}%
\end{equation}
The sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2},i_{3+r}-i_{3},\ldots\right)
$ contains at least one positive entry\footnote{\textit{Proof.} Assume the
contrary. Thus, the sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2}%
,i_{3+r}-i_{3},\ldots\right)  $ contains no positive entries. Therefore, this
sequence must consist of negative entries only (because we know that each
entry of this sequence is either positive or negative). In other words,
$i_{u+r}-i_{u}<0$ for every $u\geq1$. Hence,%
\[
\sum_{u=1}^{k}\underbrace{\left(  i_{u+r}-i_{u}\right)  }_{<0}<\sum_{u=1}%
^{k}0=0.
\]
But this contradicts%
\begin{align*}
\sum_{u=1}^{k}\left(  \underbrace{i_{u+r}}_{=i_{r+u}}-i_{u}\right)   &
=\sum_{u=1}^{k}\left(  i_{r+u}-i_{u}\right)  =\left(  \sum_{u=1}^{k}%
i_{r+u}\right)  -\left(  \sum_{u=1}^{k}i_{u}\right) \\
&  =\left(  i_{r+1}+i_{r+2}+\cdots+i_{r+k}\right)  -\left(  i_{1}+i_{2}%
+\cdots+i_{r}\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.c.periodsum})}\right)  .
\end{align*}
This contradiction shows that our assumption was wrong, qed.}, and at least
one negative entry\footnote{This is proven similarly.}. Hence, there exists at
least one $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(  u+1\right)
+r}-i_{u+1}<0$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then,
there exists no $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(
u+1\right)  +r}-i_{u+1}<0$. Hence,%
\begin{equation}
\text{every }u\geq1\text{ satisfying }i_{u+r}-i_{u}>0\text{ must satisfy
}i_{\left(  u+1\right)  +r}-i_{u+r}>0 \label{sol.perm.cycles.c.mainclaim.pf.5}%
\end{equation}
(because of (\ref{sol.perm.cycles.c.mainclaim.pf.4})).
\par
But there exists some $v\geq1$ satisfying $i_{v+r}-i_{v}>0$ (since the
sequence $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2},i_{3+r}-i_{3},\ldots\right)  $
contains at least one positive entry). Consider this $v$. Then, we have
$i_{v+r}-i_{v}>0$, therefore $i_{\left(  v+1\right)  +r}-i_{v+1}>0$ (by
(\ref{sol.perm.cycles.c.mainclaim.pf.5}), applied to $u=v$), therefore
$i_{\left(  v+2\right)  +r}-i_{v+2}>0$ (by
(\ref{sol.perm.cycles.c.mainclaim.pf.5}), applied to $u=v+1$), therefore
$i_{\left(  v+3\right)  +r}-i_{v+3}>0$ (by
(\ref{sol.perm.cycles.c.mainclaim.pf.5}), applied to $u=v+2$), and so on.
Altogether, we thus obtain
\[
i_{h+r}-i_{h}>0\ \ \ \ \ \ \ \ \ \ \text{for every }h\geq v.
\]
In other words, $i_{h}<i_{h+r}$ for every $h\geq v$. Hence, $i_{v}%
<i_{v+r}<i_{v+2r}<i_{v+3r}<\cdots$. Thus, the numbers $i_{v},i_{v+r}%
,i_{v+2r},i_{v+3r},\ldots$ are pairwise distinct; hence, the sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ contains infinitely many distinct entries.
But this contradicts the fact that this sequence is periodic. This
contradiction proves that our assumption was wrong, qed.}. Consider this $u$.
We have $i_{u}<i_{u+r}$ (since $i_{u+r}-i_{u}>0$), so that $1\leq
i_{u}<i_{u+r}\leq n$. Also, (\ref{sol.perm.cycles.c.sigma}) (applied to $p=u$)
yields $\sigma\left(  i_{u}\right)  =i_{u+1}$. Moreover,
(\ref{sol.perm.cycles.c.sigma}) (applied to $p=u+r$) yields $\sigma\left(
i_{u+r}\right)  =i_{u+r+1}=i_{\left(  u+1\right)  +r}$. Hence,
\begin{align*}
\sigma\left(  i_{u}\right)   &  =i_{u+1}>i_{\left(  u+1\right)  +r}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i_{\left(  u+1\right)  +r}%
-i_{u+1}<0\right) \\
&  =\sigma\left(  i_{u+r}\right)  .
\end{align*}
So we know that $\left(  i_{u},i_{u+r}\right)  $ is a pair of integers
satisfying $1\leq i_{u}<i_{u+r}\leq n$ and $\sigma\left(  i_{u}\right)
>\sigma\left(  i_{u+r}\right)  $. In other words, $\left(  i_{u}%
,i_{u+r}\right)  $ is an inversion of $\sigma$ (by the definition of an
\textquotedblleft inversion\textquotedblright). In other words, $\left(
i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Thus, we have found a
$u\geq1$ such that $\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}%
\sigma$. This proves (\ref{sol.perm.cycles.c.mainclaim}).

Now, let us forget that we fixed $r$. We have shown that, for every
$r\in\left\{  1,2,\ldots,k-1\right\}  $, there exists some $u\geq1$ such that
$\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Let us denote
this $u$ by $u_{r}$. Therefore, for every $r\in\left\{  1,2,\ldots
,k-1\right\}  $, we have found a $u_{r}\geq1$ such that $\left(  i_{u_{r}%
},i_{u_{r}+r}\right)  \in\operatorname*{Inv}\sigma$. The $k-1$ pairs%
\[
\left(  i_{u_{1}},i_{u_{1}+1}\right)  ,\ \left(  i_{u_{2}},i_{u_{2}+2}\right)
,\ \ldots,\ \left(  i_{u_{k-1}},i_{u_{k-1}+\left(  k-1\right)  }\right)
\]
are pairwise distinct\footnote{\textit{Proof.} Assume the contrary. Then,
there exist two distinct elements $x$ and $y$ of $\left\{  1,2,\ldots
,k-1\right\}  $ such that $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(
i_{u_{y}},i_{u_{y}+y}\right)  $. Consider these $x$ and $y$.
\par
We have $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(  i_{u_{y}},i_{u_{y}%
+y}\right)  $. In other words, $i_{u_{x}}=i_{u_{y}}$ and $i_{u_{x}+x}%
=i_{u_{y}+y}$. Since the numbers $i_{1},i_{2},\ldots,i_{k}$ are distinct (and
the sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ consists of these
numbers, repeated over and over), we obtain $u_{x}\equiv u_{y}%
\operatorname{mod}k$ from $i_{u_{x}}=i_{u_{y}}$, and we obtain $u_{x}+x\equiv
u_{y}+y\operatorname{mod}k$ from $i_{u_{x}+x}=i_{u_{y}+y}$. Subtracting the
congruence $u_{x}\equiv u_{y}\operatorname{mod}k$ from the congruence
$u_{x}+x\equiv u_{y}+y\operatorname{mod}k$, we obtain $x\equiv
y\operatorname{mod}k$. In light of $x,y\in\left\{  1,2,\ldots,k-1\right\}  $,
this shows that $x=y$. But this contradicts the fact that $x$ and $y$ are
distinct. This contradiction proves that our assumption was wrong, qed.}, and
all of them belong to $\operatorname*{Inv}\sigma$. Hence, the set
$\operatorname*{Inv}\sigma$ has at least $k-1$ elements. In other words,
$\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$. Thus, $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$.
Since $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$, this
rewrites as $\ell\left(  \operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}\right)  \geq k-1$. This solves Exercise \ref{exe.perm.cycles}
\textbf{(c)}.

\textbf{(d)} Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of
$\left[  n\right]  $. Thus, $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ is a
list of some elements of $\left[  n\right]  $ such that $i_{1},i_{2}%
,\ldots,i_{k}$ are distinct. Hence, Proposition \ref{prop.perms.lists}
\textbf{(c)} (applied to $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =\left(
i_{1},i_{2},\ldots,i_{k}\right)  $) yields that there exists a permutation
$\sigma\in S_{n}$ such that $\left(  i_{1},i_{2},\ldots,i_{k}\right)  =\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
k\right)  \right)  $. Consider such a $\sigma$.

Exercise \ref{exe.perm.cycles} \textbf{(b)} yields $\ell\left(
\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)  =k-1$. But the definition
of $\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}$ yields
$\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}=\left(
-1\right)  ^{\ell\left(  \operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)
}=\left(  -1\right)  ^{k-1}$ (since $\ell\left(  \operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}\right)  =k-1$).

Exercise \ref{exe.perm.cycles} \textbf{(a)} (applied to $1,2,\ldots,k$ instead
of $i_{1},i_{2},\ldots,i_{k}$) yields%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma
^{-1}=\operatorname*{cyc}\nolimits_{\sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  k\right)  }=\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
\]
(since $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  k\right)  \right)  =\left(  i_{1},i_{2},\ldots
,i_{k}\right)  $). Hence,
\[
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}%
_{=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma^{-1}%
}\circ\sigma=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}%
\circ\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}}=\sigma
\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}.
\]
Thus,%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\sigma\circ\operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}}=\left(  -1\right)  ^{\sigma}\cdot\underbrace{\left(
-1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}}_{=\left(
-1\right)  ^{k-1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
\tau=\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{k-1}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}}\cdot\left(  -1\right)  ^{\sigma}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\text{ and }\sigma\text{ instead
of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}},
\end{align*}
this yields $\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{k-1}$. We can cancel $\left(  -1\right)
^{\sigma}$ from this equality (since $\left(  -1\right)  ^{\sigma}\in\left\{
1,-1\right\}  $ is a nonzero integer), and thus obtain $\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{k-1}$. This solves Exercise \ref{exe.perm.cycles} \textbf{(d)}.

[\textit{Remark:} Exercise \ref{exe.perm.cycles} \textbf{(d)} can also be
solved in a different way, namely by arguing that%
\[
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}=t_{i_{1},i_{2}}\circ
t_{i_{2},i_{3}}\circ\cdots\circ t_{i_{k-1},i_{k}}%
\]
(using the notations of Definition \ref{def.transpos}) and then by applying
the equality $\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ multiple times. We leave the details
of this alternative proof to the curious reader. (That said, this alternative
proof is also the most popular proof, so it is easily found in textbooks.)]
\end{proof}
\end{verlong}

\subsection{Solution to Additional exercise \ref{exeadd.perm.Inv.sub}}

We shall now prepare for the solution of Additional exercise
\ref{exeadd.perm.Inv.sub}. First, we introduce a notation:

\begin{definition}
\label{def.sol.exeadd.perm.Inv.sub.aXb}If $X$, $X^{\prime}$, $Y$ and
$Y^{\prime}$ are four sets and if $\alpha:X\rightarrow X^{\prime}$ and
$\beta:Y\rightarrow Y^{\prime}$ are two maps, then $\alpha\times\beta$ will
denote the map%
\begin{align*}
X\times Y  &  \rightarrow X^{\prime}\times Y^{\prime},\\
\left(  x,y\right)   &  \mapsto\left(  \alpha\left(  x\right)  ,\beta\left(
y\right)  \right)  .
\end{align*}

\end{definition}

\begin{lemma}
\label{lem.sol.exeadd.perm.Inv.sub.bijbij}Let $n\in\mathbb{N}$. Let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. Let $\alpha\in
S_{n}$ and $\beta\in S_{n}$. Then, the map $\alpha\times\beta:\left[
n\right]  \times\left[  n\right]  \rightarrow\left[  n\right]  \times\left[
n\right]  $ is invertible, and its inverse is $\alpha^{-1}\times\beta^{-1}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.perm.Inv.sub.bijbij}.]Recall that $S_{n}$
is the set of all permutations of the set $\left\{  1,2,\ldots,n\right\}  $.
In other words, $S_{n}$ is the set of all permutations of the set $\left[
n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $).

We know that $\alpha\in S_{n}$. In other words, $\alpha$ is a permutation of
the set $\left[  n\right]  $ (since $S_{n}$ is the set of all permutations of
the set $\left[  n\right]  $). In other words, $\alpha$ is a bijective map
$\left[  n\right]  \rightarrow\left[  n\right]  $. Similarly, $\beta$ is a
bijective map $\left[  n\right]  \rightarrow\left[  n\right]  $. Hence,
Definition \ref{def.sol.exeadd.perm.Inv.sub.aXb} defines a map $\alpha
\times\beta:\left[  n\right]  \times\left[  n\right]  \rightarrow\left[
n\right]  \times\left[  n\right]  $.

The map $\alpha$ is bijective, and thus invertible. Hence, its inverse map
$\alpha^{-1}:\left[  n\right]  \rightarrow\left[  n\right]  $ is well-defined.
Similarly, $\beta^{-1}:\left[  n\right]  \rightarrow\left[  n\right]  $ is
well-defined. Thus, Definition \ref{def.sol.exeadd.perm.Inv.sub.aXb} defines a
map $\alpha^{-1}\times\beta^{-1}:\left[  n\right]  \times\left[  n\right]
\rightarrow\left[  n\right]  \times\left[  n\right]  $.

\begin{vershort}
Now, every $\left(  i,j\right)  \in\left[  n\right]  \times\left[  n\right]  $
satisfies
\begin{align*}
\left(  \left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  \right)  \left(  i,j\right)   &  =\left(  \alpha
^{-1}\times\beta^{-1}\right)  \left(  \underbrace{\left(  \alpha\times
\beta\right)  \left(  i,j\right)  }_{\substack{=\left(  \alpha\left(
i\right)  ,\beta\left(  j\right)  \right)  \\\text{(by the definition of
}\alpha\times\beta\text{)}}}\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \alpha\left(
i\right)  ,\beta\left(  j\right)  \right)  =\left(  \underbrace{\alpha
^{-1}\left(  \alpha\left(  i\right)  \right)  }_{=i},\underbrace{\beta
^{-1}\left(  \beta\left(  j\right)  \right)  }_{=j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha^{-1}%
\times\beta^{-1}\right) \\
&  =\left(  i,j\right)  =\operatorname*{id}\left(  i,j\right)  .
\end{align*}
Thus, $\left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  =\operatorname*{id}$. Similarly, $\left(  \alpha
\times\beta\right)  \circ\left(  \alpha^{-1}\times\beta^{-1}\right)
=\operatorname*{id}$. These two equalities show that the maps $\alpha
\times\beta$ and $\alpha^{-1}\times\beta^{-1}$ are mutually inverse. Hence,
the map $\alpha\times\beta:\left[  n\right]  \times\left[  n\right]
\rightarrow\left[  n\right]  \times\left[  n\right]  $ is invertible, and its
inverse is $\alpha^{-1}\times\beta^{-1}$. This proves Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij}.
\end{vershort}

\begin{verlong}
Now, $\left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  =\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let
$z\in\left[  n\right]  \times\left[  n\right]  $. Thus, $z=\left(  i,j\right)
$ for some $\left(  i,j\right)  \in\left[  n\right]  \times\left[  n\right]
$. Consider this $\left(  i,j\right)  $.
\par
We have%
\begin{align*}
\left(  \left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  \right)  \left(  \underbrace{z}_{=\left(  i,j\right)
}\right)   &  =\left(  \left(  \alpha^{-1}\times\beta^{-1}\right)
\circ\left(  \alpha\times\beta\right)  \right)  \left(  i,j\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \underbrace{\left(
\alpha\times\beta\right)  \left(  i,j\right)  }_{\substack{=\left(
\alpha\left(  i\right)  ,\beta\left(  j\right)  \right)  \\\text{(by the
definition of }\alpha\times\beta\text{)}}}\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \alpha\left(
i\right)  ,\beta\left(  j\right)  \right)  =\left(  \underbrace{\alpha
^{-1}\left(  \alpha\left(  i\right)  \right)  }_{=i},\underbrace{\beta
^{-1}\left(  \beta\left(  j\right)  \right)  }_{=j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha^{-1}%
\times\beta^{-1}\right) \\
&  =\left(  i,j\right)  =z=\operatorname*{id}\left(  z\right)  .
\end{align*}
\par
Now, forget that we fixed $z$. We thus have shown that $\left(  \left(
\alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha\times\beta\right)
\right)  \left(  z\right)  =\operatorname*{id}\left(  z\right)  $ for every
$z\in\left[  n\right]  \times\left[  n\right]  $. In other words, $\left(
\alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha\times\beta\right)
=\operatorname*{id}$, qed.} and $\left(  \alpha\times\beta\right)
\circ\left(  \alpha^{-1}\times\beta^{-1}\right)  =\operatorname*{id}%
$\ \ \ \ \footnote{\textit{Proof.} Let $z\in\left[  n\right]  \times\left[
n\right]  $. Thus, $z=\left(  i,j\right)  $ for some $\left(  i,j\right)
\in\left[  n\right]  \times\left[  n\right]  $. Consider this $\left(
i,j\right)  $.
\par
We have%
\begin{align*}
\left(  \left(  \alpha\times\beta\right)  \circ\left(  \alpha^{-1}\times
\beta^{-1}\right)  \right)  \left(  \underbrace{z}_{=\left(  i,j\right)
}\right)   &  =\left(  \left(  \alpha\times\beta\right)  \circ\left(
\alpha^{-1}\times\beta^{-1}\right)  \right)  \left(  i,j\right) \\
&  =\left(  \alpha\times\beta\right)  \left(  \underbrace{\left(  \alpha
^{-1}\times\beta^{-1}\right)  \left(  i,j\right)  }_{\substack{=\left(
\alpha^{-1}\left(  i\right)  ,\beta^{-1}\left(  j\right)  \right)  \\\text{(by
the definition of }\alpha^{-1}\times\beta^{-1}\text{)}}}\right) \\
&  =\left(  \alpha\times\beta\right)  \left(  \alpha^{-1}\left(  i\right)
,\beta^{-1}\left(  j\right)  \right)  =\left(  \underbrace{\alpha\left(
\alpha^{-1}\left(  i\right)  \right)  }_{=i},\underbrace{\beta\left(
\beta^{-1}\left(  j\right)  \right)  }_{=j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha\times
\beta\right) \\
&  =\left(  i,j\right)  =z=\operatorname*{id}\left(  z\right)  .
\end{align*}
\par
Now, forget that we fixed $z$. We thus have shown that $\left(  \left(
\alpha\times\beta\right)  \circ\left(  \alpha^{-1}\times\beta^{-1}\right)
\right)  \left(  z\right)  =\operatorname*{id}\left(  z\right)  $ for every
$z\in\left[  n\right]  \times\left[  n\right]  $. In other words, $\left(
\alpha\times\beta\right)  \circ\left(  \alpha^{-1}\times\beta^{-1}\right)
=\operatorname*{id}$, qed.}. Thus, the maps $\alpha\times\beta$ and
$\alpha^{-1}\times\beta^{-1}$ are mutually inverse (since $\left(  \alpha
^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha\times\beta\right)
=\operatorname*{id}$ and $\left(  \alpha\times\beta\right)  \circ\left(
\alpha^{-1}\times\beta^{-1}\right)  =\operatorname*{id}$). Hence, the map
$\alpha\times\beta:\left[  n\right]  \times\left[  n\right]  \rightarrow
\left[  n\right]  \times\left[  n\right]  $ is invertible, and its inverse is
$\alpha^{-1}\times\beta^{-1}$. This proves Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij}.
\end{verlong}
\end{proof}

\begin{lemma}
\label{lem.sol.exeadd.perm.Inv.sub.subset}Let $n\in\mathbb{N}$. Let $\sigma\in
S_{n}$ and $\tau\in S_{n}$. Then:

\textbf{(a)} We have $\operatorname*{Inv}\left(  \sigma\circ\tau\right)
\setminus\operatorname*{Inv}\tau\subseteq\left(  \tau\times\tau\right)
^{-1}\left(  \operatorname*{Inv}\sigma\right)  $. (Here, $\tau\times\tau$ is
defined as in Definition \ref{def.sol.exeadd.perm.Inv.sub.aXb}.)

\textbf{(b)} We have $\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert \leq\left\vert
\operatorname*{Inv}\sigma\right\vert $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset}.]Let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij} (applied to $\alpha=\tau$ and
$\beta=\tau$) yields that the map $\tau\times\tau:\left[  n\right]
\times\left[  n\right]  \rightarrow\left[  n\right]  \times\left[  n\right]  $
is invertible, and its inverse is $\tau^{-1}\times\tau^{-1}$. Thus, $\left(
\tau\times\tau\right)  ^{-1}=\tau^{-1}\times\tau^{-1}$.

\textbf{(a)} Let $c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)
\setminus\operatorname*{Inv}\tau$. Thus, $c\in\operatorname*{Inv}\left(
\sigma\circ\tau\right)  $ but $c\notin\operatorname*{Inv}\tau$.

We have $c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. In other
words, $c$ is an inversion of $\sigma\circ\tau$ (since $\operatorname*{Inv}%
\left(  \sigma\circ\tau\right)  $ is the set of all inversions of $\sigma
\circ\tau$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\left(  \sigma\circ\tau\right)  \left(
i\right)  >\left(  \sigma\circ\tau\right)  \left(  j\right)  $ (by the
definition of an \textquotedblleft inversion of $\sigma\circ\tau
$\textquotedblright). In other words, there exists a pair $\left(  i,j\right)
$ of integers satisfying $1\leq i<j\leq n$, $\left(  \sigma\circ\tau\right)
\left(  i\right)  >\left(  \sigma\circ\tau\right)  \left(  j\right)  $ and
$c=\left(  i,j\right)  $. Let us denote this pair $\left(  i,j\right)  $ by
$\left(  u,v\right)  $. Thus, $\left(  u,v\right)  $ is a pair of integers
satisfying $1\leq u<v\leq n$, $\left(  \sigma\circ\tau\right)  \left(
u\right)  >\left(  \sigma\circ\tau\right)  \left(  v\right)  $ and $c=\left(
u,v\right)  $.

But $\tau\left(  u\right)  <\tau\left(  v\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $\tau\left(
u\right)  \geq\tau\left(  v\right)  $. But $\tau\in S_{n}$. In other words,
$\tau$ is a permutation of the set $\left\{  1,2,\ldots,n\right\}  $ (since
$S_{n}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $). Thus, the map $\tau$ is bijective, and therefore also
injective. But $u<v$, so that $u\neq v$ and therefore $\tau\left(  u\right)
\neq\tau\left(  v\right)  $ (since $\tau$ is injective). Combining this with
$\tau\left(  u\right)  \geq\tau\left(  v\right)  $, we obtain $\tau\left(
u\right)  >\tau\left(  v\right)  $.
\par
Now, we know that $\left(  u,v\right)  $ is a pair of integers and satisfies
$1\leq u<v\leq n$ and $\tau\left(  u\right)  >\tau\left(  v\right)  $. In
other words, $\left(  u,v\right)  $ is a pair $\left(  i,j\right)  $ of
integers satisfying $1\leq i<j\leq n$ and $\tau\left(  i\right)  >\tau\left(
j\right)  $. In other words, $\left(  u,v\right)  $ is an inversion of $\tau$
(by the definition of an \textquotedblleft inversion of $\tau$%
\textquotedblright). In other words, $\left(  u,v\right)  \in
\operatorname*{Inv}\tau$ (since $\operatorname*{Inv}\tau$ is the set of all
inversions of $\tau$). But this contradicts $\left(  u,v\right)
=c\notin\operatorname*{Inv}\tau$. This contradiction shows that our assumption
was false, qed.}. Also, $1\leq\tau\left(  u\right)  $ (since $\tau\left(
u\right)  \in\left\{  1,2,\ldots,n\right\}  $) and $\tau\left(  v\right)  \leq
n$ (since $\tau\left(  v\right)  \in\left\{  1,2,\ldots,n\right\}  $).
Finally, $\sigma\left(  \tau\left(  u\right)  \right)  =\left(  \sigma
\circ\tau\right)  \left(  u\right)  >\left(  \sigma\circ\tau\right)  \left(
v\right)  =\sigma\left(  \tau\left(  v\right)  \right)  $.

Thus, $\left(  \tau\left(  u\right)  ,\tau\left(  v\right)  \right)  $ is a
pair of integers satisfying $1\leq\tau\left(  u\right)  <\tau\left(  v\right)
\leq n$ and $\sigma\left(  \tau\left(  u\right)  \right)  >\sigma\left(
\tau\left(  v\right)  \right)  $. In other words, $\left(  \tau\left(
u\right)  ,\tau\left(  v\right)  \right)  $ is a pair $\left(  i,j\right)  $
of integers satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. In other words, $\left(  \tau\left(  u\right)
,\tau\left(  v\right)  \right)  $ is an inversion of $\sigma$ (by the
definition of an \textquotedblleft inversion of $\sigma$\textquotedblright).
In other words, $\left(  \tau\left(  u\right)  ,\tau\left(  v\right)  \right)
\in\operatorname*{Inv}\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of
all inversions of $\sigma$).

Now, $c=\left(  u,v\right)  \in\left[  n\right]  \times\left[  n\right]  $
(since both $u$ and $v$ belong to $\left[  n\right]  $), and we have
\begin{align*}
\left(  \tau\times\tau\right)  \left(  \underbrace{c}_{=\left(  u,v\right)
}\right)   &  =\left(  \tau\times\tau\right)  \left(  u,v\right)  =\left(
\tau\left(  u\right)  ,\tau\left(  v\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\times\tau\right)
\\
&  \in\operatorname*{Inv}\sigma,
\end{align*}
so that $c\in\left(  \tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}%
\sigma\right)  $.

Now, forget that we fixed $c$. We thus have shown that $c\in\left(  \tau
\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)  $ for every
$c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau$. In other words, $\operatorname*{Inv}\left(
\sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau\subseteq\left(
\tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)  $. This
proves Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(a)}.

\textbf{(b)} Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(a)}
yields $\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\subseteq\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  $. Thus,%
\[
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert \leq\left\vert \left(  \tau\times
\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)  \right\vert
=\left\vert \operatorname*{Inv}\sigma\right\vert
\]
(since $\tau\times\tau$ is a bijection (since the map $\tau\times\tau$ is
invertible)). This proves Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset}
\textbf{(b)}.
\end{proof}

\begin{lemma}
\label{lem.sol.exeadd.perm.Inv.sub.l}Let $n\in\mathbb{N}$. Let $\sigma\in
S_{n}$. Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}.]We have%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}.
\end{proof}

Before we step to the solution to Exercise \ref{exeadd.perm.Inv.sub}, let us
make a short digression and use Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset}
to give a new solution of Exercise \ref{exe.ps2.2.5} \textbf{(c)}:

\begin{proof}
[Second solution to Exercise \ref{exe.ps2.2.5} \textbf{(c)}.]Let $\sigma\in
S_{n}$ and $\tau\in S_{n}$. We have $\left\vert A\setminus B\right\vert
\geq\left\vert A\right\vert -\left\vert B\right\vert $ for any two finite sets
$A$ and $B$\ \ \ \ \footnote{\textit{Proof.} Let $A$ and $B$ be two finite
sets. Then, $A\setminus B=A\setminus\left(  A\cap B\right)  $, so that
\begin{align*}
\left\vert A\setminus B\right\vert  &  =\left\vert A\setminus\left(  A\cap
B\right)  \right\vert =\left\vert A\right\vert -\underbrace{\left\vert A\cap
B\right\vert }_{\substack{\leq\left\vert B\right\vert \\\text{(since }A\cap
B\subseteq B\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A\cap
B\subseteq A\right) \\
&  \geq\left\vert A\right\vert -\left\vert B\right\vert ,
\end{align*}
qed.}. Applying this to $A=\operatorname*{Inv}\left(  \sigma\circ\tau\right)
$ and $B=\operatorname*{Inv}\tau$, we obtain $\left\vert \operatorname*{Inv}%
\left(  \sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert
\geq\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert $. Thus,%
\begin{align*}
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert  &  \leq\left\vert
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert \leq\left\vert \operatorname*{Inv}%
\sigma\right\vert \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(b)}}\right) \\
&  =\ell\left(  \sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}}\right)  .
\end{align*}
Now,%
\[
\underbrace{\ell\left(  \sigma\circ\tau\right)  }_{\substack{=\left\vert
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert \\\text{(by
Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\sigma\circ
\tau\text{ instead of }\sigma\text{))}}}-\underbrace{\ell\left(  \tau\right)
}_{\substack{=\left\vert \operatorname*{Inv}\tau\right\vert \\\text{(by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\tau\text{ instead of
}\sigma\text{))}}}=\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right\vert -\left\vert \operatorname*{Inv}\tau\right\vert
\leq\ell\left(  \sigma\right)  .
\]
In other words, $\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $. Thus, Exercise \ref{exe.ps2.2.5}
\textbf{(c)} is solved again.
\end{proof}

Now, let us finally solve Exercise \ref{exeadd.perm.Inv.sub}:

\begin{proof}
[Solution to Exercise \ref{exeadd.perm.Inv.sub}.]\textbf{(a)} We first observe
that%
\begin{equation}
\underbrace{\ell\left(  \sigma\circ\tau\right)  }_{\substack{=\left\vert
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert \\\text{(by
Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\sigma\circ
\tau\text{ instead of }\sigma\text{))}}}-\underbrace{\ell\left(  \tau\right)
}_{\substack{=\left\vert \operatorname*{Inv}\tau\right\vert \\\text{(by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\tau\text{ instead of
}\sigma\text{))}}}=\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right\vert -\left\vert \operatorname*{Inv}\tau\right\vert .
\label{sol.exeadd.perm.Inv.sub.a.triv}%
\end{equation}


Let us now prove the logical implication%
\begin{equation}
\left(  \ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \right)  \ \Longrightarrow\ \left(
\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right)  . \label{sol.exeadd.perm.Inv.sub.a.1}%
\end{equation}


\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.a.1}):} Assume that
$\ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  $ holds. We will prove that $\operatorname*{Inv}\tau
\subseteq\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $.

If two finite sets $A$ and $B$ satisfy $\left\vert A\setminus B\right\vert
\leq\left\vert A\right\vert -\left\vert B\right\vert $, then%
\begin{equation}
B\subseteq A \label{sol.exeadd.perm.Inv.sub.a.1.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.a.1.pf.1}):} Let $A$
and $B$ be two finite sets satisfying $\left\vert A\setminus B\right\vert
\leq\left\vert A\right\vert -\left\vert B\right\vert $.
\par
We have $A\setminus\left(  A\cap B\right)  =A\setminus B$, so that $\left\vert
A\setminus\left(  A\cap B\right)  \right\vert =\left\vert A\setminus
B\right\vert \leq\left\vert A\right\vert -\left\vert B\right\vert $. Adding
$\left\vert B\right\vert $ to both sides of this inequality, we obtain
$\left\vert A\setminus\left(  A\cap B\right)  \right\vert +\left\vert
B\right\vert \leq\left\vert A\right\vert $. Hence,%
\[
\left\vert A\right\vert \geq\underbrace{\left\vert A\setminus\left(  A\cap
B\right)  \right\vert }_{\substack{=\left\vert A\right\vert -\left\vert A\cap
B\right\vert \\\text{(since }A\cap B\subseteq A\text{)}}}+\left\vert
B\right\vert =\left\vert A\right\vert -\left\vert A\cap B\right\vert
+\left\vert B\right\vert .
\]
Subtracting $\left\vert A\right\vert $ from both sides of this inequality, we
obtain $0\geq-\left\vert A\cap B\right\vert +\left\vert B\right\vert $. In
other words, $\left\vert A\cap B\right\vert \geq\left\vert B\right\vert $.
Also, clearly, $A\cap B$ is a subset of $B$.
\par
But $B$ is a finite set. Hence, the only subset of $B$ having size
$\geq\left\vert B\right\vert $ is $B$ itself. In other words, if $C$ is a
subset of $B$ satisfying $\left\vert C\right\vert \geq\left\vert B\right\vert
$, then $C=B$. Applying this to $C=A\cap B$, we obtain $A\cap B=B$ (since
$A\cap B$ is a subset of $B$ satisfying $\left\vert A\cap B\right\vert
\geq\left\vert B\right\vert $). Hence, $B=A\cap B\subseteq A$. This proves
(\ref{sol.exeadd.perm.Inv.sub.a.1.pf.1}).}.

Now, Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(b)} yields
\begin{align*}
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert  &  \leq\left\vert \operatorname*{Inv}%
\sigma\right\vert =\ell\left(  \sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}}\right) \\
&  =\ell\left(  \sigma\circ\tau\right)  -\ell\left(  \tau\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  \right) \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.exeadd.perm.Inv.sub.a.triv})}\right)  .
\end{align*}
Thus, (\ref{sol.exeadd.perm.Inv.sub.a.1.pf.1}) (applied to
$A=\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ and
$B=\operatorname*{Inv}\tau$) yields $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $.

Now, forget our assumption that $\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  $. We thus have proven
that if $\ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  $, then $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. In other words, we have
proven the implication (\ref{sol.exeadd.perm.Inv.sub.a.1}).

Let us next prove the logical implication%
\begin{equation}
\left(  \operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma
\circ\tau\right)  \right)  \ \Longrightarrow\ \left(  \ell\left(  \sigma
\circ\tau\right)  =\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
\right)  . \label{sol.exeadd.perm.Inv.sub.a.2}%
\end{equation}


\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.a.2}):} Assume that
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $ holds. We will prove that $\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  $.

Consider the map $\tau\times\tau$ defined as in Definition
\ref{def.sol.exeadd.perm.Inv.sub.aXb}. Let $\left[  n\right]  $ denote the set
$\left\{  1,2,\ldots,n\right\}  $. Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij} (applied to $\alpha=\tau$ and
$\beta=\tau$) yields that the map $\tau\times\tau:\left[  n\right]
\times\left[  n\right]  \rightarrow\left[  n\right]  \times\left[  n\right]  $
is invertible, and its inverse is $\tau^{-1}\times\tau^{-1}$. Thus, $\left(
\tau\times\tau\right)  ^{-1}=\tau^{-1}\times\tau^{-1}$.

Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(a)} shows that%
\begin{equation}
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\subseteq\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  . \label{sol.exeadd.perm.Inv.sub.a.2.pf.1}%
\end{equation}
We shall now prove the reverse inclusion, i.e., we shall prove that $\left(
\tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)
\subseteq\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau$.

Indeed, fix $c\in\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  $. Thus, $c\in\left[  n\right]
\times\left[  n\right]  $ and $\left(  \tau\times\tau\right)  \left(
c\right)  \in\operatorname*{Inv}\sigma$.

We have $\left(  \tau\times\tau\right)  \left(  c\right)  \in
\operatorname*{Inv}\sigma$. In other words, $\left(  \tau\times\tau\right)
\left(  c\right)  $ is an inversion of $\sigma$ (since $\operatorname*{Inv}%
\sigma$ is the set of all inversions of $\sigma$). In other words, $\left(
\tau\times\tau\right)  \left(  c\right)  $ is a pair $\left(  i,j\right)  $ of
integers satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $ (by the definition of an \textquotedblleft
inversion of $\sigma$\textquotedblright). In other words, there exists a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $ and $\left(  \tau
\times\tau\right)  \left(  c\right)  =\left(  i,j\right)  $. Let us denote
this pair $\left(  i,j\right)  $ by $\left(  u,v\right)  $. Thus, $\left(
u,v\right)  $ is a pair of integers satisfying $1\leq u<v\leq n$,
$\sigma\left(  u\right)  >\sigma\left(  v\right)  $ and $\left(  \tau
\times\tau\right)  \left(  c\right)  =\left(  u,v\right)  $.

From $\left(  \tau\times\tau\right)  \left(  c\right)  =\left(  u,v\right)  $,
we obtain%
\begin{align*}
c  &  =\underbrace{\left(  \tau\times\tau\right)  ^{-1}}_{=\tau^{-1}\times
\tau^{-1}}\left(  u,v\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the map
}\tau\times\tau\text{ is invertible}\right) \\
&  =\left(  \tau^{-1}\times\tau^{-1}\right)  \left(  u,v\right)  =\left(
\tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)
\end{align*}
(by the definition of $\tau^{-1}\times\tau^{-1}$).

Notice that%
\begin{equation}
\left(  \sigma\circ\tau\right)  \left(  \tau^{-1}\left(  v\right)  \right)
=\sigma\left(  \underbrace{\tau\left(  \tau^{-1}\left(  v\right)  \right)
}_{=v}\right)  =\sigma\left(  v\right)
\label{sol.exeadd.perm.Inv.sub.a.2.pf.4}%
\end{equation}
and%
\begin{align}
\left(  \sigma\circ\tau\right)  \left(  \tau^{-1}\left(  u\right)  \right)
&  =\sigma\left(  \underbrace{\tau\left(  \tau^{-1}\left(  u\right)  \right)
}_{=u}\right)  =\sigma\left(  u\right) \nonumber\\
&  >\sigma\left(  v\right)  =\left(  \sigma\circ\tau\right)  \left(  \tau
^{-1}\left(  v\right)  \right)  \label{sol.exeadd.perm.Inv.sub.a.2.pf.5}%
\end{align}
(by (\ref{sol.exeadd.perm.Inv.sub.a.2.pf.4})).

Let us now prove that $\tau^{-1}\left(  u\right)  <\tau^{-1}\left(  v\right)
$. Indeed, let us assume the contrary. Thus, $\tau^{-1}\left(  u\right)
\geq\tau^{-1}\left(  v\right)  $. But $u\neq v$ (since $u<v$), so that
$\tau^{-1}\left(  u\right)  \neq\tau^{-1}\left(  v\right)  $. Combined with
$\tau^{-1}\left(  u\right)  \geq\tau^{-1}\left(  v\right)  $, this yields
$\tau^{-1}\left(  u\right)  >\tau^{-1}\left(  v\right)  $. In other words,
$\tau^{-1}\left(  v\right)  <\tau^{-1}\left(  u\right)  $. Also, $1\leq
\tau^{-1}\left(  v\right)  $ (since $\tau^{-1}\left(  v\right)  \in\left\{
1,2,\ldots,n\right\}  $) and $\tau^{-1}\left(  u\right)  \leq n$ (since
$\tau^{-1}\left(  u\right)  \in\left\{  1,2,\ldots,n\right\}  $). Finally,
$\tau\left(  \tau^{-1}\left(  v\right)  \right)  =v>u=\tau\left(  \tau
^{-1}\left(  u\right)  \right)  $.

Thus, $\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)
$ is a pair of integers satisfying $1\leq\tau^{-1}\left(  v\right)  <\tau
^{-1}\left(  u\right)  \leq n$ and $\tau\left(  \tau^{-1}\left(  v\right)
\right)  >\tau\left(  \tau^{-1}\left(  u\right)  \right)  $. In other words,
$\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is
a pair $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\tau\left(  i\right)  >\tau\left(  j\right)  $. In other words, $\left(
\tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is an
inversion of $\tau$ (by the definition of an \textquotedblleft inversion of
$\tau$\textquotedblright). In other words, $\left(  \tau^{-1}\left(  v\right)
,\tau^{-1}\left(  u\right)  \right)  \in\operatorname*{Inv}\tau$ (since
$\operatorname*{Inv}\tau$ is the set of all inversions of $\tau$). Hence,
$\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)
\in\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $. In other words, $\left(  \tau^{-1}\left(  v\right)  ,\tau
^{-1}\left(  u\right)  \right)  $ is an inversion of $\sigma\circ\tau$ (since
$\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ is the set of all
inversions of $\sigma\circ\tau$). In other words, $\left(  \tau^{-1}\left(
v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is a pair $\left(
i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\left(
\sigma\circ\tau\right)  \left(  i\right)  >\left(  \sigma\circ\tau\right)
\left(  j\right)  $ (by the definition of an \textquotedblleft inversion of
$\sigma\circ\tau$\textquotedblright). In other words, $\left(  \tau
^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is a pair of
integers satisfying $1\leq\tau^{-1}\left(  v\right)  <\tau^{-1}\left(
u\right)  \leq n$ and $\left(  \sigma\circ\tau\right)  \left(  \tau
^{-1}\left(  v\right)  \right)  >\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  u\right)  \right)  $. But $\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  v\right)  \right)  >\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  u\right)  \right)  >\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  v\right)  \right)  $ (by
(\ref{sol.exeadd.perm.Inv.sub.a.2.pf.5})), which is absurd. This contradiction
proves that our assumption was wrong. Hence, $\tau^{-1}\left(  u\right)
<\tau^{-1}\left(  v\right)  $ is proven.

Now, $1\leq\tau^{-1}\left(  u\right)  $ (since $\tau^{-1}\left(  u\right)
\in\left\{  1,2,\ldots,n\right\}  $) and $\tau^{-1}\left(  v\right)  \leq n$
(since $\tau^{-1}\left(  v\right)  \in\left\{  1,2,\ldots,n\right\}  $).
Finally, recall that (\ref{sol.exeadd.perm.Inv.sub.a.2.pf.5}) holds. Thus,
$\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is
a pair of integers satisfying $1\leq\tau^{-1}\left(  u\right)  <\tau
^{-1}\left(  v\right)  \leq n$ and $\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  u\right)  \right)  >\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  v\right)  \right)  $. In other words, $\left(  \tau
^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\left(
\sigma\circ\tau\right)  \left(  i\right)  >\left(  \sigma\circ\tau\right)
\left(  j\right)  $. In other words, $\left(  \tau^{-1}\left(  u\right)
,\tau^{-1}\left(  v\right)  \right)  $ is an inversion of $\sigma\circ\tau$
(by the definition of an \textquotedblleft inversion of $\sigma\circ\tau
$\textquotedblright). In other words, $\left(  \tau^{-1}\left(  u\right)
,\tau^{-1}\left(  v\right)  \right)  \in\operatorname*{Inv}\left(  \sigma
\circ\tau\right)  $ (since $\operatorname*{Inv}\left(  \sigma\circ\tau\right)
$ is the set of all inversions of $\sigma\circ\tau$).

Hence, $c=\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)
\right)  \in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $.

Let us now prove that $c\notin\operatorname*{Inv}\tau$. Indeed, assume the
contrary. Thus, $c\in\operatorname*{Inv}\tau$. Thus, $\left(  \tau^{-1}\left(
u\right)  ,\tau^{-1}\left(  v\right)  \right)  =c\in\operatorname*{Inv}\tau$.
In other words, $\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(
v\right)  \right)  $ is an inversion of $\tau$ (since $\operatorname*{Inv}%
\tau$ is the set of all inversions of $\tau$). In other words, $\left(
\tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\tau\left(  i\right)  >\tau\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\tau$\textquotedblright). In other words,
$\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is
a pair of integers satisfying $1\leq\tau^{-1}\left(  u\right)  <\tau
^{-1}\left(  v\right)  \leq n$ and $\tau\left(  \tau^{-1}\left(  u\right)
\right)  >\tau\left(  \tau^{-1}\left(  v\right)  \right)  $. But $\tau\left(
\tau^{-1}\left(  u\right)  \right)  >\tau\left(  \tau^{-1}\left(  v\right)
\right)  =v$ contradicts $\tau\left(  \tau^{-1}\left(  u\right)  \right)
=u<v$. This contradiction proves that our assumption was wrong. Hence,
$c\notin\operatorname*{Inv}\tau$ is proven.

Combining $c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ with
$c\notin\operatorname*{Inv}\tau$, we obtain $c\in\operatorname*{Inv}\left(
\sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau$.

Now, forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau$ for each $c\in\left(  \tau\times\tau\right)
^{-1}\left(  \operatorname*{Inv}\sigma\right)  $. In other words, we have
proven the inclusion%
\[
\left(  \tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)
\subseteq\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau.
\]
Combining this with the inclusion (\ref{sol.exeadd.perm.Inv.sub.a.2.pf.1}), we
obtain%
\[
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau=\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  .
\]
Hence,%
\[
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert =\left\vert \left(  \tau\times\tau\right)
^{-1}\left(  \operatorname*{Inv}\sigma\right)  \right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert
\]
(since the map $\tau\times\tau$ is a bijection (since $\tau\times\tau$ is
invertible)). Thus, $\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert =\ell\left(  \sigma\right)  $ (by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}). Hence,%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left\vert \operatorname*{Inv}\left(
\sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert
=\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(
\sigma\circ\tau\right)  \right) \\
&  =\ell\left(  \sigma\circ\tau\right)  -\ell\left(  \tau\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.exeadd.perm.Inv.sub.a.triv}%
)}\right)  .
\end{align*}
In other words, $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $.

Now, forget our assumption that $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. We thus have proven that
if $\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $, then $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $. In other words, we have proven the
implication (\ref{sol.exeadd.perm.Inv.sub.a.2}).

Combining the two implications (\ref{sol.exeadd.perm.Inv.sub.a.1}) and
(\ref{sol.exeadd.perm.Inv.sub.a.2}), we obtain the logical equivalence%
\[
\left(  \ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \right)  \ \Longleftrightarrow\ \left(
\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right)  .
\]
In other words, $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $. This solves Additional exercise \ref{exeadd.perm.Inv.sub}
\textbf{(a)}.

\textbf{(b)} Exercise \ref{exe.ps2.2.5} \textbf{(f)} (applied to $\sigma
\circ\tau$ instead of $\sigma$) yields $\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \underbrace{\left(  \sigma\circ\tau\right)  ^{-1}}_{=\tau
^{-1}\circ\sigma^{-1}}\right)  =\ell\left(  \tau^{-1}\circ\sigma^{-1}\right)
$. Exercise \ref{exe.ps2.2.5} \textbf{(f)} (applied to $\tau$ instead of
$\sigma$) yields $\ell\left(  \tau\right)  =\ell\left(  \tau^{-1}\right)  $.
Exercise \ref{exe.ps2.2.5} \textbf{(f)} yields $\ell\left(  \sigma\right)
=\ell\left(  \sigma^{-1}\right)  $. Hence, $\underbrace{\ell\left(
\sigma\right)  }_{=\ell\left(  \sigma^{-1}\right)  }+\underbrace{\ell\left(
\tau\right)  }_{=\ell\left(  \tau^{-1}\right)  }=\ell\left(  \sigma
^{-1}\right)  +\ell\left(  \tau^{-1}\right)  =\ell\left(  \tau^{-1}\right)
+\ell\left(  \sigma^{-1}\right)  $.

Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(a)} (applied to
$\tau^{-1}$ and $\sigma^{-1}$ instead of $\sigma$ and $\tau$) yields that
$\ell\left(  \tau^{-1}\circ\sigma^{-1}\right)  =\ell\left(  \tau^{-1}\right)
+\ell\left(  \sigma^{-1}\right)  $ holds if and only if $\operatorname*{Inv}%
\left(  \sigma^{-1}\right)  \subseteq\operatorname*{Inv}\left(  \tau^{-1}%
\circ\sigma^{-1}\right)  $. In light of the equalities $\ell\left(  \tau
^{-1}\circ\sigma^{-1}\right)  =\ell\left(  \sigma\circ\tau\right)  $ and
$\ell\left(  \tau^{-1}\right)  +\ell\left(  \sigma^{-1}\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $, we can rewrite this as follows:%
\[
\ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  \text{ holds if and only if }\operatorname*{Inv}\left(
\sigma^{-1}\right)  \subseteq\operatorname*{Inv}\left(  \tau^{-1}\circ
\sigma^{-1}\right)  .
\]
This solves Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(b)}.

\textbf{(c)} Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(a)}
(applied to $\tau\circ\sigma^{-1}$ and $\sigma$ instead of $\sigma$ and $\tau
$) yields that $\ell\left(  \tau\circ\sigma^{-1}\circ\sigma\right)
=\ell\left(  \tau\circ\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $ holds
if and only if $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\left(
\tau\circ\sigma^{-1}\circ\sigma\right)  $. In light of $\tau\circ
\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}}=\tau\circ
\operatorname*{id}=\tau$, this rewrites as follows:%
\[
\ell\left(  \tau\right)  =\ell\left(  \tau\circ\sigma^{-1}\right)
+\ell\left(  \sigma\right)  \text{ holds if and only if }\operatorname*{Inv}%
\sigma\subseteq\operatorname*{Inv}\tau.
\]
In other words, $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\tau$
holds if and only if $\ell\left(  \tau\right)  =\ell\left(  \tau\circ
\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $. This solves Additional
exercise \ref{exeadd.perm.Inv.sub} \textbf{(c)}.

\textbf{(d)} We WLOG assume that $\ell\left(  \sigma\right)  \geq\ell\left(
\tau\right)  $ (since otherwise, we can simply switch $\sigma$ and $\tau$). We
have $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\tau$ (since
$\operatorname*{Inv}\sigma=\operatorname*{Inv}\tau$). But Additional exercise
\ref{exeadd.perm.Inv.sub} \textbf{(c)} shows that $\operatorname*{Inv}%
\sigma\subseteq\operatorname*{Inv}\tau$ holds if and only if $\ell\left(
\tau\right)  =\ell\left(  \tau\circ\sigma^{-1}\right)  +\ell\left(
\sigma\right)  $. Hence, we have $\ell\left(  \tau\right)  =\ell\left(
\tau\circ\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $ (since
$\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\tau$ holds). Thus,
$\ell\left(  \tau\right)  =\ell\left(  \tau\circ\sigma^{-1}\right)
+\underbrace{\ell\left(  \sigma\right)  }_{\geq\ell\left(  \tau\right)  }%
\geq\ell\left(  \tau\circ\sigma^{-1}\right)  +\ell\left(  \tau\right)  $.
Subtracting $\ell\left(  \tau\right)  $ from both sides of this inequality, we
obtain $0\geq\ell\left(  \tau\circ\sigma^{-1}\right)  $. In other words,
$\ell\left(  \tau\circ\sigma^{-1}\right)  \leq0$.

But $\ell\left(  \tau\circ\sigma^{-1}\right)  $ is the number of inversions of
$\tau\circ\sigma^{-1}$ (by the definition of $\ell\left(  \tau\circ\sigma
^{-1}\right)  $), and thus is a nonnegative integer. Hence, $\ell\left(
\tau\circ\sigma^{-1}\right)  \geq0$. Combining this with $\ell\left(
\tau\circ\sigma^{-1}\right)  \leq0$, we obtain $\ell\left(  \tau\circ
\sigma^{-1}\right)  =0$. Thus, Corollary \ref{cor.sol.exe.ps2.2.5.d2} (applied
to $\tau\circ\sigma^{-1}$ instead of $\sigma$) yields $\tau\circ\sigma
^{-1}=\operatorname*{id}$. Hence, $\underbrace{\tau\circ\sigma^{-1}%
}_{=\operatorname*{id}}\circ\sigma=\operatorname*{id}\circ\sigma=\sigma$, so
that $\sigma=\tau\circ\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}%
}=\tau\circ\operatorname*{id}=\tau$. This solves Additional exercise
\ref{exeadd.perm.Inv.sub} \textbf{(d)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.3}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.3}.]We first notice a purely combinatorial
fact: For every $\sigma\in S_{n}$ satisfying $\sigma\neq\operatorname*{id}$,%
\begin{equation}
\text{there exists an }i\in\left\{  1,2,\ldots,n\right\}  \text{ such that
}\sigma\left(  i\right)  >i \label{sol.ps4.3.ineq}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.3.ineq}):} Let $\sigma\in S_{n}$ be
such that $\sigma\neq\operatorname*{id}$. We need to prove
(\ref{sol.ps4.3.ineq}).
\par
Assume the contrary. Thus, there exists no $i\in\left\{  1,2,\ldots,n\right\}
$ such that $\sigma\left(  i\right)  > i$. In other words, every $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies $\sigma\left(  i\right)  \leq i$.
\par
We shall now show that every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\sigma\left(  p\right)  =p. \label{sol.ps4.3.ineq.pf.goal}%
\end{equation}
\par
\textit{Proof of (\ref{sol.ps4.3.ineq.pf.goal}):} We will prove
(\ref{sol.ps4.3.ineq.pf.goal}) by strong induction over $p$. Thus, fix some
$P\in\left\{  1,2,\ldots,n\right\}  $. We assume that
(\ref{sol.ps4.3.ineq.pf.goal}) is proven for every $p<P$. We need to show that
(\ref{sol.ps4.3.ineq.pf.goal}) holds for $p=P$.
\par
We have assumed that (\ref{sol.ps4.3.ineq.pf.goal}) is proven for every $p<P$.
In other words,%
\begin{equation}
\sigma\left(  p\right)  =p\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }p<P.
\label{sol.ps4.3.ineq.pf.goal.pf.hyp}%
\end{equation}
\par
Now, we assume (for the sake of contradiction) that $\sigma\left(  P\right)
\neq P$. Recall that every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
$\sigma\left(  i\right)  \leq i$. Applying this to $i=P$, we obtain
$\sigma\left(  P\right)  \leq P$. Combined with $\sigma\left(  P\right)  \neq
P$, this yields $\sigma\left(  P\right)  <P$. Hence,
(\ref{sol.ps4.3.ineq.pf.goal.pf.hyp}) (applied to $p=\sigma\left(  P\right)
$) yields $\sigma\left(  \sigma\left(  P\right)  \right)  =\sigma\left(
P\right)  $.
\par
But the map $\sigma$ is a permutation (since $\sigma\in S_{n}$), thus
injective. Hence, from $\sigma\left(  \sigma\left(  P\right)  \right)
=\sigma\left(  P\right)  $, we obtain $\sigma\left(  P\right)  =P$. This
contradicts $\sigma\left(  P\right)  \neq P$. Hence, we have obtained a
contradiction; thus, our assumption (that $\sigma\left(  P\right)  \neq P$)
must have been wrong. We thus have $\sigma\left(  P\right)  =P$. In other
words, (\ref{sol.ps4.3.ineq.pf.goal}) holds for $p=P$. This completes the
inductive proof of (\ref{sol.ps4.3.ineq.pf.goal}).
\par
Now, (\ref{sol.ps4.3.ineq.pf.goal}) shows that every $p\in\left\{
1,2,\ldots,n\right\}  $ satisfies $\sigma\left(  p\right)
=p=\operatorname*{id}\left(  p\right)  $. In other words, $\sigma
=\operatorname*{id}$. This contradicts $\sigma\neq\operatorname*{id}$. This
contradiction proves that our assumption was wrong. Hence,
(\ref{sol.ps4.3.ineq}) is proven.}. Thus, for every $\sigma\in S_{n}$
satisfying $\sigma\neq\operatorname*{id}$, we have%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.ps4.3.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.3.0}):} Let $\sigma\in S_{n}$ be such
that $\sigma\neq\operatorname*{id}$. According to (\ref{sol.ps4.3.ineq}), we
know that there exists an $i\in\left\{  1,2,\ldots,n\right\}  $ such that
$\sigma\left(  i\right)  >i$. Let $k$ be such an $i$. Thus, $k$ is an element
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\sigma\left(  k\right)  >k$.
Hence, $k<\sigma\left(  k\right)  $.
\par
Recall that $a_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$. Applying this to $i=k$ and
$j=\sigma\left(  k\right)  $, we obtain $a_{k,\sigma\left(  k\right)  }=0$
(since $k<\sigma\left(  k\right)  $). Hence, one factor of the product
$\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ is $0$ (namely, the factor
$a_{k,\sigma\left(  k\right)  }$). Therefore, the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$ is $0$ (because if one factor of a product
is $0$, then the whole product is $0$).}.

Now, (\ref{eq.det.eq.2}) yields%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}\prod_{i=1}%
^{n}\underbrace{a_{i,\operatorname*{id}\left(  i\right)  }}_{=a_{i,i}}%
+\sum_{\substack{\sigma\in S_{n};\\\sigma\neq\operatorname*{id}}}\left(
-1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=0\\\text{(by (\ref{sol.ps4.3.0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have moved the addend for
}\sigma=\operatorname*{id}\text{ out of the sum}\right) \\
&  =\prod_{i=1}^{n}a_{i,i}+\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\neq\operatorname*{id}}}\left(  -1\right)  ^{\sigma}0}%
_{=0}=\prod_{i=1}^{n}a_{i,i}=a_{1,1}a_{2,2}\cdots a_{n,n}.
\end{align*}
This solves Exercise \ref{exe.ps4.3}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.4}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.4}.]The map $S_{n}\rightarrow
S_{n},\ \sigma\mapsto\sigma^{-1}$ (that is, the map from $S_{n}$ to $S_{n}$
which sends every permutation to its inverse) is a bijection\footnote{In fact,
this map is its own inverse: Indeed, every $\sigma\in S_{n}$ satisfies
$\left(  \sigma^{-1}\right)  ^{-1}=\sigma$.}.

Write $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Thus, $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A^{T}$). Hence, (\ref{eq.det.eq.2}) (applied to $A^{T}$
and $a_{j,i}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det\left(  A^{T}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}
\label{sol.ps4.4.short.3a}%
\end{equation}
(here, we have substituted $\sigma^{-1}$ for $\sigma$ in the sum, since the
map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma^{-1}$ is a bijection). But
every $\sigma\in S_{n}$ satisfies%
\[
\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}=\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }%
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $\sigma$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $, thus a bijection from
$\left\{  1,2,\ldots,n\right\}  $ to $\left\{  1,2,\ldots,n\right\}  $. Hence,
we can substitute $\sigma\left(  i\right)  $ for $i$ in the product
$\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}$. We thus obtain%
\[
\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}=\prod_{i=1}^{n}%
\underbrace{a_{\sigma^{-1}\left(  \sigma\left(  i\right)  \right)
,\sigma\left(  i\right)  }}_{=a_{i,\sigma\left(  i\right)  }}=\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  },
\]
qed.}. Hence, (\ref{sol.ps4.4.short.3a}) becomes%
\[
\det\left(  A^{T}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}}_{=\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\det
A\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.det.eq.2})}\right)  .
\]
This solves Exercise \ref{exe.ps4.4}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.4}.]Define a map $\Phi:S_{n}\rightarrow
S_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma^{-1}\ \ \ \ \ \ \ \ \ \ \text{for every
}\sigma\in S_{n}.
\]
Then, $\Phi\circ\Phi=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Every $\sigma\in S_{n}$ satisfies%
\begin{align*}
\left(  \Phi\circ\Phi\right)  \left(  \sigma\right)   &  =\Phi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\sigma^{-1}}\right)  =\Phi\left(
\sigma^{-1}\right)  =\left(  \sigma^{-1}\right)  ^{-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
Thus, $\Phi\circ\Phi=\operatorname*{id}$, qed.}. Hence, the maps $\Phi$ and
$\Phi$ are mutually inverse. Therefore, the map $\Phi$ is a bijection.

Write $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Thus, $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A^{T}$). Hence, (\ref{eq.det.eq.2}) (applied to $A^{T}$
and $a_{j,i}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  A^{T}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }}a_{\sigma\left(  i\right)  ,i}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\sigma
\left(  i\right)  ,i}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i\in\left\{
1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(  \sigma\right)  \right)  \left(
i\right)  ,i} \label{sol.ps4.4.3}%
\end{align}
(here, we have substituted $\Phi\left(  \sigma\right)  $ for $\sigma$ in the
sum, since the map $\Phi$ is a bijection). But every $\sigma\in S_{n}$
satisfies%
\[
\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  ,i}=\prod_{i\in\left\{
1,2,\ldots,n\right\}  }a_{i,\sigma\left(  i\right)  }%
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $\sigma$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $, thus a bijection from
$\left\{  1,2,\ldots,n\right\}  $ to $\left\{  1,2,\ldots,n\right\}  $. Hence,
we can substitute $\sigma\left(  i\right)  $ for $i$ in the product
$\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  ,i}$. We thus obtain%
\begin{align*}
\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  ,i}  &  =\prod_{i\in\left\{
1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(  \sigma\right)  \right)  \left(
\sigma\left(  i\right)  \right)  ,\sigma\left(  i\right)  }=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }\underbrace{a_{\sigma^{-1}\left(
\sigma\left(  i\right)  \right)  ,\sigma\left(  i\right)  }}%
_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(since }\sigma^{-1}\left(
\sigma\left(  i\right)  \right)  =i\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Phi\left(  \sigma\right)  =\sigma^{-1}\right) \\
&  =\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{i,\sigma\left(  i\right)  },
\end{align*}
qed.}. Hence, (\ref{sol.ps4.4.3}) becomes%
\begin{align*}
\det\left(  A^{T}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  ,i}}_{=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }a_{i,\sigma\left(  i\right)  }}%
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}}a_{i,\sigma\left(
i\right)  }\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }=\det A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.det.eq.2})}\right)  .
\end{align*}
This solves Exercise \ref{exe.ps4.4}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.ps4.5}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.5}.]Of course, both parts of Exercise
\ref{exe.ps4.5} can be solved directly using (\ref{eq.det.eq.1}). This
solution, however, is tedious (particularly for part \textbf{(b)} of this
exercise). Let us show a smarter way.

\textbf{(a)} Let $A$ be the matrix $\left(
\begin{matrix}
a & b & c & d\\
l & 0 & 0 & e\\
k & 0 & 0 & f\\
j & i & h & g
\end{matrix}
\right)  $. We want to find $\det A$.

We write the matrix $A$ in the form $A=\left(  a_{u,v}\right)  _{1\leq
u\leq4,\ 1\leq v\leq4}$\ \ \ \ \footnote{We cannot write $A=\left(
a_{i,j}\right)  _{1\leq i\leq4,\ 1\leq j\leq4}$ because the letters $i$ and
$j$ are already taken for something different.}. Thus,%
\begin{align*}
a_{1,1}  &  =a,\ \ \ \ \ \ \ \ \ \ a_{1,2}=b,\ \ \ \ \ \ \ \ \ \ a_{1,3}%
=c,\ \ \ \ \ \ \ \ \ \ a_{1,4}=d,\\
a_{2,1}  &  =l,\ \ \ \ \ \ \ \ \ \ a_{2,2}=0,\ \ \ \ \ \ \ \ \ \ a_{2,3}%
=0,\ \ \ \ \ \ \ \ \ \ a_{2,4}=e,\\
a_{3,1}  &  =k,\ \ \ \ \ \ \ \ \ \ a_{3,2}=0,\ \ \ \ \ \ \ \ \ \ a_{3,3}%
=0,\ \ \ \ \ \ \ \ \ \ a_{3,4}=f,\\
a_{4,1}  &  =j,\ \ \ \ \ \ \ \ \ \ a_{4,2}=i,\ \ \ \ \ \ \ \ \ \ a_{4,3}%
=h,\ \ \ \ \ \ \ \ \ \ a_{4,4}=g.
\end{align*}
Now, applying (\ref{eq.det.eq.1}) to $n=4$, we obtain%
\begin{equation}
\det A=\sum_{\sigma\in S_{4}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(  3\right)
}a_{4,\sigma\left(  4\right)  }. \label{sol.ps4.5.a.1}%
\end{equation}


The sum on the right hand side of (\ref{sol.ps4.5.a.1}) has $\left\vert
S_{4}\right\vert =4!=24$ addends. However, some of them are $0$. Namely, every
addend corresponding to a permutation $\sigma\in S_{4}$ satisfying
$\sigma\left(  2\right)  \notin\left\{  1,4\right\}  $ must be $0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{4}$ be such that
$\sigma\left(  2\right)  \notin\left\{  1,4\right\}  $. We must then show that
the addend on the right hand side of (\ref{sol.ps4.5.a.1}) corresponding to
this $\sigma$ must be $0$. In other words, we have to show that $\left(
-1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)  }a_{2,\sigma\left(  2\right)
}a_{3,\sigma\left(  3\right)  }a_{4,\sigma\left(  4\right)  }=0$.
\par
We have $\sigma\left(  2\right)  \notin\left\{  1,4\right\}  $, and thus
$\sigma\left(  2\right)  \in\left\{  2,3\right\}  $. Hence, $a_{2,\sigma
\left(  2\right)  }=0$ (because $a_{2,2}=0$ and $a_{2,3}=0$), and thus
$\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)  }%
\underbrace{a_{2,\sigma\left(  2\right)  }}_{=0}a_{3,\sigma\left(  3\right)
}a_{4,\sigma\left(  4\right)  }=0$, qed.}. Hence, all such addends can be
removed from the sum (without changing the value of this sum). Similarly, all
addends corresponding to permutations $\sigma\in S_{4}$ satisfying
$\sigma\left(  3\right)  \notin\left\{  1,4\right\}  $ must be $0$, and can
therefore also be removed from the sum. The addends that survive these two
removals are the ones that correspond to permutations $\sigma\in S_{4}$
satisfying $\sigma\left(  2\right)  \in\left\{  1,4\right\}  $ and
$\sigma\left(  3\right)  \in\left\{  1,4\right\}  $. It is easy to see that
there are exactly four such permutations: In one-line notation, these
permutations are $\left(  2,1,4,3\right)  $, $\left(  2,4,1,3\right)  $,
$\left(  3,1,4,2\right)  $ and $\left(  3,4,1,2\right)  $. The addends
corresponding to these permutations are $a_{1,2}a_{2,1}a_{3,4}a_{4,3}$,
$-a_{1,2}a_{2,4}a_{3,1}a_{4,3}$, $-a_{1,3}a_{2,1}a_{3,4}a_{4,2}$ and
$a_{1,3}a_{2,4}a_{3,1}a_{4,2}$. Hence, (\ref{sol.ps4.5.a.1}) simplifies to%
\begin{align*}
&  \det A\\
&  =\underbrace{a_{1,2}}_{=b}\underbrace{a_{2,1}}_{=l}\underbrace{a_{3,4}%
}_{=f}\underbrace{a_{4,3}}_{=h}-\underbrace{a_{1,2}}_{=b}\underbrace{a_{2,4}%
}_{=e}\underbrace{a_{3,1}}_{=k}\underbrace{a_{4,3}}_{=h}-\underbrace{a_{1,3}%
}_{=c}\underbrace{a_{2,1}}_{=l}\underbrace{a_{3,4}}_{=f}\underbrace{a_{4,2}%
}_{=i}+\underbrace{a_{1,3}}_{=c}\underbrace{a_{2,4}}_{=e}\underbrace{a_{3,1}%
}_{=k}\underbrace{a_{4,2}}_{=i}\\
&  =blfh-bekh-clfi+ceki.
\end{align*}
This is a simple enough formula to consider an answer to Exercise
\ref{exe.ps4.5} \textbf{(a)}, but we can simplify it even further. Namely,%
\[
\det A=blfh-bekh-clfi+ceki=\left(  bh-ci\right)  \left(  lf-ek\right)  .
\]
Exercise \ref{exe.ps4.5} \textbf{(a)} is solved.

\textbf{(b)} Let $A$ be the matrix $\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
f & 0 & 0 & 0 & g\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
\ell & m & n & o & p
\end{array}
\right)  $. We want to find $\det A$.

We write the matrix $A$ in the form $A=\left(  a_{u,v}\right)  _{1\leq
u\leq5,\ 1\leq v\leq5}$. Thus, $a_{1,1}=a$, $a_{1,2}=b$, etc.. For us, the
most important property of $A$ is that the $3\times3$-submatrix in the middle
of $A$ is filled with zeroes. In other words,%
\begin{equation}
a_{u,v}=0\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  2,3,4\right\}
\text{ and }v\in\left\{  2,3,4\right\}  . \label{sol.ps4.5.b.2}%
\end{equation}


Now, applying (\ref{eq.det.eq.1}) to $n=5$, we obtain%
\begin{equation}
\det A=\sum_{\sigma\in S_{5}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(  3\right)
}a_{4,\sigma\left(  4\right)  }a_{5,\sigma\left(  5\right)  }.
\label{sol.ps4.5.b.3}%
\end{equation}
But every $\sigma\in S_{5}$ satisfies $a_{2,\sigma\left(  2\right)
}a_{3,\sigma\left(  3\right)  }a_{4,\sigma\left(  4\right)  }=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{5}$. Then, $\sigma$ is a
permutation of $\left\{  1,2,3,4,5\right\}  $, and thus an injective map.
Therefore, the numbers $\sigma\left(  2\right)  ,\sigma\left(  3\right)
,\sigma\left(  4\right)  $ are pairwise distinct.
\par
We now claim that there exists an $u\in\left\{  2,3,4\right\}  $ such that
$\sigma\left(  u\right)  \in\left\{  2,3,4\right\}  $. In order to prove this,
we assume the contrary. Thus, every $u\in\left\{  2,3,4\right\}  $ satisfies
$\sigma\left(  u\right)  \notin\left\{  2,3,4\right\}  $. Hence, every
$u\in\left\{  2,3,4\right\}  $ satisfies $\sigma\left(  u\right)  \in\left\{
1,5\right\}  $ (since $\sigma\left(  u\right)  \in\left\{  1,2,3,4,5\right\}
$ but $\sigma\left(  u\right)  \notin\left\{  2,3,4\right\}  $). In other
words, the numbers $\sigma\left(  2\right)  ,\sigma\left(  3\right)
,\sigma\left(  4\right)  $ belong to $\left\{  1,5\right\}  $. Hence,
$\sigma\left(  2\right)  ,\sigma\left(  3\right)  ,\sigma\left(  4\right)  $
are three distinct numbers belonging to the set $\left\{  1,5\right\}  $. But
this is absurd, since the set $\left\{  1,5\right\}  $ does not contain three
distinct numbers. Hence, we have obtained a contradiction. This shows that our
assumption was wrong.
\par
We thus have shown that there exists an $u\in\left\{  2,3,4\right\}  $ such
that $\sigma\left(  u\right)  \in\left\{  2,3,4\right\}  $. Consider such a
$u$. Applying (\ref{sol.ps4.5.b.2}) to $v=\sigma\left(  u\right)  $, we now
obtain $a_{u,\sigma\left(  u\right)  }=0$. But $u\in\left\{  2,3,4\right\}  $,
so that $a_{u,\sigma\left(  u\right)  }$ is a factor in the product
$a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(  3\right)  }a_{4,\sigma
\left(  4\right)  }$. Hence, the product $a_{2,\sigma\left(  2\right)
}a_{3,\sigma\left(  3\right)  }a_{4,\sigma\left(  4\right)  }$ is $0$ (since
its factor $a_{u,\sigma\left(  u\right)  }$ is $0$), qed.}. Hence,
(\ref{sol.ps4.5.b.3}) becomes%
\[
\det A=\sum_{\sigma\in S_{5}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }\underbrace{a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(
3\right)  }a_{4,\sigma\left(  4\right)  }}_{=0}a_{5,\sigma\left(  5\right)
}=\sum_{\sigma\in S_{5}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }0a_{5,\sigma\left(  5\right)  }=0.
\]
Exercise \ref{exe.ps4.5} \textbf{(b)} is thus solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.6}}

Our solution to Exercise \ref{exe.ps4.6} relies on Lemma \ref{lem.det.sigma}.
Thus, the reader is advised to read the proof of said lemma before the
following solution.

\begin{proof}
[Solution to Exercise \ref{exe.ps4.6}.]Let us write the matrix $A$ in the form
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A\right)  =a_{i,j}. \label{sol.ps4.6.aij}%
\end{equation}


We let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} Let $B$ be an $n\times n$-matrix obtained from $A$ by switching
two rows. Thus, there exist two distinct elements $u$ and $v$ of $\left\{
1,2,\ldots,n\right\}  $ such that $B$ is the $n\times n$-matrix obtained from
$A$ by switching the $u$-th row with the $v$-th row. Consider these $u$ and
$v$.

We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$.

\begin{vershort}
Consider the transposition $t_{u,v}$ in $S_{n}$ (defined according to
Definition \ref{def.transpos}). Clearly, $t_{u,v}$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, thus a map $\left[
n\right]  \rightarrow\left[  n\right]  $. Also, $\left(  -1\right)  ^{t_{u,v}%
}=-1$ (by Exercise \ref{exe.ps4.1ab} \textbf{(b)}, applied to $i=u$ and $j=v$).

Recall that $B$ is the $n\times n$-matrix obtained from $A$ by switching the
$u$-th row with the $v$-th row. In other words, for all $k\in\left\{
1,2,\ldots,n\right\}  $, we have
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }%
t_{u,v}\left(  k\right)  \text{-th row of }A\right)
\label{sol.ps4.6.a.short.krow2}%
\end{equation}
(because $t_{u,v}$ is the permutation of $\left\{  1,2,\ldots,n\right\}  $
that switches $u$ with $v$ while leaving all other numbers fixed). Therefore,
every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\[
b_{i,j}=a_{t_{u,v}\left(  i\right)  ,j}%
\]
\footnote{\textit{Proof.} We have $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Thus, every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.a.short.bij.pf.1}%
\end{equation}
Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,
\begin{align*}
b_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }B\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.a.short.bij.pf.1})}\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }i\text{-th
row of }B}_{\substack{=\left(  \text{the }t_{u,v}\left(  i\right)  \text{-th
row of }A\right)  \\\text{(by (\ref{sol.ps4.6.a.short.krow2}), applied to
}k=i\text{)}}}\right) \\
&  =\left(  \text{the }j\text{-th entry of the }t_{u,v}\left(  i\right)
\text{-th row of }A\right) \\
&  =a_{t_{u,v}\left(  i\right)  ,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.aij}), applied to }t_{u,v}\left(  i\right)  \text{ instead of
}i\right)  ,
\end{align*}
qed.}. Hence, $B=\left(  \underbrace{b_{i,j}}_{=a_{t_{u,v}\left(  i\right)
,j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{t_{u,v}\left(
i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Therefore, we can
apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $t_{u,v}$, $A$, $a_{i,j}$ and
$B$ instead of $\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain $\det
B=\underbrace{\left(  -1\right)  ^{t_{u,v}}}_{=-1}\cdot\det A=-\det A$.
Exercise \ref{exe.ps4.6} \textbf{(a)} is thus solved.
\end{vershort}

\begin{verlong}
We know that $B$ is the $n\times n$-matrix obtained from $A$ by switching the
$u$-th row with the $v$-th row. In other words,%
\begin{align*}
\left(  \text{the }u\text{-th row of }B\right)   &  =\left(  \text{the
}v\text{-th row of }A\right)  ,\\
\left(  \text{the }v\text{-th row of }B\right)   &  =\left(  \text{the
}u\text{-th row of }A\right)  ,
\end{align*}
and%
\begin{align}
&  \left(  \left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the
}k\text{-th row of }A\right)  \right. \label{sol.ps4.6.a.krow}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }k\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }k\notin\left\{  u,v\right\}  \right)
.\nonumber
\end{align}


Consider the transposition $t_{u,v}$ in $S_{n}$ (defined according to
Definition \ref{def.transpos}). Clearly, $t_{u,v}$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, thus a map $\left[
n\right]  \rightarrow\left[  n\right]  $. Also, $\left(  -1\right)  ^{t_{u,v}%
}=-1$ (by Exercise \ref{exe.ps4.1ab} \textbf{(b)}, applied to $i=u$ and $j=v$).

The permutation $t_{u,v}$ is the permutation in $S_{n}$ which switches $u$
with $v$ while leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $
unchanged (according to the definition of $t_{u,v}$). In other words, we have
$t_{u,v}\left(  u\right)  =v$, $t_{u,v}\left(  v\right)  =u$ and%
\begin{equation}
\left(  t_{u,v}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for all }%
k\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{
u,v\right\}  \right)  . \label{sol.ps4.6.a.tuv}%
\end{equation}


Now, for all $k\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }%
t_{u,v}\left(  k\right)  \text{-th row of }A\right)  \label{sol.ps4.6.a.krow2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.a.krow2}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We need to prove (\ref{sol.ps4.6.a.krow2}). We are in
one of the following three cases:
\par
\textit{Case 1:} We have $k=u$.
\par
\textit{Case 2:} We have $k=v$.
\par
\textit{Case 3:} We have $k\notin\left\{  u,v\right\}  $.
\par
Let us first consider Case 1. In this case, we have $k=u$. Thus,
$t_{u,v}\left(  \underbrace{k}_{=u}\right)  =t_{u,v}\left(  u\right)  =v$, so
that $\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)
=\left(  \text{the }v\text{-th row of }A\right)  $. On the other hand,%
\begin{align*}
\left(  \text{the }\underbrace{k}_{=u}\text{-th row of }B\right)   &  =\left(
\text{the }u\text{-th row of }B\right)  =\left(  \text{the }v\text{-th row of
}A\right) \\
&  =\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.a.krow2}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $k=v$. Thus, $t_{u,v}\left(
\underbrace{k}_{=v}\right)  =t_{u,v}\left(  v\right)  =u$, so that $\left(
\text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  =\left(
\text{the }u\text{-th row of }A\right)  $. On the other hand,%
\begin{align*}
\left(  \text{the }\underbrace{k}_{=v}\text{-th row of }B\right)   &  =\left(
\text{the }v\text{-th row of }B\right)  =\left(  \text{the }u\text{-th row of
}A\right) \\
&  =\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.a.krow2}) is proven in Case 2.
\par
Finally, let us consider Case 3. In this case, we have $k\notin\left\{
u,v\right\}  $. Thus, $t_{u,v}\left(  k\right)  =k$ (by (\ref{sol.ps4.6.a.tuv}%
)), so that $\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of
}A\right)  =\left(  \text{the }k\text{-th row of }A\right)  $. On the other
hand,%
\begin{align*}
\left(  \text{the }k\text{-th row of }B\right)   &  =\left(  \text{the
}k\text{-th row of }A\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.a.krow})}\right) \\
&  =\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.a.krow2}) is proven in Case 3.
\par
We have now proven (\ref{sol.ps4.6.a.krow2}) in each of the three Cases 1, 2
and 3. Hence, (\ref{sol.ps4.6.a.krow2}) always holds, qed.}. Therefore, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
b_{i,j}=a_{t_{u,v}\left(  i\right)  ,j} \label{sol.ps4.6.a.bij}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.a.bij}):} We have $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.a.bij.pf.1}%
\end{equation}
Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,
\begin{align*}
b_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }B\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.a.bij.pf.1})}\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }i\text{-th
row of }B}_{\substack{=\left(  \text{the }t_{u,v}\left(  i\right)  \text{-th
row of }A\right)  \\\text{(by (\ref{sol.ps4.6.a.krow2}), applied to
}k=i\text{)}}}\right) \\
&  =\left(  \text{the }j\text{-th entry of the }t_{u,v}\left(  i\right)
\text{-th row of }A\right) \\
&  =a_{t_{u,v}\left(  i\right)  ,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.aij}), applied to }t_{u,v}\left(  i\right)  \text{ instead of
}i\right)  ,
\end{align*}
qed.}. Hence, $B=\left(  \underbrace{b_{i,j}}_{=a_{t_{u,v}\left(  i\right)
,j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{t_{u,v}\left(
i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Therefore, we can
apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $t_{u,v}$, $A$, $a_{i,j}$ and
$B$ instead of $\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain $\det
B=\underbrace{\left(  -1\right)  ^{t_{u,v}}}_{=-1}\cdot\det A=-\det A$.
Exercise \ref{exe.ps4.6} \textbf{(a)} is thus solved.
\end{verlong}

\textbf{(b)} Let $B$ be an $n\times n$-matrix obtained from $A$ by switching
two columns. Thus, $B^{T}$ is an $n\times n$-matrix obtained from $A^{T}$ by
switching two rows (because the columns of $A$ correspond to the rows of
$A^{T}$\ \ \ \ \footnote{More precisely, the columns of $A$ are the transposes
of the rows of $A^{T}$.}). Hence, Exercise \ref{exe.ps4.6} \textbf{(a)}
(applied to $A^{T}$ and $B^{T}$ instead of $A$ and $B$) yields $\det\left(
B^{T}\right)  =-\det\left(  A^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. But recall that $\det\left(  B^{T}\right)
=-\det\left(  A^{T}\right)  $. This rewrites as $\det B=-\det A$ (since
$\det\left(  B^{T}\right)  =\det B$ and $\det\left(  A^{T}\right)  =\det A$).
This solves Exercise \ref{exe.ps4.6} \textbf{(b)}.

\textbf{(c)} Assume that a row of $A$ consists of zeroes. Thus, there exists a
$u\in\left\{  1,2,\ldots,n\right\}  $ such that the $u$-th row of $A$ consists
of zeroes. Consider this $u$.

\begin{vershort}
The $u$-th row of the matrix $A$ consists of zeroes. In other words,
\begin{equation}
a_{u,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,\ldots
,n\right\}  . \label{sol.ps4.6.c.short.1}%
\end{equation}
Now, every $\sigma\in S_{n}$ satisfies $\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }=0$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then,
the $u$-th factor of the product $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}$ is $a_{u,\sigma\left(  u\right)  }=0$ (by (\ref{sol.ps4.6.c.short.1}),
applied to $j=\sigma\left(  u\right)  $). Hence, the whole product is $0$. In
other words, we have $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0$, qed.}.
Thus, (\ref{eq.det.eq.2}) shows that%
\[
\det A=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{=0}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}0=0.
\]
This solves Exercise \ref{exe.ps4.6} \textbf{(c)}.
\end{vershort}

\begin{verlong}
We have $\left(  \text{the }u\text{-th row of }A\right)  =\underbrace{\left(
0,0,\ldots,0\right)  }_{n\text{ zeroes}}$ (since the $u$-th row of $A$
consists of zeroes). Thus,%
\begin{equation}
a_{u,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,\ldots
,n\right\}  \label{sol.ps4.6.c.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.c.1}):} Let every $j\in\left\{
1,2,\ldots,n\right\}  $. Applying (\ref{sol.ps4.6.aij}) to $i=u$, we obtain%
\[
\left(  \text{the }\left(  u,j\right)  \text{-th entry of the matrix
}A\right)  =a_{u,j}.
\]
Hence,%
\begin{align*}
a_{u,j}  &  =\left(  \text{the }\left(  u,j\right)  \text{-th entry of the
matrix }A\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }A}_{=\underbrace{\left(  0,0,\ldots,0\right)  }_{n\text{ zeroes}}%
}\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\left(  0,0,\ldots
,0\right)  }_{n\text{ zeroes}}\right)  =0.
\end{align*}
This proves (\ref{sol.ps4.6.c.1}).}. Now, every $\sigma\in S_{n}$ satisfies%
\[
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $a_{u,\sigma\left(
u\right)  }=0$ (by (\ref{sol.ps4.6.c.1})). But $a_{u,\sigma\left(  u\right)
}$ is a factor of the product $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$
(namely, the factor for $i=u$). Hence, one factor of the product $\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$ (since $a_{u,\sigma
\left(  u\right)  }=0$). Therefore, the whole product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$ (because if one factor of a
product equals $0$, then the whole product equals $0$). Qed.}. Now,
(\ref{eq.det.eq.2}) shows that%
\[
\det A=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{=0}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}0=0.
\]
This solves Exercise \ref{exe.ps4.6} \textbf{(c)}.
\end{verlong}

\textbf{(d)} Assume that a column of $A$ consists of zeroes. Thus, a row of
$A^{T}$ consists of zeroes (because the columns of $A$ correspond to the rows
of $A^{T}$\ \ \ \ \footnote{More precisely, the columns of $A$ are the
transposes of the rows of $A^{T}$.}). Hence, Exercise \ref{exe.ps4.6}
\textbf{(c)} (applied to $A^{T}$ instead of $A$) yields $\det\left(
A^{T}\right)  =0$.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. This
rewrites as $\det A=0$ (since $\det\left(  A^{T}\right)  =\det A$). This
solves Exercise \ref{exe.ps4.6} \textbf{(d)}.

\textbf{(e)} Assume that $A$ has two equal rows. In other words, some two
distinct rows of $A$ are equal (where \textquotedblleft
distinct\textquotedblright\ means that these rows are in different positions,
not that they are distinct as row vectors). In other words, there exist two
distinct elements $u$ and $v$ of $\left\{  1,2,\ldots,n\right\}  $ such that%
\begin{equation}
\left(  \text{the }u\text{-th row of }A\right)  =\left(  \text{the }v\text{-th
row of }A\right)  . \label{sol.ps4.6.e.1}%
\end{equation}
Consider these $u$ and $v$.

\begin{vershort}
The equality (\ref{sol.ps4.6.e.1}) shows that%
\begin{equation}
a_{u,j}=a_{v,j}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{
1,2,\ldots,n\right\}  \label{sol.ps4.6.e.short.1a}%
\end{equation}
(because $a_{u,j}$ is the $j$-th entry of the $u$-th row of $A$, while
$a_{v,j}$ is the $j$-th entry of the $v$-th row of $A$).

Define a map $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ by
\[
\left(  \kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n\right]  \right)
.
\]
The definition of $\kappa$ shows that $\kappa\left(  v\right)  =v$ (since
$v\neq u$) but also $\kappa\left(  u\right)  =v$. Thus, $\kappa\left(
u\right)  =v=\kappa\left(  v\right)  $, in spite of $u\neq v$. Therefore, the
map $\kappa$ is not injective, and thus not bijective; in particular, $\kappa$
is not a permutation. Thus, $\kappa\notin S_{n}$. But every $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy%
\begin{equation}
a_{\kappa\left(  i\right)  ,j}=a_{i,j} \label{sol.ps4.6.e.short.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.e.short.3}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We must
prove (\ref{sol.ps4.6.e.short.3}). We must be in one of the following two
cases:
\par
\textit{Case 1:} We have $i\neq u$.
\par
\textit{Case 2:} We have $i=u$.
\par
Let us first consider Case 1. In this case, we have $i\neq u$. Thus, the
definition of $\kappa$ yields $\kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  =i$ (since $i\neq u$), so that $a_{\kappa\left(  i\right)
,j}=a_{i,j}$. Thus, (\ref{sol.ps4.6.e.short.3}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i=u$. Thus, $\kappa\left(
i\right)  =\kappa\left(  u\right)  =v$ (since $i=u$). Hence,%
\begin{align*}
a_{\kappa\left(  i\right)  ,j}  &  =a_{v,j}=a_{u,j}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.ps4.6.e.short.1a})}\right) \\
&  =a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }u=i\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.e.short.3}) is proven in Case 2.
\par
We have now proven (\ref{sol.ps4.6.e.short.3}) in both Cases 1 and 2. Thus,
(\ref{sol.ps4.6.e.short.3}) always holds, qed.}. Thus, $\left(
\underbrace{a_{\kappa\left(  i\right)  ,j}}_{=a_{i,j}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
=A$. Therefore, we can apply Lemma \ref{lem.det.sigma} \textbf{(b)} to $A$,
$a_{i,j}$ and $B$ instead of $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain
$\det A=0$. Exercise \ref{exe.ps4.6} \textbf{(e)} is thus solved.
\end{vershort}

\begin{verlong}
Define a map $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ by
\[
\left(  \kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n\right]  \right)
.
\]
Then, $\kappa\notin S_{n}$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Then, $\kappa\in S_{n}$. Thus, $\kappa$ is a permutation. Hence, the
map $\kappa$ is injective. But the definition of $\kappa$ yields
$\kappa\left(  u\right)  =\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }u\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }u=u
\end{array}
\right.  =v$ (since $u=u$). Also, $u$ and $v$ are distinct, so that we have
$v\neq u$. Hence, $\kappa\left(  v\right)  \neq\kappa\left(  u\right)  $
(since $\kappa$ is injective). But the definition of $\kappa$ yields
$\kappa\left(  v\right)  =\left\{
\begin{array}
[c]{c}%
v,\ \ \ \ \ \ \ \ \ \ \text{if }v\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }v=u
\end{array}
\right.  =v=\kappa\left(  u\right)  $. This contradicts $\kappa\left(
v\right)  \neq\kappa\left(  u\right)  $. This contradiction proves that our
assumption was wrong, qed.}. But every $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy%
\begin{equation}
a_{\kappa\left(  i\right)  ,j}=a_{i,j} \label{sol.ps4.6.e.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.e.3}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We must
prove (\ref{sol.ps4.6.e.3}). We must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq u$.
\par
\textit{Case 2:} We have $i=u$.
\par
Let us first consider Case 1. In this case, we have $i\neq u$. Thus, the
definition of $\kappa$ yields $\kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  =i$ (since $i\neq u$), so that $a_{\kappa\left(  i\right)
,j}=a_{i,j}$. Thus, (\ref{sol.ps4.6.e.3}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i=u$. Thus, the definition
of $\kappa$ yields $\kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  =v$ (since $i=u$). Hence,%
\begin{align*}
a_{\kappa\left(  i\right)  ,j}  &  =a_{v,j}=\left(  \text{the }\left(
v,j\right)  \text{-th entry of the matrix }A\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  \text{the }\left(  v,j\right)  \text{-th entry of the
matrix }A\right)  =a_{v,j}\\
\text{(by (\ref{sol.ps4.6.aij}), applied to }v\text{ instead of }i\text{)}%
\end{array}
\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }v\text{-th
row of }A}_{\substack{=\left(  \text{the }u\text{-th row of }A\right)
\\\text{(by (\ref{sol.ps4.6.e.1}))}}}\right)  =\left(  \text{the }j\text{-th
entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,j\right)  \text{-th entry of the matrix
}A\right)  =a_{u,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6.aij}),
applied to }u\text{ instead of }i\right) \\
&  =a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }u=i\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.e.3}) is proven in Case 2.
\par
We have now proven (\ref{sol.ps4.6.e.3}) in both Cases 1 and 2. Thus,
(\ref{sol.ps4.6.e.3}) always holds, qed.}. Thus, $\left(
\underbrace{a_{\kappa\left(  i\right)  ,j}}_{=a_{i,j}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
=A$. Therefore, we can apply Lemma \ref{lem.det.sigma} \textbf{(b)} to $A$,
$a_{i,j}$ and $B$ instead of $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain
$\det A=0$. Exercise \ref{exe.ps4.6} \textbf{(e)} is thus solved.
\end{verlong}

\textbf{(f)} Assume that $A$ has two equal columns. Thus, $A^{T}$ has two
equal rows (because the columns of $A$ correspond to the rows of $A^{T}%
$\ \ \ \ \footnote{More precisely, the columns of $A$ are the transposes of
the rows of $A^{T}$.}). Hence, Exercise \ref{exe.ps4.6} \textbf{(e)} (applied
to $A^{T}$ instead of $A$) yields $\det\left(  A^{T}\right)  =0$.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. This
rewrites as $\det A=0$ (since $\det\left(  A^{T}\right)  =\det A$). This
solves Exercise \ref{exe.ps4.6} \textbf{(f)}.

\textbf{(g)} Let $B$ be the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th row by $\lambda$. Thus,
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\lambda\left(  \text{the
}k\text{-th row of }A\right)  , \label{sol.ps4.6.g.krow}%
\end{equation}
whereas%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6.g.urow}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  .\nonumber
\end{align}


We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Thus, every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.g.bij}%
\end{equation}


\begin{vershort}
For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.g.short.buv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.short.buv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.g.urow}) shows that the $v$-th entry of the
$u$-th row of $B$ equals the $v$-th entry of the $u$-th row of $A$. Since the
former entry is $b_{u,v}$, while the latter entry equals $a_{u,v}$, this
rewrites as $b_{u,v}=a_{u,v}$. This proves (\ref{sol.ps4.6.g.short.buv}).}.
For every $v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=\lambda a_{k,v} \label{sol.ps4.6.g.short.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.short.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.g.krow}) shows that the $v$-th
entry of the $k$-th row of $B$ equals $\lambda$ times the $v$-th entry of the
$k$-th row of $A$. Since the former entry is $b_{k,v}$, while the latter entry
equals $a_{k,v}$, this rewrites as $b_{k,v}=\lambda a_{k,v}$. This proves
(\ref{sol.ps4.6.g.short.bkv}).}. Now, it is easy to see that every $\sigma\in
S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  } \label{sol.ps4.6.g.short.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.short.sigma}):} Let $\sigma\in
S_{n}$. Taking the $k$-th factor out of the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$, we obtain
\[
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)
}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}a_{i,\sigma\left(  i\right)  }.
\]
Multiplying both sides of this equality by $\lambda$, we obtain%
\[
\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\lambda a_{k,\sigma
\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }.
\]
Compared with%
\begin{align*}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }  &  =\underbrace{b_{k,\sigma
\left(  k\right)  }}_{\substack{=\lambda a_{k,\sigma\left(  k\right)
}\\\text{(by (\ref{sol.ps4.6.g.short.bkv}), applied to}\\v=\sigma\left(
k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }}%
_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.g.short.buv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\in\left\{
1,2,\ldots,n\right\}  \right) \\
&  =\lambda a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  },
\end{align*}
this yields $\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$. This proves
(\ref{sol.ps4.6.g.short.sigma}).}. Now, recall that $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B$
and $b_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\lambda
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.g.short.sigma}))}}}=\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\lambda\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}=\lambda\det A.
\end{align*}
Thus, Exercise \ref{exe.ps4.6} \textbf{(g)} is solved.
\end{vershort}

\begin{verlong}
For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.g.buv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.buv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.g.bij}) (applied to $i=u$ and $j=v$) yields%
\[
\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}B\right)  =b_{u,v},
\]
so that%
\begin{align*}
b_{u,v}  &  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }B}_{\substack{=\left(  \text{the }u\text{-th row of }A\right)
\\\text{(by (\ref{sol.ps4.6.g.urow}))}}}\right) \\
&  =\left(  \text{the }v\text{-th entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A\right)  =a_{u,v}%
\end{align*}
(by (\ref{sol.ps4.6.aij}), applied to $i=u$ and $j=v$), qed.}. For every
$v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=\lambda a_{k,v} \label{sol.ps4.6.g.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.g.bij}) (applied to $i=k$ and
$j=v$) yields%
\[
\left(  \text{the }\left(  k,v\right)  \text{-th entry of the matrix
}B\right)  =b_{k,v},
\]
so that%
\begin{align*}
b_{k,v}  &  =\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }k\text{-th
row of }B}_{=\lambda\left(  \text{the }k\text{-th row of }A\right)  }\right)
\\
&  =\left(  \text{the }v\text{-th entry of }\lambda\left(  \text{the
}k\text{-th row of }A\right)  \right) \\
&  =\lambda\underbrace{\left(  \text{the }v\text{-th entry of the }k\text{-th
row of }A\right)  }_{=\left(  \text{the }\left(  k,v\right)  \text{-th entry
of the matrix }A\right)  }\\
&  =\lambda\underbrace{\left(  \text{the }\left(  k,v\right)  \text{-th entry
of the matrix }A\right)  }_{\substack{=a_{k,v}\\\text{(by (\ref{sol.ps4.6.aij}%
), applied to }i=k\text{ and }j=v\text{)}}}=\lambda a_{k,v},
\end{align*}
qed.}. Now, it is easy to see that every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  } \label{sol.ps4.6.g.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.sigma}):} Let $\sigma\in S_{n}$.
We have%
\[
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }=\prod_{i\in\left\{  1,2,\ldots,n\right\}
}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)  }\cdot
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma
\left(  i\right)  }%
\]
(since $k\in\left\{  1,2,\ldots,n\right\}  $). Multiplying both sides of this
equality by $\lambda$, we obtain%
\[
\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\lambda a_{k,\sigma
\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }.
\]
Compared with%
\begin{align*}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}b_{i,\sigma\left(  i\right)  }  &  =\prod_{i\in\left\{  1,2,\ldots,n\right\}
}b_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{b_{k,\sigma\left(  k\right)  }}_{\substack{=\lambda
a_{k,\sigma\left(  k\right)  }\\\text{(by (\ref{sol.ps4.6.g.bkv}), applied
to}\\v=\sigma\left(  k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by (\ref{sol.ps4.6.g.buv}%
), applied to}\\u=i\text{ and }v=\sigma\left(  i\right)  \text{)}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\in\left\{  1,2,\ldots,n\right\}
\right) \\
&  =\lambda a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  },
\end{align*}
this yields $\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$. This proves
(\ref{sol.ps4.6.g.sigma}).}. Now, recall that $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B$
and $b_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\lambda
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.g.sigma}))}}}=\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\lambda\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}=\lambda\det A.
\end{align*}
Thus, Exercise \ref{exe.ps4.6} \textbf{(g)} is solved.
\end{verlong}

\textbf{(h)} Let $B$ be the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th column by $\lambda$. Thus, $B^{T}$ is the $n\times
n$-matrix obtained from $A^{T}$ by multiplying the $k$-th row by $\lambda$
(because the columns of $A$ correspond to the rows of $A^{T}$%
\ \ \ \ \footnote{More precisely, the columns of $A$ are the transposes of the
rows of $A^{T}$.}). Hence, Exercise \ref{exe.ps4.6} \textbf{(g)} (applied to
$A^{T}$ and $B^{T}$ instead of $A$ and $B$) yields $\det\left(  B^{T}\right)
=\lambda\det\left(  A^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. But recall that $\det\left(  B^{T}\right)
=\lambda\det\left(  A^{T}\right)  $. This rewrites as $\det B=\lambda\det A$
(since $\det\left(  B^{T}\right)  =\det B$ and $\det\left(  A^{T}\right)
=\det A$). This solves Exercise \ref{exe.ps4.6} \textbf{(h)}.

\textbf{(i)} We know that the rows of the matrix $A^{\prime}$ equal the
corresponding rows of $A$ except (perhaps) the $k$-th row. In other words,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }A^{\prime}\right)  =\left(
\text{the }u\text{-th row of }A\right)  \right. \label{sol.ps4.6.i.urowA'}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  .\nonumber
\end{align}


We know that $B$ is the $n\times n$-matrix obtained from $A$ by adding the
$k$-th row of $A^{\prime}$ to the $k$-th row of $A$. Hence,%
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }k\text{-th
row of }A\right)  +\left(  \text{the }k\text{-th row of }A^{\prime}\right)
\label{sol.ps4.6.i.krowB}%
\end{equation}
and%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6.i.urowB}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  .\nonumber
\end{align}


\begin{vershort}
We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, and we write the matrix $A^{\prime}$ in the form
$A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
$. Then, for every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
a_{u,v}^{\prime}=a_{u,v} \label{sol.ps4.6.i.short.a'auv}%
\end{equation}
\footnote{This follows from (\ref{sol.ps4.6.i.urowA'}) in a similar way as
(\ref{sol.ps4.6.g.short.buv}) follows from (\ref{sol.ps4.6.g.urow}).} and%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.i.short.bauv}%
\end{equation}
\footnote{This follows from (\ref{sol.ps4.6.i.urowB}) in a similar way as
(\ref{sol.ps4.6.g.short.buv}) follows from (\ref{sol.ps4.6.g.urow}).}. Also,
for every $v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=a_{k,v}+a_{k,v}^{\prime} \label{sol.ps4.6.i.short.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.short.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.i.krowB}) shows that
\begin{align*}
&  \left(  \text{the }v\text{-th entry of the }k\text{-th row of }B\right) \\
&  =\left(  \text{the }v\text{-th entry of the }k\text{-th row of }A\right)
+\left(  \text{the }v\text{-th entry of the }k\text{-th row of }A^{\prime
}\right)  .
\end{align*}
But the three entries appearing in this equality are $b_{k,v}$, $a_{k,v}$ and
$a_{k,v}^{\prime}$ (in this order). Thus, this equality rewrites as
$b_{k,v}=a_{k,v}+a_{k,v}^{\prime}$. This proves (\ref{sol.ps4.6.i.short.bkv}%
).}. Now, it is easy to see that every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\prod_{i=1}^{n}a_{i,\sigma
\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}
\label{sol.ps4.6.i.short.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.short.sigma}):} Let $\sigma\in
S_{n}$. Pulling the $k$-th factor out of the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$, we obtain%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)
}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}a_{i,\sigma\left(  i\right)  } \label{sol.ps4.6.i.short.sigma.pf.1}%
\end{equation}
(since $k\in\left\{  1,2,\ldots,n\right\}  $). Similarly,%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}=a_{k,\sigma\left(
k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\underbrace{a_{i,\sigma\left(  i\right)  }^{\prime}}%
_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.short.a'auv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}=a_{k,\sigma\left(  k\right)  }^{\prime}\cdot
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma
\left(  i\right)  } \label{sol.ps4.6.i.short.sigma.pf.2}%
\end{equation}
and%
\begin{align*}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }  &  =\underbrace{b_{k,\sigma
\left(  k\right)  }}_{\substack{=a_{k,\sigma\left(  k\right)  }+a_{k,\sigma
\left(  k\right)  }^{\prime}\\\text{(by (\ref{sol.ps4.6.i.short.bkv}), applied
to}\\v=\sigma\left(  k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.short.bauv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}\\
&  =\left(  a_{k,\sigma\left(  k\right)  }+a_{k,\sigma\left(  k\right)
}^{\prime}\right)  \cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.short.sigma.pf.1}))}}}+\underbrace{a_{k,\sigma\left(
k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }^{\prime}\\\text{(by
(\ref{sol.ps4.6.i.short.sigma.pf.2}))}}}\\
&  =\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }^{\prime}.
\end{align*}
This proves (\ref{sol.ps4.6.i.short.sigma}).}.
\end{vershort}

\begin{verlong}
We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Thus, every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.i.bij}%
\end{equation}
We write the matrix $A^{\prime}$ in the form $A^{\prime}=\left(
a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A^{\prime}\right)  =a_{i,j}^{\prime}. \label{sol.ps4.6.i.a'ij}%
\end{equation}


For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.i.buv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.buv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.i.bij}) (applied to $i=u$ and $j=v$) yields%
\[
\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}B\right)  =b_{u,v},
\]
so that%
\begin{align*}
b_{u,v}  &  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }B}_{\substack{=\left(  \text{the }u\text{-th row of }A\right)
\\\text{(by (\ref{sol.ps4.6.i.urowB}))}}}\right) \\
&  =\left(  \text{the }v\text{-th entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A\right)  =a_{u,v}%
\end{align*}
(by (\ref{sol.ps4.6.aij}), applied to $i=u$ and $j=v$), qed.} and%
\begin{equation}
b_{u,v}=a_{u,v}^{\prime} \label{sol.ps4.6.i.a'uv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.a'uv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.i.a'ij}) (applied to $i=u$ and $j=v$) yields%
\[
\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A^{\prime}\right)  =a_{u,v}^{\prime},
\]
so that%
\begin{align*}
a_{u,v}^{\prime}  &  =\left(  \text{the }\left(  u,v\right)  \text{-th entry
of the matrix }A^{\prime}\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }A^{\prime}}_{\substack{=\left(  \text{the }u\text{-th row of
}A\right)  \\\text{(by (\ref{sol.ps4.6.i.urowA'}))}}}\right) \\
&  =\left(  \text{the }v\text{-th entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A\right)  =a_{u,v}%
\end{align*}
(by (\ref{sol.ps4.6.aij}), applied to $i=u$ and $j=v$). Compared with
(\ref{sol.ps4.6.i.buv}), this yields $b_{u,v}=a_{u,v}^{\prime}$, qed.}. For
every $v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=a_{k,v}+a_{k,v}^{\prime} \label{sol.ps4.6.i.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.i.bij}) (applied to $i=k$ and
$j=v$) yields%
\[
\left(  \text{the }\left(  k,v\right)  \text{-th entry of the matrix
}B\right)  =b_{k,v},
\]
so that%
\begin{align*}
b_{k,v}  &  =\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }k\text{-th
row of }B}_{=\left(  \text{the }k\text{-th row of }A\right)  +\left(
\text{the }k\text{-th row of }A^{\prime}\right)  }\right) \\
&  =\left(  \text{the }v\text{-th entry of }\left(  \text{the }k\text{-th row
of }A\right)  +\left(  \text{the }k\text{-th row of }A^{\prime}\right)
\right) \\
&  =\underbrace{\left(  \text{the }v\text{-th entry of the }k\text{-th row of
}A\right)  }_{=\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }A\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the }v\text{-th entry of the
}k\text{-th row of }A^{\prime}\right)  }_{=\left(  \text{the }\left(
k,v\right)  \text{-th entry of the matrix }A^{\prime}\right)  }\\
&  =\underbrace{\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }A\right)  }_{\substack{=a_{k,v}\\\text{(by (\ref{sol.ps4.6.aij}),
applied to }i=k\text{ and }j=v\text{)}}}+\underbrace{\left(  \text{the
}\left(  k,v\right)  \text{-th entry of the matrix }A^{\prime}\right)
}_{\substack{=a_{k,v}^{\prime}\\\text{(by (\ref{sol.ps4.6.i.a'ij}), applied to
}i=k\text{ and }j=v\text{)}}}\\
&  =a_{k,v}+a_{k,v}^{\prime},
\end{align*}
qed.}. Now, it is easy to see that every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\prod_{i=1}^{n}a_{i,\sigma
\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}
\label{sol.ps4.6.i.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.sigma}):} Let $\sigma\in S_{n}$.
We have%
\begin{equation}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }=\prod_{i\in\left\{  1,2,\ldots,n\right\}
}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)  }\cdot
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma
\left(  i\right)  } \label{sol.ps4.6.i.sigma.pf.1}%
\end{equation}
(since $k\in\left\{  1,2,\ldots,n\right\}  $) and%
\begin{equation}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }^{\prime}=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }a_{i,\sigma\left(  i\right)  }^{\prime}=a_{k,\sigma\left(
k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }^{\prime}
\label{sol.ps4.6.i.sigma.pf.2}%
\end{equation}
(since $k\in\left\{  1,2,\ldots,n\right\}  $). Furthermore,%
\begin{align*}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}b_{i,\sigma\left(  i\right)  }  &  =\prod_{i\in\left\{  1,2,\ldots,n\right\}
}b_{i,\sigma\left(  i\right)  }=\underbrace{b_{k,\sigma\left(  k\right)  }%
}_{\substack{=a_{k,\sigma\left(  k\right)  }+a_{k,\sigma\left(  k\right)
}^{\prime}\\\text{(by (\ref{sol.ps4.6.i.bkv}), applied to}\\v=\sigma\left(
k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}b_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\in\left\{  1,2,\ldots,n\right\}
\right) \\
&  =\left(  a_{k,\sigma\left(  k\right)  }+a_{k,\sigma\left(  k\right)
}^{\prime}\right)  \cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}b_{i,\sigma\left(  i\right)  }\\
&  =a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by (\ref{sol.ps4.6.i.buv}%
), applied to}\\u=i\text{ and }v=\sigma\left(  i\right)  \text{)}%
}}+a_{k,\sigma\left(  k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }^{\prime}\\\text{(by
(\ref{sol.ps4.6.i.a'uv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}\\
&  =\underbrace{a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.sigma.pf.1}))}}}+\underbrace{a_{k,\sigma\left(  k\right)
}^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}a_{i,\sigma\left(  i\right)  }^{\prime}}_{\substack{=\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }^{\prime}\\\text{(by
(\ref{sol.ps4.6.i.sigma.pf.2}))}}}\\
&  =\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }^{\prime}.
\end{align*}
This proves (\ref{sol.ps4.6.i.sigma}).}.
\end{verlong}

We have $A=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$;
therefore, (\ref{eq.det.eq.2}) (applied to $A^{\prime}$ and $a_{i,j}^{\prime}$
instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det A^{\prime}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}. \label{sol.ps4.6.i.detA'}%
\end{equation}


\begin{vershort}
Now, recall that $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Hence, (\ref{eq.det.eq.2}) (applied to $B$ and $b_{i,j}$ instead of $A$ and
$a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }^{\prime}\\\text{(by (\ref{sol.ps4.6.i.short.sigma}))}}%
}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}^{\prime}\right) \\
&  =\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}+\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}}%
_{\substack{=\det A^{\prime}\\\text{(by (\ref{sol.ps4.6.i.detA'}))}}}=\det
A+\det A^{\prime}.
\end{align*}
This solves Exercise \ref{exe.ps4.6} \textbf{(i)}.
\end{vershort}

\begin{verlong}
Now, recall that $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Hence, (\ref{eq.det.eq.2}) (applied to $B$ and $b_{i,j}$ instead of $A$ and
$a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }^{\prime}\\\text{(by (\ref{sol.ps4.6.i.sigma}))}}}=\sum_{\sigma\in
S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}\right) \\
&  =\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}+\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}}%
_{\substack{=\det A^{\prime}\\\text{(by (\ref{sol.ps4.6.i.detA'}))}}}=\det
A+\det A^{\prime}.
\end{align*}
This solves Exercise \ref{exe.ps4.6} \textbf{(i)}.
\end{verlong}

\textbf{(j)} We know that $A^{\prime}$ is the $n\times n$-matrix whose columns
equal the corresponding columns of $A$ except (perhaps) the $k$-th column.
Thus, $\left(  A^{\prime}\right)  ^{T}$ is the $n\times n$-matrix whose rows
equal the corresponding rows of $A^{T}$ except (perhaps) the $k$-th row
(because the columns of $A$ correspond to the rows of $A^{T}$%
\ \ \ \ \footnote{More precisely, the columns of $A$ are the transposes of the
rows of $A^{T}$.}).

Also, we know that $B$ is the $n\times n$-matrix obtained from $A$ by adding
the $k$-th column of $A^{\prime}$ to the $k$-th column of $A$. Hence, $B^{T}$
is the $n\times n$-matrix obtained from $A^{T}$ by adding the $k$-th row of
$\left(  A^{\prime}\right)  ^{T}$ to the $k$-th row of $A^{T}$ (because the
columns of $A$ correspond to the rows of $A^{T}$).

Hence, Exercise \ref{exe.ps4.6} \textbf{(i)} (applied to $A^{T}$, $\left(
A^{\prime}\right)  ^{T}$ and $B^{T}$ instead of $A$, $A^{\prime}$ and $B$)
yields $\det\left(  B^{T}\right)  =\det\left(  A^{T}\right)  +\det\left(
\left(  A^{\prime}\right)  ^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. Finally, Exercise \ref{exe.ps4.4} (applied to
$A^{\prime}$ instead of $A$) yields $\det\left(  \left(  A^{\prime}\right)
^{T}\right)  =\det A^{\prime}$.

But recall that $\det\left(  B^{T}\right)  =\det\left(  A^{T}\right)
+\det\left(  \left(  A^{\prime}\right)  ^{T}\right)  $. This rewrites as $\det
B=\det A+\det A^{\prime}$ (since $\det\left(  B^{T}\right)  =\det B$ and
$\det\left(  A^{T}\right)  =\det A$ and $\det\left(  \left(  A^{\prime
}\right)  ^{T}\right)  =\det A^{\prime}$). This solves Exercise
\ref{exe.ps4.6} \textbf{(j)}.

[\textit{Remark:} It is tempting to regard Exercise \ref{exe.ps4.6}
\textbf{(e)} as a consequence of Exercise \ref{exe.ps4.6} \textbf{(a)},
because if a matrix $A$ has two equal rows, then switching these two rows does
not change the matrix $A$, and thus Exercise \ref{exe.ps4.6} \textbf{(a)}
(applied to $B=A$) yields that $\det A=-\det A$ in this case. However, we
cannot conclude $\det A=0$ from $\det A=-\det A$ in general, unless we know
that we can \textquotedblleft divide by $2$\textquotedblright\ (or at least
cancel a factor of $2$ from equalities) in the commutative ring $\mathbb{K}$.
For example, if $\mathbb{K}$ is the ring $\mathbb{Z}/2\mathbb{Z}$, then every
$a\in\mathbb{K}$ satisfies $a=-a$, but not every $a\in\mathbb{K}$ satisfies
$a=0$. Of course, if $\mathbb{K}=\mathbb{Z}$ or $\mathbb{K}=\mathbb{Q}$ or
$\mathbb{K}=\mathbb{R}$ or $\mathbb{K}=\mathbb{C}$, then this argument works
and thus Exercise \ref{exe.ps4.6} \textbf{(e)} does follow from Exercise
\ref{exe.ps4.6} \textbf{(a)} in these cases.]
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.6k}}

Our solution to Exercise \ref{exe.ps4.6k} relies on Lemma \ref{lem.det.sigma}.
Thus, the reader is advised to read the proof of said lemma before the
solution of the exercise.

Before we start solving Exercise \ref{exe.ps4.6k}, let us prove a simple fact:

\begin{lemma}
\label{lem.sol.ps4.6k.lem.1}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix such that some row of the matrix $A$ equals a scalar multiple of
some other row of $A$. Then,%
\begin{equation}
\det A=0. \label{sol.ps4.6k.lem.1}%
\end{equation}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.ps4.6k.lem.1}.]Some row of the matrix $A$ equals
a scalar multiple of some other row of $A$. In other words, there exist two
distinct elements $k$ and $\ell$ of $\left\{  1,2,\ldots,n\right\}  $ such
that the $k$-th row of the matrix $A$ equals a scalar multiple of the $\ell
$-th row of $A$. Consider these $k$ and $\ell$.

\begin{vershort}
Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $k$-th
row of $A$ by the $\ell$-th row of $A$. Then, the matrix $C$ has two equal
rows (namely, its $k$-th row and its $\ell$-th row). Hence, $\det C=0$ (by
Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$ instead of $A$).

Recall that the $k$-th row of the matrix $A$ equals a scalar multiple of the
$\ell$-th row of $A$. In other words, there exists a $\lambda\in\mathbb{K}$
such that%
\begin{equation}
\left(  \text{the }k\text{-th row of }A\right)  =\lambda\left(  \text{the
}\ell\text{-th row of }A\right)  . \label{sol.ps4.6k.short.lem.1.pf.1}%
\end{equation}
Consider this $\lambda$. By the construction of $C$, we have
\begin{equation}
\left(  \text{the }k\text{-th row of }C\right)  =\left(  \text{the }%
\ell\text{-th row of }A\right)  . \label{sol.ps4.6k.short.lem.1.pf.2}%
\end{equation}
Thus, (\ref{sol.ps4.6k.short.lem.1.pf.1}) becomes%
\begin{equation}
\left(  \text{the }k\text{-th row of }A\right)  =\lambda\underbrace{\left(
\text{the }\ell\text{-th row of }A\right)  }_{\substack{=\left(  \text{the
}k\text{-th row of }C\right)  \\\text{(by (\ref{sol.ps4.6k.short.lem.1.pf.2}%
))}}}=\lambda\left(  \text{the }k\text{-th row of }C\right)  .
\label{sol.ps4.6k.short.lem.1.pf.3}%
\end{equation}
On the other hand, for every $u\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$u\neq k$, we have%
\[
\left(  \text{the }u\text{-th row of }C\right)  =\left(  \text{the }u\text{-th
row of }A\right)
\]
(since the construction of $C$ involves modifying only the $k$-th row of $A$)
and thus%
\begin{equation}
\left(  \text{the }u\text{-th row of }A\right)  =\left(  \text{the }u\text{-th
row of }C\right)  . \label{sol.ps4.6k.short.lem.1.pf.4}%
\end{equation}


Now, combining (\ref{sol.ps4.6k.short.lem.1.pf.3}) with
(\ref{sol.ps4.6k.short.lem.1.pf.4}), we obtain the following description of
$A$ from $C$: The matrix $A$ is the matrix obtained from $C$ by multiplying
the $k$-th row by $\lambda$. Then, $\det A=\lambda\det C$ (by Exercise
\ref{exe.ps4.6} \textbf{(g)}, applied to $C$ and $A$ instead of $A$ and $B$).
Thus, $\det A=\lambda\underbrace{\det C}_{=0}=0$. This proves Lemma
\ref{lem.sol.ps4.6k.lem.1}.
\end{vershort}

\begin{verlong}
Let $w$ be the $\ell$-th row of $A$ (regarded, as usual, as a row vector).
Thus, $w=\left(  \text{the }\ell\text{-th row of }A\right)  $.

Recall that the $k$-th row of $A$ equals a scalar multiple of the $\ell$-th
row of $A$. In other words, the $k$-th row of $A$ equals a scalar multiple of
$w$ (since $w$ is the $\ell$-th row of $A$). In other words, there exists some
$\lambda\in\mathbb{K}$ such that
\begin{equation}
\left(  \text{the }k\text{-th row of }A\right)  =\lambda w.
\label{sol.ps4.6k.lem.2}%
\end{equation}
Consider this $\lambda$.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $k$-th
row of $A$ by the row vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }C\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.lem.3}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }k\text{-th row of }C\right)  =w. \label{sol.ps4.6k.lem.4}%
\end{equation}
The matrix $C$ has two equal rows\footnote{\textit{Proof.} The numbers $k$ and
$\ell$ are distinct. Thus, $\ell\neq k$. Hence, (\ref{sol.ps4.6k.lem.3})
(applied to $u=\ell$) yields%
\begin{align*}
\left(  \text{the }\ell\text{-th row of }C\right)   &  =\left(  \text{the
}\ell\text{-th row of }A\right)  =w\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}w=\left(  \text{the }\ell\text{-th row of }A\right)  \right) \\
&  =\left(  \text{the }k\text{-th row of }C\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.lem.4})}\right)  .
\end{align*}
In other words, the $\ell$-th row of $C$ and the $k$-th row of $C$ are equal.
Since $\ell\neq k$, this shows that the matrix $C$ has two equal rows. Qed.}.
Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$
instead of $A$).

On the other hand, let $C^{\prime}$ be the matrix obtained from $C$ by
multiplying the $k$-th row by $\lambda$. Then, $\det C^{\prime}=\lambda\det C$
(by Exercise \ref{exe.ps4.6} \textbf{(g)}, applied to $C$ and $C^{\prime}$
instead of $A$ and $B$). Thus, $\det C^{\prime}=\lambda\underbrace{\det
C}_{=0}=0$.

Recall that $C^{\prime}$ is the matrix obtained from $C$ by multiplying the
$k$-th row by $\lambda$. In other words,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }C^{\prime}\right)  =\left(
\text{the }u\text{-th row of }C\right)  \right. \label{sol.ps4.6k.lem.3'}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
while%
\begin{equation}
\left(  \text{the }k\text{-th row of }C^{\prime}\right)  =\lambda\left(
\text{the }k\text{-th row of }C\right)  . \label{sol.ps4.6k.lem.4'}%
\end{equation}


On the other hand, for every $u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th row of }A\right)  =\left(  \text{the }u\text{-th
row of }C^{\prime}\right)  \label{sol.ps4.6k.lem.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6k.lem.5}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $. We must prove (\ref{sol.ps4.6k.lem.5}).
\par
If $u\neq k$, then (\ref{sol.ps4.6k.lem.5}) follows from%
\begin{align*}
\left(  \text{the }u\text{-th row of }A\right)   &  =\left(  \text{the
}u\text{-th row of }C\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.lem.3})}\right) \\
&  =\left(  \text{the }u\text{-th row of }C^{\prime}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.lem.3'})}\right)  .
\end{align*}
Hence, if $u\neq k$, then (\ref{sol.ps4.6k.lem.5}) is proven. Therefore, for
the rest of this proof of (\ref{sol.ps4.6k.lem.5}), we can WLOG assume that we
don't have $u\neq k$. Assume this.
\par
We have $u=k$ (since we don't have $u\neq k$). Hence,%
\begin{align*}
\left(  \text{the }\underbrace{u}_{=k}\text{-th row of }A\right)   &  =\left(
\text{the }k\text{-th row of }A\right)  =\lambda\underbrace{w}%
_{\substack{=\left(  \text{the }k\text{-th row of }C\right)  \\\text{(by
(\ref{sol.ps4.6k.lem.4}))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.lem.2})}\right) \\
&  =\lambda\left(  \text{the }k\text{-th row of }C\right)  =\left(  \text{the
}\underbrace{k}_{=u}\text{-th row of }C^{\prime}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.lem.4'})}\right) \\
&  =\left(  \text{the }u\text{-th row of }C^{\prime}\right)  .
\end{align*}
Hence, (\ref{sol.ps4.6k.lem.5}) is proven.}. In other words, every row of $A$
equals the corresponding row of $C^{\prime}$. Thus, the matrices $A$ and
$C^{\prime}$ must be equal. In other words, $A=C^{\prime}$, so that $\det
A=\det C^{\prime}=0$. This proves Lemma \ref{lem.sol.ps4.6k.lem.1}.
\end{verlong}
\end{proof}

Now, let us come to the actual solution of Exercise \ref{exe.ps4.6k}.

\begin{proof}
[Solution to Exercise \ref{exe.ps4.6k}.]Let us write the $n\times n$-matrix
$A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

\textbf{(a)} We need to prove that if we add a scalar multiple of a row of $A$
to another row of $A$, then the determinant of $A$ does not change. In other
words, we need to prove that if $B$ is an $n\times n$-matrix obtained from $A$
by adding a scalar multiple of a row of $A$ to another row of $A$, then $\det
B=\det A$.

So let $B$ be an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a row of $A$ to another row of $A$. We then need to show that
$\det B=\det A$.

We know that $B$ is an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a row of $A$ to another row of $A$. In other words, there exist
two distinct elements $\ell$ and $k$ of $\left\{  1,2,\ldots,n\right\}  $ such
that $B$ is the $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of the $\ell$-th row of $A$ to the $k$-th row of $A$. Consider these
$\ell$ and $k$.

\begin{vershort}
We know that $B$ is the $n\times n$-matrix obtained from $A$ by adding a
scalar multiple of the $\ell$-th row of $A$ to the $k$-th row of $A$. In other
words, there exists a $\lambda\in\mathbb{K}$ such that $B$ is $n\times
n$-matrix obtained from $A$ by adding $\lambda\cdot\left(  \text{the }%
\ell\text{-th row of }A\right)  $ to the $k$-th row of $A$. Consider this
$\lambda$.

Let $A^{\prime}$ be the $n\times n$-matrix obtained from $A$ by replacing the
$k$-th row of $A$ by $\lambda\cdot\left(  \text{the }\ell\text{-th row of
}A\right)  $. Then,%
\begin{align}
\left(  \text{the }k\text{-th row of }A^{\prime}\right)   &  =\lambda
\cdot\underbrace{\left(  \text{the }\ell\text{-th row of }A\right)
}_{\substack{=\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)
\\\text{(since the }\ell\text{-th row of }A^{\prime}\text{ has been}%
\\\text{taken over from }A\text{ unchanged)}}}\label{sol.ps4.6k.a.short.1}\\
&  =\lambda\cdot\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)
.\nonumber
\end{align}
Thus, the $k$-th row of $A^{\prime}$ equals a scalar multiple of the $\ell$-th
row of $A^{\prime}$. Thus, some row of the matrix $A^{\prime}$ equals a scalar
multiple of some other row of $A^{\prime}$. Hence, (\ref{sol.ps4.6k.lem.1})
(applied to $A^{\prime}$ instead of $A$) shows that $\det A^{\prime}=0$.

On the other hand, $A^{\prime}$ is an $n\times n$-matrix whose rows equal the
corresponding rows of $A$ except (perhaps) the $k$-th row (because of the
construction of $A^{\prime}$). Also, $B$ is the $n\times n$-matrix obtained
from $A$ by adding $\lambda\cdot\left(  \text{the }\ell\text{-th row of
}A\right)  $ to the $k$-th row of $A$. Because of (\ref{sol.ps4.6k.a.short.1}%
), this can be rewritten as follows: $B$ is the $n\times n$-matrix obtained
from $A$ by adding the $k$-th row of $A^{\prime}$ to the $k$-th row of $A$.
Exercise \ref{exe.ps4.6} \textbf{(i)} thus yields $\det B=\det
A+\underbrace{\det A^{\prime}}_{=0}=\det A$.
\end{vershort}

\begin{verlong}
We have $\ell\neq k$ (since $\ell$ and $k$ are distinct).

We know that $B$ is the $n\times n$-matrix obtained from $A$ by adding a
scalar multiple of the $\ell$-th row of $A$ to the $k$-th row of $A$. In other
words, we have%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.a.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
while the $k$-th row of $B$ is the sum of the $k$-th row of $A$ with a scalar
multiple of the $\ell$-th row of $A$.

We know that the $k$-th row of $B$ is the sum of the $k$-th row of $A$ with a
scalar multiple of the $\ell$-th row of $A$. In other words,%
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }k\text{-th
row of }A\right)  +w, \label{sol.ps4.6k.a.2}%
\end{equation}
where $w$ is some scalar multiple of the $\ell$-th row of $A$. Consider this
$w$. There exists a $\lambda\in\mathbb{K}$ such that
\begin{equation}
w=\lambda\left(  \text{the }\ell\text{-th row of }A\right)
\label{sol.ps4.6k.a.3}%
\end{equation}
(since $w$ is some scalar multiple of the $\ell$-th row of $A$). Consider this
$\lambda$.

Let $A^{\prime}$ be the $n\times n$-matrix obtained from $A$ by replacing the
$k$-th row of $A$ by the row vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }A^{\prime}\right)  =\left(
\text{the }u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.a.5}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }k\text{-th row of }A^{\prime}\right)  =w.
\label{sol.ps4.6k.a.6}%
\end{equation}
Then, the $k$-th row of the matrix $A^{\prime}$ equals a scalar multiple of
the $\ell$-th row of $A^{\prime}$\ \ \ \ \footnote{\textit{Proof.} We have
$\ell\neq k$. Thus, (\ref{sol.ps4.6k.a.5}) (applied to $u=\ell$) yields
$\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)  =\left(
\text{the }\ell\text{-th row of }A\right)  $. Hence, $\lambda
\underbrace{\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)
}_{=\left(  \text{the }\ell\text{-th row of }A\right)  }=\lambda\left(
\text{the }\ell\text{-th row of }A\right)  =w$ (by (\ref{sol.ps4.6k.a.3})).
Compared with (\ref{sol.ps4.6k.a.6}), this yields $\left(  \text{the
}k\text{-th row of }A^{\prime}\right)  =\lambda\left(  \text{the }%
\ell\text{-th row of }A^{\prime}\right)  $. Hence, the $k$-th row of the
matrix $A^{\prime}$ equals a scalar multiple of the $\ell$-th row of
$A^{\prime}$. Qed.}. Therefore, some row of the matrix $A^{\prime}$ equals a
scalar multiple of some other row of $A^{\prime}$. Hence,
(\ref{sol.ps4.6k.lem.1}) (applied to $A^{\prime}$ instead of $A$) yields $\det
A^{\prime}=0$.

On the other hand, $A^{\prime}$ is an $n\times n$-matrix whose rows equal the
corresponding rows of $A$ except (perhaps) the $k$-th row\footnote{In fact,
(\ref{sol.ps4.6k.a.5}) says precisely that the rows of $A^{\prime}$ equal the
corresponding rows of $A$ except (perhaps) the $k$-th row.}.

Now, let $B^{\prime}$ be the $n\times n$-matrix obtained from $A$ by adding
the $k$-th row of $A^{\prime}$ to the $k$-th row of $A$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B^{\prime}\right)  =\left(
\text{the }u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.a.11}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }k\text{-th row of }B^{\prime}\right)  =\left(  \text{the
}k\text{-th row of }A\right)  +\left(  \text{the }k\text{-th row of }%
A^{\prime}\right)  . \label{sol.ps4.6k.a.12}%
\end{equation}


Exercise \ref{exe.ps4.6} \textbf{(i)} (applied to $B^{\prime}$ instead of $B$)
yields $\det B^{\prime}=\det A+\underbrace{\det A^{\prime}}_{=0}=\det A$.

Now, for every $u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th row of }B^{\prime}\right)  =\left(  \text{the
}u\text{-th row of }B\right)  \label{sol.ps4.6k.a.13}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6k.a.13}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $. We must prove (\ref{sol.ps4.6k.a.13}).
\par
If $u\neq k$, then (\ref{sol.ps4.6k.a.13}) follows from%
\begin{align*}
\left(  \text{the }u\text{-th row of }B^{\prime}\right)   &  =\left(
\text{the }u\text{-th row of }A\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.a.11})}\right) \\
&  =\left(  \text{the }u\text{-th row of }B\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.a.1})}\right)  .
\end{align*}
Hence, if $u\neq k$, then (\ref{sol.ps4.6k.a.13}) is proven. Therefore, for
the rest of this proof of (\ref{sol.ps4.6k.a.13}), we can WLOG assume that we
don't have $u\neq k$. Assume this.
\par
We have $u=k$ (since we don't have $u\neq k$). Hence,%
\begin{align*}
\left(  \text{the }\underbrace{u}_{=k}\text{-th row of }B^{\prime}\right)   &
=\left(  \text{the }k\text{-th row of }B^{\prime}\right) \\
&  =\left(  \text{the }k\text{-th row of }A\right)  +\underbrace{\left(
\text{the }k\text{-th row of }A^{\prime}\right)  }_{\substack{=w\\\text{(by
(\ref{sol.ps4.6k.a.6}))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.lem.2})}\right) \\
&  =\left(  \text{the }k\text{-th row of }A\right)  +w=\left(  \text{the
}\underbrace{k}_{=u}\text{-th row of }B\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.ps4.6k.a.2})}\right) \\
&  =\left(  \text{the }u\text{-th row of }B\right)  .
\end{align*}
Hence, (\ref{sol.ps4.6k.a.13}) is proven.}. In other words, every row of
$B^{\prime}$ equals the corresponding row of $B$. Thus, the matrices
$B^{\prime}$ and $B$ must be equal. In other words, $B^{\prime}=B$, so that
$\det B^{\prime}=\det B$.

Compared with $\det B^{\prime}=\det A$, this yields $\det B=\det A$.
\end{verlong}

Thus, we have shown that%
\begin{equation}
\det B=\det A. \label{sol.ps4.6k.a.result}%
\end{equation}
This completes our solution to Exercise \ref{exe.ps4.6k} \textbf{(a)}.

\textbf{(b)} We need to prove that if we add a scalar multiple of a column of
$A$ to another column of $A$, then the determinant of $A$ does not change. In
other words, we need to prove that if $B$ is an $n\times n$-matrix obtained
from $A$ by adding a scalar multiple of a column of $A$ to another column of
$A$, then $\det B=\det A$.

So let $B$ be an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a column of $A$ to another column of $A$. We then need to show
that $\det B=\det A$.

We know that $B$ is an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a column of $A$ to another column of $A$. Therefore, $B^{T}$ is an
$n\times n$-matrix obtained from $A^{T}$ by adding a scalar multiple of a row
of $A^{T}$ to another row of $A^{T}$ (because the columns of $A$ correspond to
the rows of $A^{T}$\ \ \ \ \footnote{More precisely, the columns of $A$ are
the transposes of the rows of $A^{T}$.}). Therefore,
(\ref{sol.ps4.6k.a.result}) (applied to $A^{T}$ and $B^{T}$ instead of $A$ and
$B$) yields $\det\left(  B^{T}\right)  =\det\left(  A^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. But recall that $\det\left(  B^{T}\right)
=\det\left(  A^{T}\right)  $. This rewrites as $\det B=\det A$ (since
$\det\left(  B^{T}\right)  =\det B$ and $\det\left(  A^{T}\right)  =\det A$).
This solves Exercise \ref{exe.ps4.6k} \textbf{(b)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.prodrule}}

Our goal is now to prove Lemma \ref{lem.prodrule}. Actually, we will prove a
more general result:

\begin{lemma}
\label{lem.prodrule.S}Let $n\in\mathbb{N}$. For every $i\in\left\{
1,2,\ldots,n\right\}  $, let $S_{i}$ be a finite set. For every $i\in\left\{
1,2,\ldots,n\right\}  $ and every $k\in S_{i}$, let $p_{i,k}$ be an element of
$\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k\in S_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{n}}\prod_{i=1}%
^{n}p_{i,k_{i}}.
\]

\end{lemma}

Let us first show the particular case of this lemma for $n=2$:

\begin{lemma}
\label{lem.prodrule.S.n=2}Let $X$ and $Y$ be two finite sets. For every $x\in
X$, let $q_{x}$ be an element of $\mathbb{K}$. For every $y\in Y$, let $r_{y}$
be an element of $\mathbb{K}$. Then,%
\[
\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}r_{y}\right)
=\sum_{\left(  x,y\right)  \in X\times Y}q_{x}r_{y}.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.prodrule.S.n=2}.]We have%
\[
\underbrace{\sum_{\left(  x,y\right)  \in X\times Y}}_{=\sum_{x\in X}%
\sum_{y\in Y}}q_{x}r_{y}=\sum_{x\in X}\underbrace{\sum_{y\in Y}q_{x}r_{y}%
}_{=q_{x}\sum_{y\in Y}r_{y}}=\sum_{x\in X}\left(  q_{x}\sum_{y\in Y}%
r_{y}\right)  =\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}%
r_{y}\right)  .
\]
This proves Lemma \ref{lem.prodrule.S.n=2}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.prodrule.S.n=2}.]The equalities (\ref{eq.sum.linear2}%
) and (\ref{eq.sum.fubini}) (and most other properties of finite sums) remain
valid if $\mathbb{A}$ is replaced by $\mathbb{K}$ throughout them. It is in
this variant that they will be used in the following proof.

For every $\lambda\in\mathbb{K}$, we have%
\begin{align}
\sum_{y\in Y}\lambda r_{y}  &  =\sum_{s\in Y}\lambda r_{s}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}y\text{ as }s\right) \nonumber\\
&  =\lambda\sum_{s\in Y}r_{s}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.sum.linear2}) (applied to }Y\text{, }r_{s}\text{ and
}\mathbb{K}\\
\text{instead of }S\text{, }a_{s}\text{ and }\mathbb{A}\text{)}%
\end{array}
\right) \nonumber\\
&  =\lambda\sum_{y\in Y}r_{y}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
renamed the summation index }s\text{ as }y\right) \nonumber\\
&  =\left(  \sum_{y\in Y}r_{y}\right)  \lambda.
\label{pf.lem.prodrule.S.n=2.1}%
\end{align}
For every $\lambda\in\mathbb{K}$, we have%
\begin{align}
\sum_{x\in X}\lambda q_{x}  &  =\sum_{s\in X}\lambda q_{s}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}y\text{ as }s\right) \nonumber\\
&  =\lambda\sum_{s\in X}q_{s}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.sum.linear2}) (applied to }X\text{, }q_{s}\text{ and
}\mathbb{K}\\
\text{instead of }S\text{, }a_{s}\text{ and }\mathbb{A}\text{)}%
\end{array}
\right) \nonumber\\
&  =\lambda\sum_{x\in X}q_{x}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
renamed the summation index }s\text{ as }x\right) \nonumber\\
&  =\left(  \sum_{x\in X}q_{x}\right)  \lambda.
\label{pf.lem.prodrule.S.n=2.2}%
\end{align}


From (\ref{eq.sum.fubini}) (applied to $\mathbb{K}$ and $q_{x}r_{y}$ instead
of $\mathbb{A}$ and $a_{\left(  x,y\right)  }$), we obtain%
\[
\sum_{x\in X}\sum_{y\in Y}q_{x}r_{y}=\sum_{\left(  x,y\right)  \in X\times
Y}q_{x}r_{y}=\sum_{y\in Y}\sum_{x\in X}q_{x}r_{y}.
\]
Hence,%
\begin{align*}
\sum_{\left(  x,y\right)  \in X\times Y}q_{x}r_{y}  &  =\sum_{x\in
X}\underbrace{\sum_{y\in Y}q_{x}r_{y}}_{\substack{=\left(  \sum_{y\in Y}%
r_{y}\right)  q_{x}\\\text{(by (\ref{pf.lem.prodrule.S.n=2.1}) (applied to
}\lambda=q_{x}\text{))}}}=\sum_{x\in X}\left(  \sum_{y\in Y}r_{y}\right)
q_{x}\\
&  =\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}r_{y}\right)
\end{align*}
(by (\ref{pf.lem.prodrule.S.n=2.2}) (applied to $\lambda=\sum_{x\in X}q_{x}%
$)). This proves Lemma \ref{lem.prodrule.S.n=2}.
\end{proof}
\end{verlong}

\begin{proof}
[Proof of Lemma \ref{lem.prodrule.S}.]We claim that%
\begin{equation}
\prod_{i=1}^{m}\sum_{k\in S_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{m}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{m}}\prod_{i=1}%
^{m}p_{i,k_{i}}. \label{pf.lem.prodrule.S.claim}%
\end{equation}
for every $m\in\left\{  0,1,\ldots,n\right\}  $.

\textit{Proof of (\ref{pf.lem.prodrule.S.claim}):} We shall prove
(\ref{pf.lem.prodrule.S.claim}) by induction over $m$:

\textit{Induction base:} Comparing
\[
\prod_{i=1}^{0}\sum_{k\in S_{i}}p_{i,k}=\left(  \text{empty product}\right)
=1
\]
with%
\begin{align*}
\sum_{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in S_{1}\times S_{2}%
\times\cdots\times S_{0}}\underbrace{\prod_{i=1}^{0}p_{i,k_{i}}}_{=\left(
\text{empty product}\right)  =1}  &  =\sum_{\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{0}}1\\
&  =\underbrace{\left\vert S_{1}\times S_{2}\times\cdots\times S_{0}%
\right\vert }_{\substack{=1\\\text{(since }S_{1}\times S_{2}\times\cdots\times
S_{0}\text{ is an empty}\\\text{Cartesian product)}}}\cdot1=1,
\end{align*}
we obtain $\prod_{i=1}^{0}\sum_{k\in S_{i}}p_{i,k}=\sum_{\left(  k_{1}%
,k_{2},\ldots,k_{0}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{0}%
}\prod_{i=1}^{0}p_{i,k_{i}}$. In other words, (\ref{pf.lem.prodrule.S.claim})
holds for $m=0$. This completes the induction base.

\textit{Induction step:} Let $M\in\left\{  0,1,\ldots,n\right\}  $ be
positive. Assume that (\ref{pf.lem.prodrule.S.claim}) holds for $m=M-1$. We
now must show that (\ref{pf.lem.prodrule.S.claim}) holds for $m=M$.

We have assumed that (\ref{pf.lem.prodrule.S.claim}) holds for $m=M-1$. In
other words, we have%
\begin{equation}
\prod_{i=1}^{M-1}\sum_{k\in S_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{M-1}}%
\prod_{i=1}^{M-1}p_{i,k_{i}}. \label{pf.lem.prodrule.S.indhyp}%
\end{equation}


\begin{vershort}
Now, the map
\begin{align*}
S_{1}\times S_{2}\times\cdots\times S_{M}  &  \rightarrow\left(  S_{1}\times
S_{2}\times\cdots\times S_{M-1}\right)  \times S_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection.\footnote{Indeed, this is the canonical bijection $S_{1}\times
S_{2}\times\cdots\times S_{M}\rightarrow\left(  S_{1}\times S_{2}\times
\cdots\times S_{M-1}\right)  \times S_{M}$.}
\end{vershort}

\begin{verlong}
We define a map%
\[
\Phi:\left(  S_{1}\times S_{2}\times\cdots\times S_{M-1}\right)  \times
S_{M}\rightarrow S_{1}\times S_{2}\times\cdots\times S_{M}%
\]
by%
\begin{equation}
\left(
\begin{array}
[c]{l}%
\Phi\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)  =\left(
s_{1},s_{2},\ldots,s_{M-1},t\right) \\
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \left(  s_{1},s_{2}%
,\ldots,s_{M-1}\right)  ,t\right)  \in\left(  S_{1}\times S_{2}\times
\cdots\times S_{M-1}\right)  \times S_{M}%
\end{array}
\right)  . \label{pf.lem.prodrule.S.Phidef}%
\end{equation}


We also define a map%
\[
\Psi:S_{1}\times S_{2}\times\cdots\times S_{M}\rightarrow\left(  S_{1}\times
S_{2}\times\cdots\times S_{M-1}\right)  \times S_{M}%
\]
by%
\begin{equation}
\left(
\begin{array}
[c]{l}%
\Psi\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right) \\
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  s_{1},s_{2},\ldots,s_{M}\right)
\in S_{1}\times S_{2}\times\cdots\times S_{M}%
\end{array}
\right)  . \label{pf.lem.prodrule.S.Psidef}%
\end{equation}


We now claim that
\begin{equation}
\Psi\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  =\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,t\right)  \label{pf.lem.prodrule.S.Psi}%
\end{equation}
for every $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  \in S_{1}\times
S_{2}\times\cdots\times S_{M-1}$ and $t\in S_{M}$.

\textit{Proof of (\ref{pf.lem.prodrule.S.Psi}):} Let $\left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{M-1}$
and $t\in S_{M}$. We have $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  \in
S_{1}\times S_{2}\times\cdots\times S_{M-1}$; in other words,
\begin{equation}
s_{i}\in S_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots
,M-1\right\}  . \label{pf.lem.prodrule.S.Psi.pf.1}%
\end{equation}


For every $i\in\left\{  1,2,\ldots,M\right\}  $, the element $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $S_{i}$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\left\{  1,2,\ldots,M\right\}  $. We must prove that
\begin{equation}%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
\text{ is a well-defined element of }S_{i}.
\label{pf.lem.prodrule.S.Psi.pf.fn1.1}%
\end{equation}
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq M$.
\par
\textit{Case 2:} We have $i=M$.
\par
Let us first consider Case 1. In this case, we have $i\neq M$. Combining this
with $i\in\left\{  1,2,\ldots,M\right\}  $, we obtain $i\in\left\{
1,2,\ldots,M\right\}  \setminus\left\{  M\right\}  =\left\{  1,2,\ldots
,M-1\right\}  $. Hence, $s_{i}\in S_{i}$ (by (\ref{pf.lem.prodrule.S.Psi.pf.1}%
)). Thus, $s_{i}$ is a well-defined element of $S_{i}$. In other words, $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $S_{i}$ (since $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
=s_{i}$ (since $i<M$)). Thus, (\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) is proven
in Case 1.
\par
Let us now consider Case 2. In this case, we have $i=M$. But $t$ is a
well-defined element of $S_{M}$. In other words, $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $S_{i}$ (since $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
=t$ (since $i=M$) and $M=i$ (since $i=M$)). Thus,
(\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) is proven in Case 2.
\par
We have now proven (\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) in each of the two
Cases 1 and 2. Thus, (\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) always holds
(since Cases 1 and 2 cover all possibilities). In other words, $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $S_{i}$. Qed.}. Hence, we can define an
$M$-tuple $\left(  q_{1},q_{2},\ldots,q_{M}\right)  \in S_{1}\times
S_{2}\times\cdots\times S_{M}$ by%
\[
\left(  q_{i}=%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,M\right\}
\right)  .
\]
Consider this $M$-tuple $\left(  q_{1},q_{2},\ldots,q_{M}\right)  $. The
definition of this $M$-tuple shows that%
\[
\left(  q_{1},q_{2},\ldots,q_{M}\right)  =\left(  s_{1},s_{2},\ldots
,s_{M-1},t\right)  .
\]


For every $i\in\left\{  1,2,\ldots,M-1\right\}  $, we have%
\[
q_{i}=%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
=s_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<M\text{ (since }i\in\left\{
1,2,\ldots,M-1\right\}  \text{)}\right)  .
\]
In other words, $\left(  q_{1},q_{2},\ldots,q_{M-1}\right)  =\left(
s_{1},s_{2},\ldots,s_{M-1}\right)  $. Also, the definition of $q_{M}$ shows
that $q_{M}=%
\begin{cases}
s_{M}, & \text{if }M<M;\\
t, & \text{if }M=M
\end{cases}
=t$ (since $M=M$).

But the definition of $\Psi$ yields
\begin{align*}
\Psi\left(  q_{1},q_{2},\ldots,q_{M}\right)   &  =\left(  \underbrace{\left(
q_{1},q_{2},\ldots,q_{M-1}\right)  }_{=\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  },\underbrace{q_{M}}_{=t}\right) \\
&  =\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)  .
\end{align*}
Comparing this with $\Psi\underbrace{\left(  q_{1},q_{2},\ldots,q_{M}\right)
}_{=\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  }=\Psi\left(  s_{1}%
,s_{2},\ldots,s_{M-1},t\right)  $, we obtain $\Psi\left(  s_{1},s_{2}%
,\ldots,s_{M-1},t\right)  =\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)
,t\right)  $. This proves (\ref{pf.lem.prodrule.S.Psi.pf.1}).

Now, we have $\Phi\circ\Psi=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} Let $\alpha\in S_{1}\times S_{2}\times
\cdots\times S_{M}$. Thus, we can write $\alpha$ in the form $\left(
s_{1},s_{2},\ldots,s_{M}\right)  $ for some $\left(  s_{1},s_{2},\ldots
,s_{M}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{M}$. Consider this
$\left(  s_{1},s_{2},\ldots,s_{M}\right)  $. Hence, $\alpha=\left(
s_{1},s_{2},\ldots,s_{M}\right)  $.
\par
Now,%
\begin{align*}
\left(  \Phi\circ\Psi\right)  \left(  \underbrace{\alpha}_{=\left(
s_{1},s_{2},\ldots,s_{M}\right)  }\right)   &  =\left(  \Phi\circ\Psi\right)
\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\Phi\left(  \underbrace{\Psi\left(
s_{1},s_{2},\ldots,s_{M}\right)  }_{\substack{=\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)  \\\text{(by the definition of
}\Psi\text{)}}}\right) \\
&  =\Phi\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
=\left(  s_{1},s_{2},\ldots,s_{M-1},s_{M}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right) \\
&  =\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\alpha=\operatorname*{id}\left(
\alpha\right)  .
\end{align*}
\par
Let us now forget that we fixed $\alpha$. We thus have shown that $\left(
\Phi\circ\Psi\right)  \left(  \alpha\right)  =\operatorname*{id}\left(
\alpha\right)  $ for every $\alpha\in S_{1}\times S_{2}\times\cdots\times
S_{M}$. In other words, $\Phi\circ\Psi=\operatorname*{id}$, qed.} and
$\Psi\circ\Phi=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let
$\beta\in\left(  S_{1}\times S_{2}\times\cdots\times S_{M-1}\right)  \times
S_{M}$. Thus, we can write $\beta$ in the form $\left(  \gamma,t\right)  $ for
some $\gamma\in S_{1}\times S_{2}\times\cdots\times S_{M-1}$ and $t\in S_{M}$.
Consider these $\gamma$ and $t$. Thus, $\beta=\left(  \gamma,t\right)  $.
\par
We have $\gamma\in S_{1}\times S_{2}\times\cdots\times S_{M-1}$. Thus, we can
write $\gamma$ in the form $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  $ for
some $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  \in S_{1}\times S_{2}%
\times\cdots\times S_{M-1}$. Consider this $\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  $. Hence, $\gamma=\left(  s_{1},s_{2},\ldots,s_{M-1}\right)
$.
\par
Now, $\beta=\left(  \underbrace{\gamma}_{=\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  },t\right)  =\left(  \left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  ,t\right)  $. Applying the map $\Psi\circ\Phi$ to both sides
of this equality, we obtain%
\begin{align*}
\left(  \Psi\circ\Phi\right)  \left(  \beta\right)   &  =\left(  \Psi\circ
\Phi\right)  \left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right) \\
&  =\Psi\left(  \underbrace{\Phi\left(  \left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  ,t\right)  }_{\substack{=\left(  s_{1},s_{2},\ldots
,s_{M-1},t\right)  \\\text{(by the definition of }\Phi\text{)}}}\right) \\
&  =\Psi\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  =\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.lem.prodrule.S.Psi})}\right) \\
&  =\beta=\operatorname*{id}\left(  \beta\right)  .
\end{align*}
\par
Let us now forget that we fixed $\beta$. We thus have shown that $\left(
\Psi\circ\Phi\right)  \left(  \beta\right)  =\operatorname*{id}\left(
\beta\right)  $ for every $\beta\in\left(  S_{1}\times S_{2}\times\cdots\times
S_{M-1}\right)  \times S_{M}$. In other words, $\Psi\circ\Phi
=\operatorname*{id}$, qed.}. These two equalities show that the maps $\Phi$
and $\Psi$ are mutually inverse. Thus, the map $\Psi$ is invertible. In other
words, the map $\Psi$ is a bijection.

Now, $\Psi$ is the map
\begin{align*}
S_{1}\times S_{2}\times\cdots\times S_{M}  &  \rightarrow\left(  S_{1}\times
S_{2}\times\cdots\times S_{M-1}\right)  \times S_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
(because $\Psi$ is the map $S_{1}\times S_{2}\times\cdots\times S_{M}%
\rightarrow\left(  S_{1}\times S_{2}\times\cdots\times S_{M-1}\right)  \times
S_{M}$ satisfying (\ref{pf.lem.prodrule.S.Psidef})). Thus, the map
\begin{align*}
S_{1}\times S_{2}\times\cdots\times S_{M}  &  \rightarrow\left(  S_{1}\times
S_{2}\times\cdots\times S_{M-1}\right)  \times S_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection (since $\Psi$ is a bijection).
\end{verlong}

For every $\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in S_{1}\times
S_{2}\times\cdots\times S_{M-1}$, we define an element $g_{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  }\in\mathbb{K}$ by
\begin{equation}
g_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  }=\prod_{i=1}^{M-1}p_{i,k_{i}}.
\label{pf.lem.prodrule.S.g=}%
\end{equation}


Now, (\ref{pf.lem.prodrule.S.indhyp}) becomes%
\begin{align}
\prod_{i=1}^{M-1}\sum_{k\in S_{i}}p_{i,k}  &  =\sum_{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{M-1}%
}\underbrace{\prod_{i=1}^{M-1}p_{i,k_{i}}}_{\substack{=g_{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  }\\\text{(by (\ref{pf.lem.prodrule.S.g=}))}%
}}\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in S_{1}\times
S_{2}\times\cdots\times S_{M-1}}g_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
}\nonumber\\
&  =\sum_{x\in S_{1}\times S_{2}\times\cdots\times S_{M-1}}g_{x}
\label{pf.lem.prodrule.S.L1}%
\end{align}
(here, we have renamed the summation index $\left(  k_{1},k_{2},\ldots
,k_{M-1}\right)  $ as $x$).

The sets $S_{1},S_{2},\ldots,S_{M-1}$ are finite. Hence, their Cartesian
product $S_{1}\times S_{2}\times\cdots\times S_{M-1}$ is also finite. Every
$\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in S_{1}\times S_{2}\times
\cdots\times S_{M}$ satisfies%
\begin{equation}
\prod_{i=1}^{M}p_{i,s_{i}}=\left(  \prod_{i=1}^{M-1}p_{i,s_{i}}\right)
p_{M,s_{M}} \label{pf.lem.prodrule.S.splioff}%
\end{equation}
(indeed, this follows by splitting off the factor for $i=M$ from the sum
$\prod_{i=1}^{M}p_{i,s_{i}}$).

Now, if we split off the factor for $i=M$ from the product $\prod_{i=1}%
^{M}\sum_{k\in S_{i}}p_{i,k}$, we obtain
\begin{align*}
\prod_{i=1}^{M}\sum_{k\in S_{i}}p_{i,k}  &  =\underbrace{\left(  \prod
_{i=1}^{M-1}\sum_{k\in S_{i}}p_{i,k}\right)  }_{\substack{=\sum_{x\in
S_{1}\times S_{2}\times\cdots\times S_{M-1}}g_{x}\\\text{(by
(\ref{pf.lem.prodrule.S.L1}))}}}\underbrace{\left(  \sum_{k\in S_{M}}%
p_{M,k}\right)  }_{\substack{=\sum_{y\in S_{M}}p_{M,y}\\\text{(here, we
renamed the}\\\text{summation index }k\text{ as }y\text{)}}}\\
&  =\left(  \sum_{x\in S_{1}\times S_{2}\times\cdots\times S_{M-1}}%
g_{x}\right)  \left(  \sum_{y\in S_{M}}p_{M,y}\right)  =\sum_{\left(
x,y\right)  \in\left(  S_{1}\times S_{2}\times\cdots\times S_{M-1}\right)
\times S_{M}}g_{x}p_{M,y}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Lemma \ref{lem.prodrule.S.n=2}, applied to}\\
X=S_{1}\times S_{2}\times\cdots\times S_{M-1}\text{, }Y=S_{M}\text{, }%
q_{x}=g_{x}\text{ and }r_{y}=p_{M,y}%
\end{array}
\right) \\
&  =\sum_{\left(  x,y\right)  \in\left(  S_{1}\times S_{2}\times\cdots\times
S_{M-1}\right)  \times S_{M}}g_{x}p_{M,y}\\
&  =\sum_{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in S_{1}\times S_{2}%
\times\cdots\times S_{M}}\underbrace{g_{\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  }}_{\substack{=\prod_{i=1}^{M-1}p_{i,s_{i}}\\\text{(by the
definition of }g_{\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  }\text{)}%
}}p_{M,s_{M}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  \left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  ,s_{M}\right)  \text{ for }\left(  x,y\right) \\
\text{in the sum, since the map}\\
S_{1}\times S_{2}\times\cdots\times S_{M}\rightarrow\left(  S_{1}\times
S_{2}\times\cdots\times S_{M-1}\right)  \times S_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)  \mapsto\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right) \\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in S_{1}\times S_{2}%
\times\cdots\times S_{M}}\underbrace{\left(  \prod_{i=1}^{M-1}p_{i,s_{i}%
}\right)  p_{M,s_{M}}}_{\substack{=\prod_{i=1}^{M}p_{i,s_{i}}\\\text{(by
(\ref{pf.lem.prodrule.S.splioff}))}}}\\
&  =\sum_{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in S_{1}\times S_{2}%
\times\cdots\times S_{M}}\prod_{i=1}^{M}p_{i,s_{i}}=\sum_{\left(  k_{1}%
,k_{2},\ldots,k_{M}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{M}%
}\prod_{i=1}^{M}p_{i,k_{i}}%
\end{align*}
(here, we have renamed the summation index $\left(  s_{1},s_{2},\ldots
,s_{M}\right)  $ as $\left(  k_{1},k_{2},\ldots,k_{M}\right)  $). In other
words, (\ref{pf.lem.prodrule.S.claim}) holds for $m=M$. This completes the
induction step. Thus, (\ref{pf.lem.prodrule.S.claim}) is proven by induction.

Now, (\ref{pf.lem.prodrule.S.claim}) (applied to $m=n$) yields%
\[
\prod_{i=1}^{n}\sum_{k\in S_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in S_{1}\times S_{2}\times\cdots\times S_{n}}\prod_{i=1}%
^{n}p_{i,k_{i}}.
\]
This proves Lemma \ref{lem.prodrule.S}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.prodrule}.]For every $i\in\left[  n\right]  $, the
set $\left[  m_{i}\right]  $ is clearly a finite set. For every $i\in\left[
n\right]  $, we know that $p_{i,1},p_{i,2},\ldots,p_{i,m_{i}}$ are elements of
$\mathbb{K}$. In other words, for every $i\in\left[  n\right]  $ and every
$k\in\left[  m_{i}\right]  $, we know that $p_{i,k}$ is an element of
$\mathbb{K}$. In other words, for every $i\in\left\{  1,2,\ldots,n\right\}  $
and every $k\in\left[  m_{i}\right]  $, we know that $p_{i,k}$ is an element
of $\mathbb{K}$ (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).
Hence, Lemma \ref{lem.prodrule.S} (applied to $S_{i}=\left[  m_{i}\right]  $)
yields%
\[
\prod_{i=1}^{n}\sum_{k\in\left[  m_{i}\right]  }p_{i,k}=\sum_{\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]  \times\left[
m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  }\prod_{i=1}%
^{n}p_{i,k_{i}}.
\]
Thus,%
\[
\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]
\times\left[  m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  }%
\prod_{i=1}^{n}p_{i,k_{i}}=\prod_{i=1}^{n}\underbrace{\sum_{k\in\left[
m_{i}\right]  }}_{=\sum_{k=1}^{m_{i}}}p_{i,k}=\prod_{i=1}^{n}\sum_{k=1}%
^{m_{i}}p_{i,k}.
\]
This proves Lemma \ref{lem.prodrule}.
\end{proof}

We have now proven Lemma \ref{lem.prodrule}, and thus solved Exercise
\ref{exe.prodrule}.

\subsection{Solution to Exercise \ref{exe.ps4.det.fibo}}

We shall solve Exercise \ref{exe.ps4.det.fibo} more or less as you would
expect, by modifying our proof of (\ref{eq.exam.det(AB).fibo.1}) in some places.

\begin{proof}
[Solution to Exercise \ref{exe.ps4.det.fibo}.]Let $B$ be the $2\times2$-matrix
$\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right)  $. Thus, $\det B=\det\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right)  =a\cdot0-1\cdot b=-b$.

Let $A$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  $. Then, $\det A=\det\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  =x_{k+2}x_{0}-x_{k+1}x_{1}$.

We now claim that%
\begin{equation}
AB^{m}=\left(
\begin{array}
[c]{cc}%
x_{m+k+2} & x_{m+k+1}\\
x_{m+1} & x_{m}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}.
\label{sol.ps4.det.fibo.BmC}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.det.fibo.BmC}):} We shall prove
(\ref{sol.ps4.det.fibo.BmC}) by induction over $m$:

\textit{Induction base:} We have $A\underbrace{B^{0}}_{=I_{2}}=AI_{2}%
=A=\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  $. Compared with $\left(
\begin{array}
[c]{cc}%
x_{0+k+2} & x_{0+k+1}\\
x_{0+1} & x_{0}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  $, this yields $AB^{0}=\left(
\begin{array}
[c]{cc}%
x_{0+k+2} & x_{0+k+1}\\
x_{0+1} & x_{0}%
\end{array}
\right)  $. In other words, (\ref{sol.ps4.det.fibo.BmC}) holds for $m=0$. This
completes the induction base.

\textit{Induction step:} Let $M$ be a positive integer. Assume that
(\ref{sol.ps4.det.fibo.BmC}) holds for $m=M-1$. We need to show that
(\ref{sol.ps4.det.fibo.BmC}) holds for $m=M$.

We have assumed that (\ref{sol.ps4.det.fibo.BmC}) holds for $m=M-1$. In other
words,%
\[
AB^{M-1}=\left(
\begin{array}
[c]{cc}%
x_{\left(  M-1\right)  +k+2} & x_{\left(  M-1\right)  +k+1}\\
x_{\left(  M-1\right)  +1} & x_{M-1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
x_{M+k+1} & x_{M+k}\\
x_{M} & x_{M-1}%
\end{array}
\right)  .
\]
Now,%
\begin{align}
A\underbrace{B^{M}}_{=B^{M-1}\cdot B}  &  =\underbrace{AB^{M-1}}_{=\left(
\begin{array}
[c]{cc}%
x_{M+k+1} & x_{M+k}\\
x_{M} & x_{M-1}%
\end{array}
\right)  }\cdot\underbrace{B}_{=\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right)  }\nonumber\\
&  =\left(
\begin{array}
[c]{cc}%
x_{M+k+1} & x_{M+k}\\
x_{M} & x_{M-1}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right) \nonumber\\
&  =\left(
\begin{array}
[c]{cc}%
x_{M+k+1}\cdot a+x_{M+k}\cdot b & x_{M+k+1}\cdot1+x_{M+k}\cdot0\\
x_{M}\cdot a+x_{M-1}\cdot b & x_{M}\cdot1+x_{M-1}\cdot0
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of a product of two
matrices}\right) \nonumber\\
&  =\left(
\begin{array}
[c]{cc}%
ax_{M+k+1}+bx_{M+k} & x_{M+k+1}\\
ax_{M}+bx_{M-1} & x_{M}%
\end{array}
\right)  . \label{sol.ps4.det.fibo.BmC.4}%
\end{align}


But (\ref{eq.det.fibo.rec}) (applied to $n=M+k+2$) yields%
\[
x_{M+k+2}=a\underbrace{x_{\left(  M+k+2\right)  -1}}_{=x_{M+k+1}%
}+b\underbrace{x_{\left(  M+k+2\right)  -2}}_{=x_{M+k}}=ax_{M+k+1}+bx_{M+k}.
\]
Also, (\ref{eq.det.fibo.rec}) (applied to $n=M+1$) yields $x_{M+1}%
=a\underbrace{x_{\left(  M+1\right)  -1}}_{=x_{M}}+b\underbrace{x_{\left(
M+1\right)  -2}}_{=x_{M-1}}=ax_{M}+bx_{M-1}$. Now,%
\[
\left(
\begin{array}
[c]{cc}%
x_{M+k+2} & x_{M+k+1}\\
x_{M+1} & x_{M}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
ax_{M+k+1}+bx_{M+k} & x_{M+k+1}\\
ax_{M}+bx_{M-1} & x_{M}%
\end{array}
\right)
\]
(since $x_{M+k+2}=ax_{M+k+1}+bx_{M+k}$ and $x_{M+1}=ax_{M}+bx_{M-1}$).
Compared with (\ref{sol.ps4.det.fibo.BmC.4}), this yields $AB^{M}=\left(
\begin{array}
[c]{cc}%
x_{M+k+2} & x_{M+k+1}\\
x_{M+1} & x_{M}%
\end{array}
\right)  $. In other words, (\ref{sol.ps4.det.fibo.BmC}) holds for $m=M$. This
completes the induction step. Thus, (\ref{sol.ps4.det.fibo.BmC}) is proven by induction.

Now, let $n>k$ be an integer. Then, $n-k>0$, so that $n-k-1\in\mathbb{N}$.
Hence, (\ref{sol.ps4.det.fibo.BmC}) (applied to $m=n-k-1$) yields%
\[
AB^{n-k-1}=\left(
\begin{array}
[c]{cc}%
x_{\left(  n-k-1\right)  +k+2} & x_{\left(  n-k-1\right)  +k+1}\\
x_{\left(  n-k-1\right)  +1} & x_{n-k-1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
x_{n+1} & x_{n}\\
x_{n-k} & x_{n-k-1}%
\end{array}
\right)  .
\]
Taking determinants on both sides of this equality, we obtain%
\[
\det\left(  AB^{n-k-1}\right)  =\det\left(
\begin{array}
[c]{cc}%
x_{n+1} & x_{n}\\
x_{n-k} & x_{n-k-1}%
\end{array}
\right)  =x_{n+1}x_{n-k-1}-x_{n}x_{n-k}.
\]
Hence,%
\begin{align*}
&  x_{n+1}x_{n-k-1}-x_{n}x_{n-k}\\
&  =\det\left(  AB^{n-k-1}\right)  =\det A\cdot\underbrace{\det\left(
B^{n-k-1}\right)  }_{\substack{=\left(  \det B\right)  ^{n-k-1}\\\text{(by
Corollary \ref{cor.det.product} \textbf{(b)}, applied}\\\text{to }2\text{ and
}n-k-1\text{ instead of }n\text{ and }k\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}2\text{ and }B^{n-k-1}\text{ instead of }n\text{ and }B\right) \\
&  =\underbrace{\det A}_{=x_{k+2}x_{0}-x_{k+1}x_{1}}\cdot\left(
\underbrace{\det B}_{=-b}\right)  ^{n-k-1}=\left(  x_{k+2}x_{0}-x_{k+1}%
x_{1}\right)  \cdot\left(  -b\right)  ^{n-k-1}\\
&  =\left(  -b\right)  ^{n-k-1}\left(  x_{k+2}x_{0}-x_{k+1}x_{1}\right)  .
\end{align*}
This solves Exercise \ref{exe.ps4.det.fibo}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.pascal}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.pascal}.]We let $B$ be the $n\times
n$-matrix $\left(  \dbinom{i-1}{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
$. We have $\dbinom{i-1}{j-1}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.}
Let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ be such that
$i<j$. Then, $i-1\in\mathbb{N}$ (since $i\geq1$) and $j-1\in\mathbb{N}$ (since
$j\geq1$) and $i-1<j-1$ (since $i<j$). Hence, (\ref{eq.binom.0}) (applied to
$i-1$ and $j-1$ instead of $m$ and $n$) yields $\dbinom{i-1}{j-1}=0$, qed.}.
Therefore, Exercise \ref{exe.ps4.3} (applied to $B$ and $\dbinom{i-1}{j-1}$
instead of $A$ and $a_{i,j}$) yields
\[
\det B=\underbrace{\dbinom{1-1}{1-1}}_{=1}\underbrace{\dbinom{2-1}{2-1}}%
_{=1}\cdots\underbrace{\dbinom{n-1}{n-1}}_{=1}=1\cdot1\cdot\cdots\cdot1=1.
\]
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) shows that
$\det\left(  B^{T}\right)  =\det B=1$.

Now, we are going to show that $A=BB^{T}$.

Indeed, let us show that%
\begin{equation}
\sum_{k=1}^{n}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}=\dbinom{i+j-2}{i-1}
\label{sol.ps4.pascal.1}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

\textit{Proof of (\ref{sol.ps4.pascal.1}):} Let $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then, $i\geq1$, so that
$i-1\in\mathbb{N}$. Hence, we can apply Theorem \ref{thm.vandermonde} to
$n=i-1$. We thus obtain%
\[
\dbinom{X+Y}{i-1}=\sum_{k=0}^{i-1}\dbinom{X}{k}\dbinom{Y}{i-1-k}.
\]
Substituting $j-1$ and $i-1$ for $X$ and $Y$ in this polynomial identity, we
obtain%
\begin{equation}
\dbinom{\left(  j-1\right)  +\left(  i-1\right)  }{i-1}=\sum_{k=0}%
^{i-1}\dbinom{j-1}{k}\dbinom{i-1}{i-1-k}. \label{sol.ps4.pascal.2}%
\end{equation}
On the other hand, for every $k\in\left\{  i+1,i+2,\ldots,n\right\}  $, we
have%
\begin{equation}
\dbinom{i-1}{k-1}=0 \label{sol.ps4.pascal.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.pascal.4}):} Let $k\in\left\{
i+1,i+2,\ldots,n\right\}  $. Then, $k>i\geq1$ and thus $k-1\in\mathbb{N}$.
Also, $k>i$, so that $k-i>i-1$ and thus $i-1<k-1$. Hence, (\ref{eq.binom.0})
(applied to $i-1$ and $k-1$ instead of $m$ and $n$) yields $\dbinom{i-1}%
{k-1}=0$, qed.}. Now, $i\leq n$, so that%
\begin{align*}
&  \sum_{k=1}^{n}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}\\
&  =\sum_{k=1}^{i}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}+\sum_{k=i+1}%
^{n}\underbrace{\dbinom{i-1}{k-1}}_{\substack{=0\\\text{(by
(\ref{sol.ps4.pascal.4}))}}}\dbinom{j-1}{k-1}\\
&  =\sum_{k=1}^{i}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}+\underbrace{\sum
_{k=i+1}^{n}0\dbinom{j-1}{k-1}}_{=0}=\sum_{k=1}^{i}\dbinom{i-1}{k-1}%
\dbinom{j-1}{k-1}\\
&  =\sum_{k=0}^{i-1}\underbrace{\dbinom{i-1}{k}}_{\substack{=\dbinom
{i-1}{i-1-k}\\\text{(by (\ref{eq.binom.symm}), applied to }i-1\text{ and
}k\text{ instead of }m\text{ and }n\text{)}}}\dbinom{j-1}{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}k-1\text{ in the sum}\right) \\
&  =\sum_{k=0}^{i-1}\dbinom{i-1}{i-1-k}\dbinom{j-1}{k}=\sum_{k=0}^{i-1}%
\dbinom{j-1}{k}\dbinom{i-1}{i-1-k}\\
&  =\dbinom{\left(  j-1\right)  +\left(  i-1\right)  }{i-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.pascal.2})}\right) \\
&  =\dbinom{i+j-2}{i-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
j-1\right)  +\left(  i-1\right)  =i+j-2\right)  .
\end{align*}
This proves (\ref{sol.ps4.pascal.1}).

Now, we have $B=\left(  \dbinom{i-1}{j-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ and therefore $B^{T}=\left(  \dbinom{j-1}{i-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (by the definition of $B^{T}$). Hence, the definition of
the product $BB^{T}$ shows that%
\[
BB^{T}=\left(  \underbrace{\sum_{k=1}^{n}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}%
}_{\substack{=\dbinom{i+j-2}{i-1}\\\text{(by (\ref{sol.ps4.pascal.1}))}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \dbinom{i+j-2}%
{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A.
\]
Hence, $A=BB^{T}$, so that%
\begin{align*}
\det A  &  =\det\left(  BB^{T}\right)  =\underbrace{\det B}_{=1}%
\cdot\underbrace{\det\left(  B^{T}\right)  }_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}B\text{ and }B^{T}\text{ instead of }A\text{ and }B\right) \\
&  =1.
\end{align*}
This solves Exercise \ref{exe.ps4.pascal}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.lem.increasing-sequences}}

\begin{proof}
[Proof of Lemma \ref{lem.increasing-sequences}.]\textbf{(b)} Let $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}$ be an
$n$-tuple satisfying $g_{1}<g_{2}<\cdots<g_{n}$. We shall derive a contradiction.

We have $m<n$, so that $n>m\geq0$. Thus, $n\geq1$ (since $n$ is an integer).
Therefore, the elements $g_{1}$ and $g_{n}$ of $\left\{  1,2,\ldots,m\right\}
$ are well-defined. Every $i\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies
$g_{i}-i\leq g_{i+1}-\left(  i+1\right)  $\ \ \ \ \footnote{\textit{Proof.}
Let $i\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $g_{i}<g_{i+1}$ (since
$g_{1}<g_{2}<\cdots<g_{n}$). Hence, $g_{i}\leq g_{i+1}-1$ (since $g_{i}$ and
$g_{i+1}$ are integers). Hence, $\underbrace{g_{i}}_{\leq g_{i+1}-1}-i\leq
g_{i+1}-1-i=g_{i+1}-\left(  i+1\right)  $, qed.}. In other words, we have
$g_{1}-1\leq g_{2}-2\leq\cdots\leq g_{n}-n$. Hence, $g_{1}-1\leq g_{n}-n$.

But $g_{n}\in\left\{  1,2,\ldots,m\right\}  $, so that $g_{n}\leq m<n$ and
thus $\underbrace{g_{n}}_{<n}-n<n-n=0$. Hence, $g_{1}-1\leq g_{n}-n<0$. On the
other hand, $g_{1}\in\left\{  1,2,\ldots,m\right\}  $, so that $g_{1}\geq1$
and thus $g_{1}-1\geq0$. This contradicts $g_{1}-1<0$.

Now, let us forget that we fixed $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $.
We thus have derived a contradiction for every $n$-tuple $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n}$. Hence, there exists no $n$-tuple $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}$
satisfying $g_{1}<g_{2}<\cdots<g_{n}$. This proves Lemma
\ref{lem.increasing-sequences} \textbf{(b)}.

\textbf{(a)} The $n$-tuple $\left(  1,2,\ldots,n\right)  $ clearly belongs to
$\left\{  1,2,\ldots,n\right\}  ^{n}$, and satisfies $1<2<\cdots<n$. Hence,
there exists an $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$,
namely the $n$-tuple $\left(  1,2,\ldots,n\right)  $. We shall now show that
$\left(  1,2,\ldots,n\right)  $ is the only such $n$-tuple.

Indeed, let $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,n\right\}  ^{n}$ be an $n$-tuple satisfying $g_{1}<g_{2}%
<\cdots<g_{n}$. We shall prove that $\left(  g_{1},g_{2},\ldots,g_{n}\right)
=\left(  1,2,\ldots,n\right)  $.

Every $i\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies $g_{i}-i\leq
g_{i+1}-\left(  i+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $g_{i}<g_{i+1}$ (since
$g_{1}<g_{2}<\cdots<g_{n}$). Hence, $g_{i}\leq g_{i+1}-1$ (since $g_{i}$ and
$g_{i+1}$ are integers). Hence, $\underbrace{g_{i}}_{\leq g_{i+1}-1}-i\leq
g_{i+1}-1-i=g_{i+1}-\left(  i+1\right)  $, qed.}. In other words, we have
$g_{1}-1\leq g_{2}-2\leq\cdots\leq g_{n}-n$. In other words,%
\begin{equation}
g_{u}-u\leq g_{v}-v \label{pf.lem.increasing-sequences.b.1}%
\end{equation}
for any two elements $u$ and $v$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $u\leq v$.

Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $1\leq i\leq n$, so that
$1\leq n$. Hence, the elements $g_{1}$ and $g_{n}$ of $\left\{  1,2,\ldots
,n\right\}  $ are well-defined.

Now, $1\leq i$. Hence, (\ref{pf.lem.increasing-sequences.b.1}) (applied to
$u=1$ and $v=i$) yields $g_{1}-1\leq g_{i}-i$. But $g_{1}\in\left\{
1,2,\ldots,n\right\}  $, so that $g_{1}\geq1$ and thus $g_{1}-1\geq0$. Hence,
$0\leq g_{1}-1\leq g_{i}-i$.

On the other hand, $i\leq n$. Therefore,
(\ref{pf.lem.increasing-sequences.b.1}) (applied to $u=i$ and $v=n$) yields
$g_{i}-i\leq g_{n}-n$. But $g_{n}\in\left\{  1,2,\ldots,n\right\}  $, so that
$g_{n}\leq n$ and thus $g_{n}-n\leq0$. Hence, $g_{i}-i\leq g_{n}-n\leq0$.
Combined with $0\leq g_{i}-i$, this yields $g_{i}-i=0$, so that $g_{i}=i$.

Now, let us forget that we fixed $i$. We thus have shown that $g_{i}=i$ for
every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words, $\left(
g_{1},g_{2},\ldots,g_{n}\right)  =\left(  1,2,\ldots,n\right)  $.

Let us now forget that we fixed $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $.
We thus have shown that if $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n}$ is an $n$-tuple satisfying
$g_{1}<g_{2}<\cdots<g_{n}$, then $\left(  g_{1},g_{2},\ldots,g_{n}\right)
=\left(  1,2,\ldots,n\right)  $. In other words, every $n$-tuple $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$
satisfying $g_{1}<g_{2}<\cdots<g_{n}$ must be equal to $\left(  1,2,\ldots
,n\right)  $. Hence, there exists at most one $n$-tuple $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n}$ (namely, the $n$-tuple $\left(  1,2,\ldots
,n\right)  $).

We now know the following two facts:

\begin{itemize}
\item There exists an $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$,
namely the $n$-tuple $\left(  1,2,\ldots,n\right)  $.

\item There exists at most one $n$-tuple $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}%
<g_{2}<\cdots<g_{n}$.
\end{itemize}

Combining these two facts, we conclude that there exists exactly one $n$-tuple
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}
^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$, namely the $n$-tuple $\left(
1,2,\ldots,n\right)  $. This proves Lemma \ref{lem.increasing-sequences}
\textbf{(a)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.sorting.basics}}

Before we give a formal proof of Proposition \ref{prop.sorting}, let us
outline the main ideas of this proof: We shall define the notion of
\textit{inversion} of an $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $
of integers\footnote{It will be defined as a pair $\left(  i,j\right)  $ of
integers satisfying $1\leq i<j\leq n$ and $a_{i}>a_{j}$. The analogy with the
notion of \textquotedblleft inversion\textquotedblright\ of a permutation is
intentional.}; then we will argue that switching two adjacent entries $a_{k}$
and $a_{k+1}$ of an $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $
which are \textquotedblleft out of order\textquotedblright\ (i.e., satisfy
$a_{k}>a_{k+1}$) reduces the number of inversions by $1$ (this is our equality
(\ref{pf.prop.sorting.a.fact5}) further below), and thus a sequence of such
switches will eventually end and therefore bring the tuple into weakly
increasing order. (This is similar to an argument in the solution of Exercise
\ref{exe.ps2.2.5} \textbf{(e)}.) Other proofs of Proposition
\ref{prop.sorting} \textbf{(a)} are possible\footnote{Roughly speaking, to
each \href{https://en.wikipedia.org/wiki/Sorting_algorithm}{sorting algorithm}
corresponds at least one proof of Proposition \ref{prop.sorting} \textbf{(a)}.
(\textquotedblleft At least\textquotedblright\ because there are often several
ways to prove the correctness of a given sorting algorithm.) The proof we just
outlined corresponds to \textquotedblleft bubble sort\textquotedblright.}.
Proposition \ref{prop.sorting} \textbf{(b)} will then easily follow (indeed,
we will argue that $a_{\sigma\left(  i\right)  }$ is the smallest integer $x$
such that at least $i$ different elements $j\in\left\{  1,2,\ldots,n\right\}
$ satisfy $a_{j}\leq x$), and Proposition \ref{prop.sorting} \textbf{(c)} will
finally follow from parts \textbf{(a)} and \textbf{(b)}.

Here is the proof in detail:

\begin{proof}
[Proof of Proposition \ref{prop.sorting}.]\textbf{(a)} Let us forget that we
fixed $a_{1},a_{2},\ldots,a_{n}$.

We shall first introduce some notations.

We let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.

If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an $n$-tuple of
integers, then:

\begin{itemize}
\item An \textit{inversion} of $\mathbf{a}$ will mean a pair $\left(
i,j\right)  \in\left[  n\right]  ^{2}$ satisfying $i<j$ and $a_{i}>a_{j}$.

\item We denote by $\operatorname*{Inv}\left(  \mathbf{a}\right)  $ the set of
all inversions of $\mathbf{a}$. Thus, $\operatorname*{Inv}\left(
\mathbf{a}\right)  \subseteq\left[  n\right]  ^{2}$. More precisely,%
\begin{align}
\operatorname*{Inv}\left(  \mathbf{a}\right)   &  =\left(  \text{the set of
all inversions of }\mathbf{a}\right) \nonumber\\
&  =\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\text{
and }a_{i}>a_{j}\right\} \label{pf.prop.sorting.a.Inv.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the inversions of }\mathbf{a}\text{ are the pairs }\left(
i,j\right)  \in\left[  n\right]  ^{2}\text{ satisfying}\\
i<j\text{ and }a_{i}>a_{j}\text{ (by the definition of an \textquotedblleft
inversion\textquotedblright)}%
\end{array}
\right) \nonumber\\
&  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{
and }a_{u}>a_{v}\right\} \label{pf.prop.sorting.a.Inv.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
i,j\right)  \text{ as }\left(  u,v\right)  \right)  .\nonumber
\end{align}


\item We denote by $\ell\left(  \mathbf{a}\right)  $ the number $\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert $. (This is
well-defined because $\operatorname*{Inv}\left(  \mathbf{a}\right)  $ is
finite (since $\operatorname*{Inv}\left(  \mathbf{a}\right)  \subseteq\left[
n\right]  ^{2}$).) Thus,%
\begin{align}
\ell\left(  \mathbf{a}\right)   &  =\left\vert \underbrace{\operatorname*{Inv}%
\left(  \mathbf{a}\right)  }_{=\left(  \text{the set of all inversions of
}\mathbf{a}\right)  }\right\vert =\left\vert \left(  \text{the set of all
inversions of }\mathbf{a}\right)  \right\vert \nonumber\\
&  =\left(  \text{the number of all inversions of }\mathbf{a}\right)  .
\label{pf.prop.sorting.a.l(sigma)}%
\end{align}


\item For every permutation $\tau\in S_{n}$, we denote by $\mathbf{a}\circ
\tau$ the $n$-tuple $\left(  a_{\tau\left(  1\right)  },a_{\tau\left(
2\right)  },\ldots,a_{\tau\left(  n\right)  }\right)  $ of integers.
\end{itemize}

We notice that if $\mathbf{a}$ is any $n$-tuple of integers, then%
\begin{equation}
\mathbf{a}\circ\left(  \sigma\circ\tau\right)  =\left(  \mathbf{a}\circ
\sigma\right)  \circ\tau\ \ \ \ \ \ \ \ \ \ \text{for any }\sigma\in
S_{n}\text{ and }\tau\in S_{n} \label{pf.prop.sorting.a.sigmatau}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.sigmatau}):} Let
$\mathbf{a}$ be any $n$-tuple of integers. Let $\sigma\in S_{n}$ and $\tau\in
S_{n}$. We must show that $\mathbf{a}\circ\left(  \sigma\circ\tau\right)
=\left(  \mathbf{a}\circ\sigma\right)  \circ\tau$.
\par
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
Thus, the definition of $\mathbf{a}\circ\left(  \sigma\circ\tau\right)  $
yields%
\[
\mathbf{a}\circ\left(  \sigma\circ\tau\right)  =\left(  a_{\left(  \sigma
\circ\tau\right)  \left(  1\right)  },a_{\left(  \sigma\circ\tau\right)
\left(  2\right)  },\ldots,a_{\left(  \sigma\circ\tau\right)  \left(
n\right)  }\right)  =\left(  a_{\sigma\left(  \tau\left(  1\right)  \right)
},a_{\sigma\left(  \tau\left(  2\right)  \right)  },\ldots,a_{\sigma\left(
\tau\left(  n\right)  \right)  }\right)
\]
(since $a_{\left(  \sigma\circ\tau\right)  \left(  i\right)  }=a_{\sigma
\left(  \tau\left(  i\right)  \right)  }$ for every $i\in\left\{
1,2,\ldots,n\right\}  $). On the other hand, the definition of $\mathbf{a}%
\circ\sigma$ yields $\mathbf{a}\circ\sigma=\left(  a_{\sigma\left(  1\right)
},a_{\sigma\left(  2\right)  },\ldots,a_{\sigma\left(  n\right)  }\right)  $
(since $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $). Therefore, the
definition of $\left(  \mathbf{a}\circ\sigma\right)  \circ\tau$ yields
$\left(  \mathbf{a}\circ\sigma\right)  \circ\tau=\left(  a_{\sigma\left(
\tau\left(  1\right)  \right)  },a_{\sigma\left(  \tau\left(  2\right)
\right)  },\ldots,a_{\sigma\left(  \tau\left(  n\right)  \right)  }\right)  $.
Compared with $\mathbf{a}\circ\left(  \sigma\circ\tau\right)  =\left(
a_{\sigma\left(  \tau\left(  1\right)  \right)  },a_{\sigma\left(  \tau\left(
2\right)  \right)  },\ldots,a_{\sigma\left(  \tau\left(  n\right)  \right)
}\right)  $, this yields $\mathbf{a}\circ\left(  \sigma\circ\tau\right)
=\left(  \mathbf{a}\circ\sigma\right)  \circ\tau$. This proves
(\ref{pf.prop.sorting.a.sigmatau}).}. Also, if $\mathbf{a}$ is any $n$-tuple
of integers, then%
\begin{equation}
\mathbf{a}\circ\operatorname*{id}=\mathbf{a} \label{pf.prop.sorting.a.id}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.id}):} Let $\mathbf{a}$ be
any $n$-tuple of integers.
\par
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
Thus, the definition of $\mathbf{a}\circ\operatorname*{id}$ yields%
\[
\mathbf{a}\circ\operatorname*{id}=\left(  a_{\operatorname*{id}\left(
1\right)  },a_{\operatorname*{id}\left(  2\right)  },\ldots
,a_{\operatorname*{id}\left(  n\right)  }\right)  =\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)
\]
(since $a_{\operatorname*{id}\left(  i\right)  }=a_{i}$ for every
$i\in\left\{  1,2,\ldots,n\right\}  $). Compared with $\mathbf{a}=\left(
a_{1},a_{2},\ldots,a_{n}\right)  $, this yields $\mathbf{a}\circ
\operatorname*{id}=\mathbf{a}$. This proves (\ref{pf.prop.sorting.a.id}).}.

If $X$, $X^{\prime}$, $Y$ and $Y^{\prime}$ are four sets and if $\alpha
:X\rightarrow X^{\prime}$ and $\beta:Y\rightarrow Y^{\prime}$ are two maps,
then $\alpha\times\beta$ will denote the map%
\begin{align*}
X\times Y  &  \rightarrow X^{\prime}\times Y^{\prime},\\
\left(  x,y\right)   &  \mapsto\left(  \alpha\left(  x\right)  ,\beta\left(
y\right)  \right)  .
\end{align*}


Recall that, for each $k\in\left\{  1,2,\ldots,n-1\right\}  $, we have defined
$s_{k}$ to be the permutation in $S_{n}$ that switches $k$ with $k+1$ but
leaves all other numbers unchanged. This permutation $s_{k}$ is a map $\left[
n\right]  \rightarrow\left[  n\right]  $ and satisfies $s_{k}^{2}%
=\operatorname*{id}$. For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the
map $s_{k}\times s_{k}:\left[  n\right]  \times\left[  n\right]
\rightarrow\left[  n\right]  \times\left[  n\right]  $ satisfies $\left(
s_{k}\times s_{k}\right)  ^{2}=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\left[  n\right]  \times\left[
n\right]  $. Then, we can write $u$ in the form $\left(  i,j\right)  $ for
some $i\in\left[  n\right]  $ and $j\in\left[  n\right]  $. Consider these $i$
and $j$. We have%
\begin{align*}
\underbrace{\left(  s_{k}\times s_{k}\right)  ^{2}}_{=\left(  s_{k}\times
s_{k}\right)  \circ\left(  s_{k}\times s_{k}\right)  }\left(  \underbrace{u}%
_{=\left(  i,j\right)  }\right)   &  =\left(  \left(  s_{k}\times
s_{k}\right)  \circ\left(  s_{k}\times s_{k}\right)  \right)  \left(  \left(
i,j\right)  \right)  =\left(  s_{k}\times s_{k}\right)  \left(
\underbrace{\left(  s_{k}\times s_{k}\right)  \left(  \left(  i,j\right)
\right)  }_{\substack{=\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)
\right)  \\\text{(by the definition of }s_{k}\times s_{k}\text{)}}}\right) \\
&  =\left(  s_{k}\times s_{k}\right)  \left(  \left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  \right)  =\left(  \underbrace{s_{k}\left(
s_{k}\left(  i\right)  \right)  }_{=s_{k}^{2}\left(  i\right)  }%
,\underbrace{s_{k}\left(  s_{k}\left(  j\right)  \right)  }_{=s_{k}^{2}\left(
j\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }s_{k}\times
s_{k}\right) \\
&  =\left(  \underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(  i\right)
,\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(  j\right)  \right)
=\left(  \underbrace{\operatorname*{id}\left(  i\right)  }_{=i}%
,\underbrace{\operatorname*{id}\left(  j\right)  }_{=j}\right)  =\left(
i,j\right)  =u=\operatorname*{id}\left(  u\right)  .
\end{align*}
\par
Now, let us forget that we fixed $u$. We thus have shown that $\left(
s_{k}\times s_{k}\right)  ^{2}\left(  u\right)  =\operatorname*{id}\left(
u\right)  $ for every $u\in\left[  n\right]  \times\left[  n\right]  $. In
other words, $\left(  s_{k}\times s_{k}\right)  ^{2}=\operatorname*{id}$,
qed.}, and thus is a bijection from $\left[  n\right]  ^{2}$ to $\left[
n\right]  ^{2}$\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{
1,2,\ldots,n-1\right\}  $. We have $\left(  s_{k}\times s_{k}\right)
\circ\left(  s_{k}\times s_{k}\right)  =\left(  s_{k}\times s_{k}\right)
^{2}=\operatorname*{id}$. Hence, the maps $s_{k}\times s_{k}$ and $s_{k}\times
s_{k}$ are mutually inverse. Hence, the map $s_{k}\times s_{k}$ is invertible,
thus a bijection. Therefore, this map $s_{k}\times s_{k}$ is a bijection from
$\left[  n\right]  ^{2}$ to $\left[  n\right]  ^{2}$ (because its domain is
$\left[  n\right]  \times\left[  n\right]  =\left[  n\right]  ^{2}$, and its
codomain is $\left[  n\right]  \times\left[  n\right]  =\left[  n\right]
^{2}$). Qed.}.

Let us now recall a simple fact: If $u$ and $v$ are two integers such that
$1\leq u<v\leq n$, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $ is such that
$\left(  u,v\right)  \neq\left(  k,k+1\right)  $, then%
\begin{equation}
s_{k}\left(  u\right)  <s_{k}\left(  v\right)  . \label{pf.prop.sorting.a.2.5}%
\end{equation}
(This was proven in the solution of Exercise \ref{exe.ps2.2.5} \textbf{(a)}.)

Now, we notice the following facts:

\begin{itemize}
\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  =\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{s_{k}\left(
u\right)  }>a_{s_{k}\left(  v\right)  }\right\}
\label{pf.prop.sorting.a.fact1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact1}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $. The definition of
$\mathbf{a}\circ s_{k}$ yields $\mathbf{a}\circ s_{k}=\left(  a_{s_{k}\left(
1\right)  },a_{s_{k}\left(  2\right)  },\ldots,a_{s_{k}\left(  n\right)
}\right)  $ (since $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $).
Hence, (\ref{pf.prop.sorting.a.Inv.2}) (applied to $\mathbf{a}\circ s_{k}$ and
$a_{s_{k}\left(  i\right)  }$ instead of $\mathbf{a}$ and $a_{i}$) yields
$\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  =\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{s_{k}\left(
u\right)  }>a_{s_{k}\left(  v\right)  }\right\}  $. This proves
(\ref{pf.prop.sorting.a.fact1}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, and if $k\in\left\{
1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
\subseteq\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}
\label{pf.prop.sorting.a.fact2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact2}):} Let $\mathbf{a}$
be an $n$-tuple of integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
\par
Let $c\in\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}%
\left(  \mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}
\right)  $. Thus, $c\in\left[  n\right]  ^{2}$, so that we can write $c$ in
the form $c=\left(  i,j\right)  $ for some $i\in\left[  n\right]  $ and
$j\in\left[  n\right]  $. Consider these $i$ and $j$.
\par
We have $\left(  i,j\right)  =c\in\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  $, so that $\left(  s_{k}\times
s_{k}\right)  \left(  \left(  i,j\right)  \right)  \in\operatorname*{Inv}%
\left(  \mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}
$. Since $\left(  s_{k}\times s_{k}\right)  \left(  \left(  i,j\right)
\right)  =\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  $
(by the definition of $\left(  s_{k}\times s_{k}\right)  $), this rewrites as
$\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)
\in\operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{  \left(
k,k+1\right)  \right\}  $. In other words, $\left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  \in\operatorname*{Inv}\left(  \mathbf{a}%
\right)  $ and $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)
\right)  \neq\left(  k,k+1\right)  $.
\par
We have $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)
\in\operatorname*{Inv}\left(  \mathbf{a}\right)  =\left\{  \left(  u,v\right)
\in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  $ (by
(\ref{pf.prop.sorting.a.Inv.2})). In other words, $\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  $ is an element of $\left[
n\right]  ^{2}$ and satisfies $s_{k}\left(  i\right)  <s_{k}\left(  j\right)
$ and $a_{s_{k}\left(  i\right)  }>a_{s_{k}\left(  j\right)  }$. We have
$s_{k}\left(  i\right)  <s_{k}\left(  j\right)  $ and $\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  \neq\left(  k,k+1\right)  $. Thus,
we can apply (\ref{pf.prop.sorting.a.2.5}) to $u=s_{k}\left(  i\right)  $ and
$v=s_{k}\left(  j\right)  $. We thus conclude $s_{k}\left(  s_{k}\left(
i\right)  \right)  <s_{k}\left(  s_{k}\left(  j\right)  \right)  $. In other
words, $i<j$ (since $s_{k}\left(  s_{k}\left(  i\right)  \right)
=\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(  i\right)
=\operatorname*{id}\left(  i\right)  =i$ and $s_{k}\left(  s_{k}\left(
j\right)  \right)  =\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(
j\right)  =\operatorname*{id}\left(  j\right)  =j$).
\par
Now, we know that $\left(  i,j\right)  $ is an element $\left(  u,v\right)  $
of $\left[  n\right]  ^{2}$ satisfying $u<v$ and $a_{s_{k}\left(  u\right)
}>a_{s_{k}\left(  v\right)  }$ (since $i<j$ and $a_{s_{k}\left(  i\right)
}>a_{s_{k}\left(  j\right)  }$). In other words,%
\[
\left(  i,j\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(
v\right)  }\right\}  =\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\]
(by (\ref{pf.prop.sorting.a.fact1})).
\par
Next, let us assume (for the sake of contradiction) that $\left(  i,j\right)
=\left(  k,k+1\right)  $. Thus, $i=k$ and $j=k+1$. Thus, $s_{k}\left(
\underbrace{i}_{=k}\right)  =s_{k}\left(  k\right)  =k+1>k=s_{k}\left(
\underbrace{k+1}_{=j}\right)  =s_{k}\left(  j\right)  $; but this contradicts
$s_{k}\left(  i\right)  <s_{k}\left(  j\right)  $. This contradiction shows
that our assumption (that $\left(  i,j\right)  =\left(  k,k+1\right)  $) was
wrong. Hence, we cannot have $\left(  i,j\right)  =\left(  k,k+1\right)  $. In
other words, we must have $\left(  i,j\right)  \neq\left(  k,k+1\right)  $.
Combined with $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  $, this yields $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  $. Thus, $c=\left(  i,j\right)  \in
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  $ for every $c\in\left(  s_{k}\times
s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  \right)  $. In other words,%
\[
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
\subseteq\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  .
\]
This proves (\ref{pf.prop.sorting.a.fact2}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, and if $k\in\left\{
1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \subseteq\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  \label{pf.prop.sorting.a.fact3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact3}):} Let $\mathbf{a}$
be an $n$-tuple of integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
\par
Let $c\in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  $. Thus, $c\in
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \subseteq\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  \subseteq\left[  n\right]  ^{2}$, so that we can
write $c$ in the form $c=\left(  i,j\right)  $ for some $i\in\left[  n\right]
$ and $j\in\left[  n\right]  $. Consider these $i$ and $j$.
\par
We have $\left(  i,j\right)  =c\in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  $. In other
words, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  $ and $\left(  i,j\right)  \neq\left(  k,k+1\right)  $.
\par
We have $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\text{ and }a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(  v\right)
}\right\}  $ (by (\ref{pf.prop.sorting.a.fact1})). In other words, $\left(
i,j\right)  $ is an element of $\left[  n\right]  ^{2}$ and satisfies $i<j$
and $a_{s_{k}\left(  i\right)  }>a_{s_{k}\left(  j\right)  }$. Applying
(\ref{pf.prop.sorting.a.2.5}) to $u=i$ and $v=j$, we obtain $s_{k}\left(
i\right)  <s_{k}\left(  j\right)  $ (since $i<j$ and $\left(  i,j\right)
\neq\left(  k,k+1\right)  $).
\par
The pair $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  $ is
a pair $\left(  u,v\right)  \in\left[  n\right]  ^{2}$ satisfying $u<v$ and
$a_{u}>a_{v}$ (since $s_{k}\left(  i\right)  <s_{k}\left(  j\right)  $ and
$a_{s_{k}\left(  i\right)  }>a_{s_{k}\left(  j\right)  }$). In other words,
$\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  \in\left\{
\left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }%
a_{u}>a_{v}\right\}  =\operatorname*{Inv}\left(  \mathbf{a}\right)  $ (by
(\ref{pf.prop.sorting.a.Inv.2})).
\par
Next, let us assume (for the sake of contradiction) that $\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  =\left(  k,k+1\right)  $. Thus,
$s_{k}\left(  i\right)  =k$ and $s_{k}\left(  j\right)  =k+1$. Hence,
$k+1=s_{k}\left(  \underbrace{k}_{=s_{k}\left(  i\right)  }\right)
=s_{k}\left(  s_{k}\left(  i\right)  \right)  =\underbrace{s_{k}^{2}%
}_{=\operatorname*{id}}\left(  i\right)  =i$ and $k=s_{k}\left(
\underbrace{k+1}_{=s_{k}\left(  j\right)  }\right)  =s_{k}\left(  s_{k}\left(
j\right)  \right)  =\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(
j\right)  =\operatorname*{id}\left(  j\right)  =j$, so that $i=k+1>k=j$. This
contradicts $i<j$. This contradiction shows that our assumption (that $\left(
s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  =\left(  k,k+1\right)
$) was wrong. Hence, we cannot have $\left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  =\left(  k,k+1\right)  $. In other words, we
must have $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)
\neq\left(  k,k+1\right)  $. Combined with $\left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  \in\operatorname*{Inv}\left(  \mathbf{a}%
\right)  $, this yields $\left(  s_{k}\left(  i\right)  ,s_{k}\left(
j\right)  \right)  \in\operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  $.
\par
The definition of $s_{k}\times s_{k}$ yields $\left(  s_{k}\times
s_{k}\right)  \left(  \left(  i,j\right)  \right)  =\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  \in\operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  $. In
other words, $\left(  i,j\right)  \in\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  $. Hence,%
\[
c=\left(  i,j\right)  \in\left(  s_{k}\times s_{k}\right)  ^{-1}\left(
\operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{  \left(
k,k+1\right)  \right\}  \right)  .
\]
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in\left(
s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
$ for every $c\in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  $. In other words,%
\[
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \subseteq\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  .
\]
This proves (\ref{pf.prop.sorting.a.fact3}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, and if $k\in\left\{
1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
=\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \label{pf.prop.sorting.a.fact4}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact4}):} Let $\mathbf{a}$
be an $n$-tuple of integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Then, combining (\ref{pf.prop.sorting.a.fact2}) with
(\ref{pf.prop.sorting.a.fact3}), we obtain $\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  =\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  \setminus\left\{  \left(  k,k+1\right)
\right\}  $. This proves (\ref{pf.prop.sorting.a.fact4}).}.

\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $ is such
that $a_{k}>a_{k+1}$, then%
\begin{equation}
\ell\left(  \mathbf{a}\circ s_{k}\right)  =\ell\left(  \mathbf{a}\right)  -1
\label{pf.prop.sorting.a.fact5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact5}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $ be such that
$a_{k}>a_{k+1}$.
\par
The definition of $\ell\left(  \mathbf{a}\right)  $ yields $\ell\left(
\mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert $.
\par
The pair $\left(  k,k+1\right)  $ is a pair $\left(  u,v\right)  \in\left[
n\right]  ^{2}$ such that $u<v$ and $a_{u}>a_{v}$ (since $k<k+1$ and
$a_{k}>a_{k+1}$). In other words,%
\[
\left(  k,k+1\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  =\operatorname*{Inv}\left(
\mathbf{a}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.sorting.a.Inv.2})}\right)  .
\]
Hence, $\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  \right\vert
=\underbrace{\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert }_{=\ell\left(  \mathbf{a}\right)  }-1=\ell\left(  \mathbf{a}%
\right)  -1$. The map $s_{k}\times s_{k}$ is a bijection, and thus we have
$\left\vert \left(  s_{k}\times s_{k}\right)  ^{-1}\left(  X\right)
\right\vert =\left\vert X\right\vert $ for every subset $X$ of $\left[
n\right]  ^{2}$. Applying this to $X=\operatorname*{Inv}\left(  \mathbf{a}%
\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  $, we obtain%
\begin{equation}
\left\vert \left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}%
\left(  \mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}
\right)  \right\vert =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  \right\vert =\ell\left(
\mathbf{a}\right)  -1. \label{pf.prop.sorting.a.fact5.pf.1}%
\end{equation}
\par
On the other hand, let us assume (for the sake of contradiction) that $\left(
k,k+1\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  $.
Thus,%
\[
\left(  k,k+1\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\text{ and }a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(  v\right)
}\right\}
\]
(by (\ref{pf.prop.sorting.a.fact1})). In other words, $\left(  k,k+1\right)  $
is a pair $\left(  u,v\right)  \in\left[  n\right]  ^{2}$ such that $u<v$ and
$a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(  v\right)  }$. In other words,
$k<k+1$ and $a_{s_{k}\left(  k\right)  }>a_{s_{k}\left(  k+1\right)  }$. Now,
$a_{s_{k}\left(  k\right)  }>a_{s_{k}\left(  k+1\right)  }$. In other words,
$a_{k+1}>a_{k}$ (since $s_{k}\left(  k\right)  =k+1$ and $s_{k}\left(
k+1\right)  =k$). This contradicts $a_{k}>a_{k+1}$. This contradiction shows
that our assumption (that $\left(  k,k+1\right)  \in\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  $) was wrong. Hence, we cannot have $\left(
k,k+1\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  $.
We thus have $\left(  k,k+1\right)  \notin\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  $. Hence, $\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  \setminus\left\{  \left(  k,k+1\right)
\right\}  =\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  $. Now,
(\ref{pf.prop.sorting.a.fact4}) becomes%
\[
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
=\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  =\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  .
\]
Therefore, (\ref{pf.prop.sorting.a.fact5.pf.1}) rewrites as $\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \right\vert
=\ell\left(  \mathbf{a}\right)  -1$.
\par
But the definition of $\ell\left(  \mathbf{a}\circ s_{k}\right)  $ yields
$\ell\left(  \mathbf{a}\circ s_{k}\right)  =\left\vert \operatorname*{Inv}%
\left(  \mathbf{a}\circ s_{k}\right)  \right\vert =\ell\left(  \mathbf{a}%
\right)  -1$. This proves (\ref{pf.prop.sorting.a.fact5}).}.

\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers satisfying $\ell\left(  \mathbf{a}\right)  =0$, then%
\begin{equation}
a_{1}\leq a_{2}\leq\cdots\leq a_{n} \label{pf.prop.sorting.a.fact7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact7}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers such that $\ell\left(  \mathbf{a}\right)  =0$.
\par
We assume (for the sake of contradiction) that there exists some $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $a_{k}>a_{k+1}$. Consider this $k$. Then,
$\left(  k,k+1\right)  $ is an element $\left(  u,v\right)  \in\left[
n\right]  ^{2}$ satisfying $u<v$ and $a_{u}>a_{v}$ (since $k<k+1$ and
$a_{k}>a_{k+1}$). In other words,%
\[
\left(  k,k+1\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  =\operatorname*{Inv}\left(
\mathbf{a}\right)
\]
(by (\ref{pf.prop.sorting.a.Inv.2})). Hence, the set $\operatorname*{Inv}%
\left(  \mathbf{a}\right)  $ contains at least one element (namely, $\left(
k,k+1\right)  $). In other words, $\left\vert \operatorname*{Inv}\left(
\mathbf{a}\right)  \right\vert \geq1$.
\par
But the definition of $\ell\left(  \mathbf{a}\right)  $ yields $\ell\left(
\mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert \geq1$. This contradicts $\ell\left(  \mathbf{a}\right)  =0$. This
contradiction shows that our assumption (that there exists some $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $a_{k}>a_{k+1}$) was wrong.
\par
Therefore, there exists no $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$a_{k}>a_{k+1}$. In other words, every $k\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies $a_{k}\leq a_{k+1}$. In other words, $a_{1}\leq a_{2}\leq\cdots\leq
a_{n}$. This proves (\ref{pf.prop.sorting.a.fact7}).}.

\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers satisfying $a_{1}\leq a_{2}\leq\cdots\leq a_{n}$, then%
\begin{equation}
\ell\left(  \mathbf{a}\right)  =0 \label{pf.prop.sorting.a.fact8}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact8}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers satisfying $a_{1}\leq a_{2}\leq\cdots\leq a_{n}$. We must prove that
$\ell\left(  \mathbf{a}\right)  =0$.
\par
Indeed, assume the contrary. Thus, $\ell\left(  \mathbf{a}\right)  \neq0$. But
the definition of $\ell\left(  \mathbf{a}\right)  $ yields $\ell\left(
\mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert $, so that $\left\vert \operatorname*{Inv}\left(  \mathbf{a}%
\right)  \right\vert =\ell\left(  \mathbf{a}\right)  \neq0$. Hence, the set
$\operatorname*{Inv}\left(  \mathbf{a}\right)  $ is nonempty. In other words,
there exists some $c\in\operatorname*{Inv}\left(  \mathbf{a}\right)  $.
Consider this $c$.
\par
Recall that $a_{1}\leq a_{2}\leq\cdots\leq a_{n}$. In other words,%
\begin{equation}
a_{u}\leq a_{v} \label{pf.prop.sorting.a.fact8.pf.leq}%
\end{equation}
for any $u\in\left[  n\right]  $ and $v\in\left[  n\right]  $ satisfying
$u<v$.
\par
But%
\[
c\in\operatorname*{Inv}\left(  \mathbf{a}\right)  =\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{u}%
>a_{v}\right\}
\]
(by (\ref{pf.prop.sorting.a.Inv.2})). In other words, $c$ has the form
$c=\left(  u,v\right)  $ for some $\left(  u,v\right)  \in\left[  n\right]
^{2}$ satisfying $u<v$ and $a_{u}>a_{v}$. Consider this $\left(  u,v\right)
\in\left[  n\right]  ^{2}$. We have $u<v$, and thus $a_{u}\leq a_{v}$ (by
(\ref{pf.prop.sorting.a.fact8.pf.leq})). This contradicts $a_{u}>a_{v}$. This
contradiction proves that our assumption was wrong.
\par
Hence, we have shown that $\ell\left(  \mathbf{a}\right)  =0$. This proves
(\ref{pf.prop.sorting.a.fact8}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, then%
\begin{equation}
\text{there exists a }\sigma\in S_{n}\text{ such that }\ell\left(
\mathbf{a}\circ\sigma\right)  =0 \label{pf.prop.sorting.a.fact6}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact6}):} We shall prove
(\ref{pf.prop.sorting.a.fact6}) by induction over $\ell\left(  \mathbf{a}%
\right)  $:
\par
\textit{Induction base:} If $\mathbf{a}$ is an $n$-tuple of integers
satisfying $\ell\left(  \mathbf{a}\right)  =0$, then we have $\ell\left(
\underbrace{\mathbf{a}\circ\operatorname*{id}}_{\substack{=\mathbf{a}%
\\\text{(by (\ref{pf.prop.sorting.a.id}))}}}\right)  =\ell\left(
\mathbf{a}\right)  =0$. Hence, if $\mathbf{a}$ is an $n$-tuple of integers
satisfying $\ell\left(  \mathbf{a}\right)  =0$, then there exists a $\sigma\in
S_{n}$ such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$ (namely,
$\sigma=\operatorname*{id}$). In other words, (\ref{pf.prop.sorting.a.fact6})
holds for $\ell\left(  \mathbf{a}\right)  =0$. This completes the induction
base.
\par
\textit{Induction step:} Let $L$ be a positive integer. Assume that
(\ref{pf.prop.sorting.a.fact6}) holds for $\ell\left(  \mathbf{a}\right)
=L-1$. We need to prove that (\ref{pf.prop.sorting.a.fact6}) holds for
$\ell\left(  \mathbf{a}\right)  =L$.
\par
We have assumed that (\ref{pf.prop.sorting.a.fact6}) holds for $\ell\left(
\mathbf{a}\right)  =L-1$. In other words, if $\mathbf{a}$ is an $n$-tuple of
integers satisfying $\ell\left(  \mathbf{a}\right)  =L-1$, then%
\begin{equation}
\text{there exists a }\sigma\in S_{n}\text{ such that }\ell\left(
\mathbf{a}\circ\sigma\right)  =0. \label{pf.prop.sorting.a.fact6.pf.hyp}%
\end{equation}
\par
Now, let $\mathbf{a}$ be an $n$-tuple of integers satisfying $\ell\left(
\mathbf{a}\right)  =L$. We shall show that there exists a $\sigma\in S_{n}$
such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$.
\par
Let us first prove that there exists some $k\in\left\{  1,2,\ldots
,n-1\right\}  $ such that $a_{k}>a_{k+1}$. In fact, assume the contrary. Thus,
there exists no $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$a_{k}>a_{k+1}$. In other words, every $k\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies $a_{k}\leq a_{k+1}$. In other words, $a_{1}\leq a_{2}\leq\cdots\leq
a_{n}$. Thus, (\ref{pf.prop.sorting.a.fact8}) shows that $\ell\left(
\mathbf{a}\right)  =0$. Hence, $0=\ell\left(  \mathbf{a}\right)  =L>0$ (since
$L$ is positive). This is absurd. This contradiction proves that our
assumption was wrong.
\par
Hence, we have proven that there exists some $k\in\left\{  1,2,\ldots
,n-1\right\}  $ such that $a_{k}>a_{k+1}$. Consider this $k$. Then,
(\ref{pf.prop.sorting.a.fact5}) shows that $\ell\left(  \mathbf{a}\circ
s_{k}\right)  =\underbrace{\ell\left(  \mathbf{a}\right)  }_{=L}-1=L-1$.
Therefore, (\ref{pf.prop.sorting.a.fact6.pf.hyp}) (applied to $\mathbf{a}\circ
s_{k}$ instead of $\mathbf{a}$) shows that there exists a $\sigma\in S_{n}$
such that $\ell\left(  \left(  \mathbf{a}\circ s_{k}\right)  \circ
\sigma\right)  =0$. Let $\tau$ be such a $\sigma$. Thus, $\tau$ is an element
of $S_{n}$ and satisfies $\ell\left(  \left(  \mathbf{a}\circ s_{k}\right)
\circ\tau\right)  =0$.
\par
Now, (\ref{pf.prop.sorting.a.sigmatau}) (applied to $s_{k}$ instead of
$\sigma$) yields $\mathbf{a}\circ\left(  s_{k}\circ\tau\right)  =\left(
\mathbf{a}\circ s_{k}\right)  \circ\tau$. Hence, $\ell\left(  \mathbf{a}%
\circ\left(  s_{k}\circ\tau\right)  \right)  =\ell\left(  \left(
\mathbf{a}\circ s_{k}\right)  \circ\tau\right)  =0$. Thus, there exists a
$\sigma\in S_{n}$ such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$
(namely, $\sigma=s_{k}\circ\tau$).
\par
Now, let us forget that we fixed $\mathbf{a}$. We thus have shown that if
$\mathbf{a}$ is an $n$-tuple of integers satisfying $\ell\left(
\mathbf{a}\right)  =L$, then there exists a $\sigma\in S_{n}$ such that
$\ell\left(  \mathbf{a}\circ\sigma\right)  =0$. In other words,
(\ref{pf.prop.sorting.a.fact6}) holds for $\ell\left(  \mathbf{a}\right)  =L$.
This completes the induction step. Thus, the induction proof of
(\ref{pf.prop.sorting.a.fact6}) is complete.}.
\end{itemize}

Now, let us prove Proposition \ref{prop.sorting} \textbf{(a)}. Let
$a_{1},a_{2},\ldots,a_{n}$ be $n$ integers. Let $\mathbf{a}$ be the $n$-tuple
$\left(  a_{1},a_{2},\ldots,a_{n}\right)  $. Then, there exists a $\sigma\in
S_{n}$ such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$ (by
(\ref{pf.prop.sorting.a.fact6})). Consider this $\sigma$. The definition of
$\mathbf{a}\circ\sigma$ shows that $\mathbf{a}\circ\sigma=\left(
a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  },\ldots
,a_{\sigma\left(  n\right)  }\right)  $ (since $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $). Therefore, (\ref{pf.prop.sorting.a.fact7})
(applied to $\mathbf{a}\circ\sigma$ and $a_{\sigma\left(  i\right)  }$ instead
of $\mathbf{a}$ and $a_{i}$) yields $a_{\sigma\left(  1\right)  }\leq
a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$.

Let us now forget that we defined $\sigma$. We thus have constructed a
$\sigma\in S_{n}$ satisfying $a_{\sigma\left(  1\right)  }\leq a_{\sigma
\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$. Therefore,
there exists a permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(
1\right)  }\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(
n\right)  }$. This proves Proposition \ref{prop.sorting} \textbf{(a)}.

\textbf{(b)} Let $\sigma\in S_{n}$ be such that $a_{\sigma\left(  1\right)
}\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }%
$. Let $i\in\left\{  1,2,\ldots,n\right\}  $. We shall now show that%
\begin{equation}
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  . \label{pf.prop.sorting.bclaim}%
\end{equation}
(This statement includes the tacit claim that the right hand side of
(\ref{pf.prop.sorting.bclaim}) is well-defined.)

\textit{Proof of (\ref{pf.prop.sorting.bclaim}):} Define a subset $X$ of
$\mathbb{Z}$ by%
\[
X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  .
\]
We shall show that $a_{\sigma\left(  i\right)  }=\min X$.

We have $a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }%
\leq\cdots\leq a_{\sigma\left(  n\right)  }$. In other words,%
\begin{equation}
a_{\sigma\left(  u\right)  }\leq a_{\sigma\left(  v\right)  }
\label{pf.prop.sorting.bclaim.pf.1}%
\end{equation}
for any two elements $u$ and $v$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $u\leq v$.

We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation, thus a bijective
map, thus an injective map.

Every $k\in\left\{  1,2,\ldots,i\right\}  $ satisfies $\sigma\left(  k\right)
\in\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
a_{\sigma\left(  i\right)  }\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  1,2,\ldots,i\right\}  $. Then, $k\leq i$ and thus $a_{\sigma
\left(  k\right)  }\leq a_{\sigma\left(  i\right)  }$ (by
(\ref{pf.prop.sorting.bclaim.pf.1}), applied to $u=k$ and $v=i$). Hence,
$\sigma\left(  k\right)  $ is an element $j$ of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $a_{j}\leq a_{\sigma\left(  i\right)  }$ (because
$a_{\sigma\left(  k\right)  }\leq a_{\sigma\left(  i\right)  }$). In other
words, $\sigma\left(  k\right)  \in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq a_{\sigma\left(  i\right)  }\right\}  $, qed.}. In other
words,%
\[
\left\{  \sigma\left(  k\right)  \ \mid\ k\in\left\{  1,2,\ldots,i\right\}
\right\}  \subseteq\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid
\ a_{j}\leq a_{\sigma\left(  i\right)  }\right\}  .
\]
Hence,%
\[
\left\vert \left\{  \sigma\left(  k\right)  \ \mid\ k\in\left\{
1,2,\ldots,i\right\}  \right\}  \right\vert \leq\left\vert \left\{
j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq a_{\sigma\left(
i\right)  }\right\}  \right\vert ,
\]
so that
\begin{align*}
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
a_{\sigma\left(  i\right)  }\right\}  \right\vert  &  \geq\left\vert
\underbrace{\left\{  \sigma\left(  k\right)  \ \mid\ k\in\left\{
1,2,\ldots,i\right\}  \right\}  }_{=\sigma\left(  \left\{  1,2,\ldots
,i\right\}  \right)  }\right\vert \\
&  =\left\vert \sigma\left(  \left\{  1,2,\ldots,i\right\}  \right)
\right\vert =\left\vert \left\{  1,2,\ldots,i\right\}  \right\vert \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the map }\sigma\text{ is
injective}\right) \\
&  =i.
\end{align*}
In other words, at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfy $a_{j}\leq a_{\sigma\left(  i\right)  }$. In other words,
$a_{\sigma\left(  i\right)  }$ is an element $x$ of $\mathbb{Z}$ such that at
least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq
x$. In other words,%
\[
a_{\sigma\left(  i\right)  }\in\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  =X.
\]


On the other hand, let $y$ be any element of $X$. We shall show that
$a_{\sigma\left(  i\right)  }\leq y$. Indeed, assume the contrary. Thus,
$a_{\sigma\left(  i\right)  }>y$. Hence,%
\begin{equation}
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
y\right\}  \right\vert <i \label{pf.prop.sorting.bclaim.pf.5}%
\end{equation}
\footnote{\textit{Proof.} Let $p\in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq y\right\}  $. Thus, $p$ is an element $j$ of $\left\{
1,2,\ldots,n\right\}  $ such that $a_{j}\leq y$. In other words, $p$ is an
element of $\left\{  1,2,\ldots,n\right\}  $ and satisfies $a_{p}\leq y$.
\par
The permutation $\sigma$ has an inverse $\sigma^{-1}$. Let $q=\sigma
^{-1}\left(  p\right)  $. Thus, $p=\sigma\left(  q\right)  $. Hence,
$a_{p}=a_{\sigma\left(  q\right)  }$, so that $a_{\sigma\left(  q\right)
}=a_{p}\leq y<a_{\sigma\left(  i\right)  }$ (since $a_{\sigma\left(  i\right)
}>y$).
\par
If we had $i\leq q$, then we would have $a_{\sigma\left(  i\right)  }\leq
a_{\sigma\left(  q\right)  }$ (by (\ref{pf.prop.sorting.bclaim.pf.1}), applied
to $u=i$ and $v=q$), which would contradict $a_{\sigma\left(  q\right)
}<a_{\sigma\left(  i\right)  }$. Hence, we cannot have $i\leq q$. Thus, we
must have $q<i$. Hence, $q\in\left\{  1,2,\ldots,i-1\right\}  $. Thus,
$p=\sigma\left(  \underbrace{q}_{\in\left\{  1,2,\ldots,i-1\right\}  }\right)
\in\sigma\left(  \left\{  1,2,\ldots,i-1\right\}  \right)  $.
\par
Let us now forget that we fixed $p$. We thus have shown that every
$p\in\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq y\right\}
$ satisfies $p\in\sigma\left(  \left\{  1,2,\ldots,i-1\right\}  \right)  $. In
other words,%
\[
\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq y\right\}
\subseteq\sigma\left(  \left\{  1,2,\ldots,i-1\right\}  \right)  .
\]
Hence,%
\begin{align*}
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
y\right\}  \right\vert  &  \leq\left\vert \sigma\left(  \left\{
1,2,\ldots,i-1\right\}  \right)  \right\vert =\left\vert \left\{
1,2,\ldots,i-1\right\}  \right\vert \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the map }\sigma\text{ is
injective}\right) \\
&  =i-1<i,
\end{align*}
qed.}.

But%
\[
y\in X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  .
\]
In other words, $y$ is an element $x$ of $\mathbb{Z}$ such that at least $i$
elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq x$. In other
words, $y$ is an element of $\mathbb{Z}$, and at least $i$ elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq y$. We have%
\[
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
y\right\}  \right\vert \geq i
\]
(since at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy
$a_{j}\leq y$). This contradicts (\ref{pf.prop.sorting.bclaim.pf.5}). This
contradiction proves that our assumption was wrong. Hence, $a_{\sigma\left(
i\right)  }\leq y$ is proven.

Now, let us forget that we fixed $y$. We thus have shown that every $y\in X$
satisfies $a_{\sigma\left(  i\right)  }\leq y$. Altogether, we thus have shown
the following two facts:

\begin{itemize}
\item We have $a_{\sigma\left(  i\right)  }\in X$.

\item Every $y\in X$ satisfies $a_{\sigma\left(  i\right)  }\leq y$.
\end{itemize}

Combining these two facts, we obtain $a_{\sigma\left(  i\right)  }=\min X$
(and, in particular, this shows that $\min X$ is well-defined). Since
\newline$X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements
}j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  $,
this rewrites as follows:%
\[
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  .
\]
Thus, (\ref{pf.prop.sorting.bclaim}) is proven.

Now, the value $a_{\sigma\left(  i\right)  }$ depends only on $a_{1}%
,a_{2},\ldots,a_{n}$ and $i$ (but not on $\sigma$) (because
(\ref{pf.prop.sorting.bclaim}) gives a description of $a_{\sigma\left(
i\right)  }$ which involves $a_{1},a_{2},\ldots,a_{n}$ and $i$, but not
$\sigma$). Thus, Proposition \ref{prop.sorting} \textbf{(b)} is proven.

\textbf{(c)} The integers $a_{1},a_{2},\ldots,a_{n}$ are distinct. In other
words, if $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots
,n\right\}  $, then%
\begin{equation}
a_{u}\neq a_{v}. \label{pf.prop.sorting.c.distinct}%
\end{equation}


We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation, thus a bijective
map, thus an injective map.

Now, we can see that:

\begin{itemize}
\item There is \textbf{at least one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.sorting} \textbf{(a)} shows that there exists a permutation
$\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(
2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$. Consider this
$\sigma$.
\par
Let $k\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $a_{\sigma\left(  k\right)
}\leq a_{\sigma\left(  k+1\right)  }$ (since $a_{\sigma\left(  1\right)  }\leq
a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$). But
on the other hand, $k\neq k+1$. Hence, $\sigma\left(  k\right)  \neq
\sigma\left(  k+1\right)  $ (since the map $\sigma$ is injective). Hence,
$a_{\sigma\left(  k\right)  }\neq a_{\sigma\left(  k+1\right)  }$ (by
(\ref{pf.prop.sorting.c.distinct}), applied to $u=\sigma\left(  k\right)  $
and $v=\sigma\left(  k+1\right)  $). Combined with $a_{\sigma\left(  k\right)
}\leq a_{\sigma\left(  k+1\right)  }$, this yields $a_{\sigma\left(  k\right)
}<a_{\sigma\left(  k+1\right)  }$.
\par
Let us now forget that we fixed $k$. We thus have shown that $a_{\sigma\left(
k\right)  }<a_{\sigma\left(  k+1\right)  }$ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. In other words, $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$.
\par
Let us now forget that we defined $\sigma$. We thus have constructed a
permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. Hence,
there is \textbf{at least one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. Qed.}.

\item There is \textbf{at most one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$\ \ \ \ \footnote{\textit{Proof.} Let
$\sigma_{1}$ and $\sigma_{2}$ be two permutations $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. We shall show that $\sigma_{1}=\sigma_{2}$.
\par
Fix $i\in\left\{  1,2,\ldots,n\right\}  $.
\par
We know that $\sigma_{1}$ is a permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. In other words, $\sigma_{1}$ is a permutation
in $S_{n}$ such that $a_{\sigma_{1}\left(  1\right)  }<a_{\sigma_{1}\left(
2\right)  }<\cdots<a_{\sigma_{1}\left(  n\right)  }$. Thus,
(\ref{pf.prop.sorting.bclaim}) (applied to $\sigma=\sigma_{1}$) yields%
\begin{equation}
a_{\sigma_{1}\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy
}a_{j}\leq x\right\}  . \label{pf.prop.sorting.c.5}%
\end{equation}
The same argument (applied to $\sigma_{2}$ instead of $\sigma_{1}$) yields%
\[
a_{\sigma_{2}\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy
}a_{j}\leq x\right\}  .
\]
Comparing this with (\ref{pf.prop.sorting.c.5}), we obtain $a_{\sigma
_{1}\left(  i\right)  }=a_{\sigma_{2}\left(  i\right)  }$.
\par
Now, if we had $\sigma_{1}\left(  i\right)  \neq\sigma_{2}\left(  i\right)  $,
then we would have $a_{\sigma_{1}\left(  i\right)  }\neq a_{\sigma_{2}\left(
i\right)  }$ (by (\ref{pf.prop.sorting.c.distinct}), applied to $u=\sigma
_{1}\left(  i\right)  $ and $v=\sigma_{2}\left(  i\right)  $), which would
contradict $a_{\sigma_{1}\left(  i\right)  }=a_{\sigma_{2}\left(  i\right)  }%
$. Thus, we cannot have $\sigma_{1}\left(  i\right)  \neq\sigma_{2}\left(
i\right)  $. Hence, we must have $\sigma_{1}\left(  i\right)  =\sigma
_{2}\left(  i\right)  $.
\par
Let us now forget that we fixed $i$. We thus have shown that $\sigma
_{1}\left(  i\right)  =\sigma_{2}\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,n\right\}  $. In other words, $\sigma_{1}=\sigma_{2}$.
\par
Let us now forget that we fixed $\sigma_{1}$ and $\sigma_{2}$. We thus have
shown that $\sigma_{1}=\sigma_{2}$ whenever $\sigma_{1}$ and $\sigma_{2}$ are
two permutations $\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. In other
words, there is \textbf{at most one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. Qed.}.
\end{itemize}

Combining these two statements, we conclude that there is a \textbf{unique}
permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. This
proves Proposition \ref{prop.sorting} \textbf{(c)}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.cauchy-binet.EI}.]We have%
\[
\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots,k_{n}\text{ are
distinct}\right\}
\]
and%
\[
\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}  .
\]


\begin{vershort}
Clearly, every element of $\mathbf{I}\times S_{n}$ can be written in the form
$\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}$.
\end{vershort}

\begin{verlong}
Every element of $\mathbf{I}\times S_{n}$ can be written in the form $\left(
\left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\alpha\in\mathbf{I}\times S_{n}$.
Then, $\alpha$ can be written in the form $\alpha=\left(  \beta,\gamma\right)
$ for some $\beta\in\mathbf{I}$ and $\gamma\in S_{n}$ (since $\alpha
\in\mathbf{I}\times S_{n}$). Consider these $\beta$ and $\gamma$. We have
$\beta\in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}
\subseteq\left[  m\right]  ^{n}$. Hence, $\beta$ can be written in the form
$\beta=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ for some $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$. Consider this
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $. Clearly, $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  =\beta\in\mathbf{I}$.
\par
But $\alpha=\left(  \underbrace{\beta}_{=\left(  k_{1},k_{2},\ldots
,k_{n}\right)  },\gamma\right)  =\left(  \left(  k_{1},k_{2},\ldots
,k_{n}\right)  ,\gamma\right)  $. Hence, $\alpha$ can be written in the form
$\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}$
(namely, for $\left(  g_{1},g_{2},\ldots,g_{n}\right)  =\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  $ and $\sigma=\gamma$).
\par
Now, let us forget that we fixed $\alpha$. We thus have proven that every
$\alpha\in\mathbf{I}\times S_{n}$ can be written in the form $\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}$. In
other words, every element of $\mathbf{I}\times S_{n}$ can be written in the
form $\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for
some $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in
S_{n}$. Qed.}.
\end{verlong}

For every $\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}$, we have $\left(  g_{\sigma\left(  1\right)
},g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(  n\right)  }\right)
\in\mathbf{E}$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  \in\mathbf{I}\times S_{n}$.
Thus, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in
S_{n}$.
\par
We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation, and thus a
bijective map, hence an injective map.
\par
We have
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $g_{1}<g_{2}<\cdots<g_{n}$. Hence, the integers $g_{1},g_{2}%
,\ldots,g_{n}$ are distinct (since $g_{1}<g_{2}<\cdots<g_{n}$). In other
words, any two distinct elements $u$ and $v$ of $\left[  n\right]  $ satisfy%
\begin{equation}
g_{u}\neq g_{v}. \label{pf.lem.cauchy-binet.EI.1.pf.1}%
\end{equation}
\par
Now, let $u$ and $v$ be two distinct elements of $\left[  n\right]  $. Thus,
$u\neq v$ (since $u$ and $v$ are distinct), so that $\sigma\left(  u\right)
\neq\sigma\left(  v\right)  $ (since $\sigma$ is injective). Hence,
$g_{\sigma\left(  u\right)  }\neq g_{\sigma\left(  v\right)  }$ (by
(\ref{pf.lem.cauchy-binet.EI.1.pf.1}), applied to $\sigma\left(  u\right)  $
and $\sigma\left(  v\right)  $ instead of $u$ and $v$).
\par
Let us now forget that we fixed $u$ and $v$. We thus have shown that any two
distinct elements $u$ and $v$ of $\left[  n\right]  $ satisfy $g_{\sigma
\left(  u\right)  }\neq g_{\sigma\left(  v\right)  }$. In other words, the
integers $g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  }%
,\ldots,g_{\sigma\left(  n\right)  }$ are distinct. Hence, $\left(
g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots
,g_{\sigma\left(  n\right)  }\right)  $ is an element $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that the integers
$k_{1},k_{2},\ldots,k_{n}$ are distinct. In other words,%
\[
\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  }%
,\ldots,g_{\sigma\left(  n\right)  }\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ \text{the
integers }k_{1},k_{2},\ldots,k_{n}\text{ are distinct}\right\}  =\mathbf{E},
\]
qed.}. Hence, we can define a map $\Phi:\mathbf{I}\times S_{n}\rightarrow
\mathbf{E}$ by setting%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\Phi\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  =\left(
g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots
,g_{\sigma\left(  n\right)  }\right) \\
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \left(  g_{1},g_{2},\ldots
,g_{n}\right)  ,\sigma\right)  \in\mathbf{I}\times S_{n}%
\end{array}
\right)  \label{pf.lem.cauchy-binet.EI.defPhi}%
\end{equation}
(since every element of $\mathbf{I}\times S_{n}$ can be written in the form
$\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}%
$). Consider this map $\Phi$.

The map $\Phi$ is the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
(since $\Phi$ is defined by (\ref{pf.lem.cauchy-binet.EI.defPhi})). In
particular, the latter map is well-defined.

The map $\Phi$ is injective\footnote{\textit{Proof.} Let $\alpha$ and $\beta$
be two elements of $\mathbf{I}\times S_{n}$ such that $\Phi\left(
\alpha\right)  =\Phi\left(  \beta\right)  $. We shall show that $\alpha=\beta
$.
\par
Let $\gamma$ be the element $\Phi\left(  \alpha\right)  =\Phi\left(
\beta\right)  $ of $\mathbf{E}$. Thus, $\gamma=\Phi\left(  \alpha\right)
=\Phi\left(  \beta\right)  $.
\par
Write $\alpha\in\mathbf{I}\times S_{n}$ in the form $\alpha=\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\pi\right)  $ for some $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\pi\in S_{n}$.
\par
Write $\beta\in\mathbf{I}\times S_{n}$ in the form $\beta=\left(  \left(
h_{1},h_{2},\ldots,h_{n}\right)  ,\tau\right)  $ for some $\left(  h_{1}%
,h_{2},\ldots,h_{n}\right)  \in\mathbf{I}$ and $\tau\in S_{n}$.
\par
We have%
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ of $\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $g_{1}<g_{2}<\cdots<g_{n}$.
\par
We have%
\[
\left(  h_{1},h_{2},\ldots,h_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  h_{1},h_{2},\ldots,h_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ of $\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  h_{1}%
,h_{2},\ldots,h_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $h_{1}<h_{2}<\cdots<h_{n}$.
\par
We have $\gamma\in\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots
,k_{n}\text{ are distinct}\right\}  $. In other words, we can write $\gamma$
in the form $\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ for some
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that
the integers $k_{1},k_{2},\ldots,k_{n}$ are distinct. Consider this $\left(
k_{1},k_{2},\ldots,k_{n}\right)  $.
\par
Applying the map $\Phi$ to both sides of the equality $\alpha=\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\pi\right)  $, we obtain%
\[
\Phi\left(  \alpha\right)  =\Phi\left(  \left(  g_{1},g_{2},\ldots
,g_{n}\right)  ,\pi\right)  =\left(  g_{\pi\left(  1\right)  },g_{\pi\left(
2\right)  },\ldots,g_{\pi\left(  n\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right)  .
\]
Thus, $\left(  g_{\pi\left(  1\right)  },g_{\pi\left(  2\right)  }%
,\ldots,g_{\pi\left(  n\right)  }\right)  =\Phi\left(  \alpha\right)
=\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $. In other words, every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{equation}
g_{\pi\left(  i\right)  }=k_{i}. \label{pf.lem.cauchy-binet.EI.2.pf.1}%
\end{equation}
Thus, every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align}
g_{j}  &  =g_{\pi\left(  \pi^{-1}\left(  j\right)  \right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=\pi\left(  \pi^{-1}\left(
j\right)  \right)  \right) \nonumber\\
&  =k_{\pi^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.1}), applied to }i=\pi^{-1}\left(  j\right)
\right)  . \label{pf.lem.cauchy-binet.EI.2.pf.1a}%
\end{align}
\par
We have $g_{1}<g_{2}<\cdots<g_{n}$. This rewrites as
\[
k_{\pi^{-1}\left(  1\right)  }<k_{\pi^{-1}\left(  2\right)  }<\cdots
<k_{\pi^{-1}\left(  n\right)  }%
\]
(because every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies $g_{j}%
=k_{\pi^{-1}\left(  j\right)  }$).
\par
Applying the map $\Phi$ to both sides of the equality $\beta=\left(  \left(
h_{1},h_{2},\ldots,h_{n}\right)  ,\tau\right)  $, we obtain%
\[
\Phi\left(  \beta\right)  =\Phi\left(  \left(  h_{1},h_{2},\ldots
,h_{n}\right)  ,\tau\right)  =\left(  h_{\tau\left(  1\right)  }%
,h_{\tau\left(  2\right)  },\ldots,h_{\tau\left(  n\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right)  .
\]
Thus, $\left(  h_{\tau\left(  1\right)  },h_{\tau\left(  2\right)  }%
,\ldots,h_{\tau\left(  n\right)  }\right)  =\Phi\left(  \beta\right)
=\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $. In other words, every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{equation}
h_{\tau\left(  i\right)  }=k_{i}. \label{pf.lem.cauchy-binet.EI.2.pf.2}%
\end{equation}
Thus, every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align}
h_{j}  &  =h_{\tau\left(  \tau^{-1}\left(  j\right)  \right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=\tau\left(  \tau^{-1}\left(
j\right)  \right)  \right) \nonumber\\
&  =k_{\tau^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.2}), applied to }i=\pi^{-1}\left(  j\right)
\right)  . \label{pf.lem.cauchy-binet.EI.2.pf.2a}%
\end{align}
\par
We have $h_{1}<h_{2}<\cdots<h_{n}$. This rewrites as%
\[
k_{\tau^{-1}\left(  1\right)  }<k_{\tau^{-1}\left(  2\right)  }<\cdots
<k_{\tau^{-1}\left(  n\right)  }%
\]
(because every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies $h_{j}%
=k_{\tau^{-1}\left(  j\right)  }$).
\par
Proposition \ref{prop.sorting} \textbf{(c)} (applied to $a_{i}=k_{i}$) yields
that there is a \textbf{unique} permutation $\sigma\in S_{n}$ such that
$k_{\sigma\left(  1\right)  }<k_{\sigma\left(  2\right)  }<\cdots
<k_{\sigma\left(  n\right)  }$ (since the integers $k_{1},k_{2},\ldots,k_{n}$
are distinct). In particular, there exists \textbf{at most one} such
permutation. In other words, if $\sigma_{1}$ and $\sigma_{2}$ are two
permutations $\sigma\in S_{n}$ satisfying $k_{\sigma\left(  1\right)
}<k_{\sigma\left(  2\right)  }<\cdots<k_{\sigma\left(  n\right)  }$, then%
\begin{equation}
\sigma_{1}=\sigma_{2}. \label{pf.lem.cauchy-binet.EI.2.pf.4}%
\end{equation}
\par
Now, $\pi^{-1}$ is a permutation $\sigma\in S_{n}$ satisfying $k_{\sigma
\left(  1\right)  }<k_{\sigma\left(  2\right)  }<\cdots<k_{\sigma\left(
n\right)  }$ (since $k_{\pi^{-1}\left(  1\right)  }<k_{\pi^{-1}\left(
2\right)  }<\cdots<k_{\pi^{-1}\left(  n\right)  }$). Also, $\tau^{-1}$ is a
permutation $\sigma\in S_{n}$ satisfying $k_{\sigma\left(  1\right)
}<k_{\sigma\left(  2\right)  }<\cdots<k_{\sigma\left(  n\right)  }$ (since
$k_{\tau^{-1}\left(  1\right)  }<k_{\tau^{-1}\left(  2\right)  }%
<\cdots<k_{\tau^{-1}\left(  n\right)  }$). Hence, we can apply
(\ref{pf.lem.cauchy-binet.EI.2.pf.4}) to $\sigma_{1}=\pi^{-1}$ and $\sigma
_{2}=\tau^{-1}$. As a result, we obtain $\pi^{-1}=\tau^{-1}$. Hence, $\pi
=\tau$. Now, every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align*}
g_{j}  &  =k_{\pi^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.1a})}\right) \\
&  =k_{\tau^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\pi=\tau\right) \\
&  =h_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.2a})}\right)  .
\end{align*}
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  =\left(  h_{1}%
,h_{2},\ldots,h_{n}\right)  $. Thus,
\[
\alpha=\left(  \underbrace{\left(  g_{1},g_{2},\ldots,g_{n}\right)
}_{=\left(  h_{1},h_{2},\ldots,h_{n}\right)  },\underbrace{\pi}_{=\tau
}\right)  =\left(  \left(  h_{1},h_{2},\ldots,h_{n}\right)  ,\tau\right)
=\beta.
\]
\par
Now, let us forget that we fixed $\alpha$ and $\beta$. We thus have shown that
if $\alpha$ and $\beta$ are two elements of $\mathbf{I}\times S_{n}$ such that
$\Phi\left(  \alpha\right)  =\Phi\left(  \beta\right)  $, then $\alpha=\beta$.
In other words, the map $\Phi$ is injective, qed.} and
surjective\footnote{\textit{Proof.} Let $\gamma\in\mathbf{E}$. We shall prove
that $\gamma\in\Phi\left(  \mathbf{I}\times S_{n}\right)  $.
\par
We have $\gamma\in\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots
,k_{n}\text{ are distinct}\right\}  $. In other words, we can write $\gamma$
in the form $\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ for some
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that
the integers $k_{1},k_{2},\ldots,k_{n}$ are distinct. Let us denote this
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ by $\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)  $. Thus, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is
an element of $\left[  m\right]  ^{n}$ such that the integers $a_{1}%
,a_{2},\ldots,a_{n}$ are distinct, and we have $\gamma=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $.
\par
Proposition \ref{prop.sorting} \textbf{(c)} yields that there is a
\textbf{unique} permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(
1\right)  }<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$
(since the integers $a_{1},a_{2},\ldots,a_{n}$ are distinct). In particular,
there exists \textbf{at least one} such permutation. Consider such a
permutation $\sigma$. Thus, $\sigma\in S_{n}$ and $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$.
\par
Now, $\left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  }%
,\ldots,a_{\sigma\left(  n\right)  }\right)  $ is an element of $\left[
m\right]  ^{n}$ satisfying $a_{\sigma\left(  1\right)  }<a_{\sigma\left(
2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. In other words, $\left(
a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  },\ldots
,a_{\sigma\left(  n\right)  }\right)  $ is an element $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  $ of $\left[  m\right]  ^{n}$ satisfying
$k_{1}<k_{2}<\cdots<k_{n}$. In other words,
\[
\left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  }%
,\ldots,a_{\sigma\left(  n\right)  }\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  =\mathbf{I}.
\]
Hence, $\Phi\left(  \left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(
2\right)  },\ldots,a_{\sigma\left(  n\right)  }\right)  ,\sigma^{-1}\right)  $
is well-defined. The definition of $\Phi\left(  \left(  a_{\sigma\left(
1\right)  },a_{\sigma\left(  2\right)  },\ldots,a_{\sigma\left(  n\right)
}\right)  ,\sigma^{-1}\right)  $ yields%
\begin{align*}
&  \Phi\left(  \left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)
},\ldots,a_{\sigma\left(  n\right)  }\right)  ,\sigma^{-1}\right) \\
&  =\left(  a_{\sigma\left(  \sigma^{-1}\left(  1\right)  \right)  }%
,a_{\sigma\left(  \sigma^{-1}\left(  2\right)  \right)  },\ldots
,a_{\sigma\left(  \sigma^{-1}\left(  n\right)  \right)  }\right)  =\left(
a_{1},a_{2},\ldots,a_{n}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{\sigma\left(  \sigma
^{-1}\left(  i\right)  \right)  }=a_{i}\text{ for every }i\in\left\{
1,2,\ldots,n\right\}  \right) \\
&  =\gamma.
\end{align*}
Thus, $\gamma=\Phi\left(  \underbrace{\left(  a_{\sigma\left(  1\right)
},a_{\sigma\left(  2\right)  },\ldots,a_{\sigma\left(  n\right)  }\right)
}_{\in\mathbf{I}},\underbrace{\sigma^{-1}}_{\in S_{n}}\right)  \in\Phi\left(
\mathbf{I}\times S_{n}\right)  $.
\par
Now, let us forget that we fixed $\gamma$. We thus have shown that $\gamma
\in\Phi\left(  \mathbf{I}\times S_{n}\right)  $ for every $\gamma\in
\mathbf{E}$. In other words, $\mathbf{E}\subseteq\Phi\left(  \mathbf{I}\times
S_{n}\right)  $. In other words, the map $\Phi$ is surjective. Qed.}. Hence,
the map $\Phi$ is bijective. In other words, the map $\Phi$ is a bijection. In
other words, the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is a bijection (because the map $\Phi$ is the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
). This concludes the proof of Lemma \ref{lem.cauchy-binet.EI}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.sorting.nmu}}

Before we start solving Exercise \ref{exe.sorting.nmu}, let us isolate a
useful fact that was proven in our above proof of Proposition
\ref{prop.sorting} \textbf{(b)}:

\begin{lemma}
\label{lem.sorting.nmu.uniprop}Let $n\in\mathbb{N}$. Let $a_{1},a_{2}%
,\ldots,a_{n}$ be $n$ integers. Let $\sigma\in S_{n}$ be such that
$a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq
a_{\sigma\left(  n\right)  }$. Let $i\in\left\{  1,2,\ldots,n\right\}  $.
Then,%
\begin{equation}
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  \label{eq.lem.sorting.nmu.uniprop.claim}%
\end{equation}
(and, in particular, the right hand side of
(\ref{eq.lem.sorting.nmu.uniprop.claim}) is well-defined).
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sorting.nmu.uniprop}.]The equality
(\ref{eq.lem.sorting.nmu.uniprop.claim}) is precisely the equality
(\ref{pf.prop.sorting.bclaim}) that was proven during our above proof of
Proposition \ref{prop.sorting} \textbf{(b)}. Hence, we do not need to prove it
again. Thus, Lemma \ref{lem.sorting.nmu.uniprop} is proven.
\end{proof}

\begin{proof}
[First solution to Exercise \ref{exe.sorting.nmu}.]Let $i\in\left\{
1,2,\ldots,m\right\}  $. We must prove that $a_{\sigma\left(  i\right)  }\leq
b_{\tau\left(  i\right)  }$.

We have $n\geq m$, so that $m\leq n$. Now, $i\in\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $ (since $m\leq n$). Hence,
$\sigma\left(  i\right)  $ is well-defined (since $\sigma$ is a map $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $ (since
$\sigma\in S_{n}$)).

Define a subset $X$ of $\mathbb{Z}$ by%
\begin{equation}
X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  .
\label{sol.sorting.nmu.X=}%
\end{equation}
Define a subset $Y$ of $\mathbb{Z}$ by%
\begin{equation}
Y=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy }b_{j}\leq x\right\}  .
\label{sol.sorting.nmu.Y=}%
\end{equation}


Lemma \ref{lem.sorting.nmu.uniprop} yields that
\begin{equation}
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  \label{sol.sorting.nmu.uniprop1}%
\end{equation}
(and, in particular, the right hand side of (\ref{sol.sorting.nmu.uniprop1})
is well-defined). Thus,%
\begin{align}
a_{\sigma\left(  i\right)  }  &  =\min\underbrace{\left\{  x\in\mathbb{Z}%
\ \mid\ \text{at least }i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}
\text{ satisfy }a_{j}\leq x\right\}  }_{\substack{=X\\\text{(by
(\ref{sol.sorting.nmu.X=}))}}}\nonumber\\
&  =\min X. \label{sol.sorting.nmu.uniprop1'}%
\end{align}


But $\tau\in S_{m}$ satisfies $b_{\tau\left(  1\right)  }\leq b_{\tau\left(
2\right)  }\leq\cdots\leq b_{\tau\left(  m\right)  }$, and we have
$i\in\left\{  1,2,\ldots,m\right\}  $. Thus, Lemma
\ref{lem.sorting.nmu.uniprop} (applied to $m$, $b_{k}$ and $\tau$ instead of
$n$, $a_{k}$ and $\sigma$) yields that%
\begin{equation}
b_{\tau\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy }%
b_{j}\leq x\right\}  \label{sol.sorting.nmu.uniprop2}%
\end{equation}
(and, in particular, the right hand side of (\ref{sol.sorting.nmu.uniprop2})
is well-defined). Thus,%
\begin{align}
b_{\tau\left(  i\right)  }  &  =\min\underbrace{\left\{  x\in\mathbb{Z}%
\ \mid\ \text{at least }i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}
\text{ satisfy }b_{j}\leq x\right\}  }_{\substack{=Y\\\text{(by
(\ref{sol.sorting.nmu.Y=}))}}}\nonumber\\
&  =\min Y. \label{sol.sorting.nmu.uniprop2'}%
\end{align}


\begin{vershort}
But clearly, $\min Y\in Y$. Hence,
\[
b_{\tau\left(  i\right)  }=\min Y\in Y=\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy
}b_{j}\leq x\right\}  .
\]
In other words, $b_{\tau\left(  i\right)  }$ is an element of $\mathbb{Z}$
such that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $ satisfy
$b_{j}\leq b_{\tau\left(  i\right)  }$.
\end{vershort}

\begin{verlong}
But if $S$ is a subset of $\mathbb{Z}$ for which $\min S$ is well-defined,
then $\min S\in S$ (since the minimum of a set must lie inside the set).
Applying this to $S=Y$, we obtain $\min Y\in Y$. Now,
(\ref{sol.sorting.nmu.uniprop2'}) becomes%
\[
b_{\tau\left(  i\right)  }=\min Y\in Y=\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy
}b_{j}\leq x\right\}  .
\]
In other words, $b_{\tau\left(  i\right)  }$ is an element $x$ of $\mathbb{Z}$
such that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $ satisfy
$b_{j}\leq x$. In other words, $b_{\tau\left(  i\right)  }$ is an element of
$\mathbb{Z}$ such that at least $i$ elements $j\in\left\{  1,2,\ldots
,m\right\}  $ satisfy $b_{j}\leq b_{\tau\left(  i\right)  }$.
\end{verlong}

\begin{vershort}
We now know that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $
satisfy $b_{j}\leq b_{\tau\left(  i\right)  }$. Each of these $i$ elements $j$
must also be an element of $\left\{  1,2,\ldots,n\right\}  $ (since
$j\in\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $)
which satisfies $a_{j}\leq b_{\tau\left(  i\right)  }$ (because
(\ref{eq.exe.sorting.nmu.ass}) (applied to $j$ instead of $i$) shows that
$a_{j}\leq b_{j}\leq b_{\tau\left(  i\right)  }$). Thus, at least $i$ elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq b_{\tau\left(
i\right)  }$.
\end{vershort}

\begin{verlong}
We now know that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $
satisfy $b_{j}\leq b_{\tau\left(  i\right)  }$. In other words,%
\[
\left\vert \left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \right\vert \geq i.
\]


But $\left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \subseteq\left\{  j\in\left\{
1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $g\in\left\{  j\in\left\{
1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq b_{\tau\left(  i\right)  }\right\}  $.
We shall prove that $g\in\left\{  j\in\left\{  1,2,\ldots,m\right\}
\ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}  $.
\par
We have $g\in\left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  $. In other words, $g$ is an element $j$
of $\left\{  1,2,\ldots,m\right\}  $ satisfying $b_{j}\leq b_{\tau\left(
i\right)  }$. In other words, $g$ is an element of $\left\{  1,2,\ldots
,m\right\}  $ satisfying $b_{g}\leq b_{\tau\left(  i\right)  }$. Now,
$g\in\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $.
Also, (\ref{eq.exe.sorting.nmu.ass}) (applied to $g$ instead of $i$) shows
that $a_{g}\leq b_{g}\leq b_{\tau\left(  i\right)  }$. Hence, $g$ is an
element of $\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{g}\leq
b_{\tau\left(  i\right)  }$. In other words, $g$ is an element $j$ of
$\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{j}\leq b_{\tau\left(
i\right)  }$. In other words, $g\in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}  $.
\par
Now, forget that we fixed $g$. We thus have shown that $g\in\left\{
j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ a_{j}\leq b_{\tau\left(  i\right)
}\right\}  $ for each $g\in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ b_{j}\leq b_{\tau\left(  i\right)  }\right\}  $. In other words,
$\left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq b_{\tau\left(
i\right)  }\right\}  \subseteq\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}  $. Qed.}. Hence,%
\[
\left\vert \left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \right\vert \leq\left\vert \left\{
j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq b_{\tau\left(  i\right)
}\right\}  \right\vert .
\]
Thus,%
\begin{align*}
&  \left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \right\vert \\
&  \geq\left\vert \left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid
\ b_{j}\leq b_{\tau\left(  i\right)  }\right\}  \right\vert \geq i.
\end{align*}
In other words, at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfy $a_{j}\leq b_{\tau\left(  i\right)  }$.
\end{verlong}

Now, we know that $b_{\tau\left(  i\right)  }$ is an element of $\mathbb{Z}$
such that at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy
$a_{j}\leq b_{\tau\left(  i\right)  }$. In other words, $b_{\tau\left(
i\right)  }$ is an element $x$ of $\mathbb{Z}$ such that at least $i$ elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq x$. In other words,%
\[
b_{\tau\left(  i\right)  }\in\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  .
\]
In light of (\ref{sol.sorting.nmu.X=}), this rewrites as $b_{\tau\left(
i\right)  }\in X$.

\begin{vershort}
But every $s\in X$ satisfies $\min X\leq s$ (since the minimum of a set is
$\leq$ to each element of this set). Applying this to $s=b_{\tau\left(
i\right)  }$, we obtain $\min X\leq b_{\tau\left(  i\right)  }$ (since
$b_{\tau\left(  i\right)  }\in X$). Thus, $b_{\tau\left(  i\right)  }\geq\min
X=a_{\sigma\left(  i\right)  }$ (by (\ref{sol.sorting.nmu.uniprop1'})). This
solves Exercise \ref{exe.sorting.nmu}.
\end{vershort}

\begin{verlong}
But if $S$ is a subset of $\mathbb{Z}$ for which $\min S$ is well-defined,
then every $s\in S$ satisfies $\min S\leq s$ (since the minimum of a set is
$\leq$ to each element of this set). Applying this to $S=X$, we conclude that
every $s\in X$ satisfies $\min X\leq s$. Applying this to $s=b_{\tau\left(
i\right)  }$, we obtain $\min X\leq b_{\tau\left(  i\right)  }$ (since
$b_{\tau\left(  i\right)  }\in X$). Thus, $b_{\tau\left(  i\right)  }\geq\min
X=a_{\sigma\left(  i\right)  }$ (by (\ref{sol.sorting.nmu.uniprop1'})). In
other words, $a_{\sigma\left(  i\right)  }\leq b_{\tau\left(  i\right)  }$.
This solves Exercise \ref{exe.sorting.nmu}.
\end{verlong}
\end{proof}

In order to give a second solution to Exercise \ref{exe.sorting.nmu}, let us
prove a lemma:

\begin{lemma}
\label{lem.sorting.nmu.stronger}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be
such that $n\geq m$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$ integers. Let
$b_{1},b_{2},\ldots,b_{m}$ be $m$ integers. Assume that%
\begin{equation}
a_{i}\leq b_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,m\right\}  . \label{eq.lem.sorting.nmu.stronger.ass1}%
\end{equation}


Let $\sigma\in S_{n}$ and $\tau\in S_{m}$. Let $i\in\left\{  1,2,\ldots
,m\right\}  $. Assume that%
\begin{equation}
a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }v\in\left\{  i,i+1,\ldots,n\right\}  .
\label{eq.lem.sorting.nmu.stronger.ass2}%
\end{equation}
Furthermore, assume that%
\begin{equation}
b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  1,2,\ldots,i\right\}  .
\label{eq.lem.sorting.nmu.stronger.ass3}%
\end{equation}
Then, $a_{\sigma\left(  i\right)  }\leq b_{\tau\left(  i\right)  }$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sorting.nmu.stronger}.]We have $i\in\left\{
1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $ (since $m\leq
n$).

We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation of $\left\{
1,2,\ldots,n\right\}  $, hence an injective map. Thus, the $n$ integers
$\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  $ are distinct. In particular, the $n-i+1$ integers $\sigma\left(
i\right)  ,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  $ are distinct.

Also, $\tau\in S_{m}$. Therefore, $\tau$ is a permutation of $\left\{
1,2,\ldots,m\right\}  $, hence an injective map. Therefore, the $m$ integers
$\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  m\right)  $
are distinct. In particular, the $i$ integers $\tau\left(  1\right)
,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  $ are distinct.

If $A$ and $B$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$, then%
\begin{equation}
A\cap B\neq\varnothing\label{pf.lem.sorting.nmu.stronger.short.AcutB}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sorting.nmu.stronger.short.AcutB}):}
Let $A$ and $B$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$. We must prove that
$A\cap B\neq\varnothing$.
\par
Indeed, assume the contrary. Thus, $A\cap B=\varnothing$. We know that $A$ and
$B$ are subsets of $\left\{  1,2,\ldots,n\right\}  $. Hence, $A\cup B$ is a
subset of $\left\{  1,2,\ldots,n\right\}  $. Thus, $\left\vert A\cup
B\right\vert \leq\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert =n$.
But the sets $A$ and $B$ are disjoint (since $A\cap B=\varnothing$), and thus
we have $\left\vert A\cup B\right\vert =\left\vert A\right\vert +\left\vert
B\right\vert $ (since the size of the union of two disjoint sets is the sum of
their sizes). Thus, $\left\vert A\right\vert +\left\vert B\right\vert
=\left\vert A\cup B\right\vert \leq n$. This contradicts $\left\vert
A\right\vert +\left\vert B\right\vert >n$. This contradiction proves that our
assumption was wrong. Hence, $A\cap B\neq\varnothing$ is proven. This proves
(\ref{pf.lem.sorting.nmu.stronger.short.AcutB}).}.

Now, let $A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $ and $B=\left\{  \tau\left(
1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \right\}  $.

Clearly,
\begin{equation}
A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)  ,\ldots
,\sigma\left(  n\right)  \right\}  \subseteq\left\{  1,2,\ldots,n\right\}
\label{pf.lem.sorting.nmu.stronger.short.subset1}%
\end{equation}
(since $\sigma$ is a map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $) and $B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $ (since $\tau$ is a map $\left\{  1,2,\ldots,m\right\}
\rightarrow\left\{  1,2,\ldots,m\right\}  $). Thus,
\begin{equation}
B\subseteq\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  . \label{pf.lem.sorting.nmu.stronger.short.subset2}%
\end{equation}


Now, from (\ref{pf.lem.sorting.nmu.stronger.short.subset1}) and
(\ref{pf.lem.sorting.nmu.stronger.short.subset2}), we see that $A$ and $B$ are
two subsets of $\left\{  1,2,\ldots,n\right\}  $. These two subsets satisfy%
\begin{align*}
&  \left\vert \underbrace{A}_{=\left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  }\right\vert +\left\vert
\underbrace{B}_{=\left\{  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  i\right)  \right\}  }\right\vert \\
&  =\underbrace{\left\vert \left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  \right\vert
}_{\substack{=n-i+1\\\text{(since the }n-i+1\text{ integers}\\\sigma\left(
i\right)  ,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \text{
are distinct)}}}+\underbrace{\left\vert \left\{  \tau\left(  1\right)
,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \right\vert
}_{\substack{=i\\\text{(since the }i\text{ integers}\\\tau\left(  1\right)
,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \text{ are distinct)}}}\\
&  =n-i+1+i=n+1>n.
\end{align*}
Hence, (\ref{pf.lem.sorting.nmu.stronger.short.AcutB}) shows that $A\cap
B\neq\varnothing$. In other words, there exists a $g\in A\cap B$. Consider
this $g$.

We have $g\in A\cap B\subseteq B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $. Hence, $b_{g}$ is well-defined. Also, $g\in\left\{
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)
\right\}  $. In other words, there exists a $u\in\left\{  1,2,\ldots
,i\right\}  $ satisfying $g=\tau\left(  u\right)  $. Consider this $u$. From
(\ref{eq.lem.sorting.nmu.stronger.ass3}), we obtain $b_{\tau\left(  u\right)
}\leq b_{\tau\left(  i\right)  }$. But $g=\tau\left(  u\right)  $ shows that
$b_{g}=b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }$.

On the other hand, $g\in A\cap B\subseteq A=\left\{  \sigma\left(  i\right)
,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{g}$ is well-defined.
Also, $g\in\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $. In other words, there exists a
$v\in\left\{  i,i+1,\ldots,n\right\}  $ such that $g=\sigma\left(  v\right)
$. Consider this $v$. From (\ref{eq.lem.sorting.nmu.stronger.ass2}), we obtain
$a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }=a_{g}$ (since
$\sigma\left(  v\right)  =g$).

But (\ref{eq.lem.sorting.nmu.stronger.ass1}) (applied to $g$ instead of $i$)
shows that $a_{g}\leq b_{g}$ (since $g\in\left\{  1,2,\ldots,m\right\}  $).
Hence, $a_{\sigma\left(  i\right)  }\leq a_{g}\leq b_{g}\leq b_{\tau\left(
i\right)  }$. This proves Lemma \ref{lem.sorting.nmu.stronger}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sorting.nmu.stronger}.]We have $\sigma\in S_{n}$. In
other words, $\sigma$ belongs to the set $S_{n}$. In other words, $\sigma$
belongs to the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of the set
$\left\{  1,2,\ldots,n\right\}  $). In other words, $\sigma$ is a permutation
of the set $\left\{  1,2,\ldots,n\right\}  $. In other words, $\sigma$ is a
bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. The map $\sigma$ is thus bijective, hence both
surjective and injective.

We have $\tau\in S_{m}$. In other words, $\tau$ belongs to the set $S_{m}$. In
other words, $\tau$ belongs to the set of all permutations of the set
$\left\{  1,2,\ldots,m\right\}  $ (since $S_{m}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,m\right\}  $). In other words,
$\tau$ is a permutation of the set $\left\{  1,2,\ldots,m\right\}  $. In other
words, $\tau$ is a bijective map $\left\{  1,2,\ldots,m\right\}
\rightarrow\left\{  1,2,\ldots,m\right\}  $. The map $\tau$ is thus bijective,
hence both surjective and injective.

We have $n\geq m$, thus $m\leq n$, hence $\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Now, $i\in\left\{  1,2,\ldots
,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $; thus, $\sigma\left(
i\right)  $ is well-defined.

If $A$ and $B$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$, then%
\begin{equation}
A\cap B\neq\varnothing\label{pf.lem.sorting.nmu.stronger.AcutB}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sorting.nmu.stronger.AcutB}):} Let $A$
and $B$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$. We must prove that
$A\cap B\neq\varnothing$.
\par
Indeed, assume the contrary. Thus, $A\cap B=\varnothing$. We know that $A$ and
$B$ are subsets of $\left\{  1,2,\ldots,n\right\}  $. Hence, $A\cup B$ is a
subset of $\left\{  1,2,\ldots,n\right\}  $. In other words, $A\cup
B\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, $\left\vert A\cup
B\right\vert \leq\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert =n$.
But the sets $A$ and $B$ are disjoint (since $A\cap B=\varnothing$), and thus
we have $\left\vert A\cup B\right\vert =\left\vert A\right\vert +\left\vert
B\right\vert $ (since the size of the union of two disjoint sets is the sum of
their sizes). Thus, $\left\vert A\right\vert +\left\vert B\right\vert
=\left\vert A\cup B\right\vert \leq n$. This contradicts $\left\vert
A\right\vert +\left\vert B\right\vert >n$. This contradiction proves that our
assumption was wrong. Hence, $A\cap B\neq\varnothing$ is proven. This proves
(\ref{pf.lem.sorting.nmu.stronger.AcutB}).}.

Now, let $A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $ and $B=\left\{  \tau\left(
1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \right\}  $.

Clearly,
\begin{equation}
A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)  ,\ldots
,\sigma\left(  n\right)  \right\}  \subseteq\left\{  1,2,\ldots,n\right\}
\label{pf.lem.sorting.nmu.stronger.subset1}%
\end{equation}
(since $\sigma$ is a map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $) and $B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $ (since $\tau$ is a map $\left\{  1,2,\ldots,m\right\}
\rightarrow\left\{  1,2,\ldots,m\right\}  $). Thus,
\begin{equation}
B\subseteq\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  . \label{pf.lem.sorting.nmu.stronger.subset2}%
\end{equation}


The $n-i+1$ integers $\sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  $ are pairwise
distinct\footnote{\textit{Proof.} Assume the contrary. Thus, the $n-i+1$
integers $\sigma\left(  i\right)  ,\sigma\left(  i+1\right)  ,\ldots
,\sigma\left(  n\right)  $ are \textbf{not} pairwise distinct. In other words,
two of these integers are equal. In other words, there exist two distinct
elements $u$ and $v$ of $\left\{  i,i+1,\ldots,n\right\}  $ such that
$\sigma\left(  u\right)  =\sigma\left(  v\right)  $. Consider these $u$ and
$v$.
\par
From $\sigma\left(  u\right)  =\sigma\left(  v\right)  $, we obtain $u=v$
(since the map $\sigma$ is injective). This contradicts the assumption that
$u$ and $v$ are distinct. This contradiction shows that our assumption was
wrong, qed.}. Hence, $\left\vert \left\{  \sigma\left(  i\right)
,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}
\right\vert =n-i+1$.

The $i$ integers $\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots
,\tau\left(  i\right)  $ are pairwise distinct\footnote{\textit{Proof.} Assume
the contrary. Thus, the $i$ integers $\tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  $ are \textbf{not} pairwise distinct.
In other words, two of these integers are equal. In other words, there exist
two distinct elements $u$ and $v$ of $\left\{  1,2,\ldots,i\right\}  $ such
that $\tau\left(  u\right)  =\tau\left(  v\right)  $. Consider these $u$ and
$v$.
\par
From $\tau\left(  u\right)  =\tau\left(  v\right)  $, we obtain $u=v$ (since
the map $\tau$ is injective). This contradicts the assumption that $u$ and $v$
are distinct. This contradiction shows that our assumption was wrong, qed.}.
Hence, $\left\vert \left\{  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  i\right)  \right\}  \right\vert =i$.

Now, from (\ref{pf.lem.sorting.nmu.stronger.subset1}) and
(\ref{pf.lem.sorting.nmu.stronger.subset2}), we see that $A$ and $B$ are two
subsets of $\left\{  1,2,\ldots,n\right\}  $. These two subsets satisfy%
\begin{align*}
&  \left\vert \underbrace{A}_{=\left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  }\right\vert +\left\vert
\underbrace{B}_{=\left\{  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  i\right)  \right\}  }\right\vert \\
&  =\underbrace{\left\vert \left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  \right\vert }%
_{=n-i+1}+\underbrace{\left\vert \left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \right\vert }_{=i}\\
&  =n-i+1+i=n+1>n.
\end{align*}
Hence, (\ref{pf.lem.sorting.nmu.stronger.AcutB}) shows that $A\cap
B\neq\varnothing$. In other words, the set $A\cap B$ is nonempty. In other
words, the set $A\cap B$ has at least one element. In other words, there
exists a $g\in A\cap B$. Consider this $g$.

We have $g\in A\cap B\subseteq B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $. Hence, $b_{g}$ is well-defined. Also, $g\in\left\{
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)
\right\}  $. In other words, there exists a $u\in\left\{  1,2,\ldots
,i\right\}  $ satisfying $g=\tau\left(  u\right)  $. Consider this $u$. From
(\ref{eq.lem.sorting.nmu.stronger.ass3}), we obtain $b_{\tau\left(  u\right)
}\leq b_{\tau\left(  i\right)  }$. But $g=\tau\left(  u\right)  $ shows that
$b_{g}=b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }$.

On the other hand, $g\in A\cap B\subseteq A=\left\{  \sigma\left(  i\right)
,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{g}$ is well-defined.
Also, $g\in\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $. In other words, there exists a
$v\in\left\{  i,i+1,\ldots,n\right\}  $ such that $g=\sigma\left(  v\right)
$. Consider this $v$. From (\ref{eq.lem.sorting.nmu.stronger.ass2}), we obtain
$a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }=a_{g}$ (since
$\sigma\left(  v\right)  =g$).

But (\ref{eq.lem.sorting.nmu.stronger.ass1}) (applied to $g$ instead of $i$)
shows that $a_{g}\leq b_{g}$ (since $g\in\left\{  1,2,\ldots,m\right\}  $).
Hence, $a_{\sigma\left(  i\right)  }\leq a_{g}\leq b_{g}\leq b_{\tau\left(
i\right)  }$. This proves Lemma \ref{lem.sorting.nmu.stronger}.
\end{proof}
\end{verlong}

\begin{proof}
[Second solution to Exercise \ref{exe.sorting.nmu}.]Let $i\in\left\{
1,2,\ldots,m\right\}  $. We must prove that $a_{\sigma\left(  i\right)  }\leq
b_{\tau\left(  i\right)  }$.

We have $a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }%
\leq\cdots\leq a_{\sigma\left(  n\right)  }$. In other words, for every
$u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}
$ satisfying $u\leq v$, we have%
\begin{equation}
a_{\sigma\left(  u\right)  }\leq a_{\sigma\left(  v\right)  }.
\label{sol.sorting.nmu.sol2.1}%
\end{equation}
Also, we have $b_{\tau\left(  1\right)  }\leq b_{\tau\left(  2\right)  }%
\leq\cdots\leq b_{\tau\left(  m\right)  }$. In other words, for every
$u\in\left\{  1,2,\ldots,m\right\}  $ and $v\in\left\{  1,2,\ldots,m\right\}
$ satisfying $u\leq v$, we have%
\begin{equation}
b_{\tau\left(  u\right)  }\leq b_{\tau\left(  v\right)  }.
\label{sol.sorting.nmu.sol2.2}%
\end{equation}


Now,%
\[
a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }v\in\left\{  i,i+1,\ldots,n\right\}
\]
\footnote{\textit{Proof.} Let $v\in\left\{  i,i+1,\ldots,n\right\}  $. Then,
$v\geq i$, so that $i\leq v$. Also, $i\in\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $ (since $m\leq n$ (since $n\geq m$))
and $v\in\left\{  i,i+1,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $ (since $i\geq1$ (since $i\in\left\{  1,2,\ldots,m\right\}  $)).
Hence, (\ref{sol.sorting.nmu.sol2.1}) (applied to $u=i$) yields $a_{\sigma
\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }$. Qed.}. Furthermore,%
\[
b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  1,2,\ldots,i\right\}
\]
\footnote{\textit{Proof.} Let $u\in\left\{  1,2,\ldots,i\right\}  $. Then,
$u\leq i$. Also, $u\in\left\{  1,2,\ldots,i\right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $ (since $i\leq m$ (since $i\in\left\{  1,2,\ldots
,m\right\}  $)) and $i\in\left\{  1,2,\ldots,m\right\}  $. Hence,
(\ref{sol.sorting.nmu.sol2.2}) (applied to $v=i$) yields $b_{\tau\left(
u\right)  }\leq b_{\tau\left(  i\right)  }$. Qed.}. Hence, Lemma
\ref{lem.sorting.nmu.stronger} shows that $a_{\sigma\left(  i\right)  }\leq
b_{\tau\left(  i\right)  }$. This solves Exercise \ref{exe.sorting.nmu} again.
\end{proof}

\subsection{Solution to Exercise \ref{exe.vander-det.s1}}

\begin{proof}
[First solution to Exercise \ref{exe.vander-det.s1}.]Our solution will imitate
the above proof of Theorem \ref{thm.vander-det} \textbf{(a)} (but it will
involve some additional complications).

For every $u\in\left\{  0,1,\ldots,n\right\}  $ and $\left(  i,j\right)
\in\left\{  1,2,\ldots,u\right\}  ^{2}$, define $a_{i,j,u}\in\mathbb{K}$ by%
\[
a_{i,j,u}=%
\begin{cases}
x_{i}^{u-j}, & \text{if }j>1;\\
x_{i}^{u}, & \text{if }j=1
\end{cases}
.
\]


For every $u\in\left\{  1,2,\ldots,n\right\}  $, let $A_{u}$ be the $u\times
u$-matrix $\left(  a_{i,j,u}\right)  _{1\leq i\leq u,\ 1\leq j\leq u}$. Then,
our goal is to prove that $\det\left(  A_{n}\right)  =\left(  x_{1}%
+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
$ (because $A_{n}$ is precisely the matrix $\left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ which appears in the statement of
the exercise).

Now, let us show that%
\begin{equation}
\det\left(  A_{u}\right)  =\left(  x_{1}+x_{2}+\cdots+x_{u}\right)
\prod_{1\leq i<j\leq u}\left(  x_{i}-x_{j}\right)
\label{sol.vander-det.s1.goal}%
\end{equation}
for every $u\in\left\{  1,2,\ldots,n\right\}  $.

\textit{Proof of (\ref{sol.vander-det.s1.goal}):} We will prove
(\ref{sol.vander-det.s1.goal}) by induction over $u$:

\textit{Induction base:} The definition of $A_{1}$ yields $A_{1}=\left(
\begin{array}
[c]{c}%
x_{1}^{1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
x_{1}%
\end{array}
\right)  $ and thus $\det\left(  A_{1}\right)  =x_{1}$. Compared with%
\[
\underbrace{\left(  x_{1}+x_{2}+\cdots+x_{1}\right)  }_{=x_{1}}%
\underbrace{\prod_{1\leq i<j\leq1}\left(  x_{i}-x_{j}\right)  }_{=\left(
\text{empty product}\right)  =1}=x_{1},
\]
this yields $\det\left(  A_{1}\right)  =\left(  x_{1}+x_{2}+\cdots
+x_{1}\right)  \prod_{1\leq i<j\leq1}\left(  x_{i}-x_{j}\right)  $. In other
words, (\ref{sol.vander-det.s1.goal}) holds for $u=1$. The induction base is
thus complete.

\textit{Induction step:} Let $U\in\left\{  2,3,\ldots,n\right\}  $. Assume
that (\ref{sol.vander-det.s1.goal}) holds for $u=U-1$. We need to prove that
(\ref{sol.vander-det.s1.goal}) holds for $u=U$.

Recall that $A_{U}=\left(  a_{i,j,U}\right)  _{1\leq i\leq U,\ 1\leq j\leq U}$
(by the definition of $A_{U}$). The matrix $A_{U}$ looks as follows:%
\[
A_{U}=\left(
\begin{array}
[c]{cccccc}%
x_{1}^{U} & x_{1}^{U-2} & x_{1}^{U-3} & \cdots & x_{1} & 1\\
x_{2}^{U} & x_{2}^{U-2} & x_{2}^{U-3} & \cdots & x_{2} & 1\\
x_{3}^{U} & x_{3}^{U-2} & x_{3}^{U-3} & \cdots & x_{3} & 1\\
x_{4}^{U} & x_{4}^{U-2} & x_{4}^{U-3} & \cdots & x_{4} & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
x_{U}^{U} & x_{U}^{U-2} & x_{U}^{U-3} & \cdots & x_{U} & 1
\end{array}
\right)  .
\]


For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, define
$b_{i,j}\in\mathbb{K}$ by%
\[
b_{i,j}=%
\begin{cases}
x_{i}^{U}-x_{U}^{2}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }1<j<U;\\
1, & \text{if }j=U
\end{cases}
.
\]
Let $B$ be the $U\times U$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$. Here is how $B$ looks like:%
\[
B=\left(
\begin{array}
[c]{cccccc}%
x_{1}^{U}-x_{U}^{2}x_{1}^{U-2} & x_{1}^{U-2}-x_{U}x_{1}^{U-3} & x_{1}%
^{U-3}-x_{U}x_{1}^{U-4} & \cdots & x_{1}-x_{U} & 1\\
x_{2}^{U}-x_{U}^{2}x_{2}^{U-2} & x_{2}^{U-2}-x_{U}x_{2}^{U-3} & x_{2}%
^{U-3}-x_{U}x_{2}^{U-4} & \cdots & x_{2}-x_{U} & 1\\
x_{3}^{U}-x_{U}^{2}x_{3}^{U-2} & x_{3}^{U-2}-x_{U}x_{3}^{U-3} & x_{3}%
^{U-3}-x_{U}x_{3}^{U-4} & \cdots & x_{3}-x_{U} & 1\\
x_{4}^{U}-x_{U}^{2}x_{4}^{U-2} & x_{4}^{U-2}-x_{U}x_{4}^{U-3} & x_{4}%
^{U-3}-x_{U}x_{4}^{U-4} & \cdots & x_{4}-x_{U} & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2} & x_{U}^{U-2}-x_{U}x_{U}^{U-3} & x_{U}%
^{U-3}-x_{U}x_{U}^{U-4} & \cdots & x_{U}-x_{U} & 1
\end{array}
\right)  .
\]
We claim that $\det B=\det\left(  A_{U}\right)  $. Indeed, here are two ways
to prove this:

\textit{First proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Exercise
\ref{exe.ps4.6k} \textbf{(b)} shows that the determinant of a $U\times
U$-matrix does not change if we subtract a multiple of one of its columns from
another column. Now, let us do the following steps (in this order):

\begin{itemize}
\item subtract $x_{U}^{2}$ times the $2$-nd column of $A_{U}$ from the $1$-st column;

\item subtract $x_{U}$ times the $3$-rd column of the resulting matrix from
the $2$-nd column;

\item subtract $x_{U}$ times the $4$-th column of the resulting matrix from
the $3$-rd column;

\item and so on, all the way until we finally subtract $x_{U}$ times the
$U$-th column of the matrix from the $\left(  U-1\right)  $-st column.
\end{itemize}

Yes, you are reading this right: At the first step we subtract $x_{U}^{2}$
times (not $x_{U}$ times) the $2$-nd column from the $1$-st column; but at all
further steps, we subtract $x_{U}$ times a column from another. Having done
all this, the resulting matrix is $B$ (according to our definition of $B$).
Thus, $\det B=\det\left(  A_{U}\right)  $ (since our subtractions never change
the determinant). This proves $\det B=\det\left(  A_{U}\right)  $.

\textit{Second proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Here is
another way to prove that $\det B=\det\left(  A_{U}\right)  $, with some less handwaving.

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, we
define $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
-x_{U}^{2}, & \text{if }i=j+1\text{ and }j=1;\\
-x_{U}, & \text{if }i=j+1\text{ and }j>1;\\
0, & \text{otherwise}%
\end{cases}
.
\]
Let $C$ be the $U\times U$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$. Here is how $C$ looks like:%
\[
C=\left(
\begin{array}
[c]{cccccc}%
1 & 0 & 0 & \cdots & 0 & 0\\
-x_{U}^{2} & 1 & 0 & \cdots & 0 & 0\\
0 & -x_{U} & 1 & \cdots & 0 & 0\\
0 & 0 & -x_{U} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & -x_{U} & 1
\end{array}
\right)  ,
\]
where the only $-x_{U}^{2}$ is in the $\left(  2,1\right)  $-th cell.

The matrix $C$ is lower-triangular, and thus Exercise \ref{exe.ps4.3} shows
that its determinant is $\det C=\underbrace{c_{1,1}}_{=1}\underbrace{c_{2,2}%
}_{=1}\cdots\underbrace{c_{U,U}}_{=1}=1$.

On the other hand, it is easy to see that $B=A_{U}C$ (check this!). Thus,
Theorem \ref{thm.det(AB)} yields $\det B=\det\left(  A_{U}\right)
\cdot\underbrace{\det C}_{=1}=\det\left(  A_{U}\right)  $. So we have proven
$\det B=\det\left(  A_{U}\right)  $ again.

Next, we observe that for every $j\in\left\{  1,2,\ldots,U-1\right\}  $, we
have%
\begin{align*}
b_{U,j}  &  =%
\begin{cases}
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2}, & \text{if }j=1;\\
x_{U}^{U-j}-x_{U}x_{U}^{U-j-1}, & \text{if }1<j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,j}\right) \\
&  =%
\begin{cases}
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2}, & \text{if }j=1;\\
x_{U}^{U-j}-x_{U}x_{U}^{U-j-1}, & \text{if }1<j<U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<U\text{ (since }j\in\left\{
1,2,\ldots,U-1\right\}  \text{)}\right) \\
&  =%
\begin{cases}
x_{U}^{U}-x_{U}^{U}, & \text{if }j=1;\\
x_{U}^{U-j}-x_{U}^{U-j}, & \text{if }1<j<U
\end{cases}
=%
\begin{cases}
0, & \text{if }j=1;\\
0, & \text{if }1<j<U
\end{cases}
=0.
\end{align*}
Hence, Theorem \ref{thm.laplace.pre} (applied to $U$, $B$ and $b_{i,j}$
instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{equation}
\det B=b_{U,U}\cdot\det\left(  \left(  b_{i,j}\right)  _{1\leq i\leq
U-1,\ 1\leq j\leq U-1}\right)  . \label{sol.vander-det.s1.detB=prod}%
\end{equation}
Let $B^{\prime}$ denote the $\left(  U-1\right)  \times\left(  U-1\right)
$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$.

The definition of $b_{U,U}$ yields%
\begin{align*}
b_{U,U}  &  =%
\begin{cases}
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2}, & \text{if }U=1;\\
x_{U}^{U-U}-x_{U}x_{U}^{U-U-1}, & \text{if }1<U<U;\\
1, & \text{if }U=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,U}\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }U=U\right)  .
\end{align*}
Thus, (\ref{sol.vander-det.s1.detB=prod}) becomes%
\[
\det B=\underbrace{b_{U,U}}_{=1}\cdot\det\left(  \underbrace{\left(
b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}}_{=B^{\prime}}\right)
=\det\left(  B^{\prime}\right)  .
\]
Compared with $\det B=\det\left(  A_{U}\right)  $, this yields%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  .
\label{sol.vander-det.s1.detAU=detB'}%
\end{equation}


Now, let us take a closer look at $B^{\prime}$. Indeed, every $\left(
i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$ satisfies%
\begin{align}
b_{i,j}  &  =%
\begin{cases}
x_{i}^{U}-x_{U}^{2}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }1<j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{i,j}\right)
\nonumber\\
&  =%
\begin{cases}
x_{i}^{U}-x_{U}^{2}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }1<j<U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j<U\text{ (since }j\in\left\{  1,2,\ldots,U-1\right\} \\
\text{(since }\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}
^{2}\text{))}%
\end{array}
\right) \nonumber\\
&  =%
\begin{cases}
\left(  x_{i}^{2}-x_{U}^{2}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
\left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}, & \text{if }1<j<U
\end{cases}
=%
\begin{cases}
\left(  x_{i}-x_{U}\right)  \left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, &
\text{if }j=1;\\
\left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}, & \text{if }1<j<U
\end{cases}
\nonumber\\
&  =\left(  x_{i}-x_{U}\right)
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }1<j<U
\end{cases}
\nonumber\\
&  =\left(  x_{i}-x_{U}\right)
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
\label{sol.vander-det.s1.bij-small}%
\end{align}
(since $1<j<U$ is equivalent to $j>1$).

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$, we
define $g_{i,j}\in\mathbb{K}$ by%
\[
g_{i,j}=%
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
.
\]
Let $G$ be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix
$\left(  g_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$. Here is how
$G$ looks like:%
\[
G=\left(
\begin{array}
[c]{cccccc}%
\left(  x_{1}+x_{U}\right)  x_{1}^{U-2} & x_{1}^{U-3} & x_{1}^{U-4} & \cdots &
x_{1} & 1\\
\left(  x_{2}+x_{U}\right)  x_{2}^{U-2} & x_{2}^{U-3} & x_{2}^{U-4} & \cdots &
x_{2} & 1\\
\left(  x_{3}+x_{U}\right)  x_{3}^{U-2} & x_{3}^{U-3} & x_{3}^{U-4} & \cdots &
x_{3} & 1\\
\left(  x_{4}+x_{U}\right)  x_{4}^{U-2} & x_{4}^{U-3} & x_{4}^{U-4} & \cdots &
x_{4} & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
\left(  x_{U-1}+x_{U}\right)  x_{U-1}^{U-2} & x_{U-1}^{U-3} & x_{U-1}^{U-4} &
\cdots & x_{U-1} & 1
\end{array}
\right)  .
\]
Now, (\ref{sol.vander-det.s1.bij-small}) becomes%
\begin{equation}
b_{i,j}=\left(  x_{i}-x_{U}\right)  \underbrace{%
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
}_{=g_{i,j}}=\left(  x_{i}-x_{U}\right)  g_{i,j}
\label{sol.vander-det.s1.bij-small-2}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$.
Hence,%
\begin{equation}
B^{\prime}=\left(  \underbrace{b_{i,j}}_{\substack{=\left(  x_{i}%
-x_{U}\right)  g_{i,j}\\\text{(by (\ref{sol.vander-det.s1.bij-small-2}))}%
}}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}=\left(  \left(  x_{i}%
-x_{U}\right)  g_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}.
\label{sol.vander-det.s1.B'}%
\end{equation}
On the other hand, the definition of $G$ yields
\begin{equation}
G=\left(  g_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}.
\label{sol.vander-det.s1.AU-1}%
\end{equation}


Now, we claim that%
\begin{equation}
\det\left(  B^{\prime}\right)  =\det G\cdot\prod_{i=1}^{U-1}\left(
x_{i}-x_{U}\right)  . \label{sol.vander-det.s1.detB'=}%
\end{equation}
This can be proven similarly to how we proved (\ref{pf.thm.vander-det.detB'=})
back in our proof of Theorem \ref{thm.vander-det}. (Of course, this time, $G$
plays the role of $A_{U-1}$.) Now, (\ref{sol.vander-det.s1.detAU=detB'})
becomes%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  =\det G\cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\label{sol.vander-det.s1.detAU=}%
\end{equation}


Now, we want to compute $\det G$. (This part is harder than the analogous part
of our proof of Theorem \ref{thm.vander-det}, because back then the role of
$G$ was played by $A_{U-1}$, and we knew $\det\left(  A_{U-1}\right)  $
directly from our induction hypothesis.)

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$, we
define two elements $p_{i,j}\in\mathbb{K}$ and $q_{i,j}\in\mathbb{K}$ by%
\[
p_{i,j}=%
\begin{cases}
x_{i}^{U-1}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
\]
and%
\[
q_{i,j}=%
\begin{cases}
x_{U}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
.
\]
We let $P$ be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix
$\left(  p_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$, and we let $Q$
be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix $\left(
q_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$. We now make the
following observations:

\begin{itemize}
\item The columns of the matrix $Q$ equal the corresponding columns of $P$
except (perhaps) the $1$-st column. The matrix $G$ is the $\left(  U-1\right)
\times\left(  U-1\right)  $-matrix obtained from $P$ by adding the $1$-st
column of $Q$ to the $1$-st column of $P$. Thus, Exercise \ref{exe.ps4.6}
\textbf{(j)} (applied to $U-1$, $1$, $P$, $Q$ and $G$ instead of $n$, $k$,
$A$, $A^{\prime}$ and $B$) yields $\det G=\det P+\det Q$.

\item We have $P=\left(  p_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$
and $A_{U-1}=\left(  a_{i,j,U-1}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}%
$. But a quick look at the definitions reveals that
\[
p_{i,j}=%
\begin{cases}
x_{i}^{U-1}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
=%
\begin{cases}
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1;\\
x_{i}^{U-1}, & \text{if }j=1
\end{cases}
=a_{i,j,U-1}%
\]
for all $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$. Hence,
$P=A_{U-1}$.

\item The matrix $Q$ is obtained from the matrix $\left(  x_{i}^{\left(
U-1\right)  -j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$ by multiplying
its $1$-st column by $x_{U}$. Hence, Exercise \ref{exe.ps4.6} \textbf{(h)}
(applied to $U-1$, $x_{U}$, $1$, $\left(  x_{i}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$ and $Q$ instead of $n$,
$\lambda$, $k$, $A$ and $B$) yields%
\begin{align*}
\det Q  &  =x_{U}\underbrace{\det\left(  \left(  x_{i}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}\right)  }_{\substack{=\prod
_{1\leq i<j\leq U-1}\left(  x_{i}-x_{j}\right)  \\\text{(by Theorem
\ref{thm.vander-det} \textbf{(a)}, applied to }n=U-1\text{)}}}\\
&  =x_{U}\prod_{1\leq i<j\leq U-1}\left(  x_{i}-x_{j}\right)  .
\end{align*}

\end{itemize}

Combining this, we obtain%
\begin{align*}
\det G  &  =\det\underbrace{P}_{=A_{U-1}}+\underbrace{\det Q}_{=x_{U}%
\prod_{1\leq i<j\leq U-1}\left(  x_{i}-x_{j}\right)  }\\
&  =\underbrace{\det\left(  A_{U-1}\right)  }_{\substack{=\left(  x_{1}%
+x_{2}+\cdots+x_{U-1}\right)  \prod_{1\leq i<j\leq U-1}\left(  x_{i}%
-x_{j}\right)  \\\text{(since we assumed that (\ref{sol.vander-det.s1.goal})
holds for }u=U-1\text{)}}}+x_{U}\prod_{1\leq i<j\leq U-1}\left(  x_{i}%
-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U-1}\right)  \prod_{1\leq i<j\leq
U-1}\left(  x_{i}-x_{j}\right)  +x_{U}\prod_{1\leq i<j\leq U-1}\left(
x_{i}-x_{j}\right) \\
&  =\underbrace{\left(  \left(  x_{1}+x_{2}+\cdots+x_{U-1}\right)
+x_{U}\right)  }_{=x_{1}+x_{2}+\cdots+x_{U}}\prod_{1\leq i<j\leq U-1}\left(
x_{i}-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \prod_{1\leq i<j\leq U-1}\left(
x_{i}-x_{j}\right)  .
\end{align*}
Hence, (\ref{sol.vander-det.s1.detAU=}) yields%
\begin{align*}
\det\left(  A_{U}\right)   &  =\underbrace{\det G}_{=\left(  x_{1}%
+x_{2}+\cdots+x_{U}\right)  \prod_{1\leq i<j\leq U-1}\left(  x_{i}%
-x_{j}\right)  }\cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \underbrace{\prod_{1\leq i<j\leq
U-1}}_{=\prod_{j=1}^{U-1}\prod_{i=1}^{j-1}}\left(  x_{i}-x_{j}\right)
\cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \left(  \prod_{j=1}^{U-1}%
\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  \right)  \cdot\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \underbrace{\prod_{1\leq i<j\leq
U}}_{=\prod_{j=1}^{U}\prod_{i=1}^{j-1}}\left(  x_{i}-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \prod_{j=1}^{U}\prod_{i=1}%
^{j-1}\left(  x_{i}-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \left(  \prod_{j=1}^{U-1}%
\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  \right)  \cdot\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the factor for
}j=U\text{ from the product}\right)  ,
\end{align*}
this yields $\det\left(  A_{U}\right)  =\left(  x_{1}+x_{2}+\cdots
+x_{U}\right)  \prod_{1\leq i<j\leq U}\left(  x_{i}-x_{j}\right)  $. In other
words, (\ref{sol.vander-det.s1.goal}) holds for $u=U$. This completes the
induction step.

Now, (\ref{sol.vander-det.s1.goal}) is proven by induction. Hence, we can
apply (\ref{sol.vander-det.s1.goal}) to $u=n$. As the result, we obtain
$\det\left(  A_{n}\right)  =\left(  x_{1}+x_{2}+\cdots+x_{n}\right)
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  $. Since $A_{n}=\left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of $A_{n}$),
this rewrites as
\[
\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(  x_{1}+x_{2}%
+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
This solves Exercise \ref{exe.vander-det.s1}.
\end{proof}

We shall give a second solution for Exercise \ref{exe.vander-det.s1} later (in
Section \ref{sect.sols.vander-det.s1.sol2}).

\subsection{Solution to Exercise \ref{exe.vander-det.xi+yj}}

We shall first sketch short solutions to parts \textbf{(a)} and \textbf{(b)}
of Exercise \ref{exe.vander-det.xi+yj}. Then, we will show a solution to part
\textbf{(c)} (which is a more elaborate and subtler version of our solution to
part \textbf{(b)}), and finally derive parts \textbf{(a)} and \textbf{(b)}
from part \textbf{(c)}. Thus, parts \textbf{(a)} and \textbf{(b)} of Exercise
\ref{exe.vander-det.xi+yj} will be solved twice (though the solutions cannot
really be called different).

\begin{proof}
[Solution sketch to parts \textbf{(a)} and \textbf{(b)} of Exercise
\ref{exe.vander-det.xi+yj}.]\textbf{(a)} Let $m\in\left\{  0,1,\ldots
,n-2\right\}  $. Thus, $m+1<n$.

Define an $n\times\left(  m+1\right)  $-matrix $B$ by%
\[
B=\left(  \dbinom{m}{j-1}x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m+1}.
\]
Define an $\left(  m+1\right)  \times n$-matrix $C$ by%
\[
C=\left(  y_{j}^{m-\left(  i-1\right)  }\right)  _{1\leq i\leq m+1,\ 1\leq
j\leq n}.
\]
Then, the definition of the product of two matrices shows that%
\[
BC=\left(  \sum_{k=1}^{m+1}\dbinom{m}{k-1}x_{i}^{k-1}y_{j}^{m-\left(
k-1\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \sum_{k=1}^{m+1}\dbinom{m}{k-1}x_{i}^{k-1}y_{j}^{m-\left(  k-1\right)  }\\
&  =\sum_{k=0}^{m}\dbinom{m}{k}x_{i}^{k}y_{j}^{m-k}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we have substituted }k\text{ for }k-1\text{ in the sum}\right) \\
&  =\left(  x_{i}+y_{j}\right)  ^{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
the binomial formula yields }\left(  x_{i}+y_{j}\right)  ^{m}=\sum_{k=0}%
^{m}\dbinom{m}{k}x_{i}^{k}y_{j}^{m-k}\right)  ,
\end{align*}
this rewrites as%
\[
BC=\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]


But recall that $m+1<n$. Hence, (\ref{eq.exam.cauchy-binet.0}) (applied to
$m+1$, $B$ and $C$ instead of $m$, $A$ and $B$) shows that $\det\left(
BC\right)  =0$. Since $BC=\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$, this rewrites as $\det\left(  \left(
\left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =0$. This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

\textbf{(b)} Define an $n\times n$-matrix $B$ by%
\[
B=\left(  \dbinom{n-1}{j-1}x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Define an $n\times n$-matrix $C$ by%
\[
C=\left(  y_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then, the definition of the product of two matrices shows that%
\[
BC=\left(  \sum_{k=1}^{n}\dbinom{n-1}{k-1}x_{i}^{k-1}y_{j}^{n-k}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \sum_{k=1}^{n}\dbinom{n-1}{k-1}x_{i}^{k-1}\underbrace{y_{j}^{n-k}}%
_{=y_{j}^{\left(  n-1\right)  -\left(  k-1\right)  }}=\sum_{k=1}^{n}%
\dbinom{n-1}{k-1}x_{i}^{k-1}y_{j}^{\left(  n-1\right)  -\left(  k-1\right)
}\\
&  =\sum_{k=0}^{n-1}\dbinom{n-1}{k}x_{i}^{k}y_{j}^{\left(  n-1\right)
-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}k-1\text{ in the sum}\right) \\
&  =\left(  x_{i}+y_{j}\right)  ^{n-1}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the binomial formula yields}\\
\left(  x_{i}+y_{j}\right)  ^{n-1}=\sum_{k=0}^{n-1}\dbinom{n-1}{k}x_{i}%
^{k}y_{j}^{\left(  n-1\right)  -k}%
\end{array}
\right)  ,
\end{align*}
this rewrites as%
\[
BC=\left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]


Theorem \ref{thm.det(AB)} shows that $\det\left(  BC\right)  =\det B\cdot\det
C$. But what are $\det B$ and $\det C$ ?

Finding $\det C$ is easy: We have $C=\left(  y_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ and thus
\[
\det C=\det\left(  \left(  y_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(b)}, applied to $y_{k}$ instead of
$x_{k}$).

To find $\det B$, we first observe that $\det\left(  \left(  x_{i}%
^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq j<i\leq
n}\left(  x_{i}-x_{j}\right)  $ (by Theorem \ref{thm.vander-det}
\textbf{(c)}). But $B=\left(  \dbinom{n-1}{j-1}x_{i}^{j-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. In other words, the matrix $B$ is obtained from the
matrix $\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ by
multiplying the whole $j$-th column with $\dbinom{n-1}{j-1}$ for every
$j\in\left\{  1,2,\ldots,n\right\}  $. Therefore, the determinant $\det B$ is
obtained by successively multiplying $\det\left(  \left(  x_{i}^{j-1}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  $ with $\dbinom{n-1}{j-1}$ for every
$j\in\left\{  1,2,\ldots,n\right\}  $ (since Exercise \ref{exe.ps4.6}
\textbf{(h)} tells us that multiplying a single column of a square matrix by a
scalar $\lambda$ results in the determinant getting multiplied by $\lambda$).
In other words,%
\begin{align*}
\det B  &  =\underbrace{\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  }_{\substack{=\prod_{1\leq j<i\leq n}\left(
x_{i}-x_{j}\right)  \\=\prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)
\\\text{(here, we renamed the}\\\text{index }\left(  j,i\right)  \text{ as
}\left(  i,j\right)  \text{)}}}\cdot\underbrace{\prod_{j=1}^{n}\dbinom
{n-1}{j-1}}_{\substack{=\prod_{k=0}^{n-1}\dbinom{n-1}{k}\\\text{(here, we
substituted }k\text{ for }j-1\text{)}}}\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)  \right)
\cdot\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  .
\end{align*}


Now,%
\begin{align*}
\det\left(  BC\right)   &  =\underbrace{\det B}_{=\left(  \prod_{1\leq i<j\leq
n}\left(  x_{j}-x_{i}\right)  \right)  \cdot\left(  \prod_{k=0}^{n-1}%
\dbinom{n-1}{k}\right)  }\cdot\underbrace{\det C}_{=\prod_{1\leq i<j\leq
n}\left(  y_{i}-y_{j}\right)  }\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)  \right)
\cdot\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)  \right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\underbrace{\left(
\prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)  \right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)  \right)  }_{=\prod_{1\leq
i<j\leq n}\left(  \left(  x_{j}-x_{i}\right)  \left(  y_{i}-y_{j}\right)
\right)  }\\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\prod_{1\leq i<j\leq
n}\underbrace{\left(  \left(  x_{j}-x_{i}\right)  \left(  y_{i}-y_{j}\right)
\right)  }_{=\left(  x_{i}-x_{j}\right)  \left(  y_{j}-y_{i}\right)  }\\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\underbrace{\prod
_{1\leq i<j\leq n}\left(  \left(  x_{i}-x_{j}\right)  \left(  y_{j}%
-y_{i}\right)  \right)  }_{=\left(  \prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}%
-y_{i}\right)  \right)  }\\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
Since $BC=\left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, this rewrites as%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(b)}.
\end{proof}

Now, as promised, we shall start from scratch, and solve Exercise
\ref{exe.vander-det.xi+yj} \textbf{(c)} first, and then solve Exercise
\ref{exe.vander-det.xi+yj} \textbf{(a)} and \textbf{(b)} again using Exercise
\ref{exe.vander-det.xi+yj} \textbf{(c)}.

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.vander-det.xi+yj}.]\textbf{(c)} We extend the
$n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  $ to an infinite
sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{K}^{\infty}$ by
setting%
\begin{equation}
p_{\ell}=0\ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\left\{
n,n+1,n+2,\ldots\right\}  . \label{sol.vander-det.xi+yj.short.c.pk=0}%
\end{equation}


Next, we define three $n\times n$-matrices $B$, $C$ and $D$:

\begin{itemize}
\item Define an $n\times n$-matrix $B$ by
\[
B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{equation}
\det\underbrace{B}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}=\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\label{sol.vander-det.xi+yj.short.c.detB}%
\end{equation}
(by Theorem \ref{thm.vander-det} \textbf{(a)}).

\item For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
the element $\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$ of $\mathbb{K}$ is
well-defined\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $j-1\geq0$. Hence, the binomial coefficient
$\dbinom{n-1-i+j}{j-1}\in\mathbb{Z}$ is well-defined. Moreover,
$n-1-\underbrace{i}_{\leq n}+\underbrace{j}_{\geq1}\geq n-1-n+1=0$, so that
$n-1-i+j\in\mathbb{N}$. Thus, $p_{n-1-i+j}\in\mathbb{K}$ is well-defined
(since we have an infinite sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)
\in\mathbb{K}^{\infty}$). Hence, the element $\dbinom{n-1-i+j}{j-1}%
p_{n-1-i+j}$ of $\mathbb{K}$ is well-defined. Qed.}. Thus, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, we can define an element
$c_{i,j}\in\mathbb{K}$ by $c_{i,j}=\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$. Consider
these elements $c_{i,j}$. Define an $n\times n$-matrix $C$ by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Consider this matrix $C$. We have $c_{i,j}=0$ for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ be such that $i<j$. From $i<j$, we obtain $i\leq
j-1$ (since $i$ and $j$ are integers). Thus, $n-1-\underbrace{i}_{\leq
j-1}+j\geq n-1-\left(  j-1\right)  +j=n$. In other words, $n-1-i+j\in\left\{
n,n+1,n+2,\ldots\right\}  $. Hence, $p_{n-1-i+j}=0$ (by
(\ref{sol.vander-det.xi+yj.short.c.pk=0}), applied to $\ell=n-1-i+j$). Hence,
$c_{i,j}=\dbinom{n-1-i+j}{j-1}\underbrace{p_{n-1-i+j}}_{=0}=0$, qed.}. Hence,
Exercise \ref{exe.ps4.3} (applied to $C$ and $\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ instead of $A$ and $\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n,n}=\prod_{i=1}^{n}\underbrace{c_{i,i}%
}_{\substack{=\dbinom{n-1-i+i}{i-1}p_{n-1-i+i}\\\text{(by the definition of
}c_{i,i}\text{)}}}\nonumber\\
&  =\prod_{i=1}^{n}\left(  \underbrace{\dbinom{n-1-i+i}{i-1}}_{=\dbinom
{n-1}{i-1}}\underbrace{p_{n-1-i+i}}_{=p_{n-1}}\right) \nonumber\\
&  =\prod_{i=1}^{n}\left(  \dbinom{n-1}{i-1}p_{n-1}\right)  =\left(
\prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  p_{n-1}^{n}\nonumber\\
&  =p_{n-1}^{n}\left(  \prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  =p_{n-1}%
^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)
\label{sol.vander-det.xi+yj.short.c.detC}%
\end{align}
(here, we have substituted $k$ for $i-1$ in the product).

\item Define an $n\times n$-matrix $D$ by
\[
D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{align}
\det\underbrace{D}_{=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}  &  =\det\left(  \left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  y_{i}-y_{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.vander-det}
\textbf{(d)}, applied to }y_{k}\text{ instead of }x_{k}\right) \nonumber\\
&  =\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)
\label{sol.vander-det.xi+yj.short.c.detD}%
\end{align}
(here, we have renamed the index $\left(  j,i\right)  $ as $\left(
i,j\right)  $ in the product).
\end{itemize}

Our goal is to prove that $\left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}=BCD$. Once this is done, we will be able to
compute $\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  $ by applying Theorem \ref{thm.det(AB)} twice.

We have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the
definition of the product of two matrices shows that%
\begin{equation}
CD=\left(  \sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}. \label{sol.vander-det.xi+yj.short.c.CD.1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
\sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}  &  =\sum_{\ell=0}^{n-1}\underbrace{c_{i,\ell
+1}}_{\substack{=\dbinom{n-1-i+\left(  \ell+1\right)  }{\left(  \ell+1\right)
-1}p_{n-1-i+\left(  \ell+1\right)  }\\\text{(by the definition of }%
c_{i,\ell+1}\text{)}}}y_{j}^{\left(  \ell+1\right)  -1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\ell+1\text{
for }k\text{ in the sum}\right) \\
&  =\sum_{\ell=0}^{n-1}\underbrace{\dbinom{n-1-i+\left(  \ell+1\right)
}{\left(  \ell+1\right)  -1}}_{\substack{=\dbinom{n-i+\ell}{\ell}%
}}\underbrace{p_{n-1-i+\left(  \ell+1\right)  }}_{=p_{n-i+\ell}}%
\underbrace{y_{j}^{\left(  \ell+1\right)  -1}}_{=y_{j}^{\ell}}\\
&  =\sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell}.
\end{align*}
Hence, (\ref{sol.vander-det.xi+yj.short.c.CD.1}) rewrites as%
\[
CD=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]


Now, $B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$CD=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the definition of the
product of two matrices shows that%
\begin{equation}
B\left(  CD\right)  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell
=0}^{n-1}\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}. \label{sol.vander-det.xi+yj.short.c.BCD.1}%
\end{equation}


Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,%
\begin{align}
&  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}{\ell
}p_{n-k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}x_{i}^{k}\left(  \sum_{\ell=0}^{n-1}\dbinom{k+\ell}{\ell
}p_{k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}n-k\text{ in the first sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=0}^{n-1}x_{i}^{k}\dbinom{k+\ell}{\ell}%
p_{k+\ell}y_{j}^{\ell}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1+k}x_{i}^{k}\underbrace{\dbinom
{k+\left(  \ell-k\right)  }{\ell-k}}_{=\dbinom{\ell}{\ell-k}}%
\underbrace{p_{k+\left(  \ell-k\right)  }}_{=p_{\ell}}y_{j}^{\ell
-k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\ell-k\text{
for }\ell\text{ in the second sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\underbrace{\sum_{\ell=k}^{n-1+k}x_{i}^{k}\dbinom{\ell
}{\ell-k}p_{\ell}y_{j}^{\ell-k}}_{\substack{=\sum_{\ell=k}^{n-1}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}\\\text{(since }k\leq n\leq
n-1+k+1\text{)}}}\nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}^{k}\dbinom{\ell}{\ell
-k}\underbrace{p_{\ell}}_{\substack{=0\\\text{(by
(\ref{sol.vander-det.xi+yj.short.c.pk=0})}\\\text{(since }\ell\in\left\{
n,n+1,\ldots,n-1+k\right\}  \subseteq\left\{  n,n+1,n+2,\ldots\right\}
\text{))}}}y_{j}^{\ell-k}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\underbrace{\sum_{\ell=n}^{n-1+k}x_{i}^{k}%
\dbinom{\ell}{\ell-k}0y_{j}^{\ell-k}}_{=0}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell
}y_{j}^{\ell-k}. \label{sol.vander-det.xi+yj.short.c.BCD.2}%
\end{align}
On the other hand, $P\left(  X\right)  =\sum_{k=0}^{n-1}p_{k}X^{k}=\sum
_{\ell=0}^{n-1}p_{\ell}X^{\ell}$ (here, we renamed the summation index $k$ as
$\ell$). Hence,%
\begin{align}
P\left(  x_{i}+y_{j}\right)   &  =\sum_{\ell=0}^{n-1}p_{\ell}%
\underbrace{\left(  x_{i}+y_{j}\right)  ^{\ell}}_{\substack{=\sum_{k=0}^{\ell
}\dbinom{\ell}{k}x_{i}^{k}y_{j}^{\ell-k}\\\text{(by the binomial formula)}%
}}=\sum_{\ell=0}^{n-1}p_{\ell}\sum_{k=0}^{\ell}\dbinom{\ell}{k}x_{i}^{k}%
y_{j}^{\ell-k}\nonumber\\
&  =\underbrace{\sum_{\ell=0}^{n-1}\sum_{k=0}^{\ell}}_{=\sum
_{\substack{\left(  \ell,k\right)  \in\left\{  0,1,\ldots,n-1\right\}
^{2};\\k\leq\ell}}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}}p_{\ell}\dbinom{\ell
}{k}x_{i}^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}p_{\ell}\underbrace{\dbinom{\ell}{k}%
}_{\substack{=\dbinom{\ell}{\ell-k}\\\text{(by (\ref{eq.binom.symm}), applied
to }\ell\text{ and }k\\\text{instead of }m\text{ and }n\text{)}}}x_{i}%
^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}p_{\ell}\dbinom{\ell}{\ell-k}x_{i}%
^{k}y_{j}^{\ell-k}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell
}{\ell-k}p_{\ell}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}%
{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.short.c.BCD.2})}\right)  .
\label{sol.vander-det.xi+yj.short.c.BCD.3}%
\end{align}


Now, let us forget that we fixed $\left(  i,j\right)  $. We thus have proven
(\ref{sol.vander-det.xi+yj.short.c.BCD.3}) for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence,%
\begin{align*}
&  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\\
&  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}%
\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=B\left(  CD\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.short.c.BCD.1})}\right) \\
&  =BCD.
\end{align*}
Hence,%
\begin{align*}
&  \det\underbrace{\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{=BCD}\\
&  =\det\left(  BCD\right)  =\det B\cdot\underbrace{\det\left(  CD\right)
}_{\substack{=\det C\cdot\det D\\\text{(by Theorem \ref{thm.det(AB)}, applied
to }C\text{ and }D\\\text{instead of }A\text{ and }B\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}B\text{ and }CD\text{ instead of }A\text{ and }B\right) \\
&  =\underbrace{\det B}_{\substack{=\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.short.c.detB}))}}%
}\cdot\underbrace{\det C}_{\substack{=p_{n-1}^{n}\left(  \prod_{k=0}%
^{n-1}\dbinom{n-1}{k}\right)  \\\text{(by
(\ref{sol.vander-det.xi+yj.short.c.detC}))}}}\cdot\underbrace{\det
D}_{\substack{=\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \\\text{(by
(\ref{sol.vander-det.xi+yj.short.c.detD}))}}}\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \cdot
p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(c)}.

\textbf{(a)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be
the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

Let $m\in\left\{  0,1,\ldots,n-1\right\}  $. (You are reading this right: We
are not requiring $m$ to belong to $\left\{  0,1,\ldots,n-2\right\}  $; the
purpose of this is to obtain a result that will bring us close to solving both
parts \textbf{(a)} and \textbf{(b)} simultaneously.)

Define an $n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  \in
\mathbb{K}^{n}$ by%
\[
\left(  p_{k}=\delta_{k,m}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
0,1,\ldots,n-1\right\}  \right)  .
\]
Let $P\left(  X\right)  \in\mathbb{K}\left[  X\right]  $ be the polynomial
$\sum_{k=0}^{n-1}p_{k}X^{k}$.

We have%
\begin{align*}
P\left(  X\right)   &  =\underbrace{\sum_{k=0}^{n-1}}_{=\sum_{k\in\left\{
0,1,\ldots,n-1\right\}  }}\underbrace{p_{k}}_{\substack{=\delta_{k,m}}%
}X^{k}=\sum_{k\in\left\{  0,1,\ldots,n-1\right\}  }\delta_{k,m}X^{k}\\
&  =\underbrace{\delta_{m,m}}_{\substack{=1\\\text{(since }k=m\text{)}}%
}X^{m}+\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}  ;\\k\neq
m}}\underbrace{\delta_{k,m}}_{\substack{=0\\\text{(since }k\neq m\text{)}%
}}X^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=m\text{ from the sum}\\
\text{(since }m\in\left\{  0,1,\ldots,n-1\right\}  \text{)}%
\end{array}
\right) \\
&  =X^{m}+\underbrace{\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}
;\\k\neq m}}0X^{k}}_{=0}=X^{m}.
\end{align*}
Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies
$P\left(  x_{i}+y_{j}\right)  =\left(  x_{i}+y_{j}\right)  ^{m}$ (since
$P\left(  X\right)  =X^{m}$). In other words, $\left(  P\left(  x_{i}%
+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \left(
x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Taking determinants on both sides of this equality, we obtain%
\begin{align}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\underbrace{p_{n-1}^{n}}_{\substack{=\delta_{n-1,m}^{n}\\\text{(since
}p_{n-1}=\delta_{n-1,m}\\\text{(by the definition of }p_{n-1}\text{))}%
}}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.vander-det.xi+yj}
\textbf{(c)}}\right) \nonumber\\
&  =\delta_{n-1,m}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\label{sol.vander-det.xi+yj.short.a.almost}%
\end{align}


Now, let us forget that we fixed $m$. We thus have proven
(\ref{sol.vander-det.xi+yj.short.a.almost}) for every $m\in\left\{
0,1,\ldots,n-1\right\}  $.

Now, let $m\in\left\{  0,1,\ldots,n-2\right\}  $. Thus, $m\neq n-1$, so that
$\delta_{n-1,m}=0$. Hence, $\delta_{n-1,m}^{n}=0^{n}=0$ (since $n$ is a
positive integer).

On the other hand, $m\in\left\{  0,1,\ldots,n-2\right\}  \subseteq\left\{
0,1,\ldots,n-1\right\}  $, and therefore
(\ref{sol.vander-det.xi+yj.short.a.almost}) holds. Hence,%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,m}^{n}}_{=0}\left(  \prod_{k=0}^{n-1}\dbinom
{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =0.
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

\textbf{(b)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ as in
the solution to Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

We have $\delta_{n-1,n-1}=1$ and thus $\delta_{n-1,n-1}^{n}=1^{n}=1$.

In our solution of Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}, we have
proven the equality (\ref{sol.vander-det.xi+yj.short.a.almost}) for every
$m\in\left\{  0,1,\ldots,n-1\right\}  $. Thus, we can apply this equality to
$m=n-1$. As a result, we obtain
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,n-1}^{n}}_{=1}\left(  \prod_{k=0}^{n-1}%
\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}%
-y_{i}\right)  \right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.vander-det.xi+yj}.]\textbf{(c)} We extend the
$n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  $ to an infinite
sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{K}^{\infty}$ by
setting%
\begin{equation}
p_{\ell}=0\ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\left\{
n,n+1,n+2,\ldots\right\}  . \label{sol.vander-det.xi+yj.c.pk=0}%
\end{equation}


Next, we define three $n\times n$-matrices $B$, $C$ and $D$:

\begin{itemize}
\item Define an $n\times n$-matrix $B$ by
\[
B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{equation}
\det\underbrace{B}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}=\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\label{sol.vander-det.xi+yj.c.detB}%
\end{equation}
(by Theorem \ref{thm.vander-det} \textbf{(a)}).

\item For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
the element $\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$ of $\mathbb{K}$ is
well-defined\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,n\right\}  $. Hence, $1\leq i\leq n$ (since
$i\in\left\{  1,2,\ldots,n\right\}  $) and $1\leq j\leq n$ (since
$j\in\left\{  1,2,\ldots,n\right\}  $). From $1\leq j$, we obtain $j-1\geq0$.
Hence, the binomial coefficient $\dbinom{n-1-i+j}{j-1}\in\mathbb{Z}$ is
well-defined.
\par
Moreover, $n-1-\underbrace{i}_{\leq n}+\underbrace{j}_{\geq1}\geq n-1-n+1=0$,
so that $n-1-i+j\in\mathbb{N}$. Thus, $p_{n-1-i+j}\in\mathbb{K}$ is
well-defined (since we have an infinite sequence $\left(  p_{0},p_{1}%
,p_{2},\ldots\right)  \in\mathbb{K}^{\infty}$). Hence, the element
$\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$ of $\mathbb{K}$ is well-defined. Qed.}.
Thus, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we can define an element $c_{i,j}\in\mathbb{K}$ by $c_{i,j}=\dbinom
{n-1-i+j}{j-1}p_{n-1-i+j}$. Consider these elements $c_{i,j}$. Define an
$n\times n$-matrix $C$ by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Consider this matrix $C$. We have $c_{i,j}=0$ for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ be such that $i<j$. From $i<j$, we obtain $i\leq
j-1$ (since $i$ and $j$ are integers). Thus, $n-1-\underbrace{i}_{\leq
j-1}+j\geq n-1-\left(  j-1\right)  +j=n$. In other words, $n-1-i+j\in\left\{
n,n+1,n+2,\ldots\right\}  $. Hence, $p_{n-1-i+j}=0$ (by
(\ref{sol.vander-det.xi+yj.c.pk=0}), applied to $\ell=n-1-i+j$). Hence,
$c_{i,j}=\dbinom{n-1-i+j}{j-1}\underbrace{p_{n-1-i+j}}_{=0}=0$, qed.}. Hence,
Exercise \ref{exe.ps4.3} (applied to $C$ and $\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ instead of $A$ and $\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n,n}=\prod_{i=1}^{n}\underbrace{c_{i,i}%
}_{\substack{=\dbinom{n-1-i+i}{i-1}p_{n-1-i+i}\\\text{(by the definition of
}c_{i,i}\text{)}}}\nonumber\\
&  =\prod_{i=1}^{n}\left(  \underbrace{\dbinom{n-1-i+i}{i-1}}_{=\dbinom
{n-1}{i-1}}\underbrace{p_{n-1-i+i}}_{\substack{=p_{n-1}\\\text{(since
}n-1-i+i=n-1\text{)}}}\right) \nonumber\\
&  =\prod_{i=1}^{n}\left(  \dbinom{n-1}{i-1}p_{n-1}\right)  =\left(
\prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  p_{n-1}^{n}\nonumber\\
&  =p_{n-1}^{n}\left(  \prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  =p_{n-1}%
^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)
\label{sol.vander-det.xi+yj.c.detC}%
\end{align}
(here, we have substituted $k$ for $i-1$ in the product).

\item Define an $n\times n$-matrix $D$ by
\[
D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{align}
\det\underbrace{D}_{=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}  &  =\det\left(  \left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  y_{i}-y_{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.vander-det}
\textbf{(d)}, applied to }y_{k}\text{ instead of }x_{k}\right) \nonumber\\
&  =\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)
\label{sol.vander-det.xi+yj.c.detD}%
\end{align}
(here, we have renamed the index $\left(  j,i\right)  $ as $\left(
i,j\right)  $ in the product).
\end{itemize}

Our goal is to prove that $\left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}=BCD$. Once this is done, we will be able to
compute $\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  $ by applying Theorem \ref{thm.det(AB)} twice.

We have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the
definition of the product of two matrices shows that%
\begin{equation}
CD=\left(  \sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}. \label{sol.vander-det.xi+yj.c.CD.1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
\sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}  &  =\sum_{k=0}^{n-1}\underbrace{c_{i,k+1}%
}_{\substack{=\dbinom{n-1-i+\left(  k+1\right)  }{\left(  k+1\right)
-1}p_{n-1-i+\left(  k+1\right)  }\\\text{(by the definition of }%
c_{i,k+1}\text{)}}}y_{j}^{\left(  k+1\right)  -1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+1\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=0}^{n-1}\underbrace{\dbinom{n-1-i+\left(  k+1\right)  }{\left(
k+1\right)  -1}}_{\substack{=\dbinom{n-i+k}{k}\\\text{(since }n-1-i+\left(
k+1\right)  =n-i+k\\\text{and }\left(  k+1\right)  -1=k\text{)}}%
}\underbrace{p_{n-1-i+\left(  k+1\right)  }}_{\substack{=p_{n-i+k}%
\\\text{(since }n-1-i+\left(  k+1\right)  =n-i+k\text{)}}}\underbrace{y_{j}%
^{\left(  k+1\right)  -1}}_{\substack{=y_{j}^{k}\\\text{(since }\left(
k+1\right)  -1=k\text{)}}}\\
&  =\sum_{k=0}^{n-1}\dbinom{n-i+k}{k}p_{n-i+k}y_{j}^{k}=\sum_{\ell=0}%
^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell}%
\end{align*}
(here, we renamed the summation index $k$ as $\ell$). Hence,
(\ref{sol.vander-det.xi+yj.c.CD.1}) becomes%
\[
CD=\left(  \underbrace{\sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}}_{=\sum_{\ell=0}%
^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell
}p_{n-i+\ell}y_{j}^{\ell}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]


Now, $B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$CD=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the definition of the
product of two matrices shows that%
\begin{equation}
B\left(  CD\right)  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell
=0}^{n-1}\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}. \label{sol.vander-det.xi+yj.c.BCD.1}%
\end{equation}


Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,%
\begin{align}
&  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}{\ell
}p_{n-k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}x_{i}^{k}\left(  \sum_{\ell=0}^{n-1}\dbinom{k+\ell}{\ell
}p_{k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}n-k\text{ in the first sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=0}^{n-1}x_{i}^{k}\dbinom{k+\ell}{\ell}%
p_{k+\ell}y_{j}^{\ell}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1+k}x_{i}^{k}\underbrace{\dbinom
{k+\left(  \ell-k\right)  }{\ell-k}}_{\substack{=\dbinom{\ell}{\ell
-k}\\\text{(since }k+\left(  \ell-k\right)  =\ell\text{)}}%
}\underbrace{p_{k+\left(  \ell-k\right)  }}_{\substack{=p_{\ell}\\\text{(since
}k+\left(  \ell-k\right)  =\ell\text{)}}}y_{j}^{\ell-k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\ell-k\text{
for }\ell\text{ in the second sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\underbrace{\sum_{\ell=k}^{n-1+k}x_{i}^{k}\dbinom{\ell
}{\ell-k}p_{\ell}y_{j}^{\ell-k}}_{\substack{=\sum_{\ell=k}^{n-1}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}\\\text{(since }k\leq n\leq
n-1+k+1\text{)}}}\nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}^{k}\dbinom{\ell}{\ell
-k}\underbrace{p_{\ell}}_{\substack{=0\\\text{(by
(\ref{sol.vander-det.xi+yj.c.pk=0})}\\\text{(since }\ell\in\left\{
n,n+1,\ldots,n-1+k\right\}  \subseteq\left\{  n,n+1,n+2,\ldots\right\}
\text{))}}}y_{j}^{\ell-k}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\underbrace{\sum_{\ell=n}^{n-1+k}x_{i}^{k}%
\dbinom{\ell}{\ell-k}0y_{j}^{\ell-k}}_{=0}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell
}y_{j}^{\ell-k}. \label{sol.vander-det.xi+yj.c.BCD.2}%
\end{align}
On the other hand, $P\left(  X\right)  =\sum_{k=0}^{n-1}p_{k}X^{k}=\sum
_{\ell=0}^{n-1}p_{\ell}X^{\ell}$ (here, we renamed the summation index $k$ as
$\ell$). Hence,%
\begin{align}
P\left(  x_{i}+y_{j}\right)   &  =\sum_{\ell=0}^{n-1}p_{\ell}%
\underbrace{\left(  x_{i}+y_{j}\right)  ^{\ell}}_{\substack{=\sum_{k=0}^{\ell
}\dbinom{\ell}{k}x_{i}^{k}y_{j}^{\ell-k}\\\text{(by the binomial formula)}%
}}=\sum_{\ell=0}^{n-1}p_{\ell}\sum_{k=0}^{\ell}\dbinom{\ell}{k}x_{i}^{k}%
y_{j}^{\ell-k}\nonumber\\
&  =\underbrace{\sum_{\ell=0}^{n-1}\sum_{k=0}^{\ell}}_{=\sum
_{\substack{\left(  \ell,k\right)  \in\left\{  0,1,\ldots,n-1\right\}
^{2};\\k\leq\ell}}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}}p_{\ell}\dbinom{\ell
}{k}x_{i}^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}p_{\ell}\underbrace{\dbinom{\ell}{k}%
}_{\substack{=\dbinom{\ell}{\ell-k}\\\text{(by (\ref{eq.binom.symm}) (applied
to }\ell\text{ and }k\\\text{instead of }m\text{ and }n\text{))}}}x_{i}%
^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}\underbrace{p_{\ell}\dbinom{\ell}%
{\ell-k}x_{i}^{k}}_{=x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell}}y_{j}^{\ell
-k}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell
}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}%
{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.c.BCD.2})}\right)  .
\label{sol.vander-det.xi+yj.c.BCD.3}%
\end{align}


Now, let us forget that we fixed $\left(  i,j\right)  $. We thus have proven
(\ref{sol.vander-det.xi+yj.c.BCD.3}) for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence,%
\begin{align*}
&  \left(  \underbrace{P\left(  x_{i}+y_{j}\right)  }_{\substack{=\sum
_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}{\ell
}p_{n-k+\ell}y_{j}^{\ell}\right)  \\\text{(by
(\ref{sol.vander-det.xi+yj.c.BCD.3}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\\
&  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}%
\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=B\left(  CD\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.c.BCD.1})}\right) \\
&  =BCD.
\end{align*}
Hence,%
\begin{align*}
&  \det\underbrace{\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{=BCD}\\
&  =\det\left(  BCD\right)  =\det B\cdot\underbrace{\det\left(  CD\right)
}_{\substack{=\det C\cdot\det D\\\text{(by Theorem \ref{thm.det(AB)}, applied
to }C\text{ and }D\\\text{instead of }A\text{ and }B\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}B\text{ and }CD\text{ instead of }A\text{ and }B\right) \\
&  =\underbrace{\det B}_{\substack{=\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.c.detB}))}}%
}\cdot\underbrace{\det C}_{\substack{=p_{n-1}^{n}\left(  \prod_{k=0}%
^{n-1}\dbinom{n-1}{k}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.c.detC}%
))}}}\cdot\underbrace{\det D}_{\substack{=\prod_{1\leq i<j\leq n}\left(
y_{j}-y_{i}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.c.detD}))}}}\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \cdot
p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(c)}.

\textbf{(a)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be
the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

Let $m\in\left\{  0,1,\ldots,n-1\right\}  $. (You are reading this right: We
are not requiring $m$ to belong to $\left\{  0,1,\ldots,n-2\right\}  $; the
purpose of this is to obtain a result that will bring us close to solving both
parts \textbf{(a)} and \textbf{(b)} simultaneously.)

Define an $n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  \in
\mathbb{K}^{n}$ by%
\[
\left(  p_{k}=\delta_{k,m}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
0,1,\ldots,n-1\right\}  \right)  .
\]
Let $P\left(  X\right)  \in\mathbb{K}\left[  X\right]  $ be the polynomial
$\sum_{k=0}^{n-1}p_{k}X^{k}$.

We have%
\begin{align*}
P\left(  X\right)   &  =\underbrace{\sum_{k=0}^{n-1}}_{=\sum_{k\in\left\{
0,1,\ldots,n-1\right\}  }}\underbrace{p_{k}}_{\substack{=\delta_{k,m}%
\\\text{(by the definition}\\\text{of }p_{k}\text{)}}}X^{k}=\sum_{k\in\left\{
0,1,\ldots,n-1\right\}  }\delta_{k,m}X^{k}\\
&  =\underbrace{\delta_{m,m}}_{\substack{=1\\\text{(since }k=m\text{)}}%
}X^{m}+\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}  ;\\k\neq
m}}\underbrace{\delta_{k,m}}_{\substack{=0\\\text{(since }k\neq m\text{)}%
}}X^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=m\text{ from the sum}\\
\text{(since }m\in\left\{  0,1,\ldots,n-1\right\}  \text{)}%
\end{array}
\right) \\
&  =X^{m}+\underbrace{\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}
;\\k\neq m}}0X^{k}}_{=0}=X^{m}.
\end{align*}
Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies
$P\left(  x_{i}+y_{j}\right)  =\left(  x_{i}+y_{j}\right)  ^{m}$ (since
$P\left(  X\right)  =X^{m}$). In other words, $\left(  P\left(  x_{i}%
+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \left(
x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Taking determinants on both sides of this equality, we obtain%
\begin{align}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\underbrace{p_{n-1}^{n}}_{\substack{=\delta_{n-1,m}^{n}\\\text{(since
}p_{n-1}=\delta_{n-1,m}\\\text{(by the definition of }p_{n-1}\text{))}%
}}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.vander-det.xi+yj}
\textbf{(c)}}\right) \nonumber\\
&  =\delta_{n-1,m}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\label{sol.vander-det.xi+yj.a.almost}%
\end{align}


Now, let us forget that we fixed $m$. We thus have proven
(\ref{sol.vander-det.xi+yj.a.almost}) for every $m\in\left\{  0,1,\ldots
,n-1\right\}  $.

Now, let $m\in\left\{  0,1,\ldots,n-2\right\}  $. Thus, $m\neq n-1$, so that
$n-1\neq m$. Thus, $\delta_{n-1,m}=0$.

But $n$ is a positive integer. Hence, $0^{n}=0$. Now, taking the equality
$\delta_{n-1,m}=0$ to the $n$-th power, we obtain $\delta_{n-1,m}^{n}=0^{n}=0$.

On the other hand, $m\in\left\{  0,1,\ldots,n-2\right\}  \subseteq\left\{
0,1,\ldots,n-1\right\}  $, and therefore (\ref{sol.vander-det.xi+yj.a.almost})
holds. Hence,%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,m}^{n}}_{=0}\left(  \prod_{k=0}^{n-1}\dbinom
{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =0.
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

\textbf{(b)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be
the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

We have $\delta_{n-1,n-1}=1$ (since $n-1=n-1$) and thus $\delta_{n-1,n-1}%
^{n}=1^{n}=1$.

In our solution of Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}, we have
proven the equality (\ref{sol.vander-det.xi+yj.a.almost}) for every
$m\in\left\{  0,1,\ldots,n-1\right\}  $. Thus, we can apply this equality to
$m=n-1$. As a result, we obtain
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,n-1}^{n}}_{=1}\left(  \prod_{k=0}^{n-1}%
\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}%
-y_{i}\right)  \right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(b)}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.cauchy-det}}

Exercise \ref{exe.cauchy-det} is obtained from \cite[Exercise 2.66(a)]{Reiner}
by setting $a_{i}=x_{i}$ and $b_{j}=-y_{j}$. (See the ancillary PDF file of
the arXiv version of \cite{Reiner} for the solutions to the exercises.)

Exercise \ref{exe.cauchy-det} can also be obtained from \cite[Theorem
2]{Gri-19.9} by setting $k=\mathbb{K}$, $m=n$, $a_{j}=y_{j}$ and $b_{i}%
=-x_{i}$ (and observing that
\begin{align*}
&  \prod_{\substack{\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\i>j}}\underbrace{\left(  \left(  y_{i}-y_{j}\right)  \left(  \left(
-x_{j}\right)  -\left(  -x_{i}\right)  \right)  \right)  }_{\substack{=\left(
y_{j}-y_{i}\right)  \left(  x_{j}-x_{i}\right)  \\=\left(  x_{j}-x_{i}\right)
\left(  y_{j}-y_{i}\right)  }}\\
&  =\prod_{\substack{\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\i>j}}\left(  \left(  x_{j}-x_{i}\right)  \left(  y_{j}-y_{i}\right)
\right)  =\prod_{\substack{\left(  j,i\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2};\\j>i}}\left(  \left(  x_{i}-x_{j}\right)  \left(
y_{i}-y_{j}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
i,j\right)  \text{ as }\left(  j,i\right)  \right) \\
&  =\prod_{1\leq i<j\leq n}\left(  \left(  x_{i}-x_{j}\right)  \left(
y_{i}-y_{j}\right)  \right)
\end{align*}
). The statement of \cite[Theorem 2]{Gri-19.9} makes the requirement that $k$
be a field; however, this is easily seen to be unnecessary for the proof.

\subsection{Solution to Exercise \ref{exe.cauchy-det-lem}}

Exercise \ref{exe.cauchy-det-lem} is the equality (11.218) in \cite[ancillary
PDF file]{Reiner}.

\subsection{Solution to Exercise \ref{exe.det.schur-lem}}

\begin{proof}
[Solution to Exercise \ref{exe.det.schur-lem}.]For every $n\times n$-matrix
$\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, we have%
\begin{equation}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod\limits_{i=1}%
^{n}c_{\sigma\left(  i\right)  ,i} \label{sol.det.schur-lem.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.schur-lem.1}):} Let $\left(
c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be any $n\times n$-matrix.
Then, Exercise \ref{exe.ps4.4} (applied to $A=\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$) yields%
\[
\det\left(  \left(  \left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  .
\]
Hence,%
\begin{align*}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
&  =\det\left(  \underbrace{\left(  \left(  c_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{\substack{=\left(  c_{j,i}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\\\text{(by the definition of the}\\\text{transpose of
a matrix)}}}\right)  =\det\left(  \left(  c_{j,i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
c_{\sigma\left(  i\right)  ,i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.det.eq.2}), applied to }\left(  c_{j,i}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }c_{j,i}\\
\text{instead of }A\text{ and }a_{i,j}%
\end{array}
\right)  .
\end{align*}
This proves (\ref{sol.det.schur-lem.1}).}. Applied to $\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, this yields%
\begin{equation}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod\limits_{i=1}%
^{n}a_{\sigma\left(  i\right)  ,i}. \label{sol.det.schur-lem.2}%
\end{equation}


Now, let $k\in\left\{  1,2,\ldots,n\right\}  $. For every $\sigma\in S_{n}$,
we have%
\begin{align}
\underbrace{\prod\limits_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }}b_{\sigma\left(  i\right)  }^{\delta_{i,k}}  &  =\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{\sigma\left(  i\right)  }%
^{\delta_{i,k}}=\underbrace{b_{\sigma\left(  k\right)  }^{\delta_{k,k}}%
}_{\substack{=b_{\sigma\left(  k\right)  }^{1}\\\text{(since }\delta
_{k,k}=1\text{)}}}\prod\limits_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\underbrace{b_{\sigma\left(  i\right)  }^{\delta_{i,k}}%
}_{\substack{=b_{\sigma\left(  i\right)  }^{0}\\\text{(since }\delta
_{i,k}=0\\\text{(since }i\neq k\text{))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the factor for
}i=k\text{ from the product}\right) \nonumber\\
&  =\underbrace{b_{\sigma\left(  k\right)  }^{1}}_{=b_{\sigma\left(  k\right)
}}\prod\limits_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}\underbrace{b_{\sigma\left(  i\right)  }^{0}}_{=1}=b_{\sigma\left(
k\right)  }\underbrace{\prod\limits_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq k}}1}_{=1}=b_{\sigma\left(  k\right)  }.
\label{sol.det.schur-lem.prod}%
\end{align}
But we can apply (\ref{sol.det.schur-lem.1}) to $\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$, and thus obtain%
\begin{align}
\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\sum\limits_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\underbrace{\prod\limits_{i=1}^{n}\left(  a_{\sigma\left(
i\right)  ,i}b_{\sigma\left(  i\right)  }^{\delta_{i,k}}\right)  }_{=\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  \left(
\prod\limits_{i=1}^{n}b_{\sigma\left(  i\right)  }^{\delta_{i,k}}\right)
}\nonumber\\
&  =\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)
\underbrace{\left(  \prod\limits_{i=1}^{n}b_{\sigma\left(  i\right)  }%
^{\delta_{i,k}}\right)  }_{\substack{=b_{\sigma\left(  k\right)  }\\\text{(by
(\ref{sol.det.schur-lem.prod}))}}}\nonumber\\
&  =\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  b_{\sigma\left(
k\right)  }. \label{sol.det.schur-lem.k-th-term}%
\end{align}


Now, let us forget that we fixed $k$. We thus have proven
(\ref{sol.det.schur-lem.k-th-term}) for every $k\in\left\{  1,2,\ldots
,n\right\}  $. On the other hand, for every $\sigma\in S_{n}$, we have%
\begin{align}
\underbrace{\sum_{k=1}^{n}}_{=\sum_{k\in\left\{  1,2,\ldots,n\right\}  }%
}b_{\sigma\left(  k\right)  }  &  =\sum_{k\in\left\{  1,2,\ldots,n\right\}
}b_{\sigma\left(  k\right)  }=\underbrace{\sum_{k\in\left\{  1,2,\ldots
,n\right\}  }}_{=\sum_{k=1}^{n}}b_{k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }k\text{ for }\sigma\left(  k\right)  \text{
in the sum,}\\
\text{since }\sigma\text{ is a bijection }\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\} \\
\text{(since }\sigma\text{ is a permutation of }\left\{  1,2,\ldots,n\right\}
\text{ (since }\sigma\in S_{n}\text{))}%
\end{array}
\right) \nonumber\\
&  =\sum_{k=1}^{n}b_{k}=b_{1}+b_{2}+\cdots+b_{n}.
\label{sol.det.schur-lem.sum-over-k}%
\end{align}
Now,%
\begin{align*}
&  \sum\limits_{k=1}^{n}\underbrace{\det\left(  \left(  a_{i,j}b_{i}%
^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
}_{\substack{=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  b_{\sigma\left(
k\right)  }\\\text{(by (\ref{sol.det.schur-lem.k-th-term}))}}}\\
&  =\sum_{k=1}^{n}\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\left(  \prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)
b_{\sigma\left(  k\right)  }=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\left(  \prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)
\underbrace{\sum_{k=1}^{n}b_{\sigma\left(  k\right)  }}_{\substack{=b_{1}%
+b_{2}+\cdots+b_{n}\\\text{(by (\ref{sol.det.schur-lem.sum-over-k}))}}}\\
&  =\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  \left(
b_{1}+b_{2}+\cdots+b_{n}\right)  =\left(  b_{1}+b_{2}+\cdots+b_{n}\right)
\underbrace{\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}}_{\substack{=\det\left(
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  \\\text{(by
(\ref{sol.det.schur-lem.2}))}}}\\
&  =\left(  b_{1}+b_{2}+\cdots+b_{n}\right)  \det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This solves Exercise \ref{exe.det.schur-lem}.
\end{proof}

\subsection{\label{sect.sols.vander-det.s1.sol2}Second solution to Exercise
\ref{exe.vander-det.s1}}

Exercise \ref{exe.det.schur-lem} can be used to give a new (and simpler)
solution to Exercise \ref{exe.vander-det.s1}, suggested by Karthik Karnik:

\begin{vershort}
\begin{proof}
[Second solution to Exercise \ref{exe.vander-det.s1}.]Exercise
\ref{exe.det.schur-lem} (applied to $a_{i,j}=x_{i}^{n-j}$ and $b_{i}=x_{i}$)
shows that%
\begin{align}
\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)   &  =\left(  x_{1}%
+x_{2}+\cdots+x_{n}\right)  \underbrace{\det\left(  \left(  x_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{\substack{=\prod
_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \\\text{(by Theorem
\ref{thm.vander-det} \textbf{(a)})}}}\nonumber\\
&  =\left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  . \label{sol.vander-det.s1.sol2.short.1}%
\end{align}


The left hand side of this equality is a sum of $n$ determinants. We shall now
show that $n-1$ of these determinants (namely, the ones that appear as addends
for $k>1$ in the sum) are $0$.

Indeed, every $k\in\left\{  2,3,\ldots,n\right\}  $ satisfies%
\begin{equation}
\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0 \label{sol.vander-det.s1.sol2.short.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.short.2}):} Let
$k\in\left\{  2,3,\ldots,n\right\}  $. Let $A$ be the $n\times n$-matrix
$\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$.
\par
Let $h\in\left\{  1,2,\ldots,n\right\}  $. Since $A=\left(  x_{i}^{n-j}%
x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, we have%
\begin{equation}
\left(  \text{the }\left(  h,k\right)  \text{-th entry of }A\right)
=x_{h}^{n-k}\underbrace{x_{h}^{\delta_{k,k}}}_{\substack{=x_{h}^{1}%
\\\text{(since }\delta_{k,k}=1\text{)}}}=x_{h}^{n-k}x_{h}^{1}=x_{h}^{n-k+1}.
\label{sol.vander-det.s1.sol2.short.2.pf.0}%
\end{equation}
On the other hand, $k-1\in\left\{  1,2,\ldots,n\right\}  $ (since
$k\in\left\{  2,3,\ldots,n\right\}  $), and thus the $\left(  h,k-1\right)
$-th entry of $A$ exists. Since $A=\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, this entry satisfies%
\[
\left(  \text{the }\left(  h,k-1\right)  \text{-th entry of }A\right)
=x_{h}^{n-\left(  k-1\right)  }\underbrace{x_{h}^{\delta_{k-1,k}}%
}_{\substack{=x_{h}^{0}\\\text{(since }\delta_{k-1,k}=0\text{)}}%
}=x_{h}^{n-\left(  k-1\right)  }x_{h}^{0}=x_{h}^{n-\left(  k-1\right)  }%
=x_{h}^{n-k+1}.
\]
Comparing this with (\ref{sol.vander-det.s1.sol2.short.2.pf.0}), we obtain%
\[
\left(  \text{the }\left(  h,k\right)  \text{-th entry of }A\right)  =\left(
\text{the }\left(  h,k-1\right)  \text{-th entry of }A\right)  .
\]
\par
Now, let us forget that we fixed $h$. We thus have shown that $\left(
\text{the }\left(  h,k\right)  \text{-th entry of }A\right)  =\left(
\text{the }\left(  h,k-1\right)  \text{-th entry of }A\right)  $ for every
$h\in\left\{  1,2,\ldots,n\right\}  $. In other words, the $k$-th column of
$A$ equals the $\left(  k-1\right)  $-st column of $A$. Thus, the matrix $A$
has two equal columns (since $k-1\neq k$). Therefore, Exercise \ref{exe.ps4.6}
\textbf{(f)} shows that $\det A=0$. Since $A=\left(  x_{i}^{n-j}x_{i}%
^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, this rewrites as
$\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0$. Qed.}. Furthermore, every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\label{sol.vander-det.s1.sol2.short.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.short.4}):} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. We need to prove
the equality (\ref{sol.vander-det.s1.sol2.short.4}). To do so, it clearly
satisfies to prove the following two claims:
\par
\textit{Claim 1:} If $j>1$, then $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=x_{i}^{n-j}%
$.
\par
\textit{Claim 2:} If $j=1$, then $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=x_{i}^{n}$.
\par
\textit{Proof of Claim 1:} Assume that $j>1$. Thus, $j\neq1$, so that
$\delta_{j,1}=0$, so that $x_{i}^{\delta_{j,1}}=x_{i}^{0}=1$. Hence,
$x_{i}^{n-j}\underbrace{x_{i}^{\delta_{j,1}}}_{=1}=x_{i}^{n-j}$, and thus
Claim 1 is proven.
\par
\textit{Proof of Claim 2:} Assume that $j=1$. Thus, $\delta_{j,1}=1$, so that
$x_{i}^{\delta_{j,1}}=x_{i}^{1}$ and thus $x_{i}^{n-j}\underbrace{x_{i}%
^{\delta_{j,1}}}_{=x_{i}^{1}}=x_{i}^{n-j}x_{i}^{1}=x_{i}^{\left(  n-j\right)
+1}=x_{i}^{\left(  n-1\right)  +1}$ (since $j=1$), so that $x_{i}^{n-j}%
x_{i}^{\delta_{j,1}}=x_{i}^{\left(  n-1\right)  +1}=x_{i}^{n}$. This proves
Claim 2.
\par
Hence, (\ref{sol.vander-det.s1.sol2.short.4}) is proven (since we have proven
Claims 1 and 2).}. Now, (\ref{sol.vander-det.s1.sol2.short.1}) shows that%
\begin{align*}
&  \left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right) \\
&  =\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\det\left(  \left(  \underbrace{x_{i}^{n-j}x_{i}^{\delta_{j,1}}%
}_{\substack{=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\\\text{(by (\ref{sol.vander-det.s1.sol2.short.4}))}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  +\sum\limits_{k=2}^{n}\underbrace{\det\left(
\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  }_{\substack{=0\\\text{(by (\ref{sol.vander-det.s1.sol2.short.2}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=1\text{ from the sum}\right) \\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  +\underbrace{\sum
\limits_{k=2}^{n}0}_{=0}\\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.s1} again.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Second solution to Exercise \ref{exe.vander-det.s1}.]Exercise
\ref{exe.det.schur-lem} (applied to $a_{i,j}=x_{i}^{n-j}$ and $b_{i}=x_{i}$)
shows that%
\begin{align}
\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)   &  =\left(  x_{1}%
+x_{2}+\cdots+x_{n}\right)  \underbrace{\det\left(  \left(  x_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{\substack{=\prod
_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \\\text{(by Theorem
\ref{thm.vander-det} \textbf{(a)})}}}\nonumber\\
&  =\left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  . \label{sol.vander-det.s1.sol2.1}%
\end{align}


Now, every $k\in\left\{  2,3,\ldots,n\right\}  $ satisfies%
\begin{equation}
\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0 \label{sol.vander-det.s1.sol2.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.2}):} Let
$k\in\left\{  2,3,\ldots,n\right\}  $. Let $A$ be the $n\times n$-matrix
$\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Thus,%
\begin{align}
\left(  \text{the }k\text{-th column of }A\right)   &  =\left(
\begin{array}
[c]{c}%
x_{1}^{n-k}x_{1}^{\delta_{k,k}}\\
x_{2}^{n-k}x_{2}^{\delta_{k,k}}\\
\vdots\\
x_{n}^{n-k}x_{n}^{\delta_{k,k}}%
\end{array}
\right)  =\left(  x_{i}^{n-k}\underbrace{x_{i}^{\delta_{k,k}}}%
_{\substack{=x_{i}^{1}\\\text{(since }\delta_{k,k}=1\\\text{(since
}k=k\text{))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\nonumber\\
&  =\left(  \underbrace{x_{i}^{n-k}x_{i}^{1}}_{=x_{i}^{\left(  n-k\right)
+1}}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}=\left(  x_{i}^{\left(
n-k\right)  +1}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\label{sol.vander-det.s1.sol2.2.pf.1}%
\end{align}
On the other hand, we have $k\in\left\{  2,3,\ldots,n\right\}  $, so that
$k-1\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $. Hence, the $\left(  k-1\right)  $-th column of $A$ exists. From
$A=\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$, we obtain%
\begin{align*}
\left(  \text{the }\left(  k-1\right)  \text{-th column of }A\right)   &
=\left(
\begin{array}
[c]{c}%
x_{1}^{n-\left(  k-1\right)  }x_{1}^{\delta_{k-1,k}}\\
x_{2}^{n-\left(  k-1\right)  }x_{2}^{\delta_{k-1,k}}\\
\vdots\\
x_{n}^{n-\left(  k-1\right)  }x_{n}^{\delta_{k-1,k}}%
\end{array}
\right)  =\left(  x_{i}^{n-\left(  k-1\right)  }\underbrace{x_{i}%
^{\delta_{k-1,k}}}_{\substack{=x_{i}^{0}\\\text{(since }\delta_{k-1,k}%
=0\\\text{(since }k-1\neq k\text{))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
1}\\
&  =\left(  \underbrace{x_{i}^{n-\left(  k-1\right)  }}_{=x_{i}^{\left(
n-k\right)  +1}}\underbrace{x_{i}^{0}}_{=1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}=\left(  x_{i}^{\left(  n-k\right)  +1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
&  =\left(  \text{the }k\text{-th column of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.vander-det.s1.sol2.2.pf.1}%
)}\right)  .
\end{align*}
Thus, the matrix $A$ has two equal columns (since $k-1\neq k$). Therefore,
Exercise \ref{exe.ps4.6} \textbf{(f)} shows that $\det A=0$. Since $A=\left(
x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
this rewrites as $\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  =0$. Qed.}. Furthermore, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\label{sol.vander-det.s1.sol2.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.4}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Thus, $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We need to
prove the equality (\ref{sol.vander-det.s1.sol2.4}).
\par
We distinguish between the following two cases:
\par
\textit{Case 1:} We have $j\neq1$.
\par
\textit{Case 2:} We have $j=1$.
\par
Let us first consider Case 1. In this case, we have $j\neq1$. Thus,
$\delta_{j,1}=0$, so that $x_{i}^{\delta_{j,1}}=x_{i}^{0}=1$. Since
$j\in\left\{  1,2,\ldots,n\right\}  $ and $j\neq1$, we have $j\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  1\right\}  =\left\{  2,3,\ldots
,n\right\}  $, so that $j>1$. Thus, $%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
=x_{i}^{n-j}$. Compared with $x_{i}^{n-j}\underbrace{x_{i}^{\delta_{j,1}}%
}_{=1}=x_{i}^{n-j}$, this yields $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
$. Thus, (\ref{sol.vander-det.s1.sol2.4}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $j=1$. Thus, $\delta
_{j,1}=1$, so that $x_{i}^{\delta_{j,1}}=x_{i}^{1}$ and thus $x_{i}%
^{n-j}\underbrace{x_{i}^{\delta_{j,1}}}_{=x_{i}^{1}}=x_{i}^{n-j}x_{i}%
^{1}=x_{i}^{\left(  n-j\right)  +1}=x_{i}^{\left(  n-1\right)  +1}$ (since
$j=1$), so that $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=x_{i}^{\left(  n-1\right)
+1}=x_{i}^{n}$. Compared with $%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
=x_{i}^{n}$ (since $j=1$), this yields $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
$. Thus, (\ref{sol.vander-det.s1.sol2.4}) is proven in Case 2.
\par
Now, we have proven (\ref{sol.vander-det.s1.sol2.4}) in each of the two Cases
1 and 2. Since these two Cases cover all possibilities, we thus see that
(\ref{sol.vander-det.s1.sol2.4}) always holds. Qed.}. Now,
(\ref{sol.vander-det.s1.sol2.1}) shows that%
\begin{align*}
&  \left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right) \\
&  =\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\det\left(  \left(  \underbrace{x_{i}^{n-j}x_{i}^{\delta_{j,1}}%
}_{\substack{=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\\\text{(by (\ref{sol.vander-det.s1.sol2.4}))}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  +\sum\limits_{k=2}^{n}\underbrace{\det\left(
\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  }_{\substack{=0\\\text{(by (\ref{sol.vander-det.s1.sol2.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=1\text{ from the sum}\right) \\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  +\underbrace{\sum
\limits_{k=2}^{n}0}_{=0}\\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.s1} again.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.det.a1a2anx}}

\begin{proof}
[First solution to Exercise \ref{exe.det.a1a2anx}.]The following solution will
rely on Exercise \ref{exe.ps4.6k} and \ref{exe.ps4.3}. Since this is not the
first time (nor the second) that we are using these exercises, I shall be brief.

Let $A$ be the $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix
$\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  $. We need to prove that $\det A=\left(  x+\sum_{i=1}^{n}%
a_{i}\right)  \prod_{i=1}^{n}\left(  x-a_{i}\right)  $.

We perform the following operations on the matrix $A$ (in this order):

\begin{itemize}
\item We subtract the $2$-nd row from the $1$-st row.

\item We subtract the $3$-rd row from the $2$-nd row.

\item $\ldots$

\item We subtract the $\left(  n+1\right)  $-th row from the $n$-th row.
\end{itemize}

As we know from Exercise \ref{exe.ps4.6k} \textbf{(a)}, these operations do
not change the determinant of the matrix. Thus, if we denote by $B$ the result
of these operations, then $\det B=\det A$. On the other hand, it is easy to
see that%
\[
B=\left(
\begin{array}
[c]{cccccc}%
x-a_{1} & a_{1}-x & 0 & \cdots & 0 & 0\\
0 & x-a_{2} & a_{2}-x & \cdots & 0 & 0\\
0 & 0 & x-a_{3} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & x-a_{n} & a_{n}-x\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  .
\]
(Here, for every $i\in\left\{  1,2,\ldots,n\right\}  $, the $i$-th row of $B$
has $i$-th entry $x-a_{i}$ and $\left(  i+1\right)  $-th entry $a_{i}-x$; all
other entries in this row are $0$. The $\left(  n+1\right)  $-th row of $B$ is
$\left(  a_{1},a_{2},\ldots,a_{n},x\right)  $.)

Next, we apply the following operations to the matrix $B$ (in this order):

\begin{itemize}
\item We add the $1$-st column to the $2$-nd column.

\item We add the $2$-nd column to the $3$-rd column.

\item $\ldots$

\item We add the $n$-th column to the $\left(  n+1\right)  $-th column.
\end{itemize}

(Notice that the order in which we are performing these operations forces
their effects to accumulate; namely, at every step, the column that we are
adding has already been modified by the previous step.) As we know from
Exercise \ref{exe.ps4.6k} \textbf{(b)}, these operations do not change the
determinant of the matrix. Thus, if we denote by $C$ the result of these
operations, then $\det C=\det B=\det A$. On the other hand, it is easy to see
that%
\[
C=\left(
\begin{array}
[c]{cccccc}%
x-a_{1} & 0 & 0 & \cdots & 0 & 0\\
0 & x-a_{2} & 0 & \cdots & 0 & 0\\
0 & 0 & x-a_{3} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & x-a_{n} & 0\\
a_{1} & a_{1}+a_{2} & a_{1}+a_{2}+a_{3} & \cdots & a_{1}+a_{2}+\cdots+a_{n} &
x+a_{1}+a_{2}+\cdots+a_{n}%
\end{array}
\right)  .
\]
This is a lower-triangular matrix. Thus, Exercise \ref{exe.ps4.3} shows that
its determinant is the product of its diagonal entries. In other words,%
\begin{align*}
\det C  &  =\left(  x-a_{1}\right)  \left(  x-a_{2}\right)  \cdots\left(
x-a_{n}\right)  \left(  x+a_{1}+a_{2}+\cdots+a_{n}\right) \\
&  =\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\end{align*}
Compared with $\det C=\det A$, this yields $\det A=\left(  x+\sum_{i=1}%
^{n}a_{i}\right)  \prod_{i=1}^{n}\left(  x-a_{i}\right)  $. This solves
Exercise \ref{exe.det.a1a2anx}.
\end{proof}

\begin{vershort}
\begin{proof}
[Second solution to Exercise \ref{exe.det.a1a2anx}.]For any $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define an element
$u_{i,j}\in\mathbb{K}$ by%
\[
u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
.
\]
This $u_{i,j}$ is well-defined\footnote{\textit{Proof.} It is sufficient to
check that $a_{j}$ is well-defined when $j<i$, and that $a_{j-1}$ is
well-defined when $j>i$ (because $x$ is always well-defined). But this is
easy:
\par
\begin{itemize}
\item If $j<i$, then $j\in\left\{  1,2,\ldots,n\right\}  $ (since $j<i\leq
n+1$ yields $j\leq n$), and thus $a_{j}$ is well-defined.
\par
\item If $j>i$, then $j\in\left\{  2,3,\ldots,n+1\right\}  $ (since $j>i\geq1$
yields $j\geq2$), and thus $a_{j-1}$ is well-defined.
\end{itemize}
}. Now, we define an $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix
$U$ by $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Thus,%
\[
U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  .
\]
Our goal is now to prove that $\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  $.

For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define
an element $s_{i,j}\in\mathbb{K}$ by%
\[
s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
.
\]
Now, we define an $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix $S$
by $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. The
matrix $S$ is upper-triangular\footnote{It looks as follows: $S=\left(
\begin{array}
[c]{cccc}%
1 & 1 & \cdots & 1\\
0 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  $.}. Since the determinant of an upper-triangular matrix is the
product of its diagonal entries, we thus obtain $\det S=1\cdot1\cdot
\cdots\cdot1=1$.

We extend the $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{K}^{n}$ to an $\left(  n+1\right)  $-tuple $\left(  a_{1}%
,a_{2},\ldots,a_{n+1}\right)  \in\mathbb{K}^{n+1}$ by setting $a_{n+1}=0$.
Thus, an element $a_{k}\in\mathbb{K}$ is defined for every $k\in\left\{
1,2,\ldots,n+1\right\}  $.

Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.short.US}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.US}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.short.US}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$1\leq i\leq n+1$ and $1\leq j\leq n+1$. Now,%
\begin{align}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}  &  =\sum_{k=1}^{j}u_{i,k}\underbrace{s_{k,j}%
}_{\substack{=1\\\text{(by the definition of}\\s_{k,j}\text{, since }k\leq
j\text{)}}}+\sum_{k=j+1}^{n+1}u_{i,k}\underbrace{s_{k,j}}%
_{\substack{=0\\\text{(by the definition of}\\s_{k,j}\text{, since
}k>j\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq j\leq n+1\right)
\nonumber\\
&  =\sum_{k=1}^{j}u_{i,k}1+\underbrace{\sum_{k=j+1}^{n+1}u_{i,k}0}_{=0}%
=\sum_{k=1}^{j}u_{i,k}1=\sum_{k=1}^{j}u_{i,k}.
\label{sol.det.a1a2anx.short.US.pf.1}%
\end{align}
\par
Now, we must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq j$.
\par
\textit{Case 2:} We have $i>j$.
\par
Let us first consider Case 1. In this case, we have $i\leq j$. The definition
of $s_{i,j}$ shows that $s_{i,j}=1$ (since $i\leq j$). Therefore,%
\begin{equation}
\underbrace{\sum_{k=1}^{j}a_{k}}_{\substack{=\sum_{k=1}^{j-1}a_{k}%
+a_{j}\\\text{(here, we have split off the}\\\text{addend for }k=j\text{ from
the sum)}}}+\underbrace{s_{i,j}}_{=1}\left(  x-a_{j}\right)  =\sum_{k=1}%
^{j-1}a_{k}+a_{j}+\left(  x-a_{j}\right)  =\sum_{k=1}^{j-1}a_{k}+x.
\label{sol.det.a1a2anx.short.US.pf.3}%
\end{equation}
\par
We have $1\leq i\leq j$. Thus, $0\leq i-1\leq j-1$. Now,
(\ref{sol.det.a1a2anx.short.US.pf.1}) becomes%
\begin{align*}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}  &  =\sum_{k=1}^{j}u_{i,k}=\underbrace{\sum
_{k=1}^{i}u_{i,k}}_{\substack{=\sum_{k=1}^{i-1}u_{i,k}+u_{i,i}\\\text{(here,
we have split off the}\\\text{addend for }k=i\text{ from the sum)}}%
}+\sum_{k=i+1}^{j}u_{i,k}\\
&  =\sum_{k=1}^{i-1}\underbrace{u_{i,k}}_{\substack{=a_{k}\\\text{(by the
definition of }u_{i,k}\text{,}\\\text{since }k<i\text{)}}}+\underbrace{u_{i,i}%
}_{\substack{=x\\\text{(by the definition of }u_{i,i}\text{,}\\\text{since
}i=i\text{)}}}+\sum_{k=i+1}^{j}\underbrace{u_{i,k}}_{\substack{=a_{k-1}%
\\\text{(by the definition of }u_{i,k}\text{,}\\\text{since }k>i\text{)}}}\\
&  =\sum_{k=1}^{i-1}a_{k}+x+\underbrace{\sum_{k=i+1}^{j}a_{k-1}}%
_{\substack{=\sum_{k=i}^{j-1}a_{k}\\\text{(here, we have substituted }k\text{
for }k-1\text{ in the sum)}}}=\sum_{k=1}^{i-1}a_{k}+x+\sum_{k=i}^{j-1}a_{k}\\
&  =\underbrace{\sum_{k=1}^{i-1}a_{k}+\sum_{k=i}^{j-1}a_{k}}_{\substack{=\sum
_{k=1}^{j-1}a_{k}\\\text{(since }0\leq i-1\leq j-1\text{)}}}+x=\sum
_{k=1}^{j-1}a_{k}+x.
\end{align*}
Compared with (\ref{sol.det.a1a2anx.short.US.pf.3}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.short.US}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i>j$. Hence, $j<i$. The
definition of $s_{i,j}$ therefore shows that $s_{i,j}=0$. Hence,%
\begin{equation}
\sum_{k=1}^{j}a_{k}+\underbrace{s_{i,j}}_{=0}\left(  x-a_{j}\right)
=\sum_{k=1}^{j}a_{k}+\underbrace{0\left(  x-a_{j}\right)  }_{=0}=\sum
_{k=1}^{j}a_{k}. \label{sol.det.a1a2anx.short.US.pf.5}%
\end{equation}
\par
But (\ref{sol.det.a1a2anx.short.US.pf.1}) becomes%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}\underbrace{u_{i,k}%
}_{\substack{=a_{k}\\\text{(by the definition of }u_{i,k}\text{,}\\\text{since
}k\leq j<i\text{)}}}=\sum_{k=1}^{j}a_{k}.
\]
Compared with (\ref{sol.det.a1a2anx.short.US.pf.5}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.short.US}) is proven in Case 2.
\par
We have thus proven (\ref{sol.det.a1a2anx.short.US}) in each of the two Cases
1 and 2. Hence, (\ref{sol.det.a1a2anx.short.US}) always holds.}.

For any two objects $i$ and $j$, we define an element $\delta_{i,j}%
\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
define an element $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=\delta_{i,j}\left(  x-a_{j}\right)  +\delta_{i,n+1}\sum_{k=1}^{j}%
a_{k}.
\]
Let $C$ be the $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix defined
by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\]


Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.short.SC}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.SC}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.short.SC}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$1\leq i\leq n+1$ and $1\leq j\leq n+1$.
\par
The definition of $s_{i,n+1}$ yields $s_{i,n+1}=1$ (since $i\leq n+1$). Now,%
\begin{align*}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}  &  =\underbrace{\sum_{r=1}^{n+1}}_{=\sum
_{r\in\left\{  1,2,\ldots,n+1\right\}  }}s_{i,r}\underbrace{c_{r,j}%
}_{\substack{=\delta_{r,j}\left(  x-a_{j}\right)  +\delta_{r,n+1}\sum
_{k=1}^{j}a_{k}\\\text{(by the definition of }c_{r,j}\text{)}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }r\right) \\
&  =\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}\left(  \delta
_{r,j}\left(  x-a_{j}\right)  +\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\right) \\
&  =\underbrace{\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}%
\delta_{r,j}\left(  x-a_{j}\right)  }_{\substack{=s_{i,j}\delta_{j,j}\left(
x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
j}}s_{i,r}\delta_{r,j}\left(  x-a_{j}\right)  \\\text{(here, we have split off
the addend for }r=j\text{ from the sum)}}}+\underbrace{\sum_{r\in\left\{
1,2,\ldots,n+1\right\}  }s_{i,r}\delta_{r,n+1}\sum_{k=1}^{j}a_{k}%
}_{\substack{=s_{i,n+1}\delta_{n+1,n+1}\sum_{k=1}^{j}a_{k}+\sum
_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}%
\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\\\text{(here, we have split off the addend
for }r=n+1\text{ from the sum)}}}\\
&  =s_{i,j}\underbrace{\delta_{j,j}}_{\substack{=1\\\text{(since }j=j\text{)}%
}}\left(  x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots
,n+1\right\}  ;\\r\neq j}}s_{i,r}\underbrace{\delta_{r,j}}%
_{\substack{=0\\\text{(since }r\neq j\text{)}}}\left(  x-a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{s_{i,n+1}}_{=1}\underbrace{\delta
_{n+1,n+1}}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{j}%
a_{k}+\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
n+1}}s_{i,r}\underbrace{\delta_{r,n+1}}_{\substack{=0\\\text{(since }r\neq
n+1\text{)}}}\sum_{k=1}^{j}a_{k}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq j}}s_{i,r}0\left(  x-a_{j}\right)  }%
_{=0}+\sum_{k=1}^{j}a_{k}+\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}0\sum_{k=1}^{j}a_{k}}_{=0}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\sum_{k=1}^{j}a_{k}=\sum_{k=1}^{j}%
a_{k}+s_{i,j}\left(  x-a_{j}\right)  .
\end{align*}
Thus, (\ref{sol.det.a1a2anx.short.SC}) is proven.}. Thus, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  =\sum_{k=1}^{n+1}u_{i,k}s_{k,j}
\label{sol.det.a1a2anx.short.SCvsUS}%
\end{equation}
(by (\ref{sol.det.a1a2anx.short.US})).

For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
c_{i,i}=x-a_{i} \label{sol.det.a1a2anx.short.cii}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.cii}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Thus, $i\neq n+1$, so that
$\delta_{i,n+1}=0$. Now, the definition of $c_{i,i}$ yields%
\[
c_{i,i}=\underbrace{\delta_{i,i}}_{\substack{=1\\\text{(since }i=i\text{)}%
}}\left(  x-a_{i}\right)  +\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{i}%
a_{k}=\left(  x-a_{i}\right)  +\underbrace{0\sum_{k=1}^{i}a_{k}}_{=0}%
=x-a_{i}.
\]
This proves (\ref{sol.det.a1a2anx.short.cii}).}. Also,%
\begin{equation}
c_{n+1,n+1}=x+\sum_{i=1}^{n}a_{i} \label{sol.det.a1a2anx.short.cn+1n+1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.cn+1n+1}):} The
definition of $c_{n+1,n+1}$ yields%
\begin{align*}
c_{n+1,n+1}  &  =\underbrace{\delta_{n+1,n+1}}_{\substack{=1\\\text{(since
}n+1=n+1\text{)}}}\left(  x-a_{n+1}\right)  +\underbrace{\delta_{n+1,n+1}%
}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{n+1}a_{k}=\left(
x-a_{n+1}\right)  +\sum_{k=1}^{n+1}a_{k}\\
&  =x+\underbrace{\left(  \sum_{k=1}^{n+1}a_{k}-a_{n+1}\right)  }_{=\sum
_{k=1}^{n}a_{k}}=x+\sum_{k=1}^{n}a_{k}=x+\sum_{i=1}^{n}a_{i}%
\end{align*}
(here, we have renamed the summation index $k$ as $i$). This proves
(\ref{sol.det.a1a2anx.short.cn+1n+1}).}.

But we have $c_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ be such that $i<j$. Thus, $1\leq i<j\leq n+1$,
so that $i<n+1$. Hence, $i\neq n+1$ and thus $\delta_{i,n+1}=0$. Also, $i<j$,
so that $i\neq j$ and thus $\delta_{i,j}=0$. Now, the definition of $c_{i,j}$
yields $c_{i,j}=\underbrace{\delta_{i,j}}_{=0}\left(  x-a_{j}\right)
+\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{j}a_{k}=0\left(  x-a_{j}\right)
+0\sum_{k=1}^{j}a_{k}=0$, qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to
$n+1$, $C$ and $c_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n+1,n+1}=\prod_{i=1}^{n+1}c_{i,i}%
\nonumber\\
&  =\left(  \prod_{i=1}^{n}\underbrace{c_{i,i}}_{\substack{=x-a_{i}\\\text{(by
(\ref{sol.det.a1a2anx.short.cii}))}}}\right)  \underbrace{c_{n+1,n+1}%
}_{\substack{=x+\sum_{i=1}^{n}a_{i}\\\text{(by
(\ref{sol.det.a1a2anx.short.cn+1n+1}))}}}\nonumber\\
&  =\left(  \prod_{i=1}^{n}\left(  x-a_{i}\right)  \right)  \left(
x+\sum_{i=1}^{n}a_{i}\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  . \label{sol.det.a1a2anx.short.detC}%
\end{align}


But recall that $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\begin{equation}
SC=\left(  \underbrace{\sum_{k=1}^{n+1}s_{i,k}c_{k,j}}_{\substack{=\sum
_{k=1}^{n+1}u_{i,k}s_{k,j}\\\text{(by (\ref{sol.det.a1a2anx.short.SCvsUS}))}%
}}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(  \sum_{k=1}%
^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\label{sol.det.a1a2anx.short.3}%
\end{equation}


On the other hand, $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\[
US=\left(  \sum_{k=1}^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq
j\leq n+1}=SC\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.a1a2anx.short.3})}\right)  .
\]


Thus,%
\begin{align*}
\det\left(  \underbrace{US}_{=SC}\right)   &  =\det\left(  SC\right)
=\underbrace{\det S}_{=1}\cdot\det C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }S\text{ and }C\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det C=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.a1a2anx.short.detC})}\right)  .
\end{align*}
Compared with%
\begin{align*}
\det\left(  US\right)   &  =\det U\cdot\underbrace{\det S}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }U\text{ and }S\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det U,
\end{align*}
this yields
\[
\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]
This solves Exercise \ref{exe.det.a1a2anx}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Second solution to Exercise \ref{exe.det.a1a2anx}.]For any $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define an element
$u_{i,j}\in\mathbb{K}$ by%
\[
u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
.
\]
This $u_{i,j}$ is well-defined\footnote{\textit{Proof.} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus, $i\in\left\{
1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots,n+1\right\}  $. We must
prove that $u_{i,j}$ is well-defined.
\par
We are in one of the following three cases:
\par
\textit{Case 1:} We have $j<i$.
\par
\textit{Case 2:} We have $j=i$.
\par
\textit{Case 3:} We have $j>i$.
\par
Let us first consider Case 1. In this case, we have $j<i$. Thus, $j<i\leq n+1$
(since $i\in\left\{  1,2,\ldots,n+1\right\}  $), so that $j\leq\left(
n+1\right)  -1$ (since $j$ and $n+1$ are integers). Thus, $j\leq\left(
n+1\right)  -1=n$. Combined with $j\geq1$ (since $j\in\left\{  1,2,\ldots
,n+1\right\}  $), this shows that $1\leq j\leq n$. In other words,
$j\in\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{j}$ is well-defined. Now,
the definition of $u_{i,j}$ says that $u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
=a_{j}$ (since $j<i$). Hence, $u_{i,j}$ is well-defined (since $a_{j}$ is
well-defined). Thus, in Case 1, we have proven that $u_{i,j}$ is well-defined.
\par
Let us next consider Case 2. In this case, we have $j=i$. Thus, the definition
of $u_{i,j}$ says that $u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
=x$ (since $j=i$). Hence, $u_{i,j}$ is well-defined (since $x$ is
well-defined). Thus, in Case 2, we have proven that $u_{i,j}$ is well-defined.
\par
Let us finally consider Case 3. In this case, we have $j>i$. Thus, $j>i\geq1$
(since $i\in\left\{  1,2,\ldots,n+1\right\}  $), so that $j-1>0$ and thus
$j-1\geq1$ (since $j-1$ is an integer). Also, $j\in\left\{  1,2,\ldots
,n+1\right\}  $, so that $j\leq n+1$ and thus $j-1\leq n$. Combined with
$j-1\geq1$, this yields $1\leq j-1\leq n$. In other words, $j-1\in\left\{
1,2,\ldots,n\right\}  $. Hence, $a_{j-1}$ is well-defined. Now, the definition
of $u_{i,j}$ says that $u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
=a_{j-1}$ (since $j>i$). Hence, $u_{i,j}$ is well-defined (since $a_{j-1}$ is
well-defined). Thus, in Case 3, we have proven that $u_{i,j}$ is well-defined.
\par
Now, in each of our three Cases 1, 2 and 3, we have shown that $u_{i,j}$ is
well-defined. Since these three Cases cover all possibilities, we thus
conclude that $u_{i,j}$ is always well-defined. Qed.}. Now, we define an
$\left(  n+1\right)  \times\left(  n+1\right)  $-matrix $U$ by $U=\left(
u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Thus,%
\begin{align*}
U  &  =\left(  \underbrace{u_{i,j}}_{=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}\\
&  =\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  .
\end{align*}
Our goal is now to prove that $\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  $.

For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define
an element $s_{i,j}\in\mathbb{K}$ by%
\[
s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
.
\]
Now, we define an $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix $S$
by $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Then,
$\det S=1$\ \ \ \ \footnote{\textit{Proof.} We have $S=\left(  s_{i,j}\right)
_{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Therefore, the definition of the
transpose of a matrix yields $S^{T}=\left(  s_{j,i}\right)  _{1\leq i\leq
n+1,\ 1\leq j\leq n+1}$.
\par
Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$ be
such that $i<j$. Then, $j>i$ (since $i<j$). Now, the definition of $s_{j,i}$
yields $s_{j,i}=%
\begin{cases}
1, & \text{if }j\leq i;\\
0, & \text{if }j>i
\end{cases}
=0$ (since $j>i$).
\par
Let us now forget that we fixed $\left(  i,j\right)  $. We thus have shown
that $s_{j,i}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n+1\right\}  ^{2}$ satisfying $i<j$. Therefore, Exercise \ref{exe.ps4.3}
(applied to $n+1$, $S^{T}$ and $s_{j,i}$ instead of $n$, $A$ and $a_{i,j}$)
shows that
\[
\det\left(  S^{T}\right)  =s_{1,1}s_{2,2}\cdots s_{n+1,n+1}=\prod_{i=1}%
^{n+1}\underbrace{s_{i,i}}_{\substack{=%
\begin{cases}
1, & \text{if }i\leq i;\\
0, & \text{if }i>i
\end{cases}
\\\text{(by the definition of }s_{i,i}\text{)}}}=\prod_{i=1}^{n+1}\underbrace{%
\begin{cases}
1, & \text{if }i\leq i;\\
0, & \text{if }i>i
\end{cases}
}_{\substack{=1\\\text{(since }i\leq i\text{)}}}=\prod_{i=1}^{n+1}1=1.
\]
But Exercise \ref{exe.ps4.4} (applied to $n+1$ and $S$ instead of $n$ and $A$)
shows that $\det\left(  S^{T}\right)  =\det S$. Comparing this with
$\det\left(  S^{T}\right)  =1$, we obtain $\det S=1$, qed.}.

We extend the $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{K}^{n}$ to an $\left(  n+1\right)  $-tuple $\left(  a_{1}%
,a_{2},\ldots,a_{n+1}\right)  \in\mathbb{K}^{n+1}$ by setting $a_{n+1}=0$.
Thus, an element $a_{k}\in\mathbb{K}$ is defined for every $k\in\left\{
1,2,\ldots,n+1\right\}  $.

Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.US}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.US}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.US}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots
,n+1\right\}  $. From $j\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq j\leq n+1$. From $i\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq i\leq n+1$. Now,%
\begin{align}
\sum_{k=1}^{n+1}u_{i,k}\underbrace{s_{k,j}}_{\substack{=%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
\\\text{(by the definition of }s_{k,j}\text{)}}}  &  =\sum_{k=1}^{n+1}u_{i,k}%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
\nonumber\\
&  =\sum_{k=1}^{j}u_{i,k}\underbrace{%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
}_{\substack{=1\\\text{(since }k\leq j\text{)}}}+\sum_{k=j+1}^{n+1}%
u_{i,k}\underbrace{%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
}_{\substack{=0\\\text{(since }k>j\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq j\leq n+1\right) \nonumber\\
&  =\sum_{k=1}^{j}u_{i,k}1+\underbrace{\sum_{k=j+1}^{n+1}u_{i,k}0}_{=0}%
=\sum_{k=1}^{j}u_{i,k}1=\sum_{k=1}^{j}u_{i,k}. \label{sol.det.a1a2anx.US.pf.1}%
\end{align}
\par
Now, we must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq j$.
\par
\textit{Case 2:} We have $i>j$.
\par
Let us first consider Case 1. In this case, we have $i\leq j$. The definition
of $s_{i,j}$ shows that $s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
=1$ (since $i\leq j$). Therefore,%
\begin{equation}
\underbrace{\sum_{k=1}^{j}a_{k}}_{\substack{=\sum_{k=1}^{j-1}a_{k}%
+a_{j}\\\text{(here, we have split off the}\\\text{addend for }k=j\text{ from
the sum)}}}+\underbrace{s_{i,j}}_{=1}\left(  x-a_{j}\right)  =\sum_{k=1}%
^{j-1}a_{k}+a_{j}+\left(  x-a_{j}\right)  =\sum_{k=1}^{j-1}a_{k}+x.
\label{sol.det.a1a2anx.US.pf.3}%
\end{equation}
\par
We have $1\leq i\leq j$. Thus, $0\leq i-1\leq j-1$. Now,
(\ref{sol.det.a1a2anx.US.pf.1}) becomes%
\begin{align*}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}  &  =\sum_{k=1}^{j}u_{i,k}=\underbrace{\sum
_{k=1}^{i}u_{i,k}}_{\substack{=\sum_{k=1}^{i-1}u_{i,k}+u_{i,i}\\\text{(here,
we have split off the}\\\text{addend for }k=i\text{ from the sum)}}%
}+\sum_{k=i+1}^{j}u_{i,k}\\
&  =\sum_{k=1}^{i-1}\underbrace{u_{i,k}}_{\substack{=%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
\\\text{(by the definition of }u_{i,k}\text{)}}}+\underbrace{u_{i,i}%
}_{\substack{=%
\begin{cases}
a_{i}, & \text{if }i<i;\\
x, & \text{if }i=i;\\
a_{i-1}, & \text{if }i>i
\end{cases}
\\\text{(by the definition of }u_{i,i}\text{)}}}+\sum_{k=i+1}^{j}%
\underbrace{u_{i,k}}_{\substack{=%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
\\\text{(by the definition of }u_{i,k}\text{)}}}\\
&  =\sum_{k=1}^{i-1}\underbrace{%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
}_{\substack{=a_{k}\\\text{(since }k<i\text{)}}}+\underbrace{%
\begin{cases}
a_{i}, & \text{if }i<i;\\
x, & \text{if }i=i;\\
a_{i-1}, & \text{if }i>i
\end{cases}
}_{\substack{=x\\\text{(since }i=i\text{)}}}+\sum_{k=i+1}^{j}\underbrace{%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
}_{\substack{=a_{k-1}\\\text{(since }k>i\text{)}}}\\
&  =\sum_{k=1}^{i-1}a_{k}+x+\underbrace{\sum_{k=i+1}^{j}a_{k-1}}%
_{\substack{=\sum_{k=i}^{j-1}a_{k}\\\text{(here, we have substituted }k\text{
for }k-1\text{ in the sum)}}}=\sum_{k=1}^{i-1}a_{k}+x+\sum_{k=i}^{j-1}a_{k}\\
&  =\underbrace{\sum_{k=1}^{i-1}a_{k}+\sum_{k=i}^{j-1}a_{k}}_{\substack{=\sum
_{k=1}^{j-1}a_{k}\\\text{(since }0\leq i-1\leq j-1\text{)}}}+x=\sum
_{k=1}^{j-1}a_{k}+x.
\end{align*}
Compared with (\ref{sol.det.a1a2anx.US.pf.3}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.US}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i>j$. Hence, $j<i$. The
definition of $s_{i,j}$ shows that $s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
=0$ (since $i>j$). Therefore,%
\begin{equation}
\sum_{k=1}^{j}a_{k}+\underbrace{s_{i,j}}_{=0}\left(  x-a_{j}\right)
=\sum_{k=1}^{j}a_{k}+\underbrace{0\left(  x-a_{j}\right)  }_{=0}=\sum
_{k=1}^{j}a_{k}. \label{sol.det.a1a2anx.US.pf.5}%
\end{equation}
\par
But (\ref{sol.det.a1a2anx.US.pf.1}) becomes%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}\underbrace{u_{i,k}}_{\substack{=%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
\\\text{(by the definition of }u_{i,k}\text{)}}}=\sum_{k=1}^{j}\underbrace{%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
}_{\substack{=a_{k}\\\text{(since }k\leq j<i\text{)}}}=\sum_{k=1}^{j}a_{k}.
\]
Compared with (\ref{sol.det.a1a2anx.US.pf.5}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.US}) is proven in Case 2.
\par
We have thus proven (\ref{sol.det.a1a2anx.US}) in each of the two Cases 1 and
2. Since these two Cases cover all possibilities, this shows that
(\ref{sol.det.a1a2anx.US}) always holds. Qed.}.

For any two objects $i$ and $j$, we define an element $\delta_{i,j}%
\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
define an element $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=\delta_{i,j}\left(  x-a_{j}\right)  +\delta_{i,n+1}\sum_{k=1}^{j}%
a_{k}.
\]
Let $C$ be the $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix defined
by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\]


Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.SC}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.SC}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.SC}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots
,n+1\right\}  $. From $j\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq j\leq n+1$. From $i\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq i\leq n+1$.
\par
The definition of $s_{i,n+1}$ yields $s_{i,n+1}=%
\begin{cases}
1, & \text{if }i\leq n+1;\\
0, & \text{if }i>n+1
\end{cases}
=1$ (since $i\leq n+1$). Now,%
\begin{align*}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}  &  =\underbrace{\sum_{r=1}^{n+1}}_{=\sum
_{r\in\left\{  1,2,\ldots,n+1\right\}  }}s_{i,r}\underbrace{c_{r,j}%
}_{\substack{=\delta_{r,j}\left(  x-a_{j}\right)  +\delta_{r,n+1}\sum
_{k=1}^{j}a_{k}\\\text{(by the definition of }c_{r,j}\text{)}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }r\right) \\
&  =\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}\left(  \delta
_{r,j}\left(  x-a_{j}\right)  +\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\right) \\
&  =\underbrace{\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}%
\delta_{r,j}\left(  x-a_{j}\right)  }_{\substack{=s_{i,j}\delta_{j,j}\left(
x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
j}}s_{i,r}\delta_{r,j}\left(  x-a_{j}\right)  \\\text{(here, we have split off
the addend for }r=j\text{ from the sum)}}}+\underbrace{\sum_{r\in\left\{
1,2,\ldots,n+1\right\}  }s_{i,r}\delta_{r,n+1}\sum_{k=1}^{j}a_{k}%
}_{\substack{=s_{i,n+1}\delta_{n+1,n+1}\sum_{k=1}^{j}a_{k}+\sum
_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}%
\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\\\text{(here, we have split off the addend
for }r=n+1\text{ from the sum)}}}\\
&  =s_{i,j}\underbrace{\delta_{j,j}}_{\substack{=1\\\text{(since }j=j\text{)}%
}}\left(  x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots
,n+1\right\}  ;\\r\neq j}}s_{i,r}\underbrace{\delta_{r,j}}%
_{\substack{=0\\\text{(since }r\neq j\text{)}}}\left(  x-a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{s_{i,n+1}}_{=1}\underbrace{\delta
_{n+1,n+1}}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{j}%
a_{k}+\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
n+1}}s_{i,r}\underbrace{\delta_{r,n+1}}_{\substack{=0\\\text{(since }r\neq
n+1\text{)}}}\sum_{k=1}^{j}a_{k}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq j}}s_{i,r}0\left(  x-a_{j}\right)  }%
_{=0}+\sum_{k=1}^{j}a_{k}+\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}0\sum_{k=1}^{j}a_{k}}_{=0}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\sum_{k=1}^{j}a_{k}=\sum_{k=1}^{j}%
a_{k}+s_{i,j}\left(  x-a_{j}\right)  .
\end{align*}
Thus, (\ref{sol.det.a1a2anx.SC}) is proven.}. Thus, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  =\sum_{k=1}^{n+1}u_{i,k}s_{k,j} \label{sol.det.a1a2anx.SCvsUS}%
\end{equation}
(by (\ref{sol.det.a1a2anx.US})).

For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
c_{i,i}=x-a_{i} \label{sol.det.a1a2anx.cii}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.cii}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $. Thus, $1\leq i\leq n$, so that $i\leq n<n+1$, and
therefore $i\neq n+1$. Hence, $\delta_{i,n+1}=0$. Now, the definition of
$c_{i,i}$ yields%
\[
c_{i,i}=\underbrace{\delta_{i,i}}_{\substack{=1\\\text{(since }i=i\text{)}%
}}\left(  x-a_{i}\right)  +\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{i}%
a_{k}=\left(  x-a_{i}\right)  +\underbrace{0\sum_{k=1}^{i}a_{k}}_{=0}%
=x-a_{i}.
\]
This proves (\ref{sol.det.a1a2anx.cii}).}. Also,%
\begin{equation}
c_{n+1,n+1}=x+\sum_{i=1}^{n}a_{i} \label{sol.det.a1a2anx.cn+1n+1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.cn+1n+1}):} The definition of
$c_{n+1,n+1}$ yields%
\begin{align*}
c_{n+1,n+1}  &  =\underbrace{\delta_{n+1,n+1}}_{\substack{=1\\\text{(since
}n+1=n+1\text{)}}}\left(  x-a_{n+1}\right)  +\underbrace{\delta_{n+1,n+1}%
}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{n+1}a_{k}=\left(
x-a_{n+1}\right)  +\underbrace{\sum_{k=1}^{n+1}a_{k}}_{\substack{=\sum
_{k=1}^{n}a_{k}+a_{n+1}\\\text{(here, we have split off the}\\\text{addend for
}k=n+1\text{ from the sum)}}}\\
&  =\left(  x-a_{n+1}\right)  +\sum_{k=1}^{n}a_{k}+a_{n+1}=x+\sum_{k=1}%
^{n}a_{k}=x+\sum_{i=1}^{n}a_{i}%
\end{align*}
(here, we have renamed the summation index $k$ as $i$). This proves
(\ref{sol.det.a1a2anx.cn+1n+1}).}.

But we have $c_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ be such that $i<j$. We have $\left(  i,j\right)
\in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, so that $i\in\left\{
1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots,n+1\right\}  $. From
$j\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain $1\leq j\leq n+1$. Thus,
$i<j\leq n+1$, so that $i\neq n+1$ and thus $\delta_{i,n+1}=0$. Also, $i<j$,
so that $i\neq j$ and thus $\delta_{i,j}=0$. Now, the definition of $c_{i,j}$
yields $c_{i,j}=\underbrace{\delta_{i,j}}_{=0}\left(  x-a_{j}\right)
+\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{j}a_{k}=0\left(  x-a_{j}\right)
+0\sum_{k=1}^{j}a_{k}=0$, qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to
$n+1$, $C$ and $c_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n+1,n+1}=\prod_{i=1}^{n+1}c_{i,i}%
\nonumber\\
&  =\left(  \prod_{i=1}^{n}\underbrace{c_{i,i}}_{\substack{=x-a_{i}\\\text{(by
(\ref{sol.det.a1a2anx.cii}))}}}\right)  \underbrace{c_{n+1,n+1}}%
_{\substack{=x+\sum_{i=1}^{n}a_{i}\\\text{(by (\ref{sol.det.a1a2anx.cn+1n+1}%
))}}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the factor}\\
\text{for }i=n+1\text{ from the product}%
\end{array}
\right) \nonumber\\
&  =\left(  \prod_{i=1}^{n}\left(  x-a_{i}\right)  \right)  \left(
x+\sum_{i=1}^{n}a_{i}\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  . \label{sol.det.a1a2anx.detC}%
\end{align}


But recall that $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\begin{equation}
SC=\left(  \underbrace{\sum_{k=1}^{n+1}s_{i,k}c_{k,j}}_{\substack{=\sum
_{k=1}^{n+1}u_{i,k}s_{k,j}\\\text{(by (\ref{sol.det.a1a2anx.SCvsUS}))}%
}}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(  \sum_{k=1}%
^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\label{sol.det.a1a2anx.3}%
\end{equation}


On the other hand, $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\[
US=\left(  \sum_{k=1}^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq
j\leq n+1}=SC\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.det.a1a2anx.3}%
)}\right)  .
\]


Thus,%
\begin{align*}
\det\left(  \underbrace{US}_{=SC}\right)   &  =\det\left(  SC\right)
=\underbrace{\det S}_{=1}\cdot\det C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }S\text{ and }C\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det C=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.a1a2anx.detC})}\right)  .
\end{align*}
Compared with%
\begin{align*}
\det\left(  US\right)   &  =\det U\cdot\underbrace{\det S}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }U\text{ and }S\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det U,
\end{align*}
this yields
\[
\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]
Since $U=\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  $, this rewrites as%
\[
\det\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]
This solves Exercise \ref{exe.det.a1a2anx}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.det.2diags}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.det.2diags}.]Let $z$ be the permutation
$\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$ (where we are using the
notations of Definition \ref{def.perm.cycles}). Then, every $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
z\left(  i\right)  =%
\begin{cases}
i-1, & \text{if }i>1;\\
n, & \text{if }i=1
\end{cases}
. \label{sol.det.2diags.short.z}%
\end{equation}
(This follows easily from the definition of $z$.) In particular, $z\left(
1\right)  =n\neq1$ (since $n>1$), and thus $z\neq\operatorname*{id}$. Every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align}
z\left(  i\right)   &  =%
\begin{cases}
i-1, & \text{if }i>1;\\
n, & \text{if }i=1
\end{cases}
\nonumber\\
&  \equiv%
\begin{cases}
i-1, & \text{if }i>1;\\
i-1, & \text{if }i=1
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }n\equiv0=\underbrace{1}_{=i}-1=i-1\operatorname{mod}n\\
\text{in the case when }i=1
\end{array}
\right) \nonumber\\
&  =i-1\operatorname{mod}n. \label{sol.det.2diags.short.zmod}%
\end{align}
Notice also that $z=\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$, so that
$\left(  -1\right)  ^{z}=\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{n,n-1,\ldots,1}}=\left(  -1\right)  ^{n-1}$ (by Exercise
\ref{exe.perm.cycles} \textbf{(d)}, applied to $k=n$ and $\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  =\left(  n,n-1,\ldots,1\right)  $).

We recall the following simple fact: If $p$ and $q$ are two elements of
$\left\{  1,2,\ldots,n\right\}  $ such that $p\equiv q\operatorname{mod}n$,
then $p=q$. We shall use this fact several times (tacitly) in the following arguments.

Now, I claim that if $\sigma\in S_{n}$ satisfies $\sigma\notin\left\{
\operatorname*{id},z\right\}  $, then%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists an }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying}\\
i\neq\sigma\left(  i\right)  \text{ and }i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n
\end{array}
\right)  . \label{sol.det.2diags.short.exi}%
\end{equation}


\textit{Proof of (\ref{sol.det.2diags.short.exi}):} Let $\sigma\in S_{n}$ be
such that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $. Thus,
$\sigma\neq\operatorname*{id}$ and $\sigma\neq z$.

We need to prove (\ref{sol.det.2diags.short.exi}). Indeed, let us assume the
contrary (for the sake of contradiction). Thus,%
\begin{equation}
\text{every }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfies either
}i=\sigma\left(  i\right)  \text{ or }i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n. \label{sol.det.2diags.short.exi.pf.ass}%
\end{equation}
\footnote{I use the words \textquotedblleft either\textquotedblleft%
/\textquotedblleft or\textquotedblright\ in a non-exclusive meaning (i.e.,
when I say \textquotedblleft either $\mathcal{A}$ or $\mathcal{B}%
$\textquotedblright, I mean to include also the case when both $\mathcal{A}$
and $\mathcal{B}$ hold simultaneously), but here it does not matter (because
we cannot have $i=\sigma\left(  i\right)  $ and $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$ at the same time).}

There exists a $J\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(
J\right)  \neq J$ (since $\sigma\neq\operatorname*{id}$). Let $j$ be the
smallest such $J$. Thus, $\sigma\left(  j\right)  \neq j$, but every $J<j$
satisfies $\sigma\left(  J\right)  =J$.

Applying (\ref{sol.det.2diags.short.exi.pf.ass}) to $i=j$, we see that either
$j=\sigma\left(  j\right)  $ or $j\equiv\sigma\left(  j\right)
+1\operatorname{mod}n$. Since $j=\sigma\left(  j\right)  $ cannot hold
(because we have $\sigma\left(  j\right)  \neq j$), we thus have
$j\equiv\sigma\left(  j\right)  +1\operatorname{mod}n$. In other words,
$\sigma\left(  j\right)  \equiv j-1\operatorname{mod}n$.

We have $j=1$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$j\neq1$, so that $j>1$. Hence, $j-1\in\left\{  1,2,\ldots,n\right\}  $. Also,
$j-1<j$. Hence, $\sigma\left(  j-1\right)  =j-1$ (since every $J<j$ satisfies
$\sigma\left(  J\right)  =J$). Thus, $\sigma\left(  j-1\right)  =j-1\equiv
\sigma\left(  j\right)  \operatorname{mod}n$. Since both $\sigma\left(
j-1\right)  $ and $\sigma\left(  j\right)  $ are elements of $\left\{
1,2,\ldots,n\right\}  $, this shows that $\sigma\left(  j-1\right)
=\sigma\left(  j\right)  $, and thus $j-1=j$ (since $\sigma$ is injective).
But this is absurd. This contradiction shows that our assumption was wrong,
qed.}. Thus, $\sigma\left(  \underbrace{1}_{=j}\right)  =\sigma\left(
j\right)  \equiv\underbrace{j}_{=1}-1=0\equiv n\operatorname{mod}n$. Since
both $\sigma\left(  1\right)  $ and $n$ belong to $\left\{  1,2,\ldots
,n\right\}  $, this shows that $\sigma\left(  1\right)  =n$.

There exists a $K\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(
K\right)  =K$\ \ \ \ \footnote{\textit{Proof.} We have $\sigma\neq z$. Hence,
there exists an $i\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(
i\right)  \neq z\left(  i\right)  $. Consider this $i$.
\par
If we had $i\equiv\sigma\left(  i\right)  +1\operatorname{mod}n$, then we
would have $\sigma\left(  i\right)  \equiv i-1\equiv z\left(  i\right)
\operatorname{mod}n$ (by (\ref{sol.det.2diags.short.zmod})), which would
entail $\sigma\left(  i\right)  =z\left(  i\right)  $ (since both
$\sigma\left(  i\right)  $ and $z\left(  i\right)  $ belong to $\left\{
1,2,\ldots,n\right\}  $); but this would contradict $\sigma\left(  i\right)
\neq z\left(  i\right)  $. Hence, we cannot have $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$.
\par
We have either $i=\sigma\left(  i\right)  $ or $i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n$ (because of (\ref{sol.det.2diags.short.exi.pf.ass})).
Thus, $i=\sigma\left(  i\right)  $ (since we cannot have $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$). Hence, there exists a $K\in\left\{
1,2,\ldots,n\right\}  $ such that $\sigma\left(  K\right)  =K$ (namely,
$K=i$). Qed.}. Let $k$ be the largest such $K$. Thus, $\sigma\left(  k\right)
=k$, but every $K>k$ satisfies $\sigma\left(  K\right)  \neq K$.

We have $n\neq1$ and thus $\sigma\left(  n\right)  \neq\sigma\left(  1\right)
$ (since $\sigma$ is injective). Thus, $\sigma\left(  n\right)  \neq
\sigma\left(  1\right)  =n$.

We cannot have $k=n$ (because otherwise, we would have $\sigma\left(
\underbrace{n}_{=k}\right)  =\sigma\left(  k\right)  =k=n$, which would
contradict $\sigma\left(  n\right)  \neq n$). Thus, $k<n$. Hence,
$k+1\in\left\{  1,2,\ldots,n\right\}  $. Therefore, $\sigma\left(  k+1\right)
\neq k+1$ (since every $K>k$ satisfies $\sigma\left(  K\right)  \neq K$, and
since $k+1>k$). Now, applying (\ref{sol.det.2diags.short.exi.pf.ass}) to
$i=k+1$, we conclude that either $k+1=\sigma\left(  k+1\right)  $ or
$k+1\equiv\sigma\left(  k+1\right)  +1\operatorname{mod}n$. Since we cannot
have $k+1=\sigma\left(  k+1\right)  $ (because $\sigma\left(  k+1\right)  \neq
k+1$), we thus must have $k+1\equiv\sigma\left(  k+1\right)
+1\operatorname{mod}n$. In other words, $k\equiv\sigma\left(  k+1\right)
\operatorname{mod}n$. Hence, $k=\sigma\left(  k+1\right)  $ (since both $k$
and $\sigma\left(  k+1\right)  $ belong to $\left\{  1,2,\ldots,n\right\}  $),
so that $\sigma\left(  k+1\right)  =k=\sigma\left(  k\right)  $. Since
$\sigma$ is injective, this yields $k+1=k$, which is absurd. This
contradiction shows that our assumption was wrong. Hence,
(\ref{sol.det.2diags.short.exi}) is proven.

Let us now write our matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. Then,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A=\left(
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
In other words, we have%
\begin{equation}
a_{i,j}=%
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\label{sol.det.2diags.short.aij}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

It is now easy to see that if $\sigma\in S_{n}$ satisfies $\sigma
\notin\left\{  \operatorname*{id},z\right\}  $, then
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.det.2diags.short.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.short.0}):} Let $\sigma\in
S_{n}$ be such that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $.
Thus, there exists an $i\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$i\neq\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n$ (because of (\ref{sol.det.2diags.short.exi})). Consider
this $i$.
\par
From (\ref{sol.det.2diags.short.aij}) (applied to $\left(  i,\sigma\left(
i\right)  \right)  $ instead of $\left(  i,j\right)  $), we obtain%
\[
a_{i,\sigma\left(  i\right)  }=%
\begin{cases}
a_{\sigma\left(  i\right)  }, & \text{if }i=\sigma\left(  i\right)  ;\\
b_{\sigma\left(  i\right)  }, & \text{if }i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
=0
\]
(since $i\neq\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(
i\right)  +1\operatorname{mod}n$).
\par
Now, let us forget that we fixed $i$. We thus have shown that there exists an
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i,\sigma\left(  i\right)
}=0$. In other words, at least one factor of the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$. Therefore, the whole product
$\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$. This proves
(\ref{sol.det.2diags.short.0}).}.

Now, (\ref{eq.det.eq.2}) becomes%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}\prod_{i=1}%
^{n}\underbrace{a_{i,\operatorname*{id}\left(  i\right)  }}_{=a_{i,i}%
}+\underbrace{\left(  -1\right)  ^{z}}_{=\left(  -1\right)  ^{n-1}}\prod
_{i=1}^{n}a_{i,z\left(  i\right)  }+\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=0\\\text{(by (\ref{sol.det.2diags.short.0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }\sigma=\operatorname*{id}\text{
and}\\
\text{for }\sigma=z\text{ from the sum (since }z\neq\operatorname*{id}\text{)}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n}a_{i,i}+\left(  -1\right)  ^{n-1}\prod_{i=1}^{n}%
a_{i,z\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)
^{\sigma}0}_{=0}\\
&  =\prod_{i=1}^{n}\underbrace{a_{i,i}}_{\substack{=%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by (\ref{sol.det.2diags.short.aij}), applied to }\left(  i,i\right)
\text{ instead of }\left(  i,j\right)  \text{)}}}+\left(  -1\right)
^{n-1}\prod_{i=1}^{n}\underbrace{a_{i,z\left(  i\right)  }}_{\substack{=%
\begin{cases}
a_{z\left(  i\right)  }, & \text{if }i=z\left(  i\right)  ;\\
b_{z\left(  i\right)  }, & \text{if }i\equiv z\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by (\ref{sol.det.2diags.short.aij}), applied to }\left(  i,z\left(
i\right)  \right)  \text{ instead of }\left(  i,j\right)  \text{)}}}\\
&  =\prod_{i=1}^{n}\underbrace{%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=a_{i}\\\text{(since }i=i\text{)}}}+\left(  -1\right)
^{n-1}\prod_{i=1}^{n}\underbrace{%
\begin{cases}
a_{z\left(  i\right)  }, & \text{if }i=z\left(  i\right)  ;\\
b_{z\left(  i\right)  }, & \text{if }i\equiv z\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=b_{z\left(  i\right)  }\\\text{(since }i\equiv z\left(
i\right)  +1\operatorname{mod}n\text{ (by (\ref{sol.det.2diags.short.zmod}%
)))}}}\\
&  =\prod_{i=1}^{n}a_{i}+\left(  -1\right)  ^{n-1}\underbrace{\prod_{i=1}%
^{n}b_{z\left(  i\right)  }}_{\substack{=\prod_{i=1}^{n}b_{i}\\\text{(here, we
have substituted }i\\\text{for }z\left(  i\right)  \text{ in the
product,}\\\text{since }z:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  \\\text{is a bijection)}}}=\underbrace{\prod_{i=1}%
^{n}a_{i}}_{=a_{1}a_{2}\cdots a_{n}}+\left(  -1\right)  ^{n-1}%
\underbrace{\prod_{i=1}^{n}b_{i}}_{=b_{1}b_{2}\cdots b_{n}}\\
&  =a_{1}a_{2}\cdots a_{n}+\left(  -1\right)  ^{n-1}b_{1}b_{2}\cdots b_{n}.
\end{align*}
This solves Exercise \ref{exe.det.2diags}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.det.2diags}.]We first make some preliminary definitions.

Let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. The
set $S_{n}$ is the set of all permutations of $\left\{  1,2,\ldots,n\right\}
$. In other words, the set $S_{n}$ is the set of all permutations of $\left[
n\right]  $ (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).

The integers $1,2,\ldots,n$ take all possible residues modulo $n$, and each of
them exactly once. In other words: For every $h\in\mathbb{Z}$, there exists a
unique element $g\in\left\{  1,2,\ldots,n\right\}  $ satisfying $g\equiv
h\operatorname{mod}n$. Since $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $, this rewrites as follows: For every $h\in\mathbb{Z}$, there
exists a unique element $g\in\left[  n\right]  $ satisfying $g\equiv
h\operatorname{mod}n$. We shall denote this $g$ by $\operatorname*{posrem}h$.
Thus, we have the following facts:

\begin{itemize}
\item For every $h\in\mathbb{Z}$, we have%
\begin{equation}
\operatorname*{posrem}h\in\left[  n\right]  \ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ \operatorname*{posrem}h\equiv h\operatorname{mod}n
\label{sol.det.2diags.posrem.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.posrem.1}):} Let
$h\in\mathbb{Z}$. We know that $\operatorname*{posrem}h$ is the unique element
$g\in\left[  n\right]  $ satisfying $g\equiv h\operatorname{mod}n$ (because
this is how we have defined $\operatorname*{posrem}h$). Thus,
$\operatorname*{posrem}h$ is an element $g\in\left[  n\right]  $ satisfying
$g\equiv h\operatorname{mod}n$. In other words, $\operatorname*{posrem}h$ is
an element of $\left[  n\right]  $ and satisfies $\operatorname*{posrem}%
h\equiv h\operatorname{mod}n$. In other words, $\operatorname*{posrem}%
h\in\left[  n\right]  $ and $\operatorname*{posrem}h\equiv h\operatorname{mod}%
n$. This proves (\ref{sol.det.2diags.posrem.1}).}.

\item If $h\in\mathbb{Z}$ and $k\in\left[  n\right]  $ are such that $k\equiv
h\operatorname{mod}n$, then%
\begin{equation}
k=\operatorname*{posrem}h \label{sol.det.2diags.posrem.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.posrem.2}):} Let
$h\in\mathbb{Z}$ and $k\in\left[  n\right]  $ be such that $k\equiv
h\operatorname{mod}n$.
\par
We know that $\operatorname*{posrem}h$ is the unique element $g\in\left[
n\right]  $ satisfying $g\equiv h\operatorname{mod}n$ (because this is how we
have defined $\operatorname*{posrem}h$). Thus, in particular,
$\operatorname*{posrem}h$ is the only such element. In other words, if
$g\in\left[  n\right]  $ is any element satisfying $g\equiv
h\operatorname{mod}n$, then $g=\operatorname*{posrem}h$. Applying this to
$g=k$, we obtain $k=\operatorname*{posrem}h$ (since $k\in\left[  n\right]  $
is an element satisfying $k\equiv h\operatorname{mod}n$). This proves
(\ref{sol.det.2diags.posrem.2}).}.

\item If $a\in\left[  n\right]  $ and $b\in\left[  n\right]  $ are such that
$a\equiv b\operatorname{mod}n$, then%
\begin{equation}
a=b \label{sol.det.2diags.posrem.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.posrem.3}):} Let $a\in\left[
n\right]  $ and $b\in\left[  n\right]  $ be such that $a\equiv
b\operatorname{mod}n$. Applying (\ref{sol.det.2diags.posrem.2}) to $h=b$ and
$k=a$, we obtain $a=\operatorname*{posrem}b$ (since $a\equiv
b\operatorname{mod}n$). Applying (\ref{sol.det.2diags.posrem.2}) to $h=b$ and
$k=b$, we obtain $b=\operatorname*{posrem}b$ (since $b\equiv
b\operatorname{mod}n$). Thus, $a=\operatorname*{posrem}b=b$. This proves
(\ref{sol.det.2diags.posrem.3}).}.
\end{itemize}

Now, fix $a\in\mathbb{Z}$. Every $g\in\left[  n\right]  $ satisfies
$\operatorname*{posrem}\left(  g+a\right)  \in\left[  n\right]  $%
\ \ \ \ \footnote{\textit{Proof.} Let $g\in\left[  n\right]  $. Then,
(\ref{sol.det.2diags.posrem.1}) (applied to $h=g+a$) shows that
$\operatorname*{posrem}\left(  g+a\right)  \in\left[  n\right]  $ and
$\operatorname*{posrem}\left(  g+a\right)  \equiv g+a\operatorname{mod}n$. In
particular, $\operatorname*{posrem}\left(  g+a\right)  \in\left[  n\right]  $,
qed.}. Hence, we can define a map $\operatorname*{shift}\nolimits_{a}:\left[
n\right]  \rightarrow\left[  n\right]  $ by setting%
\[
\left(  \operatorname*{shift}\nolimits_{a}\left(  g\right)
=\operatorname*{posrem}\left(  g+a\right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}g\in\left[  n\right]  \right)  .
\]
Consider this map $\operatorname*{shift}\nolimits_{a}$.

Let us now forget that we fixed $a\in\mathbb{Z}$. We thus have defined a map
$\operatorname*{shift}\nolimits_{a}:\left[  n\right]  \rightarrow\left[
n\right]  $ for every $a\in\mathbb{Z}$. (For example, if $n=5$ and $a=2$, then
the map $\operatorname*{shift}\nolimits_{a}$ sends $1,2,3,4,5$ to $3,4,5,1,2$, respectively.)

We observe the following facts:

\begin{itemize}
\item We have%
\begin{equation}
\operatorname*{shift}\nolimits_{0}=\operatorname*{id}
\label{sol.det.2diags.shift.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.0}):} Let $g\in\left[
n\right]  $. Then, $g\equiv g\operatorname{mod}n$. Hence,
(\ref{sol.det.2diags.posrem.2}) (applied to $h=g$ and $k=g$) shows that
$g=\operatorname*{posrem}g$. Now, the definition of $\operatorname*{shift}%
\nolimits_{0}$ shows that $\operatorname*{shift}\nolimits_{0}\left(  g\right)
=\operatorname*{posrem}\left(  \underbrace{g+0}_{=g}\right)
=\operatorname*{posrem}g$. Compared with $\operatorname*{id}\left(  g\right)
=g=\operatorname*{posrem}g$, this yields $\operatorname*{shift}\nolimits_{0}%
\left(  g\right)  =\operatorname*{id}\left(  g\right)  $.
\par
Now, let us forget that we fixed $g$. We thus have proven that
$\operatorname*{shift}\nolimits_{0}\left(  g\right)  =\operatorname*{id}%
\left(  g\right)  $ for every $g\in\left[  n\right]  $. In other words,
$\operatorname*{shift}\nolimits_{0}=\operatorname*{id}$. This proves
(\ref{sol.det.2diags.shift.0}).}.

\item We have%
\begin{equation}
\operatorname*{shift}\nolimits_{a+b}=\operatorname*{shift}\nolimits_{a}%
\circ\operatorname*{shift}\nolimits_{b}\ \ \ \ \ \ \ \ \ \ \text{for any }%
a\in\mathbb{Z}\text{ and }b\in\mathbb{Z} \label{sol.det.2diags.shift.a+b}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.a+b}):} Let
$a\in\mathbb{Z}$ and $b\in\mathbb{Z}$.
\par
Let $g\in\left[  n\right]  $. Then, $\operatorname*{shift}\nolimits_{a+b}%
\left(  g\right)  =\operatorname*{posrem}\left(  g+a+b\right)  $ (by the
definition of $\operatorname*{shift}\nolimits_{a+b}$).
\par
On the other hand, $\operatorname*{shift}\nolimits_{b}\left(  g\right)
=\operatorname*{posrem}\left(  g+b\right)  $ (by the definition of
$\operatorname*{shift}\nolimits_{b}$). Applying (\ref{sol.det.2diags.posrem.1}%
) to $h=g+b$, we obtain $\operatorname*{posrem}\left(  g+b\right)  \in\left[
n\right]  $ and $\operatorname*{posrem}\left(  g+b\right)  \equiv
g+b\operatorname{mod}n$.
\par
Moreover, $\left(  \operatorname*{shift}\nolimits_{a}\circ
\operatorname*{shift}\nolimits_{b}\right)  \left(  g\right)
=\operatorname*{shift}\nolimits_{a}\left(  \operatorname*{shift}%
\nolimits_{b}\left(  g\right)  \right)  =\operatorname*{posrem}\left(
\operatorname*{shift}\nolimits_{b}\left(  g\right)  +a\right)  $ (by the
definition of $\operatorname*{shift}\nolimits_{a}$). Applying
(\ref{sol.det.2diags.posrem.1}) to $h=\operatorname*{shift}\nolimits_{b}%
\left(  g\right)  +a$, we obtain $\operatorname*{posrem}\left(
\operatorname*{shift}\nolimits_{b}\left(  g\right)  +a\right)  \in\left[
n\right]  $ and $\operatorname*{posrem}\left(  \operatorname*{shift}%
\nolimits_{b}\left(  g\right)  +a\right)  \equiv\operatorname*{shift}%
\nolimits_{b}\left(  g\right)  +a\operatorname{mod}n$.
\par
Now,%
\begin{align*}
&  \left(  \operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}%
\nolimits_{b}\right)  \left(  g\right) \\
&  =\operatorname*{posrem}\left(  \operatorname*{shift}\nolimits_{b}\left(
g\right)  +a\right)  \equiv\underbrace{\operatorname*{shift}\nolimits_{b}%
\left(  g\right)  }_{=\operatorname*{posrem}\left(  g+b\right)  \equiv
g+b\operatorname{mod}n}+a\\
&  \equiv g+b+a=g+a+b\operatorname{mod}n.
\end{align*}
Thus, we can apply (\ref{sol.det.2diags.posrem.3}) to $h=g+a+b$ and $k=\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  $ (since $\left(  \operatorname*{shift}%
\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}\right)  \left(  g\right)
=\operatorname*{posrem}\left(  \operatorname*{shift}\nolimits_{b}\left(
g\right)  +a\right)  \in\left[  n\right]  $). As a result, we obtain $\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  =\operatorname*{posrem}\left(  g+a+b\right)  $.
Comparing this with $\operatorname*{shift}\nolimits_{a+b}\left(  g\right)
=\operatorname*{posrem}\left(  g+a+b\right)  $, we obtain $\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  =\operatorname*{shift}\nolimits_{a+b}\left(
g\right)  $.
\par
Now, let us forget that we fixed $g$. We thus have shown that $\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  =\operatorname*{shift}\nolimits_{a+b}\left(
g\right)  $ for every $g\in\left[  n\right]  $. In other words,
$\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
=\operatorname*{shift}\nolimits_{a+b}$. This proves
(\ref{sol.det.2diags.shift.a+b}).}.

\item We have%
\begin{equation}
\operatorname*{shift}\nolimits_{a}\in S_{n}\ \ \ \ \ \ \ \ \ \ \text{for every
}a\in\mathbb{Z} \label{sol.det.2diags.shift.Sn}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.Sn}):} Let
$a\in\mathbb{Z}$. From (\ref{sol.det.2diags.shift.a+b}) (applied to $b=-a$),
we obtain $\operatorname*{shift}\nolimits_{a+\left(  -a\right)  }%
=\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{-a}$.
Hence,%
\begin{align*}
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{-a}  &
=\operatorname*{shift}\nolimits_{a+\left(  -a\right)  }=\operatorname*{shift}%
\nolimits_{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }a+\left(  -a\right)
=0\right) \\
&  =\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.2diags.shift.0})}\right)  .
\end{align*}
\par
From (\ref{sol.det.2diags.shift.a+b}) (applied to $-a$ and $a$ instead of $a$
and $b$), we obtain $\operatorname*{shift}\nolimits_{\left(  -a\right)
+a}=\operatorname*{shift}\nolimits_{-a}\circ\operatorname*{shift}%
\nolimits_{a}$. Hence,%
\begin{align*}
\operatorname*{shift}\nolimits_{-a}\circ\operatorname*{shift}\nolimits_{a}  &
=\operatorname*{shift}\nolimits_{\left(  -a\right)  +a}=\operatorname*{shift}%
\nolimits_{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  -a\right)
+a=0\right) \\
&  =\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.2diags.shift.0})}\right)  .
\end{align*}
\par
The maps $\operatorname*{shift}\nolimits_{a}$ and $\operatorname*{shift}%
\nolimits_{-a}$ are mutually inverse (since $\operatorname*{shift}%
\nolimits_{a}\circ\operatorname*{shift}\nolimits_{-a}=\operatorname*{id}$ and
$\operatorname*{shift}\nolimits_{-a}\circ\operatorname*{shift}\nolimits_{a}%
=\operatorname*{id}$). Thus, the map $\operatorname*{shift}\nolimits_{a}$ is
invertible, and therefore a bijection.
\par
So the map $\operatorname*{shift}\nolimits_{a}$ is a bijection $\left[
n\right]  \rightarrow\left[  n\right]  $. In other words, the map
$\operatorname*{shift}\nolimits_{a}$ is a permutation of the set $\left[
n\right]  $. In other words, the map $\operatorname*{shift}\nolimits_{a}$ is a
permutation of the set $\left\{  1,2,\ldots,n\right\}  $ (since $\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $). In other words,
$\operatorname*{shift}\nolimits_{a}\in S_{n}$ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). This proves
(\ref{sol.det.2diags.shift.Sn}).}.
\end{itemize}

Let $z=\operatorname*{shift}\nolimits_{-1}$. Then, $z=\operatorname*{shift}%
\nolimits_{-1}\in S_{n}$ (by (\ref{sol.det.2diags.shift.Sn}), applied to
$a=-1$). In other words, $z$ is a permutation of $\left[  n\right]  $ (since
$S_{n}$ is the set of all permutations of $\left[  n\right]  $). We observe
some properties of $z$:

\begin{itemize}
\item We have $z=\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$, where we are
using the notations of Definition \ref{def.perm.cycles}. We will not use this
fact, and thus we will not prove it (but the proof is almost trivial).

\item We have%
\begin{equation}
z\left(  i\right)  \equiv i-1\operatorname{mod}n\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left[  n\right]  \label{sol.det.2diags.zmod}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.zmod}):} Let $i\in\left[
n\right]  $. Applying (\ref{sol.det.2diags.posrem.1}) to $h=i-1$, we obtain
$\operatorname*{posrem}\left(  i-1\right)  \in\left[  n\right]  $ and
$\operatorname*{posrem}\left(  i-1\right)  \equiv i-1\operatorname{mod}n$.
Now,%
\begin{align*}
\underbrace{z}_{=\operatorname*{shift}\nolimits_{-1}}\left(  i\right)   &
=\operatorname*{shift}\nolimits_{-1}\left(  i\right)  =\operatorname*{posrem}%
\left(  \underbrace{i+\left(  -1\right)  }_{=i-1}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{shift}%
\nolimits_{-1}\right) \\
&  =\operatorname*{posrem}\left(  i-1\right)  \equiv i-1\operatorname{mod}n.
\end{align*}
This proves (\ref{sol.det.2diags.zmod}).}. In other words,%
\begin{equation}
z\left(  i\right)  +1\equiv i\operatorname{mod}n\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left[  n\right]  . \label{sol.det.2diags.zmod2}%
\end{equation}
In particular,%
\begin{equation}
i\neq z\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[
n\right]  \label{sol.det.2diags.zmod0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.zmod0}):} Let $i\in\left[
n\right]  $. We have $i\equiv z\left(  i\right)  +1\operatorname{mod}n$ (by
(\ref{sol.det.2diags.zmod2})), so that $i-z\left(  i\right)  \equiv
1\not \equiv 0\operatorname{mod}n$ (since $n>1$). In other words,
$i\not \equiv z\left(  i\right)  \operatorname{mod}n$. Hence, $i\neq z\left(
i\right)  $. This proves (\ref{sol.det.2diags.zmod0}).}.

\item We have
\begin{equation}
z\left(  1\right)  =n \label{sol.det.2diags.shift.z.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.z.1}):} We have
$n\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $ and $n\equiv
0\operatorname{mod}n$. Hence, $n=\operatorname*{posrem}0$ (by
(\ref{sol.det.2diags.posrem.2}), applied to $k=n$ and $h=0$). Now,
\begin{align*}
\underbrace{z}_{=\operatorname*{shift}\nolimits_{-1}}\left(  1\right)   &
=\operatorname*{shift}\nolimits_{-1}\left(  1\right)  =\operatorname*{posrem}%
\underbrace{\left(  1+\left(  -1\right)  \right)  }_{=0}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{shift}%
\nolimits_{-1}\right) \\
&  =\operatorname*{posrem}0=n\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}n=\operatorname*{posrem}0\right)  .
\end{align*}
This proves (\ref{sol.det.2diags.shift.z.1}).}.

\item We have%
\begin{equation}
z\left(  i\right)  =i-1 \label{sol.det.2diags.shift.z.i}%
\end{equation}
for every $i\in\left[  n\right]  $ satisfying $i\neq1$%
\ \ \ \ \footnote{\textit{Proof of (\ref{sol.det.2diags.shift.z.i}):} Let
$i\in\left[  n\right]  $ be such that $i\neq1$. We have $i\in\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $. Combining this with $i\neq1$, we obtain
$i\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  1\right\}  =\left\{
2,3,\ldots,n\right\}  $, so that $i-1\in\left\{  1,2,\ldots,n-1\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $.
\par
We have $i-1\in\left[  n\right]  $ and $i-1\equiv i-1\operatorname{mod}n$.
Hence, $i-1=\operatorname*{posrem}\left(  i-1\right)  $ (by
(\ref{sol.det.2diags.posrem.2}), applied to $k=i-1$ and $h=i-1$). Now,
\begin{align*}
\underbrace{z}_{=\operatorname*{shift}\nolimits_{-1}}\left(  i\right)   &
=\operatorname*{shift}\nolimits_{-1}\left(  i\right)  =\operatorname*{posrem}%
\underbrace{\left(  i+\left(  -1\right)  \right)  }_{=i-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{shift}%
\nolimits_{-1}\right) \\
&  =\operatorname*{posrem}\left(  i-1\right)  =i-1\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i-1=\operatorname*{posrem}\left(  i-1\right)  \right)  .
\end{align*}
This proves (\ref{sol.det.2diags.shift.z.i}).}.
\end{itemize}

For every $\sigma\in S_{n}$, we let $\operatorname*{Inv}\left(  \sigma\right)
$ be the set of all inversions of the permutation $\sigma$. Thus, for every
$\sigma\in S_{n}$, we have%
\begin{align}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \nonumber\\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\label{sol.det.2diags.invl}%
\end{align}


Now, we continue stating properties of $z$:

\begin{itemize}
\item The set $\operatorname*{Inv}\left(  z\right)  $ is the set of all
inversions of $z$ (by the definition of $\operatorname*{Inv}\left(  z\right)
$). For every $u\in\left\{  2,3,\ldots,n\right\}  $, we have $\left(
1,u\right)  \in\operatorname*{Inv}\left(  z\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\left\{  2,3,\ldots,n\right\}  $.
Thus, $2\leq u\leq n$. Now, $1<2\leq u$. Thus, $1\leq1<u\leq n$. Moreover,
$u\in\left\{  2,3,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots,n\right\}
=\left[  n\right]  $ and $u\neq1$ (since $1<u$). Hence,
(\ref{sol.det.2diags.shift.z.i}) (applied to $i=u$) shows that $z\left(
u\right)  =u-1<u\leq n$. But (\ref{sol.det.2diags.shift.z.1}) shows that
$z\left(  1\right)  =n>z\left(  u\right)  $ (since $z\left(  u\right)  <n$).
\par
Thus, $\left(  1,u\right)  $ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $z\left(  i\right)  >z\left(  j\right)  $
(since $1\leq1<u\leq n$ and $z\left(  1\right)  >z\left(  u\right)  $). In
other words, $\left(  1,u\right)  $ is an inversion of $z$ (by the definition
of an \textquotedblleft inversion of $z$\textquotedblright). In other words,
$\left(  1,u\right)  \in\operatorname*{Inv}\left(  z\right)  $ (since
$\operatorname*{Inv}\left(  z\right)  $ is the set of all inversions of $z$).
Qed.}. Hence, we can define a map $\rho:\left\{  2,3,\ldots,n\right\}
\rightarrow\operatorname*{Inv}\left(  z\right)  $ by%
\[
\left(  \rho\left(  u\right)  =\left(  1,u\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  2,3,\ldots,n\right\}
\right)  .
\]
Consider this map $\rho$. The map $\rho$ is injective\footnote{\textit{Proof.}
Let $u_{1}$ and $u_{2}$ be two elements of $\left\{  2,3,\ldots,n\right\}  $
such that $\rho\left(  u_{1}\right)  =\rho\left(  u_{2}\right)  $. Then,
$\rho\left(  u_{1}\right)  =\left(  1,u_{1}\right)  $ (by the definition of
$\rho$) and $\rho\left(  u_{2}\right)  =\left(  1,u_{2}\right)  $ (by the
definition of $\rho$). Now, $\left(  1,u_{1}\right)  =\rho\left(
u_{1}\right)  =\rho\left(  u_{2}\right)  =\left(  1,u_{2}\right)  $. In other
words, $1=1$ and $u_{1}=u_{2}$.
\par
Let us now forget that we fixed $u_{1}$ and $u_{2}$. We thus have shown that
if $u_{1}$ and $u_{2}$ are two elements of $\left\{  2,3,\ldots,n\right\}  $
such that $\rho\left(  u_{1}\right)  =\rho\left(  u_{2}\right)  $, then
$u_{1}=u_{2}$. In other words, the map $\rho$ is injective, qed.} and
surjective\footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
z\right)  $. Thus, $c$ is an element of the set $\operatorname*{Inv}\left(
z\right)  $. In other words, $c$ is an inversion of $z$ (since
$\operatorname*{Inv}\left(  z\right)  $ is the set of all inversions of $z$).
In other words, $c$ is a pair $\left(  i,j\right)  $ of integers satisfying
$1\leq i<j\leq n$ and $z\left(  i\right)  >z\left(  j\right)  $ (by the
definition of an \textquotedblleft inversion of $z$\textquotedblright).
Consider this pair $\left(  i,j\right)  $. Thus, $c=\left(  i,j\right)  $.
\par
We have $i\in\left[  n\right]  $ (since $1\leq i\leq n$) and $j\in\left[
n\right]  $ (since $1\leq j\leq n$). Also, $1\leq i<j$; thus, $j>1$, so that
$j\neq1$. Hence, $z\left(  j\right)  =j-1$ (by (\ref{sol.det.2diags.shift.z.i}%
), applied to $j$ instead of $i$). From $j>1$, we obtain $j\geq2$ (since $j$
is an integer). Hence, $j\in\left\{  2,3,\ldots,n\right\}  $ (since $j\leq
n$). Thus, $\rho\left(  j\right)  $ is well-defined. The definition of $\rho$
shows that $\rho\left(  j\right)  =\left(  1,j\right)  $.
\par
We assume (for the sake of contradiction) that $i\neq1$. Thus, $z\left(
i\right)  =i-1$ (by (\ref{sol.det.2diags.shift.z.i})). Hence, $z\left(
i\right)  =\underbrace{i}_{<j}-1<j-1=z\left(  j\right)  $, which contradicts
$z\left(  i\right)  >z\left(  j\right)  $. This contradiction shows that our
assumption (that $i\neq1$) was wrong. Hence, we cannot have $i\neq1$. We thus
have $i=1$. Now, $c=\left(  \underbrace{i}_{=1},j\right)  =\left(  1,j\right)
=\rho\left(  \underbrace{j}_{\in\left\{  2,3,\ldots,n\right\}  }\right)
\in\rho\left(  \left\{  2,3,\ldots,n\right\}  \right)  $.
\par
Let us now forget that we fixed $c$. We thus have shown that $c\in\rho\left(
\left\{  2,3,\ldots,n\right\}  \right)  $ for every $c\in\operatorname*{Inv}%
\left(  z\right)  $. In other words, $\operatorname*{Inv}\left(  z\right)
\subseteq\rho\left(  \left\{  2,3,\ldots,n\right\}  \right)  $. In other
words, the map $\rho$ is surjective, qed.}. Hence, the map $\rho$ is
bijective. Thus, we have found a bijective map between the sets $\left\{
2,3,\ldots,n\right\}  $ and $\operatorname*{Inv}\left(  z\right)  $ (namely,
$\rho$). Consequently,%
\[
\left\vert \operatorname*{Inv}\left(  z\right)  \right\vert =\left\vert
\left\{  2,3,\ldots,n\right\}  \right\vert =n-1.
\]
Now, (\ref{sol.det.2diags.invl}) (applied to $\sigma=z$) shows that
$\ell\left(  z\right)  =\left\vert \operatorname*{Inv}\left(  z\right)
\right\vert =n-1$. The definition of $\left(  -1\right)  ^{z}$ now shows that
\begin{equation}
\left(  -1\right)  ^{z}=\left(  -1\right)  ^{\ell\left(  z\right)  }=\left(
-1\right)  ^{n-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(
z\right)  =n-1\right)  . \label{sol.det.2diags.signz}%
\end{equation}
(Alternatively, we could have obtained this equality from Exercise
\ref{exe.perm.cycles} \textbf{(d)} (applied to $k=n$ and $\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  =\left(  n,n-1,\ldots,1\right)  $) using the
observation that $z=\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$.)

\item Now, let us recall that $n>1$; hence, $n\neq1$. We have $z\neq
\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, $z=\operatorname*{id}$. Hence, $\underbrace{z}_{=\operatorname*{id}%
}\left(  1\right)  =\operatorname*{id}\left(  1\right)  =1$. This contradicts
$z\left(  1\right)  =n\neq1$. This contradiction proves that our assumption
was wrong; qed.}.
\end{itemize}

Now, I claim that if $\sigma\in S_{n}$ satisfies $\sigma\notin\left\{
\operatorname*{id},z\right\}  $, then%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists an }i\in\left[  n\right]  \text{ satisfying}\\
i\neq\sigma\left(  i\right)  \text{ and }i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n
\end{array}
\right)  . \label{sol.det.2diags.exi}%
\end{equation}


\textit{Proof of (\ref{sol.det.2diags.exi}):} Let $\sigma\in S_{n}$ be such
that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $. Thus, $\sigma
\neq\operatorname*{id}$ and $\sigma\neq z$.

The map $\sigma$ is an element of $S_{n}$, thus a permutation of the set
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$\sigma$ is a permutation of the set $\left[  n\right]  $ (since $\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  $). Thus, $\sigma$ is a bijection
$\left[  n\right]  \rightarrow\left[  n\right]  $. Hence, the map $\sigma$ is
bijective, and therefore injective and surjective.

There exists a $J\in\left[  n\right]  $ such that $\sigma\left(  J\right)
\neq J$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, there
exists no $J\in\left[  n\right]  $ such that $\sigma\left(  J\right)  \neq J$.
In other words, every $J\in\left[  n\right]  $ satisfies $\sigma\left(
J\right)  =J$. Thus, every $J\in\left[  n\right]  $ satisfies $\sigma\left(
J\right)  =J=\operatorname*{id}\left(  J\right)  $. In other words,
$\sigma=\operatorname*{id}$. This contradicts $\sigma\neq\operatorname*{id}$.
This contradiction proves that our assumption was wrong. Qed.}. Let $j$ be the
smallest such $J$. Thus, $j$ is an element of $\left[  n\right]  $ satisfying
$\sigma\left(  j\right)  \neq j$\ \ \ \ \footnote{\textit{Proof.} We know that
$j$ is the smallest $J\in\left[  n\right]  $ such that $\sigma\left(
J\right)  \neq J$ (by the definition of $j$). Hence, $j$ is an element $J$ of
$\left[  n\right]  $ such that $\sigma\left(  J\right)  \neq J$. In other
words, $j$ is an element of $\left[  n\right]  $ satisfying $\sigma\left(
j\right)  \neq j$. Qed.}. Moreover,
\begin{equation}
\text{every }J\in\left[  n\right]  \text{ satisfying }\sigma\left(  J\right)
\neq J\text{ must satisfy }J\geq j \label{sol.det.2diags.exi.pf.min}%
\end{equation}
\ \ \ \ \footnote{\textit{Proof of (\ref{sol.det.2diags.exi.pf.min}):} We know
that $j$ is the \textbf{smallest} $J\in\left[  n\right]  $ such that
$\sigma\left(  J\right)  \neq J$ (by the definition of $j$). Hence, no
$J\in\left[  n\right]  $ such that $\sigma\left(  J\right)  \neq J$ can be
smaller than $j$. In other words, every $J\in\left[  n\right]  $ such that
$\sigma\left(  J\right)  \neq J$ must be $\geq j$. In other words, every
$J\in\left[  n\right]  $ satisfying $\sigma\left(  J\right)  \neq J$ must
satisfy $J\geq j$. This proves (\ref{sol.det.2diags.exi.pf.min}).}.

If $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$, then
(\ref{sol.det.2diags.exi}) holds\footnote{\textit{Proof.} Assume the contrary.
Thus, $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$. So we know
that $j\neq\sigma\left(  j\right)  $ (since $\sigma\left(  j\right)  \neq j$)
and $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$.
Consequently, there exists an $i\in\left[  n\right]  $ satisfying $i\neq
\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n$ (namely, $i=j$). In other words,
(\ref{sol.det.2diags.exi}) holds. Qed.}. Hence, for the rest of this proof of
(\ref{sol.det.2diags.exi}), we can WLOG assume that we don't have
$j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$. Assume this.

We have $j\equiv\sigma\left(  j\right)  +1\operatorname{mod}n$ (since we don't
have $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$). In other
words, $\sigma\left(  j\right)  \equiv j-1\operatorname{mod}n$.

We have $j=1$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$j\neq1$. Combining this with $j\in\left[  n\right]  =\left\{  1,2,\ldots
,n\right\}  $, we obtain $j\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{
1\right\}  =\left\{  2,3,\ldots,n\right\}  $. Hence, $j-1\in\left\{
1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $. Also, $j-1<j$.
\par
If we had $\sigma\left(  j-1\right)  \neq j-1$, then we would obtain $j-1\geq
j$ (by (\ref{sol.det.2diags.exi.pf.min}), applied to $J=j-1$); but this would
contradict $j-1<j$. Hence, we cannot have $\sigma\left(  j-1\right)  \neq
j-1$. In other words, we must have $\sigma\left(  j-1\right)  =j-1$. Thus,
$\sigma\left(  j-1\right)  =j-1\equiv\sigma\left(  j\right)
\operatorname{mod}n$ (since $\sigma\left(  j\right)  \equiv
j-1\operatorname{mod}n$). Since both $\sigma\left(  j-1\right)  $ and
$\sigma\left(  j\right)  $ are elements of $\left[  n\right]  $, we can now
apply (\ref{sol.det.2diags.posrem.3}) to $\sigma\left(  j-1\right)  $ and
$\sigma\left(  j\right)  $ instead of $a$ and $b$. As a result, we obtain
$\sigma\left(  j-1\right)  =\sigma\left(  j\right)  $. Since the map $\sigma$
is injective, this shows that $j-1=j$. But this contradicts $j-1\neq j$. This
contradiction shows that our assumption was wrong, qed.}. Thus, $\sigma\left(
\underbrace{1}_{=j}\right)  =\sigma\left(  j\right)  \equiv\underbrace{j}%
_{=1}-1=0\equiv n\operatorname{mod}n$. Since both $\sigma\left(  1\right)  $
and $n$ belong to $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, we can
now apply (\ref{sol.det.2diags.posrem.3}) to $\sigma\left(  1\right)  $ and
$n$ instead of $a$ and $b$. As a result, we obtain $\sigma\left(  1\right)
=n$.

There exists a $p\in\left[  n\right]  $ such that $\sigma\left(  p\right)
\neq z\left(  p\right)  $\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, there exists no $p\in\left[  n\right]  $ such that
$\sigma\left(  p\right)  \neq z\left(  p\right)  $. In other words, every
$p\in\left[  n\right]  $ satisfies $\sigma\left(  p\right)  =z\left(
p\right)  $. In other words, $\sigma=z$. This contradicts $\sigma\neq z$. This
contradiction shows that our assumption was wrong. Qed.}. Consider this $p$.

We have $p\not \equiv \sigma\left(  p\right)  +1\operatorname{mod}%
n$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $p\equiv
\sigma\left(  p\right)  +1\operatorname{mod}n$. Hence, $\sigma\left(
p\right)  \equiv p-1\operatorname{mod}n$. But (\ref{sol.det.2diags.zmod})
(applied to $i=p$) yields $z\left(  p\right)  \equiv p-1\operatorname{mod}n$.
Hence, $\sigma\left(  p\right)  \equiv p-1\equiv z\left(  p\right)
\operatorname{mod}n$. Since both $\sigma$ and $z$ are elements of $S_{n}$, we
have $\sigma\left(  p\right)  \in\left[  n\right]  $ and $z\left(  p\right)
\in\left[  n\right]  $. Thus, (\ref{sol.det.2diags.posrem.3}) (applied to
$\sigma\left(  p\right)  $ and $z\left(  p\right)  $ instead of $a$ and $b$)
shows that $\sigma\left(  p\right)  =z\left(  p\right)  $ (since
$\sigma\left(  p\right)  \equiv z\left(  p\right)  \operatorname{mod}n$). This
contradicts $\sigma\left(  p\right)  \neq z\left(  p\right)  $. This
contradiction proves that our assumption was wrong, qed.}. If we have
$p\neq\sigma\left(  p\right)  $, then (\ref{sol.det.2diags.exi})
holds\footnote{\textit{Proof.} Assume the contrary. Thus, $p\neq\sigma\left(
p\right)  $. So we know that $p\neq\sigma\left(  p\right)  $ and
$p\not \equiv \sigma\left(  p\right)  +1\operatorname{mod}n$. Consequently,
there exists an $i\in\left[  n\right]  $ satisfying $i\neq\sigma\left(
i\right)  $ and $i\not \equiv \sigma\left(  i\right)  +1\operatorname{mod}n$
(namely, $i=p$). In other words, (\ref{sol.det.2diags.exi}) holds. Qed.}.
Hence, for the rest of this proof of (\ref{sol.det.2diags.exi}), we can WLOG
assume that we don't have $p\neq\sigma\left(  p\right)  $. Assume this.

We have $p=\sigma\left(  p\right)  $ (since we don't have $p\neq\sigma\left(
p\right)  $). Hence, there exists a $K\in\left[  n\right]  $ such that
$\sigma\left(  K\right)  =K$ (namely, $K=p$). Let $k$ be the largest such $K$.
Thus, $k$ is an element of $\left[  n\right]  $ satisfying $\sigma\left(
k\right)  =k$\ \ \ \ \footnote{\textit{Proof.} We know that $k$ is the largest
$K\in\left[  n\right]  $ such that $\sigma\left(  K\right)  =K$ (by the
definition of $k$). Hence, $k$ is an element $K$ of $\left[  n\right]  $ such
that $\sigma\left(  K\right)  =K$. In other words, $k$ is an element of
$\left[  n\right]  $ satisfying $\sigma\left(  k\right)  =k$. Qed.}.
Moreover,
\begin{equation}
\text{every }K\in\left[  n\right]  \text{ satisfying }\sigma\left(  K\right)
=K\text{ must satisfy }K\leq k \label{sol.det.2diags.exi.pf.max}%
\end{equation}
\ \ \ \ \footnote{\textit{Proof of (\ref{sol.det.2diags.exi.pf.max}):} We know
that $k$ is the \textbf{largest} $K\in\left[  n\right]  $ such that
$\sigma\left(  K\right)  =K$ (by the definition of $k$). Hence, no
$K\in\left[  n\right]  $ such that $\sigma\left(  K\right)  =K$ can be larger
than $k$. In other words, every $K\in\left[  n\right]  $ such that
$\sigma\left(  K\right)  =K$ must be $\leq k$. In other words, every
$K\in\left[  n\right]  $ satisfying $\sigma\left(  K\right)  =K$ must satisfy
$K\leq k$. This proves (\ref{sol.det.2diags.exi.pf.max}).}.

We have $n\neq1$ and thus $\sigma\left(  n\right)  \neq\sigma\left(  1\right)
$ (since the map $\sigma$ is injective). Thus, $\sigma\left(  n\right)
\neq\sigma\left(  1\right)  =n$.

If we had $k=n$, then we would have $\sigma\left(  \underbrace{n}_{=k}\right)
=\sigma\left(  k\right)  =k=n$, which would contradict $\sigma\left(
n\right)  \neq n$. Thus, we cannot have $k=n$. Hence, we have $k\neq n$. Thus,
$k+1\in\left[  n\right]  $\ \ \ \ \footnote{\textit{Proof.} Combining
$k\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $ with $k\neq n$, we
obtain $k\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  n\right\}
=\left\{  1,2,\ldots,n-1\right\}  $, so that $k+1\in\left\{  2,3,\ldots
,n\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $,
qed.}. Hence, $\sigma\left(  k+1\right)  $ is well-defined. If we had
$\sigma\left(  k+1\right)  =k+1$, then we would obtain $k+1\leq k$ (by
(\ref{sol.det.2diags.exi.pf.max}), applied to $K=k+1$), which would contradict
$k+1>k$. Hence, we cannot have $\sigma\left(  k+1\right)  =k+1$. We therefore
must have $\sigma\left(  k+1\right)  \neq k+1$. In other words, $k+1\neq
\sigma\left(  k+1\right)  $.

But we also have $k+1\not \equiv \sigma\left(  k+1\right)
+1\operatorname{mod}n$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, $k+1\equiv\sigma\left(  k+1\right)  +1\operatorname{mod}n$. Subtracting
$1$ from both sides of this congruence, we obtain $k\equiv\sigma\left(
k+1\right)  \operatorname{mod}n$. Since both $k$ and $\sigma\left(
k+1\right)  $ belong to $\left[  n\right]  $, we can therefore apply
(\ref{sol.det.2diags.posrem.3}) to $k$ and $\sigma\left(  k+1\right)  $
instead of $a$ and $b$. As a result, we obtain $k=\sigma\left(  k+1\right)  $.
Hence, $\sigma\left(  k+1\right)  =k=\sigma\left(  k\right)  $ (since
$\sigma\left(  k\right)  =k$). Since the map $\sigma$ is injective, this
yields that $k+1=k$. But this contradicts $k+1>k$. This contradiction proves
that our assumption was false, qed.}. Hence, there exists an $i\in\left[
n\right]  $ satisfying $i\neq\sigma\left(  i\right)  $ and $i\not \equiv
\sigma\left(  i\right)  +1\operatorname{mod}n$ (namely, $i=k+1$). In other
words, (\ref{sol.det.2diags.exi}) holds. This completes the proof of
(\ref{sol.det.2diags.exi}).

Let us now write our matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. Then,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A=\left(
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
In other words, we have%
\begin{equation}
a_{i,j}=%
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\label{sol.det.2diags.aij}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

It is now easy to see that if $\sigma\in S_{n}$ satisfies $\sigma
\notin\left\{  \operatorname*{id},z\right\}  $, then
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.det.2diags.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.0}):} Let $\sigma\in S_{n}$ be
such that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $. Thus, there
exists an $i\in\left[  n\right]  $ satisfying $i\neq\sigma\left(  i\right)  $
and $i\not \equiv \sigma\left(  i\right)  +1\operatorname{mod}n$ (because of
(\ref{sol.det.2diags.exi})). Consider this $i$. We have neither $i=\sigma
\left(  i\right)  $ nor $i\equiv\sigma\left(  i\right)  +1\operatorname{mod}n$
(since we have $i\neq\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(
i\right)  +1\operatorname{mod}n$).
\par
From (\ref{sol.det.2diags.aij}) (applied to $\left(  i,\sigma\left(  i\right)
\right)  $ instead of $\left(  i,j\right)  $), we obtain%
\[
a_{i,\sigma\left(  i\right)  }=%
\begin{cases}
a_{\sigma\left(  i\right)  }, & \text{if }i=\sigma\left(  i\right)  ;\\
b_{\sigma\left(  i\right)  }, & \text{if }i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
=0
\]
(since we have neither $i=\sigma\left(  i\right)  $ nor $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$).
\par
Now, let us forget that we fixed $i$. We thus have found an $i\in\left[
n\right]  $ satisfying $a_{i,\sigma\left(  i\right)  }=0$. Thus, there exists
an $i\in\left[  n\right]  $ satisfying $a_{i,\sigma\left(  i\right)  }=0$. In
other words, at least one factor of the product $\prod_{i\in\left[  n\right]
}a_{i,\sigma\left(  i\right)  }$ equals $0$. Therefore, the whole product
$\prod_{i\in\left[  n\right]  }a_{i,\sigma\left(  i\right)  }$ equals $0$
(because if at least one factor of a product equals $0$, then the whole
product equals $0$). In other words, $\prod_{i\in\left[  n\right]
}a_{i,\sigma\left(  i\right)  }=0$.
\par
Now,
\[
\underbrace{\prod_{i=1}^{n}}_{\substack{=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }=\prod_{i\in\left[  n\right]  }\\\text{(since }\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}}}a_{i,\sigma\left(
i\right)  }=\prod_{i\in\left[  n\right]  }a_{i,\sigma\left(  i\right)  }=0.
\]
This proves (\ref{sol.det.2diags.0}).}.

Let us also notice that%
\begin{equation}
\prod_{i=1}^{n}a_{i,z\left(  i\right)  }=\prod_{i=1}^{n}b_{i}
\label{sol.det.2diags.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.1}):} The map $z$ is a
permutation of $\left[  n\right]  $. In other words, the map $z$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $ (since $\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $), thus a bijection $\left\{  1,2,\ldots
,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $. Hence, we can
substitute $i$ for $z\left(  i\right)  $ in the product $\prod_{i\in\left\{
1,2,\ldots,n\right\}  }b_{z\left(  i\right)  }$. We thus obtain $\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{z\left(  i\right)  }=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{i}$.
\par
But for every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{align*}
a_{i,z\left(  i\right)  }  &  =%
\begin{cases}
a_{z\left(  i\right)  }, & \text{if }i=z\left(  i\right)  ;\\
b_{z\left(  i\right)  }, & \text{if }i\equiv z\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.det.2diags.aij}), applied to
}\left(  i,z\left(  i\right)  \right)  \text{ instead of }\left(  i,j\right)
\right) \\
&  =b_{z\left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since we don't have }i=z\left(  i\right)  \text{ (because of
(\ref{sol.det.2diags.zmod0})),}\\
\text{but we do have }i\equiv z\left(  i\right)  +1\operatorname{mod}n\text{
(by (\ref{sol.det.2diags.zmod}))}%
\end{array}
\right)  .
\end{align*}
Thus,%
\[
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}\underbrace{a_{i,z\left(  i\right)  }}_{=b_{z\left(  i\right)  }}=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{z\left(  i\right)  }%
=\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}%
}b_{i}=\prod_{i=1}^{n}b_{i}.
\]
This proves (\ref{sol.det.2diags.1}).}.

Now, (\ref{eq.det.eq.2}) becomes%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}\prod_{i=1}%
^{n}\underbrace{a_{i,\operatorname*{id}\left(  i\right)  }}%
_{\substack{=a_{i,i}\\\text{(since }\operatorname*{id}\left(  i\right)
=i\text{)}}}+\underbrace{\left(  -1\right)  ^{z}}_{\substack{=\left(
-1\right)  ^{n-1}\\\text{(by (\ref{sol.det.2diags.signz}))}}}\prod_{i=1}%
^{n}a_{i,z\left(  i\right)  }+\sum_{\substack{\sigma\in S_{n};\\\sigma
\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}%
_{\substack{=0\\\text{(by (\ref{sol.det.2diags.0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }\sigma=\operatorname*{id}\text{
and}\\
\text{for }\sigma=z\text{ from the sum (since }z\neq\operatorname*{id}\text{)}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n}a_{i,i}+\left(  -1\right)  ^{n-1}\prod_{i=1}^{n}%
a_{i,z\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)
^{\sigma}0}_{=0}\\
&  =\prod_{i=1}^{n}\underbrace{a_{i,i}}_{\substack{=%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by (\ref{sol.det.2diags.aij}), applied to }\left(  i,i\right)  \text{
instead of }\left(  i,j\right)  \text{)}}}+\left(  -1\right)  ^{n-1}%
\underbrace{\prod_{i=1}^{n}a_{i,z\left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n}b_{i}\\\text{(by (\ref{sol.det.2diags.1}))}}}\\
&  =\prod_{i=1}^{n}\underbrace{%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=a_{i}\\\text{(since }i=i\text{)}}}+\left(  -1\right)
^{n-1}\prod_{i=1}^{n}b_{i}\\
&  =\underbrace{\prod_{i=1}^{n}a_{i}}_{=a_{1}a_{2}\cdots a_{n}}+\left(
-1\right)  ^{n-1}\underbrace{\prod_{i=1}^{n}b_{i}}_{=b_{1}b_{2}\cdots b_{n}}\\
&  =a_{1}a_{2}\cdots a_{n}+\left(  -1\right)  ^{n-1}b_{1}b_{2}\cdots b_{n}.
\end{align*}
This solves Exercise \ref{exe.det.2diags}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.tridiag.isl}}

\begin{proof}
[Solution to Exercise \ref{exe.tridiag.isl}.]Define $n-1$ elements
$c_{1},c_{2},\ldots,c_{n-1}$ of $\mathbb{K}$ by%
\[
\left(  c_{i}=1\text{ for every }i\in\left\{  1,2,\ldots,n-1\right\}  \right)
.
\]
Define an $n\times n$-matrix $A$ as in Definition \ref{def.tridiag}. For every
two elements $x$ and $y$ of $\left\{  0,1,\ldots,n\right\}  $ satisfying
$x\leq y$, we define a $\left(  y-x\right)  \times\left(  y-x\right)  $-matrix
$A_{x,y}$ as in Proposition \ref{prop.tridiag.rec}.

Now, we claim that%
\begin{equation}
u_{i}=\det\left(  A_{0,i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  0,1,\ldots,n\right\}  . \label{sol.tridiag.isl.u}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.isl.u}):} We shall prove
(\ref{sol.tridiag.isl.u}) by strong induction over $i$. Thus, we fix some
$k\in\left\{  0,1,\ldots,n\right\}  $. We assume that (\ref{sol.tridiag.isl.u}%
) holds for all $i<k$. We now must show that (\ref{sol.tridiag.isl.u}) holds
for $i=k$. In other words, we must show that $u_{k}=\det\left(  A_{0,k}%
\right)  $.

This holds if $k\leq1$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=0$) yields $\det\left(
A_{0,0}\right)  =1$. Compared with $u_{0}=1$, this yields $u_{0}=\det\left(
A_{0,0}\right)  $.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=0$) yields
$\det\left(  A_{0,1}\right)  =a_{1}$. Compared with $u_{1}=a_{1}$, this yields
$u_{1}=\det\left(  A_{0,1}\right)  $.
\par
Now, $u_{k}=\det\left(  A_{0,k}\right)  $ holds if $k=0$ (because we have
$u_{0}=\det\left(  A_{0,0}\right)  $), and also holds if $k=1$ (since
$u_{1}=\det\left(  A_{0,1}\right)  $). Therefore, $u_{k}=\det\left(
A_{0,k}\right)  $ holds if $k\leq1$. Qed.}. Hence, we can WLOG assume that we
don't have $k\leq1$. Assume this.

We have $k>1$ (since we don't have $k\leq1$). Thus, $k\geq2$. Hence, $k-1$ and
$k-2$ are nonnegative integers satisfying $k-1<k$ and $k-2<k$. Hence, we can
apply (\ref{sol.tridiag.isl.u}) to $i=k-1$ (since we have assumed that
(\ref{sol.tridiag.isl.u}) holds for all $i<k$). As a result, we obtain
$u_{k-1}=\det\left(  A_{0,k-1}\right)  $. Also, we can apply
(\ref{sol.tridiag.isl.u}) to $i=k-2$ (since we have assumed that
(\ref{sol.tridiag.isl.u}) holds for all $i<k$). As a result, we obtain
$u_{k-2}=\det\left(  A_{0,k-2}\right)  $.

We have $c_{k-1}=1$ (by the definition of $c_{k-1}$).

Now, $0\leq k-2$ (since $k\geq2$). Hence, Proposition \ref{prop.tridiag.rec}
\textbf{(c)} (applied to $x=0$ and $y=k$) yields%
\[
\det\left(  A_{0,k}\right)  =a_{k}\det\left(  A_{0,k-1}\right)  -b_{k-1}%
\underbrace{c_{k-1}}_{=1}\det\left(  A_{0,k-2}\right)  =a_{k}\det\left(
A_{0,k-1}\right)  -b_{k-1}\det\left(  A_{0,k-2}\right)  .
\]
Comparing this with%
\begin{align*}
u_{k}  &  =a_{k}\underbrace{u_{k-1}}_{=\det\left(  A_{0,k-1}\right)  }%
-b_{k-1}\underbrace{u_{k-2}}_{=\det\left(  A_{0,k-2}\right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursive definition of }\left(
u_{0},u_{1},\ldots,u_{n}\right)  \right) \\
&  =a_{k}\det\left(  A_{0,k-1}\right)  -b_{k-1}\det\left(  A_{0,k-2}\right)  ,
\end{align*}
we obtain $u_{k}=\det\left(  A_{0,k}\right)  $. In other words,
(\ref{sol.tridiag.isl.u}) holds for $i=k$. This completes the induction step.
Thus, (\ref{sol.tridiag.isl.u}) is proven.

Next, we claim that%
\begin{equation}
v_{i}=\det\left(  A_{n-i,n}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  0,1,\ldots,n\right\}  . \label{sol.tridiag.isl.v}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.isl.v}):} We shall prove
(\ref{sol.tridiag.isl.v}) by strong induction over $i$. Thus, we fix some
$k\in\left\{  0,1,\ldots,n\right\}  $. We assume that (\ref{sol.tridiag.isl.v}%
) holds for all $i<k$. We now must show that (\ref{sol.tridiag.isl.v}) holds
for $i=k$. In other words, we must show that $v_{k}=\det\left(  A_{n-k,n}%
\right)  $.

This holds if $k\leq1$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=n$) yields $\det\left(
A_{n,n}\right)  =1$. Thus, $\det\left(  A_{n-0,n}\right)  =\det\left(
A_{n,n}\right)  =1$. Compared with $v_{0}=1$, this yields $v_{0}=\det\left(
A_{n-0,n}\right)  $.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=n-1$) yields
$\det\left(  A_{n-1,\left(  n-1\right)  +1}\right)  =a_{\left(  n-1\right)
+1}$. This rewrites as $\det\left(  A_{n-1,n}\right)  =a_{n}$ (since $\left(
n-1\right)  +1=n$). Compared with $v_{1}=a_{n}$, this yields $v_{1}%
=\det\left(  A_{n-1,n}\right)  $.
\par
Now, $v_{k}=\det\left(  A_{n-k,n}\right)  $ holds if $k=0$ (because we have
$v_{0}=\det\left(  A_{n-0,n}\right)  $), and also holds if $k=1$ (since
$v_{1}=\det\left(  A_{n-1,n}\right)  $). Therefore, $v_{k}=\det\left(
A_{n-k,n}\right)  $ holds if $k\leq1$. Qed.}. Hence, we can WLOG assume that
we don't have $k\leq1$. Assume this.

We have $k>1$ (since we don't have $k\leq1$). Thus, $k\geq2$. Hence, $k-1$ and
$k-2$ are nonnegative integers satisfying $k-1<k$ and $k-2<k$. Hence, we can
apply (\ref{sol.tridiag.isl.v}) to $i=k-1$ (since we have assumed that
(\ref{sol.tridiag.isl.v}) holds for all $i<k$). As a result, we obtain
$v_{k-1}=\det\left(  \underbrace{A_{n-\left(  k-1\right)  ,n}}_{=A_{n-k+1,n}%
}\right)  =\det\left(  A_{n-k+1,n}\right)  $. Also, we can apply
(\ref{sol.tridiag.isl.v}) to $i=k-2$ (since we have assumed that
(\ref{sol.tridiag.isl.v}) holds for all $i<k$). As a result, we obtain
$v_{k-2}=\det\left(  \underbrace{A_{n-\left(  k-2\right)  ,n}}_{=A_{n-k+2,n}%
}\right)  =\det\left(  A_{n-k+2,n}\right)  $.

We have $c_{n-k+1}=1$ (by the definition of $c_{n-k+1}$).

Now, $n-k\leq n-2$ (since $k\geq2$). Hence, Proposition \ref{prop.tridiag.rec}
\textbf{(d)} (applied to $x=n-k$ and $y=n$) yields%
\begin{align*}
\det\left(  A_{n-k,n}\right)   &  =a_{n-k+1}\det\left(  A_{n-k+1,n}\right)
-b_{n-k+1}\underbrace{c_{n-k+1}}_{=1}\det\left(  A_{n-k+2,n}\right) \\
&  =a_{n-k+1}\det\left(  A_{n-k+1,n}\right)  -b_{n-k+1}\det\left(
A_{n-k+2,n}\right)  .
\end{align*}
Comparing this with%
\begin{align*}
v_{k}  &  =a_{n-k+1}\underbrace{v_{k-1}}_{=\det\left(  A_{n-k+1,n}\right)
}-b_{n-k+1}\underbrace{v_{k-2}}_{=\det\left(  A_{n-k+2,n}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursive definition of }\left(
v_{0},v_{1},\ldots,v_{n}\right)  \right) \\
&  =a_{n-k+1}\det\left(  A_{n-k+1,n}\right)  -b_{n-k+1}\det\left(
A_{n-k+2,n}\right)  ,
\end{align*}
we obtain $v_{k}=\det\left(  A_{n-k,n}\right)  $. In other words,
(\ref{sol.tridiag.isl.v}) holds for $i=k$. This completes the induction step.
Thus, (\ref{sol.tridiag.isl.v}) is proven.

Now, we are almost done. In fact, applying (\ref{sol.tridiag.isl.u}) to $i=n$,
we obtain $u_{n}=\det\left(  A_{0,n}\right)  $. On the other hand, applying
(\ref{sol.tridiag.isl.v}) to $i=n$, we obtain $v_{n}=\det\left(
A_{n-n,n}\right)  =\det\left(  A_{0,n}\right)  $ (since $n-n=0$). Comparing
this with $u_{n}=\det\left(  A_{0,n}\right)  $, we obtain $u_{n}=v_{n}$.
Exercise \ref{exe.tridiag.isl} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.tridiag.cf}}

\begin{proof}
[Solution to Exercise \ref{exe.tridiag.cf}.]Assume that all denominators
appearing in Exercise \ref{exe.tridiag.cf} are invertible. For every
$k\in\left\{  1,2,\ldots,n\right\}  $, define an element $p_{k}$ of
$\mathbb{K}$ by%
\[
p_{k}=a_{k}-\dfrac{b_{k}c_{k}}{a_{k+1}-\dfrac{b_{k+1}c_{k+1}}{a_{k+2}%
-\dfrac{b_{k+2}c_{k+2}}{%
\begin{array}
[c]{ccc}%
a_{k+3}- &  & \\
& \ddots & \\
&  & -\dfrac{b_{n-2}c_{n-2}}{a_{n-1}-\dfrac{b_{n-1}c_{n-1}}{a_{n}}}%
\end{array}
}}}%
\]
\footnote{If $k=n$, then this should be interpreted as saying that
$p_{n}=a_{n}$.}. This definition of $p_{k}$ immediately gives a recursion:

\begin{itemize}
\item We have $p_{n}=a_{n}$.

\item For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, we have%
\begin{equation}
p_{k}=a_{k}-\dfrac{b_{k}c_{k}}{p_{k+1}}. \label{sol.tridiag.cf.1}%
\end{equation}

\end{itemize}

Now, we shall show that
\begin{equation}
\det\left(  A_{n-k-1,n}\right)  =p_{n-k}\det\left(  A_{n-k,n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  0,1,\ldots,n-1\right\}  .
\label{sol.tridiag.cf.main}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.cf.main}):} We shall prove
(\ref{sol.tridiag.cf.main}) by induction over $k$:

\textit{Induction base:} It is easy to see that (\ref{sol.tridiag.cf.main})
holds for $k=0$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=n$) yields $\det\left(
A_{n,n}\right)  =1$. Hence, $\underbrace{p_{n-0}}_{=p_{n}=a_{n}}\det\left(
\underbrace{A_{n-0,n}}_{=A_{n,n}}\right)  =a_{n}$.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=n-1$) yields
$\det\left(  A_{n-1,\left(  n-1\right)  +1}\right)  =a_{\left(  n-1\right)
+1}$. This rewrites as $\det\left(  A_{n-1,n}\right)  =a_{n}$ (since $\left(
n-1\right)  +1=n$). Thus, $\det\left(  \underbrace{A_{n-0-1,n}}_{=A_{n-1,n}%
}\right)  =\det\left(  A_{n-1,n}\right)  =a_{n}$. Comparing this with
$\underbrace{p_{n-0}}_{=p_{n}=a_{n}}\det\left(  \underbrace{A_{n-0,n}%
}_{=A_{n,n}}\right)  =a_{n}\underbrace{\det\left(  A_{n,n}\right)  }%
_{=1}=a_{n}$, we obtain $\det\left(  A_{n-0-1,n}\right)  =p_{n-0}\det\left(
A_{n-0,n}\right)  $. In other words, (\ref{sol.tridiag.cf.main}) holds for
$k=0$. Qed.}. This completes the induction base.

\textit{Induction step:} Let $K\in\left\{  0,1,\ldots,n-1\right\}  $ be
positive. Assume that (\ref{sol.tridiag.cf.main}) holds for $k=K-1$. We shall
show that (\ref{sol.tridiag.cf.main}) holds for $k=K$.

We have assumed that (\ref{sol.tridiag.cf.main}) holds for $k=K-1$. In other
words, we have $\det\left(  A_{n-\left(  K-1\right)  -1,n}\right)
=p_{n-\left(  K-1\right)  }\det\left(  A_{n-\left(  K-1\right)  ,n}\right)  $.
Since $n-\left(  K-1\right)  =n-K+1$, this rewrites as $\det\left(
A_{n-K+1-1,n}\right)  =p_{n-K+1}\det\left(  A_{n-K+1,n}\right)  $. Solving
this for $\det\left(  A_{n-K+1,n}\right)  $, we obtain%
\begin{equation}
\det\left(  A_{n-K+1,n}\right)  =\dfrac{\det\left(  A_{n-K+1-1,n}\right)
}{p_{n-K+1}}=\dfrac{1}{p_{n-K+1}}\det\left(  \underbrace{A_{n-K+1-1,n}%
}_{=A_{n-K,n}}\right)  =\dfrac{1}{p_{n-K+1}}\det\left(  A_{n-K,n}\right)  .
\label{sol.tridiag.cf.main.pf.2}%
\end{equation}


We have $K\in\left\{  1,2,\ldots,n-1\right\}  $ (since $K$ is positive and
belongs to $\left\{  0,1,\ldots,n-1\right\}  $). Hence, applying
(\ref{sol.tridiag.cf.1}) to $k=n-K$, we obtain%
\begin{equation}
p_{n-K}=a_{n-K}-\dfrac{b_{n-K}c_{n-K}}{p_{n-K+1}}.
\label{sol.tridiag.cf.main.pf.4}%
\end{equation}


We have $K\leq n-1$ and thus $n-K-1\geq0$. Moreover, $K\geq1$ (since $K$ is
positive), thus $n-\underbrace{K}_{\geq1}-1\leq n-1-1=n-2$. Hence, Proposition
\ref{prop.tridiag.rec} \textbf{(d)} (applied to $x=n-K-1$ and $y=n$) shows
that%
\begin{align*}
&  \det\left(  A_{n-K-1,n}\right) \\
&  =\underbrace{a_{\left(  n-K-1\right)  +1}}_{=a_{n-K}}\det\left(
\underbrace{A_{\left(  n-K-1\right)  +1,n}}_{=A_{n-K,n}}\right)
-\underbrace{b_{\left(  n-K-1\right)  +1}}_{=b_{n-K}}\underbrace{c_{\left(
n-K-1\right)  +1}}_{=c_{n-K}}\det\left(  \underbrace{A_{\left(  n-K-1\right)
+2,n}}_{=A_{n-K+1,n}}\right) \\
&  =a_{n-K}\det\left(  A_{n-K,n}\right)  -b_{n-K}c_{n-K}\underbrace{\det
\left(  A_{n-K+1,n}\right)  }_{\substack{=\dfrac{1}{p_{n-K+1}}\det\left(
A_{n-K,n}\right)  \\\text{(by (\ref{sol.tridiag.cf.main.pf.2}))}}}\\
&  =a_{n-K}\det\left(  A_{n-K,n}\right)  -b_{n-K}c_{n-K}\cdot\dfrac
{1}{p_{n-K+1}}\det\left(  A_{n-K,n}\right) \\
&  =\underbrace{\left(  a_{n-K}-b_{n-K}c_{n-K}\cdot\dfrac{1}{p_{n-K+1}%
}\right)  }_{\substack{=a_{n-K}-\dfrac{b_{n-K}c_{n-K}}{p_{n-K+1}}%
=p_{n-K}\\\text{(by (\ref{sol.tridiag.cf.main.pf.4}))}}}\det\left(
A_{n-K,n}\right)  =p_{n-K}\det\left(  A_{n-K,n}\right)  .
\end{align*}
In other words, (\ref{sol.tridiag.cf.main}) holds for $k=K$. This completes
the induction step. Thus, (\ref{sol.tridiag.cf.main}) is proven by induction.

Now, we can apply (\ref{sol.tridiag.cf.main}) to $k=n-1$. This gives us%
\[
\det\left(  A_{n-\left(  n-1\right)  -1,n}\right)  =\underbrace{p_{n-\left(
n-1\right)  }}_{=p_{1}}\det\left(  \underbrace{A_{n-\left(  n-1\right)  ,n}%
}_{=A_{1,n}}\right)  =p_{1}\det\left(  A_{1,n}\right)  .
\]
Since $n-\left(  n-1\right)  -1=0$, this rewrites as $\det\left(
A_{0,n}\right)  =p_{1}\det\left(  A_{1,n}\right)  $. Since $A_{0,n}=A$ (by
Proposition \ref{prop.tridiag.rec} \textbf{(e)}), this simplifies to $\det
A=p_{1}\det\left(  A_{1,n}\right)  $. Hence,%
\[
\dfrac{\det A}{\det\left(  A_{1,n}\right)  }=p_{1}=a_{1}-\dfrac{b_{1}c_{1}%
}{a_{2}-\dfrac{b_{2}c_{2}}{a_{3}-\dfrac{b_{3}c_{3}}{%
\begin{array}
[c]{ccc}%
a_{4}- &  & \\
& \ddots & \\
&  & -\dfrac{b_{n-2}c_{n-2}}{a_{n-1}-\dfrac{b_{n-1}c_{n-1}}{a_{n}}}%
\end{array}
}}}%
\]
(by the definition of $p_{1}$).
\end{proof}

\subsection{Solution to Exercise \ref{exe.tridiag.fib}}

\begin{proof}
[First solution to Exercise \ref{exe.tridiag.fib}.]We claim that%
\begin{equation}
\det\left(  A_{0,i}\right)  =f_{i+1}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  0,1,\ldots,n\right\}  . \label{sol.tridiag.fib.main}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.fib.main}):} We shall prove
(\ref{sol.tridiag.fib.main}) by strong induction over $i$. Thus, we fix some
$k\in\left\{  0,1,\ldots,n\right\}  $. We assume that
(\ref{sol.tridiag.fib.main}) holds for all $i<k$. We now must show that
(\ref{sol.tridiag.fib.main}) holds for $i=k$. In other words, we must show
that $\det\left(  A_{0,k}\right)  =f_{k+1}$.

This holds if $k\leq1$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=0$) yields $\det\left(
A_{0,0}\right)  =1$. Compared with $f_{0+1}=f_{1}=1$, this yields $\det\left(
A_{0,0}\right)  =f_{0+1}$.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=0$) yields
$\det\left(  A_{0,1}\right)  =a_{1}=1$. Compared with $f_{1+1}=f_{2}=1$, this
yields $\det\left(  A_{0,1}\right)  =f_{1+1}$.
\par
Now, $\det\left(  A_{0,k}\right)  =f_{k+1}$ holds if $k=0$ (because we have
$\det\left(  A_{0,0}\right)  =f_{0+1}$), and also holds if $k=1$ (since
$\det\left(  A_{0,1}\right)  =f_{1+1}$). Therefore, $\det\left(
A_{0,k}\right)  =f_{k+1}$ holds if $k\leq1$. Qed.}. Hence, we can WLOG assume
that we don't have $k\leq1$. Assume this.

We have $k>1$ (since we don't have $k\leq1$). Thus, $k\geq2$. Hence, $k-1$ and
$k-2$ are nonnegative integers satisfying $k-1<k$ and $k-2<k$. Hence, we can
apply (\ref{sol.tridiag.fib.main}) to $i=k-1$ (since we have assumed that
(\ref{sol.tridiag.fib.main}) holds for all $i<k$). As a result, we obtain
$\det\left(  A_{0,k-1}\right)  =f_{\left(  k-1\right)  +1}$. Also, we can
apply (\ref{sol.tridiag.fib.main}) to $i=k-2$ (since we have assumed that
(\ref{sol.tridiag.fib.main}) holds for all $i<k$). As a result, we obtain
$\det\left(  A_{0,k-2}\right)  =f_{\left(  k-2\right)  +1}$.

Now, $0\leq k-2$ (since $k\geq2$). Hence, Proposition \ref{prop.tridiag.rec}
\textbf{(c)} (applied to $x=0$ and $y=k$) yields%
\begin{align*}
\det\left(  A_{0,k}\right)   &  =\underbrace{a_{k}}_{\substack{=1\\\text{(by
the}\\\text{definition of }a_{k}\text{)}}}\underbrace{\det\left(
A_{0,k-1}\right)  }_{\substack{=f_{\left(  k-1\right)  +1}\\=f_{k}%
}}-\underbrace{b_{k-1}}_{\substack{=1\\\text{(by the}\\\text{definition of
}b_{k-1}\text{)}}}\underbrace{c_{k-1}}_{\substack{=-1\\\text{(by
the}\\\text{definition of }c_{k-1}\text{)}}}\underbrace{\det\left(
A_{0,k-2}\right)  }_{\substack{=f_{\left(  k-2\right)  +1}\\=f_{k-1}}}\\
&  =f_{k}-\left(  -1\right)  f_{k-1}=f_{k}+f_{k-1}.
\end{align*}
Comparing this with%
\[
f_{k+1}=f_{k}+f_{k-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursive
definition of the Fibonacci numbers}\right)  ,
\]
we obtain $\det\left(  A_{0,k}\right)  =f_{k+1}$. In other words,
(\ref{sol.tridiag.fib.main}) holds for $i=k$. This completes the induction
step. Thus, (\ref{sol.tridiag.fib.main}) is proven.

Now, applying \ref{sol.tridiag.fib.main} to $i=n$, we obtain $\det\left(
A_{0,n}\right)  =f_{n+1}$. Since $A_{0,n}=A$ (by Proposition
\ref{prop.tridiag.rec} \textbf{(e)}), we can rewrite this as $\det A=f_{n+1}$.
This solves Exercise \ref{exe.tridiag.fib}.
\end{proof}

\begin{proof}
[Second solution to Exercise \ref{exe.tridiag.fib} (sketched).]Here is a
different solution for Exercise \ref{exe.tridiag.fib}, which is far more
complicated than the previous one, but has the pedagogical advantage of
illuminating the connection between determinants and permutations, and the
combinatorics of the latter.

Exercise \ref{exe.ps2.2.3} (applied to $n+1$ instead of $n$) shows that
$f_{n+1}$ is the number of subsets $I$ of $\left\{  1,2,\ldots,n-1\right\}  $
such that no two elements of $I$ are consecutive. We shall refer to such
subsets $I$ as \textit{lacunar sets}.\footnote{We keep $n$ fixed, so a
\textquotedblleft lacunar set\textquotedblright\ will always be a subset of
$\left\{  1,2,\ldots,n-1\right\}  $.} Thus, $f_{n+1}$ is the number of all
lacunar sets.

(For example, if $n=5$, then the lacunar sets are $\varnothing$, $\left\{
1\right\}  $, $\left\{  2\right\}  $, $\left\{  3\right\}  $, $\left\{
4\right\}  $, $\left\{  1,3\right\}  $, $\left\{  1,4\right\}  $, and
$\left\{  2,4\right\}  $. Their number, unsurprisingly, is $8=f_{5+1}$.)

For any $\sigma\in S_{n}$, we define the following terminology:

\begin{itemize}
\item The \textit{excedances} of $\sigma$ are the elements $i\in\left\{
1,2,\ldots,n\right\}  $ satisfying $\sigma\left(  i\right)  >i$. For instance,
the permutation in $S_{7}$ written in one-line notation as $\left(
3,1,2,4,5,7,6\right)  $ has excedances $1$ and $6$.

\item We let $\operatorname*{Exced}\sigma$ denote the set of all excedances of
$\sigma$.

\item A permutation $\sigma\in S_{n}$ is said to be \textit{short-legged} if
every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies $\left\vert
\sigma\left(  i\right)  -i\right\vert \leq1$. For instance, the permutation in
$S_{7}$ written in one-line notation as $\left(  1,2,4,3,5,7,6\right)  $ is short-legged.
\end{itemize}

(Most of the terminology here is my own, tailored for this exercise; only the
notion of \textquotedblleft excedance\textquotedblright\ is standard. I chose
the name \textquotedblleft short-legged\textquotedblright\ because a
permutation $\sigma$ satisfying $\left\vert \sigma\left(  i\right)
-i\right\vert \leq1$ \textquotedblleft does not take $i$ very
far\textquotedblright.)

What does this all have to do with the exercise? Let us write our matrix $A$
in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Thus, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we have%
\begin{equation}
a_{i,j}=%
\begin{cases}
a_{i}, & \text{if }i=j;\\
b_{i}, & \text{if }i=j-1;\\
c_{j}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
=%
\begin{cases}
1, & \text{if }i=j;\\
1, & \text{if }i=j-1;\\
-1, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
\label{sol.tridiag.fib.sol2.aij}%
\end{equation}
(since $a_{i}=1$, $b_{i}=1$ and $c_{j}=-1$). Notice that, as a consequence of
this equality, we have%
\begin{equation}
a_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }\left\vert j-i\right\vert >1.
\label{sol.tridiag.fib.sol2.locality}%
\end{equation}


Now, (\ref{eq.det.eq.2}) gives an expression for $\det A$, but to get any
mileage out of it we need to simplify the terms $\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }$ for $\sigma\in S_{n}$. This turns out to
depend on whether the permutation $\sigma$ is short-legged or not:

\begin{itemize}
\item If a permutation $\sigma\in S_{n}$ is not short-legged, then%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0.
\label{sol.tridiag.fib.sol2.term1}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.fib.sol2.term1}):} Let $\sigma\in S_{n}$ be
not short-legged. Thus, there exists a $k\in\left\{  1,2,\ldots,n\right\}  $
satisfying $\left\vert \sigma\left(  k\right)  -k\right\vert >1$. The factor
$a_{k,\sigma\left(  k\right)  }$ corresponding to this $k$ must be $0$
(because of (\ref{sol.tridiag.fib.sol2.locality})); this forces the whole
product $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ to become $0$. Thus,
(\ref{sol.tridiag.fib.sol2.term1}) follows.

\item If a permutation $\sigma\in S_{n}$ is short-legged, then%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\left(  -1\right)  ^{\left\vert
\operatorname*{Exced}\sigma\right\vert }. \label{sol.tridiag.fib.sol2.term2}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.fib.sol2.term2}):} Let $\sigma\in S_{n}$ be
short-legged. Thus, every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
$\left\vert \sigma\left(  i\right)  -i\right\vert \leq1$. Consequently, for
every $i\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{align*}
a_{i,\sigma\left(  i\right)  }  &  =%
\begin{cases}
1, & \text{if }i=\sigma\left(  i\right)  ;\\
1, & \text{if }i=\sigma\left(  i\right)  -1;\\
-1, & \text{if }i=\sigma\left(  i\right)  +1;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.tridiag.fib.sol2.aij})}\right)
\\
&  =%
\begin{cases}
1, & \text{if }i=\sigma\left(  i\right)  ;\\
1, & \text{if }i=\sigma\left(  i\right)  -1;\\
-1, & \text{if }i=\sigma\left(  i\right)  +1;
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the inequality }\left\vert \sigma\left(  i\right)  -i\right\vert
\leq1\text{ ensures that one of the}\\
\text{conditions }i=\sigma\left(  i\right)  \text{, }i=\sigma\left(  i\right)
-1\text{ and }i=\sigma\left(  i\right)  +1\text{ must hold}%
\end{array}
\right) \\
&  =%
\begin{cases}
1, & \text{if }\sigma\left(  i\right)  \leq i;\\
-1, & \text{if }\sigma\left(  i\right)  >i;
\end{cases}
.
\end{align*}
Thus,%
\begin{align*}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }  &  =\prod_{i=1}^{n}%
\begin{cases}
1, & \text{if }\sigma\left(  i\right)  \leq i;\\
-1, & \text{if }\sigma\left(  i\right)  >i;
\end{cases}
=\left(  -1\right)  ^{\left(  \text{the number of all }i\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }\sigma\left(  i\right)  >i\right)
}\\
&  =\left(  -1\right)  ^{\left\vert \left\{  i\in\left\{  1,2,\ldots
,n\right\}  \ \mid\ \sigma\left(  i\right)  >i\right\}  \right\vert }=\left(
-1\right)  ^{\left\vert \operatorname*{Exced}\sigma\right\vert };
\end{align*}
thus, (\ref{sol.tridiag.fib.sol2.term2}) is proven.
\end{itemize}

Now, (\ref{eq.det.eq.2}) becomes%
\begin{align}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\nonumber\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is not short-legged}%
}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }}_{\substack{=0\\\text{(by (\ref{sol.tridiag.fib.sol2.term1}))}%
}}+\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is short-legged}}}\left(
-1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=\left(  -1\right)  ^{\left\vert \operatorname*{Exced}%
\sigma\right\vert }\\\text{(by (\ref{sol.tridiag.fib.sol2.term2}))}%
}}\nonumber\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is not
short-legged}}}\left(  -1\right)  ^{\sigma}0}_{=0}+\sum_{\substack{\sigma\in
S_{n};\\\sigma\text{ is short-legged}}}\left(  -1\right)  ^{\sigma}\left(
-1\right)  ^{\left\vert \operatorname*{Exced}\sigma\right\vert }\nonumber\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is short-legged}}}\left(
-1\right)  ^{\sigma}\left(  -1\right)  ^{\left\vert \operatorname*{Exced}%
\sigma\right\vert }. \label{sol.tridiag.fib.sol2.det1}%
\end{align}
We still don't see how this connects to $f_{n+1}$, though. So let us relate
short-legged permutations to lacunar sets.

For any lacunar set $I$, we can define a permutation $\tau_{I}\in S_{n}$ by
the following rule:%
\[
\tau_{I}\left(  k\right)  =%
\begin{cases}
k+1, & \text{if }k\in I;\\
k-1, & \text{if }k-1\in I;\\
k, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots,n\right\}  .
\]
In other words, $\tau_{I}$ is the permutation of $\left\{  1,2,\ldots
,n\right\}  $ which interchanges every element $i$ of $I$ with its successor
$i+1$, while leaving all remaining elements unchanged. Make sure you
understand why $\tau_{I}$ is a well-defined map $\left\{  1,2,\ldots
,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $\ \ \ \ \footnote{Here
are the two things you need to check:
\par
\begin{itemize}
\item The term $%
\begin{cases}
k+1, & \text{if }k\in I;\\
k-1, & \text{if }k-1\in I;\\
k, & \text{otherwise}%
\end{cases}
$ is unambiguous, because no $k$ satisfies both $k\in I$ and $k-1\in I$ at the
same time. (Here is where you use the lacunarity of $I$.)
\par
\item We have $%
\begin{cases}
k+1, & \text{if }k\in I;\\
k-1, & \text{if }k-1\in I;\\
k, & \text{otherwise}%
\end{cases}
$ for every $k\in\left\{  1,2,\ldots,n\right\}  $. (Here you use
$I\subseteq\left\{  1,2,\ldots,n-1\right\}  $. If $I$ were only a subset of
$\left\{  1,2,\ldots,n\right\}  $, then this would fall outside of $\left\{
1,2,\ldots,n\right\}  $ for $k=n$.)
\end{itemize}
} and a permutation\footnote{Indeed, $\tau_{I}\circ\tau_{I}=\operatorname*{id}%
$, so that $\tau_{I}$ is its own inverse.}.

(For example, if $n=7$ and $I=\left\{  2,5\right\}  $, then $\tau_{I}=\left(
1,3,2,4,6,5,7\right)  $ in one-line notation.)

It is clear that the permutation $\tau_{I}$ is short-legged. Moreover, it
satisfies%
\begin{equation}
\operatorname*{Exced}\left(  \tau_{I}\right)  =I;
\label{sol.tridiag.fib.sol2.tauI.Exced}%
\end{equation}
as a consequence, it is possible to reconstruct $I$ from $\tau_{I}$. Thus, the
permutations $\tau_{I}$ for distinct $I$ are distinct.

What is the sign $\left(  -1\right)  ^{\tau_{I}}$ ? It is easy to see (from
the construction of $\tau_{I}$) that the only inversions of $\tau_{I}$ are the
pairs $\left(  i,i+1\right)  $ for $i\in I$ (essentially, the short-leggedness
of $\tau_{I}$ prevents $\tau_{I}$ from changing the order of two non-adjacent
integers). Thus, the number of these inversions is $\left\vert I\right\vert $.
Thus, $\ell\left(  \tau_{I}\right)  =\left\vert I\right\vert $. Hence,
\begin{equation}
\left(  -1\right)  ^{\tau_{I}}=\left(  -1\right)  ^{\ell\left(  \tau
_{I}\right)  }=\left(  -1\right)  ^{\left\vert I\right\vert }.
\label{sol.tridiag.fib.sol2.tauI.sign}%
\end{equation}


So there are at least some short-legged permutations that we understand well:
the $\tau_{I}$ for lacunar sets $I$. Are there others?

It turns out that there aren't. Indeed,
\begin{equation}
\text{every short-legged }\sigma\in S_{n}\text{ has the form }\tau_{I}\text{
for some lacunar set }I. \label{sol.tridiag.fib.sol2.tauI.surj}%
\end{equation}
Before we can prove this, we shall prove two auxiliary observations:

\textit{Observation 1:} Let $\sigma\in S_{n}$ be short-legged. If
$i\in\left\{  1,2,\ldots,n\right\}  $ be such that $\sigma\left(  i\right)
=i+1$, then $\sigma\left(  i+1\right)  =i$.

\textit{Observation 2:} Let $\sigma\in S_{n}$ be short-legged. If
$i\in\left\{  1,2,\ldots,n\right\}  $ be such that $\sigma\left(  i\right)
=i-1$, then $\sigma\left(  i-1\right)  =i$.

\textit{Proof of Observation 1.} Assume the contrary. Thus, there exists some
$i\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(  i\right)  =i+1$
but $\sigma\left(  i+1\right)  \neq i$. We call such $i$'s \textit{evil}. By
our assumption, there exists at least one evil $i$. Consider the highest evil
$i$. Thus, $i+1$ is not evil.

Since $i$ is evil, we have $\sigma\left(  i\right)  =i+1$ but $\sigma\left(
i+1\right)  \neq i$. In particular, $i+1=\sigma\left(  i\right)  \in\left\{
1,2,\ldots,n\right\}  $, so that $\sigma\left(  i+1\right)  $ is well-defined.
Since $\sigma$ is short-legged, we have $\left\vert \sigma\left(  i+1\right)
-\left(  i+1\right)  \right\vert \leq1$. Hence, $\sigma\left(  i+1\right)  $
is either $i$ or $i+1$ or $i+2$. But $\sigma\left(  i+1\right)  $ cannot be
$i$ (since $\sigma\left(  i+1\right)  \neq i$) and cannot be $i+1$ either
(since this would cause $\sigma\left(  i+1\right)  =i+1=\sigma\left(
i\right)  $, which would contradict the injectivity of $\sigma$). Hence,
$\sigma\left(  i+1\right)  $ must be $i+2$. In other words, $\sigma\left(
i+1\right)  =i+2$. Moreover, the injectivity of $\sigma$ shows that
$\sigma\left(  i+2\right)  \neq\sigma\left(  i\right)  =i+1$, so that $i+1$ is
evil. But this contradicts the fact that $i+1$ is not evil. Thus, Observation
1 is proven.

\textit{Proof of Observation 2.} Analogous to Observation 1 (this time, take
the lowest evil $i$), and left to the reader.

\textit{Proof of (\ref{sol.tridiag.fib.sol2.tauI.surj}):} Let $\sigma\in
S_{n}$ be short-legged. We must show that $\sigma=\tau_{I}$ for some lacunar
set $I$.

We set $I=\operatorname*{Exced}\sigma$. (This is the only choice we can make
to have any hope for $\sigma=\tau_{I}$ to be true; indeed,
(\ref{sol.tridiag.fib.sol2.tauI.Exced}) ensures that if $\sigma=\tau_{I}$,
then $\operatorname*{Exced}\sigma=I$.)

We notice that%
\begin{equation}
\sigma\left(  i\right)  =i+1\ \ \ \ \ \ \ \ \ \ \text{for every }i\in I
\label{sol.tridiag.fib.sol2.tauI.surj.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}):} Let
$i\in I$. Thus, $i\in I=\operatorname*{Exced}\sigma$. In other words, $i$ is
an excedance of $\sigma$. Hence, $\sigma\left(  i\right)  >i$. Since
$\left\vert \sigma\left(  i\right)  -i\right\vert \leq1$ (because $\sigma$ is
short-legged), this means that $\sigma\left(  i\right)  =i+1$, qed.}. Thus,
$n$ cannot belong to $I$ (since this would entail $\sigma\left(  n\right)
=n+1$, but $n+1\notin\left\{  1,2,\ldots,n\right\}  $). Hence, $I\subseteq
\left\{  1,2,\ldots,n-1\right\}  $.

Let us first show that $I$ is a lacunar set. Indeed, assume (for the sake of
contradiction) that this is not so. Then, there exists some $i\in I$ such that
$i+1\in I$. Consider such an $i$. We have $\sigma\left(  i\right)  =i+1$ (by
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1})), and thus $\sigma\left(
i+1\right)  =i$ (by Observation 1). But $\sigma\left(  i+1\right)  =i+2$ (by
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}), applied to $i+1$ instead of $i$).
Hence, $i+2=\sigma\left(  i+1\right)  =i$, which is absurd. Hence, we have
found a contradiction. This finishes our proof that $I$ is a lacunar set.

We still need to show that we actually have $\sigma=\tau_{I}$. In other words,
we need to show that $\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ for
every $k\in\left\{  1,2,\ldots,n\right\}  $.

So let us fix $k\in\left\{  1,2,\ldots,n\right\}  $, and let us show that
$\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $. We are in one of the
following three cases:

\textit{Case 1:} We have $k\in I$.

\textit{Case 2:} We have $k-1\in I$.

\textit{Case 3:} Neither $k\in I$ nor $k-1\in I$.

\begin{itemize}
\item Let us first consider Case 1. In this case, $k\in I$. Hence,
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}) (applied to $i=k$) yields
$\sigma\left(  k\right)  =k+1$. On the other hand, the definition of $\tau
_{I}$ shows that $\tau_{I}\left(  k\right)  =k+1$ as well. Thus,
$\sigma\left(  k\right)  =k+1=\tau_{I}\left(  k\right)  $. Hence,
$\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ is proven in Case 1.

\item Let us now consider Case 2. In this case, $k-1\in I$. Hence,
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}) (applied to $i=k-1$) yields
$\sigma\left(  k-1\right)  =k$. Since $\sigma$ is injective, we have
$\sigma\left(  k\right)  \neq\sigma\left(  k-1\right)  =k$. Also, $I$ is
lacunar, so that $k-1\in I$ entails $k\notin I$; thus, $k\notin
I=\operatorname*{Exced}\sigma$, so that $k$ is not an excedance of $\sigma$.
In other words, $\sigma\left(  k\right)  \leq k$. Combined with $\sigma\left(
k\right)  \neq k$, this yields $\sigma\left(  k\right)  <k$. Since $\left\vert
\sigma\left(  k\right)  -k\right\vert \leq1$ (because $\sigma$ is
short-legged), this shows that $\sigma\left(  k\right)  =k-1$. On the other
hand, $\tau_{I}\left(  k\right)  =k-1$ by the definition of $\tau_{I}$. Thus,
$\sigma\left(  k\right)  =k-1=\tau_{I}\left(  k\right)  $. Hence,
$\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ is proven in Case 2.

\item Let us finally consider Case 3. In this case, neither $k\in I$ nor
$k-1\in I$. Let us now show that $\sigma\left(  k\right)  =k$. Indeed, assume
the contrary. Thus, $\sigma\left(  k\right)  \neq k$. As in Case 2, we can use
this (and $k\notin I$) to show that $\sigma\left(  k\right)  =k-1$.
Observation 2 thus shows that $\sigma\left(  k-1\right)  =k>k-1$, so that
$k-1$ is an excedance of $\sigma$. In other words, $k-1\in
\operatorname*{Exced}\sigma=I$. This contradicts the assumption that we do not
have $k-1\in I$. This contradiction concludes our proof of $\sigma\left(
k\right)  =k$. On the other hand, $\tau_{I}\left(  k\right)  =k$ by the
definition of $\tau_{I}$. Thus, $\sigma\left(  k\right)  =k=\tau_{I}\left(
k\right)  $. Hence, $\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ is
proven in Case 3.
\end{itemize}

We now have shown that $\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $
in all possible cases. Thus, $\sigma=\tau_{I}$. Since $I$ is a lacunar set,
this proves (\ref{sol.tridiag.fib.sol2.tauI.surj}).

All we now need to do is combine our results. For every lacunar set $I$, we
have defined a short-legged permutation $\tau_{I}\in S_{n}$. Conversely, we
know (from (\ref{sol.tridiag.fib.sol2.tauI.surj})) that every short-legged
$\sigma\in S_{n}$ has the form $\tau_{I}$ for some lacunar set $I$; we also
know that this $I$ is uniquely determined by the $\sigma$ (since the
permutations $\tau_{I}$ for distinct $I$ are distinct). Thus, we have a
bijection between the lacunar sets and the short-legged permutations in
$S_{n}$; the bijection sends every $I$ to $\tau_{I}$. Consequently,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is short-legged}}}\left(
-1\right)  ^{\sigma}\left(  -1\right)  ^{\left\vert \operatorname*{Exced}%
\sigma\right\vert }\\
&  =\sum_{I\text{ is a lacunar set}}\underbrace{\left(  -1\right)  ^{\tau_{I}%
}}_{\substack{=\left(  -1\right)  ^{\left\vert I\right\vert }\\\text{(by
(\ref{sol.tridiag.fib.sol2.tauI.sign}))}}}\underbrace{\left(  -1\right)
^{\left\vert \operatorname*{Exced}\left(  \tau_{I}\right)  \right\vert }%
}_{\substack{=\left(  -1\right)  ^{\left\vert I\right\vert }\\\text{(by
(\ref{sol.tridiag.fib.sol2.tauI.Exced}))}}}\\
&  =\sum_{I\text{ is a lacunar set}}\underbrace{\left(  -1\right)
^{\left\vert I\right\vert }\left(  -1\right)  ^{\left\vert I\right\vert }%
}_{=\left(  \left(  -1\right)  ^{\left\vert I\right\vert }\right)  ^{2}%
=1}=\sum_{I\text{ is a lacunar set}}1\\
&  =\left(  \text{the number of all lacunar sets}\right)  =f_{n+1}.
\end{align*}
Combining this with (\ref{sol.tridiag.fib.sol2.det1}), we conclude that $\det
A=f_{n+1}$.
\end{proof}

\subsection{Solution to Exercise \ref{exe.block2x2.mult}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.mult}.]This is another case where the
solution is really clear with the appropriate amount of waving hands and
pointing fingers, but on paper becomes nearly impossible to convey. I shall
therefore resort to formalism and computation.

Write the matrices $A$, $B$, $C$, $D$, $A^{\prime}$, $B^{\prime}$, $C^{\prime
}$ and $D^{\prime}$ in the forms%
\begin{align*}
A  &  =\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}%
,\ \ \ \ \ \ \ \ \ \ B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}},\\
C  &  =\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq
m},\ \ \ \ \ \ \ \ \ \ D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m^{\prime}},\\
A^{\prime}  &  =\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq
j\leq\ell},\ \ \ \ \ \ \ \ \ \ B^{\prime}=\left(  b_{i,j}^{\prime}\right)
_{1\leq i\leq m,\ 1\leq j\leq\ell^{\prime}},\\
C^{\prime}  &  =\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime
},\ 1\leq j\leq\ell},\ \ \ \ \ \ \ \ \ \ D^{\prime}=\left(  d_{i,j}^{\prime
}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell^{\prime}}.
\end{align*}
The definition of the $\left(  n+n^{\prime}\right)  \times\left(  m+m^{\prime
}\right)  $-matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ shows that%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cccccccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m} & b_{1,1} & b_{1,2} & \cdots &
b_{1,m^{\prime}}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} & b_{2,1} & b_{2,2} & \cdots &
b_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m} & b_{n,1} & b_{n,2} & \cdots &
b_{n,m^{\prime}}\\
c_{1,1} & c_{1,2} & \cdots & c_{1,m} & d_{1,1} & d_{1,2} & \cdots &
d_{1,m^{\prime}}\\
c_{2,1} & c_{2,2} & \cdots & c_{2,m} & d_{2,1} & d_{2,2} & \cdots &
d_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
c_{n^{\prime},1} & c_{n^{\prime},2} & \cdots & c_{n^{\prime},m} &
d_{n^{\prime},1} & d_{n^{\prime},2} & \cdots & d_{n^{\prime},m^{\prime}}%
\end{array}
\right) \\
&  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\end{align*}
Similarly,%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cccccccc}%
a_{1,1}^{\prime} & a_{1,2}^{\prime} & \cdots & a_{1,\ell}^{\prime} &
b_{1,1}^{\prime} & b_{1,2}^{\prime} & \cdots & b_{1,\ell^{\prime}}^{\prime}\\
a_{2,1}^{\prime} & a_{2,2}^{\prime} & \cdots & a_{2,\ell}^{\prime} &
b_{2,1}^{\prime} & b_{2,2}^{\prime} & \cdots & b_{2,\ell^{\prime}}^{\prime}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
a_{m,1}^{\prime} & a_{m,2}^{\prime} & \cdots & a_{m,\ell}^{\prime} &
b_{m,1}^{\prime} & b_{m,2}^{\prime} & \cdots & b_{m,\ell^{\prime}}^{\prime}\\
c_{1,1}^{\prime} & c_{1,2}^{\prime} & \cdots & c_{1,\ell}^{\prime} &
d_{1,1}^{\prime} & d_{1,2}^{\prime} & \cdots & d_{1,\ell^{\prime}}^{\prime}\\
c_{2,1}^{\prime} & c_{2,2}^{\prime} & \cdots & c_{2,\ell}^{\prime} &
d_{2,1}^{\prime} & d_{2,2}^{\prime} & \cdots & d_{2,\ell^{\prime}}^{\prime}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
c_{m^{\prime},1}^{\prime} & c_{m^{\prime},2}^{\prime} & \cdots & c_{m^{\prime
},\ell}^{\prime} & d_{m^{\prime},1}^{\prime} & d_{m^{\prime},2}^{\prime} &
\cdots & d_{m^{\prime},\ell^{\prime}}^{\prime}%
\end{array}
\right) \\
&  =\left(
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq m\text{ and }j\leq\ell;\\
b_{i,j-\ell}^{\prime}, & \text{if }i\leq m\text{ and }j>\ell;\\
c_{i-m,j}^{\prime}, & \text{if }i>m\text{ and }j\leq\ell;\\
d_{i-m,j-\ell}^{\prime}, & \text{if }i>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\end{align*}
Using these two equalities, we can compute the product $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $: Namely,%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.short.3}%
\end{align}


On the other hand, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$ and $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq
m,\ 1\leq j\leq\ell}$. Hence, the definition of the product of two matrices
shows that%
\begin{equation}
AA^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.short.5a}%
\end{equation}


Also, we have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$ and $C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq
m^{\prime},\ 1\leq j\leq\ell}$. Hence, the definition of the product of two
matrices shows that%
\begin{equation}
BC^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}\right)
_{1\leq i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.short.5b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.short.5a}) and
(\ref{sol.block2x2.mult.short.5b}), we obtain%
\begin{align}
AA^{\prime}+BC^{\prime}  &  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime
}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}+\left(  \sum_{k=1}^{m^{\prime}%
}b_{i,k}c_{k,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}b_{i,k-m}c_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}.
\label{sol.block2x2.mult.short.5c}%
\end{align}


Similarly,%
\begin{equation}
AB^{\prime}+BD^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq
n,\ 1\leq j\leq\ell^{\prime}}; \label{sol.block2x2.mult.short.6c}%
\end{equation}%
\begin{equation}
CA^{\prime}+DC^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq\ell}; \label{sol.block2x2.mult.short.7c}%
\end{equation}%
\begin{equation}
CB^{\prime}+DD^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.short.8c}%
\end{equation}


Now, we have the four equalities (\ref{sol.block2x2.mult.short.5c}),
(\ref{sol.block2x2.mult.short.6c}), (\ref{sol.block2x2.mult.short.7c}) and
(\ref{sol.block2x2.mult.short.8c}). Hence, the definition of the block matrix
$\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  $ (or, more precisely, the equality (\ref{eq.def.block2x2.formal}),
applied to $n$, $n^{\prime}$, $\ell$, $\ell^{\prime}$, $AA^{\prime}%
+BC^{\prime}$, $AB^{\prime}+BD^{\prime}$, $CA^{\prime}+DC^{\prime}$,
$CB^{\prime}+DD^{\prime}$, $\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum
_{k=m+1}^{m+m^{\prime}}b_{i,k-m}c_{k-m,j}^{\prime}$, $\sum_{k=1}^{m}%
a_{i,k}b_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime
}$, $\sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i,k-m}c_{k-m,j}^{\prime}$ and $\sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$,
$c_{i,j}$ and $d_{i,j}$) shows that
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.short.9}%
\end{align}


But our goal is to prove that $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  $. In other words, we want to prove that the left hand sides of the
equalities (\ref{sol.block2x2.mult.short.3}) and
(\ref{sol.block2x2.mult.short.9}) are equal. For this, it clearly suffices to
show that the right hand sides of these equalities are equal. In other words,
it suffices to show that every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots,\ell+\ell^{\prime}\right\}  $
satisfies%
\begin{align}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\nonumber\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
. \label{sol.block2x2.mult.short.entrywise}%
\end{align}


\textit{Proof of (\ref{sol.block2x2.mult.short.entrywise}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{
1,2,\ldots,\ell+\ell^{\prime}\right\}  $. Thus, $i\in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  $ and $j\in\left\{  1,2,\ldots,\ell+\ell^{\prime
}\right\}  $. We must be in one of the following four cases:

\textit{Case 1:} We have $i\leq n$ and $j\leq\ell$.

\textit{Case 2:} We have $i\leq n$ and $j>\ell$.

\textit{Case 3:} We have $i>n$ and $j\leq\ell$.

\textit{Case 4:} We have $i>n$ and $j>\ell$.

All four cases are completely analogous; we thus will only show how to deal
with Case 1. In this case, we have $i\leq n$ and $j\leq\ell$. Now, comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=a_{i,k}\\\text{(since }i\leq n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=a_{k,j}^{\prime}\\\text{(since }k\leq m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=b_{i,k-m}\\\text{(since }i\leq n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=c_{k-m,j}^{\prime}\\\text{(since }k>m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq
n\text{ and }j\leq\ell\right)  ,
\end{align*}
we obtain precisely (\ref{sol.block2x2.mult.short.entrywise}). Thus,
(\ref{sol.block2x2.mult.short.entrywise}) is proven in Case 1. As I said, the
other three cases are similar, and so (\ref{sol.block2x2.mult.short.entrywise}%
) is proven.

From (\ref{sol.block2x2.mult.short.entrywise}), we see that the right hand
sides of the equalities (\ref{sol.block2x2.mult.short.3}) and
(\ref{sol.block2x2.mult.short.9}) are equal. Hence, so are their left hand
sides. In other words, $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  $. This solves Exercise \ref{exe.block2x2.mult}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.mult}.]Write the $n\times m$-matrix
$A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.

Write the $n\times m^{\prime}$-matrix $B$ in the form $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$.

Write the $n^{\prime}\times m$-matrix $C$ in the form $C=\left(
c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$.

Write the $n^{\prime}\times m^{\prime}$-matrix $D$ in the form $D=\left(
d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}$.

Write the $m\times\ell$-matrix $A^{\prime}$ in the form $A^{\prime}=\left(
a_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq j\leq\ell}$.

Write the $m\times\ell^{\prime}$-matrix $B^{\prime}$ in the form $B^{\prime
}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq j\leq\ell^{\prime}%
}$.

Write the $m^{\prime}\times\ell$-matrix $C^{\prime}$ in the form $C^{\prime
}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell
}$.

Write the $m^{\prime}\times\ell^{\prime}$-matrix $D^{\prime}$ in the form
$D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime},\ 1\leq
j\leq\ell^{\prime}}$.

We have $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq
j\leq\ell}$, $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq
m,\ 1\leq j\leq\ell^{\prime}}$, $C^{\prime}=\left(  c_{i,j}^{\prime}\right)
_{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell}$ and $D^{\prime}=\left(
d_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell^{\prime}}%
$. Thus, (\ref{eq.def.block2x2.formal}) (applied to $m$, $m^{\prime}$, $\ell$,
$\ell^{\prime}$, $A^{\prime}$, $B^{\prime}$, $C^{\prime}$, $D^{\prime}$,
$a_{i,j}^{\prime}$, $b_{i,j}^{\prime}$, $c_{i,j}^{\prime}$ and $d_{i,j}%
^{\prime}$ instead of $n$, $n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$,
$D$, $a_{i,j}$, $b_{i,j}$, $c_{i,j}$ and $d_{i,j}$) shows that
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq m\text{ and }j\leq\ell;\\
b_{i,j-\ell}^{\prime}, & \text{if }i\leq m\text{ and }j>\ell;\\
c_{i-m,j}^{\prime}, & \text{if }i>m\text{ and }j\leq\ell;\\
d_{i-m,j-\ell}^{\prime}, & \text{if }i>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.1}%
\end{equation}


Now, we have (\ref{eq.def.block2x2.formal}) and (\ref{sol.block2x2.mult.1}).
Thus, the definition of the product of two matrices shows that%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.3}%
\end{align}


On the other hand, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$ and $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq
m,\ 1\leq j\leq\ell}$. Hence, the definition of the product of two matrices
shows that%
\begin{equation}
AA^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.5a}%
\end{equation}


Also, we have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$ and $C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq
m^{\prime},\ 1\leq j\leq\ell}$. Hence, the definition of the product of two
matrices shows that%
\begin{equation}
BC^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}\right)
_{1\leq i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.5b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.5a}) and
(\ref{sol.block2x2.mult.5b}), we obtain%
\begin{align}
AA^{\prime}+BC^{\prime}  &  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime
}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}+\left(  \sum_{k=1}^{m^{\prime}%
}b_{i,k}c_{k,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}b_{i,k-m}c_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}.
\label{sol.block2x2.mult.5c}%
\end{align}


Furthermore, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$ and $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq
j\leq\ell^{\prime}}$. Hence, the definition of the product of two matrices
shows that%
\begin{equation}
AB^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.6a}%
\end{equation}


Also, we have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$ and $D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq i\leq
m^{\prime},\ 1\leq j\leq\ell^{\prime}}$. Hence, the definition of the product
of two matrices shows that%
\begin{equation}
BD^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}b_{i,k}d_{k,j}^{\prime}\right)
_{1\leq i\leq n,\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.6b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.6a}) and
(\ref{sol.block2x2.mult.6b}), we obtain%
\begin{align}
AB^{\prime}+BD^{\prime}  &  =\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime
}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell^{\prime}}+\left(  \sum
_{k=1}^{m^{\prime}}b_{i,k}d_{k,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}b_{i,k}d_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell
^{\prime}}. \label{sol.block2x2.mult.6c}%
\end{align}


Furthermore, we have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m}$ and $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq
i\leq m,\ 1\leq j\leq\ell}$. Hence, the definition of the product of two
matrices shows that%
\begin{equation}
CA^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}\right)  _{1\leq
i\leq n^{\prime},\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.7a}%
\end{equation}


Also, we have $D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq m^{\prime}}$ and $C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq
i\leq m^{\prime},\ 1\leq j\leq\ell}$. Hence, the definition of the product of
two matrices shows that%
\begin{equation}
DC^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}d_{i,k}c_{k,j}^{\prime}\right)
_{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.7b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.7a}) and
(\ref{sol.block2x2.mult.7b}), we obtain%
\begin{align}
CA^{\prime}+DC^{\prime}  &  =\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime
}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell}+\left(  \sum
_{k=1}^{m^{\prime}}d_{i,k}c_{k,j}^{\prime}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}d_{i,k}c_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}d_{i,k-m}c_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq\ell}. \label{sol.block2x2.mult.7c}%
\end{align}


Furthermore, we have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m}$ and $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq
i\leq m,\ 1\leq j\leq\ell^{\prime}}$. Hence, the definition of the product of
two matrices shows that%
\begin{equation}
CB^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}\right)  _{1\leq
i\leq n^{\prime},\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.8a}%
\end{equation}


Also, we have $D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq m^{\prime}}$ and $D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq
i\leq m^{\prime},\ 1\leq j\leq\ell^{\prime}}$. Hence, the definition of the
product of two matrices shows that%
\begin{equation}
DD^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}d_{i,k}d_{k,j}^{\prime}\right)
_{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell^{\prime}}.
\label{sol.block2x2.mult.8b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.8a}) and
(\ref{sol.block2x2.mult.8b}), we obtain%
\begin{align}
CB^{\prime}+DD^{\prime}  &  =\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime
}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell^{\prime}}+\left(
\sum_{k=1}^{m^{\prime}}d_{i,k}d_{k,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}d_{i,k}d_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq\ell^{\prime}}. \label{sol.block2x2.mult.8c}%
\end{align}


Now, we have the four equalities (\ref{sol.block2x2.mult.5c}),
(\ref{sol.block2x2.mult.6c}), (\ref{sol.block2x2.mult.7c}) and
(\ref{sol.block2x2.mult.8c}). Hence, (\ref{eq.def.block2x2.formal}) (applied
to $n$, $n^{\prime}$, $\ell$, $\ell^{\prime}$, $AA^{\prime}+BC^{\prime}$,
$AB^{\prime}+BD^{\prime}$, $CA^{\prime}+DC^{\prime}$, $CB^{\prime}+DD^{\prime
}$, $\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}c_{k-m,j}^{\prime}$, $\sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime}$, $\sum_{k=1}%
^{m}c_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}%
c_{k-m,j}^{\prime}$ and $\sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}+\sum
_{k=m+1}^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$,
$c_{i,j}$ and $d_{i,j}$) shows that
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.9}%
\end{align}


Now, we shall show that every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots,\ell+\ell^{\prime}\right\}  $
satisfies%
\begin{align}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\nonumber\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
. \label{sol.block2x2.mult.entrywise}%
\end{align}


\textit{Proof of (\ref{sol.block2x2.mult.entrywise}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{
1,2,\ldots,\ell+\ell^{\prime}\right\}  $. Thus, $i\in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  $ and $j\in\left\{  1,2,\ldots,\ell+\ell^{\prime
}\right\}  $. We must be in one of the following two cases:

\textit{Case 1:} We have $i\leq n$.

\textit{Case 2:} We have $i>n$.

Let us first consider Case 1. In this case, we have $i\leq n$. Now, we must be
in one of the following two subcases:

\textit{Subcase 1.1:} We have $j\leq\ell$.

\textit{Subcase 1.2:} We have $j>\ell$.

Let us first consider Subcase 1.1. In this Subcase, we have $j\leq\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=a_{i,k}\\\text{(since }i\leq n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=a_{k,j}^{\prime}\\\text{(since }k\leq m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=b_{i,k-m}\\\text{(since }i\leq n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=c_{k-m,j}^{\prime}\\\text{(since }k>m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq
n\text{ and }j\leq\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 1.1.

Let us now consider Subcase 1.2. In this Subcase, we have $j>\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=a_{i,k}\\\text{(since }i\leq n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=b_{k,j-\ell}^{\prime}\\\text{(since }k\leq m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=b_{i,k-m}\\\text{(since }i\leq n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=d_{k-m,j-\ell}^{\prime}\\\text{(since }k>m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i\leq n\text{ and }j>\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 1.2.

We have thus proven (\ref{sol.block2x2.mult.entrywise}) in each of the two
Subcases 1.1 and 1.2. Since these two Subcases cover the whole Case 1, this
shows that (\ref{sol.block2x2.mult.entrywise}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $i>n$. Now, we must be in
one of the following two subcases:

\textit{Subcase 2.1:} We have $j\leq\ell$.

\textit{Subcase 2.2:} We have $j>\ell$.

Let us first consider Subcase 2.1. In this Subcase, we have $j\leq\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=c_{i-n,k}\\\text{(since }i>n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=a_{k,j}^{\prime}\\\text{(since }k\leq m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=d_{i-n,k-m}\\\text{(since }i>n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=c_{k-m,j}^{\prime}\\\text{(since }k>m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}c_{k-m,j}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}c_{k-m,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i>n\text{ and }j\leq\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 2.1.

Let us now consider Subcase 2.2. In this Subcase, we have $j>\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=c_{i-n,k}\\\text{(since }i>n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=b_{k,j-\ell}^{\prime}\\\text{(since }k\leq m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=d_{i-n,k-m}\\\text{(since }i>n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=d_{k-m,j-\ell}^{\prime}\\\text{(since }k>m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i>n\text{ and }j>\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 2.2.

We have thus proven (\ref{sol.block2x2.mult.entrywise}) in each of the two
Subcases 2.1 and 2.2. Since these two Subcases cover the whole Case 2, this
shows that (\ref{sol.block2x2.mult.entrywise}) is proven in Case 2.

We have thus proven (\ref{sol.block2x2.mult.entrywise}) in each of the two
Cases 1 and 2. Thus, (\ref{sol.block2x2.mult.entrywise}) always holds. This
completes our proof of (\ref{sol.block2x2.mult.entrywise}).

Now, (\ref{sol.block2x2.mult.3}) becomes%
\begin{align*}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \\
&  =\left(  \underbrace{\sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\\text{(by (\ref{sol.block2x2.mult.entrywise}))}}}\right)  _{1\leq i\leq
n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}\\
&  =\left(
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}\\
&  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.block2x2.mult.9}%
)}\right)  .
\end{align*}
This solves Exercise \ref{exe.block2x2.mult}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.block2x2.tridet}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.tridet}.]We shall prove Exercise
\ref{exe.block2x2.tridet} by induction over $m$:

\textit{Induction base:} Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Let $B$ be an $n\times0$-matrix\footnote{Of course, there is only
one such $n\times0$-matrix (namely, the empty matrix).}. Let $D$ be a
$0\times0$-matrix\footnote{Of course, there is only one such $0\times0$-matrix
(namely, the empty matrix).}. Then, all three matrices $B$, $0_{0\times n}$
and $D$ are empty (in the sense that each of them has either $0$ rows or $0$
columns or both), and thus we have $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =A$. Hence, $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A$. Combined with $\det D=1$ (since $D$ is a $0\times
0$-matrix), this yields $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$.

Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have proven
that every $n\in\mathbb{N}$, every $n\times n$-matrix $A$, every $n\times
0$-matrix $B$ and every $0\times0$-matrix $D$ satisfy $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=0$. This completes the induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$ be positive. Assume that
Exercise \ref{exe.block2x2.tridet} holds for $m=M-1$. We need to prove that
Exercise \ref{exe.block2x2.tridet} holds for $m=M$.

Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let $B$ be an $n\times
M$-matrix. Let $D$ be an $M\times M$-matrix.

Write the $M\times M$-matrix $D$ in the form $D=\left(  d_{i,j}\right)
_{1\leq i\leq M,\ 1\leq j\leq M}$. Hence, Theorem \ref{thm.laplace.gen}
\textbf{(a)} (applied to $M$, $D$, $d_{i,j}$ and $M$ instead of $n$, $A$,
$a_{i,j}$ and $p$) shows that%
\begin{equation}
\det D=\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det\left(  D_{\sim
M,\sim q}\right)  \label{sol.block2x2.tridet.short.indstep.detD}%
\end{equation}
(since $M\in\left\{  1,2,\ldots,M\right\}  $ (because $M>0$)).

Write the $\left(  n+M\right)  \times\left(  n+M\right)  $-matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $ in the form $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\left(  u_{i,j}\right)  _{1\leq u\leq n+M,\ 1\leq v\leq n+M}$. Thus,%
\begin{equation}
u_{n+M,q}=0\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{  1,2,\ldots
,n\right\}  , \label{sol.block2x2.tridet.short.indstep.u=0}%
\end{equation}
and%
\begin{equation}
u_{n+M,n+q}=d_{M,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,M\right\}  . \label{sol.block2x2.tridet.short.indstep.u=d}%
\end{equation}


Furthermore, for every $q\in\left\{  1,2,\ldots,M\right\}  $, we have%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }=\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  , \label{sol.block2x2.tridet.short.indstep.1}%
\end{equation}
where $B_{q}^{\prime}$ is the result of crossing out the $q$-th column in the
matrix $B$. (Draw the matrices and cross out the appropriate rows and columns
to see why this is true.)

But we have assumed that Exercise \ref{exe.block2x2.tridet} holds for $m=M-1$.
Hence, for every $q\in\left\{  1,2,\ldots,M\right\}  $, we can apply Exercise
\ref{exe.block2x2.tridet} to $M-1$, $B_{q}^{\prime}$ and $D_{\sim M,\sim q}$
instead of $m$, $B$ and $D$. As a result, for every $q\in\left\{
1,2,\ldots,M\right\}  $, we obtain%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  .
\]
Now, taking determinants in (\ref{sol.block2x2.tridet.short.indstep.1}), we
obtain%
\begin{align}
\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)   &
=\det\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right) \nonumber\\
&  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  .
\label{sol.block2x2.tridet.short.indstep.2}%
\end{align}


But $n+M>0$ and thus $n+M\in\left\{  1,2,\ldots,n+M\right\}  $. Thus, Theorem
\ref{thm.laplace.gen} \textbf{(a)} (applied to $n+M$, $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $, $u_{i,j}$ and $n+M$ instead of $n$, $A$, $a_{i,j}$ and $p$) shows
that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right) \\
&  =\sum_{q=1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}u_{n+M,q}%
\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
\underbrace{u_{n+M,q}}_{\substack{=0\\\text{(by
(\ref{sol.block2x2.tridet.short.indstep.u=0}))}}}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq n\leq n+M\right) \\
&  =\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)
+q}0\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right)  }_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  =\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
u_{n+M,q}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  =\sum_{q=1}^{M}\underbrace{\left(  -1\right)  ^{\left(  n+M\right)
+\left(  n+q\right)  }}_{\substack{=\left(  -1\right)  ^{M+q}\\\text{(since
}\left(  n+M\right)  +\left(  n+q\right)  \\=2n+M+q\equiv
M+q\operatorname{mod}2\text{)}}}\underbrace{u_{n+M,n+q}}_{\substack{=d_{M,q}%
\\\text{(by (\ref{sol.block2x2.tridet.short.indstep.u=d}))}}}\underbrace{\det
\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)
}_{\substack{=\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  \\\text{(by
(\ref{sol.block2x2.tridet.short.indstep.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n+q\text{ for
}q\text{ in the sum}\right) \\
&  =\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det A\cdot\det\left(
D_{\sim M,\sim q}\right)  =\det A\cdot\underbrace{\sum_{q=1}^{M}\left(
-1\right)  ^{M+q}d_{M,q}\det\left(  D_{\sim M,\sim q}\right)  }%
_{\substack{=\det D\\\text{(by (\ref{sol.block2x2.tridet.short.indstep.detD}%
))}}}\\
&  =\det A\cdot\det D.
\end{align*}


Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have shown
that for every $n\in\mathbb{N}$, for every $n\times n$-matrix $A$, for every
$n\times M$-matrix $B$, and for every $M\times M$-matrix $D$, we have
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=M$. This completes the induction step.
Hence, Exercise \ref{exe.block2x2.tridet} is solved by induction.
\end{proof}
\end{vershort}

\begin{verlong}
Before we prove Exercise \ref{exe.block2x2.tridet}, we state a lemma (which is
just a straightforward formalization of an obvious fact):

\begin{lemma}
\label{lem.block2x2.tridet.last-row-minor} Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$ be an $n\times m$-matrix. Let $q\in\left\{  1,2,\ldots,m\right\}  $. For
every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}_{r}\left(
j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.

\textbf{(a)} We have $\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q}%
,\ldots,m}A=\left(  a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq m-1}$.

\textbf{(b)} If $n$ is positive, then $A_{\sim n,\sim q}=\left(
a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
m-1}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.block2x2.tridet.last-row-minor}.]We have
$q\in\left\{  1,2,\ldots,m\right\}  $, so that $1\leq q\leq m$, so that $1\leq
m$. Thus, $m-1\in\mathbb{N}$.

Define an $\left(  m-1\right)  $-tuple $\left(  u_{1},u_{2},\ldots
,u_{m-1}\right)  $ by $\left(  u_{1},u_{2},\ldots,u_{m-1}\right)  =\left(
1,2,\ldots,\widehat{q},\ldots,m\right)  $. Thus, for every $j\in\left\{
1,2,\ldots,m-1\right\}  $, we have%
\begin{equation}
u_{j}=%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
=\mathbf{d}_{q}\left(  j\right)
\label{pf.lem.block2x2.tridet.last-row-minor.uj}%
\end{equation}
(since $\mathbf{d}_{q}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
$ (by the definition of $\mathbf{d}_{q}$)).

\textbf{(a)} We have%
\begin{align*}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,m}A  &
=\operatorname*{cols}\nolimits_{u_{1},u_{2},\ldots,u_{m-1}}%
A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,m\right)  =\left(  u_{1},u_{2},\ldots,u_{m-1}\right)  \right) \\
&  =\left(  a_{i,u_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq m-1}%
\end{align*}
(by the definition of $\operatorname*{cols}\nolimits_{u_{1},u_{2}%
,\ldots,u_{m-1}}A$, since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$). Thus,%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,m}A=\left(
a_{i,u_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq m-1}=\left(  a_{i,u_{j}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq m-1}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  i,j\right)  $).
Hence,%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,m}A=\left(
\underbrace{a_{i,u_{j}}}_{\substack{=a_{i,\mathbf{d}_{q}\left(  j\right)
}\\\text{(since }u_{j}=\mathbf{d}_{q}\left(  j\right)  \\\text{(by
(\ref{pf.lem.block2x2.tridet.last-row-minor.uj})))}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-1}=\left(  a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}.
\]
This proves Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(a)}.

\textbf{(b)} Assume that $n$ is positive. The definition of $A_{\sim n,\sim
q}$ yields%
\begin{align*}
A_{\sim n,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{n}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,m}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,n-1}^{1,2,\ldots,\widehat{q},\ldots,m}%
A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{n}%
,\ldots,n\right)  =\left(  1,2,\ldots,n-1\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{1,2,\ldots,n-1}^{u_{1},u_{2},\ldots,u_{m-1}%
}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,m\right)  =\left(  u_{1},u_{2},\ldots,u_{m-1}\right)  \right) \\
&  =\left(  a_{x,u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq m-1}%
\end{align*}
(by the definition of $\operatorname*{sub}\nolimits_{1,2,\ldots,n-1}%
^{u_{1},u_{2},\ldots,u_{m-1}}A$, since $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$). Thus,%
\[
A_{\sim n,\sim q}=\left(  a_{x,u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq
m-1}=\left(  a_{i,u_{j}}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m-1}%
\]
(here, we renamed the index $\left(  x,y\right)  $ as $\left(  i,j\right)  $).
Hence,%
\[
A_{\sim n,\sim q}=\left(  \underbrace{a_{i,u_{j}}}_{\substack{=a_{i,\mathbf{d}%
_{q}\left(  j\right)  }\\\text{(since }u_{j}=\mathbf{d}_{q}\left(  j\right)
\\\text{(by (\ref{pf.lem.block2x2.tridet.last-row-minor.uj})))}}}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq m-1}=\left(  a_{i,\mathbf{d}_{q}\left(
j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m-1}.
\]
This proves Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(b)}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.block2x2.tridet}.]We shall prove Exercise
\ref{exe.block2x2.tridet} by induction over $m$:

\textit{Induction base:} Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Let $B$ be an $n\times0$-matrix\footnote{Of course, there is only
one such $n\times0$-matrix (namely, the empty matrix).}. Let $D$ be a
$0\times0$-matrix\footnote{Of course, there is only one such $0\times0$-matrix
(namely, the empty matrix).}. Then, the matrix $B$ has $0$ columns (since it
is an $n\times0$-matrix), whereas the matrix $0_{0\times n}$ has $0$ rows
(since it is a $0\times n$-matrix), and the matrix $D$ has $0$ rows (since it
is a $0\times0$-matrix). Thus, each of the three matrices $B$, $D$ and
$0_{0\times n}$ has either $0$ rows or $0$ columns. Therefore%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =A \label{sol.block2x2.tridet.indbase.1}%
\end{equation}
\footnote{Here is a more formal \textit{proof of
(\ref{sol.block2x2.tridet.indbase.1}):} Write the $n\times n$-matrix $A$ in
the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
\par
Write the $n\times0$-matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq0}$.
\par
We have $0_{0\times n}=\left(  0\right)  _{1\leq i\leq0,\ 1\leq j\leq n}$ (by
the definition of $0_{0\times n}$).
\par
Write the $0\times0$-matrix $D$ in the form $D=\left(  d_{i,j}\right)  _{1\leq
i\leq0,\ 1\leq j\leq0}$.
\par
Now, (\ref{eq.def.block2x2.formal}) (applied to $n$, $0$, $0$, $0_{0\times n}$
and $0$ instead of $n^{\prime}$, $m$, $m^{\prime}$, $C$ and $c_{i,j}$) shows
that
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)   &  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+0,\ 1\leq j\leq n+0}\\
&  =\left(  \underbrace{%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
}_{\substack{=a_{i,j}\\\text{(since }i\leq n\text{ and }j\leq n\text{)}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }n+0=n\right) \\
&  =\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A.
\end{align*}
This proves (\ref{sol.block2x2.tridet.indbase.1}).}. Hence,
\begin{equation}
\det\underbrace{\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  }_{=A}=\det A. \label{sol.block2x2.tridet.indbase.2}%
\end{equation}
But $D$ is a $0\times0$-matrix, and thus has determinant $\det D=1$. Hence,
$\det A\cdot\underbrace{\det D}_{=1}=\det A$. Compared with
(\ref{sol.block2x2.tridet.indbase.2}), this yields $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$.

Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have proven
that every $n\in\mathbb{N}$, every $n\times n$-matrix $A$, every $n\times
0$-matrix $B$ and every $0\times0$-matrix $D$ satisfy $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=0$. This completes the induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$ be positive. Assume that
Exercise \ref{exe.block2x2.tridet} holds for $m=M-1$. We need to prove that
Exercise \ref{exe.block2x2.tridet} holds for $m=M$.

Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let $B$ be an $n\times
M$-matrix. Let $D$ be an $M\times M$-matrix. Let $U$ be the $\left(
n+M\right)  \times\left(  n+M\right)  $-matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $. Write the matrix $U$ in the form $U=\left(  u_{i,j}\right)
_{1\leq u\leq n+M,\ 1\leq v\leq n+M}$.

Write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

Write the $n\times M$-matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq M}$.

We have $0_{M\times n}=\left(  0\right)  _{1\leq i\leq M,\ 1\leq j\leq n}$ (by
the definition of $0_{M\times n}$).

Write the $M\times M$-matrix $D$ in the form $D=\left(  d_{i,j}\right)
_{1\leq i\leq M,\ 1\leq j\leq M}$.

We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq M}$, $0_{M\times
n}=\left(  0\right)  _{1\leq i\leq M,\ 1\leq j\leq n}$ and $D=\left(
d_{i,j}\right)  _{1\leq i\leq M,\ 1\leq j\leq M}$. Thus,
(\ref{eq.def.block2x2.formal}) (applied to $n$, $M$, $n$, $M$, $A$, $B$,
$0_{M\times n}$, $D$, $a_{i,j}$, $b_{i,j}$, $0$ and $d_{i,j}$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$,
$c_{i,j}$ and $d_{i,j}$) shows that
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}.
\label{sol.block2x2.tridet.indstep.U0}%
\end{equation}
Thus,%
\begin{align}
U  &  =\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}.
\label{sol.block2x2.tridet.indstep.U}%
\end{align}


Now, for every $q\in\left\{  1,2,\ldots,M\right\}  $, let us denote the
$n\times\left(  M-1\right)  $-matrix $\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{q},\ldots,M}B$ by $B_{q}^{\prime}$. Thus,%
\[
B_{q}^{\prime}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots
,M}B\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{  1,2,\ldots,M\right\}  .
\]


Write the $\left(  n+M\right)  \times\left(  n+M\right)  $-matrix $U$ in the
form $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}$. Thus,%
\[
\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}=U=\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}%
\]
(by (\ref{sol.block2x2.tridet.indstep.U})). In other words,%
\begin{equation}
u_{i,j}=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\label{sol.block2x2.tridet.indstep.minor.pf.1}%
\end{equation}
for all $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+M\right\}  ^{2}$.

Now, it is easy to see that%
\begin{equation}
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }=\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  \label{sol.block2x2.tridet.indstep.minor}%
\end{equation}
for every $q\in\left\{  1,2,\ldots,M\right\}  $\ \ \ \ \footnote{\textit{Proof
of (\ref{sol.block2x2.tridet.indstep.minor}):} Let $q\in\left\{
1,2,\ldots,M\right\}  $. Notice that $M$ is positive, and thus $n+M$ is
positive. Also, $q\in\left\{  1,2,\ldots,M\right\}  $, so that $n+q\in\left\{
n+1,n+2,\ldots,n+M\right\}  \subseteq\left\{  1,2,\ldots,n+M\right\}  $.
\par
For every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}_{r}\left(
j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.
\par
Now, Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(b)} (applied to
$n+M$, $n+M$, $n+q$, $U$ and $u_{i,j}$ instead of $n$, $m$, $q$, $A$ and
$a_{i,j}$) yields%
\begin{equation}
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }=\left(  u_{i,\mathbf{d}%
_{n+q}\left(  j\right)  }\right)  _{1\leq i\leq n+M-1,\ 1\leq j\leq n+M-1}
\label{sol.block2x2.tridet.indstep.minor.pf.2}%
\end{equation}
(since $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}$).
\par
On the other hand, $D=\left(  d_{i,j}\right)  _{1\leq i\leq M,\ 1\leq j\leq
M}$. Hence, Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(b)}
(applied to $M$, $M$, $D$ and $d_{i,j}$ instead of $n$, $m$, $A$ and $a_{i,j}%
$) yields%
\begin{equation}
D_{\sim M,\sim q}=\left(  d_{i,\mathbf{d}_{q}\left(  j\right)  }\right)
_{1\leq i\leq M-1,\ 1\leq j\leq M-1}
\label{sol.block2x2.tridet.indstep.minor.pf.3}%
\end{equation}
\par
Also, $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq M}$. Hence,
Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(a)} (applied to $n$,
$M$, $B$ and $b_{i,j}$ instead of $n$, $m$, $A$ and $a_{i,j}$) yields%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,M}B=\left(
b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
M-1}.
\]
Thus,%
\begin{equation}
B_{q}^{\prime}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots
,M}B=\left(  b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq M-1}. \label{sol.block2x2.tridet.indstep.minor.pf.4}%
\end{equation}
\par
Now, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
$B_{q}^{\prime}=\left(  b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq M-1}$, $0_{\left(  M-1\right)  \times n}=\left(
0\right)  _{1\leq i\leq M-1,\ 1\leq j\leq n}$ (by the definition of
$0_{\left(  M-1\right)  \times n}$) and $D_{\sim M,\sim q}=\left(
d_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq M-1,\ 1\leq j\leq
M-1}$. Thus, (\ref{eq.def.block2x2.formal}) (applied to $n$, $M-1$, $n$,
$M-1$, $A$, $B_{q}^{\prime}$, $0_{M\times n}$, $D_{\sim M,\sim q}$, $a_{i,j}$,
$b_{i,\mathbf{d}_{q}\left(  j\right)  }$, $0$ and $d_{i,\mathbf{d}_{q}\left(
j\right)  }$ instead of $n$, $n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$,
$D$, $a_{i,j}$, $b_{i,j}$, $c_{i,j}$ and $d_{i,j}$) shows that
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M-1,\ 1\leq j\leq n+M-1}.
\label{sol.block2x2.tridet.indstep.minor.pf.6}%
\end{equation}
\par
Now, we shall prove that%
\begin{equation}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\label{sol.block2x2.tridet.indstep.minor.pf.8}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+M-1\right\}  ^{2}$.
\par
\textit{Proof of (\ref{sol.block2x2.tridet.indstep.minor.pf.8}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+M-1\right\}  ^{2}$. Thus, $i\in\left\{
1,2,\ldots,n+M-1\right\}  $ and $j\in\left\{  1,2,\ldots,n+M-1\right\}  $.
\par
The definition of $\mathbf{d}_{n+q}$ yields $\mathbf{d}_{n+q}\left(  j\right)
=%
\begin{cases}
j, & \text{if }j<n+q;\\
j+1, & \text{if }j\geq n+q
\end{cases}
$. The definition of $\mathbf{d}_{q}$ yields $\mathbf{d}_{q}\left(
j-n\right)  =%
\begin{cases}
j-n, & \text{if }j-n<q;\\
j-n+1, & \text{if }j-n\geq q
\end{cases}
$ and $\mathbf{d}_{q}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
$.
\par
We have%
\begin{align*}
\mathbf{d}_{n+q}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j+1, & \text{if }j\geq n+q
\end{cases}
=%
\begin{cases}
j, & \text{if }j-n<q;\\
j+1, & \text{if }j-n\geq q
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the condition }j<n+q\text{ is equivalent to }j-n<q\text{,}\\
\text{and since the condition }j\geq n+q\text{ is equivalent to }j-n\geq q
\end{array}
\right)  .
\end{align*}
Subtracting $n$ from both sides of this equality, we obtain
\[
\mathbf{d}_{n+q}\left(  j\right)  -n=%
\begin{cases}
j, & \text{if }j-n<q;\\
j+1, & \text{if }j-n\geq q
\end{cases}
-n=%
\begin{cases}
j-n, & \text{if }j-n<q;\\
j-n+1, & \text{if }j-n\geq q
\end{cases}
=\mathbf{d}_{q}\left(  j-n\right)  .
\]
\par
Notice that%
\begin{align*}
\mathbf{d}_{n+q}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j+1, & \text{if }j\geq n+q
\end{cases}
\geq%
\begin{cases}
j, & \text{if }j<n+q;\\
j, & \text{if }j\geq n+q
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1\geq j\text{ in the case when
}j\geq n+q\right) \\
&  =j.
\end{align*}
\par
We must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq n$.
\par
\textit{Case 2:} We have $i>n$.
\par
Let us first consider Case 1. In this case, we have $i\leq n$. We are in one
of the following two subcases:
\par
\textit{Subcase 1.1:} We have $j\leq n$.
\par
\textit{Subcase 1.2:} We have $j>n$.
\par
Let us first consider Subcase 1.1. In this Subcase, we have $j\leq n$. Now,
$j\leq n<n+q$ (since $n+\underbrace{q}_{>0}>n$) and $\mathbf{d}_{n+q}\left(
j\right)  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j-1, & \text{if }j\geq n+q
\end{cases}
=j$ (since $j<n+q$). Thus,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =u_{i,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{n+q}\left(  j\right)
=j\right) \\
&  =%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1})}\right) \\
&  =a_{i,j}%
\end{align*}
(since $i\leq n$ and $j\leq n$). Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq n\text{ and }j\leq
n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
1.1.
\par
Let us now consider Subcase 1.2. In this Subcase, we have $j>n$. Hence,
$\mathbf{d}_{n+q}\left(  j\right)  \geq j>n$. Now,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =%
\begin{cases}
a_{i,\mathbf{d}_{n+q}\left(  j\right)  } & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
b_{i,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n;\\
0, & \text{if }i>n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
d_{i-n,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i>n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1}), applied to }\left(
i,\mathbf{d}_{n+q}\left(  j\right)  \right)  \text{ instead of }\left(
i,j\right)  \right) \\
&  =b_{i,\mathbf{d}_{n+q}\left(  j\right)  -n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\leq n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  >n\right) \\
&  =b_{i,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathbf{d}_{n+q}\left(  j\right)  -n=\mathbf{d}_{q}\left(
j-n\right)  \right)  .
\end{align*}
Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=b_{i,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\leq n\text{ and }j>n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
1.2.
\par
We have thus proven (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) in each of
the two Subcases 1.1 and 1.2. Since these two Subcases cover the whole Case 1,
this shows that (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) holds in Case
1.
\par
Let us now consider Case 2. In this case, we have $i>n$. We are in one of the
following two subcases:
\par
\textit{Subcase 2.1:} We have $j\leq n$.
\par
\textit{Subcase 2.2:} We have $j>n$.
\par
Let us first consider Subcase 2.1. In this Subcase, we have $j\leq n$. Now,
$j\leq n<n+q$ (since $n+\underbrace{q}_{>0}>n$) and $\mathbf{d}_{n+q}\left(
j\right)  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j-1, & \text{if }j\geq n+q
\end{cases}
=j$ (since $j<n+q$). Thus,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =u_{i,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{n+q}\left(  j\right)
=j\right) \\
&  =%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1})}\right) \\
&  =0
\end{align*}
(since $i>n$ and $j\leq n$). Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i>n\text{ and }j\leq n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
2.1.
\par
Let us now consider Subcase 2.2. In this Subcase, we have $j>n$. Hence,
$\mathbf{d}_{n+q}\left(  j\right)  \geq j>n$. Now,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =%
\begin{cases}
a_{i,\mathbf{d}_{n+q}\left(  j\right)  } & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
b_{i,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n;\\
0, & \text{if }i>n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
d_{i-n,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i>n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1}), applied to }\left(
i,\mathbf{d}_{n+q}\left(  j\right)  \right)  \text{ instead of }\left(
i,j\right)  \right) \\
&  =d_{i-n,\mathbf{d}_{n+q}\left(  j\right)  -n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i>n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  >n\right) \\
&  =d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathbf{d}_{n+q}\left(  j\right)  -n=\mathbf{d}_{q}\left(
j-n\right)  \right)  .
\end{align*}
Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i>n\text{ and }j>n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
2.2.
\par
We have thus proven (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) in each of
the two Subcases 2.1 and 2.2. Since these two Subcases cover the whole Case 2,
this shows that (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) holds in Case
2.
\par
Thus, we have shown that (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) holds
in each of the two Cases 1 and 2. Hence,
(\ref{sol.block2x2.tridet.indstep.minor.pf.8}) always holds. This completes
the proof of (\ref{sol.block2x2.tridet.indstep.minor.pf.8}).
\par
Now, (\ref{sol.block2x2.tridet.indstep.minor.pf.2}) becomes%
\begin{align*}
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }  &  =\left(
\underbrace{u_{i,\mathbf{d}_{n+q}\left(  j\right)  }}_{\substack{=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\\\text{(by (\ref{sol.block2x2.tridet.indstep.minor.pf.8}))}}}\right)  _{1\leq
i\leq n+M-1,\ 1\leq j\leq n+M-1}\\
&  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M-1,\ 1\leq j\leq n+M-1}=\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)
\end{align*}
(by (\ref{sol.block2x2.tridet.indstep.minor.pf.6})). This proves
(\ref{sol.block2x2.tridet.indstep.minor}).}. Hence, for every $q\in\left\{
1,2,\ldots,M\right\}  $, we have%
\begin{equation}
\det\left(  U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)
=\det A\cdot\det\left(  D_{\sim M,\sim q}\right)
\label{sol.block2x2.tridet.indstep.minor-det}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.block2x2.tridet.indstep.minor-det}):} Let
$q\in\left\{  1,2,\ldots,M\right\}  $. We have assumed that Exercise
\ref{exe.block2x2.tridet} holds for $m=M-1$. Hence, we can apply Exercise
\ref{exe.block2x2.tridet} to $M-1$, $B_{q}^{\prime}$ and $D_{\sim M,\sim q}$
instead of $m$, $B$ and $D$. As a result, we obtain%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  .
\]
Since $\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }$, this
rewrites as $\det\left(  U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)
}\right)  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  $. This proves
(\ref{sol.block2x2.tridet.indstep.minor-det}).}.

Furthermore, for every $q\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
u_{n+M,q}=0 \label{sol.block2x2.tridet.indstep.u=0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.block2x2.tridet.indstep.u=0}):} Let
$q\in\left\{  1,2,\ldots,n\right\}  $. Thus, $q\leq n$. Also,
$n+\underbrace{M}_{>0}>n$. Now, (\ref{sol.block2x2.tridet.indstep.minor.pf.1})
(applied to $\left(  i,j\right)  =\left(  n+M,q\right)  $) yields%
\[
u_{n+M,q}=%
\begin{cases}
a_{n+M,q} & \text{if }n+M\leq n\text{ and }q\leq n;\\
b_{n+M,q-n}, & \text{if }n+M\leq n\text{ and }q>n;\\
0, & \text{if }n+M>n\text{ and }q\leq n;\\
d_{n+M-n,q-n}, & \text{if }n+M>n\text{ and }q>n
\end{cases}
=0
\]
(since $n+M>n$ and $q\leq n$). This proves
(\ref{sol.block2x2.tridet.indstep.u=0}).}. On the other hand, for every
$q\in\left\{  1,2,\ldots,M\right\}  $, we have%
\begin{equation}
u_{n+M,n+q}=d_{M,q} \label{sol.block2x2.tridet.indstep.u=d}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.block2x2.tridet.indstep.u=d}):} Let
$q\in\left\{  1,2,\ldots,M\right\}  $. Thus, $q>0$, so that $n+\underbrace{q}%
_{>0}>n$. Also, $n+\underbrace{M}_{>0}>n$. Now,
(\ref{sol.block2x2.tridet.indstep.minor.pf.1}) (applied to $\left(
i,j\right)  =\left(  n+M,n+q\right)  $) yields%
\begin{align*}
u_{n+M,n+q}  &  =%
\begin{cases}
a_{n+M,n+q} & \text{if }n+M\leq n\text{ and }n+q\leq n;\\
b_{n+M,n+q-n}, & \text{if }n+M\leq n\text{ and }n+q>n;\\
0, & \text{if }n+M>n\text{ and }n+q\leq n;\\
d_{n+M-n,n+q-n}, & \text{if }n+M>n\text{ and }n+q>n
\end{cases}
\\
&  =d_{n+M-n,n+q-n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n+M>n\text{ and
}n+q>n\right) \\
&  =d_{M,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n+M-n=M\text{ and
}n+q-n=q\right)  ,
\end{align*}
This proves (\ref{sol.block2x2.tridet.indstep.u=d}).}.

Now, recall that $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq
n+M}$ and $n+M\in\left\{  1,2,\ldots,n+M\right\}  $ (since $n+M$ is positive).
Hence, Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to $n+M$, $U$,
$u_{i,j}$ and $n+M$ instead of $n$, $A$, $a_{i,j}$ and $p$) shows that%
\begin{align}
\det U  &  =\sum_{q=1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)
+q}u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
\underbrace{u_{n+M,q}}_{\substack{=0\\\text{(by
(\ref{sol.block2x2.tridet.indstep.u=0}))}}}\det\left(  U_{\sim\left(
n+M\right)  ,\sim q}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq n\leq n+M\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)
+q}0\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right)  }_{=0}%
+\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}u_{n+M,q}%
\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{M}\underbrace{\left(  -1\right)  ^{\left(  n+M\right)
+\left(  n+q\right)  }}_{\substack{=\left(  -1\right)  ^{M+q}\\\text{(since
}\left(  n+M\right)  +\left(  n+q\right)  =2n+M+q\equiv M+q\operatorname{mod}%
2\text{)}}}\underbrace{u_{n+M,n+q}}_{\substack{=d_{M,q}\\\text{(by
(\ref{sol.block2x2.tridet.indstep.u=d}))}}}\underbrace{\det\left(
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)  }%
_{\substack{=\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  \\\text{(by
(\ref{sol.block2x2.tridet.indstep.minor-det}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n+q\text{ for
}q\text{ in the sum}\right) \nonumber\\
&  =\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det A\cdot\det\left(
D_{\sim M,\sim q}\right) \nonumber\\
&  =\det A\cdot\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det\left(
D_{\sim M,\sim q}\right)  . \label{sol.block2x2.tridet.indstep.detU}%
\end{align}


On the other hand, $D=\left(  d_{i,j}\right)  _{1\leq i\leq M,\ 1\leq j\leq
M}$ and $M\in\left\{  1,2,\ldots,M\right\}  $ (since $M>0$). Hence, Theorem
\ref{thm.laplace.gen} \textbf{(a)} (applied to $M$, $D$, $d_{i,j}$ and $M$
instead of $n$, $A$, $a_{i,j}$ and $p$) shows that%
\[
\det D=\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det\left(  D_{\sim
M,\sim q}\right)  .
\]
Thus, (\ref{sol.block2x2.tridet.indstep.detU}) becomes%
\[
\det U=\det A\cdot\underbrace{\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}%
d_{M,q}\det\left(  D_{\sim M,\sim q}\right)  }_{=\det D}=\det A\cdot\det D.
\]
Since $U=\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $, this rewrites as $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\det A\cdot\det D$.

Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have shown
that for every $n\in\mathbb{N}$, for every $n\times n$-matrix $A$, for every
$n\times M$-matrix $B$, and for every $M\times M$-matrix $D$, we have
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=M$. This completes the induction step.
Hence, Exercise \ref{exe.block2x2.tridet} is solved by induction.
\end{proof}
\end{verlong}

\subsection{Second solution to Exercise \ref{exe.ps4.5}}

\begin{proof}
[Second solution to Exercise \ref{exe.ps4.5} (sketched).]\textbf{(b)} We have%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
f & 0 & 0 & 0 & g\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
\ell & m & n & o & p
\end{array}
\right) \\
&  =-\det\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
\ell & m & n & o & p\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
f & 0 & 0 & 0 & g
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(a)}, because we have just}\\
\text{switched the }2\text{-nd and the }5\text{-th rows of the matrix}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{ccccc}%
d & b & c & a & e\\
o & m & n & \ell & p\\
0 & 0 & 0 & h & i\\
0 & 0 & 0 & j & k\\
0 & 0 & 0 & f & g
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(b)}, because we have just}\\
\text{switched the }1\text{-st and the }4\text{-th columns of the matrix}%
\end{array}
\right) \\
&  =\underbrace{\det\left(
\begin{array}
[c]{ccc}%
d & b & c\\
o & m & n\\
0 & 0 & 0
\end{array}
\right)  }_{\substack{=0\\\text{(by Exercise \ref{exe.ps4.6} \textbf{(c)})}%
}}\cdot\det\left(
\begin{array}
[c]{cc}%
j & k\\
f & g
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.block2x2.tridet}, applied to }\left(
\begin{array}
[c]{ccc}%
d & b & c\\
o & m & n\\
0 & 0 & 0
\end{array}
\right)  \text{,}\\
\left(
\begin{array}
[c]{cc}%
a & e\\
\ell & p\\
h & i
\end{array}
\right)  \text{, }\left(
\begin{array}
[c]{cc}%
j & k\\
f & g
\end{array}
\right)  \text{, }2\text{ and }3\text{ instead of }A\text{, }B\text{,
}D\text{, }m\text{ and }n
\end{array}
\right) \\
&  =0.
\end{align*}
This solves Exercise \ref{exe.ps4.5} \textbf{(b)}.

\textbf{(a)} We have%
\begin{align*}
&  \det\left(
\begin{matrix}
a & b & c & d\\
l & 0 & 0 & e\\
k & 0 & 0 & f\\
j & i & h & g
\end{matrix}
\right) \\
&  =-\det\left(
\begin{matrix}
a & b & c & d\\
j & i & h & g\\
k & 0 & 0 & f\\
l & 0 & 0 & e
\end{matrix}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(a)}, because we have just}\\
\text{switched the }2\text{-nd and the }4\text{-th rows of the matrix}%
\end{array}
\right) \\
&  =\det\left(
\begin{matrix}
c & b & a & d\\
h & i & j & g\\
0 & 0 & k & f\\
0 & 0 & l & e
\end{matrix}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(b)}, because we have just}\\
\text{switched the }1\text{-st and the }3\text{-th columns of the matrix}%
\end{array}
\right) \\
&  =\underbrace{\det\left(
\begin{matrix}
c & b\\
h & i
\end{matrix}
\right)  }_{=ci-bh}\cdot\underbrace{\det\left(
\begin{matrix}
k & f\\
l & e
\end{matrix}
\right)  }_{=ek-lf}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.block2x2.tridet}, applied to }\left(
\begin{matrix}
c & b\\
h & i
\end{matrix}
\right)  \text{,}\\
\left(
\begin{matrix}
a & d\\
j & g
\end{matrix}
\right)  \text{, }\left(
\begin{matrix}
k & f\\
l & e
\end{matrix}
\right)  \text{, }2\text{ and }2\text{ instead of }A\text{, }B\text{,
}D\text{, }m\text{ and }n
\end{array}
\right) \\
&  =\left(  ci-bh\right)  \left(  ek-lf\right)  =\left(  bh-ci\right)  \left(
lf-ek\right)  .
\end{align*}
This solves Exercise \ref{exe.ps4.5} \textbf{(a)}. (Notice that we have
obtained the result in its factored form!)
\end{proof}

\subsection{Solution to Exercise \ref{exe.adj(AB)}}

\begin{vershort}
Before we start solving this exercise, let us show some lemmas. The first of
them is a (somewhat disguised) particular case of the Cauchy-Binet formula:
\end{vershort}

\begin{verlong}
Before we start solving this exercise, let us show some lemmas. The first of
them formalizes the (intuitively obvious) fact that the elements of a finite
set of integers can be listed in increasing order in exactly one way:

\begin{lemma}
\label{lem.adj(AB).set.increase}Let $n\in\mathbb{N}$. Let $a_{1},a_{2}%
,\ldots,a_{n}$ be $n$ integers such that $a_{1}<a_{2}<\cdots<a_{n}$. Let
$b_{1},b_{2},\ldots,b_{n}$ be $n$ integers such that $b_{1}<b_{2}<\cdots
<b_{n}$. Assume that $\left\{  a_{1},a_{2},\ldots,a_{n}\right\}  =\left\{
b_{1},b_{2},\ldots,b_{n}\right\}  $. Then, $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  =\left(  b_{1},b_{2},\ldots,b_{n}\right)  $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).set.increase}.]We have $b_{1}<b_{2}%
<\cdots<b_{n}$. In other words, if $p$ and $q$ are two elements of $\left\{
1,2,\ldots,n\right\}  $ satisfying $p<q$, then%
\begin{equation}
b_{p}<b_{q}. \label{pf.lem.adj(AB).set.increase.binc}%
\end{equation}
Hence, if $p$ and $q$ are two elements of $\left\{  1,2,\ldots,n\right\}  $
satisfying $b_{p}=b_{q}$, then%
\begin{equation}
p=q \label{pf.lem.adj(AB).set.increase.bdj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.adj(AB).set.increase.bdj}):} Let $p$
and $q$ be two elements of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$b_{p}=b_{q}$. If we had $p<q$, then we would have $b_{p}<b_{q}$ (by
(\ref{pf.lem.adj(AB).set.increase.binc})), which would contradict $b_{p}%
=b_{q}$. Thus, we cannot have $p<q$. In other words, we must have $p\geq q$.
If we had $q<p$, then we would have $b_{q}<b_{p}$ (by
(\ref{pf.lem.adj(AB).set.increase.binc}), applied to $q$ and $p$ instead of
$p$ and $q$)), which would contradict $b_{q}=b_{p}$. Thus, we cannot have
$q<p$. In other words, we must have $q\geq p$. Combined with $p\geq q$, this
shows that $p=q$. This proves (\ref{pf.lem.adj(AB).set.increase.bdj}).}.

We define a map $A:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $ as follows:

Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $a_{i}\in\left\{  a_{1}%
,a_{2},\ldots,a_{n}\right\}  =\left\{  b_{1},b_{2},\ldots,b_{n}\right\}  $.
Hence, there exists a $j\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$a_{i}=b_{j}$. Moreover, this $j$ is unique\footnote{\textit{Proof.} Let
$j_{1}$ and $j_{2}$ be two elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$. We shall show that $j_{1}=j_{2}$.
\par
We know that $j_{1}$ is an element $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$. In other words, $j_{1}$ is an element of $\left\{
1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j_{1}}$.
\par
We know that $j_{2}$ is an element $j\in\left\{  2,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$. In other words, $j_{2}$ is an element of $\left\{
1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j_{2}}$.
\par
We have $b_{j_{1}}=a_{i}=b_{j_{2}}$. Hence, $j_{1}=j_{2}$ (by
(\ref{pf.lem.adj(AB).set.increase.bdj}), applied to $p=j_{1}$ and $q=j_{2}$).
\par
Now, let us forget that we fixed $j_{1}$ and $j_{2}$. We thus have proven that
if $j_{1}$ and $j_{2}$ are two elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$, then $j_{1}=j_{2}$. In other words, any two elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j}$ must be equal.
Hence, the $j\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j}$ is
unique (because we already know that such a $j$ exists). Qed.}. We define
$A\left(  i\right)  $ to be this $j$. Thus, $A\left(  i\right)  $ is the
unique $j\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j}$. In
other words, $A\left(  i\right)  $ is an element of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $a_{i}=b_{A\left(  i\right)  }$.

Thus, we have defined $A\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  $
for every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words, we have
defined a map $A:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $.

Thus, we have defined a map $A:\left\{  1,2,\ldots,n\right\}  \rightarrow
\left\{  1,2,\ldots,n\right\}  $ which satisfies%
\begin{equation}
\left(  a_{i}=b_{A\left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,\ldots,n\right\}  \right)  .
\label{pf.lem.adj(AB).set.increase.A}%
\end{equation}
An analogous construction (with $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $,
$\left(  b_{1},b_{2},\ldots,b_{n}\right)  $ and $A$ replaced by $\left(
b_{1},b_{2},\ldots,b_{n}\right)  $, $\left(  a_{1},a_{2},\ldots,a_{n}\right)
$ and $B$) allows us to construct a map $B:\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}  $ which satisfies%
\begin{equation}
\left(  b_{i}=a_{B\left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,\ldots,n\right\}  \right)  .
\label{pf.lem.adj(AB).set.increase.B}%
\end{equation}
Consider this map $B$.

We have $A\circ B=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, $a_{B\left(  i\right)
}=b_{A\left(  B\left(  i\right)  \right)  }$ (by
(\ref{pf.lem.adj(AB).set.increase.A}), applied to $B\left(  i\right)  $
instead of $i$). Hence, $b_{A\left(  B\left(  i\right)  \right)  }=a_{B\left(
i\right)  }=b_{i}$ (by (\ref{pf.lem.adj(AB).set.increase.B})). Thus, $A\left(
B\left(  i\right)  \right)  =i$ (by (\ref{pf.lem.adj(AB).set.increase.bdj}),
applied to $p=A\left(  B\left(  i\right)  \right)  $ and $q=i$). Thus,
$\left(  A\circ B\right)  \left(  i\right)  =A\left(  B\left(  i\right)
\right)  =i=\operatorname*{id}\left(  i\right)  $.
\par
Now, let us forget that we fixed $i$. Thus, we have shown that $\left(  A\circ
B\right)  \left(  i\right)  =\operatorname*{id}\left(  i\right)  $ for every
$i\in\left\{  1,2,\ldots,n\right\}  $. In other words, $A\circ
B=\operatorname*{id}$, qed.}. The same argument (but applied to $\left(
b_{1},b_{2},\ldots,b_{n}\right)  $, $\left(  a_{1},a_{2},\ldots,a_{n}\right)
$, $B$ and $A$ instead of $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $,
$\left(  b_{1},b_{2},\ldots,b_{n}\right)  $, $A$ and $B$) shows that $B\circ
A=\operatorname*{id}$.

The maps $A$ and $B$ are mutually inverse (since $A\circ B=\operatorname*{id}$
and $B\circ A=\operatorname*{id}$). Thus, the map $A$ is invertible. In other
words, the map $A$ is a bijection. Hence, $A$ is a bijection $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $. In other
words, $A$ is a permutation of the set $\left\{  1,2,\ldots,n\right\}  $. In
other words, $A\in S_{n}$ (since $S_{n}$ is the set of all permutations of the
set $\left\{  1,2,\ldots,n\right\}  $).

The integers $b_{1},b_{2},\ldots,b_{n}$ are distinct (because of
(\ref{pf.lem.adj(AB).set.increase.bdj})). Proposition \ref{prop.sorting}
\textbf{(c)} (applied to $\left(  b_{1},b_{2},\ldots,b_{n}\right)  $ instead
of $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $) thus shows that there is a
\textbf{unique} permutation $\sigma\in S_{n}$ such that $b_{\sigma\left(
1\right)  }<b_{\sigma\left(  2\right)  }<\cdots<b_{\sigma\left(  n\right)  }$.
Hence, there exists at most one such permutation. In other words, if
$\sigma_{1}$ and $\sigma_{2}$ are two permutations $\sigma\in S_{n}$ such that
$b_{\sigma\left(  1\right)  }<b_{\sigma\left(  2\right)  }<\cdots
<b_{\sigma\left(  n\right)  }$, then%
\begin{equation}
\sigma_{1}=\sigma_{2}. \label{pf.lem.adj(AB).set.increase.uni}%
\end{equation}


The permutation $\operatorname*{id}\in S_{n}$ satisfies $b_{\operatorname*{id}%
\left(  1\right)  }<b_{\operatorname*{id}\left(  2\right)  }<\cdots
<b_{\operatorname*{id}\left(  n\right)  }$\ \ \ \ \footnote{In fact, this is
just a way to rewrite the chain of inequalities $b_{1}<b_{2}<\cdots<b_{n}$
(which is true by assumption).}. In other words, the permutation
$\operatorname*{id}\in S_{n}$ is a permutation $\sigma\in S_{n}$ such that
$b_{\sigma\left(  1\right)  }<b_{\sigma\left(  2\right)  }<\cdots
<b_{\sigma\left(  n\right)  }$.

We have $a_{1}<a_{2}<\cdots<a_{n}$. This rewrites as $b_{A\left(  1\right)
}<b_{A\left(  2\right)  }<\cdots<b_{A\left(  n\right)  }$ (since
$a_{i}=b_{A\left(  i\right)  }$ for every $i\in\left\{  1,2,\ldots,n\right\}
$ (by (\ref{pf.lem.adj(AB).set.increase.A}))). Thus, $A$ is a permutation
$\sigma\in S_{n}$ such that $b_{\sigma\left(  1\right)  }<b_{\sigma\left(
2\right)  }<\cdots<b_{\sigma\left(  n\right)  }$.

Thus, we know that $A$ and $\operatorname*{id}$ are two permutations
$\sigma\in S_{n}$ such that $b_{\sigma\left(  1\right)  }<b_{\sigma\left(
2\right)  }<\cdots<b_{\sigma\left(  n\right)  }$. Hence,
(\ref{pf.lem.adj(AB).set.increase.uni}) (applied to $\sigma_{1}=A$ and
$\sigma_{2}=\operatorname*{id}$) shows that $A=\operatorname*{id}$.

But (\ref{pf.lem.adj(AB).set.increase.A}) shows that%
\begin{align*}
\left(  a_{1},a_{2},\ldots,a_{n}\right)   &  =\left(  b_{A\left(  1\right)
},b_{A\left(  2\right)  },\ldots,b_{A\left(  n\right)  }\right)  =\left(
b_{\operatorname*{id}\left(  1\right)  },b_{\operatorname*{id}\left(
2\right)  },\ldots,b_{\operatorname*{id}\left(  n\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\operatorname*{id}\right) \\
&  =\left(  b_{1},b_{2},\ldots,b_{n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }b_{\operatorname*{id}\left(  i\right)  }=b_{i}\text{ for every
}i\in\left\{  1,2,\ldots,n\right\}  \right)  .
\end{align*}
This proves Lemma \ref{lem.adj(AB).set.increase}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.adj(AB).cauchy-binet}Let $n$ be a positive integer. Let $A$ be an
$\left(  n-1\right)  \times n$-matrix. Let $B$ be an $n\times\left(
n-1\right)  $-matrix. Then,%
\[
\det\left(  AB\right)  =\sum_{k=1}^{n}\det\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}B\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).cauchy-binet}.]Let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $. Define a subset $\mathbf{I}$
of $\left[  n\right]  ^{n-1}$ by%
\[
\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)  \in\left[
n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots<k_{n-1}\right\}  .
\]


Theorem \ref{thm.cauchy-binet} (applied to $n-1$ and $n$ instead of $n$ and
$m$) yields%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n-1}\leq n}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}%
}B\right)  . \label{pf.lem.adj(AB).cauchy-binet.short.1}%
\end{align}
Recall that the summation sign $\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n-1}\leq
n-1}$ is an abbreviation for \newline$\sum_{\substack{\left(  g_{1}%
,g_{2},\ldots,g_{n-1}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n-1}%
;\\g_{1}<g_{2}<\cdots<g_{n-1}}}$, which can be rewritten as $\sum_{\left(
g_{1},g_{2},\ldots,g_{n-1}\right)  \in\mathbf{I}}$ (because the $\left(
n-1\right)  $-tuples $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left\{
1,2,\ldots,n\right\}  ^{n-1}$ satisfying $g_{1}<g_{2}<\cdots<g_{n-1}$ are
precisely the elements of $\mathbf{I}$). Therefore,
(\ref{pf.lem.adj(AB).cauchy-binet.short.1}) can be rewritten as%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\mathbf{I}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}%
}B\right)  . \label{pf.lem.adj(AB).cauchy-binet.short.2}%
\end{align}


Now, let us take a closer look at $\mathbf{I}$. The set $\mathbf{I}$ consists
of all $\left(  n-1\right)  $-tuples $\left(  k_{1},k_{2},\ldots
,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}$ satisfying $k_{1}<k_{2}%
<\cdots<k_{n-1}$. There are only $n$ such $\left(  n-1\right)  $-tuples:
namely, the $\left(  n-1\right)  $-tuples $\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ for $k\in\left\{  1,2,\ldots,n\right\}  $. This is
intuitively clear: If you want to choose an $\left(  n-1\right)  $-tuple
$\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  \in\mathbf{I}$, you can simply
decide which of the $n$ elements $1,2,\ldots,n$ you do \textbf{not} want to be
an entry of $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $, and then the
$\left(  n-1\right)  $-tuple $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $
will have to be the list of all the remaining $n-1$ elements of $\left\{
1,2,\ldots,n\right\}  $ in increasing order. Let us formalize this argument a
bit more:

For every $k\in\left\{  1,2,\ldots,n\right\}  $, we have $\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  \in\mathbf{I}$ (for obvious reasons).
Hence, we can define a map%
\[
\Phi:\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I}%
\]
by%
\[
\left(  \Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots,n\right\}
\right)  .
\]
Consider this map $\Phi$. This map $\Phi$ is
injective\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ be such that $\Phi\left(  i\right)
=\Phi\left(  j\right)  $. We shall show that $i=j$.
\par
The definition of $\Phi$ yields $\Phi\left(  i\right)  =\left(  1,2,\ldots
,\widehat{i},\ldots,n\right)  $. Hence, $i$ is the only element of $\left\{
1,2,\ldots,n\right\}  $ that does not appear in $\Phi\left(  i\right)  $.
Similarly, $j$ is the only element of $\left\{  1,2,\ldots,n\right\}  $ that
does not appear in $\Phi\left(  j\right)  $. In other words, $j$ is the only
element of $\left\{  1,2,\ldots,n\right\}  $ that does not appear in
$\Phi\left(  i\right)  $ (since $\Phi\left(  i\right)  =\Phi\left(  j\right)
$). Comparing this with the fact that $i$ is the only element of $\left\{
1,2,\ldots,n\right\}  $ that does not appear in $\Phi\left(  i\right)  $, we
conclude that $i=j$.
\par
Now, let us forget that we fixed $i$ and $j$. We thus have proven that if
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}
$ are such that $\Phi\left(  i\right)  =\Phi\left(  j\right)  $, then $i=j$.
In other words, the map $\Phi$ is injective.} and
surjective\footnote{\textit{Proof.} Let $\mathbf{g}\in\mathbf{I}$. We shall
show that $\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
We have $\mathbf{g}\in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots
,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots
<k_{n-1}\right\}  $. In other words, \textbf{$g$} can be written in the form
$\mathbf{g}=\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ for some $\left(
g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left[  n\right]  ^{n-1}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n-1}$. Consider this $\left(  g_{1},g_{2}%
,\ldots,g_{n-1}\right)  $.
\par
The integers $g_{1},g_{2},\ldots,g_{n-1}$ are distinct (since $g_{1}%
<g_{2}<\cdots<g_{n-1}$). Thus, $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  $
is an $\left(  n-1\right)  $-element subset of $\left[  n\right]  $.
Therefore, its complement $\left[  n\right]  \setminus\left\{  g_{1}%
,g_{2},\ldots,g_{n-1}\right\}  $ is a $1$-element subset of $\left[  n\right]
$ (since $n-\left(  n-1\right)  =1$). In other words, $\left[  n\right]
\setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  =\left\{  k\right\}  $
for some $k\in\left[  n\right]  $. Consider this $k$.
\par
We have $k\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $. Since
$\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq\left[  n\right]  $, we
have%
\[
\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  =\left[  n\right]  \setminus
\underbrace{\left(  \left[  n\right]  \setminus\left\{  g_{1},g_{2}%
,\ldots,g_{n-1}\right\}  \right)  }_{=\left\{  k\right\}  }=\left[  n\right]
\setminus\left\{  k\right\}  .
\]
\par
Now, recall that $g_{1}<g_{2}<\cdots<g_{n-1}$. Hence, the $\left(  n-1\right)
$-tuple $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ is the list of all
elements of the set $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  $ in
increasing order. Since $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  =\left[
n\right]  \setminus\left\{  k\right\}  $, this rewrites as follows: The
$\left(  n-1\right)  $-tuple $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ is
the list of all elements of the set $\left[  n\right]  \setminus\left\{
k\right\}  $ in increasing order. But clearly the latter list is $\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  $. Thus, the $\left(  n-1\right)
$-tuple $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ is the list $\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  $. In other words, $\left(
g_{1},g_{2},\ldots,g_{n-1}\right)  =\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $, so that%
\[
\mathbf{g}=\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  =\Phi\left(  k\right)  \in\Phi\left(  \left\{
1,2,\ldots,n\right\}  \right)  .
\]
\par
Now, let us forget that we fixed $\mathbf{g}$. We thus have proven that
$\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ for every
$\mathbf{g}\in\mathbf{I}$. In other words, $\mathbf{I}\subseteq\Phi\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. In other words, the map $\Phi$ is
surjective.}. Hence, the map $\Phi$ is a bijection. In other words, the map%
\[
\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I}%
,\ \ \ \ \ \ \ \ \ \ k\mapsto\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\]
is a bijection (since this map is precisely $\Phi$).

Now, (\ref{pf.lem.adj(AB).cauchy-binet.short.2}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\mathbf{I}}\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n-1}}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right) \\
&  =\underbrace{\sum_{k\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{k=1}^{n}%
}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}A\right)  \cdot\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  1,2,\ldots,\widehat{k},\ldots
,n\right)  \text{ for}\\
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \text{ in the sum, since the map}\\
\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I},\ k\mapsto\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  \text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{k=1}^{n}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}B\right)  .
\end{align*}
This proves Lemma \ref{lem.adj(AB).cauchy-binet}.
\end{proof}
\end{vershort}

\begin{verlong}
This lemma is a particular case of the Cauchy-Binet formula. Here is how it
can be derived from the latter, in detail:

\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).cauchy-binet}.]Let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $. Define a subset $\mathbf{I}$
of $\left[  n\right]  ^{n-1}$ by%
\[
\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)  \in\left[
n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots<k_{n-1}\right\}  .
\]


Theorem \ref{thm.cauchy-binet} (applied to $n-1$ and $n$ instead of $n$ and
$m$) yields%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\underbrace{\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n-1}\leq n}}%
_{\substack{=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n-1};\\g_{1}<g_{2}<\cdots<g_{n-1}}%
}\\=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left[
n\right]  ^{n-1};\\g_{1}<g_{2}<\cdots<g_{n-1}}}\\\text{(since }\left\{
1,2,\ldots,n-1\right\}  =\left[  n\right]  \text{)}}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right)
\nonumber\\
&  =\underbrace{\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\left[  n\right]  ^{n-1};\\g_{1}<g_{2}<\cdots<g_{n-1}}}}_{\substack{=\sum
_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n-1}\right\}  }\\=\sum_{\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\mathbf{I}}\\\text{(since }\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n-1}\right\}  =\mathbf{I}\text{)}}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right)
\nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\mathbf{I}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}%
}B\right)  . \label{pf.lem.adj(AB).cauchy-binet.1}%
\end{align}


Now, for every $k\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  \in\mathbf{I}%
\]
\footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let us
denote the $\left(  n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ by $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  $. Thus,%
\begin{equation}
t_{i}=%
\begin{cases}
i, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-1\right\}  .
\label{pf.lem.adj(AB).cauchy-binet.wd.pf.1}%
\end{equation}
\par
Now, let $i\in\left\{  1,2,\ldots,n-2\right\}  $. We shall prove that
$t_{i}<t_{i+1}$. Indeed, let us assume the contrary (for the sake of
contradiction). Thus, $t_{i}\geq t_{i+1}$. But%
\begin{align*}
t_{i}  &  =%
\begin{cases}
i, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.adj(AB).cauchy-binet.wd.pf.1})}\right) \\
&  \leq%
\begin{cases}
i+1, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq i+1\text{ in the case when
}i<k\right) \\
&  =i+1,
\end{align*}
so that%
\begin{align*}
i+1  &  \geq t_{i}\geq t_{i+1}=%
\begin{cases}
i+1, & \text{if }i+1<k;\\
\left(  i+1\right)  +1, & \text{if }i+1\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.adj(AB).cauchy-binet.wd.pf.1}), applied to }i+1\text{ instead of
}i\right) \\
&  \geq%
\begin{cases}
i+1, & \text{if }i+1<k;\\
i+1, & \text{if }i+1\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i+1\right)  +1\geq i+1\text{
in the case when }i+1\geq k\right) \\
&  =i+1.
\end{align*}
Combining $i+1\geq t_{i}$ with $t_{i}\geq i+1$, we obtain $i+1=t_{i}$. Hence,
if we had $i<k$, then we would have%
\[
i+1=t_{i}=%
\begin{cases}
i, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
=i\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<k\right)  ,
\]
which would contradict $i+1\neq i$. Therefore, we cannot have $i<k$. Thus, we
have $i\geq k$. Thus, $i+1\geq k$. Now,%
\begin{align*}
t_{i+1}  &  =%
\begin{cases}
i+1, & \text{if }i+1<k;\\
\left(  i+1\right)  +1, & \text{if }i+1\geq k
\end{cases}
=\left(  i+1\right)  +1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i+1\geq
k\right) \\
&  >i+1=t_{i}\geq t_{i+1}.
\end{align*}
This is absurd. Thus, we have obtained a contradiction. This contradiction
proves that our assumption was wrong. Hence, $t_{i}<t_{i+1}$.
\par
Now, let us forget that we fixed $i$. We thus have shown that $t_{i}<t_{i+1}$
for every $i\in\left\{  1,2,\ldots,n-2\right\}  $. In other words,
$t_{1}<t_{2}<\cdots<t_{n-1}$.
\par
Also, $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  \in\left[  n\right]  ^{n-1}$. Hence, $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  $ is an element of $\left[  n\right]
^{n-1}$ and satisfies $t_{1}<t_{2}<\cdots<t_{n-1}$. In other words, $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  $ is an element $\left(  k_{1},k_{2}%
,\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}$ satisfying $\left(
k_{1},k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}$. In other
words,
\[
\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n-1}\right\}  =\mathbf{I}.
\]
Thus, $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  \in\left(  t_{1}%
,t_{2},\ldots,t_{n-1}\right)  =\mathbf{I}$, qed.}. Hence, we can define a map%
\[
\Phi:\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I}%
\]
by%
\[
\left(  \Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots,n\right\}
\right)  .
\]
Consider this map $\Phi$. This map $\Phi$ is
injective\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ be such that $\Phi\left(  i\right)
=\Phi\left(  j\right)  $. We shall show that $i=j$.
\par
The definition of $\Phi$ yields $\Phi\left(  i\right)  =\left(  1,2,\ldots
,\widehat{i},\ldots,n\right)  $. Hence,%
\begin{align*}
&  \left(  \text{the set of all entries of }\underbrace{\Phi\left(  i\right)
}_{=\left(  1,2,\ldots,\widehat{i},\ldots,n\right)  }\right) \\
&  =\left(  \text{the set of all entries of }\left(  1,2,\ldots,\widehat{i}%
,\ldots,n\right)  \right) \\
&  =\left\{  1,2,\ldots,\widehat{i},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  i\right\}  .
\end{align*}
Thus,%
\begin{align*}
&  \left\{  1,2,\ldots,n\right\}  \setminus\underbrace{\left(  \text{the set
of all entries of }\Phi\left(  i\right)  \right)  }_{=\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  i\right\}  }\\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left(  \left\{  1,2,\ldots
,n\right\}  \setminus\left\{  i\right\}  \right)  =\left\{  i\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{  i\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  \text{ (since }i\in\left\{  1,2,\ldots,n\right\}
\text{)}\right)  .
\end{align*}
The same argument (applied to $j$ instead of $i$) shows that%
\[
\left\{  1,2,\ldots,n\right\}  \setminus\left(  \text{the set of all entries
of }\Phi\left(  j\right)  \right)  =\left\{  j\right\}  .
\]
\par
Now,%
\begin{align*}
\left\{  i\right\}   &  =\left\{  1,2,\ldots,n\right\}  \setminus\left(
\text{the set of all entries of }\underbrace{\Phi\left(  i\right)  }%
_{=\Phi\left(  j\right)  }\right) \\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left(  \text{the set of all
entries of }\Phi\left(  j\right)  \right)  =\left\{  j\right\}  .
\end{align*}
Hence, $i\in\left\{  i\right\}  =\left\{  j\right\}  $, so that $i=j$.
\par
Now, let us forget that we fixed $i$ and $j$. We thus have proven that if
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}
$ are such that $\Phi\left(  i\right)  =\Phi\left(  j\right)  $, then $i=j$.
In other words, the map $\Phi$ is injective, qed.} and
surjective\footnote{\textit{Proof.} Let $\mathbf{g}\in\mathbf{I}$. We shall
show that $\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
We have%
\begin{align*}
\mathbf{g}  &  \in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots
,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots
<k_{n-1}\right\} \\
&  =\left\{  \left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left[  n\right]
^{n-1}\ \mid\ g_{1}<g_{2}<\cdots<g_{n-1}\right\}
\end{align*}
(here, we have renamed the index $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $
as $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $). In other words,
\textbf{$g$} can be written in the form $\mathbf{g}=\left(  g_{1},g_{2}%
,\ldots,g_{n-1}\right)  $ for some $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\left[  n\right]  ^{n-1}$ satisfying $g_{1}<g_{2}<\cdots<g_{n-1}$. Consider
this $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $.
\par
The integers $g_{1},g_{2},\ldots,g_{n-1}$ are distinct (since $g_{1}%
<g_{2}<\cdots<g_{n-1}$). Hence, they are $n-1$ distinct integers. In other
words, we have $\left\vert \left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}
\right\vert =n-1$. Also, each $i\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies $g_{i}\in\left[  n\right]  $ (since $\left(  g_{1},g_{2}%
,\ldots,g_{n-1}\right)  \in\left[  n\right]  ^{n-1}$). In other words,
$\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq\left[  n\right]  $.
Hence,%
\[
\left\vert \left[  n\right]  \setminus\left\{  g_{1},g_{2},\ldots
,g_{n-1}\right\}  \right\vert =\underbrace{\left\vert \left[  n\right]
\right\vert }_{=n}-\underbrace{\left\vert \left\{  g_{1},g_{2},\ldots
,g_{n-1}\right\}  \right\vert }_{=n-1}=n-\left(  n-1\right)  =1.
\]
In other words, $\left[  n\right]  \setminus\left\{  g_{1},g_{2}%
,\ldots,g_{n-1}\right\}  $ is a one-element set. Hence, the set $\left[
n\right]  \setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  $ has the form
$\left\{  k\right\}  $ for some object $k$. Consider this $k$.
\par
We have $\left[  n\right]  \setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}%
\right\}  =\left\{  k\right\}  $, so that $k\in\left\{  k\right\}  =\left[
n\right]  \setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq
\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $.
\par
Let us denote the $\left(  n-1\right)  $-tuple $\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  $ by $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)
$. Thus,%
\begin{align*}
\left(  t_{1},t_{2},\ldots,t_{n-1}\right)   &  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right) \\
&  \in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)
\in\left[  n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots<k_{n-1}\right\}  .
\end{align*}
In other words, $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $ of $\left[  n\right]  ^{n-1}$
satisfying $k_{1}<k_{2}<\cdots<k_{n-1}$. In other words, $\left(  t_{1}%
,t_{2},\ldots,t_{n-1}\right)  $ is an element of $\left[  n\right]  ^{n-1}$
and satisfies $t_{1}<t_{2}<\cdots<t_{n-1}$.
\par
Clearly,
\begin{align*}
\left\{  t_{1},t_{2},\ldots,t_{n-1}\right\}   &  =\left(  \text{the set of all
entries of the list }\underbrace{\left(  t_{1},t_{2},\ldots,t_{n-1}\right)
}_{=\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  }\right) \\
&  =\left(  \text{the set of all entries of the list }\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  \right) \\
&  =\left\{  1,2,\ldots,\widehat{k},\ldots,n\right\}  =\underbrace{\left\{
1,2,\ldots,n\right\}  }_{=\left[  n\right]  }\setminus\underbrace{\left\{
k\right\}  }_{=\left[  n\right]  \setminus\left\{  g_{1},g_{2},\ldots
,g_{n-1}\right\}  }\\
&  =\left[  n\right]  \setminus\left(  \left[  n\right]  \setminus\left\{
g_{1},g_{2},\ldots,g_{n-1}\right\}  \right)  =\left\{  g_{1},g_{2}%
,\ldots,g_{n-1}\right\}
\end{align*}
(since $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq\left[
n\right]  $).
\par
Now, Lemma \ref{lem.adj(AB).set.increase} (applied to $n-1$, $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  $ and $\left(  g_{1},g_{2},\ldots
,g_{n-1}\right)  $ instead of $n$, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $
and $\left(  b_{1},b_{2},\ldots,b_{n}\right)  $) shows that $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  =\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
$. Compared with $\mathbf{g}=\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $,
this yields%
\[
\mathbf{g}=\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  =\Phi\left(  k\right)
\]
(since $\Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
$ (by the definition of $\Phi\left(  k\right)  $)). Thus, $\mathbf{g}%
=\Phi\left(  \underbrace{k}_{\in\left\{  1,2,\ldots,n\right\}  }\right)
\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
Now, let us forget that we fixed $\mathbf{g}$. We thus have proven that
$\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ for every
$\mathbf{g}\in\mathbf{I}$. In other words, $\mathbf{I}\subseteq\Phi\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. In other words, the map $\Phi$ is
surjective, qed.}. Hence, the map $\Phi$ is bijective. In other words, the map
$\Phi$ is a bijection. In other words, the map%
\begin{align*}
\left\{  1,2,\ldots,n\right\}   &  \rightarrow\mathbf{I},\\
k  &  \mapsto\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\end{align*}
is a bijection (because the map $\Phi$ is the map
\begin{align*}
\left\{  1,2,\ldots,n\right\}   &  \rightarrow\mathbf{I},\\
k  &  \mapsto\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\end{align*}
(since we have $\Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ for every $k\in\left\{  1,2,\ldots,n\right\}  $)).

Now, (\ref{pf.lem.adj(AB).cauchy-binet.1}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\mathbf{I}}\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n-1}}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right) \\
&  =\underbrace{\sum_{k\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{k=1}^{n}%
}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}A\right)  \cdot\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  1,2,\ldots,\widehat{k},\ldots
,n\right)  \text{ for}\\
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \text{ in the sum, since the map}\\
\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I},\ k\mapsto\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  \text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{k=1}^{n}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}B\right)  .
\end{align*}
This proves Lemma \ref{lem.adj(AB).cauchy-binet}.
\end{proof}
\end{verlong}

Here comes one more simple lemma:

\begin{lemma}
\label{lem.adj(AB).minor-of-AB}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$ and
$p\in\mathbb{N}$. Let $A$ be an $n\times p$-matrix. Let $B$ be a $p\times
m$-matrix. Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $. Let $j_{1},j_{2},\ldots,j_{v}$ be some elements of
$\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  =\left(  \operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}B\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).minor-of-AB}.]Let us write the $n\times
p$-matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq p}$. Let us write the $p\times m$-matrix $B$ in the form $B=\left(
b_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq m}$.

The definition of the product of two matrices yields $AB=\left(  \sum
_{k=1}^{p}a_{i,k}b_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ (since
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq p}$ and $B=\left(
b_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq m}$). Thus, the definition of
$\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  $ yields%
\begin{equation}
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  =\left(  \sum_{k=1}^{p}a_{i_{x},k}b_{k,j_{y}%
}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}.
\label{pf.lem.adj(AB).minor-of-AB.1}%
\end{equation}


On the other hand, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
p}$. Hence, the definition of $\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A$ yields%
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(  a_{i_{x}%
,j}\right)  _{1\leq x\leq u,\ 1\leq j\leq p}=\left(  a_{i_{i},j}\right)
_{1\leq i\leq u,\ 1\leq j\leq p}%
\]
\footnote{The double use of the letter \textquotedblleft$i$\textquotedblright%
\ in \textquotedblleft$i_{i}$\textquotedblright\ might appear confusing. The
first \textquotedblleft$i$\textquotedblright\ is part of the notation $i_{k}$
for $k\in\left\{  1,2,\ldots,u\right\}  $; the second \textquotedblleft%
$i$\textquotedblright\ is an element of $\left\{  1,2,\ldots,u\right\}  $.
These two \textquotedblleft$i$\textquotedblright s are unrelated to each
other. I hope the reader can easily tell them apart by the fact that the
\textquotedblleft$i$\textquotedblright\ that is part of the notation $i_{k}$
always appears with a subscript, whereas the second \textquotedblleft%
$i$\textquotedblright\ never does.} (here, we renamed the index $\left(
x,j\right)  $ as $\left(  i,j\right)  $).

Also, $B=\left(  b_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq m}$. Hence, the
definition of $\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B$
yields%
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B=\left(  b_{i,j_{y}%
}\right)  _{1\leq i\leq p,\ 1\leq y\leq v}=\left(  b_{i,j_{j}}\right)  _{1\leq
i\leq p,\ 1\leq j\leq v}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  i,j\right)  $).

We have $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(
a_{i_{i},j}\right)  _{1\leq i\leq u,\ 1\leq j\leq p}$ and
$\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B=\left(  b_{i,j_{j}%
}\right)  _{1\leq i\leq p,\ 1\leq j\leq v}$. The definition of the product of
two matrices thus yields%
\begin{align*}
\left(  \operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)
\cdot\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B\right)
&  =\left(  \sum_{k=1}^{p}a_{i_{i},k}b_{k,j_{j}}\right)  _{1\leq i\leq
u,\ 1\leq j\leq v}\\
&  =\left(  \sum_{k=1}^{p}a_{i_{x},k}b_{k,j_{y}}\right)  _{1\leq x\leq
u,\ 1\leq y\leq v}%
\end{align*}
(here, we have renamed the index $\left(  i,j\right)  $ as $\left(
x,y\right)  $). Comparing this with (\ref{pf.lem.adj(AB).minor-of-AB.1}), we
obtain%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  =\left(  \operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}B\right)  .
\]
This proves Lemma \ref{lem.adj(AB).minor-of-AB}.
\end{proof}

We note in passing that Lemma \ref{lem.adj(AB).minor-of-AB} leads to the
following generalization of Theorem \ref{thm.cauchy-binet}:

\begin{corollary}
\label{cor.adj(AB).cauchy-binet-general}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$
and $p\in\mathbb{N}$. Let $A$ be an $n\times p$-matrix. Let $B$ be a $p\times
m$-matrix. Let $u\in\mathbb{N}$. Let $i_{1},i_{2},\ldots,i_{u}$ be some
elements of $\left\{  1,2,\ldots,n\right\}  $. Let $j_{1},j_{2},\ldots,j_{u}$
be some elements of $\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\det\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}%
^{j_{1},j_{2},\ldots,j_{u}}\left(  AB\right)  \right)  =\sum_{1\leq
g_{1}<g_{2}<\cdots<g_{u}\leq p}\det\left(  \operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{g_{1},g_{2},\ldots,g_{u}}A\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}}^{j_{1},j_{2}%
,\ldots,j_{u}}B\right)  .
\]
(Here, the summation sign \textquotedblleft$\sum_{1\leq g_{1}<g_{2}%
<\cdots<g_{u}\leq p}$\textquotedblright\ has to be interpreted as
\textquotedblleft$\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{u}\right)
\in\left\{  1,2,\ldots,p\right\}  ^{u};\\g_{1}<g_{2}<\cdots<g_{u}}%
}$\textquotedblright, in analogy to Remark \ref{rmk.cauchy-binet.sumsign}.)
\end{corollary}

Corollary \ref{cor.adj(AB).cauchy-binet-general} is precisely the formula
\cite[(1.10)]{NoumiYamada}\footnote{We notice that the notation $A_{j_{1}%
,j_{2},\ldots,j_{u}}^{i_{1},i_{2},\ldots,i_{u}}$ in \cite{NoumiYamada} is
equivalent to our notation $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{u}}A$.}. We shall not use Corollary
\ref{cor.adj(AB).cauchy-binet-general} in what follows, but let us
nevertheless prove it:

\begin{proof}
[Proof of Corollary \ref{cor.adj(AB).cauchy-binet-general}.]Fix any $\left(
g_{1},g_{2},\ldots,g_{u}\right)  \in\left\{  1,2,\ldots,p\right\}  ^{u}$.
Applying Proposition \ref{prop.submatrix.easy} \textbf{(d)} to $p$, $u$ and
$\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ instead of $m$, $v$ and $\left(
j_{1},j_{2},\ldots,j_{v}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{g_{1},g_{2}%
,\ldots,g_{u}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{u}}A\right)
=\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{u}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]
Applying Proposition \ref{prop.submatrix.easy} \textbf{(d)} to $p$, $B$, $u$
and $\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ instead of $n$, $A$, $v$ and
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}}^{j_{1},j_{2}%
,\ldots,j_{u}}B=\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)
=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{u}}B\right)  .
\]


Now, Lemma \ref{lem.adj(AB).minor-of-AB} (applied to $v=u$) shows that%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{u}}\left(  AB\right)  =\left(  \operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)  .
\]
Hence,%
\begin{align*}
&  \det\left(  \underbrace{\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{u}}\left(  AB\right)  }_{=\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  \cdot\left(
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)  }\right) \\
&  =\det\left(  \left(  \operatorname*{rows}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{u}}B\right)  \right) \\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{u}\leq p}\det\left(
\underbrace{\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{u}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)
}_{=\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{g_{1}%
,g_{2},\ldots,g_{u}}A}\right)  \cdot\det\left(
\underbrace{\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{u}}\left(
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)
}_{=\operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}}^{j_{1}%
,j_{2},\ldots,j_{u}}B}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.cauchy-binet} (applied to }u\text{, }p\text{,
}\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\text{ and}\\
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\text{ instead of
}n\text{, }m\text{, }A\text{ and }B\text{)}%
\end{array}
\right) \\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{u}\leq p}\det\left(  \operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{g_{1},g_{2},\ldots,g_{u}}A\right)
\cdot\det\left(  \operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}%
}^{j_{1},j_{2},\ldots,j_{u}}B\right)  .
\end{align*}
This proves Corollary \ref{cor.adj(AB).cauchy-binet-general}.
\end{proof}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.adj(AB)}.]Let $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $1\leq u\leq n$, so that $n\geq1$.

For every $k\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
=A_{\sim u,\sim k} \label{sol.adj(AB).short.A}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).short.A}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We can apply Proposition \ref{prop.submatrix.easy}
\textbf{(d)} to $n-1$, $n-1$, $\left(  1,2,\ldots,\widehat{u},\ldots,n\right)
$ and $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  $ instead of $u$, $v$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  $. As a result, we obtain%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{k},\ldots,n}A=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  .
\]
But the definition of $A_{\sim u,\sim k}$ yields%
\[
A_{\sim u,\sim k}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,n}A=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  .
\]
This proves (\ref{sol.adj(AB).short.A}).} and%
\begin{equation}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
=B_{\sim k,\sim v} \label{sol.adj(AB).short.B}%
\end{equation}
\footnote{This holds for similar reasons.}.

We have%
\begin{equation}
\left(  AB\right)  _{\sim u,\sim v}=\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  \cdot\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
\label{sol.adj(AB).short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).short.1}):} The definition of
$\left(  AB\right)  _{\sim u,\sim v}$ yields%
\[
\left(  AB\right)  _{\sim u,\sim v}=\operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}^{1,2,\ldots,\widehat{v},\ldots,n}\left(  AB\right)
=\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,n}A\right)  \cdot\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}B\right)
\]
(by Lemma \ref{lem.adj(AB).minor-of-AB}, applied to $m=n$, $p=n$, $u=n-1$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(  1,2,\ldots,\widehat{u}%
,\ldots,n\right)  $ and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  =\left(
1,2,\ldots,\widehat{v},\ldots,n\right)  $). This proves
(\ref{sol.adj(AB).short.1}).}. Taking determinants on both sides of this
equation, we obtain%
\begin{align}
&  \det\left(  \left(  AB\right)  _{\sim u,\sim v}\right) \nonumber\\
&  =\det\left(  \left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)  \right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  }_{\substack{=A_{\sim
u,\sim k}\\\text{(by (\ref{sol.adj(AB).short.A}))}}}\right)  \cdot\det\left(
\underbrace{\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\right)  }_{\substack{=B_{\sim k,\sim v}\\\text{(by
(\ref{sol.adj(AB).short.B}))}}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Lemma \ref{lem.adj(AB).cauchy-binet}, applied to }%
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\\
\text{and }\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\text{ instead of }A\text{ and }B
\end{array}
\right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  A_{\sim u,\sim k}\right)  \cdot\det\left(
B_{\sim k,\sim v}\right)  . \label{sol.adj(AB).short.5}%
\end{align}


Let us now forget that we fixed $\left(  u,v\right)  $. We thus have proven
(\ref{sol.adj(AB).short.5}) for every $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$.

Now, the definition of $\operatorname*{adj}\left(  AB\right)  $ yields%
\begin{equation}
\operatorname*{adj}\left(  AB\right)  =\left(  \left(  -1\right)  ^{i+j}%
\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{sol.adj(AB).short.L1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \left(  -1\right)  ^{i+j}\underbrace{\det\left(  \left(  AB\right)  _{\sim
j,\sim i}\right)  }_{\substack{=\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right)  \\\text{(by
(\ref{sol.adj(AB).short.5}), applied to }\left(  u,v\right)  =\left(
j,i\right)  \text{)}}}\\
&  =\left(  -1\right)  ^{i+j}\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{i+j}}_{\substack{=\left(
-1\right)  ^{\left(  i+k\right)  +\left(  k+j\right)  }\\\text{(since
}i+j\equiv i+j+2k=\left(  i+k\right)  +\left(  k+j\right)  \operatorname{mod}%
2\text{)}}}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim
k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{\left(  i+k\right)
+\left(  k+j\right)  }}_{=\left(  -1\right)  ^{i+k}\left(  -1\right)  ^{k+j}%
}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim k,\sim
i}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
.
\end{align*}
Thus, (\ref{sol.adj(AB).short.L1}) becomes%
\begin{align}
\operatorname*{adj}\left(  AB\right)   &  =\left(  \underbrace{\left(
-1\right)  ^{i+j}\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)
}_{=\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  =\left(  \sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n} \label{sol.adj(AB).short.L2}%
\end{align}


On the other hand, we have $\operatorname*{adj}B=\left(  \left(  -1\right)
^{i+j}\det\left(  B_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ (by the definition of $\operatorname*{adj}B$) and
$\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the
definition of $\operatorname*{adj}A$). Therefore, the definition of the
product of two matrices shows that%
\[
\operatorname*{adj}B\cdot\operatorname*{adj}A=\left(  \sum_{k=1}^{n}\left(
-1\right)  ^{i+k}\det\left(  B_{\sim k,\sim i}\right)  \cdot\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]
Compared with (\ref{sol.adj(AB).short.L2}), this yields $\operatorname*{adj}%
\left(  AB\right)  =\operatorname*{adj}B\cdot\operatorname*{adj}A$. This
solves Exercise \ref{exe.adj(AB)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.adj(AB)}.]Let $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $u\in\left\{  1,2,\ldots,n\right\}  $ and
$v\in\left\{  1,2,\ldots,n\right\}  $. From $u\in\left\{  1,2,\ldots
,n\right\}  $, we obtain $1\leq u\leq n$, so that $n\geq1$ and therefore
$n-1\in\mathbb{N}$ and $n>0$.

For every $k\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
=A_{\sim u,\sim k} \label{sol.adj(AB).A}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).A}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We can apply Proposition \ref{prop.submatrix.easy}
\textbf{(d)} to $n-1$, $n-1$, $\left(  1,2,\ldots,\widehat{u},\ldots,n\right)
$ and $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  $ instead of $u$, $v$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  $. As a result, we obtain%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{k},\ldots,n}A=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  .
\]
But the definition of $A_{\sim u,\sim k}$ yields%
\[
A_{\sim u,\sim k}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,n}A=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  .
\]
This proves (\ref{sol.adj(AB).A}).} and%
\begin{equation}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
=B_{\sim k,\sim v} \label{sol.adj(AB).B}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).B}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We can apply Proposition \ref{prop.submatrix.easy}
\textbf{(d)} to $n-1$, $n-1$, $B$, $\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ and $\left(  1,2,\ldots,\widehat{v},\ldots,n\right)  $,
instead of $u$, $v$, $A$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and
$\left(  j_{1},j_{2},\ldots,j_{v}\right)  $. As a result, we obtain%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}^{1,2,\ldots
,\widehat{v},\ldots,n}B=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}B\right)  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}B\right)  .
\]
But the definition of $B_{\sim k,\sim v}$ yields%
\[
B_{\sim k,\sim v}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{k}%
,\ldots,n}^{1,2,\ldots,\widehat{v},\ldots,n}B=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)  .
\]
This proves (\ref{sol.adj(AB).B}).}.

We have%
\begin{equation}
\left(  AB\right)  _{\sim u,\sim v}=\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  \cdot\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
\label{sol.adj(AB).1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).1}):} The definition of $\left(
AB\right)  _{\sim u,\sim v}$ yields%
\[
\left(  AB\right)  _{\sim u,\sim v}=\operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}^{1,2,\ldots,\widehat{v},\ldots,n}\left(  AB\right)
=\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,n}A\right)  \cdot\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}B\right)
\]
(by Lemma \ref{lem.adj(AB).minor-of-AB}, applied to $m=n$, $p=n$, $u=n-1$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(  1,2,\ldots,\widehat{u}%
,\ldots,n\right)  $ and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  =\left(
1,2,\ldots,\widehat{v},\ldots,n\right)  $). This proves (\ref{sol.adj(AB).1}%
).}. Hence,%
\begin{align}
&  \det\left(  \underbrace{\left(  AB\right)  _{\sim u,\sim v}}_{=\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
\cdot\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\right)  }\right) \nonumber\\
&  =\det\left(  \left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)  \right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  }_{\substack{=A_{\sim
u,\sim k}\\\text{(by (\ref{sol.adj(AB).A}))}}}\right)  \cdot\det\left(
\underbrace{\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\right)  }_{\substack{=B_{\sim k,\sim v}\\\text{(by (\ref{sol.adj(AB).B}%
))}}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Lemma \ref{lem.adj(AB).cauchy-binet}, applied to }%
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\\
\text{and }\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\text{ instead of }A\text{ and }B
\end{array}
\right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  A_{\sim u,\sim k}\right)  \cdot\det\left(
B_{\sim k,\sim v}\right)  . \label{sol.adj(AB).5}%
\end{align}


Let us now forget that we fixed $\left(  u,v\right)  $. We thus have proven
(\ref{sol.adj(AB).5}) for every $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$.

Now, the definition of $\operatorname*{adj}\left(  AB\right)  $ yields%
\begin{equation}
\operatorname*{adj}\left(  AB\right)  =\left(  \left(  -1\right)  ^{i+j}%
\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{sol.adj(AB).L1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \left(  -1\right)  ^{i+j}\underbrace{\det\left(  \left(  AB\right)  _{\sim
j,\sim i}\right)  }_{\substack{=\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right)  \\\text{(by
(\ref{sol.adj(AB).5}), applied to }\left(  u,v\right)  =\left(  j,i\right)
\text{)}}}\\
&  =\left(  -1\right)  ^{i+j}\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{i+j}}_{\substack{=\left(
-1\right)  ^{\left(  i+k\right)  +\left(  k+j\right)  }\\\text{(since
}i+j\equiv i+j+2k=\left(  i+k\right)  +\left(  k+j\right)  \operatorname{mod}%
2\text{)}}}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim
k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{\left(  i+k\right)
+\left(  k+j\right)  }}_{=\left(  -1\right)  ^{i+k}\left(  -1\right)  ^{k+j}%
}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim k,\sim
i}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\underbrace{\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim k,\sim
i}\right)  }_{=\det\left(  B_{\sim k,\sim i}\right)  \cdot\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  }\\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
.
\end{align*}
Thus, (\ref{sol.adj(AB).L1}) becomes%
\begin{align}
\operatorname*{adj}\left(  AB\right)   &  =\left(  \underbrace{\left(
-1\right)  ^{i+j}\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)
}_{=\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  =\left(  \sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n} \label{sol.adj(AB).L2}%
\end{align}


On the other hand, we have $\operatorname*{adj}B=\left(  \left(  -1\right)
^{i+j}\det\left(  B_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ (by the definition of $\operatorname*{adj}B$) and
$\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the
definition of $\operatorname*{adj}A$). Therefore, the definition of the
product of two matrices shows that%
\[
\operatorname*{adj}B\cdot\operatorname*{adj}A=\left(  \sum_{k=1}^{n}\left(
-1\right)  ^{i+k}\det\left(  B_{\sim k,\sim i}\right)  \cdot\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]
Compared with (\ref{sol.adj(AB).L2}), this yields $\operatorname*{adj}\left(
AB\right)  =\operatorname*{adj}B\cdot\operatorname*{adj}A$. This solves
Exercise \ref{exe.adj(AB)}.
\end{proof}
\end{verlong}

\begin{thebibliography}{999999999}                                                                                        %


\bibitem[Aigner07]{Aigner07}%
\href{http://doi.org/10.1007/978-3-540-39035-0}{Martin Aigner, \textit{A
Course in Enumeration}, Graduate Texts in Mathematics \#238, Springer 2007.}

\bibitem[AigZie]{AigZie}Martin Aigner, G\"{u}nter M. Ziegler, \textit{Proofs
from the Book}, 4th edition, Springer 2010.

\bibitem[AndDos]{AndDos}Titu Andreescu, Gabriel Dospinescu, \textit{Problems
from the Book}, XYZ Press 2008.

\bibitem[Artin]{Artin}Michael Artin, \textit{Algebra}, 2nd edition, Pearson 2010.

\bibitem[Axler]{Axler}\href{http://linear.axler.net/}{Sheldon Axler,
\textit{Linear Algebra Done Right}, 3rd edition, Springer 2015.}

\bibitem[BenDre07]{BenDre-Vand}%
\href{http://home.wlu.edu/~dresdeng/papers/VDM.pdf}{Arthur T. Benjamin and
Gregory P. Dresden, \textit{A Combinatorial Proof of Vandermonde's
Determinant}, The American Mathematical Monthly, Vol. 114, No. 4 (Apr., 2007),
pp. 338--341.}

\bibitem[BenQui04]{BenQui-fib}%
\href{https://www.math.hmc.edu/~benjamin/papers/bama.pdf}{Arthur T. Benjamin
and Jennifer J. Quinn, \textit{Proofs that Really Count: The Magic of
Fibonacci Numbers and More}, Mathematical Adventures for Students and
Amateurs, (David F. Hayes and Tatiana Shubin, editors), Spectrum Series of
MAA, pp. 83--98, 2004.}

\bibitem[Bergma15]{Bergman-Lang}George M. Bergman, \textit{A Companion to
Lang's Algebra}, website (2015).\newline%
\href{https://math.berkeley.edu/~gbergman/.C.to.L/}{https://math.berkeley.edu/\symbol{126}%
gbergman/.C.to.L/}

\bibitem[BirMac99]{BirkMac}Saunders Mac Lane, Garrett Birkhoff,
\textit{Algebra}, 3rd edition, AMS Chelsea Publishing 1999.

\bibitem[BjoBre05]{BjoBre05}\href{http://doi.org/10.1007/3-540-27596-7}{Anders
Bj\"{o}rner, Francesco Brenti, \textit{Combinatorics of Coxeter Groups},
Graduate Texts in Mathematics \#231, Springer 2005.}

\bibitem[Bona12]{Bona12}Miklos Bona, \textit{Combinatorics of Permutations},
2nd edition, CRC Press 2012.

\bibitem[Conrad]{Conrad}Keith Conrad, \textit{Sign of permutations},
\newline\url{http://www.math.uconn.edu/~kconrad/blurbs/grouptheory/sign.pdf} .

\bibitem[EdeStra04]{EdelStrang}%
\href{https://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Edelman189-197.pdf}{Alan
Edelman and Gilbert Strang, \textit{Pascal Matrices}, American Mathematical
Monthly, Vol. 111, No. 3 (March 2004), pp. 189--197.}

\bibitem[Eisenk]{Eisenk-planepartits}%
\href{http://arxiv.org/abs/math/9712261v2}{Theresia Eisenk\"{o}lbl,
\textit{Rhombus Tilings of a Hexagon with Three Fixed Border Tiles},
arXiv:math/9712261v2.} Published in: J. Combin. Theory Ser. A 88 (1999), pp. 368--378.

\bibitem[Gessel79]{Gessel-Vand}Ira Gessel, \textit{Tournaments and
Vandermonde's Determinant}, Journal of Graph Theory, Vol. 3 (1979), pp. 305--307.

\bibitem[Gill12]{Gill}%
\href{http://cseweb.ucsd.edu/~gill/CILASite/Resources/LinearAlgebra.pdf}{S.
Gill Williamson, \textit{Matrix Canonical Forms: notational skills and proof
techniques}, 2012.}

\bibitem[Goodman]{Goodman}Frederick M. Goodman, \textit{Algebra: Abstract and
Concrete}, edition 2.6, 1 May 2015.\newline%
\url{http://homepage.math.uiowa.edu/~goodman/algebrabook.dir/book.2.6.pdf} .

\bibitem[Gould10]{Gould-I}\href{http://www.math.wvu.edu/~gould/Vol.4.PDF}{H.
W. Gould, \textit{Combinatorial Identities: Table I: Intermediate Techniques
for Summing Finite Series}, Edited and Compiled by Jocelyn Quaintance, May 3,
2010}.

\bibitem[Gri10]{GriHyp}Darij Grinberg, \textit{A hyperfactorial divisibility},
version of 27 July 2015.\newline\url{http://web.mit.edu/~darij/www/}

\bibitem[Gri11]{Gri-zeck}Darij Grinberg, \textit{Zeckendorf family identities
generalized}, arXiv preprint
\texttt{\href{http://arxiv.org/abs/1103.4507v1}{arXiv:1103.4507v1}}.

\bibitem[Gri09]{Gri-19.9}\href{http://web.mit.edu/~darij/www/19-9ML.pdf}{Darij
Grinberg, \textit{Solution to Problem 19.9 from \textquotedblleft Problems
from the Book\textquotedblright}}.\newline\url{http://web.mit.edu/~darij/www/solutions.html}

\bibitem[GriRei15]{Reiner}Darij Grinberg, Victor Reiner, \textit{Hopf algebras
in Combinatorics}, August 25, 2015. arXiv preprint
\href{http://www.arxiv.org/abs/1409.8356v3}{\texttt{arXiv:1409.8356v3}}.
\newline A version which is more often updated can be found at
\url{http://www.math.umn.edu/~reiner/Classes/HopfComb.pdf} .

\bibitem[GrKnPa]{GKP}Ronald L. Graham, Donald E. Knuth, Oren Patashnik,
\textit{Concrete Mathematics, Second Edition}, Addison-Wesley 1994.

\bibitem[Hefferon]{Hefferon}Jim Hefferon, \textit{Linear Algebra}, version of
22 December 2014,\newline\url{http://joshua.smcvt.edu/linearalgebra/} .

\bibitem[Herstein]{Herstein}I. N. Herstein, \textit{Topics in Algebra}, 2nd
edition, John Wiley \& Sons, 1975.

\bibitem[HoffKun]{HoffmanKunze}Kenneth Hoffman, Ray Kunze, \textit{Linear
algebra}, 2nd edition, Prentice-Hall 1971.

\bibitem[Hunger03]{Hungerford-03}%
\href{http://link.springer.com/book/10.1007/978-1-4612-6101-8}{Thomas W.
Hungerford, \textit{Algebra}, 12th printing, Springer 2003.}

\bibitem[Hunger14]{Hungerford}Thomas W. Hungerford, \textit{Abstract Algebra:
An Introduction}, 3rd edition, Brooks/Cole 2014.

\bibitem[Knuth88]{Knuth-fib}%
\href{http://dx.doi.org/10.1016/0893-9659(89)90131-6}{Donald E. Knuth,
\textit{Fibonacci Multiplication}, Appl. Math. Lett., Vol. 1, No. 1, pp.
57--60, 1988}.

\bibitem[Knutson]{Knutson}Allen Knutson,
\textit{\href{http://www.math.cornell.edu/~allenk/schubnotes.pdf}{Schubert
polynomials and Symmetric Functions}}, lecture notes, July 28, 2012.

\bibitem[Kratt]{Krattenthaler}Christian Krattenthaler, \textit{Advanced
Determinant Calculus}, S\'{e}minaire Lotharingien Combin. 42 (1999) (The
Andrews Festschrift), paper B42q, 67 pp.,
\href{http://arxiv.org/abs/math/9902004v3}{arXiv:math/9902004v3}.

\bibitem[Lampe]{Lampe}Philipp Lampe, \textit{Cluster algebras},\newline%
\url{http://www.math.uni-bielefeld.de/~lampe/teaching/cluster/cluster.pdf} .

\begin{verlong}
A version with my corrections:\newline%
\url{https://www.dropbox.com/s/aush67ecrx6twlf/cluster - version 4 dec 2013 EV.pdf?dl=0}
.
\end{verlong}

\bibitem[Lang02]{Lang02}%
\href{http://link.springer.com/book/10.1007/978-1-4613-0041-0}{Serge Lang,
\textit{Algebra}, Revised Third Edition, Graduate Texts in Mathematics \#211,
Springer 2002.}

\bibitem[Laue]{Laue-det}Hartmut Laue, \textit{Determinants}, version 17 May
2015,\newline%
\url{http://www.uni-kiel.de/math/algebra/laue/homepagetexte/det.pdf} .

\bibitem[LeeSchi]{LS2}Kyungyong Lee, Ralf Schiffler, \textit{A Combinatorial
Formula for Rank }$2$\textit{ Cluster Variables},\newline%
\url{http://arxiv.org/abs/1106.0952v3} .

\bibitem[LeeSch2]{LS}Kyungyong Lee, Ralf Schiffler, \textit{Positivity for
cluster algebras},\newline\url{http://arxiv.org/abs/1306.2415v3} .

\bibitem[Leeuwen]{Leeuwen-aS}%
\href{http://wwwmathlabo.univ-poitiers.fr/~maavl/pdf/alt-Schur.pdf}{Marc A. A.
van Leeuwen, \textit{Schur functions and alternating sums}}, Electronic
Journal of Combinatorics Vol 11(2) A5 (2006), also available as
\href{http://arxiv.org/abs/math.CO/0602357}{arXiv:math.CO/0602357}.

\bibitem[LeLeMe16]{LeLeMe16}Eric Lehman, F. Thomson Leighton, Albert R. Meyer,
\textit{Mathematics for Computer Science}, revised Friday 18th March,
2016,\newline\url{https://courses.csail.mit.edu/6.042/spring16/mcs.pdf} .

\bibitem[Loehr11]{Loehr-BC}%
\href{http://www.math.vt.edu/people/nloehr/bijbook.html}{Nicholas A. Loehr,
\textit{Bijective Combinatorics}, Chapman \& Hall/CRC 2011.}

\bibitem[Muir]{Muir}%
\href{http://phalanstere.univ-mlv.fr/~al/#Classiques}{Thomas Muir, \textit{The
theory of determinants in the historical order of development}, 5 volumes
(1906--1930), later reprinted by Dover.}

\bibitem[NouYam]{NoumiYamada}%
\href{http://arxiv.org/abs/math-ph/0203030v2}{Masatoshi Noumi, Yasuhiko
Yamada, \textit{Tropical Robinson-Schensted-Knuth correspondence and
birational Weyl group actions}, arXiv:math-ph/0203030v2.}

\bibitem[Prasolov]{Prasolov}Viktor V. Prasolov,
\textit{\href{http://www2.math.su.se/~mleites/books/prasolov-1994-problems.pdf}{\textit{Problems
and Theorems in Linear Algebra}}}, Translations of Mathematical Monographs,
vol. \#134, AMS 1994.

\bibitem[Pretzel]{Pretzel}Oliver Pretzel, \textit{On reorienting graphs by
pushing down maximal vertices, }Order, 1986, Volume 3, Issue 2, pp. 135--153.

\bibitem[Richma]{Richman}%
\href{http://www.ams.org/journals/proc/1988-103-04/S0002-9939-1988-0954974-5/}{Fred
Richman, \textit{Nontrivial uses of trivial rings}, Proceedings of the
American Mathematical Society, Volume 103, Number 4, August 1988, pp.
1012--1014.}

\bibitem[Rote]{Rote}G\"{u}nter Rote, \textit{Division-Free Algorithms for the
Determinant and the Pfaffian: Algebraic and Combinatorial Approaches},
Computational Discrete Mathematics, Lecture Notes in Computer Science, Volume
2122, 2001, pp. 119--135.

\bibitem[Silvest]{Silvest}%
\href{http://www.mth.kcl.ac.uk/~jrs/gazette/blocks.pdf}{John R. Silvester,
\textit{Determinants of Block Matrices}, The Mathematical Gazette, Vol. 84,
No. 501 (Nov., 2000), pp. 460--467.}

\bibitem[Stan11]{Stanley-EC1}Richard Stanley, \textit{Enumerative
Combinatorics, volume 1}, Second edition, version of 15 July 2011. Available
at \url{http://math.mit.edu/~rstan/ec/} .

\bibitem[Stan01]{Stanley-EC2}Richard Stanley, \textit{Enumerative
Combinatorics, volume 2}, First edition 2001.

\bibitem[Stembr]{Stembridge}%
\href{http://www.combinatorics.org/ojs/index.php/eljc/article/view/v9i1n5}{John
R. Stembridge, \textit{A Concise Proof of the Littlewood-Richardson Rule}, The
Electronic Journal of Combinatorics, Volume 9 (2002), Note \#N5.}

\bibitem[Tenner04]{Tenner-NMU}Bridget Eileen Tenner, \textit{A Non-Messing-Up
Phenomenon for Posets}, Annals of Combinatorics 11 (2007), pp.
101--114.\newline A preprint is available as
\href{http://arxiv.org/abs/math/0404396}{arXiv:math/0404396}.

\bibitem[Walker87]{Walker87}%
\href{https://www.math.nmsu.edu/~elbert/AbsAlgeb.pdf}{Elbert A. Walker,
\textit{Introduction to Abstract Algebra}, Random House/Birkhauser, New York,
1987.}

\bibitem[William03]{William03}%
\href{http://people.mpim-bonn.mpg.de/geordie/Hecke.pdf}{Geordie Williamson,
\textit{Mind your }$P$\textit{ and }$Q$\textit{-symbols: Why the
Kazhdan-Lusztig basis of the Hecke algebra of type A is cellular}, Honours
thesis at the University of Sydney, October 2003.}

\bibitem[Zeilbe]{Zeilbe}%
\href{http://www.math.rutgers.edu/~zeilberg/mamarimY/DM85.pdf}{Doron
Zeilberger, \textit{A combinatorial approach to matrix algebra}, Discrete
Mathematics 56 (1985), pp. 61--72.}
\end{thebibliography}


\end{document}