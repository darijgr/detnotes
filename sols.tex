\documentclass[numbers=enddot,12pt,final,onecolumn,notitlepage]{scrartcl}%
\usepackage[headsepline,footsepline,manualmark]{scrlayer-scrpage}
\usepackage[all,cmtip]{xy}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{framed}
\usepackage{comment}
\usepackage{color}
\usepackage{hyperref}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{needspace}
\usepackage{tabls}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{LastRevised=Thursday, May 25, 2017 01:49:58}
%TCIDATA{SuppressPackageManagement}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Language=American English}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
%EndMSIPreambleData
\usetikzlibrary{arrows}
\newcounter{exer}
\newcounter{exera}
\theoremstyle{definition}
\newtheorem{theo}{Theorem}[section]
\newenvironment{theorem}[1][]
{\begin{theo}[#1]\begin{leftbar}}
{\end{leftbar}\end{theo}}
\newtheorem{lem}[theo]{Lemma}
\newenvironment{lemma}[1][]
{\begin{lem}[#1]\begin{leftbar}}
{\end{leftbar}\end{lem}}
\newtheorem{prop}[theo]{Proposition}
\newenvironment{proposition}[1][]
{\begin{prop}[#1]\begin{leftbar}}
{\end{leftbar}\end{prop}}
\newtheorem{defi}[theo]{Definition}
\newenvironment{definition}[1][]
{\begin{defi}[#1]\begin{leftbar}}
{\end{leftbar}\end{defi}}
\newtheorem{remk}[theo]{Remark}
\newenvironment{remark}[1][]
{\begin{remk}[#1]\begin{leftbar}}
{\end{leftbar}\end{remk}}
\newtheorem{coro}[theo]{Corollary}
\newenvironment{corollary}[1][]
{\begin{coro}[#1]\begin{leftbar}}
{\end{leftbar}\end{coro}}
\newtheorem{conv}[theo]{Convention}
\newenvironment{condition}[1][]
{\begin{conv}[#1]\begin{leftbar}}
{\end{leftbar}\end{conv}}
\newtheorem{quest}[theo]{Question}
\newenvironment{algorithm}[1][]
{\begin{quest}[#1]\begin{leftbar}}
{\end{leftbar}\end{quest}}
\newtheorem{warn}[theo]{Warning}
\newenvironment{conclusion}[1][]
{\begin{warn}[#1]\begin{leftbar}}
{\end{leftbar}\end{warn}}
\newtheorem{conj}[theo]{Conjecture}
\newenvironment{conjecture}[1][]
{\begin{conj}[#1]\begin{leftbar}}
{\end{leftbar}\end{conj}}
\newtheorem{exam}[theo]{Example}
\newenvironment{example}[1][]
{\begin{exam}[#1]\begin{leftbar}}
{\end{leftbar}\end{exam}}
\newtheorem{exmp}[exer]{Exercise}
\newenvironment{exercise}[1][]
{\begin{exmp}[#1]\begin{leftbar}}
{\end{leftbar}\end{exmp}}
\newtheorem{exetwo}[exera]{Additional exercise}
\newenvironment{addexercise}[1][]
{\begin{exetwo}[#1]\begin{leftbar}}
{\end{leftbar}\end{exetwo}}
\newenvironment{statement}{\begin{quote}}{\end{quote}}
\iffalse
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\fi
\let\sumnonlimits\sum
\let\prodnonlimits\prod
\let\cupnonlimits\bigcup
\let\capnonlimits\bigcap
\renewcommand{\sum}{\sumnonlimits\limits}
\renewcommand{\prod}{\prodnonlimits\limits}
\renewcommand{\bigcup}{\cupnonlimits\limits}
\renewcommand{\bigcap}{\capnonlimits\limits}
\setlength\tablinesep{3pt}
\setlength\arraylinesep{3pt}
\setlength\extrarulesep{3pt}
\voffset=0cm
\hoffset=-0.7cm
\setlength\textheight{22.5cm}
\setlength\textwidth{15.5cm}
\newenvironment{verlong}{}{}
\newenvironment{vershort}{}{}
\newenvironment{noncompile}{}{}
\excludecomment{verlong}
\includecomment{vershort}
\excludecomment{noncompile}
\newcommand{\id}{\operatorname{id}}
\ihead{Notes on the combinatorial fundamentals of algebra}
\ohead{page \thepage}
\cfoot{}
\begin{document}

\title{Notes on the combinatorial fundamentals of algebra\thanks{old title: PRIMES
2015 reading project: problems and solutions}}
\author{Darij Grinberg}
\date{
%TCIMACRO{\TeXButton{today}{\today}}%
%BeginExpansion
\today
%EndExpansion
}
\maketitle
\tableofcontents

\section{\label{chp.intro}Introduction}

These notes are meant as a detailed introduction to the basic combinatorics
that underlies the \textquotedblleft explicit\textquotedblright\ part of
abstract algebra (i.e., the theory of determinants, and concrete families of
polynomials). They cover permutations and determinants (from a combinatorial
viewpoint -- no linear algebra is presumed), as well as some basic material on
binomial coefficients and recurrent (Fibonacci-like) sequences. The reader is
assumed to be familiar with (low-level) \textquotedblleft contest
mathematics\textquotedblright\ (i.e., have a good proficiency with high-school
mathematics) and mature enough to read combinatorial proofs.

These notes were originally written for the PRIMES reading project I have
mentored in 2015. The goal of the project was to become familiar with some
fundamentals of algebra and combinatorics (particularly the ones needed to
understand the literature on cluster algebras).

The notes are unfinished and probably full of misprints. I thank Anya Zhang
and Karthik Karnik (the two students taking part in the project) for finding
some errors! Thanks also to the PRIMES project at MIT, which gave the impetus
for the writing of this notes; and to George Lusztig for the sponsorship of my
mentoring position in this project.

\subsection{Prerequisites}

Let me first discuss the prerequisites for a reader of these notes. At the
current moment, I assume that the reader

\begin{itemize}
\item has a good grasp on basic school-level mathematics (integers, rational
numbers, prime numbers, etc.);

\item has some experience with proofs (mathematical induction, strong
induction, proof by contradiction, the concept of \textquotedblleft
WLOG\textquotedblright, etc.) and mathematical notation (functions,
subscripts, cases, what it means for an object to be \textquotedblleft
well-defined\textquotedblright, etc.)\footnote{A great introduction into these
matters (and many others!) is the free book \cite{LeLeMe16} by Lehman,
Leighton and Meyer. (\textbf{Practical note:} As of 2017, this book is still
undergoing frequent revisions; thus, the version I am citing below might be
outdated by the time you are reading this. I therefore suggest searching for
possibly newer versions on the internet. Unfortunately, you will also find
many older versions, often as the first google hits. Try searching for the
title of the book along with the current year to find something up-to-date.)
\par
Another introduction to proofs and mathematical workmanship is Day's
\cite{Day-proofs} (but beware that the definition of polynomials in
\cite[Chapter 5]{Day-proofs} is the wrong one for our purposes). There are
also several books on this subject.};

\item knows what a polynomial is (at least over $\mathbb{Z}$ and $\mathbb{Q}$)
and how polynomials differ from polynomial functions\footnote{See Section
\ref{sect.polynomials-emergency} below for a quick survey of what this means,
and which sources to consult for the precise definitions.};

\item knows the most basic properties of binomial coefficients (e.g., how
$\dbinom{n}{k}$ counts $k$-element subsets of an $n$-element set);

\item knows the basics of modular arithmetic (e.g., if $a\equiv
b\operatorname{mod}n$ and $c\equiv d\operatorname{mod}n$, then $ac\equiv
bd\operatorname{mod}n$);

\item is familiar with the summation sign ($\sum$) and the product sign
($\prod$) and knows how to transform them (e.g., interchanging summations, and
substituting the index)\footnote{See Section \ref{sect.sums-repetitorium}
below for a quick overview of the notations that we will need.};

\item has some familiarity with matrices (i.e., knows how to add and to
multiply them).
\end{itemize}

Probably a few more requirements creep in at certain points of the notes,
which I have overlooked. Some examples and remarks rely on additional
knowledge (such as analysis, graph theory, abstract algebra); however, these
can be skipped.

\begin{noncompile}
Basics on sums and products

In the notes below, we shall make much use of the summation sign ($\sum$) and
the product sign ($\prod$). In this short section, we shall introduce these
signs and

For instance, the reader should understand the following two arguments:

For any nonnegative integer $n$, we have%
\begin{align*}
2\sum_{i=0}^{n}i  &  =\sum_{i=0}^{n}i+\sum_{i=0}^{n}%
i\ \ \ \ \ \ \ \ \ \ \left(  \text{since }2q=q+q\text{ for every }%
q\in\mathbb{Q}\right) \\
&  =\sum_{i=0}^{n}i+\sum_{i=0}^{n}\left(  n-i\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n-i\text{ for
}i\text{ in the second sum}\right) \\
&  =\sum_{i=0}^{n}\underbrace{\left(  i+\left(  n-i\right)  \right)  }%
_{=n}=\sum_{i=0}^{n}n=\left(  n+1\right)  n=n\left(  n+1\right)
\end{align*}
and thus $\sum_{i=0}^{n}i=\dfrac{n\left(  n+1\right)  }{2}$. Since $\sum
_{i=0}^{n}i=0+\sum_{i=1}^{n}i=\sum_{i=1}^{n}i$, this rewrites as $\sum
_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }{2}$.

[to add:

-- $\sum_{s\in S}$

-- $\prod_{s\in S}$

-- def $\sum_{i=a}^{b}$

-- def $\prod_{i=a}^{b}$

-- explain empty sums \& products \& less-than-empty (do not \textquotedblleft
sum backwards\textquotedblright)

-- renaming the summation index

-- substituting index (with several examples)

-- splitting a sum/product on a predicate

-- splitting a sum (careful because of less-than-empty) by interval

-- splitting off addends

-- Fubini (and explain what it means)

-- telescope rule

-- $\sum_{i}\sum_{j}=\sum_{j}\sum_{i}$ in rectangles and triangles

-- little Gauss with 2 proofs (nice and ugly, both showing something)

-- search for ``famous formula'' and replace it by a correct reference

-- $\sum_{i=1}^{n}i^{2}$

-- infinite sums with $0$s and products with $1$s

-- product rule

-- $\sum1 = $ cardinality

-- cardinality of union of disjoint sets as sum

-- cardinality of cartesian product of sets as product

]
\end{noncompile}

\subsection{Notations}

\begin{itemize}
\item In the following, we use $\mathbb{N}$ to denote the set $\left\{
0,1,2,\ldots\right\}  $. (Be warned that some other authors use the letter
$\mathbb{N}$ for $\left\{  1,2,3,\ldots\right\}  $ instead.)

\item We let $\mathbb{Q}$ denote the set of all rational numbers; we let
$\mathbb{R}$ be the set of all real numbers; we let $\mathbb{C}$ be the set of
all complex numbers.

\item If $X$ and $Y$ are two sets, then we shall use the notation
\textquotedblleft$X\rightarrow Y,\ x\mapsto E$\textquotedblright\ (where $x$
is some symbol which has no specific meaning in the current context, and where
$E$ is some expression which usually involves $x$) for \textquotedblleft the
map from $X$ to $Y$ which sends every $x\in X$ to $E$\textquotedblright. For
example, \textquotedblleft$\mathbb{N}\rightarrow\mathbb{N},\ x\mapsto
x^{2}+x+6$\textquotedblright\ means the map from $\mathbb{N}$ to $\mathbb{N}$
which sends every $x\in\mathbb{N}$ to $x^{2}+x+6$. For another example,
\textquotedblleft$\mathbb{N}\rightarrow\mathbb{Q},\ x\mapsto\dfrac{x}{1+x}%
$\textquotedblright\ denotes the map from $\mathbb{N}$ to $\mathbb{Q}$ which
sends every $x\in\mathbb{N}$ to $\dfrac{x}{1+x}$.\ \ \ \ \footnote{A word of
warning: Of course, the notation \textquotedblleft$X\rightarrow Y,\ x\mapsto
E$\textquotedblright\ does not always make sense; indeed, the map that it
stands for might sometimes not exist. For instance, the notation
\textquotedblleft$\mathbb{N}\rightarrow\mathbb{Q},\ x\mapsto\dfrac{x}{1-x}%
$\textquotedblright\ does not actually define a map, because the map that it
is supposed to define (i.e., the map from $\mathbb{N}$ to $\mathbb{Q}$ which
sends every $x\in\mathbb{N}$ to $\dfrac{x}{1-x}$) does not exist (since
$\dfrac{x}{1-x}$ is not defined for $x=1$). For another example, the notation
\textquotedblleft$\mathbb{N}\rightarrow\mathbb{Z},\ x\mapsto\dfrac{x}{1+x}%
$\textquotedblright\ does not define a map, because the map that it is
supposed to define (i.e., the map from $\mathbb{N}$ to $\mathbb{Z}$ which
sends every $x\in\mathbb{N}$ to $\dfrac{x}{1+x}$) does not exist (for $x=2$,
we have $\dfrac{x}{1+x}=\dfrac{2}{1+2}\notin\mathbb{Z}$, which shows that a
map from $\mathbb{N}$ to $\mathbb{Z}$ cannot send this $x$ to this $\dfrac
{x}{1+x}$). Thus, when defining a map from $X$ to $Y$ (using whatever
notation), do not forget to check that it is well-defined (i.e., that your
definition specifies precisely one image for each $x\in X$, and that these
images all lie in $Y$). In many cases, this is obvious or very easy to check
(I will usually not even mention this check), but in some cases, this is a
difficult task.}

\item If $S$ is a set, then the \textit{powerset} of $S$ means the set of all
subsets of $S$. This powerset will be denoted by $\mathcal{P}\left(  S\right)
$. For example, the powerset of $\left\{  1,2\right\}  $ is $\mathcal{P}%
\left(  \left\{  1,2\right\}  \right)  =\left\{  \varnothing,\left\{
1\right\}  ,\left\{  2\right\}  ,\left\{  1,2\right\}  \right\}  $.
\end{itemize}

Further notations will be defined whenever they arise for the first time.

\subsection{\label{sect.sums-repetitorium}Sums and products: a synopsis}

In this section, I will recall the definitions of the $\sum$ and $\prod$ signs
and collect some of their basic properties (without proofs). When I say
\textquotedblleft recall\textquotedblright, I am implying that the reader has
at least some prior acquaintance (and, ideally, experience) with these signs;
for a first introduction, this section is probably too brief and too abstract.
Ideally, you should use this section to familiarize yourself with my
(sometimes idiosyncratic) notations.

Throughout Section \ref{sect.sums-repetitorium}, we let $\mathbb{A}$ be one of
the sets $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and
$\mathbb{C}$.

\subsubsection{Definition of $\sum$}

Let us first define the $\sum$ sign. There are actually several (slightly
different, but still closely related) notations involving the $\sum$ sign; let
us define the most important of them:

\begin{itemize}
\item If $S$ is a finite set, and if $a_{s}$ is an element of $\mathbb{A}$ for
each $s\in S$, then $\sum_{s\in S}a_{s}$ denotes the sum of all of these
elements $a_{s}$. Formally, this sum is defined by recursion on $\left\vert
S\right\vert $, as follows:

\begin{itemize}
\item If $S=\varnothing$, then $\sum_{s\in S}a_{s}$ is defined to be $0$.

\item Let $n\in\mathbb{N}$. Assume that we have defined $\sum_{s\in S}a_{s}$
for every finite set $S$ with $\left\vert S\right\vert =n$ (and every choice
of elements $a_{s}$ of $\mathbb{A}$). Now, if $S$ is a finite set with
$\left\vert S\right\vert =n+1$ (and if $a_{s}\in\mathbb{A}$ are chosen for all
$s\in S$), then $\sum_{s\in S}a_{s}$ is defined by picking any $t\in
S$\ \ \ \ \footnote{This is possible, because $S$ is nonempty (in fact,
$\left\vert S\right\vert =n+1>n\geq0$).} and setting%
\begin{equation}
\sum_{s\in S}a_{s}=a_{t}+\sum_{s\in S\setminus\left\{  t\right\}  }a_{s}.
\label{eq.sum.def.1}%
\end{equation}
It is not immediately clear why this definition is legitimate: The right hand
side of (\ref{eq.sum.def.1}) is defined using a choice of $t$, but we want our
value of $\sum_{s\in S}a_{s}$ to depend only on $S$ and on the $a_{s}$ (not on
some arbitrarily chosen $t\in S$). However, it is possible to prove that the
right hand side of (\ref{eq.sum.def.1}) is actually independent of $t$ (that
is, any two choices of $t$ will lead to the same result).
\end{itemize}

\textbf{Examples:}

\begin{itemize}
\item If $S=\left\{  4,7,9\right\}  $ and $a_{s}=\dfrac{1}{s^{2}}$ for every
$s\in S$, then $\sum_{s\in S}a_{s}=a_{4}+a_{7}+a_{9}=\dfrac{1}{4^{2}}%
+\dfrac{1}{7^{2}}+\dfrac{1}{9^{2}}=\dfrac{6049}{63504}$.

\item If $S=\left\{  1,2,\ldots,n\right\}  $ (for some $n\in\mathbb{N}$) and
$a_{s}=s^{2}$ for every $s\in S$, then $\sum_{s\in S}a_{s}=\sum_{s\in S}%
s^{2}=1^{2}+2^{2}+\cdots+n^{2}$. (There is a formula saying that the right
hand side of this equality is $\dfrac{1}{6}n\left(  2n+1\right)  \left(
n+1\right)  $.)
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{s\in S}a_{s}$ is usually pronounced \textquotedblleft sum
of the $a_{s}$ over all $s\in S$\textquotedblright\ or \textquotedblleft sum
of the $a_{s}$ with $s$ ranging over $S$\textquotedblright\ or
\textquotedblleft sum of the $a_{s}$ with $s$ running through all elements of
$S$\textquotedblright. The letter \textquotedblleft$s$\textquotedblright\ in
the sum is called the \textquotedblleft summation index\textquotedblright%
\footnote{The plural of the word \textquotedblleft index\textquotedblright%
\ here is \textquotedblleft indices\textquotedblright, not \textquotedblleft
indexes\textquotedblright.}, and its exact choice is immaterial (for example,
you can rewrite $\sum_{s\in S}a_{s}$ as $\sum_{t\in S}a_{t}$ or as $\sum
_{\Phi\in S}a_{\Phi}$ or as $\sum_{\spadesuit\in S}a_{\spadesuit}$), as long
as it does not already have a different meaning outside of the sum\footnote{If
it already has a different meaning, then it must not be used as a summation
index! For example, you must not write \textquotedblleft every $n\in
\mathbb{N}$ satisfies $\sum_{n\in\left\{  0,1,\ldots,n\right\}  }%
n=\dfrac{n\left(  n+1\right)  }{2}$\textquotedblright, because here the
summation index $n$ clashes with a different meaning of the letter $n$.}.
(Ultimately, a summation index is the same kind of placeholder variable as the
\textquotedblleft$s$\textquotedblright\ in the statement \textquotedblleft for
all $s\in S$, we have $a_{s}+2a_{s}=3a_{s}$\textquotedblright, or as a loop
variable in a for-loop in programming.) The sign $\sum$ itself is called
\textquotedblleft the summation sign\textquotedblright\ or \textquotedblleft
the $\sum$ sign\textquotedblright. The numbers $a_{s}$ are called the
\textit{addends} (or \textit{summands}) of the sum $\sum_{s\in S}a_{s}$. More
precisely, for any given $t\in S$, we can refer to the number $a_{t}$ as the
\textquotedblleft addend corresponding to the index $t$\textquotedblright\ (or
as the \textquotedblleft addend for $s=t$\textquotedblright, or as the
\textquotedblleft addend for $t$\textquotedblright) of the sum $\sum_{s\in
S}a_{s}$.

\item When the set $S$ is empty, the sum $\sum_{s\in S}a_{s}$ is called an
\textit{empty sum}. Our definition implies that any empty sum is $0$. This
convention is used throughout mathematics, except in rare occasions where a
slightly subtler version of it is used\footnote{Do not worry about this
subtler version for the time being. If you really want to know what it is: Our
above definition is tailored to the cases when the $a_{s}$ are numbers (i.e.,
elements of one of the sets $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$,
$\mathbb{R}$ and $\mathbb{C}$). In more advanced settings, one tends to take
sums of the form $\sum_{s\in S}a_{s}$ where the $a_{s}$ are not numbers but
(for example) elements of a commutative ring $\mathbb{K}$. (See Definition
\ref{def.commring} for the definition of a commutative ring.) In such cases,
one wants the sum $\sum_{s\in S}a_{s}$ for an empty set $S$ to be not the
integer $0$, but the zero of the commutative ring $\mathbb{K}$ (which is
sometimes distinct from the integer $0$). This has the slightly confusing
consequence that the meaning of the sum $\sum_{s\in S}a_{s}$ for an empty set
$S$ depends on what ring $\mathbb{K}$ the $a_{s}$ belong to, even if (for an
empty set $S$) there are no $a_{s}$ to begin with! But in practice, the choice
of $\mathbb{K}$ is always clear from context, so this is not ambiguous.
\par
A similar caveat applies to the other versions of the $\sum$ sign, as well as
to the $\prod$ sign defined further below; I shall not elaborate on it
further.}. If someone tells you that empty sums are undefined, you should not
be listening!

\item The summation index does not always have to be a single letter. For
instance, if $S$ is a set of pairs, then we can write $\sum_{\left(
x,y\right)  \in S}a_{\left(  x,y\right)  }$ (meaning the same as $\sum_{s\in
S}a_{s}$). Here is an example of this notation:%
\[
\sum_{\left(  x,y\right)  \in\left\{  1,2,3\right\}  ^{2}}\dfrac{x}{y}%
=\dfrac{1}{1}+\dfrac{1}{2}+\dfrac{1}{3}+\dfrac{2}{1}+\dfrac{2}{2}+\dfrac{2}%
{3}+\dfrac{3}{1}+\dfrac{3}{2}+\dfrac{3}{3}%
\]
(here, we are using the notation $\sum_{\left(  x,y\right)  \in S}a_{\left(
x,y\right)  }$ with $S=\left\{  1,2,3\right\}  ^{2}$ and $a_{\left(
x,y\right)  }=\dfrac{x}{y}$). Note that we could not have rewritten this sum
in the form $\sum_{s\in S}a_{s}$ with a single-letter variable $s$ without
introducing an extra notation such as $a_{\left(  x,y\right)  }$ for the
quotients $\dfrac{x}{y}$.

\item Mathematicians don't seem to have reached an agreement on the operator
precedence of the $\sum$ sign. By this I mean the following question: Does
$\sum_{s\in S}a_{s}+b$ (where $b$ is some other element of $\mathbb{A}$) mean
$\sum_{s\in S}\left(  a_{s}+b\right)  $ or $\left(  \sum_{s\in S}a_{s}\right)
+b$ ? In my experience, the second interpretation (i.e., reading it as
$\left(  \sum_{s\in S}a_{s}\right)  +b$) is more widespread, and this is the
interpretation that I will follow. Nevertheless, be on the watch for possible
misunderstandings, as someone might be using the first interpretation when you
expect it the least!\footnote{This is similar to the notorious disagreement
about whether $a/bc$ means $\left(  a/b\right)  \cdot c$ or $a/\left(
bc\right)  $.}

However, the situation is different for products and nested sums. For
instance, the expression $\sum_{s\in S}ba_{s}c$ is understood to mean
$\sum_{s\in S}\left(  ba_{s}c\right)  $, and a nested sum like $\sum_{s\in
S}\sum_{t\in T}a_{s,t}$ (where $S$ and $T$ are two sets, and where $a_{s,t}$
is an element of $\mathbb{A}$ for each pair $\left(  s,t\right)  \in S\times
T$) is to be read as $\sum_{s\in S}\left(  \sum_{t\in T}a_{s,t}\right)  $.

\item Speaking of nested sums: they mean exactly what they seem to mean. For
instance, $\sum_{s\in S}\sum_{t\in T}a_{s,t}$ is what you get if you compute
the sum $\sum_{t\in T}a_{s,t}$ for each $s\in S$, and then sum up all of these
sums together. In a nested sum $\sum_{s\in S}\sum_{t\in T}a_{s,t}$, the first
summation sign ($\sum_{s\in S}$) is called the \textquotedblleft outer
summation\textquotedblright, and the second summation sign ($\sum_{t\in T}$)
is called the \textquotedblleft inner summation\textquotedblright.

\item We have required the set $S$ to be finite when defining $\sum_{s\in
S}a_{s}$. Of course, this requirement was necessary for our definition, and
there is no way to make sense of infinite sums such as $\sum_{s\in\mathbb{Z}%
}s^{2}$. However, \textbf{some} infinite sums can be made sense of. The
simplest case is when the set $S$ might be infinite, but only finitely many
among the $a_{s}$ are nonzero. In this case, we can define $\sum_{s\in S}%
a_{s}$ simply by discarding the zero addends and summing the finitely many
remaining addends. Other situations in which infinite sums make sense appear
in analysis and in topological algebra (e.g., power series).

\item The sum $\sum_{s\in S}a_{s}$ always belongs to $\mathbb{A}%
$.\ \ \ \ \footnote{Recall that we have assumed $\mathbb{A}$ to be one of the
sets $\mathbb{N}$, $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$,
and that we have assumed the $a_{s}$ to belong to $\mathbb{A}$.} For instance,
a sum of elements of $\mathbb{N}$ belongs to $\mathbb{N}$; a sum of elements
of $\mathbb{R}$ belongs to $\mathbb{R}$, and so on.
\end{itemize}

\item A slightly more complicated version of the summation sign is the
following: Let $S$ be a finite set, and let $\mathcal{A}\left(  s\right)  $ be
a logical statement defined for every $s\in S$\ \ \ \ \footnote{Formally
speaking, this means that $\mathcal{A}$ is a map from $S$ to the set of all
logical statements. Such a map is called a \textit{predicate}.}. For example,
$S$ can be $\left\{  1,2,3,4\right\}  $, and $\mathcal{A}\left(  s\right)  $
can be the statement \textquotedblleft$s$ is even\textquotedblright. For each
$s\in S$ satisfying $\mathcal{A}\left(  s\right)  $, let $a_{s}$ be an element
of $\mathbb{A}$. Then, the sum $\sum_{\substack{s\in S;\\\mathcal{A}\left(
s\right)  }}a_{s}$ is defined by%
\[
\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\sum
_{s\in\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)  \right\}  }a_{s}.
\]
In other words, $\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }%
}a_{s}$ is the sum of the $a_{s}$ for all $s\in S$ which satisfy
$\mathcal{A}\left(  s\right)  $.

\textbf{Examples:}

\begin{itemize}
\item If $S=\left\{  1,2,3,4,5\right\}  $, then $\sum_{\substack{s\in
S;\\s\text{ is even}}}a_{s}=a_{2}+a_{4}$. (Of course, $\sum_{\substack{s\in
S;\\s\text{ is even}}}a_{s}$ is $\sum_{\substack{s\in S;\\\mathcal{A}\left(
s\right)  }}a_{s}$ when $\mathcal{A}\left(  s\right)  $ is defined to be the
statement \textquotedblleft$s$ is even\textquotedblright.)

\item If $S=\left\{  1,2,\ldots,n\right\}  $ (for some $n\in\mathbb{N}$) and
$a_{s}=s^{2}$ for every $s\in S$, then $\sum_{\substack{s\in S;\\s\text{ is
even}}}a_{s}=a_{2}+a_{4}+\cdots+a_{k}$, where $k$ is the largest even number
among $1,2,\ldots,n$ (that is, $k=n$ if $n$ is even, and $k=n-1$ otherwise).
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}$
is usually pronounced \textquotedblleft sum of the $a_{s}$ over all $s\in S$
satisfying $\mathcal{A}\left(  s\right)  $\textquotedblright. The semicolon
after \textquotedblleft$s\in S$\textquotedblright\ is often omitted or
replaced by a colon or a comma. Many authors often omit the \textquotedblleft%
$s\in S$\textquotedblright\ part (so they simply write $\sum_{\mathcal{A}%
\left(  s\right)  }a_{s}$) when it is clear enough what the $S$ is. (For
instance, they would write $\sum_{1\leq s\leq5}s^{2}$ instead of
$\sum_{\substack{s\in\mathbb{N};\\1\leq s\leq5}}s^{2}$.)

\item The set $S$ needs not be finite in order for $\sum_{\substack{s\in
S;\\\mathcal{A}\left(  s\right)  }}a_{s}$ to be defined; it suffices that the
set $\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)  \right\}  $ be finite
(i.e., that only finitely many $s\in S$ satisfy $\mathcal{A}\left(  s\right)
$).

\item The sum $\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}$
is said to be \textit{empty} whenever the set $\left\{  t\in S\ \mid
\ \mathcal{A}\left(  t\right)  \right\}  $ is empty (i.e., whenever no $s\in
S$ satisfies $\mathcal{A}\left(  s\right)  $).
\end{itemize}

\item Finally, here is the simplest version of the summation sign: Let $u$ and
$v$ be two integers. We agree to understand the set $\left\{  u,u+1,\ldots
,v\right\}  $ to be empty when $u>v$. Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in\left\{  u,u+1,\ldots,v\right\}  $. Then,
$\sum_{s=u}^{v}a_{s}$ is defined by%
\[
\sum_{s=u}^{v}a_{s}=\sum_{s\in\left\{  u,u+1,\ldots,v\right\}  }a_{s}.
\]


\textbf{Examples:}

\begin{itemize}
\item We have $\sum_{s=3}^{8}\dfrac{1}{s}=\sum_{s\in\left\{  3,4,\ldots
,8\right\}  }\dfrac{1}{s}=\dfrac{1}{3}+\dfrac{1}{4}+\dfrac{1}{5}+\dfrac{1}%
{6}+\dfrac{1}{7}+\dfrac{1}{8}=\dfrac{341}{280}$.

\item We have $\sum_{s=3}^{3}\dfrac{1}{s}=\sum_{s\in\left\{  3\right\}
}\dfrac{1}{s}=\dfrac{1}{3}$.

\item We have $\sum_{s=3}^{2}\dfrac{1}{s}=\sum_{s\in\varnothing}\dfrac{1}%
{s}=0$.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{s=u}^{v}a_{s}$ is usually pronounced \textquotedblleft
sum of the $a_{s}$ for all $s$ from $u$ to $v$ (inclusive)\textquotedblright.
It is often written $a_{u}+a_{u+1}+\cdots+a_{v}$, but this latter notation has
its drawbacks: In order to understand an expression like $a_{u}+a_{u+1}%
+\cdots+a_{v}$, one needs to correctly guess the pattern (which can be
unintuitive when the $a_{s}$ themselves are complicated: for example, it takes
a while to find the \textquotedblleft moving parts\textquotedblright\ in the
expression $\dfrac{2\cdot7}{3+2}+\dfrac{3\cdot7}{3+3}+\cdots+\dfrac{7\cdot
7}{3+7}$, whereas the notation $\sum_{s=2}^{7}\dfrac{s\cdot7}{3+s}$ for the
same sum is perfectly clear).

\item In the sum $\sum_{s=u}^{v}a_{s}$, the integer $u$ is called the
\textit{lower limit} (of the sum), whereas the integer $v$ is called the
\textit{upper limit} (of the sum). The sum is said to \textit{start} (or
\textit{begin}) at $u$ and \textit{end} at $v$.

\item The sum $\sum_{s=u}^{v}a_{s}$ is said to be \textit{empty} whenever
$u>v$. In other words, a sum of the form $\sum_{s=u}^{v}a_{s}$ is empty
whenever it \textquotedblleft ends before it has begun\textquotedblright.
However, a sum which \textquotedblleft ends right after it
begins\textquotedblright\ (i.e., a sum $\sum_{s=u}^{v}a_{s}$ with $u=v$) is
not empty; it just has one addend only. (This is unlike integrals, which are
$0$ whenever their lower and upper limit are equal.)

\item Let me stress once again that a sum $\sum_{s=u}^{v}a_{s}$ with $u>v$ is
empty and equals $0$. It does not matter how much greater $u$ is than $v$. So,
for example, $\sum_{s=1}^{-5}s=0$. The fact that the upper bound ($-5$) is
much smaller than the lower bound ($1$) does not mean that you have to
subtract rather than add.
\end{itemize}
\end{itemize}

Thus we have introduced the main three forms of the summation sign. Some mild
variations on them appear in the literature (e.g., there is a slightly awkward
notation $\sum_{\substack{s=u;\\\mathcal{A}\left(  s\right)  }}^{v}a_{s}$ for
$\sum_{\substack{s\in\left\{  u,u+1,\ldots,v\right\}  ;\\\mathcal{A}\left(
s\right)  }}a_{s}$).

\subsubsection{Properties of $\sum$}

Let me now show some basic properties of summation signs that are important in
making them useful:

\begin{itemize}
\item \underline{\textbf{Splitting-off:}} Let $S$ be a finite set. Let $t\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=a_{t}+\sum_{s\in S\setminus\left\{  t\right\}  }a_{s}.
\label{eq.sum.split-off}%
\end{equation}
(This is precisely the equality (\ref{eq.sum.def.1}).) This formula
(\ref{eq.sum.split-off}) allows us to \textquotedblleft split
off\textquotedblright\ an addend from a sum.

\textbf{Example:} If $n\in\mathbb{N}$, then
\[
\sum_{s\in\left\{  1,2,\ldots,n+1\right\}  }a_{s}=a_{n+1}+\sum_{s\in\left\{
1,2,\ldots,n\right\}  }a_{s}%
\]
(by (\ref{eq.sum.split-off}), applied to $S=\left\{  1,2,\ldots,n+1\right\}  $
and $t=n+1$), but also%
\[
\sum_{s\in\left\{  1,2,\ldots,n+1\right\}  }a_{s}=a_{1}+\sum_{s\in\left\{
2,3,\ldots,n+1\right\}  }a_{s}%
\]
(by (\ref{eq.sum.split-off}), applied to $S=\left\{  1,2,\ldots,n+1\right\}  $
and $t=1$).

\item \underline{\textbf{Splitting:}} Let $S$ be a finite set. Let $X$ and $Y$
be two subsets of $S$ such that $X\cap Y=\varnothing$ and $X\cup Y=S$.
(Equivalently, $X$ and $Y$ are two subsets of $S$ such that each element of
$S$ lies in \textbf{exactly} one of $X$ and $Y$.) Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{s\in X}a_{s}+\sum_{s\in Y}a_{s}. \label{eq.sum.split}%
\end{equation}
(Here, as we explained, $\sum_{s\in X}a_{s}+\sum_{s\in Y}a_{s}$ stands for
$\left(  \sum_{s\in X}a_{s}\right)  +\left(  \sum_{s\in Y}a_{s}\right)  $.)
The idea behind (\ref{eq.sum.split}) is that if we want to add a bunch of
numbers (the $a_{s}$ for $s\in S$), we can proceed by splitting it into two
\textquotedblleft sub-bunches\textquotedblright\ (one \textquotedblleft
sub-bunch\textquotedblright\ consisting of the $a_{s}$ for $s\in X$, and the
other consisting of the $a_{s}$ for $s\in Y$), then take the sum of each of
these two sub-bunches, and finally add together the two sums.

\textbf{Examples:}

\begin{itemize}
\item If $n\in\mathbb{N}$, then%
\[
\sum_{s\in\left\{  1,2,\ldots,2n\right\}  }a_{s}=\sum_{s\in\left\{
1,3,\ldots,2n-1\right\}  }a_{s}+\sum_{s\in\left\{  2,4,\ldots,2n\right\}
}a_{s}%
\]
(by (\ref{eq.sum.split}), applied to $S=\left\{  1,2,\ldots,2n\right\}  $,
$X=\left\{  1,3,\ldots,2n-1\right\}  $ and $Y=\left\{  2,4,\ldots,2n\right\}
$.)

\item If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, then%
\[
\sum_{s\in\left\{  -m,-m+1,\ldots,n\right\}  }a_{s}=\sum_{s\in\left\{
-m,-m+1,\ldots,0\right\}  }a_{s}+\sum_{s\in\left\{  1,2,\ldots,n\right\}
}a_{s}%
\]
(by (\ref{eq.sum.split}), applied to $S=\left\{  -m,-m+1,\ldots,n\right\}  $,
$X=\left\{  -m,-m+1,\ldots,0\right\}  $ and $Y=\left\{  1,2,\ldots,n\right\}
$.)

\item If $u$, $v$ and $w$ are three integers such that $u-1\leq v\leq w$, and
if $a_{s}$ is an element of $\mathbb{A}$ for each $s\in\left\{  u,u+1,\ldots
,w\right\}  $, then%
\begin{equation}
\sum_{s=u}^{w}a_{s}=\sum_{s=u}^{v}a_{s}+\sum_{s=v+1}^{w}a_{s}.
\label{eq.sum.split.uvw}%
\end{equation}
This follows from (\ref{eq.sum.split}), applied to $S=\left\{  u,u+1,\ldots
,w\right\}  $, $X=\left\{  u,u+1,\ldots,v\right\}  $ and $Y=\left\{
v+1,v+2,\ldots,w\right\}  $. Notice that the requirement $u-1\leq v\leq w$ is
important; otherwise, the $X\cap Y=\varnothing$ and $X\cup Y=S$ condition
would not hold!
\end{itemize}

\item \underline{\textbf{Splitting using a predicate:}} Let $S$ be a finite
set. Let $\mathcal{A}\left(  s\right)  $ be a logical statement for each $s\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)
}}a_{s}+\sum_{\substack{s\in S;\\\text{not }\mathcal{A}\left(  s\right)
}}a_{s} \label{eq.sum.split.pred}%
\end{equation}
(where \textquotedblleft not $\mathcal{A}\left(  s\right)  $\textquotedblright%
\ means the negation of $\mathcal{A}\left(  s\right)  $). This simply follows
from (\ref{eq.sum.split}), applied to $X=\left\{  s\in S\ \mid\ \mathcal{A}%
\left(  s\right)  \right\}  $ and $Y=\left\{  s\in S\ \mid\ \text{not
}\mathcal{A}\left(  s\right)  \right\}  $.

\textbf{Example:} If $S\subseteq\mathbb{Z}$, then%
\[
\sum_{s\in S}a_{s}=\sum_{\substack{s\in S;\\s\text{ is even}}}a_{s}%
+\sum_{\substack{s\in S;\\s\text{ is odd}}}a_{s}%
\]
(because \textquotedblleft$s$ is odd\textquotedblright\ is the negation of
\textquotedblleft$s$ is even\textquotedblright).

\item \underline{\textbf{Summing equal values:}} Let $S$ be a finite set. Let
$a$ be an element of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{s\in S}a=\left\vert S\right\vert \cdot a. \label{eq.sum.equal}%
\end{equation}
In other words, if all addends of a sum are equal to one and the same element
$a$, then the sum is just the number of its addends times $a$. In particular,%
\[
\sum_{s\in S}1=\left\vert S\right\vert \cdot1=\left\vert S\right\vert .
\]


\item \underline{\textbf{Splitting an addend:}} Let $S$ be a finite set. For
every $s\in S$, let $a_{s}$ and $b_{s}$ be elements of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{s\in S}\left(  a_{s}+b_{s}\right)  =\sum_{s\in S}a_{s}+\sum_{s\in
S}b_{s}. \label{eq.sum.linear1}%
\end{equation}


\textbf{Remark:} Of course, similar rules hold for other forms of summations:
If $\mathcal{A}\left(  s\right)  $ is a logical statement for each $s\in S$,
then%
\[
\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}\left(  a_{s}%
+b_{s}\right)  =\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }%
}a_{s}+\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}b_{s}.
\]
If $u$ and $v$ are two integers, then%
\begin{equation}
\sum_{s=u}^{v}\left(  a_{s}+b_{s}\right)  =\sum_{s=u}^{v}a_{s}+\sum_{s=u}%
^{v}b_{s}. \label{eq.sum.linear1.c}%
\end{equation}


\item \underline{\textbf{Factoring out:}} Let $S$ be a finite set. For every
$s\in S$, let $a_{s}$ be an element of $\mathbb{A}$. Also, let $\lambda$ be an
element of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{s\in S}\lambda a_{s}=\lambda\sum_{s\in S}a_{s}. \label{eq.sum.linear2}%
\end{equation}
Again, similar rules hold for the other types of summation sign.

\item \underline{\textbf{Zeros sum to zero:}} Let $S$ be a finite set. Then,%
\[
\sum_{s\in S}0=0.
\]
That is, any sum of zeroes is zero.

\textbf{Remark:} This applies even to infinite sums! Do not be fooled by the
infiniteness of a sum: There are no reasonable situations where an infinite
sum of zeroes is defined to be anything other than zero. The infinity does not
\textquotedblleft compensate\textquotedblright\ for the zero.

\item \underline{\textbf{Dropping zeroes:}} Let $S$ be a finite set. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Let $T$ be a subset
of $S$ such that every $s\in T$ satisfies $a_{s}=0$. Then,%
\[
\sum_{s\in S}a_{s}=\sum_{s\in S\setminus T}a_{s}.
\]
(That is, any addends which are zero can be removed from a sum without
changing the sum's value.)

\item \underline{\textbf{Renaming the index:}} Let $S$ be a finite set. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\sum_{s\in S}a_{s}=\sum_{t\in S}a_{t}.
\]
This is just saying that the summation index in a sum can be renamed at will,
as long as its name does not clash with other notation.

\item \underline{\textbf{Substituting the index I:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{t}$
be an element of $\mathbb{A}$ for each $t\in T$. Then,%
\begin{equation}
\sum_{t\in T}a_{t}=\sum_{s\in S}a_{f\left(  s\right)  }. \label{eq.sum.subs1}%
\end{equation}
(The idea here is that the sum $\sum_{s\in S}a_{f\left(  s\right)  }$ contains
the same addends as the sum $\sum_{t\in T}a_{t}$.)

\textbf{Examples:}

\begin{itemize}
\item For any $n\in\mathbb{N}$, we have%
\[
\sum_{t\in\left\{  1,2,\ldots,n\right\}  }t^{3}=\sum_{s\in\left\{
-n,-n+1,\ldots,-1\right\}  }\left(  -s\right)  ^{3}.
\]
(This follows from (\ref{eq.sum.subs1}), applied to $S=\left\{  -n,-n+1,\ldots
,-1\right\}  $, \newline$T=\left\{  1,2,\ldots,n\right\}  $, $f\left(
s\right)  =-s$, and $a_{t}=t^{3}$.)

\item The sets $S$ and $T$ in (\ref{eq.sum.subs1}) may well be the same. For
example, for any $n\in\mathbb{N}$, we have%
\[
\sum_{t\in\left\{  1,2,\ldots,n\right\}  }t^{3}=\sum_{s\in\left\{
1,2,\ldots,n\right\}  }\left(  n+1-s\right)  ^{3}.
\]
(This follows from (\ref{eq.sum.subs1}), applied to $S=\left\{  1,2,\ldots
,n\right\}  $, $T=\left\{  1,2,\ldots,n\right\}  $, $f\left(  s\right)
=n+1-s$ and $a_{t}=t^{3}$.)

\item More generally: Let $u$ and $v$ be two integers. Then, the map
\newline$\left\{  u,u+1,\ldots,v\right\}  \rightarrow\left\{  u,u+1,\ldots
,v\right\}  $ sending each $s\in\left\{  u,u+1,\ldots,v\right\}  $ to $u+v-s$
is a bijection\footnote{Check this!}. Hence, we can substitute $u+v-s$ for $s$
in the sum $\sum_{s=u}^{v}a_{s}$ whenever an element $a_{s}$ of $\mathbb{A}$
is given for each $s\in\left\{  u,u+1,\ldots,v\right\}  $. We thus obtain the
formula%
\[
\sum_{s=u}^{v}a_{s}=\sum_{s=u}^{v}a_{u+v-s}.
\]

\end{itemize}

\textbf{Remark:}

\begin{itemize}
\item When I use (\ref{eq.sum.subs1}) to rewrite the sum $\sum_{t\in T}a_{t}$
as $\sum_{s\in S}a_{f\left(  s\right)  }$, I say that I have \textquotedblleft
substituted $f\left(  s\right)  $ for $t$ in the sum\textquotedblright.
Conversely, when I use (\ref{eq.sum.subs1}) to rewrite the sum $\sum_{s\in
S}a_{f\left(  s\right)  }$ as $\sum_{t\in T}a_{t}$, I say that I have
\textquotedblleft substituted $t$ for $f\left(  s\right)  $ in the
sum\textquotedblright.

\item For convenience, I have chosen $s$ and $t$ as summation indices in
(\ref{eq.sum.subs1}). But as before, they can be chosen to be any letters not
otherwise used. It is perfectly okay to use one and the same letter for both
of them, e.g., to write $\sum_{s\in T}a_{s}=\sum_{s\in S}a_{f\left(  s\right)
}$.

\item Here is the probably most famous example of substitution in a sum: Fix a
nonnegative integer $n$. Then, we can substitute $n-i$ for $i$ in the sum
$\sum_{i=0}^{n}i$ (since the map $\left\{  0,1,\ldots,n\right\}
\rightarrow\left\{  0,1,\ldots,n\right\}  ,\ i\mapsto n-i$ is a bijection).
Thus, we obtain%
\[
\sum_{i=0}^{n}i=\sum_{i=0}^{n}\left(  n-i\right)  .
\]
Now,%
\begin{align*}
2\sum_{i=0}^{n}i  &  =\sum_{i=0}^{n}i+\underbrace{\sum_{i=0}^{n}i}%
_{=\sum_{i=0}^{n}\left(  n-i\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}2q=q+q\text{ for every }q\in\mathbb{Q}\right) \\
&  =\sum_{i=0}^{n}i+\sum_{i=0}^{n}\left(  n-i\right) \\
&  =\sum_{i=0}^{n}\underbrace{\left(  i+\left(  n-i\right)  \right)  }%
_{=n}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have used
(\ref{eq.sum.linear1.c}) backwards}\right) \\
&  =\sum_{i=0}^{n}n=\left(  n+1\right)  n\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sum.equal})}\right) \\
&  =n\left(  n+1\right)  ,
\end{align*}
and therefore%
\begin{equation}
\sum_{i=0}^{n}i=\dfrac{n\left(  n+1\right)  }{2}. \label{eq.sum.littlegauss1}%
\end{equation}
Since $\sum_{i=0}^{n}i=0+\sum_{i=1}^{n}i=\sum_{i=1}^{n}i$, this rewrites as%
\begin{equation}
\sum_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }{2}. \label{eq.sum.littlegauss2}%
\end{equation}

\end{itemize}

\item \underline{\textbf{Substituting the index II:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{s}$
be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{t\in T}a_{f^{-1}\left(  t\right)  }.
\label{eq.sum.subs2}%
\end{equation}
This is, of course, just (\ref{eq.sum.subs1}) but applied to $T$, $S$ and
$f^{-1}$ instead of $S$, $T$ and $f$. (Nevertheless, I prefer to mention
(\ref{eq.sum.subs2}) separately because it often is used in this very form.)

\item \underline{\textbf{Telescoping sums:}} Let $u$ and $v$ be two integers
such that $u-1\leq v$. Let $a_{s}$ be an element of $\mathbb{A}$ for each
$s\in\left\{  u-1,u,\ldots,v\right\}  $. Then,%
\begin{equation}
\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  =a_{v}-a_{u-1}.
\label{eq.sum.telescope}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item Let us give a new proof of (\ref{eq.sum.littlegauss2}). Indeed, fix a
nonnegative integer $n$. An easy computation reveals that%
\begin{equation}
s=\dfrac{s\left(  s+1\right)  }{2}-\dfrac{\left(  s-1\right)  \left(  \left(
s-1\right)  +1\right)  }{2} \label{eq.sum.littlegauss2.pf2.1}%
\end{equation}
for each $s\in\mathbb{Z}$. Thus,%
\begin{align*}
\sum_{i=1}^{n}i  &  =\sum_{s=1}^{n}s=\sum_{s=1}^{n}\left(  \dfrac{s\left(
s+1\right)  }{2}-\dfrac{\left(  s-1\right)  \left(  \left(  s-1\right)
+1\right)  }{2}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sum.littlegauss2.pf2.1})}\right) \\
&  =\dfrac{n\left(  n+1\right)  }{2}-\underbrace{\dfrac{\left(  1-1\right)
\left(  \left(  1-1\right)  +1\right)  }{2}}_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.telescope}), applied to
}u=1\text{, }v=n\text{ and }a_{s}=\dfrac{s\left(  s+1\right)  }{2}\right) \\
&  =\dfrac{n\left(  n+1\right)  }{2}.
\end{align*}
Thus, (\ref{eq.sum.littlegauss2}) is proven again. This kind of proof works
often when we need to prove a formula like (\ref{eq.sum.littlegauss2}); the
only tricky part was to \textquotedblleft guess\textquotedblright\ the right
value of $a_{s}$, which is straightforward if you know what you are looking
for (you want $a_{n}-a_{1}$ to be $\dfrac{n\left(  n+1\right)  }{2}$), but
rather tricky if you don't.

\item Other examples for the use of (\ref{eq.sum.telescope}) can be found on
\href{https://en.wikipedia.org/wiki/Telescoping_series}{the Wikipedia page for
\textquotedblleft telescoping series\textquotedblright}. Let me add just one
more example: Given $n\in\mathbb{N}$, we want to compute $\sum_{i=1}^{n}%
\dfrac{1}{\sqrt{i}+\sqrt{i+1}}$. (Here, of course, we need to take
$\mathbb{A}=\mathbb{R}$ or $\mathbb{A}=\mathbb{C}$.) We proceed as follows:
For every positive integer $i$, we have%
\[
\dfrac{1}{\sqrt{i}+\sqrt{i+1}}=\dfrac{\left(  \sqrt{i+1}-\sqrt{i}\right)
}{\left(  \sqrt{i}+\sqrt{i+1}\right)  \left(  \sqrt{i+1}-\sqrt{i}\right)
}=\sqrt{i+1}-\sqrt{i}%
\]
(since $\left(  \sqrt{i}+\sqrt{i+1}\right)  \left(  \sqrt{i+1}-\sqrt
{i}\right)  =\left(  \sqrt{i+1}\right)  ^{2}-\left(  \sqrt{i}\right)
^{2}=\left(  i+1\right)  -i=1$). Thus,%
\begin{align*}
&  \sum_{i=1}^{n}\dfrac{1}{\sqrt{i}+\sqrt{i+1}}\\
&  =\sum_{i=1}^{n}\left(  \sqrt{i+1}-\sqrt{i}\right)  =\sum_{s=2}^{n+1}\left(
\sqrt{s}-\sqrt{s-1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }s-1\text{ for }i\text{ in the sum,}\\
\text{since the map }\left\{  2,3,\ldots,n+1\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  ,\ s\mapsto s-1\\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sqrt{n+1}-\underbrace{\sqrt{2-1}}_{=\sqrt{1}=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.telescope}), applied to
}u=2\text{, }v=n+1\text{ and }a_{s}=\sqrt{s}-\sqrt{s-1}\right) \\
&  =\sqrt{n+1}-1.
\end{align*}

\end{itemize}

\textbf{Remark:} When we use the equality (\ref{eq.sum.telescope}) to rewrite
the sum $\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  $ as $a_{v}-a_{u-1}$, we
can say that the sum $\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  $
\textquotedblleft telescopes\textquotedblright\ to $a_{v}-a_{u-1}$. A sum like
$\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)  $ is said to be a
\textquotedblleft telescoping sum\textquotedblright. This terminology
references the idea that the sum $\sum_{s=u}^{v}\left(  a_{s}-a_{s-1}\right)
$ \textquotedblleft shrink\textquotedblright\ to the simple difference
$a_{v}-a_{u-1}$ like a telescope does when it is collapsed.

\item \underline{\textbf{Restricting to a subset:}} Let $S$ be a finite set.
Let $T$ be a subset of $S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each
$s\in T$. Then,%
\[
\sum_{\substack{s\in S;\\s\in T}}a_{s}=\sum_{s\in T}a_{s}.
\]
This is because the $s\in S$ satisfying $s\in T$ are exactly the elements of
$T$.

\textbf{Remark:} Here is a slightly more general form of this rule: Let $S$ be
a finite set. Let $T$ be a subset of $S$. Let $\mathcal{A}\left(  s\right)  $
be a logical statement for each $s\in S$. Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in T$ satisfying $\mathcal{A}\left(  s\right)  $.
Then,%
\[
\sum_{\substack{s\in S;\\s\in T;\\\mathcal{A}\left(  s\right)  }}a_{s}%
=\sum_{\substack{s\in T;\\\mathcal{A}\left(  s\right)  }}a_{s}.
\]


\item \underline{\textbf{Splitting a sum by a value of a function:}} Let $S$
be a finite set. Let $W$ be a set. Let $f:S\rightarrow W$ be a map. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{w\in W}\sum_{\substack{s\in S;\\f\left(  s\right)
=w}}a_{s}.\label{eq.sum.sheph}%
\end{equation}
The idea behind this formula is the following: The left hand side is the sum
of all $a_{s}$ for $s\in S$. The right hand side is the same sum, but split in
a particular way: First, for each $w\in W$, we sum the $a_{s}$ for all $s\in
S$ satisfying $f\left(  s\right)  =w$, and then we take the sum of all these
\textquotedblleft partial sums\textquotedblright.

\textbf{Examples:}

\begin{itemize}
\item Let $n\in\mathbb{N}$. Then,%
\begin{equation}
\sum_{s\in\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  }s^{3}%
=\sum_{w\in\left\{  0,1,\ldots,n\right\}  }\sum_{\substack{s\in\left\{
-n,-\left(  n-1\right)  ,\ldots,n\right\}  ;\\\left\vert s\right\vert
=w}}s^{3}.\label{eq.sum.sheph.exam1}%
\end{equation}
(This follows from (\ref{eq.sum.sheph}), applied to $S=\left\{  -n,-\left(
n-1\right)  ,\ldots,n\right\}  $, $W=\left\{  0,1,\ldots,n\right\}  $ and
$f\left(  s\right)  =\left\vert s\right\vert $.) You might wonder what you
gain by this observation. But actually, it allows you to compute the sum: For
any $w\in\left\{  0,1,\ldots,n\right\}  $, the sum $\sum_{\substack{s\in
\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  ;\\\left\vert s\right\vert
=w}}s^{3}$ is $0$\ \ \ \ \footnote{\textit{Proof.} If $w=0$, then this sum
$\sum_{\substack{s\in\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}
;\\\left\vert s\right\vert =w}}s^{3}$ consists of one addend only, and this
addend is $0^{3}$. If $w>0$, then this sum has two addends, namely $\left(
-w\right)  ^{3}$ and $w^{3}$. In either case, the sum is $0$ (because
$0^{3}=0$ and $\left(  -w\right)  ^{3}+w^{3}=-w^{3}+w^{3}=0$).}, and therefore
(\ref{eq.sum.sheph.exam1}) becomes%
\[
\sum_{s\in\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  }s^{3}%
=\sum_{w\in\left\{  0,1,\ldots,n\right\}  }\underbrace{\sum_{\substack{s\in
\left\{  -n,-\left(  n-1\right)  ,\ldots,n\right\}  ;\\\left\vert s\right\vert
=w}}s^{3}}_{=0}=\sum_{w\in\left\{  0,1,\ldots,n\right\}  }0=0.
\]
Thus, a strategic application of (\ref{eq.sum.sheph}) can help in evaluating a sum.

\item Let $S$ be a finite set. Let $W$ be a set. Let $f:S\rightarrow W$ be a
map. If we apply (\ref{eq.sum.sheph}) to $a_{s}=1$, then we obtain%
\[
\sum_{s\in S}1=\sum_{w\in W}\underbrace{\sum_{\substack{s\in S;\\f\left(
s\right)  =w}}1}_{\substack{=\left\vert \left\{  s\in S\ \mid\ f\left(
s\right)  =w\right\}  \right\vert \cdot1\\=\left\vert \left\{  s\in
S\ \mid\ f\left(  s\right)  =w\right\}  \right\vert }}=\sum_{w\in W}\left\vert
\left\{  s\in S\ \mid\ f\left(  s\right)  =w\right\}  \right\vert .
\]
Since $\sum_{s\in S}1=\left\vert S\right\vert \cdot1=\left\vert S\right\vert
$, this rewrites as follows:%
\begin{equation}
\left\vert S\right\vert =\sum_{w\in W}\left\vert \left\{  s\in S\ \mid
\ f\left(  s\right)  =w\right\}  \right\vert .\label{eq.sum.sheph.exam2}%
\end{equation}
This equality is often called the \textit{shepherd's principle}, because it is
connected to the joke that \textquotedblleft in order to count a flock of
sheep, just count the legs and divide by $4$\textquotedblright. The connection
is somewhat weak, actually; the equality (\ref{eq.sum.sheph.exam2}) is better
regarded as a formalization of the (less funny) idea that in order to count
all legs of a flock of sheep, you can count the legs of every single sheep,
and then sum the resulting numbers over all sheep in the flock. Think of the
$S$ in (\ref{eq.sum.sheph.exam2}) as the set of all legs of all sheep in the
flock; think of $W$ as the set of all sheep in the flock; and think of $f$ as
the function which sends every leg to the (hopefully uniquely determined)
sheep it belongs to.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item If $f:S\rightarrow W$ is a map between two sets $S$ and $W$, and if $w$
is an element of $W$, then it is common to denote the set $\left\{  s\in
S\ \mid\ f\left(  s\right)  =w\right\}  $ by $f^{-1}\left(  w\right)  $.
(Formally speaking, this notation might clash with the notation $f^{-1}\left(
w\right)  $ for the actual preimage of $w$ when $f$ happens to be bijective;
but in practice, this causes far less confusion than it might seem to.) Using
this notation, we can rewrite (\ref{eq.sum.sheph}) as follows:%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{w\in W}\underbrace{\sum_{\substack{s\in S;\\f\left(
s\right)  =w}}}_{=\sum_{s\in f^{-1}\left(  w\right)  }}a_{s}=\sum_{w\in W}%
\sum_{s\in f^{-1}\left(  w\right)  }a_{s}.\label{eq.sum.sheph.preimg}%
\end{equation}


\item When I rewrite a sum $\sum_{s\in S}a_{s}$ as $\sum_{w\in W}%
\sum_{\substack{s\in S;\\f\left(  s\right)  =w}}a_{s}$ (or as $\sum_{w\in
W}\sum_{s\in f^{-1}\left(  w\right)  }a_{s}$), I say that I am
\textquotedblleft splitting the sum according to the value of $f\left(
s\right)  $\textquotedblright. (Though, most of the time, I shall be doing
such manipulations without explicit mention.)
\end{itemize}

\item \underline{\textbf{Splitting a sum into subsums:}} Let $S$ be a finite
set. Let $S_{1},S_{2},\ldots,S_{n}$ be finitely many subsets of $S$. Assume
that these subsets $S_{1},S_{2},\ldots,S_{n}$ are pairwise disjoint (i.e., we
have $S_{i}\cap S_{j}=\varnothing$ for any two distinct elements $i$ and $j$
of $\left\{  1,2,\ldots,n\right\}  $) and their union is $S$. (Thus, every
element of $S$ lies in precisely one of the subsets $S_{1},S_{2},\ldots,S_{n}%
$.) Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}=\sum_{w=1}^{n}\sum_{s\in S_{w}}a_{s}.\label{eq.sum.split-n}%
\end{equation}
This is a generalization of (\ref{eq.sum.split}) (indeed, (\ref{eq.sum.split})
is obtained from (\ref{eq.sum.split-n}) by setting $n=2$, $S_{1}=X$ and
$S_{2}=Y$). It is also a consequence of (\ref{eq.sum.sheph}): Indeed, set
$W=\left\{  1,2,\ldots,n\right\}  $, and define a map $f:S\rightarrow W$ to
send each $s\in S$ to the unique $w\in\left\{  1,2,\ldots,n\right\}  $ for
which $s\in S_{w}$. Then, every $w\in W$ satisfies $\sum_{\substack{s\in
S;\\f\left(  s\right)  =w}}a_{s}=\sum_{s\in S_{w}}a_{s}$; therefore,
(\ref{eq.sum.sheph}) becomes (\ref{eq.sum.split-n}).

\textbf{Example:} If we set $a_{s}=1$ for each $s\in S$, then
(\ref{eq.sum.split-n}) becomes%
\[
\sum_{s\in S}1=\sum_{w=1}^{n}\underbrace{\sum_{s\in S_{w}}1}_{=\left\vert
S_{w}\right\vert }=\sum_{w=1}^{n}\left\vert S_{w}\right\vert .
\]
Hence,
\[
\sum_{w=1}^{n}\left\vert S_{w}\right\vert =\sum_{s\in S}1=\left\vert
S\right\vert .
\]


\item \underline{\textbf{Fubini's theorem (interchanging the order of
summation):}} Let $X$ and $Y$ be two finite sets. Let $a_{\left(  x,y\right)
}$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in X\times Y$.
Then,%
\begin{equation}
\sum_{x\in X}\sum_{y\in Y}a_{\left(  x,y\right)  }=\sum_{\left(  x,y\right)
\in X\times Y}a_{\left(  x,y\right)  }=\sum_{y\in Y}\sum_{x\in X}a_{\left(
x,y\right)  }.\label{eq.sum.fubini}%
\end{equation}
This is called \textit{Fubini's theorem for finite sums}, and is a lot easier
to prove than what analysts tend to call Fubini's theorem. I shall sketch a
proof shortly (in the Remark below); but first, let me give some intuition for
the statement. Imagine that you have a rectangular table filled with numbers.
If you want to sum the numbers in the table, you can proceed in several ways.
One way is to sum the numbers in each row, and then sum all the sums you have
obtained. Another way is to sum the numbers in each column, and then sum all
the obtained sums. Either way, you get the same result -- namely, the sum of
all numbers in the table. This is essentially what (\ref{eq.sum.fubini}) says,
at least when $X=\left\{  1,2,\ldots,n\right\}  $ and $Y=\left\{
1,2,\ldots,m\right\}  $ for some integers $n$ and $m$. In this case, the
numbers $a_{\left(  x,y\right)  }$ can be viewed as forming a table, where
$a_{\left(  x,y\right)  }$ is placed in the cell at the intersection of row
$x$ with column $y$. When $X$ and $Y$ are arbitrary finite sets (not
necessarily $\left\{  1,2,\ldots,n\right\}  $ and $\left\{  1,2,\ldots
,m\right\}  $), then you need to slightly stretch your imagination in order to
see the $a_{\left(  x,y\right)  }$ as \textquotedblleft forming a
table\textquotedblright; in fact, there is no obvious order in which the
numbers appear in a row or column, but there is still a notion of rows and columns.

\textbf{Examples:}

\begin{itemize}
\item Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $a_{\left(  x,y\right)
}$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $. Then,
\begin{equation}
\sum_{x=1}^{n}\sum_{y=1}^{m}a_{\left(  x,y\right)  }=\sum_{\left(  x,y\right)
\in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}
}a_{\left(  x,y\right)  }=\sum_{y=1}^{m}\sum_{x=1}^{n}a_{\left(  x,y\right)
}.\label{eq.sum.fubini.nm}%
\end{equation}
(This follows from (\ref{eq.sum.fubini}), applied to $X=\left\{
1,2,\ldots,n\right\}  $ and $Y=\left\{  1,2,\ldots,m\right\}  $.) We can
rewrite the equality (\ref{eq.sum.fubini.nm}) without using $\sum$ signs; it
then takes the following form:%
\begin{align*}
&  \left(  a_{\left(  1,1\right)  }+a_{\left(  1,2\right)  }+\cdots+a_{\left(
1,m\right)  }\right)  \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  2,1\right)  }+a_{\left(
2,2\right)  }+\cdots+a_{\left(  2,m\right)  }\right)  \\
&  \ \ \ \ \ \ \ \ \ \ +\cdots\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  n,1\right)  }+a_{\left(
n,2\right)  }+\cdots+a_{\left(  n,m\right)  }\right)  \\
&  =a_{\left(  1,1\right)  }+a_{\left(  1,2\right)  }+\cdots+a_{\left(
n,m\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{this is the sum of all
}nm\text{ numbers }a_{\left(  x,y\right)  }\right)  \\
&  =\left(  a_{\left(  1,1\right)  }+a_{\left(  2,1\right)  }+\cdots
+a_{\left(  n,1\right)  }\right)  \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  1,2\right)  }+a_{\left(
2,2\right)  }+\cdots+a_{\left(  n,2\right)  }\right)  \\
&  \ \ \ \ \ \ \ \ \ \ +\cdots\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  a_{\left(  1,m\right)  }+a_{\left(
2,m\right)  }+\cdots+a_{\left(  n,m\right)  }\right)  .
\end{align*}


\item Here is a concrete application of (\ref{eq.sum.fubini.nm}): Let
$n\in\mathbb{N}$ and $m\in\mathbb{N}$. We want to compute $\sum_{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  }xy$. (This is the sum of all entries of the $n\times m$
multiplication table.) Applying (\ref{eq.sum.fubini.nm}) to $a_{\left(
x,y\right)  }=xy$, we obtain%
\[
\sum_{x=1}^{n}\sum_{y=1}^{m}xy=\sum_{\left(  x,y\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  }xy=\sum_{y=1}%
^{m}\sum_{x=1}^{n}xy.
\]
Hence,%
\begin{align*}
&  \sum_{\left(  x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  }xy\\
&  =\sum_{x=1}^{n}\underbrace{\sum_{y=1}^{m}xy}_{\substack{=\sum_{s=1}%
^{m}xs=x\sum_{s=1}^{m}s\\\text{(by (\ref{eq.sum.linear2}), applied to
}S=\left\{  1,2,\ldots,m\right\}  \text{,}\\a_{s}=s\text{ and }\lambda
=x\text{)}}}=\sum_{x=1}^{n}x\underbrace{\sum_{s=1}^{m}s}_{\substack{=\sum
_{i=1}^{m}i=\dfrac{m\left(  m+1\right)  }{2}\\\text{(by
(\ref{eq.sum.littlegauss2}), applied to }m\\\text{instead of }n\text{)}}}\\
&  =\sum_{x=1}^{n}x\dfrac{m\left(  m+1\right)  }{2}=\sum_{x=1}^{n}%
\dfrac{m\left(  m+1\right)  }{2}x=\sum_{s=1}^{n}\dfrac{m\left(  m+1\right)
}{2}s\\
&  =\dfrac{m\left(  m+1\right)  }{2}\underbrace{\sum_{s=1}^{n}s}%
_{\substack{=\sum_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }{2}\\\text{(by
(\ref{eq.sum.littlegauss2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.linear2}), applied to
}S=\left\{  1,2,\ldots,n\right\}  \text{, }a_{s}=s\text{ and }\lambda
=\dfrac{m\left(  m+1\right)  }{2}\right)  \\
&  =\dfrac{m\left(  m+1\right)  }{2}\cdot\dfrac{n\left(  n+1\right)  }{2}.
\end{align*}

\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item I have promised to outline a proof of (\ref{eq.sum.fubini}). Here it
comes: Let $S=X\times Y$ and $W=Y$, and let $f:S\rightarrow W$ be the map
which sends every pair $\left(  x,y\right)  $ to its second entry $y$. Then,
(\ref{eq.sum.sheph.preimg}) shows that%
\begin{equation}
\sum_{s\in X\times Y}a_{s}=\sum_{w\in Y}\sum_{s\in f^{-1}\left(  w\right)
}a_{s}.\label{eq.sum.fubini.pf.1}%
\end{equation}
But for every given $w\in Y$, the set $f^{-1}\left(  w\right)  $ is simply the
set of all pairs $\left(  x,w\right)  $ with $x\in X$. Thus, for every given
$w\in Y$, there is a bijection $g_{w}:X\rightarrow f^{-1}\left(  w\right)  $
given by%
\[
g_{w}\left(  x\right)  =\left(  x,w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}x\in X.
\]
Hence, for every given $w\in Y$, we can substitute $g_{w}\left(  x\right)  $
for $s$ in the sum $\sum_{s\in f^{-1}\left(  w\right)  }a_{s}$, and thus
obtain%
\[
\sum_{s\in f^{-1}\left(  w\right)  }a_{s}=\sum_{x\in X}\underbrace{a_{g_{w}%
\left(  x\right)  }}_{\substack{=a_{\left(  x,w\right)  }\\\text{(since }%
g_{w}\left(  x\right)  =\left(  x,w\right)  \text{)}}}=\sum_{x\in X}a_{\left(
x,w\right)  }.
\]
Hence, (\ref{eq.sum.fubini.pf.1}) becomes%
\[
\sum_{s\in X\times Y}a_{s}=\sum_{w\in Y}\underbrace{\sum_{s\in f^{-1}\left(
w\right)  }a_{s}}_{=\sum_{x\in X}a_{\left(  x,w\right)  }}=\sum_{w\in Y}%
\sum_{x\in X}a_{\left(  x,w\right)  }=\sum_{y\in Y}\sum_{x\in X}a_{\left(
x,y\right)  }%
\]
(here, we have renamed the summation index $w$ as $y$ in the outer sum).
Therefore,%
\[
\sum_{y\in Y}\sum_{x\in X}a_{\left(  x,y\right)  }=\sum_{s\in X\times Y}%
a_{s}=\sum_{\left(  x,y\right)  \in X\times Y}a_{\left(  x,y\right)  }%
\]
(here, we have renamed the summation index $s$ as $\left(  x,y\right)  $).
Thus, we have proven the second part of the equality (\ref{eq.sum.fubini}).
The first part can be proven similarly.

\item I like to abbreviate the equality (\ref{eq.sum.fubini.nm}) as follows:%
\begin{equation}
\sum_{x=1}^{n}\sum_{y=1}^{m}=\sum_{\left(  x,y\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  }=\sum_{y=1}%
^{m}\sum_{x=1}^{n}.\label{eq.sum.fubini.eq-sums}%
\end{equation}
This is an \textquotedblleft equality between summation
signs\textquotedblright; it should be understood as follows: Every time you
see an \textquotedblleft$\sum_{x=1}^{n}\sum_{y=1}^{m}$\textquotedblright\ in
an expression, you can replace it by a \textquotedblleft$\sum_{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  }$\textquotedblright\ or by a \textquotedblleft$\sum_{y=1}^{m}%
\sum_{x=1}^{n}$\textquotedblright, and similarly the other ways round.
\end{itemize}

\item \underline{\textbf{Triangular Fubini's theorem I:}} The equality
(\ref{eq.sum.fubini.nm}) formalizes the idea that we can sum the entries of a
rectangular table by first tallying each row and then adding together, or
first tallying each column and adding together. The same holds for triangular
tables. More precisely: Let $n\in\mathbb{N}$. Let $T_{n}$ be the set $\left\{
\left(  x,y\right)  \in\left\{  1,2,3,\ldots\right\}  ^{2}\ \mid\ x+y\leq
n\right\}  $. (For instance, if $n=3$, then $T_{n}=T_{3}=\left\{  \left(
1,1\right)  ,\left(  1,2\right)  ,\left(  2,1\right)  \right\}  $.) Let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(
x,y\right)  \in T_{n}$. Then,%
\begin{equation}
\sum_{x=1}^{n}\sum_{y=1}^{n-x}a_{\left(  x,y\right)  }=\sum_{\left(
x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }=\sum_{y=1}^{n}\sum_{x=1}%
^{n-y}a_{\left(  x,y\right)  }.\label{eq.sum.fubini.triangle}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item In the case when $n=4$, the formula (\ref{eq.sum.fubini.triangle})
(rewritten without the use of $\sum$ signs) looks as follows:%
\begin{align*}
&  \left(  a_{\left(  1,1\right)  }+a_{\left(  1,2\right)  }+a_{\left(
1,3\right)  }\right)  +\left(  a_{\left(  2,1\right)  }+a_{\left(  2,2\right)
}\right)  +a_{\left(  3,1\right)  }\\
&  =\left(  \text{the sum of the }a_{\left(  x,y\right)  }\text{ for all
}\left(  x,y\right)  \in T_{4}\right)  \\
&  =\left(  a_{\left(  1,1\right)  }+a_{\left(  2,1\right)  }+a_{\left(
3,1\right)  }\right)  +\left(  a_{\left(  1,2\right)  }+a_{\left(  2,2\right)
}\right)  +a_{\left(  1,3\right)  }.
\end{align*}


\item Let us use (\ref{eq.sum.fubini.triangle}) to compute $\left\vert
T_{n}\right\vert $. Indeed, we can apply (\ref{eq.sum.fubini.triangle}) to
$a_{\left(  x,y\right)  }=1$. Thus, we obtain%
\[
\sum_{x=1}^{n}\sum_{y=1}^{n-x}1=\sum_{\left(  x,y\right)  \in T_{n}}%
1=\sum_{y=1}^{n}\sum_{x=1}^{n-y}1.
\]
Hence,%
\[
\sum_{x=1}^{n}\sum_{y=1}^{n-x}1=\sum_{\left(  x,y\right)  \in T_{n}%
}1=\left\vert T_{n}\right\vert ,
\]
so that%
\begin{align*}
\left\vert T_{n}\right\vert  &  =\sum_{x=1}^{n}\underbrace{\sum_{y=1}^{n-x}%
1}_{=n-x}=\sum_{x=1}^{n}\left(  n-x\right)  =\sum_{i=0}^{n-1}i\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }i\text{ for }n-x\text{ in the sum,}\\
\text{since the map }\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
0,1,\ldots,n-1\right\}  ,\ x\mapsto n-x\\
\text{is a bijection}%
\end{array}
\right)  \\
&  =\dfrac{\left(  n-1\right)  \left(  \left(  n-1\right)  +1\right)  }%
{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.littlegauss1}), applied
to }n-1\text{ instead of }n\right)  \\
&  =\dfrac{\left(  n-1\right)  n}{2}.
\end{align*}

\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{\left(  x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }$
in (\ref{eq.sum.fubini.triangle}) can also be rewritten as $\sum
_{\substack{\left(  x,y\right)  \in\left\{  1,2,3,\ldots\right\}
^{2};\\x+y\leq n}}a_{\left(  x,y\right)  }$.

\item Let us prove (\ref{eq.sum.fubini.triangle}). Indeed, the proof will be
very similar to our proof of (\ref{eq.sum.fubini}) above. Let $S=T_{n}$ and
$W=\left\{  1,2,\ldots,n\right\}  $, and let $f:S\rightarrow W$ be the map
which sends every pair $\left(  x,y\right)  $ to its second entry $y$. Then,
(\ref{eq.sum.sheph.preimg}) shows that%
\begin{equation}
\sum_{s\in T_{n}}a_{s}=\sum_{w\in W}\sum_{s\in f^{-1}\left(  w\right)  }%
a_{s}.\label{eq.sum.fubini.triangle.pf.1}%
\end{equation}
But for every given $w\in W$, the set $f^{-1}\left(  w\right)  $ is simply the
set of all pairs $\left(  x,w\right)  $ with $x\in\left\{  1,2,\ldots
,n-w\right\}  $. Thus, for every given $w\in W$, there is a bijection
$g_{w}:\left\{  1,2,\ldots,n-w\right\}  \rightarrow f^{-1}\left(  w\right)  $
given by%
\[
g_{w}\left(  x\right)  =\left(  x,w\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}x\in\left\{  1,2,\ldots,n-w\right\}  .
\]
Hence, for every given $w\in W$, we can substitute $g_{w}\left(  x\right)  $
for $s$ in the sum $\sum_{s\in f^{-1}\left(  w\right)  }a_{s}$, and thus
obtain%
\[
\sum_{s\in f^{-1}\left(  w\right)  }a_{s}=\underbrace{\sum_{x\in\left\{
1,2,\ldots,n-w\right\}  }}_{=\sum_{x=1}^{n-w}}\underbrace{a_{g_{w}\left(
x\right)  }}_{\substack{=a_{\left(  x,w\right)  }\\\text{(since }g_{w}\left(
x\right)  =\left(  x,w\right)  \text{)}}}=\sum_{x=1}^{n-w}a_{\left(
x,w\right)  }.
\]
Hence, (\ref{eq.sum.fubini.triangle.pf.1}) becomes%
\[
\sum_{s\in T_{n}}a_{s}=\underbrace{\sum_{w\in W}}_{\substack{=\sum_{w=1}%
^{n}\\\text{(since }W=\left\{  1,2,\ldots,n\right\}  \text{)}}%
}\underbrace{\sum_{s\in f^{-1}\left(  w\right)  }a_{s}}_{=\sum_{x=1}%
^{n-w}a_{\left(  x,w\right)  }}=\sum_{w=1}^{n}\sum_{x=1}^{n-w}a_{\left(
x,w\right)  }=\sum_{y=1}^{n}\sum_{x=1}^{n-y}a_{\left(  x,y\right)  }%
\]
(here, we have renamed the summation index $w$ as $y$ in the outer sum).
Therefore,%
\[
\sum_{y=1}^{n}\sum_{x=1}^{n-y}a_{\left(  x,y\right)  }=\sum_{s\in T_{n}}%
a_{s}=\sum_{\left(  x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }.
\]
Thus, we have proven the second part of the equality
(\ref{eq.sum.fubini.triangle}). The first part can be proven similarly.
\end{itemize}

\item \underline{\textbf{Triangular Fubini's theorem II:}} Here is another
equality similar to (\ref{eq.sum.fubini.triangle}). Let $n\in\mathbb{N}$. Let
$Q_{n}$ be the set $\left\{  \left(  x,y\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}\ \mid\ x\leq y\right\}  $. (For instance, if $n=3$, then
$Q_{n}=Q_{3}=\left\{  \left(  1,1\right)  ,\left(  1,2\right)  ,\left(
1,3\right)  ,\left(  2,2\right)  ,\left(  2,3\right)  ,\left(  3,3\right)
\right\}  $.) Let $a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$ for
each $\left(  x,y\right)  \in Q_{n}$. Then,%
\begin{equation}
\sum_{x=1}^{n}\sum_{y=x}^{n}a_{\left(  x,y\right)  }=\sum_{\left(  x,y\right)
\in Q_{n}}a_{\left(  x,y\right)  }=\sum_{y=1}^{n}\sum_{x=1}^{y}a_{\left(
x,y\right)  }. \label{eq.sum.fubini.triangle2}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item Let us use (\ref{eq.sum.fubini.triangle2}) to compute $\left\vert
Q_{n}\right\vert $. Indeed, we can apply (\ref{eq.sum.fubini.triangle2}) to
$a_{\left(  x,y\right)  }=1$. Thus, we obtain%
\[
\sum_{x=1}^{n}\sum_{y=x}^{n}1=\sum_{\left(  x,y\right)  \in Q_{n}}1=\sum
_{y=1}^{n}\sum_{x=1}^{y}1.
\]
Hence,%
\[
\sum_{y=1}^{n}\sum_{x=1}^{y}1=\sum_{\left(  x,y\right)  \in Q_{n}}1=\left\vert
Q_{n}\right\vert ,
\]
so that%
\[
\left\vert Q_{n}\right\vert =\sum_{y=1}^{n}\underbrace{\sum_{x=1}^{y}1}%
_{=y}=\sum_{y=1}^{n}y=\sum_{i=1}^{n}i=\dfrac{n\left(  n+1\right)  }%
{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.littlegauss2})}\right)
.
\]

\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The sum $\sum_{\left(  x,y\right)  \in Q_{n}}a_{\left(  x,y\right)  }$
in (\ref{eq.sum.fubini.triangle2}) can also be rewritten as $\sum
_{\substack{\left(  x,y\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\x\leq
y}}a_{\left(  x,y\right)  }$. It is also often written as $\sum_{1\leq x\leq
y\leq n}a_{\left(  x,y\right)  }$.

\item The proof of (\ref{eq.sum.fubini.triangle2}) is similar to that of
(\ref{eq.sum.fubini.triangle}).
\end{itemize}

\item \underline{\textbf{Fubini's theorem with a predicate:}} Let $X$ and $Y$
be two finite sets. For every pair $\left(  x,y\right)  \in X\times Y$, let
$\mathcal{A}\left(  x,y\right)  $ be a logical statement. For each $\left(
x,y\right)  \in X\times Y$ satisfying $\mathcal{A}\left(  x,y\right)  $, let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$. Then,%
\begin{equation}
\sum_{x\in X}\sum_{\substack{y\in Y;\\\mathcal{A}\left(  x,y\right)
}}a_{\left(  x,y\right)  }=\sum_{\substack{\left(  x,y\right)  \in X\times
Y;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(  x,y\right)  }=\sum_{y\in
Y}\sum_{\substack{x\in X;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(
x,y\right)  }. \label{eq.sum.fubini.predicate}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item For any $n\in\mathbb{N}$ and $m\in\mathbb{N}$, we have%
\begin{align*}
\sum_{x\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{y\in\left\{
1,2,\ldots,m\right\}  ;\\x+y\text{ is even}}}xy  &  =\sum_{\substack{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  ;\\x+y\text{ is even}}}xy\\
&  =\sum_{y\in\left\{  1,2,\ldots,m\right\}  }\sum_{\substack{x\in\left\{
1,2,\ldots,n\right\}  ;\\x+y\text{ is even}}}xy.
\end{align*}
(This follows from (\ref{eq.sum.fubini.predicate}), applied to $X=\left\{
1,2,\ldots,n\right\}  $, $Y=\left\{  1,2,\ldots,m\right\}  $ and
$\mathcal{A}\left(  x,y\right)  =\left(  \text{\textquotedblleft}x+y\text{ is
even\textquotedblright}\right)  $.)
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item We have assumed that the sets $X$ and $Y$ are finite. But
(\ref{eq.sum.fubini.predicate}) is still valid if we replace this assumption
by the weaker assumption that only finitely many $\left(  x,y\right)  \in
X\times Y$ satisfy $\mathcal{A}\left(  x,y\right)  $.

\item It is not hard to prove (\ref{eq.sum.fubini.predicate}) by suitably
adapting our proof of (\ref{eq.sum.fubini}).

\item The equality (\ref{eq.sum.fubini.triangle}) can be derived from
(\ref{eq.sum.fubini.predicate}) by setting $X=\left\{  1,2,\ldots,n\right\}
$, $Y=\left\{  1,2,\ldots,n\right\}  $ and $\mathcal{A}\left(  x,y\right)
=\left(  \text{\textquotedblleft}x+y\leq n\text{\textquotedblright}\right)  $.
Similarly, the equality (\ref{eq.sum.fubini.triangle}) can be derived from
(\ref{eq.sum.fubini.predicate}) by setting $X=\left\{  1,2,\ldots,n\right\}
$, $Y=\left\{  1,2,\ldots,n\right\}  $ and $\mathcal{A}\left(  x,y\right)
=\left(  \text{\textquotedblleft}x\leq y\text{\textquotedblright}\right)  $.
\end{itemize}

\item \underline{\textbf{Interchange of predicates:}} Let $S$ be a finite set.
For every $s\in S$, let $\mathcal{A}\left(  s\right)  $ and $\mathcal{B}%
\left(  s\right)  $ be two equivalent logical statements. (\textquotedblleft
Equivalent\textquotedblright\ means that $\mathcal{A}\left(  s\right)  $ holds
if and only if $\mathcal{B}\left(  s\right)  $ holds.) Let $a_{s}$ be an
element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\sum_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\sum
_{\substack{s\in S;\\\mathcal{B}\left(  s\right)  }}a_{s}.
\]
(If you regard equivalent logical statements as identical, then you will see
this as a tautology. If not, it is still completely obvious, since the
equivalence of $\mathcal{A}\left(  s\right)  $ with $\mathcal{B}\left(
s\right)  $ shows that $\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)
\right\}  =\left\{  t\in S\ \mid\ \mathcal{B}\left(  t\right)  \right\}  $.)

\item \underline{\textbf{Substituting the index I with a predicate:}} Let $S$
and $T$ be two finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective}
map. Let $a_{t}$ be an element of $\mathbb{A}$ for each $t\in T$. For every
$t\in T$, let $\mathcal{A}\left(  t\right)  $ be a logical statement. Then,%
\begin{equation}
\sum_{\substack{t\in T;\\\mathcal{A}\left(  t\right)  }}a_{t}=\sum
_{\substack{s\in S;\\\mathcal{A}\left(  f\left(  s\right)  \right)
}}a_{f\left(  s\right)  }. \label{eq.sum.subs1-pred}%
\end{equation}


\textbf{Remarks:}

\begin{itemize}
\item The equality (\ref{eq.sum.subs1-pred}) is a generalization of
(\ref{eq.sum.subs1}). There is a similar generalization of (\ref{eq.sum.subs2}).

\item The equality (\ref{eq.sum.subs1-pred}) can be easily derived from
(\ref{eq.sum.subs1}). Indeed, let $S^{\prime}$ be the subset $\left\{  s\in
S\ \mid\ \mathcal{A}\left(  f\left(  s\right)  \right)  \right\}  $ of $S$,
and let $T^{\prime}$ be the subset $\left\{  t\in T\ \mid\ \mathcal{A}\left(
t\right)  \right\}  $ of $T$. Then, the map $S^{\prime}\rightarrow T^{\prime
},\ s\mapsto f\left(  s\right)  $ is well-defined and a
bijection\footnote{This is easy to see.}, and thus (\ref{eq.sum.subs1})
(applied to $S^{\prime}$, $T^{\prime}$ and this map instead of $S$, $T$ and
$f$) yields $\sum_{t\in T^{\prime}}a_{t}=\sum_{s\in S^{\prime}}a_{f\left(
s\right)  }$. But this is precisely the equality (\ref{eq.sum.subs1-pred}),
because clearly we have $\sum_{t\in T^{\prime}}=\sum_{\substack{t\in
T;\\\mathcal{A}\left(  t\right)  }}$ and $\sum_{s\in S^{\prime}}%
=\sum_{\substack{s\in S;\\\mathcal{A}\left(  f\left(  s\right)  \right)  }}$.
\end{itemize}
\end{itemize}

\subsubsection{Definition of $\prod$}

We shall now define the $\prod$ sign. Since the $\prod$ sign is (in many
aspects) analogous to the $\sum$ sign, we shall be brief and confine ourselves
to the bare necessities; we trust the reader to transfer most of what we said
about $\sum$ to the case of $\prod$. In particular, we shall give very few
examples and no proofs.

\begin{itemize}
\item If $S$ is a finite set, and if $a_{s}$ is an element of $\mathbb{A}$ for
each $s\in S$, then $\prod\nolimits_{s\in S}a_{s}$ denotes the product of all
of these elements $a_{s}$. Formally, this product is defined by recursion on
$\left\vert S\right\vert $, as follows:

\begin{itemize}
\item If $S=\varnothing$, then $\prod_{s\in S}a_{s}$ is defined to be $1$.

\item Let $n\in\mathbb{N}$. Assume that we have defined $\prod_{s\in S}a_{s}$
for every finite set $S$ with $\left\vert S\right\vert =n$ (and every choice
of elements $a_{s}$ of $\mathbb{A}$). Now, if $S$ is a finite set with
$\left\vert S\right\vert =n+1$ (and if $a_{s}\in\mathbb{A}$ are chosen for all
$s\in S$), then $\prod_{s\in S}a_{s}$ is defined by picking any $t\in S$ and
setting%
\begin{equation}
\prod_{s\in S}a_{s}=a_{t}\cdot\prod_{s\in S\setminus\left\{  t\right\}  }%
a_{s}. \label{eq.prod.def.1}%
\end{equation}
As for $\sum_{s\in S}a_{s}$, this definition is not obviously legitimate, but
it can be proven to be legitimate nevertheless.
\end{itemize}

\textbf{Examples:}

\begin{itemize}
\item If $S=\left\{  1,2,\ldots,n\right\}  $ (for some $n\in\mathbb{N}$) and
$a_{s}=s$ for every $s\in S$, then $\prod_{s\in S}a_{s}=\prod_{s\in S}%
s=1\cdot2\cdot\cdots\cdot n$. This number $1\cdot2\cdot\cdots\cdot n$ is
denoted by $n!$ and called the \textit{factorial of }$n$.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The product $\prod_{s\in S}a_{s}$ is usually pronounced
\textquotedblleft product of the $a_{s}$ over all $s\in S$\textquotedblright%
\ or \textquotedblleft product of the $a_{s}$ with $s$ ranging over
$S$\textquotedblright\ or \textquotedblleft product of the $a_{s}$ with $s$
running through all elements of $S$\textquotedblright. The letter
\textquotedblleft$s$\textquotedblright\ in the product is called the
\textquotedblleft product index\textquotedblright, and its exact choice is
immaterial, as long as it does not already have a different meaning outside of
the product. The sign $\prod$ itself is called \textquotedblleft the product
sign\textquotedblright\ or \textquotedblleft the $\prod$
sign\textquotedblright. The numbers $a_{s}$ are called the \textit{factors} of
the product $\prod_{s\in S}a_{s}$. More precisely, for any given $t\in S$, we
can refer to the number $a_{t}$ as the \textquotedblleft factor corresponding
to the index $t$\textquotedblright\ (or as the \textquotedblleft factor for
$s=t$\textquotedblright, or as the \textquotedblleft factor for $t$%
\textquotedblright) of the product $\prod_{s\in S}a_{s}$.

\item When the set $S$ is empty, the product $\prod_{s\in S}a_{s}$ is called
an \textit{empty product}. Our definition implies that any empty product is
$1$. This convention is used throughout mathematics, except in rare occasions
where a slightly subtler version of it is used\footnote{Just as with sums, the
subtlety lies in the fact that mathematicians sometimes want an empty product
to be not the integer $1$ but the unity of some ring. As before, this does not
matter for us right now.}.

\item If $a\in\mathbb{A}$ and $n\in\mathbb{N}$, then the $n$-th power of $a$
(written $a^{n}$) is defined by%
\[
a^{n}=\underbrace{a\cdot a\cdot\cdots\cdot a}_{n\text{ times}}=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }a.
\]
Thus, $a^{0}$ is an empty product, and therefore equal to $1$. This holds for
any $a\in\mathbb{A}$, including $0$; thus, $0^{0}=1$. \textbf{There is nothing
controversial about the equality }$0^{0}=1$; it is a consequence of the only
reasonable definition of the $n$-th power of a number. If anyone tells you
that $0^{0}$ is \textquotedblleft undefined\textquotedblright\ or
\textquotedblleft indeterminate\textquotedblright\ or \textquotedblleft can be
$0$ or $1$ or anything, depending on the context\textquotedblright, do not
listen to them.\footnote{I am talking about the \textbf{number} $0^{0}$ here.
There is also something called \textquotedblleft the
\href{https://en.wikipedia.org/wiki/Indeterminate form}{indeterminate form}
$0^{0}$\textquotedblright, which is a much different story.}

\item The product index (just like a summation index) needs not be a single
letter; it can be a pair or a triple, for example.

\item Mathematicians don't seem to have reached an agreement on the operator
precedence of the $\prod$ sign. My convention is that the product sign has
higher precedence than the plus sign (so an expression like $\prod_{s\in
S}a_{s}+b$ must be read as $\left(  \prod_{s\in S}a_{s}\right)  +b$, and not
as $\prod_{s\in S}\left(  a_{s}+b\right)  $); this is, of course, in line with
the standard convention that multiplication-like operations have higher
precedence than addition-like operations (\textquotedblleft
PEMDAS\textquotedblright). Be warned that some authors disagree even with this
convention. I strongly advise against writing things like $\prod_{s\in S}%
a_{s}b$, since it might mean both $\left(  \prod_{s\in S}a_{s}\right)  b$ and
$\prod_{s\in S}\left(  a_{s}b\right)  $ depending on the weather. In
particular, I advise against writing things like $\prod_{s\in S}a_{s}%
\cdot\prod_{s\in S}b_{s}$ without parentheses (although I do use a similar
convention for sums, namely $\sum_{s\in S}a_{s}+\sum_{s\in S}b_{s}$, and I
find it to be fairly harmless). These rules are not carved in stone, and you
should use whatever conventions make \textbf{you} safe from ambiguity; either
way, you should keep in mind that other authors make different choices.

\item We have required the set $S$ to be finite when defining $\prod_{s\in
S}a_{s}$. Such products are not generally defined when $S$ is infinite.
However, \textbf{some} infinite products can be made sense of. The simplest
case is when the set $S$ might be infinite, but only finitely many among the
$a_{s}$ are distinct from $1$. In this case, we can define $\prod_{s\in
S}a_{s}$ simply by discarding the factors which equal $1$ and multiplying the
finitely many remaining factors. Other situations in which infinite products
make sense appear in analysis and in topological algebra.

\item The product $\prod_{s\in S}a_{s}$ always belongs to $\mathbb{A}$.
\end{itemize}

\item A slightly more complicated version of the product sign is the
following: Let $S$ be a finite set, and let $\mathcal{A}\left(  s\right)  $ be
a logical statement defined for every $s\in S$. For each $s\in S$ satisfying
$\mathcal{A}\left(  s\right)  $, let $a_{s}$ be an element of $\mathbb{A}$.
Then, the product $\prod_{\substack{s\in S;\\\mathcal{A}\left(  s\right)
}}a_{s}$ is defined by%
\[
\prod_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\prod
_{s\in\left\{  t\in S\ \mid\ \mathcal{A}\left(  t\right)  \right\}  }a_{s}.
\]


\item Finally, here is the simplest version of the product sign: Let $u$ and
$v$ be two integers. As before, we understand the set $\left\{  u,u+1,\ldots
,v\right\}  $ to be empty when $u>v$. Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in\left\{  u,u+1,\ldots,v\right\}  $. Then,
$\prod_{s=u}^{v}a_{s}$ is defined by%
\[
\prod_{s=u}^{v}a_{s}=\prod_{s\in\left\{  u,u+1,\ldots,v\right\}  }a_{s}.
\]


\textbf{Examples:}

\begin{itemize}
\item We have $\prod_{s=1}^{n}s=1\cdot2\cdot\cdots\cdot n=n!$ for each
$n\in\mathbb{N}$.
\end{itemize}

\textbf{Remarks:}

\begin{itemize}
\item The product $\prod_{s=u}^{v}a_{s}$ is usually pronounced
\textquotedblleft product of the $a_{s}$ for all $s$ from $u$ to $v$
(inclusive)\textquotedblright. It is often written $a_{u}\cdot a_{u+1}%
\cdot\cdots\cdot a_{v}$ (or just $a_{u}a_{u+1}\cdots a_{v}$), but this latter
notation has the same drawbacks as the similar notation $a_{u}+a_{u+1}%
+\cdots+a_{v}$ for $\sum_{s=u}^{v}a_{s}$.

\item The product $\prod_{s=u}^{v}a_{s}$ is said to be \textit{empty} whenever
$u>v$. As with sums, it does not matter how much smaller $v$ is than $u$; as
long as $v$ is smaller than $u$, the product is empty and equals $1$.
\end{itemize}
\end{itemize}

Thus we have introduced the main three forms of the product sign.

\subsubsection{Properties of $\prod$}

Now, let me summarize the most important properties of the $\prod$ sign. These
properties mirror the properties of $\sum$ discussed before; thus, I will
again be brief.

\begin{itemize}
\item \underline{\textbf{Splitting-off:}} Let $S$ be a finite set. Let $t\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=a_{t}\cdot\prod_{s\in S\setminus\left\{  t\right\}  }%
a_{s}.
\]


\item \underline{\textbf{Splitting:}} Let $S$ be a finite set. Let $X$ and $Y
$ be two subsets of $S$ such that $X\cap Y=\varnothing$ and $X\cup Y=S$.
(Equivalently, $X$ and $Y$ are two subsets of $S$ such that each element of
$S$ lies in \textbf{exactly} one of $X$ and $Y$.) Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\left(  \prod_{s\in X}a_{s}\right)  \cdot\left(
\prod_{s\in Y}a_{s}\right)  .
\]


\item \underline{\textbf{Splitting using a predicate:}} Let $S$ be a finite
set. Let $\mathcal{A}\left(  s\right)  $ be a logical statement for each $s\in
S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\left(  \prod_{\substack{s\in S; \\\mathcal{A}\left(
s\right)  }}a_{s}\right)  \cdot\left(  \prod_{\substack{s\in S; \\\text{not
}\mathcal{A}\left(  s\right)  }}a_{s}\right)  .
\]


\item \underline{\textbf{Multiplying equal values:}} Let $S$ be a finite set.
Let $a$ be an element of $\mathbb{A}$. Then,%
\[
\prod_{s\in S}a=a^{\left\vert S\right\vert }.
\]


\item \underline{\textbf{Splitting a factor:}} Let $S$ be a finite set. For
every $s\in S$, let $a_{s}$ and $b_{s}$ be elements of $\mathbb{A}$. Then,%
\begin{equation}
\prod_{s\in S}\left(  a_{s}b_{s}\right)  =\left(  \prod_{s\in S}a_{s}\right)
\cdot\left(  \prod_{s\in S}b_{s}\right)  . \label{eq.prod.linear1}%
\end{equation}


\textbf{Examples:}

\begin{itemize}
\item Here is a frequently used particular case of (\ref{eq.prod.linear1}):
Let $S$ be a finite set. For every $s\in S$, let $b_{s}$ be an element of
$\mathbb{A}$. Let $a$ be an element of $\mathbb{A}$. Then,
(\ref{eq.prod.linear1}) (applied to $a_{s}=a$) yields%
\begin{equation}
\prod_{s\in S}\left(  ab_{s}\right)  =\underbrace{\left(  \prod_{s\in
S}a\right)  }_{=a^{\left\vert S\right\vert }}\cdot\left(  \prod_{s\in S}%
b_{s}\right)  =a^{\left\vert S\right\vert }\cdot\left(  \prod_{s\in S}%
b_{s}\right)  . \label{eq.prod.linear1.ex1}%
\end{equation}


\item Here is an even further particular case: Let $S$ be a finite set. For
every $s\in S$, let $b_{s}$ be an element of $\mathbb{A}$. Then,%
\[
\prod_{s\in S}\underbrace{\left(  -b_{s}\right)  }_{=\left(  -1\right)  b_{s}%
}=\prod_{s\in S}\left(  \left(  -1\right)  b_{s}\right)  =\left(  -1\right)
^{\left\vert S\right\vert }\cdot\left(  \prod_{s\in S}b_{s}\right)
\]
(by (\ref{eq.prod.linear1.ex1}), applied to $a=-1$).
\end{itemize}

\item \underline{\textbf{Factoring out an exponent:}} Let $S$ be a finite set.
For every $s\in S$, let $a_{s}$ be an element of $\mathbb{A}$. Also, let
$\lambda\in\mathbb{N}$. Then,%
\[
\prod_{s\in S}a_{s}^{\lambda}=\left(  \prod_{s\in S}a_{s}\right)  ^{\lambda}.
\]


\item \underline{\textbf{Ones multiply to one:}} Let $S$ be a finite set.
Then,%
\[
\prod_{s\in S}1=1.
\]


\item \underline{\textbf{Dropping ones:}} Let $S$ be a finite set. Let $a_{s}$
be an element of $\mathbb{A}$ for each $s\in S$. Let $T$ be a subset of $S$
such that every $s\in T$ satisfies $a_{s}=1$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{s\in S\setminus T}a_{s}.
\]


\item \underline{\textbf{Renaming the index:}} Let $S$ be a finite set. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{t\in S}a_{t}.
\]


\item \underline{\textbf{Substituting the index I:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{t}$
be an element of $\mathbb{A}$ for each $t\in T$. Then,%
\[
\prod_{t\in T}a_{t}=\prod_{s\in S}a_{f\left(  s\right)  }.
\]


\item \underline{\textbf{Substituting the index II:}} Let $S$ and $T$ be two
finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective} map. Let $a_{s}$
be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{t\in T}a_{f^{-1}\left(  t\right)  }.
\]


\item \underline{\textbf{Telescoping products:}} Let $u$ and $v$ be two
integers such that $u-1\leq v$. Let $a_{s}$ be an element of $\mathbb{A}$ for
each $s\in\left\{  u-1,u,\ldots,v\right\}  $. Then,%
\begin{equation}
\prod_{s=u}^{v}\dfrac{a_{s}}{a_{s-1}}=\dfrac{a_{v}}{a_{u-1}}
\label{eq.prod.telescope}%
\end{equation}
(provided that $a_{s-1}\neq0$ for all $s\in\left\{  u,u+1,\ldots,v\right\}  $).

\textbf{Examples:}

\begin{itemize}
\item Let $n$ be a positive integer. Then,%
\begin{align*}
\prod_{s=2}^{n}\underbrace{\left(  1-\dfrac{1}{s}\right)  }_{=\dfrac{s-1}%
{s}=\dfrac{1/s}{1/\left(  s-1\right)  }} &  =\prod_{s=2}^{n}\dfrac
{1/s}{1/\left(  s-1\right)  }=\dfrac{1/n}{1/\left(  2-1\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.prod.telescope}), applied to
}u=2\text{, }v=n\text{ and }a_{s}=1/s\right)  \\
&  =\dfrac{1}{n}.
\end{align*}

\end{itemize}

\item \underline{\textbf{Restricting to a subset:}} Let $S$ be a finite set.
Let $T$ be a subset of $S$. Let $a_{s}$ be an element of $\mathbb{A}$ for each
$s\in T$. Then,%
\[
\prod_{\substack{s\in S;\\s\in T}}a_{s}=\prod_{s\in T}a_{s}.
\]
\textbf{Remark:} Here is a slightly more general form of this rule: Let $S$ be
a finite set. Let $T$ be a subset of $S$. Let $\mathcal{A}\left(  s\right)  $
be a logical statement for each $s\in S$. Let $a_{s}$ be an element of
$\mathbb{A}$ for each $s\in T$ satisfying $\mathcal{A}\left(  s\right)  $.
Then,%
\[
\prod_{\substack{s\in S;\\s\in T;\\\mathcal{A}\left(  s\right)  }}a_{s}%
=\prod_{\substack{s\in T;\\\mathcal{A}\left(  s\right)  }}a_{s}.
\]


\item \underline{\textbf{Splitting a product by a value of a function:}} Let
$S$ be a finite set. Let $W$ be a set. Let $f:S\rightarrow W$ be a map. Let
$a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{s\in S}a_{s}=\prod_{w\in W}\prod_{\substack{s\in S;\\f\left(  s\right)
=w}}a_{s}.
\]
(The right hand side is to be read as $\prod_{w\in W}\left(  \prod
_{\substack{s\in S;\\f\left(  s\right)  =w}}a_{s}\right)  $.)

\item \underline{\textbf{Splitting a product into subproducts:}} Let $S$ be a
finite set. Let $S_{1},S_{2},\ldots,S_{n}$ be finitely many subsets of $S$.
Assume that these subsets $S_{1},S_{2},\ldots,S_{n}$ are pairwise disjoint
(i.e., we have $S_{i}\cap S_{j}=\varnothing$ for any two distinct elements $i$
and $j$ of $\left\{  1,2,\ldots,n\right\}  $) and their union is $S$. (Thus,
every element of $S$ lies in precisely one of the subsets $S_{1},S_{2}%
,\ldots,S_{n}$.) Let $a_{s}$ be an element of $\mathbb{A}$ for each $s\in S$.
Then,%
\[
\prod_{s\in S}a_{s}=\prod_{w=1}^{n}\prod_{s\in S_{w}}a_{s}.
\]


\item \underline{\textbf{Fubini's theorem (interchanging the order of
multiplication):}} Let $X$ and $Y$ be two finite sets. Let $a_{\left(
x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in
X\times Y$. Then,%
\[
\prod_{x\in X}\prod_{y\in Y}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in X\times Y}a_{\left(  x,y\right)  }=\prod_{y\in Y}\prod_{x\in
X}a_{\left(  x,y\right)  }.
\]


In particular, if $n$ and $m$ are two elements of $\mathbb{N}$, and if
$a_{\left(  x,y\right)  }$ is an element of $\mathbb{A}$ for each $\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  $, then%
\[
\prod_{x=1}^{n}\prod_{y=1}^{m}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots
,m\right\}  }a_{\left(  x,y\right)  }=\prod_{y=1}^{m}\prod_{x=1}^{n}a_{\left(
x,y\right)  }.
\]


\item \underline{\textbf{Triangular Fubini's theorem I:}} Let $n\in\mathbb{N}%
$. Let $T_{n}$ be the set \newline$\left\{  \left(  x,y\right)  \in\left\{
1,2,3,\ldots\right\}  ^{2}\ \mid\ x+y\leq n\right\}  $. Let $a_{\left(
x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(  x,y\right)  \in
T_{n}$. Then,%
\[
\prod_{x=1}^{n}\prod_{y=1}^{n-x}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in T_{n}}a_{\left(  x,y\right)  }=\prod_{y=1}^{n}\prod
_{x=1}^{n-y}a_{\left(  x,y\right)  }.
\]


\item \underline{\textbf{Triangular Fubini's theorem II:}} Let $n\in
\mathbb{N}$. Let $Q_{n}$ be the set \newline$\left\{  \left(  x,y\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}\ \mid\ x\leq y\right\}  $. Let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$ for each $\left(
x,y\right)  \in Q_{n}$. Then,%
\[
\prod_{x=1}^{n}\prod_{y=x}^{n}a_{\left(  x,y\right)  }=\prod_{\left(
x,y\right)  \in Q_{n}}a_{\left(  x,y\right)  }=\prod_{y=1}^{n}\prod_{x=1}%
^{y}a_{\left(  x,y\right)  }.
\]


\item \underline{\textbf{Fubini's theorem with a predicate:}} Let $X$ and $Y$
be two finite sets. For every pair $\left(  x,y\right)  \in X\times Y$, let
$\mathcal{A}\left(  x,y\right)  $ be a logical statement. For each $\left(
x,y\right)  \in X\times Y$ satisfying $\mathcal{A}\left(  x,y\right)  $, let
$a_{\left(  x,y\right)  }$ be an element of $\mathbb{A}$. Then,%
\[
\prod_{x\in X}\prod_{\substack{y\in Y;\\\mathcal{A}\left(  x,y\right)
}}a_{\left(  x,y\right)  }=\prod_{\substack{\left(  x,y\right)  \in X\times
Y;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(  x,y\right)  }=\prod_{y\in
Y}\prod_{\substack{x\in X;\\\mathcal{A}\left(  x,y\right)  }}a_{\left(
x,y\right)  }.
\]


\item \underline{\textbf{Interchange of predicates:}} Let $S$ be a finite set.
For every $s\in S$, let $\mathcal{A}\left(  s\right)  $ and $\mathcal{B}%
\left(  s\right)  $ be two equivalent logical statements. (\textquotedblleft
Equivalent\textquotedblright\ means that $\mathcal{A}\left(  s\right)  $ holds
if and only if $\mathcal{B}\left(  s\right)  $ holds.) Let $a_{s}$ be an
element of $\mathbb{A}$ for each $s\in S$. Then,%
\[
\prod_{\substack{s\in S;\\\mathcal{A}\left(  s\right)  }}a_{s}=\prod
_{\substack{s\in S;\\\mathcal{B}\left(  s\right)  }}a_{s}.
\]


\item \underline{\textbf{Substituting the index I with a predicate:}} Let $S$
and $T$ be two finite sets. Let $f:S\rightarrow T$ be a \textbf{bijective}
map. Let $a_{t}$ be an element of $\mathbb{A}$ for each $t\in T$. For every
$t\in T$, let $\mathcal{A}\left(  t\right)  $ be a logical statement. Then,%
\[
\prod_{\substack{t\in T;\\\mathcal{A}\left(  t\right)  }}a_{t}=\prod
_{\substack{s\in S;\\\mathcal{A}\left(  f\left(  s\right)  \right)
}}a_{f\left(  s\right)  }.
\]

\end{itemize}

\subsection{\label{sect.polynomials-emergency}Polynomials: a precise
definition}

As I have already mentioned in the above list of prerequisites, the notion of
polynomials (in one and in several indeterminates) will be used in these
notes. Most likely, the reader already has at least a vague understanding of
this notion (e.g., from high school); this vague understanding is probably
sufficient for reading most of these notes. But polynomials are one of the
most important notions in algebra (if not to say in mathematics), and the
reader will likely encounter them over and over; sooner or later, it will
happen that the vague understanding is not sufficient and some subtleties do
matter. For that reason, anyone serious about doing abstract algebra should
know a complete and correct definition of polynomials and have some experience
working with it. I shall not give a complete definition of the most general
notion of polynomials in these notes, but I will comment on some of the
subtleties and define an important special case (that of polynomials in one
variable with rational coefficients) in the present section. A reader is
probably best advised to skip this section on their first read.

It is not easy to find a good (formal and sufficiently general) treatment of
polynomials in textbooks. Various authors tend to skimp on subtleties and
technical points such as the notion of an \textquotedblleft
indeterminate\textquotedblright, or the precise meaning of \textquotedblleft
formal expression\textquotedblright\ in the slogan \textquotedblleft a
polynomial is a formal expression\textquotedblright\ (the best texts do not
use this vague slogan at all), or the definition of the degree of the zero
polynomial, or the difference between regarding polynomials as sequences
(which is the classical viewpoint and particularly useful for polynomials in
one variable) and regarding polynomials as elements of a monoid ring (which is
important in the case of several variables, since it allows us to regard the
polynomial rings $\mathbb{Q}\left[  X\right]  $ and $\mathbb{Q}\left[
Y\right]  $ as two distinct subrings of $\mathbb{Q}\left[  X,Y\right]  $).
They also tend to take some questionable shortcuts, such as defining
polynomials in $n$ variables (by induction over $n$) as one-variable
polynomials over the ring of $\left(  n-1\right)  $-variable polynomials (this
shortcut has several shortcomings, such as making the symmetric role of the
$n$ variables opaque, and functioning only for finitely many variables).

More often than not, the polynomials we will be using will be polynomials in
one variable. These are usually handled well in good books on abstract algebra
-- e.g., in \cite[\S 4.5]{Walker87}, in \cite[Appendix G]{Hungerford}, in
\cite[Chapter III, \S 5]{Hungerford-03}, in \cite[Chapter A-3]{Rotman15}, in
\cite[\S 4.1, \S 4.2]{HoffmanKunze} (although in \cite[\S 4.1, \S 4.2]%
{HoffmanKunze}, only polynomials over fields are studied, but the definition
applies to commutative rings mutatis mutandis), in \cite[\S 8]{AmaEsc05}, and
in \cite[Chapter III, \S 6]{BirkMac}. Most of these treatments rely on the
notion of a \textit{commutative ring}, which is not difficult but somewhat
abstract (I shall introduce it below in Section \ref{sect.commring}).

Let me give a brief survey of the notion of univariate polynomials (i.e.,
polynomials in one variable). I shall define them as sequences. For the sake
of simplicity, I shall only talk of polynomials with rational coefficients.
Similarly, one can define polynomials with integer coefficients, with real
coefficients, or with complex coefficients; of course, one then has to replace
each \textquotedblleft$\mathbb{Q}$\textquotedblright\ by a \textquotedblleft%
$\mathbb{Z}$\textquotedblright, an \textquotedblleft$\mathbb{R}$%
\textquotedblright\ or a \textquotedblleft$\mathbb{C}$\textquotedblright.

The rough idea behind the definition of a polynomial is that a polynomial with
rational coefficients should be a \textquotedblleft formal
expression\textquotedblright\ which is built out of rational numbers, an
\textquotedblleft indeterminate\textquotedblright\ $X$ as well as addition,
subtraction and multiplication signs, such as $X^{4}-27X+\dfrac{3}{2}$ or
$-X^{3}+2X+1$ or $\dfrac{1}{3}\left(  X-3\right)  \cdot X^{2}$ or
$X^{4}+7X^{3}\left(  X-2\right)  $ or $-15$. We have not explicitly allowed
powers, but we understand $X^{n}$ to mean the product $\underbrace{XX\cdots
X}_{n\text{ times}}$ (or $1$ when $n=0$). Notice that division is not allowed,
so we cannot get $\dfrac{X}{X+1}$ (but we can get $\dfrac{3}{2}X$, because
$\dfrac{3}{2}$ is a rational number). Notice also that a polynomial can be a
single rational number, since we never said that $X$ must necessarily be used;
for instance, $-15$ and $0$ are polynomials.

This is, of course, not a valid definition. One problem with it that it does
not explain what a \textquotedblleft formal expression\textquotedblright\ is.
For starters, we want an expression that is well-defined -- i.e., into that we
can substitute a rational number for $X$ and obtain a valid term. For example,
$X-+\cdot5$ is not well-defined, so it does not fit our bill; neither is the
\textquotedblleft empty expression\textquotedblright. Furthermore, when do we
want two \textquotedblleft formal expressions\textquotedblright\ to be viewed
as one and the same polynomial? Do we want to equate $X\left(  X+2\right)  $
with $X^{2}+2X$ ? Do we want to equate $0X^{3}+2X+1$ with $2X+1$ ? The answer
is \textquotedblleft yes\textquotedblright\ both times, but a general rule is
not easy to give if we keep talking of \textquotedblleft formal
expressions\textquotedblright.

We \textit{could} define two polynomials $p\left(  X\right)  $ and $q\left(
X\right)  $ to be equal if and only if, for every number $\alpha\in\mathbb{Q}%
$, the values $p\left(  \alpha\right)  $ and $q\left(  \alpha\right)  $
(obtained by substituting $\alpha$ for $X$ in $p$ and in $q$, respectively)
are equal. This would be tantamount to treating polynomials as
\textit{functions}: it would mean that we identify a polynomial $p\left(
X\right)  $ with the function $\mathbb{Q}\rightarrow\mathbb{Q},\ \alpha\mapsto
p\left(  \alpha\right)  $. Such a definition would work well as long as we
would do only rather basic things with it\footnote{And some authors, such as
Axler in \cite[Chapter 4]{Axler}, do use this definition.}, but as soon as we
would try to go deeper, we would encounter technical issues which would make
it inadequate and painful\footnote{Here are the three most important among
these issues:
\par
\begin{itemize}
\item One of the strengths of polynomials is that we can evaluate them not
only at numbers, but also at many other things, e.g., at square matrices:
Evaluating the polynomial $X^{2}-3X$ at the square matrix $\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  $ gives $\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  ^{2}-3\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
-5 & 0\\
0 & -5
\end{array}
\right)  $. However, a function must have a well-defined domain, and does not
make sense outside of this domain. So, if the polynomial $X^{2}-3X$ is
regarded as the function $\mathbb{Q}\rightarrow\mathbb{Q},\ \alpha
\mapsto\alpha^{2}-3\alpha$, then it makes no sense to evaluate this polynomial
at the matrix $\left(
\begin{array}
[c]{cc}%
1 & 3\\
-1 & 2
\end{array}
\right)  $, just because this matrix does not lie in the domain $\mathbb{Q}$
of the function. We could, of course, extend the domain of the function to
(say) the set of square matrices over $\mathbb{Q}$, but then we would still
have the same problem with other things that we want to evaluate polynomials
at. At some point we want to be able to evaluate polynomials at functions and
at other polynomials, and if we would try to achieve this by extending the
domain, we would have to do this over and over, because each time we extend
the domain, we get even more polynomials to evaluate our polynomials at; thus,
the definition would be eternally \textquotedblleft hunting its own
tail\textquotedblright! (We could resolve this difficulty by defining
polynomials as \textit{natural transformations} in the sense of category
theory. I do not want to even go into this definition here, as it would take
several pages to properly introduce. At this point, it is not worth the
hassle.)
\par
\item Let $p\left(  X\right)  $ be a polynomial with real coefficients. Then,
it should be obvious that $p\left(  X\right)  $ can also be viewed as a
polynomial with complex coefficients: For instance, if $p\left(  X\right)  $
was defined as $3X+\dfrac{7}{2}X\left(  X-1\right)  $, then we can view the
numbers $3$, $\dfrac{7}{2}$ and $-1$ appearing in its definition as complex
numbers, and thus get a polynomial with complex coefficients. But wait! What
if two polynomials $p\left(  X\right)  $ and $q\left(  X\right)  $ are equal
when viewed as polynomials with real coefficients, but when viewed as
polynomials with complex coefficients become distinct (because when we view
them as polynomials with complex coefficients, their domains become extended,
and a new complex $\alpha$ might perhaps no longer satisfy $p\left(
\alpha\right)  =q\left(  \alpha\right)  $ )? This does not actually happen,
but ruling this out is not obvious if you regard polynomials as functions.
\par
\item (This requires some familiarity with finite fields:) Treating
polynomials as functions works reasonably well for polynomials with integer,
rational, real and complex coefficients (as long as one is not too demanding).
But we will eventually want to consider polynomials with coefficients in any
arbitrary commutative ring $\mathbb{K}$. An example for a commutative ring
$\mathbb{K}$ is the finite field $\mathbb{F}_{p}$ with $p$ elements, where $p$
is a prime. (This finite field $\mathbb{F}_{p}$ is better known as the ring of
integers modulo $p$.) If we define polynomials with coefficients in
$\mathbb{F}_{p}$ as functions $\mathbb{F}_{p}\rightarrow\mathbb{F}_{p}$, then
we really run into problems; for example, the polynomials $X$ and $X^{p}$ over
this field become identical as functions!
\end{itemize}
}. Also, if we equated polynomials with the functions they describe, then we
would waste the word \textquotedblleft polynomial\textquotedblright\ on a
concept (a function described by a polynomial) that already has a word for it
(namely, \textit{polynomial function}).

The preceding paragraphs should have convinced you that it is worth defining
\textquotedblleft polynomials\textquotedblright\ in a way that, on the one
hand, conveys the concept that they are more \textquotedblleft formal
expressions\textquotedblright\ than \textquotedblleft
functions\textquotedblright, but on the other hand, is less nebulous than
\textquotedblleft formal expression\textquotedblright. Here is one such definition:

\begin{definition}
\label{def.polynomial-univar}\textbf{(a)} A \textit{univariate polynomial with
rational coefficients} means a sequence $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  \in\mathbb{Q}^{\infty}$ of elements of $\mathbb{Q}$ such that%
\begin{equation}
\text{all but finitely many }k\in\mathbb{N}\text{ satisfy }p_{k}=0.
\label{eq.def.polynomial-univar.finite}%
\end{equation}
Here, the phrase \textquotedblleft all but finitely many $k\in\mathbb{N}$
satisfy $p_{k}=0$\textquotedblright\ means \textquotedblleft there exists some
finite subset $J$ of $\mathbb{N}$ such that every $k\in\mathbb{N}\setminus J$
satisfies $p_{k}=0$\textquotedblright. (See Definition \ref{def.allbutfin} for
the general definition of \textquotedblleft all but finitely
many\textquotedblright, and Section \ref{sect.infperm} for some practice with
this concept.) More concretely, the condition
(\ref{eq.def.polynomial-univar.finite}) can be rewritten as follows: The
sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)  $ contains only zeroes from
some point on (i.e., there exists some $N\in\mathbb{N}$ such that
$p_{N}=p_{N+1}=p_{N+2}=\cdots=0$).

For the remainder of this definition, \textquotedblleft univariate polynomial
with rational coefficients\textquotedblright\ will be abbreviated as
\textquotedblleft polynomial\textquotedblright.

For example, the sequences $\left(  0,0,0,\ldots\right)  $, $\left(
1,3,5,0,0,0,\ldots\right)  $, $\left(  4,0,-\dfrac{2}{3},5,0,0,0,\ldots
\right)  $, $\left(  0,-1,\dfrac{1}{2},0,0,0,\ldots\right)  $ (where the
\textquotedblleft$\ldots$\textquotedblright\ stand for infinitely many zeroes)
are polynomials, but the sequence $\left(  1,1,1,\ldots\right)  $ (where the
\textquotedblleft$\ldots$\textquotedblright\ stands for infinitely many $1$'s)
is not (since it does not satisfy (\ref{eq.def.polynomial-univar.finite})).

So we have defined a polynomial as an infinite sequence of rational numbers
with a certain property. So far, this does not seem to reflect any intuition
of polynomials as \textquotedblleft formal expressions\textquotedblright.
However, we shall soon (namely, in Definition \ref{def.polynomial-univar}
\textbf{(j)}) identify the polynomial $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  \in\mathbb{Q}^{\infty}$ with the \textquotedblleft formal
expression\textquotedblright\ $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ (this is an
infinite sum, but due to (\ref{eq.def.polynomial-univar.finite}) all but its
first few terms are $0$ and thus can be neglected). For instance, the
polynomial $\left(  1,3,5,0,0,0,\ldots\right)  $ will be identified with the
\textquotedblleft formal expression\textquotedblright\ $1+3X+5X^{2}%
+0X^{3}+0X^{4}+0X^{5}+\cdots=1+3X+5X^{2}$. Of course, we cannot do this
identification right now, since we do not have a reasonable definition of $X$.

\textbf{(b)} We let $\mathbb{Q}\left[  X\right]  $ denote the set of all
univariate polynomials with rational coefficients. Given a polynomial
$p=\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $,
we denote the numbers $p_{0},p_{1},p_{2},\ldots$ as the \textit{coefficients}
of $p$. More precisely, for every $i\in\mathbb{N}$, we shall refer to $p_{i}$
as the $i$\textit{-th coefficient} of $p$. (Do not forget that we are counting
from $0$ here: any polynomial \textquotedblleft begins\textquotedblright\ with
its $0$-th coefficient.) The $0$-th coefficient of $p$ is also known as the
\textit{constant term} of $p$.

Instead of \textquotedblleft the $i$-th coefficient of $p$\textquotedblright,
we often also say \textquotedblleft the \textit{coefficient before }$X^{i}%
$\textit{ of }$p$\textquotedblright\ or \textquotedblleft the
\textit{coefficient of }$X^{i}$ \textit{in }$p$\textquotedblright.

Thus, any polynomial $p\in\mathbb{Q}\left[  X\right]  $ is the sequence of its coefficients.

\textbf{(c)} We denote the polynomial $\left(  0,0,0,\ldots\right)
\in\mathbb{Q}\left[  X\right]  $ by $\mathbf{0}$. We will also write $0$ for
it when no confusion with the number $0$ is possible. The polynomial
$\mathbf{0}$ is called the \textit{zero polynomial}. A polynomial
$p\in\mathbb{Q}\left[  X\right]  $ is said to be \textit{nonzero} if
$p\neq\mathbf{0}$.

\textbf{(d)} We denote the polynomial $\left(  1,0,0,0,\ldots\right)
\in\mathbb{Q}\left[  X\right]  $ by $\mathbf{1}$. We will also write $1$ for
it when no confusion with the number $1$ is possible.

\textbf{(e)} For any $\lambda\in\mathbb{Q}$, we denote the polynomial $\left(
\lambda,0,0,0,\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ by
$\operatorname*{const}\lambda$. We call it the \textit{constant polynomial
with value }$\lambda$. It is often useful to identify $\lambda\in\mathbb{Q}$
with $\operatorname*{const}\lambda\in\mathbb{Q}\left[  X\right]  $. Notice
that $\mathbf{0}=\operatorname*{const}0$ and $\mathbf{1}=\operatorname*{const}%
1$.

\textbf{(f)} Now, let us define the sum, the difference and the product of two
polynomials. Indeed, let $a=\left(  a_{0},a_{1},a_{2},\ldots\right)
\in\mathbb{Q}\left[  X\right]  $ and $b=\left(  b_{0},b_{1},b_{2}%
,\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ be two polynomials. Then, we
define three polynomials $a+b$, $a-b$ and $a\cdot b$ in $\mathbb{Q}\left[
X\right]  $ by%
\begin{align*}
a+b  &  =\left(  a_{0}+b_{0},a_{1}+b_{1},a_{2}+b_{2},\ldots\right)  ;\\
a-b  &  =\left(  a_{0}-b_{0},a_{1}-b_{1},a_{2}-b_{2},\ldots\right)  ;\\
a\cdot b  &  =\left(  c_{0},c_{1},c_{2},\ldots\right)  ,
\end{align*}
where%
\[
c_{k}=\sum_{i=0}^{k}a_{i}b_{k-i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
k\in\mathbb{N}.
\]
We call $a+b$ the \textit{sum} of $a$ and $b$; we call $a-b$ the
\textit{difference} of $a$ and $b$; we call $a\cdot b$ the \textit{product} of
$a$ and $b$. We abbreviate $a\cdot b$ by $ab$.

For example,%
\begin{align*}
\left(  1,2,2,0,0,\ldots\right)  +\left(  3,0,-1,0,0,0,\ldots\right)   &
=\left(  4,2,1,0,0,0,\ldots\right)  ;\\
\left(  1,2,2,0,0,\ldots\right)  -\left(  3,0,-1,0,0,0,\ldots\right)   &
=\left(  -2,2,3,0,0,0,\ldots\right)  ;\\
\left(  1,2,2,0,0,\ldots\right)  \cdot\left(  3,0,-1,0,0,0,\ldots\right)   &
=\left(  3,6,5,-2,-2,0,0,0,\ldots\right)  .
\end{align*}


The definition of $a+b$ essentially says that \textquotedblleft polynomials
are added coefficientwise\textquotedblright\ (i.e., in order to obtain the sum
of two polynomials $a$ and $b$, it suffices to add each coefficient of $a$ to
the corresponding coefficient of $b$). Similarly, the definition of $a-b$ says
the same thing about subtraction. The definition of $a\cdot b$ is more
surprising. However, it loses its mystique when we identify the polynomials
$a$ and $b$ with the \textquotedblleft formal expressions\textquotedblright%
\ $a_{0}+a_{1}X+a_{2}X^{2}+\cdots$ and $b_{0}+b_{1}X+b_{2}X^{2}+\cdots$
(although, at this point, we do not know what these expressions really mean);
indeed, it simply says that
\[
\left(  a_{0}+a_{1}X+a_{2}X^{2}+\cdots\right)  \left(  b_{0}+b_{1}X+b_{2}%
X^{2}+\cdots\right)  =c_{0}+c_{1}X+c_{2}X^{2}+\cdots,
\]
where $c_{k}=\sum_{i=0}^{k}a_{i}b_{k-i}$ for every $k\in\mathbb{N}$. This is
precisely what one would expect, because if you expand $\left(  a_{0}%
+a_{1}X+a_{2}X^{2}+\cdots\right)  \left(  b_{0}+b_{1}X+b_{2}X^{2}%
+\cdots\right)  $ using the distributive law and collect equal powers of $X$,
then you get precisely $c_{0}+c_{1}X+c_{2}X^{2}+\cdots$. Thus, the definition
of $a\cdot b$ has been tailored to make the distributive law hold.

(By the way, why is $a\cdot b$ a polynomial? That is, why does it satisfy
(\ref{eq.def.polynomial-univar.finite}) ? The proof is easy, but we omit it.)

Addition, subtraction and multiplication of polynomials satisfy some of the
same rules as addition, subtraction and multiplication of numbers. For
example, the commutative laws $a+b=b+a$ and $ab=ba$ are valid for polynomials
just as they are for numbers; same holds for the associative laws $\left(
a+b\right)  +c=a+\left(  b+c\right)  $ and $\left(  ab\right)  c=a\left(
bc\right)  $ and the distributive laws $\left(  a+b\right)  c=ac+bc$ and
$a\left(  b+c\right)  =ab+ac$.

The set $\mathbb{Q}\left[  X\right]  $, endowed with the operations $+$ and
$\cdot$ just defined, and with the elements $\mathbf{0}$ and $\mathbf{1}$, is
a commutative ring (where we are using the notations of Definition
\ref{def.commring}). It is called the \textit{(univariate) polynomial ring
over }$\mathbb{Q}$.

\textbf{(g)} Let $a=\left(  a_{0},a_{1},a_{2},\ldots\right)  \in
\mathbb{Q}\left[  X\right]  $ and $\lambda\in\mathbb{Q}$. Then, $\lambda a$
denotes the polynomial $\left(  \lambda a_{0},\lambda a_{1},\lambda
a_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $. (This equals the
polynomial $\left(  \operatorname*{const}\lambda\right)  \cdot a$; thus,
identifying $\lambda$ with $\operatorname*{const}\lambda$ does not cause any
inconsistencies here.)

\textbf{(h)} If $p=\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}%
\left[  X\right]  $ is a nonzero polynomial, then the \textit{degree} of $p$
is defined to be the maximum $i\in\mathbb{N}$ satisfying $p_{i}\neq0$. If
$p\in\mathbb{Q}\left[  X\right]  $ is the zero polynomial, then the degree of
$p$ is defined to be $-\infty$. (Here, $-\infty$ is just a fancy symbol, not a
number.) For example, $\deg\left(  1,4,0,-1,0,0,0,\ldots\right)  =3$.

\textbf{(i)} If $a=\left(  a_{0},a_{1},a_{2},\ldots\right)  \in\mathbb{Q}%
\left[  X\right]  $ and $n\in\mathbb{N}$, then a polynomial $a^{n}%
\in\mathbb{Q}\left[  X\right]  $ is defined to be the product
$\underbrace{aa\cdots a}_{n\text{ times}}$. (This is understood to be
$\mathbf{1}$ when $n=0$. In general, an empty product of polynomials is always
understood to be $\mathbf{1}$.)

\textbf{(j)} We let $X$ denote the polynomial $\left(  0,1,0,0,0,\ldots
\right)  \in\mathbb{Q}\left[  X\right]  $. (This is the polynomial whose
$1$-st coefficient is $1$ and whose other coefficients are $0$.) This
polynomial is called the \textit{indeterminate} of $\mathbb{Q}\left[
X\right]  $. It is easy to see that, for any $n\in\mathbb{N}$, we have%
\[
X^{n}=\left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}},1,0,0,0,\ldots
\right)  .
\]


This polynomial $X$ finally provides an answer to the questions
\textquotedblleft what is an indeterminate\textquotedblright\ and
\textquotedblleft what is a formal expression\textquotedblright. Namely, let
$\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ be
any polynomial. Then, the sum $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ is well-defined
(it is an infinite sum, but due to (\ref{eq.def.polynomial-univar.finite}) it
has only finitely many nonzero addends), and it is easy to see that this sum
equals $\left(  p_{0},p_{1},p_{2},\ldots\right)  $. Thus,
\[
\left(  p_{0},p_{1},p_{2},\ldots\right)  =p_{0}+p_{1}X+p_{2}X^{2}%
+\cdots\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  \in\mathbb{Q}\left[  X\right]  .
\]
This finally allows us to write a polynomial $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  $ as a sum $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ while remaining
honest; the sum $p_{0}+p_{1}X+p_{2}X^{2}+\cdots$ is no longer a
\textquotedblleft formal expression\textquotedblright\ of unclear meaning, nor
a function, but it is just an alternative way to write the sequence $\left(
p_{0},p_{1},p_{2},\ldots\right)  $. So, at last, our notion of a polynomial
resembles the intuitive notion of a polynomial!

Of course, we can write polynomials as finite sums as well. Indeed, if
$\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{Q}\left[  X\right]  $ is
a polynomial and $N$ is a nonnegative integer such that every $n>N$ satisfies
$p_{n}=0$, then%
\[
\left(  p_{0},p_{1},p_{2},\ldots\right)  =p_{0}+p_{1}X+p_{2}X^{2}+\cdots
=p_{0}+p_{1}X+\cdots+p_{N}X^{N}%
\]
(because addends can be discarded when they are $0$). For example, $\left(
4,1,0,0,0,\ldots\right)  =4+1X=4+X$ and $\left(  \dfrac{1}{2},0,\dfrac{1}%
{3},0,0,0,\ldots\right)  =\dfrac{1}{2}+0X+\dfrac{1}{3}X^{2}=\dfrac{1}%
{2}+\dfrac{1}{3}X^{2}$.

\textbf{(k)} For our definition of polynomials to be fully compatible with our
intuition, we are missing only one more thing: a way to evaluate a polynomial
at a number, or some other object (e.g., another polynomial or a function).
This is easy: Let $p=\left(  p_{0},p_{1},p_{2},\ldots\right)  \in
\mathbb{Q}\left[  X\right]  $ be a polynomial, and let $\alpha\in\mathbb{Q}$.
Then, $p\left(  \alpha\right)  $ means the number $p_{0}+p_{1}\alpha
+p_{2}\alpha^{2}+\cdots\in\mathbb{Q}$. (Again, the infinite sum $p_{0}%
+p_{1}\alpha+p_{2}\alpha^{2}+\cdots$ makes sense because of
(\ref{eq.def.polynomial-univar.finite}).) Similarly, we can define $p\left(
\alpha\right)  $ when $\alpha\in\mathbb{R}$ (but in this case, $p\left(
\alpha\right)  $ will be an element of $\mathbb{R}$) or when $\alpha
\in\mathbb{C}$ (in this case, $p\left(  \alpha\right)  \in\mathbb{C}$) or when
$\alpha$ is a square matrix with rational entries (in this case, $p\left(
\alpha\right)  $ will also be such a matrix) or when $\alpha$ is another
polynomial (in this case, $p\left(  \alpha\right)  $ is such a polynomial as well).

For example, if $p=\left(  1,-2,0,3,0,0,0,\ldots\right)  =1-2X+3X^{3}$, then
$p\left(  \alpha\right)  =1-2\alpha+3\alpha^{3}$ for every $\alpha$.

The map $\mathbb{Q}\rightarrow\mathbb{Q},\ \alpha\mapsto p\left(
\alpha\right)  $ is called the \textit{polynomial function described by }$p$.
As we said above, this function is not $p$, and it is not a good idea to
equate it with $p$.

If $\alpha$ is a number (or a square matrix, or another polynomial), then
$p\left(  \alpha\right)  $ is called the result of \textit{evaluating }$p$
\textit{at }$X=\alpha$ (or, simply, evaluating $p$ at $\alpha$), or the result
of \textit{substituting }$\alpha$\textit{ for }$X$\textit{ in }$p$. This
notation, of course, reminds of functions; nevertheless, (as we already said a
few times) $p$ is \textbf{not a function}.

Probably the simplest three cases of evaluation are the following ones:

\begin{itemize}
\item We have $p\left(  0\right)  =p_{0}+p_{1}0^{1}+p_{2}0^{2}+\cdots=p_{0}$.
In other words, evaluating $p$ at $X=0$ yields the constant term of $p$.

\item We have $p\left(  1\right)  =p_{0}+p_{1}1^{1}+p_{2}1^{2}+\cdots
=p_{0}+p_{1}+p_{2}+\cdots$. In other words, evaluating $p$ at $X=1$ yields the
sum of all coefficients of $p$.

\item We have $p\left(  X\right)  =p_{0}+p_{1}X^{1}+p_{2}X^{2}+\cdots
=p_{0}+p_{1}X+p_{2}X^{2}+\cdots=p$. In other words, evaluating $p$ at $X=X$
yields $p$ itself. This allows us to write $p\left(  X\right)  $ for $p$. Many
authors do so, just in order to stress that $p$ is a polynomial and that the
indeterminate is called $X$. It should be kept in mind that $X$ is \textbf{not
a variable} (just as $p$ is \textbf{not a function}); it is the (fixed!)
sequence $\left(  0,1,0,0,0,\ldots\right)  \in\mathbb{Q}\left[  X\right]  $
which serves as the indeterminate for polynomials in $\mathbb{Q}\left[
X\right]  $.
\end{itemize}

\textbf{(l)} Often, one wants (or is required) to give an indeterminate a name
other than $X$. (For instance, instead of polynomials with rational
coefficients, we could be considering polynomials whose coefficients
themselves are polynomials in $\mathbb{Q}\left[  X\right]  $; and then, we
would not be allowed to use the letter $X$ for the \textquotedblleft
new\textquotedblright\ indeterminate anymore, as it already means the
indeterminate of $\mathbb{Q}\left[  X\right]  $ !) This can be done, and the
rules are the following: Any letter (that does not already have a meaning) can
be used to denote the indeterminate; but then, the set of all polynomials has
to be renamed as $\mathbb{Q}\left[  \eta\right]  $, where $\eta$ is this
letter. For instance, if we want to denote the indeterminate as $x$, then we
have to denote the set by $\mathbb{Q}\left[  x\right]  $.

It is furthermore convenient to regard the sets $\mathbb{Q}\left[
\eta\right]  $ for different letters $\eta$ as distinct. Thus, for example,
the polynomial $3X^{2}+1$ is not the same as the polynomial $3Y^{2}+1$. (The
reason for doing so is that one sometimes wishes to view both of these
polynomials as polynomials in the two variables $X$ and $Y$.) Formally
speaking, this means that we should define a polynomial in $\mathbb{Q}\left[
\eta\right]  $ to be not just a sequence $\left(  p_{0},p_{1},p_{2}%
,\ldots\right)  $ of rational numbers, but actually a pair $\left(  \left(
p_{0},p_{1},p_{2},\ldots\right)  ,\text{\textquotedblleft}\eta
\text{\textquotedblright}\right)  $ of a sequence of rational numbers and the
letter $\eta$. (Here, \textquotedblleft$\eta$\textquotedblright\ really means
the letter $\eta$, not the sequence $\left(  0,1,0,0,0,\ldots\right)  $.) This
is, of course, a very technical point which is of little relevance to most of
mathematics; it becomes important when one tries to implement polynomials in a
programming language.

\textbf{(m)} As already explained, we can replace $\mathbb{Q}$ by $\mathbb{Z}%
$, $\mathbb{R}$, $\mathbb{C}$ or any other commutative ring $\mathbb{K}$ in
the above definition. (See Definition \ref{def.commring} for the definition of
a commutative ring.) When $\mathbb{Q}$ is replaced by a commutative ring
$\mathbb{K}$, the notion of \textquotedblleft univariate polynomials with
rational coefficients\textquotedblright\ becomes \textquotedblleft univariate
polynomials with coefficients in $\mathbb{K}$\textquotedblright\ (also known
as \textquotedblleft univariate polynomials over $\mathbb{K}$%
\textquotedblright), and the set of such polynomials is denoted by
$\mathbb{K}\left[  X\right]  $ rather than $\mathbb{Q}\left[  X\right]  $.
\end{definition}

So much for univariate polynomials.

Polynomials in multiple variables are (in my opinion) treated the best in
\cite[Chapter II, \S 3]{Lang02}, where they are introduced as elements of a
monoid ring. However, this treatment is rather abstract and uses a good deal
of algebraic language\footnote{Also, the book \cite{Lang02} is notorious for
its unpolished writing; it is best read with Bergman's companion
\cite{Bergman-Lang} at hand.}. The treatments in \cite[\S 4.5]{Walker87}, in
\cite[Chapter A-3]{Rotman15} and in \cite[Chapter IV, \S 4]{BirkMac} use the
above-mentioned recursive shortcut that makes them inferior (in my opinion). A
neat (and rather elementary) treatment of polynomials in $n$ variables (for
finite $n$) can be found in \cite[Chapter III, \S 5]{Hungerford-03} and in
\cite[\S 8]{AmaEsc05}; it generalizes the viewpoint we used in Definition
\ref{def.polynomial-univar} for univariate polynomials above\footnote{You are
reading right: The analysis textbook \cite{AmaEsc05} is one of the few sources
I am aware of to define the (algebraic!) notion of polynomials precisely and
well.}.

\section{\label{chp.binom}On binomial coefficients}

The present chapter is about \textit{binomial coefficients}. They are used in
almost every part of mathematics, and studying them provides good
opportunities to practice the arts of mathematical induction and of finding
combinatorial bijections.

Identities involving binomial coefficients are legion, and books have been
written about them (let me mention \cite[Chapter 5]{GKP} as a highly readable
introduction; but, e.g., \href{http://www.math.wvu.edu/~gould/}{Henry W.
Gould's website} goes far deeper down the rabbit hole). We shall only study a
few of these identities.

\subsection{Definitions and basic properties}

\subsubsection{The definition}

Let us first define binomial coefficients:

\begin{definition}
\label{def.binom}Let $n\in\mathbb{N}$.

\textbf{(a)} The \textit{formal binomial coefficient} $\dbinom{X}{n}$ is a
polynomial of degree $n$ in one indeterminate $X$, with rational coefficients.
It is defined by%
\[
\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!}.
\]
(When $n=0$, then the numerator of this fraction (i.e., the product $X\left(
X-1\right)  \cdots\left(  X-n+1\right)  $) is an empty product. By convention,
an empty product is always defined to be $1$.)

\textbf{(b)} The polynomial $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)
\cdots\left(  X-n+1\right)  }{n!}$ can be evaluated at any integer (i.e., we
can substitute any integer for $X$), or at any rational number, or even at any
complex number, or at any other polynomial (for example, we can substitute
$X^{2}+2$ for $X$ to obtain $\dbinom{X^{2}+2}{n}$, which is again a polynomial).

Whenever $m$ is an integer (or rational number, or complex number), we denote
by $\dbinom{m}{n}$ the result of evaluating the polynomial $\dbinom{X}{n}$ at
$X=m$. These numbers $\dbinom{m}{n}$ are the so-called \textit{binomial
coefficients}. The number $\dbinom{m}{n}$ is often pronounced
\textquotedblleft$m$ choose $n$\textquotedblright.
\end{definition}

\begin{example}
The binomial coefficient $\dbinom{4}{2}$ is the result of evaluating the
polynomial $\dbinom{X}{2}=\dfrac{X\left(  X-1\right)  }{2!}=\dfrac{X\left(
X-1\right)  }{2}$ at $X=4$; thus, it equals $\dfrac{4\left(  4-1\right)  }%
{2}=6$.
\end{example}

The binomial coefficients $\dbinom{m}{n}$ form the so-called
\textit{\href{https://en.wikipedia.org/wiki/Pascal's_triangle}{\textit{Pascal's
triangle}}}\footnote{More precisely, the numbers $\dbinom{m}{n}$ for
$m\in\mathbb{N}$ and $n\in\left\{  0,1,\ldots,m\right\}  $ form Pascal's
triangle. Nevertheless, the \textquotedblleft other\textquotedblright%
\ binomial coefficients (particularly the ones where $m$ is a negative
integer) are highly useful, too.}. Let us state a few basic properties of
these numbers:

\subsubsection{Simple formulas}

\begin{proposition}
\label{prop.binom.mn}We have%
\begin{equation}
\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}
\label{eq.binom.mn}%
\end{equation}
for every $m\in\mathbb{Z}$ and $n\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.mn}.]Let $m\in\mathbb{Z}$ and
$n\in\mathbb{N}$. The equality (\ref{eq.binom.mn}) follows by evaluating both
sides of the identity $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(
X-n+1\right)  }{n!}$ at $X=m$. Thus, Proposition \ref{prop.binom.mn} is proven.
\end{proof}

The equality (\ref{eq.binom.mn}) is how the binomial coefficients $\dbinom
{m}{n}$ are usually defined in textbooks. Thus, our detour through polynomials
was not necessary in order to define them. (But this detour will reveal to be
useful soon, when we will prove some properties of binomial coefficients.)

\begin{proposition}
\label{prop.binom.00}\textbf{(a)} We have
\begin{equation}
\dbinom{X}{0}=1. \label{eq.binom.00X}%
\end{equation}


\textbf{(b)} We have%
\begin{equation}
\dbinom{m}{0}=1 \label{eq.binom.00}%
\end{equation}
for every $m\in\mathbb{Z}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.00}.]\textbf{(a)} The definition of
$\dbinom{X}{0}$ yields
\[
\dbinom{X}{0}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-0+1\right)  }{0!}.
\]
Since $X\left(  X-1\right)  \cdots\left(  X-0+1\right)  =\left(  \text{a
product of }0\text{ integers}\right)  =1$, this rewrites as $\dbinom{X}%
{0}=\dfrac{1}{0!}=1$ (since $0!=1$). This proves Proposition
\ref{prop.binom.00} \textbf{(a)}.

\textbf{(b)} Let $m\in\mathbb{Z}$. Then, (\ref{eq.binom.00}) follows by
substituting $m$ for $X$ in (\ref{eq.binom.00X}). This proves Proposition
\ref{prop.binom.00} \textbf{(b)}.
\end{proof}

\begin{proposition}
\label{prop.binom.formula}We have%
\begin{equation}
\dbinom{m}{n}=\dfrac{m!}{n!\left(  m-n\right)  !} \label{eq.binom.formula}%
\end{equation}
for any $m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m\geq n$.
\end{proposition}

\begin{remark}
\textbf{Caution:} The formula (\ref{eq.binom.formula}) holds only for
$m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m\geq n$. Thus, neither
$\dbinom{-3}{2}$ nor $\dbinom{1/3}{3}$ nor $\dbinom{2}{5}$ nor the
\textbf{polynomial} $\dbinom{X}{3}$ can be evaluated using this formula!
Definition \ref{def.binom} thus can be used to evaluate $\dbinom{m}{n}$ in
many more cases than (\ref{eq.binom.formula}) does.
\end{remark}

\begin{proof}
[Proof of Proposition \ref{prop.binom.formula}.]Let $m\in\mathbb{N}$ and
$n\in\mathbb{N}$ be such that $m\geq n$. Then, evaluating both sides of the
identity $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(
X-n+1\right)  }{n!}$ at $X=m$, we obtain%
\[
\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!},
\]
so that $n!\cdot\dbinom{m}{n}=m\left(  m-1\right)  \cdots\left(  m-n+1\right)
$. But%
\begin{align*}
m!  &  =m\left(  m-1\right)  \cdots1=\left(  m\left(  m-1\right)
\cdots\left(  m-n+1\right)  \right)  \cdot\underbrace{\left(  \left(
m-n\right)  \left(  m-n-1\right)  \cdots1\right)  }_{=\left(  m-n\right)  !}\\
&  =\left(  m\left(  m-1\right)  \cdots\left(  m-n+1\right)  \right)
\cdot\left(  m-n\right)  !,
\end{align*}
so that $\dfrac{m!}{\left(  m-n\right)  !}=m\left(  m-1\right)  \cdots\left(
m-n+1\right)  $. Comparing this with $n!\cdot\dbinom{m}{n}=m\left(
m-1\right)  \cdots\left(  m-n+1\right)  $, we obtain $n!\cdot\dbinom{m}%
{n}=\dfrac{m!}{\left(  m-n\right)  !}$. Dividing this equality by $n!$, we
obtain $\dbinom{m}{n}=\dfrac{m!}{n!\left(  m-n\right)  !}$. Thus, Proposition
\ref{prop.binom.formula} is proven.
\end{proof}

\begin{proposition}
\label{prop.binom.0}We have
\begin{equation}
\dbinom{m}{n}=0 \label{eq.binom.0}%
\end{equation}
for every $m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m<n$.
\end{proposition}

\begin{remark}
\textbf{Caution:} The formula (\ref{eq.binom.0}) is not true if we drop the
condition $m\in\mathbb{N}$. For example, $\dbinom{-3}{2}=6\neq0$ despite
$-3<2$.
\end{remark}

\begin{proof}
[Proof of Proposition \ref{prop.binom.0}.]Let $m\in\mathbb{N}$ and
$n\in\mathbb{N}$ be such that $m<n$. Evaluating both sides of the identity
$\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!}$
at $X=m$, we obtain%
\[
\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}.
\]


But $m\geq0$ (since $m\in\mathbb{N}$) and $m<n$. Hence, $m-m$ is one of the
$n$ integers $m,m-1,\ldots,m-n+1$. Thus, one of the $n$ factors of the product
$m\left(  m-1\right)  \cdots\left(  m-n+1\right)  $ is $m-m=0$. Therefore, the
whole product $m\left(  m-1\right)  \cdots\left(  m-n+1\right)  $ is $0$
(because if one of the factors of a product is $0$, then the whole product
must be $0$). Thus, $m\left(  m-1\right)  \cdots\left(  m-n+1\right)  =0$.
Hence,%
\begin{align*}
\dbinom{m}{n}  &  =\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)
}{n!}=\dfrac{0}{n!}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\left(
m-1\right)  \cdots\left(  m-n+1\right)  =0\right) \\
&  =0.
\end{align*}
This proves Proposition \ref{prop.binom.0}.
\end{proof}

\begin{proposition}
\label{prop.binom.symm}We have%
\begin{equation}
\dbinom{m}{n}=\dbinom{m}{m-n} \label{eq.binom.symm}%
\end{equation}
for any $m\in\mathbb{N}$ and $n\in\mathbb{N}$ satisfying $m\geq n$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.symm}.]Let $m\in\mathbb{N}$ and
$n\in\mathbb{N}$ be such that $m\geq n$. Then, $m-n\in\mathbb{N}$ (since
$m\geq n$) and $m\geq m-n$ (since $n\geq0$ (since $n\in\mathbb{N}$)). Hence,
(\ref{eq.binom.formula}) (applied to $m-n$ instead of $n$) yields
\[
\dbinom{m}{m-n}=\dfrac{m!}{\left(  m-n\right)  !\left(  m-\left(  m-n\right)
\right)  !}=\dfrac{m!}{\left(  m-\left(  m-n\right)  \right)  !\left(
m-n\right)  !}=\dfrac{m!}{n!\left(  m-n\right)  !}%
\]
(since $m-\left(  m-n\right)  =n$). Compared with (\ref{eq.binom.formula}),
this yields $\dbinom{m}{n}=\dbinom{m}{m-n}$. Proposition \ref{prop.binom.symm}
is thus proven.
\end{proof}

\begin{proposition}
\label{prop.binom.mm}We have%
\begin{equation}
\dbinom{m}{m}=1 \label{eq.binom.mm}%
\end{equation}
for every $m\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.mm}.]Let $m\in\mathbb{N}$. Then,
(\ref{eq.binom.symm}) (applied to $n=m$) yields $\dbinom{m}{m}=\dbinom{m}%
{m-m}=\dbinom{m}{0}=1$ (according to (\ref{eq.binom.00})). This proves
Proposition \ref{prop.binom.mm}.
\end{proof}

\begin{exercise}
\label{exe.multinom1}Let $m\in\mathbb{N}$ and $\left(  k_{1},k_{2}%
,\ldots,k_{m}\right)  \in\mathbb{N}^{m}$. Prove that $\dfrac{\left(
k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}!}$ is a positive integer.
\end{exercise}

\begin{remark}
\label{rmk.multinom1}Let $m\in\mathbb{N}$ and $\left(  k_{1},k_{2}%
,\ldots,k_{m}\right)  \in\mathbb{N}^{m}$. Exercise \ref{exe.multinom1} shows
that $\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots
k_{m}!}$ is a positive integer. This positive integer is called a
\textit{multinomial coefficient}, and is often denoted by $\dbinom{n}%
{k_{1},k_{2},\ldots,k_{m}}$, where $n=k_{1}+k_{2}+\cdots+k_{m}$. (We shall
avoid this particular notation, since it makes the meaning of $\dbinom{n}{k}$
slightly ambiguous: It could mean both the binomial coefficient $\dbinom{n}%
{k}$ and the multinomial coefficient $\dbinom{n}{k_{1},k_{2},\ldots,k_{m}}$
for $\left(  k_{1},k_{2},\ldots,k_{m}\right)  =\left(  k\right)  $.
Fortunately, the ambiguity is not really an issue, because the only situation
in which both meanings make sense is when $k=n\in\mathbb{N}$, but in this case
both interpretations give the same value $1$.)
\end{remark}

\subsubsection{The recurrence relation of the binomial coefficients}

\begin{proposition}
\label{prop.binom.rec}\textbf{(a)} We have%
\begin{equation}
\dbinom{X}{n}=\dbinom{X-1}{n}+\dbinom{X-1}{n-1} \label{eq.binom.rec}%
\end{equation}
for any $n\in\left\{  1,2,3,\ldots\right\}  $.

\textbf{(b)} We have%
\begin{equation}
\dbinom{m}{n}=\dbinom{m-1}{n-1}+\dbinom{m-1}{n} \label{eq.binom.rec.m}%
\end{equation}
for any $m\in\mathbb{Z}$ and $n\in\left\{  1,2,3,\ldots\right\}  $.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.rec}.]\textbf{(a)} Let $n\in\left\{
1,2,3,\ldots\right\}  $. We have $n!=n\cdot\left(  n-1\right)  !$, so that
$\left(  n-1\right)  !=n!/n$ and thus $\dfrac{1}{\left(  n-1\right)  !}%
=\dfrac{1}{n!/n}=\dfrac{1}{n!}\cdot n$.

The definition of $\dbinom{X}{n-1}$ yields
\begin{align*}
\dbinom{X}{n-1}  &  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-\left(
n-1\right)  +1\right)  }{\left(  n-1\right)  !}\\
&  =\dfrac{1}{\left(  n-1\right)  !}\left(  X\left(  X-1\right)  \cdots\left(
X-\left(  n-1\right)  +1\right)  \right)  .
\end{align*}
Substituting $X-1$ for $X$ in this equality, we obtain%
\begin{align}
\dbinom{X-1}{n-1}  &  =\underbrace{\dfrac{1}{\left(  n-1\right)  !}}%
_{=\dfrac{1}{n!}\cdot n}\left(  \left(  X-1\right)  \underbrace{\left(
\left(  X-1\right)  -1\right)  }_{=X-2}\cdots\underbrace{\left(  \left(
X-1\right)  -\left(  n-1\right)  +1\right)  }_{=X-n+1}\right) \nonumber\\
&  =\dfrac{1}{n!}\cdot n\left(  \left(  X-1\right)  \left(  X-2\right)
\cdots\left(  X-n+1\right)  \right)  . \label{eq.binom.rec.pf.1}%
\end{align}


On the other hand,
\[
\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }%
{n!}=\dfrac{1}{n!}\left(  X\left(  X-1\right)  \cdots\left(  X-n+1\right)
\right)  .
\]
Substituting $X-1$ for $X$ in this equality, we find%
\begin{align*}
\dbinom{X-1}{n}  &  =\dfrac{1}{n!}\left(  \left(  X-1\right)
\underbrace{\left(  \left(  X-1\right)  -1\right)  }_{=X-2}\cdots
\underbrace{\left(  \left(  X-1\right)  -n+1\right)  }_{=X-n}\right) \\
&  =\dfrac{1}{n!}\underbrace{\left(  \left(  X-1\right)  \left(  X-2\right)
\cdots\left(  X-n\right)  \right)  }_{=\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  \cdot\left(  X-n\right)  }\\
&  =\dfrac{1}{n!}\left(  \left(  X-1\right)  \left(  X-2\right)  \cdots\left(
X-n+1\right)  \right)  \cdot\left(  X-n\right) \\
&  =\dfrac{1}{n!}\left(  X-n\right)  \cdot\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  .
\end{align*}
Adding (\ref{eq.binom.rec.pf.1}) to this equality, we obtain%
\begin{align*}
&  \dbinom{X-1}{n}+\dbinom{X-1}{n-1}\\
&  =\dfrac{1}{n!}\left(  X-n\right)  \cdot\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ +\dfrac{1}{n!}\cdot n\left(  \left(  X-1\right)
\left(  X-2\right)  \cdots\left(  X-n+1\right)  \right) \\
&  =\dfrac{1}{n!}\underbrace{\left(  \left(  X-n\right)  +n\right)  }%
_{=X}\cdot\left(  \left(  X-1\right)  \left(  X-2\right)  \cdots\left(
X-n+1\right)  \right) \\
&  =\dfrac{1}{n!}\underbrace{X\cdot\left(  \left(  X-1\right)  \left(
X-2\right)  \cdots\left(  X-n+1\right)  \right)  }_{=X\left(  X-1\right)
\cdots\left(  X-n+1\right)  }=\dfrac{1}{n!}\left(  X\left(  X-1\right)
\cdots\left(  X-n+1\right)  \right)  =\dbinom{X}{n}.
\end{align*}
This proves Proposition \ref{prop.binom.rec} \textbf{(a)}.

\textbf{(b)} Let $m\in\mathbb{Z}$ and $n\in\left\{  1,2,3,\ldots\right\}  $.
Then, (\ref{eq.binom.rec.m}) follows by substituting $m$ for $X$ in
(\ref{eq.binom.rec}). This proves Proposition \ref{prop.binom.rec}
\textbf{(b)}.
\end{proof}

The formula (\ref{eq.binom.rec.m}) is known as the \textit{recurrence relation
of the binomial coefficients}\footnote{Often it is extended to the case $n=0$
by setting $\dbinom{X}{-1}=0$. It then follows from (\ref{eq.binom.00}) in
this case.
\par
The formula (\ref{eq.binom.rec.m}) is responsible for the fact that
\textquotedblleft every number in Pascal's triangle is the sum of the two
numbers above it\textquotedblright. (Of course, if you use this fact as a
\textit{definition} of Pascal's triangle, then (\ref{eq.binom.rec.m}) is
conversely responsible for the fact that the numbers in this triangle are the
binomial coefficients.)}.

\subsubsection{The combinatorial interpretation of binomial coefficients}

\begin{proposition}
\label{prop.binom.subsets}If $m\in\mathbb{N}$ and $n\in\mathbb{N}$, and if $S$
is an $m$-element set, then
\begin{equation}
\dbinom{m}{n}\text{ is the number of all }n\text{-element subsets of
}S\text{.} \label{eq.binom.subsets}%
\end{equation}

\end{proposition}

In less formal terms, Proposition \ref{prop.binom.subsets} says the following:
If $m\in\mathbb{N}$ and $n\in\mathbb{N}$, then $\dbinom{m}{n}$ is the number
of ways to pick out $n$ among $m$ given objects, without
replacement\footnote{That is, one must not pick out the same object twice.}
and without regard for the order in which they are picked out. (Probabilists
call this \textquotedblleft unordered samples without
replacement\textquotedblright.)

\begin{example}
Proposition \ref{prop.binom.subsets} (applied to $m=4$, $n=2$ and $S=\left\{
0,1,2,3\right\}  $) shows that $\dbinom{4}{2}$ is the number of all
$2$-element subsets of $\left\{  0,1,2,3\right\}  $ (since $\left\{
0,1,2,3\right\}  $ is a $4$-element set). And indeed, this is easy to verify
by brute force: The $2$-element subsets of $\left\{  0,1,2,3\right\}  $ are%
\[
\left\{  0,1\right\}  ,\ \left\{  0,2\right\}  ,\ \left\{  0,3\right\}
,\ \left\{  1,2\right\}  ,\ \left\{  1,3\right\}  \text{ and }\left\{
2,3\right\}  ,
\]
so there are $6=\dbinom{4}{2}$ of them.
\end{example}

\begin{remark}
\textbf{Caution:} Proposition \ref{prop.binom.subsets} says nothing about
binomial coefficients $\dbinom{m}{n}$ with negative $m$. Indeed, there are no
$m$-element sets $S$ when $m$ is negative; thus, Proposition
\ref{prop.binom.subsets} would be vacuously true\footnotemark\ when $m$ is
negative, but this would not help us computing binomial coefficients
$\dbinom{m}{n}$ with negative $m$.

Actually, when $m\in\mathbb{Z}$ is negative, the number $\dbinom{m}{n}$ is
positive for $n$ even and negative for $n$ odd (easy exercise), and so an
interpretation of $\dbinom{m}{n}$ as a number of ways to do something is
rather unlikely. (On the other hand, $\left(  -1\right)  ^{n}\dbinom{m}{n}$
does have such an interpretation.)
\end{remark}

\footnotetext{A mathematical statement of the form \textquotedblleft if
$\mathcal{A}$, then $\mathcal{B}$\textquotedblright\ is said to be
\textit{vacuously true} if $\mathcal{A}$ never holds. For example, the
statement \textquotedblleft if $0=1$, then every integer is
odd\textquotedblright\ is vacuously true, because $0=1$ is false. Proposition
\ref{prop.binom.subsets} is vacuously true when $m$ is negative, because the
condition \textquotedblleft$S$ is an $m$-element set\textquotedblright\ never
holds when $m$ is negative.
\par
By the laws of logic, a vacuously true statement is always true! This may
sound counterintuitive, but really makes a lot of sense: A statement
\textquotedblleft if $\mathcal{A}$, then $\mathcal{B}$\textquotedblright\ only
says anything about situations where $\mathcal{A}$ holds. If $\mathcal{A}$
never holds, then it therefore says nothing. And saying nothing is a safe way
to remain truthful.}

\begin{remark}
Some authors (for example, those of \cite{LeLeMe16}) use
(\ref{eq.binom.subsets}) as the \textit{definition} of $\dbinom{m}{n}$. This
is a legitimate definition of $\dbinom{m}{n}$ in the case when $m$ and $n$ are
nonnegative integers (and, of course, equivalent to our definition); but it is
not as general as ours, since it does not extend to negative (or non-integer)
$m$.
\end{remark}

\begin{exercise}
\label{exe.prop.binom.subsets}Prove Proposition \ref{prop.binom.subsets}.
\end{exercise}

\subsubsection{Upper negation}

\begin{proposition}
\label{prop.binom.upper-neg}We have%
\begin{equation}
\dbinom{m}{n}=\left(  -1\right)  ^{n}\dbinom{n-m-1}{n}
\label{eq.binom.upper-neg}%
\end{equation}
for any $m\in\mathbb{Z}$ and $n\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.upper-neg}.]Let $m\in\mathbb{Z}$ and
$n\in\mathbb{N}$.

Evaluating both sides of the identity $\dbinom{X}{n}=\dfrac{X\left(
X-1\right)  \cdots\left(  X-n+1\right)  }{n!}$ at $X=n-m-1$, we obtain%
\begin{align*}
\dbinom{n-m-1}{n}  &  =\dfrac{\left(  n-m-1\right)  \left(  \left(
n-m-1\right)  -1\right)  \cdots\left(  \left(  n-m-1\right)  -n+1\right)
}{n!}\\
&  =\dfrac{1}{n!}\left(  n-m-1\right)  \left(  \left(  n-m-1\right)
-1\right)  \cdots\underbrace{\left(  \left(  n-m-1\right)  -n+1\right)
}_{=-m}\\
&  =\dfrac{1}{n!}\underbrace{\left(  n-m-1\right)  \left(  \left(
n-m-1\right)  -1\right)  \cdots\left(  -m\right)  }_{\substack{=\left(
-m\right)  \left(  -m+1\right)  \cdots\left(  n-m-1\right)  \\\text{(here, we
have just reversed the order of the factors in the product)}}}\\
&  =\dfrac{1}{n!}\underbrace{\left(  -m\right)  }_{=\left(  -1\right)
m}\underbrace{\left(  -m+1\right)  }_{=\left(  -1\right)  \left(  m-1\right)
}\cdots\underbrace{\left(  n-m-1\right)  }_{=\left(  -1\right)  \left(
m-n+1\right)  }\\
&  =\dfrac{1}{n!}\left(  \left(  -1\right)  m\right)  \left(  \left(
-1\right)  \left(  m-1\right)  \right)  \cdots\left(  \left(  -1\right)
\left(  m-n+1\right)  \right) \\
&  =\dfrac{1}{n!}\left(  -1\right)  ^{n}\left(  m\left(  m-1\right)
\cdots\left(  m-n+1\right)  \right)  ,
\end{align*}
so that%
\begin{align*}
\left(  -1\right)  ^{n}\dbinom{n-m-1}{n}  &  =\left(  -1\right)  ^{n}%
\cdot\dfrac{1}{n!}\left(  -1\right)  ^{n}\left(  m\left(  m-1\right)
\cdots\left(  m-n+1\right)  \right) \\
&  =\underbrace{\left(  -1\right)  ^{n}\left(  -1\right)  ^{n}}%
_{\substack{=\left(  -1\right)  ^{2n}=1\\\text{(since }2n\text{ is even)}%
}}\cdot\dfrac{1}{n!}\left(  m\left(  m-1\right)  \cdots\left(  m-n+1\right)
\right) \\
&  =\dfrac{1}{n!}\left(  m\left(  m-1\right)  \cdots\left(  m-n+1\right)
\right)  =\dfrac{m\left(  m-1\right)  \cdots\left(  m-n+1\right)  }{n!}.
\end{align*}
Compared with $\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(
m-n+1\right)  }{n!}$ (which is obtained by evaluating both sides of the
identity $\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(
X-n+1\right)  }{n!}$ at $X=m$), this yields $\dbinom{m}{n}=\left(  -1\right)
^{n}\dbinom{n-m-1}{n}$. Proposition \ref{prop.binom.upper-neg} is therefore proven.
\end{proof}

The formula (\ref{eq.binom.upper-neg}) is known as the \textit{upper negation
formula} and holds more generally for any $m$ for which it makes sense (in
particular, $m$ can be a polynomial or a complex number). In particular, we
have the following:

\begin{proposition}
\label{prop.binom.upper-neg.X}We have%
\begin{equation}
\dbinom{X}{n}=\left(  -1\right)  ^{n}\dbinom{n-X-1}{n}
\label{eq.binom.upper-neg.X}%
\end{equation}
(an identity between polynomials in $X$) for every $n\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.upper-neg.X}.]To prove Proposition
\ref{prop.binom.upper-neg.X}, just replace every appearance of
\textquotedblleft$m$\textquotedblright\ by \textquotedblleft$X$%
\textquotedblright\ in our proof of Proposition \ref{prop.binom.upper-neg}.
\end{proof}

\subsubsection{Binomial coefficients of integers are integers}

\begin{proposition}
\label{prop.binom.int}We have%
\begin{equation}
\dbinom{m}{n}\in\mathbb{Z} \label{eq.binom.int}%
\end{equation}
for any $m\in\mathbb{Z}$ and $n\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.int}.]Let $m\in\mathbb{Z}$ and
$n\in\mathbb{N}$. We need to show (\ref{eq.binom.int}). We are in one of the
following two cases:

\textit{Case 1:} We have $m\geq0$.

\textit{Case 2:} We have $m<0$.

Let us first consider Case 1. In this case, we have $m\geq0$. Hence,
$m\in\mathbb{N}$. Thus, there exists an $m$-element set $S$ (for example,
$S=\left\{  1,2,\ldots,m\right\}  $). Consider such an $S$. Then, $\dbinom
{m}{n}$ is the number of all $n$-element subsets of $S$ (because of
(\ref{eq.binom.subsets})). Hence, $\dbinom{m}{n}$ is a nonnegative integer, so
that $\dbinom{m}{n}\in\mathbb{N}\subseteq\mathbb{Z}$. This proves
(\ref{eq.binom.int}) in Case 1.

Let us now consider Case 2. In this case, we have $m<0$. Thus, $m\leq-1$
(since $m$ is an integer), so that $m+1\leq0$, so that
$n-m-1=n-\underbrace{\left(  m+1\right)  }_{\leq0}\geq n\geq0$. Hence,
$n-m-1\in\mathbb{N}$. Therefore, there exists an $\left(  n-m-1\right)
$-element set $S$ (for example, $S=\left\{  1,2,\ldots,n-m-1\right\}  $).
Consider such an $S$. Then, $\dbinom{n-m-1}{n}$ is the number of all
$n$-element subsets of $S$ (because of (\ref{eq.binom.subsets}), applied to
$n-m-1$ instead of $m$). Hence, $\dbinom{n-m-1}{n}\in\mathbb{N}\subseteq
\mathbb{Z}$. Now, (\ref{eq.binom.upper-neg}) shows that $\dbinom{m}{n}=\left(
-1\right)  ^{n}\underbrace{\dbinom{n-m-1}{n}}_{\in\mathbb{Z}}\in\left(
-1\right)  ^{n}\mathbb{Z}\subseteq\mathbb{Z}$ (where $\left(  -1\right)
^{n}\mathbb{Z}$ means $\left\{  \left(  -1\right)  ^{n}z\ \mid\ z\in
\mathbb{Z}\right\}  $). This proves (\ref{eq.binom.int}) in Case 2.

We thus have proven (\ref{eq.binom.int}) in each of the two Cases 1 and 2, and
can therefore conclude that (\ref{eq.binom.int}) always holds. Thus,
Proposition \ref{prop.binom.int} is proven.
\end{proof}

This was the simplest proof of Proposition \ref{prop.binom.int} that I am
aware of. There is another which proceeds by induction on $m$ (using
(\ref{eq.binom.rec})), but this induction needs two induction steps
($m\rightarrow m+1$ and $m\rightarrow m-1$) in order to reach all integers
(positive and negative). There is yet another proof using basic number theory
(specifically, checking how often a prime $p$ appears in the numerator and the
denominator of $\dbinom{m}{n}=\dfrac{m\left(  m-1\right)  \cdots\left(
m-n+1\right)  }{n!}$), but this is not quite easy.

\subsubsection{The binomial formula}

\begin{proposition}
\label{prop.binom.binomial}We have%
\begin{equation}
\left(  X+Y\right)  ^{n}=\sum_{k=0}^{n}\dbinom{n}{k}X^{k}Y^{n-k}
\label{eq.binom.binomial}%
\end{equation}
(as an equality between two polynomials in $X$ and $Y$) for every
$n\in\mathbb{N}$.
\end{proposition}

Proposition \ref{prop.binom.binomial} is the famous \textit{binomial formula}
(also known as the \textit{binomial theorem} or the \textit{binomial
expression}) and has a well-known standard proof by induction over $n$ (using
(\ref{eq.binom.rec.m}) and (\ref{eq.binom.00}))\footnote{See Exercise
\ref{exe.prop.binom.binomial} for this proof.}. Some versions of it hold for
negative $n$ as well (but not in the exact form (\ref{eq.binom.binomial}), and
not without restrictions).

\begin{exercise}
\label{exe.prop.binom.binomial}Prove Proposition \ref{prop.binom.binomial}.
\end{exercise}

\subsubsection{The absorption identity}

\begin{proposition}
\label{prop.binom.X-1}We have%
\begin{equation}
\dbinom{X}{n}=\dfrac{X}{n}\dbinom{X-1}{n-1} \label{eq.binom.X-1}%
\end{equation}
for any $n\in\left\{  1,2,3,\ldots\right\}  $.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.X-1}.]Let $n\in\left\{  1,2,3,\ldots
\right\}  $. The definition of $\dbinom{X}{n-1}$ yields $\dbinom{X}%
{n-1}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-\left(  n-1\right)
+1\right)  }{\left(  n-1\right)  !}$. Substituting $X-1$ for $X$ in this
equality, we obtain%
\begin{align*}
\dbinom{X-1}{n-1}  &  =\dfrac{\left(  X-1\right)  \left(  \left(  X-1\right)
-1\right)  \cdots\left(  \left(  X-1\right)  -\left(  n-1\right)  +1\right)
}{\left(  n-1\right)  !}\\
&  =\dfrac{\left(  X-1\right)  \left(  X-2\right)  \cdots\left(  X-n+1\right)
}{\left(  n-1\right)  !}%
\end{align*}
(since $\left(  X-1\right)  -1=X-2$ and $\left(  X-1\right)  -\left(
n-1\right)  +1=X-n+1$). Multiplying both sides of this equality by $\dfrac
{X}{n}$, we obtain%
\begin{align*}
\dfrac{X}{n}\dbinom{X-1}{n-1}  &  =\dfrac{X}{n}\cdot\dfrac{\left(  X-1\right)
\left(  X-2\right)  \cdots\left(  X-n+1\right)  }{\left(  n-1\right)  !}\\
&  =\dfrac{X\left(  X-1\right)  \left(  X-2\right)  \cdots\left(
X-n+1\right)  }{n\left(  n-1\right)  !}=\dfrac{X\left(  X-1\right)
\cdots\left(  X-n+1\right)  }{n!}%
\end{align*}
(since $X\left(  X-1\right)  \left(  X-2\right)  \cdots\left(  X-n+1\right)
=X\left(  X-1\right)  \cdots\left(  X-n+1\right)  $ and $n\left(  n-1\right)
!=n!$). Compared with%
\[
\dbinom{X}{n}=\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)  }{n!},
\]
this yields $\dbinom{X}{n}=\dfrac{X}{n}\dbinom{X-1}{n-1}$. This proves
Proposition \ref{prop.binom.X-1}.
\end{proof}

The relation (\ref{eq.binom.X-1}) (and the identity obtained from it by
substituting an integer $m$ for $X$) is called the \textit{absorption
identity} in \cite[\S 5.1]{GKP}.

\subsubsection{Trinomial revision}

\begin{proposition}
\label{prop.binom.trinom-rev}\textbf{(a)} If $a\in\mathbb{N}$ and
$i\in\mathbb{N}$ are such that $i\geq a$, then%
\begin{equation}
\dbinom{X}{i}\dbinom{i}{a}=\dbinom{X}{a}\dbinom{X-a}{i-a}.
\label{eq.binom.trinom-rev}%
\end{equation}


\textbf{(b)} If $m\in\mathbb{Z}$, $a\in\mathbb{N}$ and $i\in\mathbb{N}$ are
such that $i\geq a$, then%
\begin{equation}
\dbinom{m}{i}\dbinom{i}{a}=\dbinom{m}{a}\dbinom{m-a}{i-a}.
\label{eq.binom.trinom-rev.m}%
\end{equation}

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.binom.trinom-rev}.]\textbf{(a)} Let
$a\in\mathbb{N}$ and $i\in\mathbb{N}$ be such that $i\geq a$. We have%
\begin{align*}
&  \underbrace{\dbinom{X}{a}}_{\substack{=\dfrac{X\left(  X-1\right)
\cdots\left(  X-a+1\right)  }{a!}\\\text{(by the definition of }\dbinom{X}%
{a}\text{)}}}\underbrace{\dbinom{X-a}{i-a}}_{\substack{=\dfrac{\left(
X-a\right)  \left(  \left(  X-a\right)  -1\right)  \cdots\left(  \left(
X-a\right)  -\left(  i-a\right)  +1\right)  }{\left(  i-a\right)
!}\\\text{(by the definition of }\dbinom{X-a}{i-a}\text{)}}}\\
&  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-a+1\right)  }{a!}\cdot
\dfrac{\left(  X-a\right)  \left(  \left(  X-a\right)  -1\right)
\cdots\left(  \left(  X-a\right)  -\left(  i-a\right)  +1\right)  }{\left(
i-a\right)  !}\\
&  =\dfrac{1}{a!\cdot\left(  i-a\right)  !}\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\underbrace{\left(  X\left(  X-1\right)
\cdots\left(  X-a+1\right)  \right)  \cdot\left(  \left(  X-a\right)  \left(
\left(  X-a\right)  -1\right)  \cdots\left(  \left(  X-a\right)  -\left(
i-a\right)  +1\right)  \right)  }_{\substack{=X\left(  X-1\right)
\cdots\left(  \left(  X-a\right)  -\left(  i-a\right)  +1\right)  \\=X\left(
X-1\right)  \cdots\left(  X-i+1\right)  \\\text{(since }\left(  X-a\right)
-\left(  i-a\right)  =X-i\text{)}}}\\
&  =\dfrac{1}{a!\cdot\left(  i-a\right)  !}\cdot X\left(  X-1\right)
\cdots\left(  X-i+1\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \underbrace{\dbinom{X}{i}}_{\substack{=\dfrac{X\left(  X-1\right)
\cdots\left(  X-i+1\right)  }{i!}\\\text{(by the definition of }\dbinom{X}%
{i}\text{)}}}\underbrace{\dbinom{i}{a}}_{\substack{=\dfrac{i!}{a!\left(
i-a\right)  !}\\\text{(by (\ref{eq.binom.formula}), applied to }m=i\text{ and
}n=a\text{)}}}\\
&  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-i+1\right)  }{i!}\cdot
\dfrac{i!}{a!\left(  i-a\right)  !}=\dfrac{1}{a!\cdot\left(  i-a\right)
!}\cdot X\left(  X-1\right)  \cdots\left(  X-i+1\right)  ,
\end{align*}
this yields $\dbinom{X}{i}\dbinom{i}{a}=\dbinom{X}{a}\dbinom{X-a}{i-a}$. This
proves Proposition \ref{prop.binom.trinom-rev} \textbf{(a)}.

[Notice that we used (\ref{eq.binom.formula}) to simplify $\dbinom{i}{a}$ in
this proof. Do not be tempted to use (\ref{eq.binom.formula}) to simplify
$\dbinom{X}{i}$, $\dbinom{X}{a}$ and $\dbinom{X-a}{i-a}$: The $X$ in these
expressions is a polynomial indeterminate, and (\ref{eq.binom.formula}) cannot
be applied to it!]

\textbf{(b)} Let $m\in\mathbb{Z}$, $a\in\mathbb{N}$ and $i\in\mathbb{N}$ be
such that $i\geq a$. The formula (\ref{eq.binom.trinom-rev.m}) is obtained
from (\ref{eq.binom.trinom-rev}) by substituting $m$ for $X$. This proves
Proposition \ref{prop.binom.trinom-rev} \textbf{(b)}.
\end{proof}

Proposition \ref{prop.binom.trinom-rev} \textbf{(b)} is a simple and yet
highly useful formula, which Graham, Knuth and Patashnik call
\textit{trinomial revision} in \cite[Table 174]{GKP}.

\subsection{Binomial coefficients and polynomials}

Recall that any polynomial $P\in\mathbb{Q}\left[  X\right]  $ (that is, any
polynomial in the indeterminate $X$ with rational coefficients) can be
quasi-uniquely written in the form $P\left(  X\right)  =\sum_{i=0}^{d}%
c_{i}X^{i}$ with rational $c_{0},c_{1},\ldots,c_{d}$. The word
\textquotedblleft quasi-uniquely\textquotedblright\ here means that the
coefficients $c_{0},c_{1},\ldots,c_{d}$ are uniquely determined when
$d\in\mathbb{N}$ is specified; they are not literally unique because we can
always increase $d$ by adding new $0$ coefficients (for example, the
polynomial $\left(  1+X\right)  ^{2}$ can be written both as $1+2X+X^{2}$ and
as $1+2X+X^{2}+0X^{3}+0X^{4}$).

It is not hard to check that an analogue of this statement holds with the
$X^{i}$ replaced by the $\dbinom{X}{i}$:

\begin{proposition}
\label{prop.hartshorne}\textbf{(a)} Any polynomial $P\in\mathbb{Q}\left[
X\right]  $ can be quasi-uniquely written in the form $P\left(  X\right)
=\sum_{i=0}^{d}c_{i}\dbinom{X}{i}$ with rational $c_{0},c_{1},\ldots,c_{d}$.
(Again, \textquotedblleft quasi-uniquely\textquotedblright\ means that we can
always increase $d$ by adding new $0$ coefficients, but apart from this the
$c_{0},c_{1},\ldots,c_{d}$ are uniquely determined.)

\textbf{(b)} The polynomial $P$ is \textit{integer-valued} (i.e., its values
at integers are integers) if and only if these rationals $c_{0},c_{1}%
,\ldots,c_{d}$ are integers.
\end{proposition}

(We will not use this fact below, but it gives context to Theorem
\ref{thm.vandermonde} and Exercise \ref{exe.ps1.1.2} further below. The
\textquotedblleft if\textquotedblright\ part of Proposition
\ref{prop.hartshorne} \textbf{(b)} follows from (\ref{eq.binom.int}).)

We shall now prove some facts and give some exercises about binomial
coefficients; but let us first prove a fundamental property of polynomials:

\begin{lemma}
\label{lem.polyeq}\textbf{(a)} Let $P$ be a polynomial in the indeterminate
$X$ with rational coefficients. Assume that $P\left(  x\right)  =0$ for all
$x\in\mathbb{N}$. Then, $P=0$ as polynomials\footnotemark.

\textbf{(b)} Let $P$ and $Q$ be two polynomials in the indeterminate $X$ with
rational coefficients. Assume that $P\left(  x\right)  =Q\left(  x\right)  $
for all $x\in\mathbb{N}$. Then, $P=Q$ as polynomials.

\textbf{(c)} Let $P$ be a polynomial in the indeterminates $X$ and $Y$ with
rational coefficients. Assume that $P\left(  x,y\right)  =0$ for all
$x\in\mathbb{N}$ and $y\in\mathbb{N}$. Then, $P=0$ as polynomials.

\textbf{(d)} Let $P$ and $Q$ be two polynomials in the indeterminates $X$ and
$Y$ with rational coefficients. Assume that $P\left(  x,y\right)  =Q\left(
x,y\right)  $ for all $x\in\mathbb{N}$ and $y\in\mathbb{N}$. Then, $P=Q$ as polynomials.
\end{lemma}

\footnotetext{Recall that two polynomials are said to be equal if and only if
their respective coefficients are equal.}

Probably you have seen this lemma proven at least once in your life, but let
me still prove it for the sake of completeness.

\begin{proof}
[Proof of Lemma \ref{lem.polyeq}.]\textbf{(a)} The polynomial $P$ satisfies
$P\left(  x\right)  =0$ for every $x\in\mathbb{N}$. Hence, every
$x\in\mathbb{N}$ is a root of $P$. Thus, the polynomial $P$ has infinitely
many roots. But a nonzero polynomial in one variable (with rational
coefficients) can only have finitely many roots\footnote{In fact, a stronger
statement holds: A nonzero polynomial in one variable (with rational
coefficients) having degree $n\geq0$ has at most $n$ roots. See, for example,
\cite[Corollary 1.8.24]{Goodman} for a proof.}. If $P$ was nonzero, this would
force a contradiction with the sentence before. So $P$ must be zero. In other
words, $P=0$. Lemma \ref{lem.polyeq} \textbf{(a)} is proven.

\textbf{(b)} Every $x\in\mathbb{N}$ satisfies $\left(  P-Q\right)  \left(
x\right)  =P\left(  x\right)  -Q\left(  x\right)  =0$ (since $P\left(
x\right)  =Q\left(  x\right)  $). Hence, Lemma \ref{lem.polyeq} \textbf{(a)}
(applied to $P-Q$ instead of $P$) yields $P-Q=0$. Thus, $P=Q$. Lemma
\ref{lem.polyeq} \textbf{(b)} is thus proven.

\textbf{(c)} Every $x\in\mathbb{N}$ and $y\in\mathbb{N}$ satisfy%
\begin{equation}
P\left(  x,y\right)  =0. \label{pf.lem.polyeq.b.2}%
\end{equation}


We can write the polynomial $P$ in the form $P=\sum_{k=0}^{d}P_{k}\left(
X\right)  Y^{k}$, where $d$ is an integer and where each $P_{k}\left(
X\right)  $ (for $0\leq k\leq d$) is a polynomial in the single variable $X$.
Consider this $d$ and these $P_{k}\left(  X\right)  $.

Fix $\alpha\in\mathbb{N}$. Every $x\in\mathbb{N}$ satisfies%
\begin{align*}
P\left(  \alpha,x\right)   &  =\sum_{k=0}^{d}P_{k}\left(  \alpha\right)
x^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }\alpha\text{ and
}x\text{ for }X\text{ and }Y\text{ in }P=\sum_{k=0}^{d}P_{k}\left(  X\right)
Y^{k}\right)  ,
\end{align*}
so that $\sum_{k=0}^{d}P_{k}\left(  \alpha\right)  x^{k}=P\left(
\alpha,x\right)  =0$ (by (\ref{pf.lem.polyeq.b.2}), applied to $\alpha$ and
$x$ instead of $x$ and $y$).

Therefore, Lemma \ref{lem.polyeq} \textbf{(a)} (applied to $\sum_{k=0}%
^{d}P_{k}\left(  \alpha\right)  X^{k}$ instead of $P$) yields that
\[
\sum_{k=0}^{d}P_{k}\left(  \alpha\right)  X^{k}=0
\]
as polynomials (in the indeterminate $X$). In other words, all coefficients of
the polynomial $\sum_{k=0}^{d}P_{k}\left(  \alpha\right)  X^{k}$ are $0$. In
other words, $P_{k}\left(  \alpha\right)  =0$ for all $k\in\left\{
0,1,\ldots,d\right\}  $.

Now, let us forget that we fixed $\alpha$. We thus have shown that
$P_{k}\left(  \alpha\right)  =0$ for all $k\in\left\{  0,1,\ldots,d\right\}  $
and $\alpha\in\mathbb{N}$.

Let us now fix $k\in\left\{  0,1,\ldots,d\right\}  $. Then, $P_{k}\left(
\alpha\right)  =0$ for all $\alpha\in\mathbb{N}$. In other words,
$P_{k}\left(  x\right)  =0$ for all $x\in\mathbb{N}$. Hence, Lemma
\ref{lem.polyeq} \textbf{(a)} (applied to $P=P_{k}$) yields that $P_{k}=0$ as polynomials.

Let us forget that we fixed $k$. We thus have proven that $P_{k}=0$ as
polynomials for each $k\in\left\{  0,1,\ldots,d\right\}  $. Hence,
$P=\sum_{k=0}^{d}\underbrace{P_{k}\left(  X\right)  }_{=0}Y^{k}=0$. This
proves Lemma \ref{lem.polyeq} \textbf{(c)}.

\textbf{(d)} Every $x\in\mathbb{N}$ and $y\in\mathbb{N}$ satisfy%
\[
\left(  P-Q\right)  \left(  x,y\right)  =P\left(  x,y\right)  -Q\left(
x,y\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\left(  x,y\right)
=Q\left(  x,y\right)  \right)  .
\]
Hence, Lemma \ref{lem.polyeq} \textbf{(c)} (applied to $P-Q$ instead of $P$)
yields $P-Q=0$. Thus, $P=Q$. Lemma \ref{lem.polyeq} \textbf{(d)} is proven.
\end{proof}

Of course, Lemma \ref{lem.polyeq} can be generalized to polynomials in more
than two variables (the proof of Lemma \ref{lem.polyeq} \textbf{(c)}
essentially suggests how to prove this generalization by induction over the
number of variables).\footnote{If you know what a commutative ring is, you
might wonder whether Lemma \ref{lem.polyeq} can also be generalized to
polynomials with coefficients from other commutative rings (e.g., from
$\mathbb{R}$ or $\mathbb{C}$) instead of rational coefficients. In other
words, what happens if we replace \textquotedblleft rational
coefficients\textquotedblright\ by \textquotedblleft coefficients in
$R$\textquotedblright\ throughout Lemma \ref{lem.polyeq}, where $R$ is some
commutative ring? (Of course, we will then have to also replace $P\left(
x\right)  $ by $P\left(  x\cdot1_{R}\right)  $ and so on.)
\par
The answer is that Lemma \ref{lem.polyeq} becomes generally false if we don't
require anything more specific on $R$. However, there are certain conditions
on $R$ that make Lemma \ref{lem.polyeq} remain valid. For instance, Lemma
\ref{lem.polyeq} remains valid for $R=\mathbb{Z}$, for $R=\mathbb{R}$ and for
$R=\mathbb{C}$, as well as for $R$ being any polynomial ring over $\mathbb{Z}%
$, $\mathbb{Q}$, $\mathbb{R}$ or $\mathbb{C}$. More generally, Lemma
\ref{lem.polyeq} is valid if $R$ is any field of characteristic $0$ (i.e., any
field such that the elements $n\cdot1_{R}$ for $n$ ranging over $\mathbb{N}$
are pairwise distinct), or any subring of such a field.}

\subsection{The Chu-Vandermonde identity}

The following fact is known as the \textit{Chu-Vandermonde identity}%
\footnote{See
\href{https://en.wikipedia.org/wiki/Vandermonde\%27s_identity\%23Chu-Vandermonde_identity}{the
Wikipedia page} for part of its history. Usually, the equality $\dbinom
{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}$ for two
\textbf{nonnegative integers} $x$ and $y$ is called the \textit{Vandermonde
identity}, whereas the name \textquotedblleft\textit{Chu-Vandermonde
identity}\textquotedblright\ is used for the identity $\dbinom{X+Y}{n}%
=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}$ in which $X$ and $Y$ are
\textbf{indeterminates}. However, this seems to be mostly a matter of
convention (which isn't even universally followed); and anyway the two
identities are easily derived from one another as we will see in the first
proof of Theorem \ref{thm.vandermonde}.}:

\begin{theorem}
\label{thm.vandermonde}Let $n\in\mathbb{N}$. Then,%
\[
\dbinom{X+Y}{n}=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}%
\]
(an equality between polynomials in two variables $X$ and $Y$).
\end{theorem}

We will give two proofs of this theorem: one combinatorial, and one
algebraic.\footnote{Note that Theorem~\ref{thm.vandermonde} appears in
\cite[(5.27)]{GKP}, where it is called \textit{Vandermonde's convolution}. The
first proof of Theorem~\ref{thm.vandermonde} we shall show below is just a
more detailed writeup of the proof given there.}

\begin{proof}
[First proof of Theorem \ref{thm.vandermonde}.]Let us first show that%
\begin{equation}
\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}
\label{pf.thm.vandermonde.eq.xy}%
\end{equation}
for any $x\in\mathbb{N}$ and $y\in\mathbb{N}$.

Keep in mind that (\ref{pf.thm.vandermonde.eq.xy}) and Theorem
\ref{thm.vandermonde} are different claims: The $x$ and $y$ in
(\ref{pf.thm.vandermonde.eq.xy}) are nonnegative integers, while the $X$ and
$Y$ in Theorem \ref{thm.vandermonde} are indeterminates!

\textit{Proof of (\ref{pf.thm.vandermonde.eq.xy}):} For every $N\in\mathbb{N}%
$, we let $\left[  N\right]  $ denote the $N$-element set $\left\{
1,2,\ldots,N\right\}  $.

Let $x\in\mathbb{N}$ and $y\in\mathbb{N}$. Recall that $\dbinom{x+y}{n}$ is
the number of $n$-element subsets of a given $\left(  x+y\right)  $-element
set\footnote{This follows from (\ref{eq.binom.subsets}).}. Since $\left[
x+y\right]  $ is an $\left(  x+y\right)  $-element set, we thus conclude that
$\dbinom{x+y}{n}$ is the number of $n$-element subsets of $\left[  x+y\right]
$.

But let us count the $n$-element subsets of $\left[  x+y\right]  $ in a
different way (i.e., find a different expression for the number of $n$-element
subsets of $\left[  x+y\right]  $). Namely, we can choose an $n$-element
subset $S$ of $\left[  x+y\right]  $ by means of the following process:

\begin{enumerate}
\item We decide how many elements of this subset $S$ will be among the numbers
$1,2,\ldots,x$. Let $k$ be the number of these elements. Clearly, $k$ must be
an integer between $0$ and $n$ (inclusive)\footnote{Because the subset $S$
will have $n$ elements in total, and thus at most $n$ of them can be among the
numbers $1,2,\ldots,x$.}.

\item Then, we choose these $k$ elements of $S$ among the numbers
$1,2,\ldots,x$. This can be done in $\dbinom{x}{k}$ different ways (because we
are choosing $k$ out of $x$ numbers, with no repetitions, and with no regard
for their order; in other words, we are choosing a $k$-element subset of
$\left\{  1,2,\ldots,x\right\}  $).

\item Then, we choose the remaining $n-k$ elements of $S$ (because $S$ should
have $n$ elements in total) among the remaining numbers $x+1,x+2,\ldots,x+y$.
This can be done in $\dbinom{y}{n-k}$ ways (because we are choosing $n-k$ out
of $y$ numbers, with no repetitions, and with no regard for their order).
\end{enumerate}

This process makes it clear that the total number of ways to choose an
$n$-element subset $S$ of $\left[  x+y\right]  $ is $\sum_{k=0}^{n}\dbinom
{x}{k}\dbinom{y}{n-k}$. In other words, the number of $n$-element subsets of
$\left[  x+y\right]  $ is $\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}$. But
earlier, we have shown that the same number is $\dbinom{x+y}{n}$. Comparing
these two results, we conclude that $\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom
{x}{k}\dbinom{y}{n-k}$. Thus, (\ref{pf.thm.vandermonde.eq.xy}) is proven.

Now, we need to prove Theorem \ref{thm.vandermonde} itself. We define two
polynomials $P$ and $Q$ in the indeterminates $X$ and $Y$ with rational
coefficients by setting%
\begin{align*}
P  &  =\dbinom{X+Y}{n};\\
Q  &  =\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}%
\end{align*}
\footnote{These are both polynomials since $\dbinom{X+Y}{n}$, $\dbinom{X}{k}$
and $\dbinom{Y}{n-k}$ are polynomials in $X$ and $Y$.}. The equality
(\ref{pf.thm.vandermonde.eq.xy}) (which we have proven) states that $P\left(
x,y\right)  =Q\left(  x,y\right)  $ for all $x\in\mathbb{N}$ and
$y\in\mathbb{N}$. Thus, Lemma \ref{lem.polyeq} \textbf{(d)} yields that $P=Q$.
Recalling how $P$ and $Q$ are defined, we can rewrite this as $\dbinom{X+Y}%
{n}=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}$. This proves Theorem
\ref{thm.vandermonde}.
\end{proof}

The argument that we used at the end of the above proof to derive Theorem
\ref{thm.vandermonde} from (\ref{pf.thm.vandermonde.eq.xy}) is a very common
argument that appears in proofs of equalities for formal binomial
coefficients. Formal binomial coefficients $\dbinom{X}{n}$ are polynomials
that can be evaluated at many different values\footnote{For example, terms
like $\dbinom{-1/2}{3}$, $\dbinom{2+\sqrt{3}}{5}$ and $\dbinom{-7}{0}$ make
perfect sense. Generally, $\dbinom{X}{n}$ is a polynomial in $X$ with rational
coefficients, and thus we can substitute any complex number for $X$. So
$\dbinom{m}{n}$ is well-defined for all $m\in\mathbb{C}$ and $n\in\mathbb{N}$.
(But we cannot substitute arbitrary complex numbers for $n$. So far we have
only defined $\dbinom{X}{n}$ for $n\in\mathbb{N}$. It is usual to define
$\dbinom{X}{n}$ to mean $0$ for negative integers $n$, and using analysis
(specifically, the $\Gamma$ function) it is possible to give a reasonable
meaning to $\dbinom{m}{n}$ for $m$ and $n$ being reals, but this will no
longer be a polynomial in $m$.)}, but their combinatorial interpretation (via
counting subsets) only makes sense when they are evaluated at nonnegative
integers. Thus, if we want to prove an identity of the form $P=Q$ (where $P$
and $Q$ are two polynomials, say, in two indeterminates $X$ and $Y$) using the
combinatorial interpretation of binomial coefficients, then a reasonable
tactic is to first show that $P\left(  x,y\right)  =Q\left(  x,y\right)  $ for
all $x\in\mathbb{N}$ and $y\in\mathbb{N}$ (using combinatorics), and then to
use something like Lemma \ref{lem.polyeq} in order to conclude that $P$ and
$Q$ are equal as polynomials. We shall see this tactic used a few more
times.\footnote{This tactic is called \textquotedblleft the polynomial
argument\textquotedblright\ in \cite[\S 5.1]{GKP}.}

But we promised an algebraic proof of Theorem \ref{thm.vandermonde} as well;
let us show it now:

\begin{proof}
[Second proof of Theorem \ref{thm.vandermonde}.]We shall prove Theorem
\ref{thm.vandermonde} by induction over $n$.

\textit{Induction base:} We have $\dbinom{X}{0}=1$. Substituting $Y$ for $X$
in this equality, we obtain $\dbinom{Y}{0}=1$. Hence,
\begin{equation}
\sum_{k=0}^{0}\dbinom{X}{k}\dbinom{Y}{0-k}=\underbrace{\dbinom{X}{0}}%
_{=1}\underbrace{\dbinom{Y}{0-0}}_{=\dbinom{Y}{0}=1}=1.
\label{pf.thm.vandermonde.pf.2.1}%
\end{equation}


But substituting $X+Y$ for $X$ in $\dbinom{X}{0}=1$, we obtain $\dbinom
{X+Y}{0}=1$. Compared with (\ref{pf.thm.vandermonde.pf.2.1}), this yields
$\dbinom{X+Y}{0}=\sum_{k=0}^{0}\dbinom{X}{k}\dbinom{Y}{0-k}$. In other words,
Theorem \ref{thm.vandermonde} holds for $n=0$. This completes the induction base.

\textit{Induction step:} Let $N$ be a positive integer. Assume that Theorem
\ref{thm.vandermonde} holds for $n=N-1$. We need to prove that Theorem
\ref{thm.vandermonde} holds for $n=N$. In other words, we need to prove that%
\begin{equation}
\dbinom{X+Y}{N}=\sum_{k=0}^{N}\dbinom{X}{k}\dbinom{Y}{N-k}.
\label{pf.thm.vandermonde.pf.2.goal}%
\end{equation}
(Don't be fooled by the $N$ being uppercase! The $X$ and the $Y$ are
indeterminates, while the $N$ is a fixed positive integer.)

We have assumed that Theorem \ref{thm.vandermonde} holds for $n=N-1$. In other
words, we have%
\begin{equation}
\dbinom{X+Y}{N-1}=\sum_{k=0}^{N-1}\dbinom{X}{k}\dbinom{Y}{\left(  N-1\right)
-k}. \label{pf.thm.vandermonde.pf.2.4}%
\end{equation}


Substituting $X-1$ for $X$ in this equality, we obtain%
\begin{align*}
\dbinom{X-1+Y}{N-1}  &  =\sum_{k=0}^{N-1}\dbinom{X-1}{k}\dbinom{Y}{\left(
N-1\right)  -k}\\
&  =\sum_{k=1}^{N}\dbinom{X-1}{k-1}\underbrace{\dbinom{Y}{\left(  N-1\right)
-\left(  k-1\right)  }}_{=\dbinom{Y}{N-k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k-1\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=1}^{N}\dbinom{X-1}{k-1}\dbinom{Y}{N-k}.
\end{align*}
Since $X-1+Y=X+Y-1$, this rewrites as%
\begin{equation}
\dbinom{X+Y-1}{N-1}=\sum_{k=1}^{N}\dbinom{X-1}{k-1}\dbinom{Y}{N-k}.
\label{pf.thm.vandermonde.pf.2.5}%
\end{equation}
On the other hand, we can substitute $Y-1$ for $Y$ in the equality
(\ref{pf.thm.vandermonde.pf.2.4}). As a result, we obtain%
\begin{equation}
\dbinom{X+Y-1}{N-1}=\sum_{k=0}^{N-1}\dbinom{X}{k}\underbrace{\dbinom
{Y-1}{\left(  N-1\right)  -k}}_{=\dbinom{Y-1}{N-k-1}}=\sum_{k=0}^{N-1}%
\dbinom{X}{k}\dbinom{Y-1}{N-k-1}. \label{pf.thm.vandermonde.pf.2.6}%
\end{equation}


Next, we notice a simple consequence of (\ref{eq.binom.X-1}): We have%
\begin{equation}
\dfrac{X}{N}\dbinom{X-1}{a-1}=\dfrac{a}{N}\dbinom{X}{a}%
\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\left\{  1,2,3,\ldots\right\}
\label{pf.thm.vandermonde.pf.2.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.vandermonde.pf.2.7}):} Let
$a\in\left\{  1,2,3,\ldots\right\}  $. Then, (\ref{eq.binom.X-1}) (applied to
$n=a$) yields $\dbinom{X}{a}=\dfrac{X}{a}\dbinom{X-1}{a-1}$. Hence,%
\[
\dfrac{a}{N}\underbrace{\dbinom{X}{a}}_{=\dfrac{X}{a}\dbinom{X-1}{a-1}%
}=\underbrace{\dfrac{a}{N}\cdot\dfrac{X}{a}}_{=\dfrac{X}{N}}\dbinom{X-1}%
{a-1}=\dfrac{X}{N}\dbinom{X-1}{a-1}.
\]
This proves (\ref{pf.thm.vandermonde.pf.2.7}).}. Also,%
\begin{equation}
\dfrac{Y}{N}\dbinom{Y-1}{a-1}=\dfrac{a}{N}\dbinom{Y}{a}%
\ \ \ \ \ \ \ \ \ \ \text{for every }a\in\left\{  1,2,3,\ldots\right\}  .
\label{pf.thm.vandermonde.pf.2.8}%
\end{equation}
(This follows by substituting $Y$ for $X$ in (\ref{pf.thm.vandermonde.pf.2.7}).)

We have%
\begin{align*}
\dfrac{X}{N}\underbrace{\dbinom{X+Y-1}{N-1}}_{\substack{=\sum_{k=1}^{N}%
\dbinom{X-1}{k-1}\dbinom{Y}{N-k}\\\text{(by (\ref{pf.thm.vandermonde.pf.2.5}%
))}}}  &  =\dfrac{X}{N}\sum_{k=1}^{N}\dbinom{X-1}{k-1}\dbinom{Y}{N-k}\\
&  =\sum_{k=1}^{N}\underbrace{\dfrac{X}{N}\dbinom{X-1}{k-1}}%
_{\substack{=\dfrac{k}{N}\dbinom{X}{k}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.7}),}\\\text{applied to }a=k\text{)}}}\dbinom
{Y}{N-k}=\sum_{k=1}^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}.
\end{align*}
Compared with%
\begin{align*}
&  \sum_{k=0}^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}\\
&  =\underbrace{\dfrac{0}{N}}_{=0}\dbinom{X}{0}\dbinom{Y}{N-0}+\sum_{k=1}%
^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}=\sum_{k=1}^{N}\dfrac{k}{N}%
\dbinom{X}{k}\dbinom{Y}{N-k},
\end{align*}
this yields%
\begin{equation}
\dfrac{X}{N}\dbinom{X+Y-1}{N-1}=\sum_{k=0}^{N}\dfrac{k}{N}\dbinom{X}{k}%
\dbinom{Y}{N-k}. \label{pf.thm.vandermonde.pf.2.11}%
\end{equation}


We also have%
\begin{align*}
\dfrac{Y}{N}\underbrace{\dbinom{X+Y-1}{N-1}}_{\substack{=\sum_{k=0}%
^{N-1}\dbinom{X}{k}\dbinom{Y-1}{N-k-1}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.6}))}}}  &  =\dfrac{Y}{N}\sum_{k=0}^{N-1}%
\dbinom{X}{k}\dbinom{Y-1}{N-k-1}\\
&  =\sum_{k=0}^{N-1}\dbinom{X}{k}\underbrace{\dfrac{Y}{N}\dbinom{Y-1}{N-k-1}%
}_{\substack{=\dfrac{N-k}{N}\dbinom{Y}{N-k}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.8}),}\\\text{applied to }a=N-k\text{)}}}\\
&  =\sum_{k=0}^{N-1}\dbinom{X}{k}\dfrac{N-k}{N}\dbinom{Y}{N-k}=\sum
_{k=0}^{N-1}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}.
\end{align*}
Compared with%
\begin{align*}
\sum_{k=0}^{N}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}  &  =\sum_{k=0}%
^{N-1}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}+\underbrace{\dfrac{N-N}{N}%
}_{=0}\dbinom{X}{N}\dbinom{Y}{N-N}\\
&  =\sum_{k=0}^{N-1}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k},
\end{align*}
this yields%
\begin{equation}
\dfrac{Y}{N}\dbinom{X+Y-1}{N-1}=\sum_{k=0}^{N}\dfrac{N-k}{N}\dbinom{X}%
{k}\dbinom{Y}{N-k}. \label{pf.thm.vandermonde.pf.2.12}%
\end{equation}


Now, (\ref{eq.binom.X-1}) (applied to $n=N$) yields%
\[
\dbinom{X}{N}=\dfrac{X}{N}\dbinom{X-1}{N-1}.
\]
Substituting $X+Y$ for $X$ in this equality, we obtain%
\begin{align*}
\dbinom{X+Y}{N}  &  =\underbrace{\dfrac{X+Y}{N}}_{=\dfrac{X}{N}+\dfrac{Y}{N}%
}\dbinom{X+Y-1}{N-1}=\left(  \dfrac{X}{N}+\dfrac{Y}{N}\right)  \dbinom
{X+Y-1}{N-1}\\
&  =\underbrace{\dfrac{X}{N}\dbinom{X+Y-1}{N-1}}_{\substack{=\sum_{k=0}%
^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}\\\text{(by
(\ref{pf.thm.vandermonde.pf.2.11}))}}}+\underbrace{\dfrac{Y}{N}\dbinom
{X+Y-1}{N-1}}_{\substack{=\sum_{k=0}^{N}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom
{Y}{N-k}\\\text{(by (\ref{pf.thm.vandermonde.pf.2.12}))}}}\\
&  =\sum_{k=0}^{N}\dfrac{k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}+\sum_{k=0}%
^{N}\dfrac{N-k}{N}\dbinom{X}{k}\dbinom{Y}{N-k}\\
&  =\sum_{k=0}^{N}\left(  \underbrace{\dfrac{k}{N}+\dfrac{N-k}{N}}%
_{=1}\right)  \dbinom{X}{k}\dbinom{Y}{N-k}=\sum_{k=0}^{N}\dbinom{X}{k}%
\dbinom{Y}{N-k}.
\end{align*}
This proves (\ref{pf.thm.vandermonde.pf.2.goal}). In other words, Theorem
\ref{thm.vandermonde} holds for $n=N$. This completes the induction step.
Thus, the induction proof of Theorem \ref{thm.vandermonde} is complete.
\end{proof}

Let us give some sample applications of Theorem \ref{thm.vandermonde}:

\begin{proposition}
\label{prop.vandermonde.consequences}

\textbf{(a)} For every $x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and
$n\in\mathbb{N}$, we have
\[
\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom{y}{n-k}.
\]


\textbf{(b)} For every $x\in\mathbb{N}$ and $y\in\mathbb{Z}$, we have%
\[
\dbinom{x+y}{x}=\sum_{k=0}^{x}\dbinom{x}{k}\dbinom{y}{k}.
\]


\textbf{(c)} For every $n\in\mathbb{N}$, we have%
\[
\dbinom{2n}{n}=\sum_{k=0}^{n}\dbinom{n}{k}^{2}.
\]


\textbf{(d)} For every $x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and
$n\in\mathbb{N}$, we have%
\[
\dbinom{x-y}{n}=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{n-k}%
\dbinom{k+y-1}{k}.
\]


\textbf{(e)} For every $x\in\mathbb{N}$ and $y\in\mathbb{Z}$ and
$n\in\mathbb{N}$ with $x\leq n$, we have%
\[
\dbinom{y-x-1}{n-x}=\sum_{k=0}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}%
{x}\dbinom{y}{n-k}.
\]


\textbf{(f)} For every $x\in\mathbb{N}$ and $y\in\mathbb{N}$ and
$n\in\mathbb{N}$, we have%
\[
\dbinom{n+1}{x+y+1}=\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}.
\]


\textbf{(g)} For every $x\in\mathbb{Z}$ and $y\in\mathbb{N}$ and
$n\in\mathbb{N}$ satisfying $x+y\geq0$ and $n\geq x$, we have%
\[
\dbinom{x+y}{n}=\sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}.
\]

\end{proposition}

\begin{remark}
I have learnt Proposition \ref{prop.vandermonde.consequences} \textbf{(f)}
from \href{http://www.artofproblemsolving.com/community/c6h447278}{the AoPS
forum}. Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} is a
generalization of Proposition \ref{prop.vandermonde.consequences} \textbf{(b)}.
\end{remark}

\begin{proof}
[Proof of Proposition \ref{prop.vandermonde.consequences}.]\textbf{(a)} Let
$x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and $n\in\mathbb{N}$. Theorem
\ref{thm.vandermonde} yields $\dbinom{X+Y}{n}=\sum_{k=0}^{n}\dbinom{X}%
{k}\dbinom{Y}{n-k}$. Substituting $x$ and $y$ for $X$ and $Y$ in this
equality, we obtain $\dbinom{x+y}{n}=\sum_{k=0}^{n}\dbinom{x}{k}\dbinom
{y}{n-k}$. Thus, Proposition \ref{prop.vandermonde.consequences} \textbf{(a)}
is proven.

\textbf{(b)} Let $x\in\mathbb{N}$ and $y\in\mathbb{Z}$. Proposition
\ref{prop.vandermonde.consequences} \textbf{(a)} (applied to $y$, $x$ and $x$
instead of $x$, $y$ and $n$) yields%
\[
\dbinom{y+x}{x}=\sum_{k=0}^{x}\dbinom{y}{k}\dbinom{x}{x-k}.
\]
Compared with
\[
\sum_{k=0}^{x}\underbrace{\dbinom{x}{k}}_{\substack{=\dbinom{x}{x-k}%
\\\text{(by (\ref{eq.binom.symm}), applied to }m=x\text{ and }n=k\text{)}%
}}\dbinom{y}{k}=\sum_{k=0}^{x}\dbinom{x}{x-k}\dbinom{y}{k}=\sum_{k=0}%
^{x}\dbinom{y}{k}\dbinom{x}{x-k},
\]
this yields $\dbinom{y+x}{x}=\sum_{k=0}^{x}\dbinom{x}{k}\dbinom{y}{k}$. Since
$y+x=x+y$, this rewrites as $\dbinom{x+y}{x}=\sum_{k=0}^{x}\dbinom{x}%
{k}\dbinom{y}{k}$. This proves Proposition \ref{prop.vandermonde.consequences}
\textbf{(b)}.

\textbf{(c)} Let $n\in\mathbb{N}$. Applying Proposition
\ref{prop.vandermonde.consequences} \textbf{(b)} to $x=n$ and $y=n$, we obtain%
\[
\dbinom{n+n}{n}=\sum_{k=0}^{n}\underbrace{\dbinom{n}{k}\dbinom{n}{k}%
}_{=\dbinom{n}{k}^{2}}=\sum_{k=0}^{n}\dbinom{n}{k}^{2}.
\]
Since $n+n=2n$, this rewrites as $\dbinom{2n}{n}=\sum_{k=0}^{n}\dbinom{n}%
{k}^{2}$. This proves Proposition \ref{prop.vandermonde.consequences}
\textbf{(c)}.

\textbf{(d)} Let $x\in\mathbb{Z}$ and $y\in\mathbb{Z}$ and $n\in\mathbb{N}$.
Proposition \ref{prop.vandermonde.consequences} \textbf{(a)} (applied to $-y$
instead of $y$) yields%
\begin{align*}
\dbinom{x+\left(  -y\right)  }{n}  &  =\sum_{k=0}^{n}\dbinom{x}{k}\dbinom
{-y}{n-k}=\sum_{k=0}^{n}\dbinom{x}{n-k}\underbrace{\dbinom{-y}{n-\left(
n-k\right)  }}_{=\dbinom{-y}{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }n-k\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=0}^{n}\dbinom{x}{n-k}\underbrace{\dbinom{-y}{k}}%
_{\substack{=\left(  -1\right)  ^{k}\dbinom{k-\left(  -y\right)  -1}%
{k}\\\text{(by (\ref{eq.binom.upper-neg}), applied to }k\text{ and }-y\text{
instead of }n\text{ and }m\text{)}}}\\
&  =\sum_{k=0}^{n}\underbrace{\dbinom{x}{n-k}\left(  -1\right)  ^{k}%
}_{=\left(  -1\right)  ^{k}\dbinom{x}{n-k}}\underbrace{\dbinom{k-\left(
-y\right)  -1}{k}}_{=\dbinom{k+y-1}{k}}\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{n-k}\dbinom{k+y-1}{k}.
\end{align*}
Since $x+\left(  -y\right)  =x-y$, this rewrites as $\dbinom{x-y}{n}%
=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{n-k}\dbinom{k+y-1}{k}$. This
proves Proposition \ref{prop.vandermonde.consequences} \textbf{(d)}.

\textbf{(e)} Let $x\in\mathbb{N}$ and $y\in\mathbb{Z}$ and $n\in\mathbb{N}$ be
such that $x\leq n$. From $x\in\mathbb{N}$, we obtain $0\leq x$ and thus
$0\leq x\leq n$. We notice that every integer $k\geq x$ satisfies%
\begin{equation}
\dbinom{k}{k-x}=\dbinom{k}{x} \label{pf.prop.vandermonde.consequences.e.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.vandermonde.consequences.e.1}):} Let
$k$ be an integer such that $k\geq x$. Thus, $k-x\in\mathbb{N}$. Recall that
$k\geq x$. Hence, (\ref{eq.binom.symm}) (applied to $k$ and $x$ instead of $m$
and $n$) yields $\dbinom{k}{x}=\dbinom{k}{k-x}$. This proves
(\ref{pf.prop.vandermonde.consequences.e.1}).}. Furthermore, $n-x\in
\mathbb{N}$ (since $x\leq n$). Hence, we can apply Proposition
\ref{prop.vandermonde.consequences} \textbf{(a)} to $y$, $-x-1$ and $n-x$
instead of $x$, $y$ and $n$. As a result, we obtain%
\begin{align*}
\dbinom{y+\left(  -x-1\right)  }{n-x}  &  =\sum_{k=0}^{n-x}\dbinom{y}%
{k}\underbrace{\dbinom{-x-1}{\left(  n-x\right)  -k}}_{\substack{=\left(
-1\right)  ^{\left(  n-x\right)  -k}\dbinom{\left(  \left(  n-x\right)
-k\right)  -\left(  -x-1\right)  -1}{\left(  n-x\right)  -k}\\\text{(by
(\ref{eq.binom.upper-neg}), applied to }-x-1\text{ and }\left(  n-x\right)
-k\\\text{instead of }m\text{ and }n\text{)}}}\\
&  =\sum_{k=0}^{n-x}\dbinom{y}{k}\left(  -1\right)  ^{\left(  n-x\right)
-k}\underbrace{\dbinom{\left(  \left(  n-x\right)  -k\right)  -\left(
-x-1\right)  -1}{\left(  n-x\right)  -k}}_{\substack{=\dbinom{n-k}{\left(
n-x\right)  -k}\\\text{(since }\left(  \left(  n-x\right)  -k\right)  -\left(
-x-1\right)  -1=n-k\text{)}}}\\
&  =\sum_{k=0}^{n-x}\dbinom{y}{k}\left(  -1\right)  ^{\left(  n-x\right)
-k}\dbinom{n-k}{\left(  n-x\right)  -k}\\
&  =\underbrace{\sum_{k=n-\left(  n-x\right)  }^{n}}_{\substack{=\sum
_{k=x}^{n}\\\text{(since }n-\left(  n-x\right)  =x\text{)}}}\dbinom{y}%
{n-k}\underbrace{\left(  -1\right)  ^{\left(  n-x\right)  -\left(  n-k\right)
}}_{\substack{=\left(  -1\right)  ^{k-x}\\\text{(since }\left(  n-x\right)
-\left(  n-k\right)  =k-x\text{)}}}\underbrace{\dbinom{n-\left(  n-k\right)
}{\left(  n-x\right)  -\left(  n-k\right)  }}_{\substack{=\dbinom{k}%
{k-x}\\\text{(since }n-\left(  n-k\right)  =k\\\text{and }\left(  n-x\right)
-\left(  n-k\right)  =k-x\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n-k\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=x}^{n}\dbinom{y}{n-k}\left(  -1\right)  ^{k-x}\underbrace{\dbinom
{k}{k-x}}_{\substack{=\dbinom{k}{x}\\\text{(by
(\ref{pf.prop.vandermonde.consequences.e.1}))}}}=\sum_{k=x}^{n}\dbinom{y}%
{n-k}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\\
&  =\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}.
\end{align*}
Compared with%
\begin{align*}
&  \sum_{k=0}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}\\
&  =\sum_{k=0}^{x-1}\left(  -1\right)  ^{k-x}\underbrace{\dbinom{k}{x}%
}_{\substack{=0\\\text{(by (\ref{eq.binom.0}), applied to }k\text{ and
}x\\\text{instead of }m\text{ and }n\text{ (since }k<x\text{))}}}\dbinom
{y}{n-k}+\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq x\leq n\right) \\
&  =\underbrace{\sum_{k=0}^{x-1}\left(  -1\right)  ^{k-x}0\dbinom{y}{n-k}%
}_{=0}+\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}%
{n-k}=\sum_{k=x}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\dbinom{y}{n-k},
\end{align*}
this yields%
\[
\dbinom{y+\left(  -x-1\right)  }{n-x}=\sum_{k=0}^{n}\left(  -1\right)
^{k-x}\dbinom{k}{x}\dbinom{y}{n-k}.
\]
In other words,
\[
\dbinom{y-x-1}{n-x}=\sum_{k=0}^{n}\left(  -1\right)  ^{k-x}\dbinom{k}%
{x}\dbinom{y}{n-k}%
\]
(since $y+\left(  -x-1\right)  =y-x-1$). This proves Proposition
\ref{prop.vandermonde.consequences} \textbf{(e)}.

\textbf{(f)} Let $x\in\mathbb{N}$ and $y\in\mathbb{N}$ and $n\in\mathbb{N}$.
We must be in one of the following two cases:

\textit{Case 1:} We have $n<x+y$.

\textit{Case 2:} We have $n\geq x+y$.

Let us first consider Case 1. In this case, we have $n<x+y$. Thus,
$n+1<x+y+1$. Therefore, $\dbinom{n+1}{x+y+1}=0$ (by (\ref{eq.binom.0}),
applied to $n+1$ and $x+y+1$ instead of $m$ and $n$). But every $k\in\left\{
0,1,\ldots,n\right\}  $ satisfies $\dbinom{k}{x}\dbinom{n-k}{y}=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  0,1,\ldots,n\right\}  $.
We need to show that $\dbinom{k}{x}\dbinom{n-k}{y}=0$.
\par
If we have $k<x$, then we have $\dbinom{k}{x}=0$ (by (\ref{eq.binom.0}),
applied to $k$ and $x$ instead of $m$ and $n$). Therefore, if we have $k<x$,
then $\underbrace{\dbinom{k}{x}}_{=0}\dbinom{n-k}{y}=0$. Hence, for the rest
of this proof of $\dbinom{k}{x}\dbinom{n-k}{y}=0$, we can WLOG assume that we
don't have $k<x$. Assume this.
\par
We have $k\leq n$ (since $k\in\left\{  0,1,\ldots,n\right\}  $) and thus
$n-k\in\mathbb{N}$.
\par
We have $k\geq x$ (since we don't have $k<x$), and thus $n-\underbrace{k}%
_{\geq x}\leq n-x<y$ (since $n<x+y$). Hence, $\dbinom{n-k}{y}=0$ (by
(\ref{eq.binom.0}), applied to $n-k$ and $y$ instead of $m$ and $n$).
Therefore, $\dbinom{k}{x}\underbrace{\dbinom{n-k}{y}}_{=0}=0$, qed.}. Hence,
$\sum_{k=0}^{n}\underbrace{\dbinom{k}{x}\dbinom{n-k}{y}}_{=0}=\sum_{k=0}%
^{n}0=0$. Compared with $\dbinom{n+1}{x+y+1}=0$, this yields $\dbinom
{n+1}{x+y+1}=\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}$. Thus, Proposition
\ref{prop.vandermonde.consequences} \textbf{(f)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $n\geq x+y$. Hence, $n-y\geq
x$ (since $x\in\mathbb{N}$), so that $\left(  n-y\right)  -x\in\mathbb{N}$.
Also, $n-y\geq x\geq0$ and thus $n-y\in\mathbb{N}$. Moreover, $x\leq n-y$.
Therefore, we can apply Proposition \ref{prop.vandermonde.consequences}
\textbf{(e)} to $-y-1$ and $n-y$ instead of $y$ and $n$. As a result, we
obtain%
\begin{align}
&  \dbinom{\left(  -y-1\right)  -x-1}{\left(  n-y\right)  -x}\nonumber\\
&  =\sum_{k=0}^{n-y}\left(  -1\right)  ^{k-x}\dbinom{k}{x}\underbrace{\dbinom
{-y-1}{\left(  n-y\right)  -k}}_{\substack{=\left(  -1\right)  ^{\left(
n-y\right)  -k}\dbinom{\left(  \left(  n-y\right)  -k\right)  -\left(
-y-1\right)  -1}{\left(  n-y\right)  -k}\\\text{(by (\ref{eq.binom.upper-neg}%
), applied to }-y-1\text{ and }\left(  n-y\right)  -k\\\text{instead of
}m\text{ and }n\text{)}}}\nonumber\\
&  =\sum_{k=0}^{n-y}\left(  -1\right)  ^{k-x}\underbrace{\dbinom{k}{x}\left(
-1\right)  ^{\left(  n-y\right)  -k}}_{=\left(  -1\right)  ^{\left(
n-y\right)  -k}\dbinom{k}{x}}\underbrace{\dbinom{\left(  \left(  n-y\right)
-k\right)  -\left(  -y-1\right)  -1}{\left(  n-y\right)  -k}}%
_{\substack{=\dbinom{n-k}{\left(  n-y\right)  -k}\\\text{(since }\left(
\left(  n-y\right)  -k\right)  -\left(  -y-1\right)  -1=n-k\text{)}%
}}\nonumber\\
&  =\sum_{k=0}^{n-y}\underbrace{\left(  -1\right)  ^{k-x}\left(  -1\right)
^{\left(  n-y\right)  -k}}_{\substack{=\left(  -1\right)  ^{\left(
k-x\right)  +\left(  \left(  n-y\right)  -k\right)  }=\left(  -1\right)
^{n-x-y}\\\text{(since }\left(  k-x\right)  +\left(  \left(  n-y\right)
-k\right)  =n-x-y\text{)}}}\dbinom{k}{x}\dbinom{n-k}{\left(  n-y\right)
-k}\nonumber\\
&  =\sum_{k=0}^{n-y}\left(  -1\right)  ^{n-x-y}\dbinom{k}{x}\dbinom
{n-k}{\left(  n-y\right)  -k}=\left(  -1\right)  ^{n-x-y}\sum_{k=0}%
^{n-y}\dbinom{k}{x}\dbinom{n-k}{\left(  n-y\right)  -k}.
\label{pf.prop.vandermonde.consequences.f.0}%
\end{align}


But every $k\in\left\{  0,1,\ldots,n-y\right\}  $ satisfies%
\begin{equation}
\dbinom{n-k}{\left(  n-y\right)  -k}=\dbinom{n-k}{y}
\label{pf.prop.vandermonde.consequences.f.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.vandermonde.consequences.f.0}):} Let
$k\in\left\{  0,1,\ldots,n-y\right\}  $. Then, $k\in\mathbb{N}$ and $n-y\geq
k$. From $n-y\geq k$, we obtain $n\geq y+k$, so that $n-k\geq y$. Thus,
$n-k\geq y\geq0$, so that $n-k\in\mathbb{N}$. Hence, (\ref{eq.binom.symm})
(applied to $n-k$ and $y$ instead of $m$ and $n$) yields $\dbinom{n-k}%
{y}=\dbinom{n-k}{\left(  n-k\right)  -y}=\dbinom{n-k}{\left(  n-y\right)  -k}$
(since $\left(  n-k\right)  -y=\left(  n-y\right)  -k$). This proves
(\ref{pf.prop.vandermonde.consequences.f.1}).}. Thus,
(\ref{pf.prop.vandermonde.consequences.f.0}) yields%
\begin{align*}
\dbinom{\left(  -y-1\right)  -x-1}{\left(  n-y\right)  -x}  &  =\left(
-1\right)  ^{n-x-y}\sum_{k=0}^{n-y}\dbinom{k}{x}\underbrace{\dbinom
{n-k}{\left(  n-y\right)  -k}}_{\substack{=\dbinom{n-k}{y}\\\text{(by
(\ref{pf.prop.vandermonde.consequences.f.1}))}}}\\
&  =\left(  -1\right)  ^{n-x-y}\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}.
\end{align*}
Compared with%
\begin{align*}
\dbinom{\left(  -y-1\right)  -x-1}{\left(  n-y\right)  -x}  &
=\underbrace{\left(  -1\right)  ^{\left(  n-y\right)  -x}}_{\substack{=\left(
-1\right)  ^{n-x-y}\\\text{(since }\left(  n-y\right)  -x=n-x-y\text{)}%
}}\underbrace{\dbinom{\left(  \left(  n-y\right)  -x\right)  -\left(  \left(
-y-1\right)  -x-1\right)  -1}{\left(  n-y\right)  -x}}_{\substack{=\dbinom
{n+1}{n-x-y}\\\text{(since }\left(  \left(  n-y\right)  -x\right)  -\left(
\left(  -y-1\right)  -x-1\right)  -1=n+1\\\text{and }\left(  n-y\right)
-x=n-x-y\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.binom.upper-neg}), applied to }\left(  -y-1\right)
-x-1\text{ and }\left(  n-y\right)  -x\\
\text{instead of }m\text{ and }n
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n-x-y}\dbinom{n+1}{n-x-y},
\end{align*}
this yields%
\[
\left(  -1\right)  ^{n-x-y}\dbinom{n+1}{n-x-y}=\left(  -1\right)  ^{n-x-y}%
\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}.
\]
We can cancel $\left(  -1\right)  ^{n-x-y}$ from this equality (because
$\left(  -1\right)  ^{n-x-y}\neq0$). As a result, we obtain%
\begin{equation}
\dbinom{n+1}{n-x-y}=\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}.
\label{pf.prop.vandermonde.consequences.f.5}%
\end{equation}


But $0\leq n-y$ (since $n-y\in\mathbb{N}$) and $n-y\leq n$ (since
$y\in\mathbb{N}$). Also, every $k\in\left\{  n-y+1,n-y+2,\ldots,n\right\}  $
satisfies%
\begin{equation}
\dbinom{n-k}{y}=0 \label{pf.prop.vandermonde.consequences.f.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.vandermonde.consequences.f.2}):} Let
$k\in\left\{  n-y+1,n-y+2,\ldots,n\right\}  $. Then, $k\leq n$ and $k>n-y$.
Hence, $n-k\in\mathbb{N}$ (since $k\leq n$) and $n-\underbrace{k}%
_{>n-y}<n-\left(  n-y\right)  =y$. Therefore, (\ref{eq.binom.0}) (applied to
$n-k$ and $y$ instead of $m$ and $n$) yields $\dbinom{n-k}{y}=0$. This proves
(\ref{pf.prop.vandermonde.consequences.f.2}).}. Hence,%
\begin{align}
\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}  &  =\sum_{k=0}^{n-y}\dbinom{k}%
{x}\dbinom{n-k}{y}+\sum_{k=n-y+1}^{n}\dbinom{k}{x}\underbrace{\dbinom{n-k}{y}%
}_{\substack{=0\\\text{(by (\ref{pf.prop.vandermonde.consequences.f.2}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq n-y\leq n\right) \nonumber\\
&  =\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom{n-k}{y}+\underbrace{\sum
_{k=n-y+1}^{n}\dbinom{k}{x}0}_{=0}=\sum_{k=0}^{n-y}\dbinom{k}{x}\dbinom
{n-k}{y}\nonumber\\
&  =\dbinom{n+1}{n-x-y}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.vandermonde.consequences.f.5})}\right)  .
\label{pf.prop.vandermonde.consequences.f.9}%
\end{align}


Finally, $n+1\in\mathbb{N}$ and $x+y+1\in\mathbb{N}$ (since $x\in\mathbb{N}$
and $y\in\mathbb{N}$) and $\underbrace{n}_{\geq x+y}+1\geq x+y+1$. Hence,
(\ref{eq.binom.symm}) (applied to $n+1$ and $x+y+1$ instead of $m$ and $n$)
yields
\[
\dbinom{n+1}{x+y+1}=\dbinom{n+1}{\left(  n+1\right)  -\left(  x+y+1\right)
}=\dbinom{n+1}{n-x-y}%
\]
(since $\left(  n+1\right)  -\left(  x+y+1\right)  =n-x-y$). Comparing this
with (\ref{pf.prop.vandermonde.consequences.f.9}), we obtain%
\[
\dbinom{n+1}{x+y+1}=\sum_{k=0}^{n}\dbinom{k}{x}\dbinom{n-k}{y}.
\]
Thus, Proposition \ref{prop.vandermonde.consequences} \textbf{(f)} is proven
in Case 2.

We have now proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(f)} in both Cases 1 and 2; thus, Proposition
\ref{prop.vandermonde.consequences} \textbf{(f)} always holds.

\textbf{(g)} Let $x\in\mathbb{Z}$ and $y\in\mathbb{N}$ and $n\in\mathbb{N}$ be
such that $x+y\geq0$ and $n\geq x$. We have $x+y\in\mathbb{N}$ (since
$x+y\geq0$). We must be in one of the following two cases:

\textit{Case 1:} We have $x+y<n$.

\textit{Case 2:} We have $x+y\geq n$.

Let us first consider Case 1. In this case, we have $x+y<n$. Thus,
$\dbinom{x+y}{n}=0$ (by (\ref{eq.binom.0}), applied to $m=x+y$). But every
$k\in\left\{  0,1,\ldots,x+y\right\}  $ satisfies $\dbinom{y}{n+k-x}%
=0$\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  0,1,\ldots,x+y\right\}
$. Then, $k\geq0$, so that $n+\underbrace{k}_{\geq0}-x\geq n-x>y$ (since
$n>x+y$ (since $x+y<n$)). In other words, $y<n+k-x$. Also, $n+k-x>y\geq0$, so
that $n+k-x\in\mathbb{N}$. Hence, $\dbinom{y}{n+k-x}=0$ (by (\ref{eq.binom.0}%
), applied to $y$ and $n+k-x$ instead of $m$ and $n$). Qed.}. Thus,
$\sum_{k=0}^{x+y}\dbinom{x}{k}\underbrace{\dbinom{y}{n+k-x}}_{=0}=\sum
_{k=0}^{x+y}\dbinom{x}{k}0=0$. Compared with $\dbinom{x+y}{n}=0$, this yields
$\dbinom{x+y}{n}=\sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}$. Thus,
Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $x+y\geq n$. Hence,
$\dbinom{x+y}{n}=\dbinom{x+y}{x+y-n}$ (by (\ref{eq.binom.symm}), applied to
$m=x+y$). Also, $x+y-n\in\mathbb{N}$ (since $x+y\geq n$). Therefore,
Proposition \ref{prop.vandermonde.consequences} \textbf{(a)} (applied to
$x+y-n$ instead of $n$) yields%
\[
\dbinom{x+y}{x+y-n}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{x+y-n-k}.
\]
Since $\dbinom{x+y}{n}=\dbinom{x+y}{x+y-n}$, this rewrites as
\begin{equation}
\dbinom{x+y}{n}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{x+y-n-k}.
\label{pf.prop.vandermonde.consequences.g.4}%
\end{equation}
But every $k\in\left\{  0,1,\ldots,x+y-n\right\}  $ satisfies $\dbinom
{y}{x+y-n-k}=\dbinom{y}{n+k-x}$\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  0,1,\ldots,x+y-n\right\}  $. Then, $0\leq k\leq x+y-n$. Hence,
$x+y-n\geq k$, so that $x+y-n-k\in\mathbb{N}$. Also, $y\geq x+y-n-k$ (since
$y-\left(  x+y-n-k\right)  =\underbrace{n}_{\geq x}+\underbrace{k}_{\geq
0}-x\geq x+0-x=0$). Therefore, (\ref{eq.binom.symm}) (applied to $y$ and
$x+y-n-k$ instead of $m$ and $n$) yields $\dbinom{y}{x+y-n-k}=\dbinom
{y}{y-\left(  x+y-n-k\right)  }=\dbinom{y}{n+k-x}$ (since $y-\left(
x+y-n-k\right)  =n+k-x$), qed.}. Hence,
(\ref{pf.prop.vandermonde.consequences.g.4}) becomes%
\begin{equation}
\dbinom{x+y}{n}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\underbrace{\dbinom{y}%
{x+y-n-k}}_{=\dbinom{y}{n+k-x}}=\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom
{y}{n+k-x}. \label{pf.prop.vandermonde.consequences.g.6}%
\end{equation}


On the other hand, we have $0\leq n\leq x+y$ and thus $0\leq x+y-n\leq x+y$.
But every $k\in\mathbb{N}$ satisfying $k>x+y-n$ satisfies
\begin{equation}
\dbinom{y}{n+k-x}=0 \label{pf.prop.vandermonde.consequences.g.7}%
\end{equation}
\footnote{\textit{Proof.} Let $k\in\mathbb{N}$ be such that $k>x+y-n$. Then,
$n+\underbrace{k}_{>x+y-n}-x>n+\left(  x+y-n\right)  -x=y$. In other words,
$y<n+k-x$. Also, $n+k-x>y\geq0$, so that $n+k-x\in\mathbb{N}$. Hence,
(\ref{eq.binom.0}) (applied to $y$ and $n+k-x$ instead of $m$ and $n$) yields
$\dbinom{y}{n+k-x}=0$, qed.}. Hence,
\begin{align*}
&  \sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}\\
&  =\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{n+k-x}+\sum_{k=\left(
x+y-n\right)  +1}^{x+y}\dbinom{x}{k}\underbrace{\dbinom{y}{n+k-x}%
}_{\substack{=0\\\text{(by (\ref{pf.prop.vandermonde.consequences.g.7}) (since
}k>x+y-n\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq x+y-n\leq x+y\right) \\
&  =\sum_{k=0}^{x+y-n}\dbinom{x}{k}\dbinom{y}{n+k-x}+\underbrace{\sum
_{k=\left(  x+y-n\right)  +1}^{x+y}\dbinom{x}{k}0}_{=0}=\sum_{k=0}%
^{x+y-n}\dbinom{x}{k}\dbinom{y}{n+k-x}.
\end{align*}
Compared with (\ref{pf.prop.vandermonde.consequences.g.6}), this yields%
\[
\dbinom{x+y}{n}=\sum_{k=0}^{x+y}\dbinom{x}{k}\dbinom{y}{n+k-x}.
\]
This proves Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} in
Case 2.

Proposition \ref{prop.vandermonde.consequences} \textbf{(g)} is thus proven in
each of the two Cases 1 and 2. Therefore, Proposition
\ref{prop.vandermonde.consequences} \textbf{(g)} holds in full generality.
\end{proof}

\begin{remark}
The proof of Proposition \ref{prop.vandermonde.consequences} given above
illustrates a useful technique: the use of upper negation (i.e., the equality
(\ref{eq.binom.upper-neg})) to transform one equality into another. In a nutshell,

\begin{itemize}
\item we have proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(d)} by applying Proposition \ref{prop.vandermonde.consequences}
\textbf{(a)} to $-y$ instead of $y$, and then rewriting the result using upper negation;

\item we have proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(e)} by applying Proposition \ref{prop.vandermonde.consequences}
\textbf{(a)} to $y$, $-x-1$ and $n-x$ instead of $x$, $y$ and $n$, and then
rewriting the resulting identity using upper negation;

\item we have proven Proposition \ref{prop.vandermonde.consequences}
\textbf{(f)} by applying Proposition \ref{prop.vandermonde.consequences}
\textbf{(e)} to $-y-1$ and $n-y$ instead of $y$ and $n$, and rewriting the
resulting identity using upper negation.
\end{itemize}

Thus, by substitution and rewriting using upper negation, one single equality
(namely, Proposition \ref{prop.vandermonde.consequences} \textbf{(a)}) has
morphed into three other equalities. Note, in particular, that no negative
numbers appear in Proposition \ref{prop.vandermonde.consequences}
\textbf{(f)}, but yet we proved it by substituting negative values for $x$ and
$y$.
\end{remark}

\subsection{Further results}

\begin{exercise}
\label{exe.ps1.1.2}Let $n$ be a nonnegative integer. Prove that there exist
\textbf{nonnegative} integers $c_{i,j}$ for all $0\leq i\leq n$ and $0\leq
j\leq n$ such that%
\begin{equation}
\dbinom{XY}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{X}{i}\dbinom{Y}{j}
\label{eq.exe.1.2.claim}%
\end{equation}
(an equality between polynomials in two variables $X$ and $Y$).
\end{exercise}

Notice that the integers $c_{i,j}$ in Exercise \ref{exe.ps1.1.2} can depend on
the $n$ (besides depending on $i$ and $j$). We just have not included the $n$
in the notation because it is fixed.

We shall now state two results that are used by Lee and Schiffler in their
celebrated proof of positivity for cluster algebras \cite{LS} (one of the
recent breakthroughs in cluster algebra theory). Specifically, our Exercise
\ref{exe.ps1.1.3} is (essentially) \cite[Lemma 5.11]{LS}, and our Proposition
\ref{exe.ps1.1.4} is (essentially) \cite[Lemma 5.12]{LS}\footnote{We say
\textquotedblleft essentially\textquotedblright\ because the $X$ in
\cite[Lemma 5.11]{LS} and in \cite[Lemma 5.12]{LS} is a variable ranging over
the nonnegative integers rather than an indeterminate. But this does not make
much of a difference (indeed, Lemma \ref{lem.polyeq} \textbf{(b)} allows us to
easily derive our Exercise \ref{exe.ps1.1.3} and Proposition \ref{exe.ps1.1.4}
from \cite[Lemma 5.11]{LS} and \cite[Lemma 5.12]{LS}, and of course the
converse implication is obvious).}.

\begin{exercise}
\label{exe.ps1.1.3}Let $a$, $b$ and $c$ be three nonnegative integers. Prove
that the polynomial $\dbinom{aX+b}{c}$ in the variable $X$ (this is a
polynomial in $X$ of degree $\leq c$) can be written as a sum $\sum_{i=0}%
^{c}d_{i}\dbinom{X}{i}$ with \textbf{nonnegative} $d_{i}$.
\end{exercise}

\begin{proposition}
\label{exe.ps1.1.4}Let $a$ and $b$ be two nonnegative integers. There exist
\textbf{nonnegative} integers $e_{0},e_{1},\ldots,e_{a+b}$ such that%
\[
\dbinom{X}{a}\dbinom{X}{b}=\sum_{i=0}^{a+b}e_{i}\dbinom{X}{i}%
\]
(an equality between polynomials in $X$).
\end{proposition}

\begin{proof}
[First proof of Proposition \ref{exe.ps1.1.4}.]For every $N\in\mathbb{N}$, we
let $\left[  N\right]  $ denote the $N$-element set $\left\{  1,2,\ldots
,N\right\}  $.

For every set $S$, we let an $S$\textit{-junction} mean a pair $\left(
A,B\right)  $, where $A$ is an $a$-element subset of $S$ and where $B$ is a
$b$-element subset of $S$ such that $A\cup B=S$. (We do not mention $a$ and
$b$ in our notation, because $a$ and $b$ are fixed.)

For example, if $a=2$ and $b=3$, then $\left(  \left\{  1,4\right\}  ,\left\{
2,3,4\right\}  \right)  $ is a $\left[  4\right]  $-junction, and $\left(
\left\{  2,4\right\}  ,\left\{  1,4,6\right\}  \right)  $ is a $\left\{
1,2,4,6\right\}  $-junction, but $\left(  \left\{  1,3\right\}  ,\left\{
2,3,5\right\}  \right)  $ is not a $\left[  5\right]  $-junction (since
$\left\{  1,3\right\}  \cup\left\{  2,3,5\right\}  \neq\left[  5\right]  $).

For every $i\in\mathbb{N}$, we let $e_{i}$ be the number of all $\left[
i\right]  $-junctions. Then, if $S$ is any $i$-element set, then%
\begin{equation}
e_{i}\text{ is the number of all }S\text{-junctions} \label{sol.ps1.1.4.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps1.1.4.1}):} Let $S$ be any $i$-element
set. We know that $e_{i}$ is the number of all $\left[  i\right]  $-junctions.
We want to prove that $e_{i}$ is the number of all $S$-junctions. Roughly
speaking, this is obvious, because we can \textquotedblleft relabel the
elements of $S$ as $1,2,\ldots,i$\textquotedblright\ (since $S$ is an
$i$-element set), and then the $S$-junctions become precisely the $\left[
i\right]  $-junctions.
\par
Here is a formal way to make this argument: The sets $\left[  i\right]  $ and
$S$ have the same number of elements (indeed, both are $i$-element sets).
Hence, there exists a bijection $\phi:S\rightarrow\left[  i\right]  $. Fix
such a $\phi$. Now, the $S$-junctions are in a 1-to-1 correspondence with the
$\left[  i\right]  $-junctions (namely, to every $S$-junction $\left(
A,B\right)  $ corresponds the $\left[  i\right]  $-junction $\left(
\phi\left(  A\right)  ,\phi\left(  B\right)  \right)  $, and conversely, to
every $\left[  i\right]  $-junction $\left(  A^{\prime},B^{\prime}\right)  $
corresponds the $S$-junction $\left(  \phi^{-1}\left(  A^{\prime}\right)
,\phi^{-1}\left(  B^{\prime}\right)  \right)  $). Hence, the number of all
$S$-junctions equals the number of $\left[  i\right]  $-junctions. Since the
latter number is $e_{i}$, this shows that the former number is also $e_{i}$.
This proves (\ref{sol.ps1.1.4.1}).}.

Now, let us show that%
\begin{equation}
\dbinom{x}{a}\dbinom{x}{b}=\sum_{i=0}^{a+b}e_{i}\dbinom{x}{i}
\label{sol.ps1.1.4.xclaim}%
\end{equation}
for every $x\in\mathbb{N}$.

\textit{Proof of (\ref{sol.ps1.1.4.xclaim}):} Let $x\in\mathbb{N}$. How many
ways are there to choose a pair $\left(  A,B\right)  $ consisting of an
$a$-element subset $A$ of $\left[  x\right]  $ and a $b$-element subset $B$ of
$\left[  x\right]  $ ?

Let us give two different answers to this question. The first answer is the
straightforward one: To choose a pair $\left(  A,B\right)  $ consisting of an
$a$-element subset $A$ of $\left[  x\right]  $ and a $b$-element subset $B$ of
$\left[  x\right]  $, we need to choose an $a$-element subset $A$ of $\left[
x\right]  $ and a $b$-element subset $B$ of $\left[  x\right]  $. There are
$\dbinom{x}{a}\dbinom{x}{b}$ total ways to do this (since there are
$\dbinom{x}{a}$ choices for $A$\ \ \ \ \footnote{This follows from
(\ref{eq.binom.subsets}).}, and $\dbinom{x}{b}$ choices for $B$%
\ \ \ \ \footnote{Again, this follows from (\ref{eq.binom.subsets}).}, and
these choices are independent). In other words, the number of all pairs
$\left(  A,B\right)  $ consisting of an $a$-element subset $A$ of $\left[
x\right]  $ and a $b$-element subset $B$ of $\left[  x\right]  $ equals
$\dbinom{x}{a}\dbinom{x}{b}$.

On the other hand, here is a more imaginative procedure to choose a pair
$\left(  A,B\right)  $ consisting of an $a$-element subset $A$ of $\left[
x\right]  $ and a $b$-element subset $B$ of $\left[  x\right]  $:

\begin{enumerate}
\item We choose how many elements the union $A\cup B$ will have. In other
words, we choose an $i\in\mathbb{N}$ that will satisfy $\left\vert A\cup
B\right\vert =i$. This $i$ must be an integer between $0$ and $a+b$
(inclusive)\footnote{\textit{Proof.} Clearly, $i$ cannot be smaller than $0$.
But $i$ also cannot be larger than $a+b$ (since $i$ will have to satisfy
$i=\left\vert A\cup B\right\vert \leq\underbrace{\left\vert A\right\vert
}_{=a}+\underbrace{\left\vert B\right\vert }_{=b}=a+b$). Thus, $i$ must be an
integer between $0$ and $a+b$ (inclusive).}.

\item We choose a subset $S$ of $\left[  x\right]  $, which will serve as the
union $A\cup B$. This subset $S$ must be an $i$-element subset of $\left[
x\right]  $ (because we will have $\left\vert \underbrace{S}_{=A\cup
B}\right\vert =\left\vert A\cup B\right\vert =i$). Thus, there are $\dbinom
{x}{i}$ ways to choose it (since we need to choose an $i$-element subset of
$\left[  x\right]  $).

\item Now, it remains to choose the pair $\left(  A,B\right)  $ itself. This
pair must be a pair of subsets of $\left[  x\right]  $ satisfying $\left\vert
A\right\vert =a$, $\left\vert B\right\vert =b$, $A\cup B=S$ and $\left\vert
A\cup B\right\vert =i$. We can forget about the $\left\vert A\cup B\right\vert
=i$ condition, since it automatically follows from $A\cup B=S$ (because
$\left\vert S\right\vert =i$). So we need to choose a pair $\left(
A,B\right)  $ of subsets of $\left[  x\right]  $ satisfying $\left\vert
A\right\vert =a$, $\left\vert B\right\vert =b$ and $A\cup B=S$. In other
words, we need to choose a pair $\left(  A,B\right)  $ of subsets of $S$
satisfying $\left\vert A\right\vert =a$, $\left\vert B\right\vert =b$ and
$A\cup B=S$\ \ \ \ \footnote{Here, we have replaced \textquotedblleft subsets
of $\left[  x\right]  $\textquotedblright\ by \textquotedblleft subsets of
$S$\textquotedblright, because the condition $A\cup B=S$ forces $A$ and $B$ to
be subsets of $S$.}. In other words, we need to choose an $S$-junction (since
this is how an $S$-junction was defined). This can be done in exactly $e_{i}$
ways (according to (\ref{sol.ps1.1.4.1})).
\end{enumerate}

Thus, in total, there are $\sum_{i=0}^{a+b}\dbinom{x}{i}e_{i}$ ways to perform
this procedure. Hence, the total number of all pairs $\left(  A,B\right)  $
consisting of an $a$-element subset $A$ of $\left[  x\right]  $ and a
$b$-element subset $B$ of $\left[  x\right]  $ equals $\sum_{i=0}^{a+b}%
\dbinom{x}{i}e_{i}$. But earlier, we have shown that this number is
$\dbinom{x}{a}\dbinom{x}{b}$. Comparing these two results, we conclude that
$\dbinom{x}{a}\dbinom{x}{b}=\sum_{i=0}^{a+b}\dbinom{x}{i}e_{i}=\sum
_{i=0}^{a+b}e_{i}\dbinom{x}{i}$. Thus, (\ref{sol.ps1.1.4.xclaim}) is proven.

Now, we define two polynomials $P$ and $Q$ in the indeterminate $X$ with
rational coefficients by setting%
\[
P=\dbinom{X}{a}\dbinom{X}{b};\ \ \ \ \ \ \ \ \ \ Q=\sum_{i=0}^{a+b}%
e_{i}\dbinom{X}{i}.
\]
The equality (\ref{sol.ps1.1.4.xclaim}) (which we have proven) states that
$P\left(  x\right)  =Q\left(  x\right)  $ for all $x\in\mathbb{N}$. Thus,
Lemma \ref{lem.polyeq} \textbf{(b)} yields that $P=Q$. Recalling how $P$ and
$Q$ are defined, we see that this rewrites as $\dbinom{X}{a}\dbinom{X}{b}%
=\sum_{i=0}^{a+b}e_{i}\dbinom{X}{i}$. This proves Proposition
\ref{exe.ps1.1.4}.
\end{proof}

\begin{proof}
[Second proof of Proposition \ref{exe.ps1.1.4}.]Here is an algebraic proof of
Proposition \ref{exe.ps1.1.4} (based on a suggestion of math.stackexchange
user tcamps in a comment on
\href{http://math.stackexchange.com/questions/1342384}{question \#1342384}).

Theorem \ref{thm.vandermonde} (applied to $n=b$) yields%
\[
\dbinom{X+Y}{b}=\sum_{k=0}^{b}\dbinom{X}{k}\dbinom{Y}{b-k}.
\]
This is a polynomial identity in $X$ and $Y$; we can thus substitute $X-a$ and
$a$ for $X$ and $Y$. As a result of this substitution, we obtain%
\[
\dbinom{\left(  X-a\right)  +a}{b}=\sum_{k=0}^{b}\dbinom{X-a}{k}\dbinom
{a}{b-k}.
\]
Since $\left(  X-a\right)  +a=X$, this rewrites as%
\begin{align*}
\dbinom{X}{b}  &  =\sum_{k=0}^{b}\dbinom{X-a}{k}\dbinom{a}{b-k}=\sum
_{i=a}^{a+b}\dbinom{X-a}{i-a}\underbrace{\dbinom{a}{b-\left(  i-a\right)  }%
}_{=\dbinom{a}{a+b-i}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }i-a\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{i=a}^{a+b}\dbinom{X-a}{i-a}\dbinom{a}{a+b-i}.
\end{align*}
Multiplying both sides of this identity with $\dbinom{X}{a}$, we obtain%
\begin{align}
\dbinom{X}{a}\dbinom{X}{b}  &  =\dbinom{X}{a}\sum_{i=a}^{a+b}\dbinom{X-a}%
{i-a}\dbinom{a}{a+b-i}=\sum_{i=a}^{a+b}\underbrace{\dbinom{X}{a}\dbinom
{X-a}{i-a}}_{\substack{=\dbinom{X}{i}\dbinom{i}{a}\\\text{(by
(\ref{eq.binom.trinom-rev}))}}}\dbinom{a}{a+b-i}\nonumber\\
&  =\sum_{i=a}^{a+b}\dbinom{X}{i}\dbinom{i}{a}\dbinom{a}{a+b-i}=\sum
_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}.
\label{sol.ps1.1.4.sol2.thm}%
\end{align}


Now, let us define $a+b+1$ nonnegative integers $e_{0},e_{1},\ldots,e_{a+b}$
by%
\begin{equation}
e_{i}=\left\{
\begin{array}
[c]{c}%
\dbinom{i}{a}\dbinom{a}{a+b-i},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq a;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  0,1,\ldots
,a+b\right\}  . \label{sol.ps1.1.4.sol2.ei}%
\end{equation}
Then,%
\begin{align*}
\sum_{i=0}^{a+b}e_{i}\dbinom{X}{i}  &  =\sum_{i=a}^{a+b}\dbinom{i}{a}%
\dbinom{a}{a+b-i}\dbinom{X}{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by our
definition of }e_{0},e_{1},\ldots,e_{a+b}\right) \\
&  =\dbinom{X}{a}\dbinom{X}{b}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps1.1.4.sol2.thm})}\right)  .
\end{align*}
Thus, Proposition \ref{exe.ps1.1.4} is proven again.
\end{proof}

\begin{remark}
Comparing our two proofs of Proposition \ref{exe.ps1.1.4}, it is natural to
suspect that the $e_{0},e_{1},\ldots,e_{a+b}$ defined in the First proof are
identical with the $e_{0},e_{1},\ldots,e_{a+b}$ defined in the Second proof.
This actually follows from general principles (namely, from the word
\textquotedblleft unique\textquotedblright\ in Proposition
\ref{prop.hartshorne} \textbf{(a)}), but there is also a simple combinatorial
reason. Namely, let $i\in\left\{  0,1,\ldots,a+b\right\}  $. We shall show
that the $e_{i}$ defined in the First proof equals the $e_{i}$ defined in the
Second proof.

The $e_{i}$ defined in the First proof is the number of all $\left[  i\right]
$-junctions. An $\left[  i\right]  $-junction is a pair $\left(  A,B\right)
$, where $A$ is an $a$-element subset of $\left[  i\right]  $ and where $B$ is
a $b$-element subset of $\left[  i\right]  $ such that $A\cup B=\left[
i\right]  $. Here is a way to construct an $\left[  i\right]  $-junction:

\begin{itemize}
\item First, we pick the set $A$. There are $\dbinom{i}{a}$ ways to do this,
since $A$ has to be an $a$-element subset of the $i$-element set $\left[
i\right]  $.

\item Then, we pick the set $B$. This has to be a $b$-element subset of the
$i$-element set $\left[  i\right]  $ satisfying $A\cup B=\left[  i\right]  $.
The equality $A\cup B=\left[  i\right]  $ means that $B$ has to contain the
$i-a$ element of $\left[  i\right]  \setminus A$; but the remaining $b-\left(
i-a\right)  =a+b-i$ elements of $B$ can be chosen arbitrarily among the $a$
elements of $A$. Thus, there are $\dbinom{a}{a+b-i}$ ways to choose $B$ (since
we have to choose $a+b-i$ elements of $B$ among the $a$ elements of $A$).
\end{itemize}

Thus, the number of all $\left[  i\right]  $-junctions is $\dbinom{i}%
{a}\dbinom{a}{a+b-i}$. This can be rewritten in the form $\left\{
\begin{array}
[c]{c}%
\dbinom{i}{a}\dbinom{a}{a+b-i},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq a;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $ (because if $i<a$, then $\dbinom{i}{a}=0$ and thus $\dbinom{i}%
{a}\dbinom{a}{a+b-i}=0$). Thus, we have shown that the number of all $\left[
i\right]  $-junctions is $\left\{
\begin{array}
[c]{c}%
\dbinom{i}{a}\dbinom{a}{a+b-i},\ \ \ \ \ \ \ \ \ \ \text{if }i\geq a;\\
0,\ \ \ \ \ \ \ \ \ \ \text{otherwise}%
\end{array}
\right.  $. In other words, the $e_{i}$ defined in the First proof equals the
$e_{i}$ defined in the Second proof.
\end{remark}

Here is an assortment of other identities that involve binomial coefficients:

\begin{proposition}
\label{prop.binom.bin-id}\textbf{(a)} Every $x\in\mathbb{Z}$, $y\in\mathbb{Z}$
and $n\in\mathbb{N}$ satisfy $\left(  x+y\right)  ^{n}=\sum_{k=0}^{n}%
\dbinom{n}{k}x^{k}y^{n-k}$.

\textbf{(b)} Every $n\in\mathbb{N}$ satisfies $\sum_{k=0}^{n}\dbinom{n}%
{k}=2^{n}$.

\textbf{(c)} Every $n\in\mathbb{N}$ satisfies $\sum_{k=0}^{n}\left(
-1\right)  ^{k}\dbinom{n}{k}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n\neq0
\end{cases}
$.

\textbf{(d)} Every $n\in\mathbb{Z}$, $i\in\mathbb{N}$ and $a\in\mathbb{N}$
satisfying $i\geq a$ satisfy $\dbinom{n}{i}\dbinom{i}{a}=\dbinom{n}{a}%
\dbinom{n-a}{i-a}$.

\textbf{(e)} Every $n\in\mathbb{N}$ and $m\in\mathbb{Z}$ satisfy $\sum
_{i=0}^{n}\dbinom{n}{i}\dbinom{m+i}{n}=\sum_{i=0}^{n}\dbinom{n}{i}\dbinom
{m}{i}2^{i}$.

\textbf{(f)} Every $a\in\mathbb{N}$, $b\in\mathbb{N}$ and $x\in\mathbb{Z}$
satisfy $\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{x+i}{a+b}=\dbinom
{x}{a}\dbinom{x}{b}$.

\textbf{(g)} Every $a\in\mathbb{N}$, $b\in\mathbb{N}$ and $x\in\mathbb{Z}$
satisfy $\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{a+b+x-i}{a+b}%
=\dbinom{a+x}{a}\dbinom{b+x}{b}$.
\end{proposition}

(Parts \textbf{(e)} and \textbf{(f)} of Proposition \ref{prop.binom.bin-id}
are from
\href{http://artofproblemsolving.com/community/c6h626709p4577721}{AoPS}. Part
\textbf{(g)} is a restatement of \cite[(6.93)]{Gould-I}.)

\begin{proof}
[Proof of Proposition \ref{prop.binom.bin-id}.]\textbf{(a)} Let $x\in
\mathbb{Z}$, $y\in\mathbb{Z}$ and $n\in\mathbb{N}$. Substituting $X=x$ and
$Y=y$ into (\ref{eq.binom.binomial}), we obtain $\left(  x+y\right)  ^{n}%
=\sum_{k=0}^{n}\dbinom{n}{k}x^{k}y^{n-k}$. This proves Proposition
\ref{prop.binom.bin-id} \textbf{(a)}.

\textbf{(b)} Let $n\in\mathbb{N}$. Applying Proposition
\ref{prop.binom.bin-id} \textbf{(a)} to $x=1$ and $y=1$, we obtain $\left(
1+1\right)  ^{n}=\sum_{k=0}^{n}\dbinom{n}{k}\underbrace{1^{k}}_{=1}%
\underbrace{1^{n-k}}_{=1}=\sum_{k=0}^{n}\dbinom{n}{k}$, thus $\sum_{k=0}%
^{n}\dbinom{n}{k}=\left(  \underbrace{1+1}_{=2}\right)  ^{n}=2^{n}$. This
proves Proposition \ref{prop.binom.bin-id} \textbf{(b)}.

\textbf{(c)} Let $n\in\mathbb{N}$. Applying Proposition
\ref{prop.binom.bin-id} \textbf{(a)} to $x=-1$ and $y=1$, we obtain $\left(
-1+1\right)  ^{n}=\sum_{k=0}^{n}\dbinom{n}{k}\left(  -1\right)  ^{k}%
\underbrace{1^{n-k}}_{=1}=\sum_{k=0}^{n}\dbinom{n}{k}\left(  -1\right)
^{k}=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}$, thus $\sum_{k=0}%
^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}=\left(  \underbrace{-1+1}%
_{=0}\right)  ^{n}=0^{n}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n\neq0
\end{cases}
$. This proves Proposition \ref{prop.binom.bin-id} \textbf{(c)}.

\textbf{(d)} Let $n\in\mathbb{Z}$, $i\in\mathbb{N}$ and $a\in\mathbb{N}$ be
such that $i\geq a$. Substituting $n$ for $X$ in the equality
(\ref{eq.binom.trinom-rev}), we obtain $\dbinom{n}{i}\dbinom{i}{a}=\dbinom
{n}{a}\dbinom{n-a}{i-a}$. This proves Proposition \ref{prop.binom.bin-id}
\textbf{(d)}.

\textbf{(e)} Let $n\in\mathbb{N}$ and $m\in\mathbb{Z}$. Clearly, every
$p\in\mathbb{N}$ satisfies%
\begin{align}
\sum_{i=0}^{p}\dbinom{p}{i}  &  =\sum_{k=0}^{p}\dbinom{p}{k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}i\text{ as }k\right) \nonumber\\
&  =2^{p} \label{pf.prop.binom.bin-id.e.4}%
\end{align}
(by Proposition \ref{prop.binom.bin-id} \textbf{(b)}, applied to $p$ instead
of $n$).

Now, let $i\in\left\{  0,1,\ldots,n\right\}  $. Applying Proposition
\ref{prop.vandermonde.consequences} \textbf{(a)} to $x=i$ and $y=m$, we obtain%
\begin{align}
&  \dbinom{i+m}{n}\nonumber\\
&  =\sum_{k=0}^{n}\dbinom{i}{k}\dbinom{m}{n-k}\nonumber\\
&  =\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}+\sum_{k=i+1}^{n}%
\underbrace{\dbinom{i}{k}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}),
applied to }i\text{ and }k\\\text{instead of }m\text{ and }n\text{ (since
}i<k\text{))}}}\dbinom{m}{n-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq
i\leq n\right) \nonumber\\
&  =\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}+\underbrace{\sum_{k=i+1}%
^{n}0\dbinom{m}{n-k}}_{=0}=\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}.
\label{pf.prop.binom.bin-id.e.1}%
\end{align}


Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.prop.binom.bin-id.e.1}) for every $i\in\left\{  0,1,\ldots,n\right\}
$. Now,%
\begin{align*}
&  \sum_{i=0}^{n}\dbinom{n}{i}\underbrace{\dbinom{m+i}{n}}_{\substack{=\dbinom
{i+m}{n}=\sum_{k=0}^{i}\dbinom{i}{k}\dbinom{m}{n-k}\\\text{(by
(\ref{pf.prop.binom.bin-id.e.1}))}}}\\
&  =\sum_{i=0}^{n}\dbinom{n}{i}\left(  \sum_{k=0}^{i}\dbinom{i}{k}\dbinom
{m}{n-k}\right)  =\underbrace{\sum_{i=0}^{n}\sum_{k=0}^{i}}_{=\sum_{k=0}%
^{n}\sum_{i=k}^{n}}\underbrace{\dbinom{n}{i}\dbinom{i}{k}}_{\substack{=\dbinom
{n}{k}\dbinom{n-k}{i-k}\\\text{(by Proposition \ref{prop.binom.bin-id}
\textbf{(d)},}\\\text{applied to }a=k\text{ (since }i\geq k\text{))}}%
}\dbinom{m}{n-k}\\
&  =\sum_{k=0}^{n}\sum_{i=k}^{n}\dbinom{n}{k}\dbinom{n-k}{i-k}\dbinom{m}%
{n-k}=\sum_{k=0}^{n}\dbinom{n}{k}\dbinom{m}{n-k}\sum_{i=k}^{n}\dbinom
{n-k}{i-k}\\
&  =\sum_{k=0}^{n}\underbrace{\dbinom{n}{k}}_{\substack{=\dbinom{n}%
{n-k}\\\text{(by (\ref{eq.binom.symm}), applied to }n\text{ and }%
k\\\text{instead of }m\text{ and }n\text{ (since }n\geq k\text{))}}}\dbinom
{m}{n-k}\sum_{i=0}^{n-k}\dbinom{n-k}{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}i-k\text{ in the second sum}\right) \\
&  =\sum_{k=0}^{n}\dbinom{n}{n-k}\dbinom{m}{n-k}\sum_{i=0}^{n-k}\dbinom
{n-k}{i}=\sum_{k=0}^{n}\dbinom{n}{k}\dbinom{m}{k}\underbrace{\sum_{i=0}%
^{k}\dbinom{k}{i}}_{\substack{=2^{k}\\\text{(by
(\ref{pf.prop.binom.bin-id.e.4}), applied to }p=k\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}n-k\text{ in the first sum}\right) \\
&  =\sum_{k=0}^{n}\dbinom{n}{k}\dbinom{m}{k}2^{k}=\sum_{i=0}^{n}\dbinom{n}%
{i}\dbinom{m}{i}2^{i}%
\end{align*}
(here, we have renamed the summation index $k$ as $i$). This proves
Proposition \ref{prop.binom.bin-id} \textbf{(e)}.

\textbf{(f)} Let $a\in\mathbb{N}$, $b\in\mathbb{N}$ and $x\in\mathbb{Z}$. Let
us first work with polynomials in the indeterminate $X$ (rather than functions
in the variable $x\in\mathbb{Z}$). Recall that%
\begin{equation}
\dbinom{X}{a}\dbinom{X}{b}=\sum_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}%
{a+b-i}\dbinom{X}{i}. \label{pf.prop.binom.bin-id.f.1}%
\end{equation}
(Indeed, this is precisely the identity (\ref{sol.ps1.1.4.sol2.thm}) which was
proven in the Second proof of Proposition \ref{exe.ps1.1.4}.)

Clearly,%
\begin{align}
&  \sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\nonumber\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}+\sum
_{i=b+1}^{a+b}\dbinom{a}{i}\underbrace{\dbinom{b}{i}}_{\substack{=0\\\text{(by
(\ref{eq.binom.0}), applied to }b\text{ and }i\\\text{instead of }m\text{ and
}n\text{ (since }b<i\text{))}}}\dbinom{X+i}{a+b}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq b\leq a+b\right) \nonumber\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}+\underbrace{\sum
_{i=b+1}^{a+b}\dbinom{a}{i}0\dbinom{X+i}{a+b}}_{=0}\nonumber\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}.
\label{pf.prop.binom.bin-id.f.2}%
\end{align}


Theorem \ref{thm.vandermonde} (applied to $n=a+b$) yields%
\begin{equation}
\dbinom{X+Y}{a+b}=\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{Y}{a+b-k}.
\label{pf.prop.binom.bin-id.f.2a}%
\end{equation}


For every $i\in\left\{  0,1,\ldots,b\right\}  $, we have%
\[
\dbinom{X+i}{a+b}=\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{i}{a+b-k}.
\]
(This follows by substituting $Y=i$ in (\ref{pf.prop.binom.bin-id.f.2a}).)
Hence,%
\begin{align*}
&  \sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{b}{i}\underbrace{\dbinom{X+i}{a+b}%
}_{=\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{i}{a+b-k}}\\
&  =\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{b}{i}\sum_{k=0}^{a+b}\dbinom{X}%
{k}\dbinom{i}{a+b-k}\\
&  =\underbrace{\sum_{i=0}^{a+b}\sum_{k=0}^{a+b}}_{=\sum_{k=0}^{a+b}\sum
_{i=0}^{a+b}}\dbinom{a}{i}\underbrace{\dbinom{b}{i}\dbinom{X}{k}\dbinom
{i}{a+b-k}}_{=\dbinom{i}{a+b-k}\dbinom{b}{i}\dbinom{X}{k}}\\
&  =\sum_{k=0}^{a+b}\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom
{b}{i}\dbinom{X}{k}=\sum_{k=0}^{a+b}\dbinom{X}{k}\sum_{i=0}^{a+b}\dbinom{a}%
{i}\dbinom{i}{a+b-k}\dbinom{b}{i}.
\end{align*}
Compared with (\ref{pf.prop.binom.bin-id.f.2}), this yields%
\begin{align}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\nonumber\\
&  =\sum_{k=0}^{a+b}\dbinom{X}{k}\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom
{i}{a+b-k}\dbinom{b}{i}. \label{pf.prop.binom.bin-id.f.3}%
\end{align}
But for every $k\in\left\{  0,1,\ldots,a+b\right\}  $, we have%
\[
\sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom{b}{i}=\dbinom{a}%
{a+b-k}\sum_{j=0}^{k}\dbinom{k-b}{k-j}\dbinom{b}{a+b-j}%
\]
\footnote{\textit{Proof.} Let $k\in\left\{  0,1,\ldots,a+b\right\}  $. Then,
$a+b-k\in\left\{  0,1,\ldots,a+b\right\}  $, so that $0\leq a+b-k\leq a+b$.
Now,%
\begin{align*}
&  \sum_{i=0}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom{b}{i}\\
&  =\sum_{i=0}^{\left(  a+b-k\right)  -1}\dbinom{a}{i}\underbrace{\dbinom
{i}{a+b-k}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}), applied to }i\text{
and}\\a+b-k\text{ instead of }m\text{ and }n\\\text{(since }i<a+b-k\text{))}%
}}\dbinom{b}{i}+\sum_{i=a+b-k}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom
{b}{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq a+b-k\leq a+b\right) \\
&  =\underbrace{\sum_{i=0}^{\left(  a+b-k\right)  -1}\dbinom{a}{i}0\dbinom
{b}{i}}_{=0}+\sum_{i=a+b-k}^{a+b}\dbinom{a}{i}\dbinom{i}{a+b-k}\dbinom{b}{i}\\
&  =\sum_{i=a+b-k}^{a+b}\underbrace{\dbinom{a}{i}\dbinom{i}{a+b-k}%
}_{\substack{=\dbinom{a}{a+b-k}\dbinom{a-\left(  a+b-k\right)  }{i-\left(
a+b-k\right)  }\\\text{(by Proposition \ref{prop.binom.bin-id} \textbf{(d)},
applied to}\\a\text{ and }a+b-k\text{ instead of }n\text{ and }a\text{ (since
}i\geq a+b-k\text{))}}}\dbinom{b}{i}\\
&  =\sum_{i=a+b-k}^{a+b}\dbinom{a}{a+b-k}\dbinom{a-\left(  a+b-k\right)
}{i-\left(  a+b-k\right)  }\dbinom{b}{i}\\
&  =\sum_{j=0}^{k}\dbinom{a}{a+b-k}\underbrace{\dbinom{a-\left(  a+b-k\right)
}{\left(  a+b-j\right)  -\left(  a+b-k\right)  }}_{\substack{=\dbinom
{k-b}{k-j}\\\text{(since }a-\left(  a+b-k\right)  =k-b\text{ and}\\\left(
a+b-j\right)  -\left(  a+b-k\right)  =k-j\text{)}}}\dbinom{b}{a+b-j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }a+b-j\text{
for }i\text{ in the sum}\right) \\
&  =\sum_{j=0}^{k}\dbinom{a}{a+b-k}\dbinom{k-b}{k-j}\dbinom{b}{a+b-j}%
=\dbinom{a}{a+b-k}\sum_{j=0}^{k}\dbinom{k-b}{k-j}\dbinom{b}{a+b-j},
\end{align*}
qed.}. Hence, (\ref{pf.prop.binom.bin-id.f.3}) becomes%
\begin{align}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\nonumber\\
&  =\sum_{k=0}^{a+b}\dbinom{X}{k}\underbrace{\sum_{i=0}^{a+b}\dbinom{a}%
{i}\dbinom{i}{a+b-k}\dbinom{b}{i}}_{=\dbinom{a}{a+b-k}\sum_{j=0}^{k}%
\dbinom{k-b}{k-j}\dbinom{b}{a+b-j}}\nonumber\\
&  =\sum_{k=0}^{a+b}\dbinom{X}{k}\dbinom{a}{a+b-k}\sum_{j=0}^{k}\dbinom
{k-b}{k-j}\dbinom{b}{a+b-j}\nonumber\\
&  =\sum_{i=0}^{a+b}\dbinom{X}{i}\dbinom{a}{a+b-i}\sum_{j=0}^{i}\dbinom
{i-b}{i-j}\dbinom{b}{a+b-j} \label{pf.prop.binom.bin-id.f.7}%
\end{align}
(here, we renamed the summation index $k$ as $i$ in the first sum).
Furthermore, every $i\in\left\{  0,1,\ldots,a+b\right\}  $ satisfies
\[
\sum_{j=0}^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}=\dbinom{i}{a}%
\]
\footnote{\textit{Proof.} Let $i\in\left\{  0,1,\ldots,a+b\right\}  $. Thus,
$0\leq i\leq a+b$. We have
\begin{align}
\sum_{j=0}^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}  &  =\sum_{k=0}%
^{i}\underbrace{\dbinom{i-b}{i-\left(  i-k\right)  }}_{\substack{=\dbinom
{i-b}{k}\\\text{(since }i-\left(  i-k\right)  =k\text{)}}}\underbrace{\dbinom
{b}{a+b-\left(  i-k\right)  }}_{\substack{=\dbinom{b}{\left(  a+b\right)
+k-i}\\\text{(since }a+b-\left(  i-k\right)  =\left(  a+b\right)
+k-i\text{))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i-k\text{ for
}j\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=0}^{i}\dbinom{i-b}{k}\dbinom{b}{\left(  a+b\right)  +k-i}.
\label{pf.prop.binom.bin-id.f.8.pf.1}%
\end{align}
On the other hand, we have $b\in\mathbb{N}$, $\left(  i-b\right)  +b=i\geq0$
and $a\geq i-b$ (since $a+b\geq i$). Therefore, we can apply Proposition
\ref{prop.vandermonde.consequences} \textbf{(g)} to $i-b$, $b$ and $a$ instead
of $x$, $y$ and $n$. As a result, we obtain%
\[
\dbinom{\left(  i-b\right)  +b}{a}=\sum_{k=0}^{\left(  i-b\right)  +b}%
\dbinom{i-b}{k}\underbrace{\dbinom{b}{a+k-\left(  i-b\right)  }}%
_{\substack{=\dbinom{b}{\left(  a+b\right)  +k-i}\\\text{(since }a+k-\left(
i-b\right)  =\left(  a+b\right)  +k-i\text{)}}}=\sum_{k=0}^{\left(
i-b\right)  +b}\dbinom{i-b}{k}\dbinom{b}{\left(  a+b\right)  +k-i}.
\]
Since $\left(  i-b\right)  +b=i$, this rewrites as
\[
\dbinom{i}{a}=\sum_{k=0}^{i}\dbinom{i-b}{k}\dbinom{b}{\left(  a+b\right)
+k-i}.
\]
Compared with (\ref{pf.prop.binom.bin-id.f.8.pf.1}), this yields%
\[
\sum_{j=0}^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}=\dbinom{i}{a},
\]
qed.}.

Hence, (\ref{pf.prop.binom.bin-id.f.7}) becomes%
\begin{align*}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{X+i}{a+b}\\
&  =\sum_{i=0}^{a+b}\dbinom{X}{i}\dbinom{a}{a+b-i}\underbrace{\sum_{j=0}%
^{i}\dbinom{i-b}{i-j}\dbinom{b}{a+b-j}}_{=\dbinom{i}{a}}\\
&  =\sum_{i=0}^{a+b}\underbrace{\dbinom{X}{i}\dbinom{a}{a+b-i}\dbinom{i}{a}%
}_{=\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}}=\sum_{i=0}^{a+b}\dbinom{i}%
{a}\dbinom{a}{a+b-i}\dbinom{X}{i}\\
&  =\sum_{i=0}^{a-1}\underbrace{\dbinom{i}{a}}_{\substack{=0\\\text{(by
(\ref{eq.binom.0}), applied to }i\text{ and }a\\\text{instead of }m\text{ and
}n\text{ (since }i<a\text{))}}}\dbinom{a}{a+b-i}\dbinom{X}{i}+\sum_{i=a}%
^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq a\leq a+b\right) \\
&  =\underbrace{\sum_{i=0}^{a-1}0\dbinom{a}{a+b-i}\dbinom{X}{i}}_{=0}%
+\sum_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}\\
&  =\sum_{i=a}^{a+b}\dbinom{i}{a}\dbinom{a}{a+b-i}\dbinom{X}{i}=\dbinom{X}%
{a}\dbinom{X}{b}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.binom.bin-id.f.1})}\right)  .
\end{align*}
Substituting $X=x$ in this equality, we obtain%
\[
\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{x+i}{a+b}=\dbinom{x}{a}%
\dbinom{x}{b}.
\]
This proves Proposition \ref{prop.binom.bin-id} \textbf{(f)}.

\textbf{(g)} Let $a\in\mathbb{N}$, $b\in\mathbb{N}$ and $x\in\mathbb{Z}$. From
(\ref{eq.binom.upper-neg}) (applied to $m=-x-1$ and $n=a$), we obtain
$\dbinom{-x-1}{a}=\left(  -1\right)  ^{a}\dbinom{a-\left(  -x-1\right)  -1}%
{a}=\left(  -1\right)  ^{a}\dbinom{a+x}{a}$ (since $a-\left(  -x-1\right)
-1=a+x$). The same argument (applied to $b$ instead of $a$) shows that
$\dbinom{-x-1}{b}=\left(  -1\right)  ^{b}\dbinom{b+x}{b}$.

Now, Proposition \ref{prop.binom.bin-id} \textbf{(f)} (applied to $-x-1$
instead of $x$) shows that%
\begin{align}
\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\dbinom{\left(  -x-1\right)  +i}{a+b}
&  =\underbrace{\dbinom{-x-1}{a}}_{=\left(  -1\right)  ^{a}\dbinom{a+x}{a}%
}\underbrace{\dbinom{-x-1}{b}}_{=\left(  -1\right)  ^{b}\dbinom{b+x}{b}%
}\nonumber\\
&  =\left(  -1\right)  ^{a}\dbinom{a+x}{a}\left(  -1\right)  ^{b}\dbinom
{b+x}{b}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{a}\left(  -1\right)  ^{b}}_{=\left(
-1\right)  ^{a+b}}\dbinom{a+x}{a}\dbinom{b+x}{b}\nonumber\\
&  =\left(  -1\right)  ^{a+b}\dbinom{a+x}{a}\dbinom{b+x}{b}.
\label{pf.prop.binom.bin-id.g.3}%
\end{align}
But every $i\in\left\{  0,1,\ldots,b\right\}  $ satisfies%
\begin{align*}
\dbinom{\left(  -x-1\right)  +i}{a+b}  &  =\left(  -1\right)  ^{a+b}%
\dbinom{a+b-\left(  \left(  -x-1\right)  +i\right)  -1}{a+b}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.binom.upper-neg}), applied to
}m=\left(  -x-1\right)  +i\text{ and }n=a+b\right) \\
&  =\left(  -1\right)  ^{a+b}\dbinom{a+b+x-i}{a+b}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a+b-\left(  \left(  -x-1\right)
+i\right)  -1=a+b+x-i\right)  .
\end{align*}
Hence,%
\begin{align*}
&  \sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\underbrace{\dbinom{\left(
-x-1\right)  +i}{a+b}}_{=\left(  -1\right)  ^{a+b}\dbinom{a+b+x-i}{a+b}}\\
&  =\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}\left(  -1\right)  ^{a+b}%
\dbinom{a+b+x-i}{a+b}=\left(  -1\right)  ^{a+b}\sum_{i=0}^{b}\dbinom{a}%
{i}\dbinom{b}{i}\dbinom{a+b+x-i}{a+b}.
\end{align*}
Comparing this with (\ref{pf.prop.binom.bin-id.g.3}), we obtain%
\[
\left(  -1\right)  ^{a+b}\sum_{i=0}^{b}\dbinom{a}{i}\dbinom{b}{i}%
\dbinom{a+b+x-i}{a+b}=\left(  -1\right)  ^{a+b}\dbinom{a+x}{a}\dbinom{b+x}%
{b}.
\]
We can cancel $\left(  -1\right)  ^{a+b}$ from this equality (since $\left(
-1\right)  ^{a+b}\neq0$), and thus obtain $\sum_{i=0}^{b}\dbinom{a}{i}%
\dbinom{b}{i}\dbinom{a+b+x-i}{a+b}=\dbinom{a+x}{a}\dbinom{b+x}{b}$. This
proves Proposition \ref{prop.binom.bin-id} \textbf{(g)}.
\end{proof}

Many more examples of equalities with binomial coefficients, as well as
advanced tactics for proving such equalities, can be found in \cite[Chapter
5]{GKP}.

\subsection{Additional exercises}

This section contains some further exercises. These will not be used in the
rest of the notes, and they can be skipped at will\footnote{The same, of
course, can be said for many of the standard exercises.}. I am not planning to
provide solutions for them.

\begin{addexercise}
\label{exeadd.prop.vandermonde.consequences.f}Find a different proof of
Proposition \ref{prop.vandermonde.consequences} \textbf{(f)} that uses a
double-counting argument (i.e., counting some combinatorial objects in two
different ways, and then concluding that the results are equal).

[\textbf{Hint:} How many $\left(  x+y+1\right)  $-element subsets does the set
$\left\{  1,2,\ldots,n+1\right\}  $ have? Now, for a given $k\in\left\{
0,1,\ldots,n\right\}  $, how many $\left(  x+y+1\right)  $-element subsets
whose $\left(  x+1\right)  $-th smallest element is $k+1$ does the set
$\left\{  1,2,\ldots,n+1\right\}  $ have?]
\end{addexercise}

\begin{addexercise}
\label{exeadd.multichoose}Let $n\in\mathbb{N}$ and $k\in\mathbb{N}$ be fixed.
Show that the number of all $k$-tuples $\left(  a_{1},a_{2},\ldots
,a_{k}\right)  \in\mathbb{N}^{k}$ satisfying $a_{1}+a_{2}+\cdots+a_{k}=n$
equals $\dbinom{n+k-1}{k}$.
\end{addexercise}

\begin{remark}
Additional exercise \ref{exeadd.multichoose} can be restated in terms of
multisets. Namely, let $n\in\mathbb{N}$ and $k\in\mathbb{N}$ be fixed. Also,
fix a $k$-element set $K$. Then, the number of $n$-element multisets whose
elements all belong to $K$ is $\dbinom{n+k-1}{k}$. Indeed, we can WLOG assume
that $K=\left\{  1,2,\ldots,k\right\}  $ (otherwise, just relabel the elements
of $K$); then, the multisets whose elements all belong to $K$ are in bijection
with the $k$-tuples $\left(  a_{1},a_{2},\ldots,a_{k}\right)  \in
\mathbb{N}^{k}$. The bijection sends a multiset $M$ to the $k$-tuple $\left(
m_{1}\left(  M\right)  ,m_{2}\left(  M\right)  ,\ldots,m_{k}\left(  M\right)
\right)  $, where each $m_{i}\left(  M\right)  $ is the multiplicity of the
element $i$ in $M$. The size of a multiset $M$ corresponds to the sum
$a_{1}+a_{2}+\cdots+a_{k}$ of the entries of the resulting $k$-tuple; thus, we
get a bijection between

\begin{itemize}
\item the $n$-element multisets whose elements all belong to $K$
\end{itemize}

and

\begin{itemize}
\item the $k$-tuples $\left(  a_{1},a_{2},\ldots,a_{k}\right)  \in
\mathbb{N}^{k}$ satisfying $a_{1}+a_{2}+\cdots+a_{k}=n$.
\end{itemize}

As a consequence, Additional exercise \ref{exeadd.multichoose} shows that the
number of the former multisets is $\dbinom{n+k-1}{k}$.

Similarly, we can reinterpret the classical combinatorial interpretation of
$\dbinom{k}{n}$ (as the number of $n$-element subsets of $\left\{
1,2,\ldots,k\right\}  $) as follows: The number of all $k$-tuples $\left(
a_{1},a_{2},\ldots,a_{k}\right)  \in\left\{  0,1\right\}  ^{k}$ satisfying
$a_{1}+a_{2}+\cdots+a_{k}=n$ equals $\dbinom{k}{n}$.
\end{remark}

\begin{addexercise}
\label{exeadd.multichoose-app}Let $n\in\mathbb{N}$ and $k\in\mathbb{N}$. Prove
that%
\[
\sum_{u=0}^{k}\dbinom{n+u-1}{u}\dbinom{n}{k-2u}=\dbinom{n+k-1}{k}.
\]
Here, $\dbinom{a}{b}$ is defined to be $0$ when $b<0$.
\end{addexercise}

\begin{noncompile}
See \url{http://www.cip.ifi.lmu.de/~grinberg/QEDMO4P13.pdf} for a solution to
the preceding exercise.
\end{noncompile}

\begin{addexercise}
\label{exeadd.bininv}Let $N\in\mathbb{N}$. The \textit{binomial transform} of
a finite sequence $\left(  f_{0},f_{1},\ldots,f_{N}\right)  \in\mathbb{Z}%
^{N+1}$ is defined to be the sequence $\left(  g_{0},g_{1},\ldots
,g_{N}\right)  $ defined by%
\[
g_{n}=\sum_{i=0}^{n}\left(  -1\right)  ^{i}\dbinom{n}{i}f_{i}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\left\{  0,1,\ldots,N\right\}  .
\]


\textbf{(a)} Let $\left(  f_{0},f_{1},\ldots,f_{N}\right)  \in\mathbb{Z}%
^{N+1}$ be a finite sequence of integers. Let $\left(  g_{0},g_{1}%
,\ldots,g_{N}\right)  $ be the binomial transform of $\left(  f_{0}%
,f_{1},\ldots,f_{N}\right)  $. Show that $\left(  f_{0},f_{1},\ldots
,f_{N}\right)  $ is, in turn, the binomial transform of $\left(  g_{0}%
,g_{1},\ldots,g_{N}\right)  $.

\textbf{(b)} Find the binomial transform of the sequence $\left(
1,1,\ldots,1\right)  $.

\textbf{(c)} For any given $a\in\mathbb{N}$, find the binomial transform of
the sequence $\left(  \dbinom{0}{a},\dbinom{1}{a},\ldots,\dbinom{N}{a}\right)
$.

\textbf{(d)} For any given $q\in\mathbb{Z}$, find the binomial transform of
the sequence $\left(  q^{0},q^{1},\ldots,q^{N}\right)  $.

\textbf{(e)} Find the binomial transform of the sequence $\left(
1,0,1,0,1,0,\ldots\right)  $ (this ends with $1$ if $N$ is even, and with $0$
if $N$ is odd).

\textbf{(f)} Let $B:\mathbb{Z}^{N+1}\rightarrow\mathbb{Z}^{N+1}$ be the map
which sends every sequence $\left(  f_{0},f_{1},\ldots,f_{N}\right)
\in\mathbb{Z}^{N+1}$ to its binomial transform $\left(  g_{0},g_{1}%
,\ldots,g_{N}\right)  \in\mathbb{Z}^{N+1}$. Thus, part \textbf{(a)} of this
exercise states that $B^{2}=\operatorname*{id}$.

On the other hand, let $W:\mathbb{Z}^{N+1}\rightarrow\mathbb{Z}^{N+1}$ be the
map which sends every sequence $\left(  f_{0},f_{1},\ldots,f_{N}\right)
\in\mathbb{Z}^{N+1}$ to $\left(  \left(  -1\right)  ^{N}f_{N},\left(
-1\right)  ^{N}f_{N-1},\ldots,\left(  -1\right)  ^{N}f_{0}\right)
\in\mathbb{Z}^{N+1}$. It is rather clear that $W^{2}=\operatorname*{id}$.

Show that, furthermore, $B\circ W\circ B=W\circ B\circ W$ and $\left(  B\circ
W\right)  ^{3}=\operatorname*{id}$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.AoPS333199}For any $n\in\mathbb{N}$ and $m\in\mathbb{N}$, define
a polynomial $Z_{m,n}\in\mathbb{Z}\left[  X\right]  $ by%
\[
Z_{m,n}=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\left(
X^{n-k}-1\right)  ^{m}.
\]
Show that $Z_{m,n}=Z_{n,m}$ for any $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
\end{addexercise}

\begin{noncompile}
Two solutions to the preceding exercise are sketched in
\url{http://www.artofproblemsolving.com/community/c6h333199p1782800} . More
generally, $Y_{m,n}=Y_{n,m}$, where $Y_{m,n}$ is the polynomial%
\[
\sum_{k=0}^{n}Y^{k}\dbinom{n}{k}\left(  X^{n-k}+Y\right)  ^{m}.
\]

\end{noncompile}

\begin{addexercise}
\label{exeadd.AoPS262752}Let $n\in\mathbb{N}$. Prove%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\]
(an identity between polynomials in $\mathbb{Q}\left[  X\right]  $).

[\textbf{Hint:} It is enough to prove this when $X$ is replaced by a
nonnegative integer $r$ (why?). Now that you have gotten rid of polynomials,
introduce new polynomials. Namely, compute the coefficient of $X^{n}$ in
$\left(  1+X\right)  ^{r}\left(  1-X\right)  ^{r}$. Compare with the
coefficient of $X^{n}$ in $\left(  1-X^{2}\right)  ^{r}$.]
\end{addexercise}

\begin{noncompile}
Source: \cite[(5.55)]{GKP}; see also
\url{http://www.artofproblemsolving.com/community/c6h262752} .
\end{noncompile}

The following exercise is a variation on (\ref{eq.binom.int}):

\begin{addexercise}
\label{exeadd.choose.a/b}Let $a$ and $b$ be two integers such that $b\neq0$.
Let $n\in\mathbb{N}$. Show that there exists some $N\in\mathbb{N}$ such that
$b^{N}\dbinom{a/b}{n}\in\mathbb{Z}$.

[\textbf{Hint:} I am not aware of a combinatorial solution to this exercise!
(I.e., I don't know what the numbers $b^{N}\dbinom{a/b}{n}$ count, even when
they are nonnegative.) All solutions that I know use some (elementary) number
theory. For the probably slickest (although unmotivated) solution, basic
modular arithmetic suffices; here is a roadmap: First, show that if $b$ and
$c$ are integers such that $c>0$, then there exists an $s\in\mathbb{Z}$ such
that $b^{c-1}\equiv sb^{c}\operatorname{mod}c$\ \ \ \ \footnotemark. Apply
this to $c=n!$ and conclude that $b^{n!}\left(  a/b-i\right)  \equiv
b^{n!}\left(  sa-i\right)  \operatorname{mod}n!$ for every $i\in\mathbb{Z}$.
Now use $\dbinom{sa}{n}\in\mathbb{Z}$.]
\end{addexercise}

\footnotetext{To prove this, argue that at least two of $b^{0},b^{1}%
,\ldots,b^{c}$ are congruent modulo $c$.}

\section{\label{chp.recur}Recurrent sequences}

\subsection{Basics}

Two of the most famous integer sequences defined recursively are the Fibonacci
sequence and the Lucas sequence:

\begin{itemize}
\item The
\textit{\href{https://en.wikipedia.org/wiki/Fibonacci_number}{Fibonacci
sequence}} is the sequence $\left(  f_{0},f_{1},f_{2},\ldots\right)  $ of
integers which is defined recursively by $f_{0}=0$, $f_{1}=1$, and
$f_{n}=f_{n-1}+f_{n-2}$ for all $n\geq2$. Its first terms are%
\begin{align*}
f_{0}  &  =0,\ \ \ \ \ \ \ \ \ \ f_{1}=1,\ \ \ \ \ \ \ \ \ \ f_{2}%
=1,\ \ \ \ \ \ \ \ \ \ f_{3}=2,\ \ \ \ \ \ \ \ \ \ f_{4}%
=3,\ \ \ \ \ \ \ \ \ \ f_{5}=5,\\
f_{6}  &  =8,\ \ \ \ \ \ \ \ \ \ f_{7}=13,\ \ \ \ \ \ \ \ \ \ f_{8}%
=21,\ \ \ \ \ \ \ \ \ \ f_{9}=34,\ \ \ \ \ \ \ \ \ \ f_{10}=55,\\
f_{11}  &  =89,\ \ \ \ \ \ \ \ \ \ f_{12}=144,\ \ \ \ \ \ \ \ \ \ f_{13}=233.
\end{align*}
(Some authors prefer to start the sequence at $f_{1}$ rather than $f_{0}$; of
course, the recursive definition then needs to be modified to require
$f_{2}=1$ instead of $f_{0}=0$.)

\item The \textit{\href{https://en.wikipedia.org/wiki/Lucas_number}{Lucas
sequence}} is the sequence $\left(  \ell_{0},\ell_{1},\ell_{2},\ldots\right)
$ of integers which is defined recursively by $\ell_{0}=2$, $\ell_{1}=1$, and
$\ell_{n}=\ell_{n-1}+\ell_{n-2}$ for all $n\geq2$. Its first terms are%
\begin{align*}
\ell_{0}  &  =2,\ \ \ \ \ \ \ \ \ \ \ell_{1}=1,\ \ \ \ \ \ \ \ \ \ \ell
_{2}=3,\ \ \ \ \ \ \ \ \ \ \ell_{3}=4,\ \ \ \ \ \ \ \ \ \ \ell_{4}%
=7,\ \ \ \ \ \ \ \ \ \ \ell_{5}=11,\\
\ell_{6}  &  =18,\ \ \ \ \ \ \ \ \ \ \ell_{7}=29,\ \ \ \ \ \ \ \ \ \ \ell
_{8}=47,\ \ \ \ \ \ \ \ \ \ \ell_{9}=76,\ \ \ \ \ \ \ \ \ \ \ell_{10}=123,\\
\ell_{11}  &  =199,\ \ \ \ \ \ \ \ \ \ \ell_{12}=322,\ \ \ \ \ \ \ \ \ \ \ell
_{13}=521.
\end{align*}

\end{itemize}

A lot of papers have been written about these two sequences, the relations
between them, and the identities that hold for their terms.\footnote{See
\url{https://oeis.org/A000045} and \url{https://oeis.org/A000032} for an
overview of their properties.} One of their most striking properties is that
they can be computed explicitly, albeit using irrational numbers. In fact, the
\textit{Binet formula} says that the $n$-th Fibonacci number $f_{n}$ can be
computed by%
\begin{equation}
f_{n}=\dfrac{1}{\sqrt{5}}\varphi^{n}-\dfrac{1}{\sqrt{5}}\psi^{n},
\label{eq.binet.f}%
\end{equation}
where $\varphi=\dfrac{1+\sqrt{5}}{2}$ and $\psi=\dfrac{1-\sqrt{5}}{2}$ are the
two solutions of the quadratic equation $X^{2}-X-1=0$. (The number $\varphi$
is known as the \textit{golden ratio}; the number $\psi$ can be obtained from
it by $\psi=1-\varphi=-1/\varphi$.) A similar formula, using the very same
numbers $\varphi$ and $\psi$, exists for the Lucas numbers:%
\begin{equation}
\ell_{n}=\varphi^{n}+\psi^{n}. \label{eq.binet.l}%
\end{equation}


\begin{remark}
How easy is it to compute $f_{n}$ and $\ell_{n}$ using the formulas
(\ref{eq.binet.f}) and (\ref{eq.binet.l})?

This is a nontrivial question. Indeed, if you are careless, you may find them
rather useless. For instance, if you try to compute $f_{n}$ using the formula
(\ref{eq.binet.f}) and using approximate values for the irrational numbers
$\varphi$ and $\psi$, then you might end up with a wrong value for $f_{n}$,
because the error in the approximate value for $\varphi$ propagates when you
take $\varphi$ to the $n$-th power. (And for high enough $n$, the error will
become larger than $1$, so you will not be able to get the correct value by
rounding.) The greater $n$ is, the more precise you need a value for $\varphi$
to approximate $f_{n}$ this way. Thus, approximating $\varphi$ is not a good
way to compute $f_{n}$. (Actually, the opposite is true: You can use
(\ref{eq.binet.f}) to approximate $\varphi$ by computing Fibonacci numbers.
Namely, it is easy to show that $\varphi=\lim\limits_{n\rightarrow\infty
}\dfrac{f_{n}}{f_{n-1}}$.)

A better approach to using (\ref{eq.binet.f}) is to work with the exact values
of $\varphi$ and $\psi$. To do so, you need to know how to add, subtract,
multiply and divide real numbers of the form $a+b\sqrt{5}$ with $a,b\in
\mathbb{Q}$ without ever using approximations. (Clearly, $\varphi$, $\psi$ and
$\sqrt{5}$ all have this form.) There are rules for this, which are simple to
check:%
\begin{align*}
\left(  a+b\sqrt{5}\right)  +\left(  c+d\sqrt{5}\right)   &  =\left(
a+c\right)  +\left(  b+d\right)  \sqrt{5};\\
\left(  a+b\sqrt{5}\right)  -\left(  c+d\sqrt{5}\right)   &  =\left(
a-c\right)  +\left(  b-d\right)  \sqrt{5};\\
\left(  a+b\sqrt{5}\right)  \cdot\left(  c+d\sqrt{5}\right)   &  =\left(
ac+5bd\right)  +\left(  bc+ad\right)  \sqrt{5};\\
\dfrac{a+b\sqrt{5}}{c+d\sqrt{5}}  &  =\dfrac{\left(  ac-5bd\right)  +\left(
bc-ad\right)  \sqrt{5}}{c^{2}-5d^{2}}\ \ \ \ \ \ \ \ \ \ \text{for }\left(
c,d\right)  \neq\left(  0,0\right)  .
\end{align*}
(The last rule is an instance of \textquotedblleft rationalizing the
denominator\textquotedblright.) These rules give you a way to exactly compute
things like $\varphi^{n}$, $\dfrac{1}{\sqrt{5}}\varphi^{n}$, $\psi^{n}$ and
$\dfrac{1}{\sqrt{5}}\psi^{n}$, and thus also $f_{n}$ and $\ell_{n}$. If you
use
\href{https://en.wikipedia.org/wiki/Exponentiation_by_squaring}{exponentiation
by squaring} to compute $n$-th powers, this actually becomes a fast algorithm
(a lot faster than just computing $f_{n}$ and $\ell_{n}$ using the
recurrence). So, yes, (\ref{eq.binet.f}) and (\ref{eq.binet.l}) are useful.
\end{remark}

We shall now study a generalization of both the Fibonacci and the Lucas
sequences, and generalize (\ref{eq.binet.f}) and (\ref{eq.binet.l}) to a
broader class of sequences.

\begin{definition}
\label{def.abrec} If $a$ and $b$ are two complex numbers, then a sequence
$\left(  x_{0},x_{1},x_{2},\ldots\right)  $ of complex numbers will be called
$\left(  a,b\right)  $\textit{-recurrent} if every $n\geq2$ satisfies%
\[
x_{n}=ax_{n-1}+bx_{n-2}.
\]

\end{definition}

So, the Fibonacci sequence and the Lucas sequences are $\left(  1,1\right)
$-recurrent. An $\left(  a,b\right)  $-recurrent sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $ is fully determined by the four values $a$, $b$,
$x_{0}$ and $x_{1}$, and can be constructed for any choice of these four
values. Here are some further examples of $\left(  a,b\right)  $-recurrent sequences:

\begin{itemize}
\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
2,-1\right)  $-recurrent if and only if every $n\geq2$ satisfies
$x_{n}=2x_{n-1}-x_{n-2}$. In other words, a sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $ is $\left(  2,-1\right)  $-recurrent if and only
if every $n\geq2$ satisfies $x_{n}-x_{n-1}=x_{n-1}-x_{n-2}$. In other words, a
sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(  2,-1\right)
$-recurrent if and only if $x_{1}-x_{0}=x_{2}-x_{1}=x_{3}-x_{2}=\cdots$. In
other words, the $\left(  2,-1\right)  $-recurrent sequences are precisely the
arithmetic progressions.

\item Geometric progressions are also $\left(  a,b\right)  $-recurrent for
appropriate $a$ and $b$. Namely, any geometric progression $\left(
u,uq,uq^{2},uq^{3},\ldots\right)  $ is $\left(  q,0\right)  $-recurrent, since
every $n\geq2$ satisfies $uq^{n}=q\cdot uq^{n-1}+0\cdot uq^{n-2}$. However,
not every $\left(  q,0\right)  $-recurrent sequence $\left(  x_{0},x_{1}%
,x_{2},\ldots\right)  $ is a geometric progression (since the condition
$x_{n}=qx_{n-1}+0x_{n-2}$ for all $n\geq2$ says nothing about $x_{0}$, and
thus $x_{0}$ can be arbitrary).

\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
0,1\right)  $-recurrent if and only if every $n\geq2$ satisfies $x_{n}%
=x_{n-2}$. In other words, a sequence $\left(  x_{0},x_{1},x_{2}%
,\ldots\right)  $ is $\left(  0,1\right)  $-recurrent if and only if it has
the form $\left(  u,v,u,v,u,v,\ldots\right)  $ for two complex numbers $u$ and
$v$.

\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
1,0\right)  $-recurrent if and only if every $n\geq2$ satisfies $x_{n}%
=x_{n-1}$. In other words, a sequence $\left(  x_{0},x_{1},x_{2}%
,\ldots\right)  $ is $\left(  1,0\right)  $-recurrent if and only if it has
the form $\left(  u,v,v,v,v,\ldots\right)  $ for two complex numbers $u$ and
$v$. Notice that $u$ is not required to be equal to $v$, because we never
claimed that $x_{n}=x_{n-1}$ holds for $n=1$.

\item A sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
1,-1\right)  $-recurrent if and only if every $n\geq2$ satisfies
$x_{n}=x_{n-1}-x_{n-2}$. Curiously, it turns out that every such sequence is
$6$-periodic (i.e., it satisfies $x_{n+6}=x_{n}$ for every $n\in\mathbb{N}$),
because every $n\in\mathbb{N}$ satisfies%
\begin{align*}
x_{n+6}  &  =\underbrace{x_{n+5}}_{=x_{n+4}-x_{n+3}}-x_{n+4}=\left(
x_{n+4}-x_{n+3}\right)  -x_{n+4}=-\underbrace{x_{n+3}}_{=x_{n+2}-x_{n+1}}\\
&  =-\left(  \underbrace{x_{n+2}}_{=x_{n+1}-x_{n}}-x_{n+1}\right)  =-\left(
x_{n+1}-x_{n}-x_{n+1}\right)  =x_{n}.
\end{align*}
More precisely, a sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is
$\left(  1,0\right)  $-recurrent if and only if it has the form $\left(
u,v,v-u,-u,-v,u-v,\ldots\right)  $ (where the \textquotedblleft$\ldots
$\textquotedblright\ stands for \textquotedblleft repeat the preceding $6$
values over and over\textquotedblright\ here) for two complex numbers $u$ and
$v$.

\item The above three examples notwithstanding, most $\left(  a,b\right)
$-recurrent sequences of course are not periodic. However, here is another
example which provides a great supply of non-periodic $\left(  a,b\right)
$-recurrent sequences and, at the same time, explains why we get so many
periodic ones: If $\alpha$ is any angle, then the sequences%
\begin{align*}
&  \left(  \sin\left(  0\alpha\right)  ,\sin\left(  1\alpha\right)
,\sin\left(  2\alpha\right)  ,\ldots\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
&  \left(  \cos\left(  0\alpha\right)  ,\cos\left(  1\alpha\right)
,\cos\left(  2\alpha\right)  ,\ldots\right)
\end{align*}
are $\left(  2\cos\alpha,-1\right)  $-recurrent. More generally, if $\alpha$
and $\beta$ are two angles, then the sequence%
\[
\left(  \sin\left(  \beta+0\alpha\right)  ,\sin\left(  \beta+1\alpha\right)
,\sin\left(  \beta+2\alpha\right)  ,\ldots\right)
\]
is $\left(  2\cos\alpha,-1\right)  $-recurrent\footnote{\textit{Proof.} Let
$\alpha$ and $\beta$ be two angles. We need to show that the sequence $\left(
\sin\left(  \beta+0\alpha\right)  ,\sin\left(  \beta+1\alpha\right)
,\sin\left(  \beta+2\alpha\right)  ,\ldots\right)  $ is $\left(  2\cos
\alpha,-1\right)  $-recurrent. In other words, we need to prove that%
\[
\sin\left(  \beta+n\alpha\right)  =2\cos\alpha\sin\left(  \beta+\left(
n-1\right)  \alpha\right)  +\left(  -1\right)  \sin\left(  \beta+\left(
n-2\right)  \alpha\right)
\]
for every $n\geq2$. So fix $n\geq2$.
\par
One of the well-known trigonometric identities states that $\sin x+\sin
y=2\sin\dfrac{x+y}{2}\cos\dfrac{x-y}{2}$ for any two angles $x$ and $y$.
Applying this to $x=\beta+n\alpha$ and $y=\beta+\left(  n-2\right)  \alpha$,
we obtain%
\begin{align*}
\sin\left(  \beta+n\alpha\right)  +\sin\left(  \beta+\left(  n-2\right)
\alpha\right)   &  =2\sin\underbrace{\dfrac{\left(  \beta+n\alpha\right)
+\left(  \beta+\left(  n-2\right)  \alpha\right)  }{2}}_{=\beta+\left(
n-1\right)  \alpha}\cos\underbrace{\dfrac{\left(  \beta+n\alpha\right)
-\left(  \beta+\left(  n-2\right)  \alpha\right)  }{2}}_{=\alpha}\\
&  =2\sin\left(  \beta+\left(  n-1\right)  \alpha\right)  \cos\alpha
=2\cos\alpha\sin\left(  \beta+\left(  n-1\right)  \alpha\right)  .
\end{align*}
Hence,
\begin{align*}
\sin\left(  \beta+n\alpha\right)   &  =2\cos\alpha\sin\left(  \beta+\left(
n-1\right)  \alpha\right)  -\sin\left(  \beta+\left(  n-2\right)
\alpha\right) \\
&  =2\cos\alpha\sin\left(  \beta+\left(  n-1\right)  \alpha\right)  +\left(
-1\right)  \sin\left(  \beta+\left(  n-2\right)  \alpha\right)  ,
\end{align*}
qed.}. When $\alpha\in2\pi\mathbb{Q}$ (that is, some integer multiple of
$\alpha$ equals some integer multiple of $2\pi$), this sequence is periodic.
\end{itemize}

\subsection{Explicit formulas (\`{a} la Binet)}

Now, we can get an explicit formula (similar to (\ref{eq.binet.f}) and
(\ref{eq.binet.l})) for every term of an $\left(  a,b\right)  $-recurrent
sequence (in terms of $a$, $b$, $x_{0}$ and $x_{1}$) in the case when
$a^{2}+4b\neq0$. Here is how this works:

\begin{remark}
\label{rmk.binet}Let $a$ and $b$ be complex numbers such that $a^{2}+4b\neq0$.
Let $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ be an $\left(  a,b\right)
$-recurrent sequence. We want to construct an explicit formula for each
$x_{n}$ in terms of $x_{0}$, $x_{1}$, $a$ and $b$.

To do so, we let $q_{+}$ and $q_{-}$ be the two solutions of the quadratic
equation $X^{2}-aX-b=0$, namely%
\[
q_{+}=\dfrac{a+\sqrt{a^{2}+4b}}{2}\ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ q_{-}=\dfrac{a-\sqrt{a^{2}+4b}}{2}.
\]
We notice that $q_{+}\neq q_{-}$ (since $a^{2}+4b\neq0$). It is easy to see
that the sequences $\left(  1,q_{+},q_{+}^{2},q_{+}^{3},\ldots\right)  $ and
$\left(  1,q_{-},q_{-}^{2},q_{-}^{3},\ldots\right)  $ are $\left(  a,b\right)
$-recurrent. As a consequence, for any two complex numbers $\lambda_{+}$ and
$\lambda_{-}$, the sequence%
\[
\left(  \lambda_{+}+\lambda_{-},\lambda_{+}q_{+}+\lambda_{-}q_{-},\lambda
_{+}q_{+}^{2}+\lambda_{-}q_{-}^{2},\ldots\right)
\]
(the $n$-th term of this sequence, with $n$ starting at $0$, is $\lambda
_{+}q_{+}^{n}+\lambda_{-}q_{-}^{n}$) must also be $\left(  a,b\right)
$-recurrent (check this!). We denote this sequence by $L_{\lambda_{+}%
,\lambda_{-}}$.

We now need to find two complex numbers $\lambda_{+}$ and $\lambda_{-}$ such
that this sequence $L_{\lambda_{+},\lambda_{-}}$ is our sequence $\left(
x_{0},x_{1},x_{2},\ldots\right)  $. In order to do so, we only need to ensure
that $\lambda_{+}+\lambda_{-}=x_{0}$ and $\lambda_{+}q_{+}+\lambda_{-}%
q_{-}=x_{1}$ (because once this holds, it will follow that the sequences
$L_{\lambda_{+},\lambda_{-}}$ and $\left(  x_{0},x_{1},x_{2},\ldots\right)  $
have the same first two terms; and this will yield that these two sequences
are identical, because two $\left(  a,b\right)  $-recurrent sequences with the
same first two terms must be identical). That is, we need to solve the system
of linear equations%
\[
\left\{
\begin{array}
[c]{c}%
\lambda_{+}+\lambda_{-}=x_{0};\\
\lambda_{+}q_{+}+\lambda_{-}q_{-}=x_{1}%
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{in the unknowns }\lambda_{+}\text{ and
}\lambda_{-}.
\]
Thanks to $q_{+}\neq q_{-}$, this system has a unique solution:%
\[
\lambda_{+}=\dfrac{x_{1}-q_{-}x_{0}}{q_{+}-q_{-}};\ \ \ \ \ \ \ \ \ \ \lambda
_{-}=\dfrac{q_{+}x_{0}-x_{1}}{q_{+}-q_{-}}.
\]
Thus, if we set $\left(  \lambda_{+},\lambda_{-}\right)  $ to be this
solution, then $\left(  x_{0},x_{1},x_{2},\ldots\right)  =L_{\lambda
_{+},\lambda_{-}}$, so that%
\begin{equation}
x_{n}=\lambda_{+}q_{+}^{n}+\lambda_{-}q_{-}^{n}
\label{rmk.recursive.binet-general}%
\end{equation}
for every nonnegative integer $n$. This is an explicit formula, at least if
the square roots do not disturb you. When $x_{0}=x_{1}=a=b=1$, you get the
famous Binet formula (\ref{eq.binet.f}) for the Fibonacci sequence.
\end{remark}

In the next exercise you will see what happens if the $a^{2}+4b\neq0$
condition does not hold.

\begin{exercise}
\label{exe.ps2.2.1}Let $a$ and $b$ be complex numbers such that $a^{2}+4b=0$.
Consider an $\left(  a,b\right)  $-recurrent sequence $\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  $. Find an explicit formula for each $x_{n}$ in
terms of $x_{0}$, $x_{1}$, $a$ and $b$.

[\textbf{Note:} The polynomial $X^{2}-aX-b$ has a double root here. Unlike the
case of two distinct roots studied above, you won't see any radicals here. The
explicit formula really deserves the name \textquotedblleft
explicit\textquotedblright.]
\end{exercise}

Remark \ref{rmk.binet} and Exercise \ref{exe.ps2.2.1}, combined, solve the
problem of finding an explicit formula for any term of an $\left(  a,b\right)
$-recurrent sequence when $a$ and $b$ are complex numbers, at least if you
don't mind having square roots in your formula. Similar tactics can be used to
find explicit forms for the more general case of sequences satisfying
\textquotedblleft homogeneous linear recurrences with constant
coefficients\textquotedblright\footnote{These are sequences $\left(
x_{0},x_{1},x_{2},\ldots\right)  $ which satisfy%
\[
\left(  x_{n}=c_{1}x_{n-1}+c_{2}x_{n-2}+\cdots+c_{k}x_{n-k}%
\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq k\right)
\]
for a fixed $k\in\mathbb{N}$ and a fixed $k$-tuple $\left(  c_{1},c_{2}%
,\ldots,c_{k}\right)  $ of complex numbers. When $k=2$, these are the $\left(
c_{1},c_{2}\right)  $-recurrent sequences.}, although instead of square roots
you will now need roots of higher-degree polynomials. (See \cite[\S 22.3.2
(\textquotedblleft Solving Homogeneous Linear Recurrences\textquotedblright%
)]{LeLeMe16} for an outline of this; see also \cite[Topic \textquotedblleft
Linear Recurrences\textquotedblright]{Hefferon} for a linear-algebraic introduction.)

\subsection{Further results}

Here are some more exercises from the theory of recurrent sequences. I am not
going particularly deep here, but we may encounter generalizations later.

First, an example: If we \textquotedblleft split\textquotedblright\ the
Fibonacci sequence
\[
\left(  f_{0},f_{1},f_{2},\ldots\right)  =\left(  1,1,2,3,5,8,\ldots\right)
\]
into two subsequences
\[
\left(  f_{0},f_{2},f_{4},\ldots\right)  =\left(  1,2,5,13,\ldots\right)
\qquad\text{and} \qquad\left(  f_{1},f_{3},f_{5},\ldots\right)  =\left(
1,3,8,21,\ldots\right)
\]
(each of which contains every other Fibonacci number), then it turns out that
each of these two subsequences is $\left(  3,-1\right)  $%
-recurrent\footnote{In other words, we have $f_{2n}=3f_{2\left(  n-1\right)
}+\left(  -1\right)  f_{2\left(  n-2\right)  }$ and $f_{2n+1}=3f_{2\left(
n-1\right)  +1}+\left(  -1\right)  f_{2\left(  n-2\right)  +1}$ for every
$n\geq2$.}. This is rather easy to prove, but one can always ask for
generalizations: What happens if we start with an arbitrary $\left(
a,b\right)  $-recurrent sequence, instead of the Fibonacci numbers? What
happens if we split it into three, four or more subsequences? The answer is
rather nice:

\begin{exercise}
\label{exe.ps2.2.2}Let $a$ and $b$ be complex numbers. Let $\left(
x_{0},x_{1},x_{2},\ldots\right)  $ be an $\left(  a,b\right)  $-recurrent sequence.

\textbf{(a)} Prove that the sequences $\left(  x_{0},x_{2},x_{4}%
,\ldots\right)  $ and $\left(  x_{1},x_{3},x_{5},\ldots\right)  $ are $\left(
c,d\right)  $-recurrent for some complex numbers $c$ and $d$. Find these $c$
and $d$.

\textbf{(b)} Prove that the sequences $\left(  x_{0},x_{3},x_{6}%
,\ldots\right)  $, $\left(  x_{1},x_{4},x_{7},\ldots\right)  $ and $\left(
x_{2},x_{5},x_{8},\ldots\right)  $ are $\left(  c,d\right)  $-recurrent for
some (other) complex numbers $c$ and $d$.

\textbf{(c)} For every nonnegative integers $N$ and $K$, prove that the
sequence $\left(  x_{K},x_{N+K},x_{2N+K},x_{3N+K},\ldots\right)  $ is $\left(
c,d\right)  $-recurrent for some complex numbers $c$ and $d$ which depend only
on $N$, $a$ and $b$ (but not on $K$ or $x_{0}$ or $x_{1}$).
\end{exercise}

The next exercise gives a combinatorial interpretation of the Fibonacci numbers:

\begin{exercise}
\label{exe.ps2.2.3}Recall that the Fibonacci numbers $f_{0},f_{1},f_{2}%
,\ldots$ are defined recursively by $f_{0}=0$, $f_{1}=1$ and $f_{n}%
=f_{n-1}+f_{n-2}$ for all $n\geq2$. For every positive integer $n$, show that
$f_{n}$ is the number of subsets $I$ of $\left\{  1,2,\ldots,n-2\right\}  $
such that no two elements of $I$ are consecutive (i.e., there exists no
$i\in\mathbb{Z}$ such that both $i$ and $i+1$ belong to $I$). For instance,
for $n=5$, these subsets are $\varnothing$, $\left\{  1\right\}  $, $\left\{
2\right\}  $, $\left\{  3\right\}  $ and $\left\{  1,3\right\}  $.
\end{exercise}

Notice that $\left\{  1,2,\ldots,-1\right\}  $ is to be understood as the
empty set (since there are no integers $x$ satisfying $1\leq x\leq-1$). (So
Exercise \ref{exe.ps2.2.3}, applied to $n=1$, says that $f_{1}$ is the number
of subsets $I$ of the empty set such that no two elements of $I$ are
consecutive. This is correct, because the empty set has only one subset, which
of course is empty and thus has no consecutive elements; and the Fibonacci
number $f_{1}$ is precisely $1$.)

\begin{remark}
\label{rmk.fib.dominos}Exercise \ref{exe.ps2.2.3} is equivalent to another
known combinatorial interpretation of the Fibonacci numbers.

Namely, let $n$ be a positive integer. Consider a rectangular table of
dimensions $2\times\left(  n-1\right)  $ (that is, with $2$ rows and $n-1$
columns). How many ways are there to subdivide this table into dominos? (A
\textit{domino} means a set of two adjacent boxes.)

For $n=5$, there are $5$ ways:%
\begin{align*}
&
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
%{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
%& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
%\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
\end{tabular}%
%EndExpansion
\ ,\ \ \ \ \ \ \ \ \ \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
%& \phantom{a} \\ \cline{3-4}
%\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
& \phantom{a} \\ \cline{3-4}
\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ ,\ \ \ \ \ \ \ \ \ \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
%& \phantom{a} \\ \cline{1-2}
%\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
& \phantom{a} \\ \cline{1-2}
\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ ,\\
&
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
%& \phantom{a} \\ \cline{2-3}
%\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
& \phantom{a} \\ \cline{2-3}
\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ ,\ \ \ \ \ \ \ \ \ \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
%\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%EndExpansion
\ .
\end{align*}
In the general case, there are $f_{n-1}$ ways. Why?

As promised, this result is equivalent to Exercise \ref{exe.ps2.2.3}. Let us
see why. Let $P$ be a way to subdivide the table into dominos. We say that a
\textit{horizontal domino} is a domino which consists of two adjacent boxes in
the same row; similarly, we define a vertical domino. It is easy to see that
(in the subdivision $P$) each column of the table is covered either by a
single vertical domino, or by two horizontal dominos (in which case either
both of them \textquotedblleft begin\textquotedblright\ in this column, or
both of them \textquotedblleft end\textquotedblright\ in this column). Let
$J\left(  P\right)  $ be the set of all $i\in\left\{  1,2,\ldots,n-1\right\}
$ such that the $i$-th column of the table is covered by two horizontal
dominos, both of which \textquotedblleft begin\textquotedblright\ in this
column. For instance,%
\begin{align*}
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
%{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
%& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
%\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \multicolumn{1}%
{c}{\phantom{a}} & \phantom{a} \\ \hline\multicolumn{1}{| c}{\phantom{a}}
& \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a} \\ \hline
\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  1,3\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
%& \phantom{a} \\ \cline{3-4}
%\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}}
& \phantom{a} \\ \cline{3-4}
\phantom{a} & \phantom{a} & \multicolumn{1}{c}{\phantom{a}} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  3\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
%& \phantom{a} \\ \cline{1-2}
%\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a}
& \phantom{a} \\ \cline{1-2}
\multicolumn{1}{| c}{\phantom{a}} & \phantom{a} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  1\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
%& \phantom{a} \\ \cline{2-3}
%\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
%\\ \hline\end{tabular}}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a}
& \phantom{a} \\ \cline{2-3}
\phantom{a} & \multicolumn{1}{ c}{\phantom{a}} & \phantom{a} & \phantom{a}
\\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\left\{  2\right\}  ;\\
J\left(  \
%TCIMACRO{\TeXButton{tabular}{\begin{tabular}{ | c | c | c | c | }
%\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
%\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%}}%
%BeginExpansion
\begin{tabular}{ | c | c | c | c | }
\hline\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\
\phantom{a} & \phantom{a} & \phantom{a} & \phantom{a} \\ \hline\end{tabular}%
%EndExpansion
\ \right)   &  =\varnothing.
\end{align*}
It is easy to see that the set $J\left(  P\right)  $ is a subset of $\left\{
1,2,\ldots,n-2\right\}  $ containing no two consecutive integers. Moreover,
this set $J\left(  P\right)  $ uniquely determines $P$, and for every subset
$I$ of $\left\{  1,2,\ldots,n-2\right\}  $ containing no two consecutive
integers, there exists some way $P$ to subdivide the table into dominos such
that $J\left(  P\right)  =I$.

Hence, the number of all ways to subdivide the table into dominos equals the
number of all subsets $I$ of $\left\{  1,2,\ldots,n-2\right\}  $ containing no
two consecutive integers. Exercise \ref{exe.ps2.2.3} says that this latter
number is $f_{n-1}$; therefore, so is the former number.

(I have made this remark because I found it instructive. If you merely want a
proof that the number of all ways to subdivide the table into dominos equals
$f_{n-1}$, then I guess it is easier to just prove it by induction without
taking the detour through Exercise \ref{exe.ps2.2.3}. This proof is sketched
in \cite[\S 7.1]{GKP}, followed by an informal yet insightful discussion of
\textquotedblleft infinite sums of dominos\textquotedblright\ and various
related ideas.)
\end{remark}

Either Exercise \ref{exe.ps2.2.3} or Remark \ref{rmk.fib.dominos} can be used
to prove properties of Fibonacci numbers in a combinatorial way; see
\cite{BenQui-fib} for some examples of such proofs.

Here is another formula for certain recursive sequences, coming out of a
recent paper on cluster algebras\footnote{Specifically, Exercise
\ref{exe.ps2.2.S} is part of \cite[Definition 1]{LS2}, but I have reindexed
the sequence and fixed the missing upper bound in the sum.}:

\Needspace{4cm}

\begin{exercise}
\label{exe.ps2.2.S}Let $r\in\mathbb{Z}$. Define a sequence $\left(
c_{0},c_{1},c_{2},\ldots\right)  $ of integers recursively by $c_{0}=0$,
$c_{1}=1$ and $c_{n}=rc_{n-1}-c_{n-2}$ for all $n\geq2$. Show that%
\begin{equation}
c_{n}=\sum_{i=0}^{n-1}\left(  -1\right)  ^{i}\dbinom{n-1-i}{i}r^{n-1-2i}
\label{eq.exe.2.S}%
\end{equation}
for every $n\in\mathbb{N}$. Here, we use the following convention: Any
expression of the form $a\cdot b$, where $a$ is $0$, has to be interpreted as
$0$, even if $b$ is undefined.\footnotemark
\end{exercise}

\footnotetext{The purpose of this convention is to make sure that the right
hand side of (\ref{eq.exe.2.S}) is well-defined, even though the expression
$r^{n-1-2i}$ that appears in it might be undefined (it will be undefined when
$r=0$ and $n-1-2i<0$).
\par
Of course, the downside of this convention is that we might not have $a\cdot
b=b\cdot a$ (because $a\cdot b$ might be well-defined while $b\cdot a$ is not,
or vice versa).}

\subsection{Additional exercises}

This section contains some further exercises. As the earlier \textquotedblleft
additional exercises\textquotedblright, these will not be relied on in the
rest of this text, and solutions will not be provided.

\begin{addexercise}
\label{exeadd.rec.qn-rn}Let $q$ and $r$ be two complex numbers. Prove that the
sequence $\left(  q^{0}-r^{0},q^{1}-r^{1},q^{2}-r^{2},\ldots\right)  $ is
$\left(  a,b\right)  $-recurrent for two appropriately chosen $a$ and $b$.
Find these $a$ and $b$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.fib.floor}Let $\varphi$ be the golden ratio (i.e., the real
number $\dfrac{1+\sqrt{5}}{2}$). Let $\left(  f_{0},f_{1},f_{2},\ldots\right)
$ be the Fibonacci sequence.

\textbf{(a)} Show that $f_{n+1}-\varphi f_{n}=\dfrac{1}{\sqrt{5}}\psi^{n}$ for
every $n\in\mathbb{N}$, where $\psi=\dfrac{1-\sqrt{5}}{2}$. (Notice that
$\psi=\dfrac{1-\sqrt{5}}{2}\approx-0.618$ lies between $-1$ and $0$, and thus
the powers $\psi^{n}$ converge to $0$ as $n\rightarrow\infty$. So
$f_{n+1}-\varphi f_{n}\rightarrow0$ as $n\rightarrow\infty$, and consequently
$\dfrac{f_{n+1}}{f_{n}}\rightarrow\varphi$ as well.)

\textbf{(b)} Show that%
\[
f_{n}=\operatorname*{round}\left(  \dfrac{1}{\sqrt{5}}\varphi^{n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{N}.
\]
Here, if $x$ is a real number, then $\operatorname*{round}x$ denotes the
integer closest to $x$ (where, in case of a tie, we take the higher of the two candidates\footnotemark).
\end{addexercise}

\footnotetext{This does not really matter in our situation, because $\dfrac
{1}{\sqrt{5}}\varphi^{n}$ will never be a half-integer.}

\begin{addexercise}
\label{exeadd.fib.zeckendorf}Let $\left(  f_{0},f_{1},f_{2},\ldots\right)  $
be the Fibonacci sequence. A set $I$ of integers is said to be
\textit{lacunar} if no two elements of $I$ are consecutive (i.e., there exists
no $i\in I$ such that $i+1\in I$). Show that, for every $n\in\mathbb{N}$,
there exists a unique lacunar subset $S$ of $\left\{  2,3,4,\ldots\right\}  $
such that $n=\sum_{s\in S}f_{s}$.

(For example, if $n=17$, then $S=\left\{  2,4,7\right\}  $, because
$17=1+3+13=f_{2}+f_{4}+f_{7}$.)
\end{addexercise}

\begin{remark}
The representation of $n$ in the form $n=\sum_{s\in S}f_{s}$ in Exercise
\ref{exeadd.fib.zeckendorf} is known as the
\textit{\href{https://en.wikipedia.org/wiki/Zeckendorf's_theorem}{\textit{Zeckendorf
representation}}} of $n$. It has a number of interesting properties and trivia
related to it; for example, there is
\href{https://www.encyclopediaofmath.org/index.php/Zeckendorf_representation}{a
rule of thumb for converting miles into kilometers that uses it}. It can also
be used to define a curious \textquotedblleft Fibonacci
multiplication\textquotedblright\ operation on nonnegative integers
\cite{Knuth-fib}.
\end{remark}

\begin{addexercise}
\label{exeadd.fib.zeckendids}Let $\left(  f_{0},f_{1},f_{2},\ldots\right)  $
be the Fibonacci sequence.

\textbf{(a)} Prove the identities%
\begin{align*}
1f_{n}  &  =f_{n}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq0;\\
2f_{n}  &  =f_{n-2}+f_{n+1}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2;\\
3f_{n}  &  =f_{n-2}+f_{n+2}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2;\\
4f_{n}  &  =f_{n-2}+f_{n}+f_{n+2}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2.
\end{align*}


\textbf{(b)} Notice that the right hand sides of these identities have a
specific form: they are sums of $f_{n+t}$ for $t$ ranging over a lacunar
subset of $\mathbb{Z}$. (See Additional exercise \ref{exeadd.fib.zeckendorf}
for the definition of \textquotedblleft lacunar\textquotedblright.) Try to
find similar identities for $5f_{n}$ and $6f_{n}$.

\textbf{(c)} Prove that such identities exist in general. More precisely,
prove the following: Let $T$ be a finite set, and $a_{t}$ be an integer for
every $t\in T$. Then, there exists a unique lacunar subset $S$ of $\mathbb{Z}$
such that
\begin{align*}
\sum\limits_{t\in T}f_{n+a_{t}}  &  =\sum\limits_{s\in S}f_{n+s}%
\ \ \ \ \ \ \ \ \ \ \text{for every }n\in\mathbb{Z}\text{ which}\\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{ satisfies }n\geq\max\left(
\left\{  -a_{t}\mid t\in T\right\}  \cup\left\{  -s\mid s\in S\right\}
\right)  .
\end{align*}
(The condition $n\geq\max\left(  \left\{  -a_{t}\mid t\in T\right\}
\cup\left\{  -s\mid s\in S\right\}  \right)  $ merely ensures that all the
$f_{n+a_{t}}$ and $f_{n+s}$ are well-defined.)
\end{addexercise}

\begin{remark}
Additional exercise \ref{exeadd.fib.zeckendids} \textbf{(c)} is \cite[Theorem
1]{Gri-zeck}. I'd be delighted to see other proofs!

Similarly I am highly interested in analogues of Additional exercises
\ref{exeadd.fib.zeckendorf} and \ref{exeadd.fib.zeckendids} for other $\left(
a,b\right)  $-recurrent sequences (e.g., Lucas numbers).
\end{remark}

\begin{addexercise}
\label{exeadd.rec.addition}\textbf{(a)} Let $\left(  f_{0},f_{1},f_{2}%
,\ldots\right)  $ be the Fibonacci sequence. Prove that $f_{m+n}=f_{m}%
f_{n+1}+f_{m-1}f_{n}$ for any positive integer $m$ and any $n\in\mathbb{N}$.

\textbf{(b)} Generalize to $\left(  a,b\right)  $-recurrent sequences with
arbitrary $a$ and $b$.

\textbf{(c)} Let $\left(  f_{0},f_{1},f_{2},\ldots\right)  $ be the Fibonacci
sequence. Prove that $f_{m}\mid f_{mk}$ for any $m\in\mathbb{N}$ and
$k\in\mathbb{N}$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.rec.fibonomial}\textbf{(a)} Let $\left(  f_{0},f_{1}%
,f_{2},\ldots\right)  $ be the Fibonacci sequence. For every $n\in\mathbb{N}$
and $k\in\mathbb{N}$ satisfying $0\leq k\leq n$, define a rational number
$\dbinom{n}{k}_{F}$ by%
\[
\dbinom{n}{k}_{F}=\dfrac{f_{n}f_{n-1}\cdots f_{n-k+1}}{f_{k}f_{k-1}\cdots
f_{1}}.
\]
This is called the $\left(  n,k\right)  $-th \textit{Fibonomial coefficient}
(in analogy to the binomial coefficient $\dbinom{n}{k}=\dfrac{n\left(
n-1\right)  \cdots\left(  n-k+1\right)  }{k\left(  k-1\right)  \cdots1}$).

Show that $\dbinom{n}{k}_{F}$ is an integer.

\textbf{(b)} Try to extend as many identities for binomial coefficients as you
can to Fibonomial coefficients.

\textbf{(c)} Generalize to $\left(  a,b\right)  $-recurrent sequences with
arbitrary $a$ and $b$.
\end{addexercise}

\section{\label{chp.perm}Permutations}

This chapter is devoted to permutations. We first recall how they are defined.

\subsection{Permutations and the symmetric group}

\begin{definition}
\label{def.composition}First, let us stipulate, once and for all, how we
define the composition of two maps: If $X$, $Y$ and $Z$ are three sets, and if
$\alpha:X\rightarrow Y$ and $\beta:Y\rightarrow Z$ are two maps, then
$\beta\circ\alpha$ denotes the map from $X$ to $Z$ which sends every $x\in X$
to $\beta\left(  \alpha\left(  x\right)  \right)  $. This map $\beta
\circ\alpha$ is called the \textit{composition} of $\beta$ and $\alpha$ (and
is sometimes abbreviated as $\beta\alpha$). This is the classical notation for
composition of maps, and the reason why I am so explicitly reminding you of it
is that some people (e.g., Herstein in \cite{Herstein}) use a different
convention that conflicts with it: They write maps \textquotedblleft on the
right\textquotedblright\ (i.e., they denote the image of an element $x\in X$
under the map $\alpha:X\rightarrow Y$ by $x^{\alpha}$ or $x\alpha$ instead of
$\alpha\left(  x\right)  $), and they define composition \textquotedblleft the
other way round\textquotedblright\ (i.e., they write $\alpha\circ\beta$ for
what we call $\beta\circ\alpha$). They have reasons for what they are doing,
but I shall use the classical notation because most of the literature agrees
with it.
\end{definition}

\begin{definition}
Let us also recall what it means for two maps to be \textit{inverse}.

Let $X$ and $Y$ be two sets. Two maps $f : X \to Y$ and $g : Y \to X$ are said
to be \textit{mutually inverse} if they satisfy $g \circ f = \operatorname{id}%
_{X}$ and $f \circ g = \operatorname{id}_{Y}$. (In other words, two maps $f :
X \to Y$ and $g : Y \to X$ are mutually inverse if and only if every $x \in X$
satisfies $g\left(  f\left(  x\right)  \right)  = x$ and every $y \in Y$
satisfies $f\left(  g\left(  y\right)  \right)  = y$.)

Let $f : X \to Y$ is a map. If there exists a map $g : Y \to X$ such that $f$
and $g$ are mutually inverse, then this map $g$ is unique (this is easy to
check) and is called the \textit{inverse} of $f$ and denoted by $f^{-1}$. In
this case, the map $f$ is said to be \textit{invertible}. It is easy to see
that if $g$ is the inverse of $f$, then $f$ is the inverse of $g$.

It is well-known that a map $f : X \to Y$ is invertible if and only if $f$ is
bijective (i.e., both injective and surjective). The words ``invertible'' and
``bijective'' are thus synonyms (at least when used for a map between two sets
-- in other situations, they can be rather different). Nevertheless, both of
them are commonly used, often by the same authors (since they convey slightly
different mental images).

A bijective map is also called a \textit{bijection} or a \textit{1-to-1
correspondence} (or a \textit{one-to-one correspondence}). When there is a
bijection from $X$ to $Y$, one says that the elements of $X$ are \textit{in
bijection with} (or \textit{in one-to-one correspondence with}) the elements
of $Y$. It is well-known that two sets $X$ and $Y$ have the same cardinality
if and only if there exists a bijection from $X$ to $Y$.
\end{definition}

\begin{definition}
\label{def.permutation}A \textit{permutation} of a set $X$ means a bijection
from $X$ to $X$. The permutations of a given set $X$ can be composed (i.e., if
$\alpha$ and $\beta$ are two permutations of $X$, then so is $\alpha\circ
\beta$) and have inverses (which, again, are permutations of $X$). More precisely:

\begin{itemize}
\item If $\alpha$ and $\beta$ are two permutations of a given set $X$, then
the composition $\alpha\circ\beta$ is again a permutation of $X$.

\item Any three permutations $\alpha$, $\beta$ and $\gamma$ of $X$ satisfy
$\left(  \alpha\circ\beta\right)  \circ\gamma=\alpha\circ\left(  \beta
\circ\gamma\right)  $. (This holds, more generally, for arbitrary maps which
can be composed.)

\item The identity map $\operatorname*{id}:X\rightarrow X$ (this is the map
which sends every element $x\in X$ to itself) is a permutation of $X$; it is
also called the \textit{identity permutation}. Every permutation $\alpha$ of
$X$ satisfies $\operatorname*{id}\circ\alpha=\alpha$ and $\alpha
\circ\operatorname*{id}=\alpha$. (Again, this can be generalized to arbitrary maps.)

\item For every permutation $\alpha$ of $X$, the inverse map $\alpha^{-1}$ is
well-defined and is again a permutation of $X$. We have $\alpha\circ
\alpha^{-1}=\operatorname*{id}$ and $\alpha^{-1}\circ\alpha=\operatorname*{id}%
$.
\end{itemize}

In the lingo of algebraists, these four properties show that the set of all
permutations of $X$ is a
\href{https://en.wikipedia.org/?title=Group (mathematics)}{group} whose binary
operation is composition, and whose neutral element is the identity
permutation $\operatorname*{id}:X\rightarrow X$. This group is known as the
\textit{symmetric group of the set }$X$. (We will define the notion of a group
later, in Definition \ref{def.group}; thus you might not understand the
preceding two sentences at this point. If you do not care about groups, you
should just remember that the symmetric group of $X$ is the set of all
permutations of $X$.)
\end{definition}

\begin{remark}
Some authors define a permutation of a finite set $X$ to mean a list of all
elements of $X$, each occurring exactly once. This is \textbf{not} the meaning
that the word \textquotedblleft permutation\textquotedblright\ has in these
notes! It is a different notion which, for historical reasons, has been called
\textquotedblleft permutation\textquotedblright\ as well. On
\href{https://en.wikipedia.org/wiki/Permutation\%23Definition_and_one-line_notation}{the
Wikipedia page for \textquotedblleft permutation\textquotedblright}, the two
notions are called \textquotedblleft active\textquotedblright\ and
\textquotedblleft passive\textquotedblright, respectively: An
\textquotedblleft active\textquotedblright\ permutation of $X$ means a
bijection from $X$ to $X$ (that is, a permutation of $X$ in our meaning of
this word), whereas a \textquotedblleft passive\textquotedblright\ permutation
of $X$ means a list of all elements of $X$, each occurring exactly once. For
example, if $X=\left\{  \text{\textquotedblleft cat\textquotedblright,
\textquotedblleft dog\textquotedblright, \textquotedblleft
archaeopteryx\textquotedblright}\right\}  $, then the map%
\begin{align*}
\text{\textquotedblleft cat\textquotedblright\ }  &  \mapsto\text{
\textquotedblleft archaeopteryx\textquotedblright},\\
\text{\textquotedblleft archaeopteryx\textquotedblright\ }  &  \mapsto\text{
\textquotedblleft dog\textquotedblright},\\
\text{\textquotedblleft dog\textquotedblright\ }  &  \mapsto\text{
\textquotedblleft cat\textquotedblright}%
\end{align*}
is an \textquotedblleft active\textquotedblright\ permutation of $X$, whereas
the list $\left(  \text{\textquotedblleft dog\textquotedblright,
\textquotedblleft cat\textquotedblright, \textquotedblleft
archaeopteryx\textquotedblright}\right)  $ is a \textquotedblleft
passive\textquotedblright\ permutation of $X$.

When $X$ is the set $\left\{  1,2,\ldots,n\right\}  $ for some $n\in
\mathbb{N}$, then it is possible to equate each \textquotedblleft
active\textquotedblright\ permutation of $X$ with a \textquotedblleft
passive\textquotedblright\ permutation of $X$ (namely, its one-line notation,
defined below). More generally, this can be done when $X$ comes with a fixed
total order. In general, if $X$ is a finite set, then the number of
\textquotedblleft active\textquotedblright\ permutations of $X$ equals the
number of \textquotedblleft passive\textquotedblright\ permutations of $X$
(and both numbers equal $\left\vert X\right\vert !$), but until you fix some
ordering of the elements of $X$, there is no \textquotedblleft
natural\textquotedblright\ way to match the \textquotedblleft
passive\textquotedblright\ permutations with the \textquotedblleft
active\textquotedblright\ ones. (And when $X$ is infinite, the notion of a
\textquotedblleft passive\textquotedblright\ permutation is not even well-defined.)

To reiterate: For us, the word \textquotedblleft permutation\textquotedblright%
\ shall always mean an \textquotedblleft active\textquotedblright\ permutation!
\end{remark}

Recall that $\mathbb{N}=\left\{  0,1,2,\ldots\right\}  $. Fix $n\in\mathbb{N}$.

Let $S_{n}$ be the symmetric group of the set $\left\{  1,2,\ldots,n\right\}
$. This is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $. It contains the identity permutation $\operatorname*{id}\in
S_{n}$ which sends every $i\in\left\{  1,2,\ldots,n\right\}  $ to $i$. A
well-known fact states that the size of this group is $\left\vert
S_{n}\right\vert =n!$ (that is, there are exactly $n!$ permutations of
$\left\{  1,2,\ldots,n\right\}  $).

We will often write a permutation $\sigma\in S_{n}$ as the list $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $ of its values. This is known as the \textit{one-line
notation} for permutations (because it is a single-rowed list, as opposed to
e.g. the two-line notation which is a two-rowed
table).\footnote{Combinatorialists often omit the parentheses and the commas
(i.e., they just write $\sigma\left(  1\right)  \sigma\left(  2\right)
\cdots\sigma\left(  n\right)  $, hoping that noone will mistake this for a
product), since there is unfortunately another notation for permutations (the
\textit{cycle notation}) which also writes them as lists (actually, lists of
lists) but where the lists have a different meaning.} For instance, the
permutation in $S_{3}$ which sends $1$ to $2$, $2$ to $1$ and $3$ to $3$ is
written $\left(  2,1,3\right)  $ in one-line notation.

The exact relation between lists and permutations is given by the following
simple fact:

\begin{proposition}
\label{prop.perms.lists}Let $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $.

\textbf{(a)} If $\sigma\in S_{n}$, then each element of $\left[  n\right]  $
appears exactly once in the list $\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.

\textbf{(b)} If $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $ is a list of
elements of $\left[  n\right]  $ such that each element of $\left[  n\right]
$ appears exactly once in this list $\left(  p_{1},p_{2},\ldots,p_{n}\right)
$, then there exists a unique permutation $\sigma\in S_{n}$ such that $\left(
p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.

\textbf{(c)} Let $k\in\left\{  0,1,\ldots,n\right\}  $. If $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ is a list of some elements of $\left[
n\right]  $ such that $p_{1},p_{2},\ldots,p_{k}$ are distinct, then there
exists a permutation $\sigma\in S_{n}$ such that $\left(  p_{1},p_{2}%
,\ldots,p_{k}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  k\right)  \right)  $.
\end{proposition}

At this point, let us clarify what we mean by \textquotedblleft
distinct\textquotedblright: Several objects $u_{1},u_{2},\ldots,u_{k}$ are
said to be \textit{distinct} if every $i\in\left\{  1,2,\ldots,k\right\}  $
and $j\in\left\{  1,2,\ldots,k\right\}  $ satisfying $i\neq j$ satisfy
$u_{i}\neq u_{j}$. (Some people call this \textquotedblleft pairwise
distinct\textquotedblright.) So, for example, the numbers $2,1,6$ are
distinct, but the numbers $6,1,6$ are not (although $6$ and $1$ are distinct).
Instead of saying that some objects $u_{1},u_{2},\ldots,u_{k}$ are distinct,
we can also say that \textquotedblleft the list $\left(  u_{1},u_{2}%
,\ldots,u_{k}\right)  $ has no repetitions\textquotedblright\footnote{A
repetition just means an element which occurs more than once in the list. It
does not matter whether the occurrences are at consecutive positions or not.}.

\begin{remark}
The $\sigma$ in Proposition \ref{prop.perms.lists} \textbf{(b)} is uniquely
determined, but the $\sigma$ in Proposition \ref{prop.perms.lists}
\textbf{(c)} is not (in general). More precisely, in Proposition
\ref{prop.perms.lists} \textbf{(c)}, there are $\left(  n-k\right)  !$
possible choices of $\sigma$ that work. (This is easy to check.)
\end{remark}

\begin{proof}
[Proof of Proposition \ref{prop.perms.lists}.]Proposition
\ref{prop.perms.lists} is a really basic fact and its proof is simple. I am
going to present the proof at high detail in order to make sure you correctly
understand every notion involved in it; if you find it obvious, you are
(probably) getting it right and you don't need to read my boring proof.

Recall that $S_{n}$ is the set of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $. In other words, $S_{n}$ is the set of all
permutations of the set $\left[  n\right]  $ (since $\left\{  1,2,\ldots
,n\right\}  =\left[  n\right]  $).

\textbf{(a)} Let $\sigma\in S_{n}$. Let $i\in\left[  n\right]  $.

We have $\sigma\in S_{n}$. In other words, $\sigma$ is a permutation of
$\left[  n\right]  $ (since $S_{n}$ is the set of all permutations of the set
$\left[  n\right]  $). In other words, $\sigma$ is a bijective map $\left[
n\right]  \rightarrow\left[  n\right]  $. Hence, $\sigma$ is both surjective
and injective.

Now, we make the following two observations:

\begin{itemize}
\item The number $i$ appears in the list $\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)
$\ \ \ \ \footnote{\textit{Proof.} The map $\sigma$ is surjective. Hence,
there exists some $j\in\left[  n\right]  $ such that $i=\sigma\left(
j\right)  $. In other words, the number $i$ appears in the list $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $. Qed.}.

\item The number $i$ appears at most once in the list $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)
$\ \ \ \ \footnote{\textit{Proof.} Let us assume the contrary (for the sake of
contradiction). Thus, $i$ appears more than once in the list $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $. In other words, $i$ appears at least twice in this list.
In other words, there exist two distinct elements $p$ and $q$ of $\left[
n\right]  $ such that $\sigma\left(  p\right)  =i$ and $\sigma\left(
q\right)  =i$. Consider these $p$ and $q$.
\par
We have $p\neq q$ (since $p$ and $q$ are distinct), so that $\sigma\left(
p\right)  \neq\sigma\left(  q\right)  $ (since $\sigma$ is injective). This
contradicts $\sigma\left(  p\right)  =i=\sigma\left(  q\right)  $. This
contradiction proves that our assumption was wrong, qed.}.
\end{itemize}

Combining these two observations, we conclude that the number $i$ appears
exactly once in the list $\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.

Let us now forget that we fixed $i$. We thus have shown that if $i\in\left[
n\right]  $, then $i$ appears exactly once in the list $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.
In other words, each element of $\left[  n\right]  $ appears exactly once in
the list $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $. This proves Proposition
\ref{prop.perms.lists} \textbf{(a)}.

\textbf{(b)} Let $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $ be a list of
elements of $\left[  n\right]  $ such that each element of $\left[  n\right]
$ appears exactly once in this list $\left(  p_{1},p_{2},\ldots,p_{n}\right)
$.

We have $p_{i}\in\left[  n\right]  $ for every $i\in\left[  n\right]  $ (since
$\left(  p_{1},p_{2},\ldots,p_{n}\right)  $ is a list of elements of $\left[
n\right]  $).

We define a map $\tau:\left[  n\right]  \rightarrow\left[  n\right]  $ by
setting%
\begin{equation}
\left(  \tau\left(  i\right)  =p_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left[  n\right]  \right)  . \label{pf.prop.perms.lists.b.1}%
\end{equation}
(This is well-defined, because we have $p_{i}\in\left[  n\right]  $ for every
$i\in\left[  n\right]  $.) The map $\tau$ is
injective\footnote{\textit{Proof.} Let $u$ and $v$ be two elements of $\left[
n\right]  $ such that $\tau\left(  u\right)  =\tau\left(  v\right)  $. We
shall show that $u=v$.
\par
Indeed, we assume the contrary (for the sake of contradiction). Thus, $u\neq
v$.
\par
The definition of $\tau\left(  u\right)  $ shows that $\tau\left(  u\right)
=p_{u}$. But we also have $\tau\left(  u\right)  =\tau\left(  v\right)
=p_{v}$ (by the definition of $\tau\left(  v\right)  $). Now, the element
$\tau\left(  u\right)  $ of $\left[  n\right]  $ appears (at least) twice in
the list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $: once at the $u$-th
position (since $\tau\left(  u\right)  =p_{u}$), and again at the $v$-th
position (since $\tau\left(  u\right)  =p_{v}$). (And these are two distinct
positions, because $u\neq v$.)
\par
But let us recall that each element of $\left[  n\right]  $ appears exactly
once in this list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. Hence, no
element of $\left[  n\right]  $ appears more than once in the list $\left(
p_{1},p_{2},\ldots,p_{n}\right)  $. In particular, $\tau\left(  u\right)  $
cannot appear more than once in this list $\left(  p_{1},p_{2},\ldots
,p_{n}\right)  $. This contradicts the fact that $\tau\left(  u\right)  $
appears twice in the list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $.
\par
This contradiction shows that our assumption was wrong. Hence, $u=v$ is
proven.
\par
Now, let us forget that we fixed $u$ and $v$. We thus have proven that if $u$
and $v$ are two elements of $\left[  n\right]  $ such that $\tau\left(
u\right)  =\tau\left(  v\right)  $, then $u=v$. In other words, the map $\tau$
is injective. Qed.} and surjective\footnote{\textit{Proof.} Let $u\in\left[
n\right]  $. Each element of $\left[  n\right]  $ appears exactly once in the
list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. Applying this to the element
$u$ of $\left[  n\right]  $, we conclude that $u$ appears exactly once in the
list $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. In other words, there exists
exactly one $i\in\left[  n\right]  $ such that $u=p_{i}$. Consider this $i$.
The definition of $\tau$ yields $\tau\left(  i\right)  =p_{i}$. Compared with
$u=p_{i}$, this yields $\tau\left(  i\right)  =u$.
\par
Hence, there exists a $j\in\left[  n\right]  $ such that $\tau\left(
j\right)  =u$ (namely, $j=i$).
\par
Let us now forget that we fixed $u$. We thus have proven that for every
$u\in\left[  n\right]  $, there exists a $j\in\left[  n\right]  $ such that
$\tau\left(  j\right)  =u$. In other words, the map $\tau$ is surjective.
Qed.}. Hence, the map $\tau$ is bijective. In other words, $\tau$ is a
permutation of $\left[  n\right]  $ (since $\tau$ is a map $\left[  n\right]
\rightarrow\left[  n\right]  $). In other words, $\tau\in S_{n}$ (since
$S_{n}$ is the set of all permutations of the set $\left[  n\right]  $).
Clearly, $\left(  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots
,\tau\left(  n\right)  \right)  =\left(  p_{1},p_{2},\ldots,p_{n}\right)  $
(because of (\ref{pf.prop.perms.lists.b.1})), so that $\left(  p_{1}%
,p_{2},\ldots,p_{n}\right)  =\left(  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  n\right)  \right)  $.

Hence, there exists a permutation $\sigma\in S_{n}$ such that \newline$\left(
p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $ (namely,
$\sigma=\tau$). Moreover, there exists \textbf{at most one} such
permutation\footnote{\textit{Proof.} Let $\sigma_{1}$ and $\sigma_{2}$ be two
permutations $\sigma\in S_{n}$ such that $\left(  p_{1},p_{2},\ldots
,p_{n}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $. Thus, $\sigma_{1}$ is a
permutation in $S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)
=\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  $, and $\sigma_{2}$ is a
permutation in $S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)
=\left(  \sigma_{2}\left(  1\right)  ,\sigma_{2}\left(  2\right)
,\ldots,\sigma_{2}\left(  n\right)  \right)  $.
\par
We have $\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  =\left(  p_{1},p_{2},\ldots
,p_{n}\right)  =\left(  \sigma_{2}\left(  1\right)  ,\sigma_{2}\left(
2\right)  ,\ldots,\sigma_{2}\left(  n\right)  \right)  $. In other words,
every $i\in\left[  n\right]  $ satisfies $\sigma_{1}\left(  i\right)
=\sigma_{2}\left(  i\right)  $. In other words, $\sigma_{1}=\sigma_{2}$.
\par
Let us now forget that we fixed $\sigma_{1}$ and $\sigma_{2}$. We thus have
shown that if $\sigma_{1}$ and $\sigma_{2}$ are two permutations $\sigma\in
S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)  =\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $, then $\sigma_{1}=\sigma_{2}$. In other words, any two
permutations $\sigma\in S_{n}$ such that $\left(  p_{1},p_{2},\ldots
,p_{n}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $ must be equal to each other. In
other words, there exists \textbf{at most one} permutation $\sigma\in S_{n}$
such that $\left(  p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $.
Qed.}. Combining the claims of the previous two sentences, we conclude that
there exists a unique permutation $\sigma\in S_{n}$ such that $\left(
p_{1},p_{2},\ldots,p_{n}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $. This
proves Proposition \ref{prop.perms.lists} \textbf{(b)}.

\textbf{(c)} Let $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ be a list of some
elements of $\left[  n\right]  $ such that $p_{1},p_{2},\ldots,p_{k}$ are
distinct. Thus, the list $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ contains
$k$ of the $n$ elements of $\left[  n\right]  $ (because $p_{1},p_{2}%
,\ldots,p_{k}$ are distinct). Let $q_{1},q_{2},\ldots,q_{n-k}$ be the
remaining $n-k$ elements of $\left[  n\right]  $ (listed in any arbitrary
order, with no repetition). Then, $\left(  p_{1},p_{2},\ldots,p_{k}%
,q_{1},q_{2},\ldots,q_{n-k}\right)  $ is a list of all $n$ elements of
$\left[  n\right]  $, with no repetitions\footnote{It has no repetitions
because:
\par
\begin{itemize}
\item there are no repetitions among $p_{1},p_{2},\ldots,p_{k}$;
\par
\item there are no repetitions among $q_{1},q_{2},\ldots,q_{n-k}$;
\par
\item the two lists $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ and $\left(
q_{1},q_{2},\ldots,q_{n-k}\right)  $ have no elements in common (because we
defined $q_{1},q_{2},\ldots,q_{n-k}$ to be the \textquotedblleft
remaining\textquotedblright\ $n-k$ elements of $\left[  n\right]  $, where
\textquotedblleft remaining\textquotedblright\ means \textquotedblleft not
contained in the list $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $%
\textquotedblright).
\end{itemize}
}. In other words, each element of $\left[  n\right]  $ appears exactly once
in this list $\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  $ (and each entry in this list is an element of $\left[
n\right]  $). Hence, we can apply Proposition \ref{prop.perms.lists}
\textbf{(b)} to $\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  $ instead of $\left(  p_{1},p_{2},\ldots,p_{n}\right)  $. As
a consequence, we conclude that there exists a unique permutation $\sigma\in
S_{n}$ such that $\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $. Let $\tau$ be this $\sigma$.

Thus, $\tau\in S_{n}$ is a permutation such that
\[
\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots,q_{n-k}\right)  =\left(
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  n\right)
\right)  .
\]
Now,%
\begin{align*}
&  \left(  p_{1},p_{2},\ldots,p_{k}\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  p_{1},p_{2},\ldots,p_{k},q_{1},q_{2},\ldots
,q_{n-k}\right)  }_{=\left(  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  n\right)  \right)  }\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  n\right)
\right)  \right) \\
&  =\left(  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(
k\right)  \right)  .
\end{align*}
Hence, there exists a permutation $\sigma\in S_{n}$ such that \newline$\left(
p_{1},p_{2},\ldots,p_{k}\right)  =\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  k\right)  \right)  $ (namely,
$\sigma=\tau$). This proves Proposition \ref{prop.perms.lists} \textbf{(c)}.
\end{proof}

\subsection{Inversions, lengths and the permutations $s_{i} \in S_{n}$}

For each $i\in\left\{  1,2,\ldots,n-1\right\}  $, let $s_{i}$ be the
permutation in $S_{n}$ that switches $i$ with $i+1$ but leaves all other
numbers unchanged. Formally speaking, $s_{i}$ is the permutation in $S_{n}$
given by%
\[
\left(  s_{i}\left(  k\right)  =%
\begin{cases}
i+1, & \text{if }k=i;\\
i, & \text{if }k=i+1;\\
k, & \text{if }k\notin\left\{  i,i+1\right\}
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for all }k\in\left\{  1,2,\ldots,n\right\}  \right)
.
\]
Thus, in one-line notation%
\[
s_{i}=\left(  1,2,\ldots,i-1,i+1,i,i+2,\ldots,n\right)  .
\]
Notice that $s_{i}^{2}=\operatorname*{id}$ for every $i\in\left\{
1,2,\ldots,n-1\right\}  $. (Here, we are using the notation $\alpha^{2}$ for
$\alpha\circ\alpha$, where $\alpha$ is a permutation in $S_{n}$.)

\begin{exercise}
\label{exe.ps2.2.4}\textbf{(a)} Show that $s_{i}\circ s_{i+1}\circ
s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}$ for all $i\in\left\{  1,2,\ldots
,n-2\right\}  $.

\textbf{(b)} Show that every permutation $\sigma\in S_{n}$ can be written as a
composition of several permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). For example, if $n=3$, then the permutation
$\left(  3,1,2\right)  $ in $S_{3}$ can be written as the composition
$s_{2}\circ s_{1}$, while the permutation $\left(  3,2,1\right)  $ in $S_{3}$
can be written as the composition $s_{1}\circ s_{2}\circ s_{1}$ or also as the
composition $s_{2}\circ s_{1}\circ s_{2}$.

[\textbf{Hint:} If you do not immediately see why this works, consider reading further.]

\textbf{(c)} Let $w_{0}$ denote the permutation in $S_{n}$ which sends each
$k\in\left\{  1,2,\ldots,n\right\}  $ to $n+1-k$. (In one-line notation, this
$w_{0}$ is written as $\left(  n,n-1,\ldots,1\right)  $.) Find an
\textbf{explicit} way to write $w_{0}$ as a composition of several
permutations of the form $s_{i}$ (with $i\in\left\{  1,2,\ldots,n-1\right\}  $).
\end{exercise}

\begin{remark}
Symmetric groups appear in almost all parts of mathematics; unsurprisingly,
there is no universally accepted notation for them. We are using the notation
$S_{n}$ for the $n$-th symmetric group; other common notations for it are
$\mathfrak{S}_{n}$, $\Sigma_{n}$ and $\operatorname*{Sym}\left(  n\right)  $.
The permutations that we call $s_{1},s_{2},\ldots,s_{n-1}$ are often called
$\sigma_{1},\sigma_{2},\ldots,\sigma_{n-1}$. As already mentioned in
Definition \ref{def.composition}, some people write the composition of maps
\textquotedblleft backwards\textquotedblright, which causes their $\sigma
\circ\tau$ to be our $\tau\circ\sigma$, etc.. (Sadly, most authors are so sure
that their notation is standard that they never bother to define it.)

In the language of group theory, the statement of Exercise \ref{exe.ps2.2.4}
\textbf{(b)} says (or, more precisely, yields) that the permutations
$s_{1},s_{2},\ldots,s_{n-1}$ generate the group $S_{n}$.
\end{remark}

\begin{definition}
If $\sigma\in S_{n}$ is a permutation, then an \textit{inversion} of $\sigma$
means a pair $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$
and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. For instance, the
inversions of the permutation $\left(  3,1,2\right)  $ in $S_{3}$ are $\left(
1,2\right)  $ and $\left(  1,3\right)  $ (because $3>1$ and $3>2$), while the
only inversion of the permutation $\left(  1,3,2\right)  $ in $S_{3}$ is
$\left(  2,3\right)  $ (since $3>2$).

If $\sigma\in S_{n}$ is a permutation, then the \textit{length} of $\sigma$
means the number of inversions of $\sigma$. This length is denoted by
$\ell\left(  \sigma\right)  $; it is a nonnegative integer.
\end{definition}

Any $\sigma\in S_{n}$ satisfies $0\leq\ell\left(  \sigma\right)  \leq
\dbinom{n}{2}$ (since the number of inversions of $\sigma$ is clearly no
larger than the total number of pairs $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$; but the latter number is $\dbinom{n}{2}$). The
only permutation in $S_{n}$ having length $0$ is the identity permutation
$\operatorname*{id}=\left(  1,2,\ldots,n\right)  \in S_{n}$%
\ \ \ \ \footnote{The fact that the identity permutation $\operatorname*{id}%
\in S_{n}$ has length $\ell\left(  \operatorname*{id}\right)  =0$ is trivial.
The fact that it is the only one such permutation is easy (it essentially
follows from Exercise \ref{exe.ps2.2.5} \textbf{(d)}).}.

\begin{exercise}
\label{exe.ps2.2.5}\textbf{(a)} Show that every permutation $\sigma\in S_{n}$
and every $k\in\left\{  1,2,\ldots,n-1\right\}  $ satisfy%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  \label{eq.exe.2.5.a.1}%
\end{equation}
and%
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  <\sigma^{-1}\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  >\sigma^{-1}\left(  k+1\right)
\end{array}
\right.  . \label{eq.exe.2.5.a.2}%
\end{equation}


\textbf{(b)} Show that any two permutations $\sigma$ and $\tau$ in $S_{n}$
satisfy $\ell\left(  \sigma\circ\tau\right)  \equiv\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \operatorname{mod}2$.

\textbf{(c)} Show that any two permutations $\sigma$ and $\tau$ in $S_{n}$
satisfy $\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  $.

\textbf{(d)} If $\sigma\in S_{n}$ is a permutation satisfying $\sigma\left(
1\right)  \leq\sigma\left(  2\right)  \leq\cdots\leq\sigma\left(  n\right)  $,
then show that $\sigma=\operatorname*{id}$.

\textbf{(e)} Let $\sigma\in S_{n}$. Show that $\sigma$ can be written as a
composition of $\ell\left(  \sigma\right)  $ permutations of the form $s_{k}$
(with $k\in\left\{  1,2,\ldots,n-1\right\}  $).

\textbf{(f)} Let $\sigma\in S_{n}$. Then, show that $\ell\left(
\sigma\right)  =\ell\left(  \sigma^{-1}\right)  $.

\textbf{(g)} Let $\sigma\in S_{n}$. Show that $\ell\left(  \sigma\right)  $ is
the smallest $N\in\mathbb{N}$ such that $\sigma$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $).
\end{exercise}

\begin{example}
\label{exa.2.5}Let us justify Exercise \ref{exe.ps2.2.5} \textbf{(a)} on an
example. The solution to Exercise \ref{exe.ps2.2.5} \textbf{(a)} given below
is essentially a (tiresome) formalization of the ideas seen in this example.

Let $n=5$, $k=3$ and $\sigma=\left(  4,2,1,5,3\right)  $ (written in one-line
notation). Then, $\sigma\circ s_{k}=\left(  4,2,5,1,3\right)  $; this is the
permutation obtained by switching the $k$-th and the $\left(  k+1\right)  $-th
entry of $\sigma$ (where the word \textquotedblleft entry\textquotedblright%
\ refers to the one-line notation). On the other hand, $s_{k}\circ
\sigma=\left(  3,2,1,5,4\right)  $; this is the permutation obtained by
switching the entry $k$ with the entry $k+1$ of $\sigma$. Mind the difference
between these two operations.

The inversions of $\sigma=\left(  4,2,1,5,3\right)  $ are $\left(  1,2\right)
$, $\left(  1,3\right)  $, $\left(  1,5\right)  $, $\left(  2,3\right)  $ and
$\left(  4,5\right)  $. These are the pairs $\left(  i,j\right)  $ of
positions such that $i$ is before $j$ (that is, $i<j$) but the $i$-th entry of
$\sigma$ is larger than the $j$-th entry of $\sigma$ (that is, $\sigma\left(
i\right)  >\sigma\left(  j\right)  $). In other words, these are the pairs of
positions at which the entries of $\sigma$ are out of order. On the other
hand, the inversions of $s_{k}\circ\sigma=\left(  3,2,1,5,4\right)  $ are
$\left(  1,2\right)  $, $\left(  1,3\right)  $, $\left(  2,3\right)  $ and
$\left(  4,5\right)  $. These are precisely the inversions of $\sigma$ except
for $\left(  1,5\right)  $. This is no surprise: In fact, $s_{k}\circ\sigma$
is obtained from $\sigma$ by switching the entry $k$ with the entry $k+1$, and
this operation clearly preserves all inversions other than the one that is
directly being turned around (i.e., the inversion $\left(  i,j\right)  $ where
$\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}  =\left\{
k,k+1\right\}  $; in our case, this is the inversion $\left(  1,5\right)  $).
In general, when $\sigma^{-1}\left(  k\right)  >\sigma^{-1}\left(  k+1\right)
$ (that is, when $k$ appears further left than $k+1$ in the one-line notation
of $\sigma$), the inversions of $s_{k}\circ\sigma$ are the inversions of
$\sigma$ except for $\left(  \sigma^{-1}\left(  k+1\right)  ,\sigma
^{-1}\left(  k\right)  \right)  $. Therefore, in this case, the number of
inversions of $s_{k}\circ\sigma$ equals the number of inversions of $\sigma$
plus $1$. That is, in this case, $\ell\left(  s_{k}\circ\sigma\right)
=\ell\left(  \sigma\right)  +1$. When $\sigma^{-1}\left(  k\right)
<\sigma^{-1}\left(  k+1\right)  $, a similar argument shows $\ell\left(
s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)  -1$. This explains why
(\ref{eq.exe.2.5.a.2}) holds (although formalizing this argument will be tedious).

The inversions of $\sigma\circ s_{k}=\left(  4,2,5,1,3\right)  $ are $\left(
1,2\right)  $, $\left(  1,4\right)  $, $\left(  1,5\right)  $, $\left(
2,4\right)  $, $\left(  3,4\right)  $ and $\left(  3,5\right)  $. Unlike the
inversions of $s_{k}\circ\sigma$, these are not directly related to the
inversions of $\sigma$, so the argument in the previous paragraph does not
prove (\ref{eq.exe.2.5.a.1}). However, instead of considering inversions of
$\sigma$, one can consider inversions of $\sigma^{-1}$. These are even more
intuitive: They are the pairs of integers $\left(  i,j\right)  $ with $1\leq
i<j\leq n$ such that $i$ appears further right than $j$ in the one-line
notation of $\sigma$. For instance, the inversions of $\sigma^{-1}$ are
$\left(  1,2\right)  $, $\left(  1,4\right)  $, $\left(  2,4\right)  $,
$\left(  3,4\right)  $ and $\left(  3,5\right)  $, whereas the inversions of
$\left(  \sigma\circ s_{k}\right)  ^{-1}$ are all of these and also $\left(
1,5\right)  $. But there is no need to repeat our proof of
(\ref{eq.exe.2.5.a.2}); it is easier to deduce (\ref{eq.exe.2.5.a.1}) from
(\ref{eq.exe.2.5.a.2}) by applying (\ref{eq.exe.2.5.a.2}) to $\sigma^{-1}$
instead of $\sigma$ and appealing to Exercise \ref{exe.ps2.2.5} \textbf{(f)}.
(Again, see the solution below for the details.)
\end{example}

Notice that Exercise \ref{exe.ps2.2.5} \textbf{(e)} immediately yields
Exercise \ref{exe.ps2.2.4} \textbf{(b)}.

\begin{remark}
When $n=0$ or $n=1$, we have $\left\{  1,2,\ldots,n-1\right\}  =\varnothing$.
Hence, Exercise \ref{exe.ps2.2.4} \textbf{(e)} looks strange in the case when
$n=0$ or $n=1$, because in this case, there are no permutations of the form
$s_{k}$ to begin with. Nevertheless, it is correct. Indeed, when $n=0$ or
$n=1$, there is only one permutation $\sigma\in S_{n}$, namely the identity
permutation $\operatorname*{id}$, and it has length $\ell\left(
\sigma\right)  =\ell\left(  \operatorname*{id}\right)  =0$. Thus, in this
case, Exercise \ref{exe.ps2.2.4} \textbf{(e)} claims that $\operatorname*{id}$
can be written as a composition of $0$ permutations of the form $s_{k}$ (with
$k\in\left\{  1,2,\ldots,n-1\right\}  $). This is true: Even from an empty set
we can always pick $0$ elements; and the composition of $0$ permutations will
be $\operatorname*{id}$.
\end{remark}

\begin{remark}
The word \textquotedblleft length\textquotedblright\ for $\ell\left(
\sigma\right)  $ can be confusing: It does not refer to the length of the
$n$-tuple $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  n\right)  \right)  $ (which is $n$). The reason why it
is called \textquotedblleft length\textquotedblright\ is Exercise
\ref{exe.ps2.2.5} \textbf{(g)}: it says that $\ell\left(  \sigma\right)  $ is
the smallest number of permutations of the form $s_{k}$ which can be
multiplied to give $\sigma$; thus, it is the smallest possible length of an
expression of $\sigma$ as a product of $s_{k}$'s.

The use of the word \textquotedblleft length\textquotedblright, unfortunately,
is not standard across literature. Some authors call \textquotedblleft Coxeter
length\textquotedblright\ what we call \textquotedblleft
length\textquotedblright, and use the word \textquotedblleft
length\textquotedblright\ itself for a different notion.
\end{remark}

\begin{exercise}
\label{exe.ps2.2.6}Let $\sigma\in S_{n}$. In Exercise \ref{exe.ps2.2.4}
\textbf{(b)}, we have seen that $\sigma$ can be written as a composition of
several permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots
,n-1\right\}  $). Usually there will be several ways to do so (for instance,
$\operatorname*{id}=s_{1}\circ s_{1}=s_{2}\circ s_{2}=\cdots=s_{n-1}\circ
s_{n-1}$). Show that, whichever of these ways we take, the number of
permutations composed will be congruent to $\ell\left(  \sigma\right)  $
modulo $2$.
\end{exercise}

\subsection{\label{sect.sign}The sign of a permutation}

\begin{definition}
\label{def.perm.sign}We define the \textit{sign} of a permutation $\sigma\in
S_{n}$ as the integer $\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$. We
denote this sign by $\left(  -1\right)  ^{\sigma}$ or $\operatorname*{sign}%
\sigma$ or $\operatorname*{sgn}\sigma$. We say that a permutation $\sigma$ is
\textit{even} if its sign is $1$ (that is, if $\ell\left(  \sigma\right)  $ is
even), and \textit{odd} if its sign is $-1$ (that is, if $\ell\left(
\sigma\right)  $ is odd).
\end{definition}

Signs of permutations have the following properties:

\begin{itemize}
\item The sign of the identity permutation $\operatorname*{id}\in S_{n}$ is
$\left(  -1\right)  ^{\operatorname*{id}}=1$ (because the definition of
$\left(  -1\right)  ^{\operatorname*{id}}$ yields $\left(  -1\right)
^{\operatorname*{id}}=\left(  -1\right)  ^{\ell\left(  \operatorname*{id}%
\right)  }=1$ (since $\ell\left(  \operatorname*{id}\right)  =0$)). In other
words, $\operatorname*{id}\in S_{n}$ is even.

\item For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the sign of the
permutation $s_{k}\in S_{n}$ is $\left(  -1\right)  ^{s_{k}}=-1$%
\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Applying (\ref{eq.exe.2.5.a.1}) to $\sigma=\operatorname*{id}$, we obtain
\begin{align*}
\ell\left(  \operatorname*{id}\circ s_{k}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \operatorname*{id}\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if
}\operatorname*{id}\left(  k\right)  <\operatorname*{id}\left(  k+1\right)
;\\
\ell\left(  \operatorname*{id}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if
}\operatorname*{id}\left(  k\right)  >\operatorname*{id}\left(  k+1\right)
\end{array}
\right. \\
&  =\underbrace{\ell\left(  \operatorname*{id}\right)  }_{=0}%
+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{id}\left(  k\right)
=k<k+1=\operatorname*{id}\left(  k+1\right)  \right) \\
&  =1.
\end{align*}
This rewrites as $\ell\left(  s_{k}\right)  =1$ (since $\operatorname*{id}%
\circ s_{k}=s_{k}$). Now, the definition of $\left(  -1\right)  ^{s_{k}}$
yields $\left(  -1\right)  ^{s_{k}}=\left(  -1\right)  ^{\ell\left(
s_{k}\right)  }=-1$ (since $\ell\left(  s_{k}\right)  =1$), qed.}.

\item If $\sigma$ and $\tau$ are two permutations in $S_{n}$, then $\left(
-1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(
-1\right)  ^{\tau}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$ and
$\tau\in S_{n}$. Exercise \ref{exe.ps2.2.5} \textbf{(b)} yields $\ell\left(
\sigma\circ\tau\right)  \equiv\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  \operatorname{mod}2$, so that $\left(  -1\right)  ^{\ell\left(
\sigma\circ\tau\right)  }=\left(  -1\right)  ^{\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  }=\left(  -1\right)  ^{\ell\left(  \sigma\right)
}\cdot\left(  -1\right)  ^{\ell\left(  \tau\right)  }$. But the definition of
the sign of a permutation yields $\left(  -1\right)  ^{\sigma\circ\tau
}=\left(  -1\right)  ^{\ell\left(  \sigma\circ\tau\right)  }$, $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$ and
$\left(  -1\right)  ^{\tau}=\left(  -1\right)  ^{\ell\left(  \tau\right)  }$.
Hence, $\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\ell\left(
\sigma\circ\tau\right)  }=\underbrace{\left(  -1\right)  ^{\ell\left(
\sigma\right)  }}_{=\left(  -1\right)  ^{\sigma}}\cdot\underbrace{\left(
-1\right)  ^{\ell\left(  \tau\right)  }}_{=\left(  -1\right)  ^{\tau}}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$, qed.}.

\item If $\sigma\in S_{n}$, then $\left(  -1\right)  ^{\sigma^{-1}}=\left(
-1\right)  ^{\sigma}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$.
The definition of $\left(  -1\right)  ^{\sigma^{-1}}$ yields $\left(
-1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\ell\left(  \sigma^{-1}\right)
}$. But recall that $\ell\left(  \sigma\right)  =\ell\left(  \sigma
^{-1}\right)  $. The definition of $\left(  -1\right)  ^{\sigma}$ yields
$\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)
}=\left(  -1\right)  ^{\ell\left(  \sigma^{-1}\right)  }$ (since $\ell\left(
\sigma\right)  =\ell\left(  \sigma^{-1}\right)  $). Compared with $\left(
-1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\ell\left(  \sigma^{-1}\right)
}$, this yields $\left(  -1\right)  ^{\sigma^{-1}}=\left(  -1\right)
^{\sigma}$, qed.}.
\end{itemize}

The first and the third of these properties are often summarized as the
statement that \textquotedblleft sign is a group homomorphism from the group
$S_{n}$ to the multiplicative group $\left\{  1,-1\right\}  $%
\textquotedblright. In this statement, \textquotedblleft
sign\textquotedblright\ means the map from $S_{n}$ to $\left\{  1,-1\right\}
$ which sends every permutation $\sigma$ to its sign $\left(  -1\right)
^{\ell\left(  \sigma\right)  }$, and the \textquotedblleft multiplicative
group $\left\{  1,-1\right\}  $\textquotedblright\ means the group $\left\{
1,-1\right\}  $ whose binary operation is multiplication.

We have defined the sign of a permutation $\sigma\in S_{n}$. More generally,
it is possible to define the sign of a permutation of an arbitrary finite set
$X$, even though the length of such a permutation is not defined!\footnote{How
does it work? If $X$ is a finite set, then we can always find a bijection
$\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$.
(Constructing such a bijection is tantamount to writing down a list of all
elements of $X$, with no duplicates.) Given such a bijection $\phi$, we can
define the sign of any permutation $\sigma$ of $X$ as follows:%
\begin{equation}
\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\phi\circ\sigma\circ
\phi^{-1}}. \label{eq.ps2.S(X).sign.teaser}%
\end{equation}
Here, the right hand side is well-defined because $\phi\circ\sigma\circ
\phi^{-1}$ is a permutation of $\left\{  1,2,\ldots,n\right\}  $. What is not
immediately obvious is that this sign is independent on the choice of $\phi$,
and that it is a group homomorphism to $\left\{  1,-1\right\}  $ (that is, we
have $\left(  -1\right)  ^{\operatorname*{id}}=1$ and $\left(  -1\right)
^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau
}$). We will prove these facts further below (in Exercise \ref{exe.ps4.2}).}

\begin{exercise}
\label{exe.ps2.2.7}Let $n\geq2$. Show that the number of even permutations in
$S_{n}$ is $n!/2$, and the number of odd permutations in $S_{n}$ is also
$n!/2$.
\end{exercise}

The sign of a permutation is used in the combinatorial definition of the
determinant. Let us briefly show this definition now; we shall return to it
later (in Chapter \ref{chp.det}) to study it in much more detail.

\begin{definition}
\label{def.det.old}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix (say, with complex
entries, although this does not matter much -- it suffices that the entries
can be added and multiplied and the axioms of associativity, distributivity,
commutativity, unity etc. hold). The \textit{determinant} $\det A$ of $A$ is
defined as%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)
}a_{2,\sigma\left(  2\right)  }\cdots a_{n,\sigma\left(  n\right)  }.
\label{eq.det.old}%
\end{equation}

\end{definition}

Let me try to describe the sum (\ref{eq.det.old}) in slightly more visual
terms: The sum (\ref{eq.det.old}) has $n!$ addends, each of which has the form
\textquotedblleft$\left(  -1\right)  ^{\sigma}$ times a
product\textquotedblright. The product has $n$ factors, which are entries of
$A$, and are chosen in such a way that there is exactly one entry taken from
each row and exactly one from each column. Which precise entries are taken
depends on $\sigma$: namely, for each $i$, we take the $\sigma\left(
i\right)  $-th entry from the $i$-th row.

Convince yourself that the classical formulas%
\begin{align*}
\det\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)   &  =a;\\
\det\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)   &  =ad-bc;\\
\det\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)   &  =aei+bfg+cdh-ahf-bdi-ceg
\end{align*}
are particular cases of (\ref{eq.det.old}). Whenever $n\geq2$, the sum in
(\ref{eq.det.old}) contains precisely $n!/2$ plus signs and $n!/2$ minus signs
(because of Exercise \ref{exe.ps2.2.7}).

Definition \ref{def.det.old} is merely one of several equivalent definitions
of the determinant. You will probably see two of them in an average linear
algebra class. Each of them has its own advantages and drawbacks. Definition
\ref{def.det.old} is the most direct, assuming that one knows about the sign
of a permutation.

\subsection{\label{sect.infperm}Infinite permutations}

(This section is optional; it explores some technical material which is useful
in combinatorics, but is not necessary for what follows. I advise the reader
to skip it at the first read.)

We have introduced the notion of a permutation of an arbitrary set; but so
far, we have only studied permutations of finite sets. In this section (which
is tangential to our project; probably nothing from this section will be used
ever after), let me discuss permutations of the infinite set $\left\{
1,2,3,\ldots\right\}  $. (A lot of what I say below can be easily adapted to
the sets $\mathbb{N}$ and $\mathbb{Z}$ as well.)

We recall that a permutation of a set $X$ means a bijection from $X$ to $X$.

Let $S_{\infty}$ be the symmetric group of the set $\left\{  1,2,3,\ldots
\right\}  $. This is the set of all permutations of $\left\{  1,2,3,\ldots
\right\}  $. It contains the identity permutation $\operatorname*{id}\in
S_{\infty}$ which sends every $i\in\left\{  1,2,3,\ldots\right\}  $ to $i$.
The set $S_{\infty}$ is uncountable\footnote{More generally, while a finite
set of size $n$ has $n!$ permutations, an infinite set $S$ has uncountably
many permutations (even if $S$ is countable).}.

We shall try to study $S_{\infty}$ similarly to how we studied $S_{n}$ for
$n\in\mathbb{N}$. However, we soon will notice that the analogy between
$S_{\infty}$ and $S_{n}$ will break down.\footnote{The uncountability of
$S_{\infty}$ is the first hint that $S_{\infty}$ is \textquotedblleft too
large\textquotedblright\ a set to be a good analogue of the finite set $S_{n}%
$.} To amend this, we shall define a subset $S_{\left(  \infty\right)  }$ of
$S_{\infty}$ (mind the parentheses around the \textquotedblleft$\infty
$\textquotedblright) which is smaller and more wieldy, and indeed shares many
of the properties of the finite symmetric group $S_{n}$.

We define $S_{\left(  \infty\right)  }$ as follows:%
\begin{equation}
S_{\left(  \infty\right)  }=\left\{  \sigma\in S_{\infty}\ \mid\ \sigma\left(
i\right)  =i\text{ for all but finitely many }i\in\left\{  1,2,3,\ldots
\right\}  \right\}  . \label{eq.S(infty).def}%
\end{equation}
Let us first explain what \textquotedblleft all but finitely many
$i\in\left\{  1,2,3,\ldots\right\}  $\textquotedblright\ means:

\begin{definition}
\label{def.allbutfin}Let $I$ be a set. Let $\mathcal{A}\left(  i\right)  $ be
a statement for every $i\in I$. Then, we say that \textquotedblleft%
$\mathcal{A}\left(  i\right)  $ for all but finitely many $i\in I$%
\textquotedblright\ if and only if there exists some finite subset $J$ of $I$
such that every $i\in I\setminus J$ satisfies $\mathcal{A}\left(  i\right)  $.\ \ \ \ \footnotemark
\end{definition}

\footnotetext{Thus, the statement \textquotedblleft$\mathcal{A}\left(
i\right)  $ for all but finitely many $i\in I$\textquotedblright\ can be
restated as \textquotedblleft$\mathcal{A}\left(  i\right)  $ holds for all
$i\in I$, apart from finitely many exceptions\textquotedblright\ or as
\textquotedblleft there are only finitely many $i\in I$ which do not satisfy
$\mathcal{A}\left(  i\right)  $\textquotedblright. I prefer the first wording,
because it makes the most sense in constructive logic.
\par
\textbf{Caution:} Do not confuse the words \textquotedblleft all but finitely
many $i\in I$\textquotedblright\ in this definition with the words
\textquotedblleft infinitely many $i\in I$\textquotedblright. For instance, it
is true that $n$ is even for infinitely many $n\in\mathbb{Z}$, but it is not
true that $n$ is even for all but finitely many $n\in\mathbb{Z}$. Conversely,
it is true that $n>1$ for all but finitely many $n\in\left\{  1,2\right\}  $
(because the only $n\in\left\{  1,2\right\}  $ which does not satisfy $n>1$ is
$1$), but it is not true that $n>1$ for infinitely many $n\in\left\{
1,2\right\}  $ (because there are no infinitely many $n\in\left\{
1,2\right\}  $ to begin with).
\par
You will encounter the \textquotedblleft all but finitely
many\textquotedblright\ formulation often in abstract algebra. (Some people
abbreviate it as \textquotedblleft almost all\textquotedblright, but this
abbreviation means other things as well.)} Thus, for a permutation $\sigma\in
S_{\infty}$, we have the following equivalence of statements:%
\begin{align*}
&  \ \left(  \sigma\left(  i\right)  =i\text{ for all but finitely many }%
i\in\left\{  1,2,3,\ldots\right\}  \right) \\
&  \Longleftrightarrow\ \left(  \text{there exists some finite subset }J\text{
of }\left\{  1,2,3,\ldots\right\}  \text{ such that}\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{every }i\in\left\{  1,2,3,\ldots\right\}
\setminus J\text{ satisfies }\sigma\left(  i\right)  =i\right) \\
&  \Longleftrightarrow\ \left(  \text{there exists some finite subset }J\text{
of }\left\{  1,2,3,\ldots\right\}  \text{ such that}\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{the only }i\in\left\{  1,2,3,\ldots
\right\}  \text{ that satisfy }\sigma\left(  i\right)  \neq i\text{ are
elements of }J\right) \\
&  \Longleftrightarrow\ \left(  \text{the set of all }i\in\left\{
1,2,3,\ldots\right\}  \text{ that satisfy }\sigma\left(  i\right)  \neq
i\text{ is}\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{contained in some finite subset }J\text{
of }\left\{  1,2,3,\ldots\right\}  \right) \\
&  \Longleftrightarrow\ \left(  \text{there are only finitely many }%
i\in\left\{  1,2,3,\ldots\right\}  \text{ that satisfy }\sigma\left(
i\right)  \neq i\right)  .
\end{align*}
Hence, (\ref{eq.S(infty).def}) rewrites as follows:%
\[
S_{\left(  \infty\right)  }=\left\{  \sigma\in S_{\infty}\ \mid\ \text{there
are only finitely many }i\in\left\{  1,2,3,\ldots\right\}  \text{ that satisfy
}\sigma\left(  i\right)  \neq i\right\}  .
\]


\begin{example}
Here is an example of a permutation which is in $S_{\infty}$ but not in
$S_{\left(  \infty\right)  }$: Let $\tau$ be the permutation of $\left\{
1,2,3,\ldots\right\}  $ given by%
\[
\left(  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\tau\left(  3\right)
,\tau\left(  4\right)  ,\tau\left(  5\right)  ,\tau\left(  6\right)
,\ldots\right)  =\left(  2,1,4,3,6,5,\ldots\right)  .
\]
(It adds $1$ to every odd positive integer, and subtracts $1$ from every even
positive integer.) Then, $\tau\in S_{\infty}$ but $\tau\notin S_{\left(
\infty\right)  }$.
\end{example}

On the other hand, let us show some examples of permutations in $S_{\left(
\infty\right)  }$. For each $i\in\left\{  1,2,3,\ldots\right\}  $, let $s_{i}$
be the permutation in $S_{\infty}$ that switches $i$ with $i+1$ but leaves all
other numbers unchanged. (This is similar to the permutation $s_{i}$ in
$S_{n}$ that was defined earlier. We have taken the liberty to re-use the name
$s_{i}$, hoping that no confusion will arise.)

Again, we have $s_{i}^{2}=\operatorname*{id}$ for every $i\in\left\{
1,2,3,\ldots\right\}  $ (where $\alpha^{2}$ means $\alpha\circ\alpha$ for any
$\alpha\in S_{\infty}$).

\begin{proposition}
\label{prop.S(infty).si}We have $s_{k}\in S_{\left(  \infty\right)  }$ for
every $k\in\left\{  1,2,3,\ldots\right\}  $.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.S(infty).si}.]Let $k\in\left\{  1,2,3,\ldots
\right\}  $. The permutation $s_{k}$ has been defined as the permutation in
$S_{\infty}$ that switches $k$ with $k+1$ but leaves all other numbers
unchanged. In other words, it satisfies $s_{k}\left(  k\right)  =k+1$,
$s_{k}\left(  k+1\right)  =k$ and%
\begin{equation}
s_{k}\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,3,\ldots\right\}  \text{ such that }i\notin\left\{  k,k+1\right\}  .
\label{pf.prop.S(infty).si.1}%
\end{equation}


Now, every $i\in\left\{  1,2,3,\ldots\right\}  \setminus\left\{
k,k+1\right\}  $ satisfies $s_{k}\left(  i\right)  =i$%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}
\setminus\left\{  k,k+1\right\}  $. Thus, $i\in\left\{  1,2,3,\ldots\right\}
$ and $i\notin\left\{  k,k+1\right\}  $. Hence, (\ref{pf.prop.S(infty).si.1})
shows that $s_{k}\left(  i\right)  =i$, qed.}. Hence, there exists some finite
subset $J$ of $\left\{  1,2,3,\ldots\right\}  $ such that every $i\in\left\{
1,2,3,\ldots\right\}  \setminus J$ satisfies $s_{k}\left(  i\right)  =i$
(namely, $J=\left\{  k,k+1\right\}  $). In other words, $s_{k}\left(
i\right)  =i$ for all but finitely many $i\in\left\{  1,2,3,\ldots\right\}  $.

Thus, $s_{k}$ is an element of $S_{\infty}$ satisfying $s_{k}\left(  i\right)
=i$ for all but finitely many $i\in\left\{  1,2,3,\ldots\right\}  $. Hence,%
\[
s_{k}\in\left\{  \sigma\in S_{\infty}\ \mid\ \sigma\left(  i\right)  =i\text{
for all but finitely many }i\in\left\{  1,2,3,\ldots\right\}  \right\}
=S_{\left(  \infty\right)  }.
\]
This proves Proposition \ref{prop.S(infty).si}.
\end{proof}

Permutations can be composed and inverted, leading to new permutations. Let us
first see that the same is true for elements of $S_{\left(  \infty\right)  }$:

\begin{proposition}
\label{prop.S(infty).group}\textbf{(a)} The identity permutation
$\operatorname*{id}\in S_{\infty}$ of $\left\{  1,2,3,\ldots\right\}  $
satisfies $\operatorname*{id}\in S_{\left(  \infty\right)  }$.

\textbf{(b)} For every $\sigma\in S_{\left(  \infty\right)  }$ and $\tau\in
S_{\left(  \infty\right)  }$, we have $\sigma\circ\tau\in S_{\left(
\infty\right)  }$.

\textbf{(c)} For every $\sigma\in S_{\left(  \infty\right)  }$, we have
$\sigma^{-1}\in S_{\left(  \infty\right)  }$.
\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).group}.]We have defined $S_{\left(
\infty\right)  }$ as the set of all $\sigma\in S_{\infty}$ such that
$\sigma\left(  i\right)  =i$ for all but finitely many $i\in\left\{
1,2,3,\ldots\right\}  $. In other words, $S_{\left(  \infty\right)  }$ is the
set of all $\sigma\in S_{\infty}$ such that there exists a finite subset $K$
of $\left\{  1,2,3,\ldots\right\}  $ such that $\left(  \text{every }%
i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies }\sigma\left(
i\right)  =i\right)  $. As a consequence, we have the following two facts:

\begin{itemize}
\item If $K$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and if
$\gamma\in S_{\infty}$ is a permutation such that%
\begin{equation}
\left(  \text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{
satisfies }\gamma\left(  i\right)  =i\right)  ,
\label{pf.prop.S(infty).group.short.lem.hyp}%
\end{equation}
then%
\begin{equation}
\gamma\in S_{\left(  \infty\right)  }.
\label{pf.prop.S(infty).group.short.lem}%
\end{equation}


\item If $\gamma\in S_{\left(  \infty\right)  }$, then
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists some finite subset }K\text{ of }\left\{  1,2,3,\ldots
\right\} \\
\text{such that every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{
satisfies }\gamma\left(  i\right)  =i
\end{array}
\right)  . \label{pf.prop.S(infty).group.short.lem2}%
\end{equation}

\end{itemize}

We can now step to the actual proof of Proposition \ref{prop.S(infty).group}.

\textbf{(a)} Every $i\in\left\{  1,2,3,\ldots\right\}  \setminus\varnothing$
satisfies $\operatorname*{id}\left(  i\right)  =i$. Thus,
(\ref{pf.prop.S(infty).group.short.lem}) (applied to $K=\varnothing$ and
$\gamma=\operatorname*{id}$) yields $\operatorname*{id}\in S_{\left(
\infty\right)  }$. This proves Proposition \ref{prop.S(infty).group}
\textbf{(a)}.

\textbf{(b)} Let $\sigma\in S_{\left(  \infty\right)  }$ and $\tau\in
S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.short.lem2}) (applied to $\gamma=\sigma$),
we conclude that there exists some finite subset $K$ of $\left\{
1,2,3,\ldots\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}
\setminus K$ satisfies $\sigma\left(  i\right)  =i$. Let us denote this $K$ by
$J_{1}$. Thus, $J_{1}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}
$, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{1}\text{
satisfies }\sigma\left(  i\right)  =i.
\label{pf.prop.S(infty).group.short.b.1}%
\end{equation}


From (\ref{pf.prop.S(infty).group.short.lem2}) (applied to $\gamma=\tau$), we
conclude that there exists some finite subset $K$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus K$
satisfies $\tau\left(  i\right)  =i$. Let us denote this $K$ by $J_{2}$. Thus,
$J_{2}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{2}\text{
satisfies }\tau\left(  i\right)  =i. \label{pf.prop.S(infty).group.short.b.2}%
\end{equation}


The sets $J_{1}$ and $J_{2}$ are finite. Hence, their union $J_{1}\cup J_{2}$
is finite. Moreover,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus\left(  J_{1}\cup
J_{2}\right)  \text{ satisfies }\left(  \sigma\circ\tau\right)  \left(
i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}
\setminus\left(  J_{1}\cup J_{2}\right)  $. Thus, $i\in\left\{  1,2,3,\ldots
\right\}  $ and $i\notin J_{1}\cup J_{2}$.
\par
We have $i\notin J_{1}\cup J_{2}$ and thus $i\notin J_{1}$ (since
$J_{1}\subseteq J_{1}\cup J_{2}$). Hence, $i\in\left\{  1,2,3,\ldots\right\}
\setminus J_{1}$. Similarly, $i\in\left\{  1,2,3,\ldots\right\}  \setminus
J_{2}$. Thus, (\ref{pf.prop.S(infty).group.short.b.2}) yields $\tau\left(
i\right)  =i$. Hence, $\left(  \sigma\circ\tau\right)  \left(  i\right)
=\sigma\left(  \underbrace{\tau\left(  i\right)  }_{=i}\right)  =\sigma\left(
i\right)  =i$ (by (\ref{pf.prop.S(infty).group.short.b.1})), qed.}. Therefore,
(\ref{pf.prop.S(infty).group.short.lem}) (applied to $K=J_{1}\cup J_{2}$ and
$\gamma=\sigma\circ\tau$) yields $\sigma\circ\tau\in S_{\left(  \infty\right)
}$. This proves Proposition \ref{prop.S(infty).group} \textbf{(b)}.

\textbf{(c)} Let $\sigma\in S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.short.lem2}) (applied to $\gamma=\sigma$),
we conclude that there exists some finite subset $K$ of $\left\{
1,2,3,\ldots\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}
\setminus K$ satisfies $\sigma\left(  i\right)  =i$. Consider this $K$. Thus,
$K$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).group.short.c.3}%
\end{equation}


Now,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies
}\sigma^{-1}\left(  i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}  \setminus
K$. Thus, $\sigma\left(  i\right)  =i$ (according to
(\ref{pf.prop.S(infty).group.short.c.3})), so that $\sigma^{-1}\left(
i\right)  =i$, qed.}. Therefore, (\ref{pf.prop.S(infty).group.short.lem})
(applied to $\gamma=\sigma^{-1}$) yields $\sigma^{-1}\in S_{\left(
\infty\right)  }$. This proves Proposition \ref{prop.S(infty).group}
\textbf{(c)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).group}.]Let us record the following facts:

\begin{itemize}
\item If $K$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and if
$\gamma\in S_{\infty}$ is a permutation such that%
\begin{equation}
\left(  \text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{
satisfies }\gamma\left(  i\right)  =i\right)  ,
\label{pf.prop.S(infty).group.lem.hyp}%
\end{equation}
then%
\begin{equation}
\gamma\in S_{\left(  \infty\right)  }. \label{pf.prop.S(infty).group.lem}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.S(infty).group.lem}):} Let $K$ be a
finite subset of $\left\{  1,2,3,\ldots\right\}  $, and let $\gamma\in
S_{\infty}$ be a permutation such that (\ref{pf.prop.S(infty).group.lem.hyp})
holds. We need to prove that $\gamma\in S_{\left(  \infty\right)  }$.
\par
Every $i\in\left\{  1,2,3,\ldots\right\}  \setminus K$ satisfies
$\gamma\left(  i\right)  =i$ (since (\ref{pf.prop.S(infty).group.lem.hyp})
holds). Thus, there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\gamma\left(  i\right)  =i$ (namely, $J=K$). In other words,
$\gamma\left(  i\right)  =i$ for all but finitely many $i\in\left\{
1,2,3,\ldots\right\}  $.
\par
Thus, $\gamma$ is an element of $S_{\infty}$ satisfying $\gamma\left(
i\right)  =i$ for all but finitely many $i\in\left\{  1,2,3,\ldots\right\}  $.
Hence,%
\[
\gamma\in\left\{  \sigma\in S_{\infty}\ \mid\ \sigma\left(  i\right)  =i\text{
for all but finitely many }i\in\left\{  1,2,3,\ldots\right\}  \right\}
=S_{\left(  \infty\right)  }.
\]
This proves (\ref{pf.prop.S(infty).group.lem}).}

\item If $\gamma\in S_{\left(  \infty\right)  }$, then
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists some finite subset }J\text{ of }\left\{  1,2,3,\ldots
\right\} \\
\text{such that every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{
satisfies }\gamma\left(  i\right)  =i
\end{array}
\right)  \label{pf.prop.S(infty).group.lem2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.S(infty).group.lem2}):} Let
$\gamma\in S_{\left(  \infty\right)  }$. Then,
\[
\gamma\in S_{\left(  \infty\right)  }=\left\{  \sigma\in S_{\infty}%
\ \mid\ \sigma\left(  i\right)  =i\text{ for all but finitely many }%
i\in\left\{  1,2,3,\ldots\right\}  \right\}  .
\]
In other words, $\gamma$ is an element of $S_{\infty}$ such that
$\gamma\left(  i\right)  =i$ for all but finitely many $i\in\left\{
1,2,3,\ldots\right\}  $.
\par
Now, we know that $\gamma\left(  i\right)  =i$ for all but finitely many
$i\in\left\{  1,2,3,\ldots\right\}  $. In other words, there exists some
finite subset $J$ of $\left\{  1,2,3,\ldots\right\}  $ such that every
$i\in\left\{  1,2,3,\ldots\right\}  \setminus J$ satisfies $\gamma\left(
i\right)  =i$. This proves (\ref{pf.prop.S(infty).group.lem2}).}.
\end{itemize}

We can now step to the actual proof of Proposition \ref{prop.S(infty).group}.

\textbf{(a)} Every $i\in\left\{  1,2,3,\ldots\right\}  \setminus\varnothing$
satisfies $\operatorname*{id}\left(  i\right)  =i$. Thus,
(\ref{pf.prop.S(infty).group.lem}) (applied to $K=\varnothing$ and
$\gamma=\operatorname*{id}$) yields $\operatorname*{id}\in S_{\left(
\infty\right)  }$. This proves Proposition \ref{prop.S(infty).group}
\textbf{(a)}.

\textbf{(b)} Let $\sigma\in S_{\left(  \infty\right)  }$ and $\tau\in
S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.lem2}) (applied to $\gamma=\sigma$), we
conclude that there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\sigma\left(  i\right)  =i$. Let us denote this $J$ by $J_{1}$.
Thus, $J_{1}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{1}\text{
satisfies }\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).group.b.1}%
\end{equation}


From (\ref{pf.prop.S(infty).group.lem2}) (applied to $\gamma=\tau$), we
conclude that there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\tau\left(  i\right)  =i$. Let us denote this $J$ by $J_{2}$. Thus,
$J_{2}$ is a finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J_{2}\text{
satisfies }\tau\left(  i\right)  =i. \label{pf.prop.S(infty).group.b.2}%
\end{equation}


The sets $J_{1}$ and $J_{2}$ are finite. Hence, their union $J_{1}\cup J_{2}$
is finite. Moreover,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus\left(  J_{1}\cup
J_{2}\right)  \text{ satisfies }\left(  \sigma\circ\tau\right)  \left(
i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}
\setminus\left(  J_{1}\cup J_{2}\right)  $. Thus, $i\in\left\{  1,2,3,\ldots
\right\}  $ and $i\notin J_{1}\cup J_{2}$.
\par
If we had $i\in J_{1}$, then we would have $i\in J_{1}\subseteq J_{1}\cup
J_{2}$, which would contradict $i\notin J_{1}\cup J_{2}$. Thus, we cannot have
$i\in J_{1}$. In other words, we have $i\notin J_{1}$. Hence, $i\in\left\{
1,2,3,\ldots\right\}  \setminus J_{1}$. Similarly, $i\in\left\{
1,2,3,\ldots\right\}  \setminus J_{2}$. Thus,
(\ref{pf.prop.S(infty).group.b.2}) yields $\tau\left(  i\right)  =i$. Hence,
$\left(  \sigma\circ\tau\right)  \left(  i\right)  =\sigma\left(
\underbrace{\tau\left(  i\right)  }_{=i}\right)  =\sigma\left(  i\right)  =i$
(by (\ref{pf.prop.S(infty).group.b.1})), qed.}. Therefore,
(\ref{pf.prop.S(infty).group.lem}) (applied to $K=J_{1}\cup J_{2}$ and
$\gamma=\sigma\circ\tau$) yields $\sigma\circ\tau\in S_{\left(  \infty\right)
}$. This proves Proposition \ref{prop.S(infty).group} \textbf{(b)}.

\textbf{(c)} Let $\sigma\in S_{\left(  \infty\right)  }$.

From (\ref{pf.prop.S(infty).group.lem2}) (applied to $\gamma=\sigma$), we
conclude that there exists some finite subset $J$ of $\left\{  1,2,3,\ldots
\right\}  $ such that every $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$
satisfies $\sigma\left(  i\right)  =i$. Consider this $J$. Thus, $J$ is a
finite subset of $\left\{  1,2,3,\ldots\right\}  $, and%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).group.c.3}%
\end{equation}


Now,
\[
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{ satisfies
}\sigma^{-1}\left(  i\right)  =i
\]
\footnote{\textit{Proof.} Let $i\in\left\{  1,2,3,\ldots\right\}  \setminus
J$. Thus, $\sigma\left(  i\right)  =i$ (according to
(\ref{pf.prop.S(infty).group.c.3})), so that $\sigma^{-1}\left(  i\right)
=i$, qed.}. Therefore, (\ref{pf.prop.S(infty).group.lem}) (applied to $K=J$
and $\gamma=\sigma^{-1}$) yields $\sigma^{-1}\in S_{\left(  \infty\right)  }$.
This proves Proposition \ref{prop.S(infty).group} \textbf{(c)}.
\end{proof}
\end{verlong}

In the language of group theorists, Proposition \ref{prop.S(infty).group} show
that $S_{\left(  \infty\right)  }$ is a subgroup of the group $S_{\infty}$.
The elements of $S_{\left(  \infty\right)  }$ are called the \textit{finitary
permutations of }$\left\{  1,2,3,\ldots\right\}  $, and $S_{\left(
\infty\right)  }$ is called the \textit{finitary symmetric group of }$\left\{
1,2,3,\ldots\right\}  $.

We now have the following analogue of Exercise \ref{exe.ps2.2.4} (without its
part \textbf{(c)}):

\begin{exercise}
\label{exe.ps2.2.4'}\textbf{(a)} Show that $s_{i}\circ s_{i+1}\circ
s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}$ for all $i\in\left\{  1,2,3,\ldots
\right\}  $.

\textbf{(b)} Show that every permutation $\sigma\in S_{\left(  \infty\right)
}$ can be written as a composition of several permutations of the form $s_{k}$
(with $k\in\left\{  1,2,3,\ldots\right\}  $).
\end{exercise}

\begin{remark}
In the language of group theory, the statement of Exercise \ref{exe.ps2.2.4'}
\textbf{(b)} says (or, more precisely, yields) that the permutations
$s_{1},s_{2},s_{3},\ldots$ generate the group $S_{\left(  \infty\right)  }$.
\end{remark}

If $\sigma\in S_{\infty}$ is a permutation, then an \textit{inversion} of
$\sigma$ means a pair $\left(  i,j\right)  $ of integers satisfying $1\leq
i<j$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. This definition
of an inversion is, of course, analogous to the definition of an inversion of
a $\sigma\in S_{n}$; thus we could try to define the length of a $\sigma\in
S_{\infty}$. However, here we run into troubles: A permutation $\sigma\in
S_{\infty}$ might have infinitely many inversions!

It is here that we really need to restrict ourselves to $S_{\left(
\infty\right)  }$. This indeed helps:

\begin{proposition}
\label{prop.S(infty).inversions}Let $\sigma\in S_{\left(  \infty\right)  }$. Then:

\textbf{(a)} There exists some $N\in\left\{  1,2,3,\ldots\right\}  $ such that
every integer $i>N$ satisfies $\sigma\left(  i\right)  =i$.

\textbf{(b)} There are only finitely many inversions of $\sigma$.
\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).inversions}.]\textbf{(a)} We can
apply (\ref{pf.prop.S(infty).group.short.lem2}) to $\gamma=\sigma$. As a
consequence, we obtain that there exists some finite subset $K$ of $\left\{
1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus K\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).inversions.short.a.J}%
\end{equation}
Consider this $K$.

The set $K$ is finite. Hence, the set $K\cup\left\{  1\right\}  $ is finite;
this set is also nonempty (since it contains $1$) and a subset of $\left\{
1,2,3,\ldots\right\}  $. Therefore, this set $K\cup\left\{  1\right\}  $ has a
greatest element (since every finite nonempty subset of $\left\{
1,2,3,\ldots\right\}  $ has a greatest element). Let $n$ be this greatest
element. Clearly, $n\in K\cup\left\{  1\right\}  \subseteq\left\{
1,2,3,\ldots\right\}  $, so that $n>0$.

Every $j\in K\cup\left\{  1\right\}  $ satisfies
\begin{equation}
j\leq n \label{pf.prop.S(infty).inversions.short.a.1}%
\end{equation}
(since $n$ is the greatest element of $K\cup\left\{  1\right\}  $). Now, let
$i$ be an integer such that $i>n$. Then, $i>n>0$, so that $i$ is a positive
integer. If we had $i\in K$, then we would have $i\in K\subseteq K\cup\left\{
1\right\}  $ and thus $i\leq n$ (by
(\ref{pf.prop.S(infty).inversions.short.a.1}), applied to $j=i$), which would
contradict $i>n$. Hence, we cannot have $i\in K$. We thus have $i\notin K$.
Since $i\in\left\{  1,2,3,\ldots\right\}  $, this shows that $i\in\left\{
1,2,3,\ldots\right\}  \setminus K$. Thus, $\sigma\left(  i\right)  =i$ (by
(\ref{pf.prop.S(infty).inversions.short.a.J})).

Let us now forget that we fixed $i$. We thus have shown that every integer
$i>n$ satisfies $\sigma\left(  i\right)  =i$. Hence, Proposition
\ref{prop.S(infty).inversions} \textbf{(a)} holds (we can take $N=n$).

\textbf{(b)} Proposition \ref{prop.S(infty).inversions} \textbf{(a)} shows
that there exists some $N\in\left\{  1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every integer }i>N\text{ satisfies }\sigma\left(  i\right)  =i.
\label{pf.prop.S(infty).inversions.short.b.N}%
\end{equation}
Consider such an $N$. We shall now show that
\[
\text{every inversion of }\sigma\text{ is an element of }\left\{
1,2,\ldots,N\right\}  ^{2}.
\]


In fact, let $c$ be an inversion of $\sigma$. We will show that $c$ is an
element of $\left\{  1,2,\ldots,N\right\}  ^{2}$.

We know that $c$ is an inversion of $\sigma$. In other words, $c$ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j$ and $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright). Consider this
$\left(  i,j\right)  $. We then have $i\leq N$\ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.short.b.N}) shows that $\sigma\left(
i\right)  =i$. Also, $i<j$, so that $j>i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.short.b.N}) (applied to $j$ instead of $i$)
shows that $\sigma\left(  j\right)  =j$. Thus, $\sigma\left(  i\right)
=i<j=\sigma\left(  j\right)  $. This contradicts $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. This contradiction shows that our assumption was
wrong, qed.} and $j\leq N$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, $j>N$. Hence, (\ref{pf.prop.S(infty).inversions.short.b.N})
(applied to $j$ instead of $i$) shows that $\sigma\left(  j\right)  =j$. Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  =j>N$. Therefore,
(\ref{pf.prop.S(infty).inversions.short.b.N}) (applied to $\sigma\left(
i\right)  $ instead of $i$) yields $\sigma\left(  \sigma\left(  i\right)
\right)  =\sigma\left(  i\right)  $. But $\sigma$ is a permutation, and thus
an injective map. Hence, from $\sigma\left(  \sigma\left(  i\right)  \right)
=\sigma\left(  i\right)  $, we obtain $\sigma\left(  i\right)  =i$. Thus,
$\sigma\left(  i\right)  =i<j=\sigma\left(  j\right)  $. This contradicts
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. This contradiction shows
that our assumption was wrong, qed.}. Consequently, $\left(  i,j\right)
\in\left\{  1,2,\ldots,N\right\}  ^{2}$. Hence, $c=\left(  i,j\right)
\in\left\{  1,2,\ldots,N\right\}  ^{2}$.

Now, let us forget that we fixed $c$. We thus have shown that if $c$ is an
inversion of $\sigma$, then $c$ is an element of $\left\{  1,2,\ldots
,N\right\}  ^{2}$. In other words, every inversion of $\sigma$ is an element
of $\left\{  1,2,\ldots,N\right\}  ^{2}$. Thus, there are only finitely many
inversions of $\sigma$ (since there are only finitely many elements of
$\left\{  1,2,\ldots,N\right\}  ^{2}$). Proposition
\ref{prop.S(infty).inversions} \textbf{(b)} is thus proven.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.S(infty).inversions}.]\textbf{(a)} We can
apply (\ref{pf.prop.S(infty).group.lem2}) to $\gamma=\sigma$. As a
consequence, we obtain that there exists some finite subset $J$ of $\left\{
1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every }i\in\left\{  1,2,3,\ldots\right\}  \setminus J\text{ satisfies
}\sigma\left(  i\right)  =i. \label{pf.prop.S(infty).inversions.a.J}%
\end{equation}
Consider this $J$.

The set $J$ is finite. Hence, the set $J\cup\left\{  1\right\}  $ is finite;
this set is also nonempty (since it contains $1$) and a subset of $\left\{
1,2,3,\ldots\right\}  $. Therefore, this set $J\cup\left\{  1\right\}  $ has a
greatest element (since every finite nonempty subset of $\left\{
1,2,3,\ldots\right\}  $ has a greatest element). Let $n$ be this greatest
element. Clearly, $n\in J\cup\left\{  1\right\}  \subseteq\left\{
1,2,3,\ldots\right\}  $, so that $n>0$.

Every $j\in J\cup\left\{  1\right\}  $ satisfies
\begin{equation}
j\leq n \label{pf.prop.S(infty).inversions.a.1}%
\end{equation}
(since $n$ is the greatest element of $J\cup\left\{  1\right\}  $). Now, let
$i$ be an integer such that $i>n$. Then, $i>n>0$, so that $i$ is a positive
integer. If we had $i\in J$, then we would have $i\in J\subseteq J\cup\left\{
1\right\}  $ and thus $i\leq n$ (by (\ref{pf.prop.S(infty).inversions.a.1}),
applied to $j=i$), which would contradict $i>n$. Hence, we cannot have $i\in
J$. We thus have $i\notin J$. Since $i\in\left\{  1,2,3,\ldots\right\}  $,
this shows that $i\in\left\{  1,2,3,\ldots\right\}  \setminus J$. Thus,
$\sigma\left(  i\right)  =i$ (by (\ref{pf.prop.S(infty).inversions.a.J})).

Let us now forget that we fixed $i$. We thus have shown that every integer
$i>n$ satisfies $\sigma\left(  i\right)  =i$. Hence, there exists some
$N\in\left\{  1,2,3,\ldots\right\}  $ such that every integer $i>N$ satisfies
$\sigma\left(  i\right)  =i$ (namely, $N=n$). Proposition
\ref{prop.S(infty).inversions} \textbf{(a)} is thus proven.

\textbf{(b)} Proposition \ref{prop.S(infty).inversions} \textbf{(a)} shows
that there exists some $N\in\left\{  1,2,3,\ldots\right\}  $ such that%
\begin{equation}
\text{every integer }i>N\text{ satisfies }\sigma\left(  i\right)  =i.
\label{pf.prop.S(infty).inversions.b.N}%
\end{equation}
Consider such an $N$. We shall now show that
\[
\text{every inversion of }\sigma\text{ is an element of }\left\{
1,2,\ldots,N\right\}  ^{2}.
\]


In fact, let $c$ be an inversion of $\sigma$. We will show that $c$ is an
element of $\left\{  1,2,\ldots,N\right\}  ^{2}$.

We know that $c$ is an inversion of $\sigma$. In other words, $c$ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j$ and $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright). Consider this
$\left(  i,j\right)  $. We then have $i\leq N$\ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.b.N}) shows that $\sigma\left(  i\right)
=i$. Also, $i<j$, so that $j>i>N$. Hence,
(\ref{pf.prop.S(infty).inversions.b.N}) (applied to $j$ instead of $i$) shows
that $\sigma\left(  j\right)  =j$. Thus, $\sigma\left(  i\right)
=i<j=\sigma\left(  j\right)  $. This contradicts $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. This contradiction shows that our assumption was
wrong, qed.} and $j\leq N$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, $j>N$. Hence, (\ref{pf.prop.S(infty).inversions.b.N}) (applied
to $j$ instead of $i$) shows that $\sigma\left(  j\right)  =j$. Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  =j>N$. Therefore,
(\ref{pf.prop.S(infty).inversions.b.N}) (applied to $\sigma\left(  i\right)  $
instead of $i$) yields $\sigma\left(  \sigma\left(  i\right)  \right)
=\sigma\left(  i\right)  $. But $\sigma$ is a permutation, and thus an
injective map. Hence, from $\sigma\left(  \sigma\left(  i\right)  \right)
=\sigma\left(  i\right)  $, we obtain $\sigma\left(  i\right)  =i$. Thus,
$\sigma\left(  i\right)  =i<j=\sigma\left(  j\right)  $. This contradicts
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. This contradiction shows
that our assumption was wrong, qed.}. From $1\leq i\leq N$, we obtain
$i\in\left\{  1,2,\ldots,N\right\}  $. From $1\leq j\leq N$, we obtain
$j\in\left\{  1,2,\ldots,N\right\}  $. Combining $i\in\left\{  1,2,\ldots
,N\right\}  $ with $j\in\left\{  1,2,\ldots,N\right\}  $, we obtain $\left(
i,j\right)  \in\left\{  1,2,\ldots,N\right\}  ^{2}$. Hence, $c=\left(
i,j\right)  \in\left\{  1,2,\ldots,N\right\}  ^{2}$. In other words, $c$ is an
element of $\left\{  1,2,\ldots,N\right\}  ^{2}$.

Now, let us forget that we fixed $c$. We thus have shown that if $c$ is an
inversion of $\sigma$, then $c$ is an element of $\left\{  1,2,\ldots
,N\right\}  ^{2}$. In other words, every inversion of $\sigma$ is an element
of $\left\{  1,2,\ldots,N\right\}  ^{2}$. Thus, there are only finitely many
inversions of $\sigma$ (since there are only finitely many elements of
$\left\{  1,2,\ldots,N\right\}  ^{2}$). Proposition
\ref{prop.S(infty).inversions} \textbf{(b)} is thus proven.
\end{proof}
\end{verlong}

Actually, Proposition \ref{prop.S(infty).inversions} \textbf{(b)} has a
converse: If a permutation $\sigma\in S_{\infty}$ has only finitely many
inversions, then $\sigma$ belongs to $S_{\left(  \infty\right)  }$. This is
easy to prove; but we won't use this.

If $\sigma\in S_{\left(  \infty\right)  }$ is a permutation, then the
\textit{length} of $\sigma$ means the number of inversions of $\sigma$. This
is well-defined, because there are only finitely many inversions of $\sigma$
(by Proposition \ref{prop.S(infty).inversions} \textbf{(b)}). The length of
$\sigma$ is denoted by $\ell\left(  \sigma\right)  $; it is a nonnegative
integer. The only permutation having length $0$ is the identity permutation
$\operatorname*{id}\in S_{\infty}$.

We have the following analogue of Exercise \ref{exe.ps2.2.5}:

\begin{exercise}
\label{exe.ps2.2.5'}\textbf{(a)} Show that every permutation $\sigma\in
S_{\left(  \infty\right)  }$ and every $k\in\left\{  1,2,3,\ldots\right\}  $
satisfy%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  \label{eq.exe.2.5'.a.1}%
\end{equation}
and%
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  <\sigma^{-1}\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(
k\right)  >\sigma^{-1}\left(  k+1\right)
\end{array}
\right.  . \label{eq.exe.2.5'.a.2}%
\end{equation}


\textbf{(b)} Show that any two permutations $\sigma$ and $\tau$ in $S_{\left(
\infty\right)  }$ satisfy $\ell\left(  \sigma\circ\tau\right)  \equiv
\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  \operatorname{mod}2$.

\textbf{(c)} Show that any two permutations $\sigma$ and $\tau$ in $S_{\left(
\infty\right)  }$ satisfy $\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $.

\textbf{(d)} If $\sigma\in S_{\left(  \infty\right)  }$ is a permutation
satisfying $\sigma\left(  1\right)  \leq\sigma\left(  2\right)  \leq
\sigma\left(  3\right)  \leq\cdots$, then show that $\sigma=\operatorname*{id}%
$.

\textbf{(e)} Let $\sigma\in S_{\left(  \infty\right)  }$. Show that $\sigma$
can be written as a composition of $\ell\left(  \sigma\right)  $ permutations
of the form $s_{k}$ (with $k\in\left\{  1,2,3,\ldots\right\}  $).

\textbf{(f)} Let $\sigma\in S_{\left(  \infty\right)  }$. Then, show that
$\ell\left(  \sigma\right)  =\ell\left(  \sigma^{-1}\right)  $.

\textbf{(g)} Let $\sigma\in S_{\left(  \infty\right)  }$. Show that
$\ell\left(  \sigma\right)  $ is the smallest $N\in\mathbb{N}$ such that
$\sigma$ can be written as a composition of $N$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,3,\ldots\right\}  $).
\end{exercise}

We also have an analogue of Exercise \ref{exe.ps2.2.6}:

\begin{exercise}
\label{exe.ps2.2.6'}Let $\sigma\in S_{\left(  \infty\right)  }$. In Exercise
\ref{exe.ps2.2.4'} \textbf{(b)}, we have seen that $\sigma$ can be written as
a composition of several permutations of the form $s_{k}$ (with $k\in\left\{
1,2,3,\ldots\right\}  $). Usually there will be several ways to do so (for
instance, $\operatorname*{id}=s_{1}\circ s_{1}=s_{2}\circ s_{2}=s_{3}\circ
s_{3}=\cdots$). Show that, whichever of these ways we take, the number of
permutations composed will be congruent to $\ell\left(  \sigma\right)  $
modulo $2$.
\end{exercise}

Having defined the length of a permutation $\sigma\in S_{\left(
\infty\right)  }$, we can now define the sign of such a permutation. Again, we
mimic the definition of the sign of a $\sigma\in S_{n}$:

\begin{definition}
\label{def.perm.sign'}We define the \textit{sign} of a permutation $\sigma\in
S_{\left(  \infty\right)  }$ as the integer $\left(  -1\right)  ^{\ell\left(
\sigma\right)  }$. We denote this sign by $\left(  -1\right)  ^{\sigma}$ or
$\operatorname*{sign}\sigma$ or $\operatorname*{sgn}\sigma$. We say that a
permutation $\sigma$ is \textit{even} if its sign is $1$ (that is, if
$\ell\left(  \sigma\right)  $ is even), and \textit{odd} if its sign is $-1$
(that is, if $\ell\left(  \sigma\right)  $ is odd).
\end{definition}

Signs of permutations have the following properties:

\begin{itemize}
\item The sign of the identity permutation $\operatorname*{id}\in S_{\left(
\infty\right)  }$ is $\left(  -1\right)  ^{\operatorname*{id}}=1$. In other
words, $\operatorname*{id}\in S_{\left(  \infty\right)  }$ is even.

\item For every $k\in\left\{  1,2,3,\ldots\right\}  $, the sign of the
permutation $s_{k}\in S_{\left(  \infty\right)  }$ is $\left(  -1\right)
^{s_{k}}=-1$.

\item If $\sigma$ and $\tau$ are two permutations in $S_{\left(
\infty\right)  }$, then $\left(  -1\right)  ^{\sigma\circ\tau}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$.

\item If $\sigma\in S_{\left(  \infty\right)  }$, then $\left(  -1\right)
^{\sigma^{-1}}=\left(  -1\right)  ^{\sigma}$.
\end{itemize}

The proofs of all these properties are analogous to the proofs of the
analogous properties for permutations in $S_{n}$.

\begin{remark}
We have defined the sign of a permutation $\sigma\in S_{\left(  \infty\right)
}$. No such notion exists for permutations $\sigma\in S_{\infty}$. In fact,
one can show that if an element $\lambda_{\sigma}$ of $\left\{  1,-1\right\}
$ is chosen for each $\sigma\in S_{\infty}$ in such a way that every two
permutations $\sigma,\tau\in S_{\infty}$ satisfy $\lambda_{\sigma\circ\tau
}=\lambda_{\sigma}\cdot\lambda_{\tau}$, then all of the $\lambda_{\sigma}$ are
$1$. (Indeed, this follows from a result of Oystein Ore; see \url{http://mathoverflow.net/questions/54371}\ .)
\end{remark}

\begin{remark}
For every $n\in\mathbb{N}$ and every $\sigma\in S_{n}$, we can define a
permutation $\sigma_{\left(  \infty\right)  }\in S_{\left(  \infty\right)  }$
by setting%
\[
\sigma_{\left(  \infty\right)  }\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
\sigma\left(  i\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }i\leq n;\\
i,\ \ \ \ \ \ \ \ \ \ \text{if }i>n
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,3,\ldots\right\}
.
\]
Essentially, $\sigma_{\left(  \infty\right)  }$ is the permutation $\sigma$
extended to the set of all positive integers in the laziest possible way: It
just sends each $i>n$ to itself.

For every $n\in\mathbb{N}$, there is an injective map $\mathbf{i}_{n}%
:S_{n}\rightarrow S_{\left(  \infty\right)  }$ defined as follows:%
\[
\mathbf{i}_{n}\left(  \sigma\right)  =\sigma_{\left(  \infty\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\]
This map $\mathbf{i}_{n}$ is an example of what algebraists call a
\textit{group homomorphism}: It satisfies%
\begin{align*}
\mathbf{i}_{n}\left(  \operatorname*{id}\right)   &  =\operatorname*{id};\\
\mathbf{i}_{n}\left(  \sigma\circ\tau\right)   &  =\mathbf{i}_{n}\left(
\sigma\right)  \circ\mathbf{i}_{n}\left(  \tau\right)
\ \ \ \ \ \ \ \ \ \ \text{for all }\sigma,\tau\in S_{n};\\
\mathbf{i}_{n}\left(  \sigma^{-1}\right)   &  =\left(  \mathbf{i}_{n}\left(
\sigma\right)  \right)  ^{-1}\ \ \ \ \ \ \ \ \ \ \text{for all }\sigma\in
S_{n}.
\end{align*}
(This is all easy to check.) Thus, we can consider the image $\mathbf{i}%
_{n}\left(  S_{n}\right)  $ of $S_{n}$ under this map as a \textquotedblleft
copy\textquotedblright\ of $S_{n}$ which is \textquotedblleft just as good as
the original\textquotedblright\ (i.e., composition in this copy behaves in the
same way as composition in the original). It is easy to characterize this copy
inside $S_{\left(  \infty\right)  }$: Namely,%
\[
\mathbf{i}_{n}\left(  S_{n}\right)  =\left\{  \sigma\in S_{\left(
\infty\right)  }\ \mid\ \sigma\left(  i\right)  =i\text{ for all }i>n\right\}
.
\]


Using Proposition \ref{prop.S(infty).inversions} \textbf{(a)}, it is easy to
check that $S_{\left(  \infty\right)  }=\bigcup_{n\in\mathbb{N}}\mathbf{i}%
_{n}\left(  S_{n}\right)  =\mathbf{i}_{0}\left(  S_{0}\right)  \cup
\mathbf{i}_{1}\left(  S_{1}\right)  \cup\mathbf{i}_{2}\left(  S_{2}\right)
\cup\cdots$. Therefore, many properties of $S_{\left(  \infty\right)  }$ can
be derived from analogous properties of $S_{n}$ for finite $n$. For example,
using this tactic, we could easily derive Exercise \ref{exe.ps2.2.5'} from
Exercise \ref{exe.ps2.2.5}, and derive Exercise \ref{exe.ps2.2.6'} from
Exercise \ref{exe.ps2.2.6}. (However, we can just as well solve Exercises
\ref{exe.ps2.2.5'} and \ref{exe.ps2.2.6'} by copying the solutions of
Exercises \ref{exe.ps2.2.5} and \ref{exe.ps2.2.6} and making the necessary
changes; this is how I solve these exercises further below.)
\end{remark}

\subsection{More on lengths of permutations}

Let us summarize some of what we have learnt about permutations. We have
defined the length $\ell\left(  \sigma\right)  $ and the inversions of a
permutation $\sigma\in S_{n}$, where $n$ is a nonnegative integer. We recall
the basic properties of these objects:

\begin{itemize}
\item For each $k\in\left\{  1,2,\ldots,n-1\right\}  $, we defined $s_{k}$ to
be the permutation in $S_{n}$ that switches $k$ with $k+1$ but leaves all
other numbers unchanged. These permutations satisfy $s_{i}^{2}%
=\operatorname*{id}$ for every $i\in\left\{  1,2,\ldots,n-1\right\}  $ and%
\begin{equation}
s_{i}\circ s_{i+1}\circ s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}%
\ \ \ \ \ \ \ \ \ \ \text{for all }i\in\left\{  1,2,\ldots,n-2\right\}
\label{eq.perms.braid3}%
\end{equation}
(according to Exercise \ref{exe.ps2.2.4} \textbf{(a)}). Also, it is easy to
check that%
\begin{equation}
s_{i}\circ s_{j}=s_{j}\circ s_{i}\ \ \ \ \ \ \ \ \ \ \text{for all }%
i,j\in\left\{  1,2,\ldots,n-1\right\}  \text{ with }\left\vert i-j\right\vert
>1. \label{eq.perms.braid2}%
\end{equation}


\item An \textit{inversion} of a permutation $\sigma\in S_{n}$ means a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. The \textit{length}
$\ell\left(  \sigma\right)  $ of a permutation $\sigma\in S_{n}$ is the number
of inversions of $\sigma$.

\item Any two permutations $\sigma\in S_{n}$ and $\tau\in S_{n}$ satisfy%
\begin{equation}
\ell\left(  \sigma\circ\tau\right)  \equiv\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \operatorname{mod}2 \label{eq.perms.length.mod2}%
\end{equation}
(by Exercise \ref{exe.ps2.2.5} \textbf{(b)}) and%
\begin{equation}
\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \label{eq.perms.length.leq}%
\end{equation}
(by Exercise \ref{exe.ps2.2.5} \textbf{(c)}).

\item If $\sigma\in S_{n}$, then $\ell\left(  \sigma\right)  =\ell\left(
\sigma^{-1}\right)  $ (according to Exercise \ref{exe.ps2.2.5} \textbf{(f)}).

\item If $\sigma\in S_{n}$, then $\ell\left(  \sigma\right)  $ is the smallest
$N\in\mathbb{N}$ such that $\sigma$ can be written as a composition of $N$
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). (This follows from Exercise \ref{exe.ps2.2.5} \textbf{(g)}.)
\end{itemize}

By now, we know almost all about the $s_{k}$'s and about the lengths of
permutations that is necessary for studying determinants. (\textquotedblleft
Almost\textquotedblright\ because Exercise \ref{exe.ps4.1ab} below will also
be useful.) I shall now sketch some more advanced properties of these things,
partly as a curiosity, partly to further your intuition; none of these
properties shall be used further below.

First, here is a way to visualize lengths of permutations using graph theory:

Fix $n\in\mathbb{N}$. We define the $n$\textit{-th right Bruhat graph} to be
the (undirected) graph whose vertices are the permutations $\sigma\in S_{n}$,
and whose edges are defined as follows: Two vertices $\sigma\in S_{n}$ and
$\tau\in S_{n}$ are adjacent if and only if there exists a $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\sigma=\tau\circ s_{k}$. (This condition
is clearly symmetric in $\sigma$ and $\tau$: If $\sigma=\tau\circ s_{k}$, then
$\tau=\sigma\circ s_{k}$.) For instance, the $3$-rd right Bruhat graph looks
as follows:%
\[%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%& 321 \ar@{-}[dl] \ar@{-}[dr] & \\
%312 \ar@{-}[d] & & 231 \ar@{-}[d] \\
%132 \ar@{-}[dr] & & 213 \ar@{-}[dl] \\
%& 123 &
%}}}%
%BeginExpansion
\xymatrix{
& 321 \ar@{-}[dl] \ar@{-}[dr] & \\
312 \ar@{-}[d] & & 231 \ar@{-}[d] \\
132 \ar@{-}[dr] & & 213 \ar@{-}[dl] \\
& 123 &
}%
%EndExpansion
,
\]
where we are writing permutations in one-line notation (and omitting
parentheses and commas). The $4$-th right Bruhat graph can be seen
\href{https://en.wikipedia.org/wiki/File:Symmetric_group_4;_Cayley_graph_1,2,6_%283D%29.svg}{on
Wikipedia}.\footnote{Don't omit the word \textquotedblleft
right\textquotedblright\ in \textquotedblleft right Bruhat
graph\textquotedblright; else it means a different graph with more edges.}

These graphs have lots of properties. There is a canonical way to direct their
edges: The edge between $\sigma$ and $\tau$ is directed towards the vertex
with the larger length. (The lengths of $\sigma$ and $\tau$ always differ by
$1$ if there is an edge between $\sigma$ and $\tau$.) This way, the $n$-th
right Bruhat graph is an acyclic directed graph. It therefore defines a
partially ordered set, called the \textit{right permutohedron order}%
\footnote{also known as the \textit{right weak order} or \textit{right weak
Bruhat order} (but, again, do not omit the words \textquotedblleft
right\textquotedblright\ and \textquotedblleft weak\textquotedblright)} on
$S_{n}$, whose elements are the permutations $\sigma\in S_{n}$ and whose order
relation is defined as follows: We have $\sigma\leq\tau$ if and only if there
is a directed path from $\sigma$ to $\tau$ in the directed $n$-th right Bruhat
graph. If you know
\href{https://en.wikipedia.org/wiki/Lattice_%28order%29}{the (combinatorial)
notion of a lattice}, you might notice that this right permutohedron order
\href{http://mathoverflow.net/questions/158945/why-is-the-right-permutohedron-order-aka-weak-order-on-s-n-a-lattice}{is
a lattice}.

The word \textquotedblleft permutohedron\textquotedblright\ in
\textquotedblleft permutohedron order\textquotedblright\ hints at what might
be its least expected property: The $n$-th Bruhat graph can be viewed as the
graph formed by the vertices and the edges of a certain polytope in
$n$-dimensional space $\mathbb{R}^{n}$. This polytope -- called the
$n$\textit{-th
\href{https://en.wikipedia.org/wiki/Permutohedron}{\textit{permutohedron}}%
}\footnote{Some spell it \textquotedblleft\textit{permutahedron}%
\textquotedblright\ instead. The word is of relatively recent origin (1969).}
-- is the convex hull of the points $\left(  \sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  \right)  $ for
$\sigma\in S_{n}$. These points are its vertices; however, its vertex $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  $ corresponds to the vertex $\sigma^{-1}$ (not $\sigma$) of
the $n$-th Bruhat graph. Feel free to roam
\href{https://en.wikipedia.org/wiki/Permutohedron}{its Wikipedia page} for
other (combinatorial and geometric) curiosities.

The notion of a length fits perfectly into this whole picture. For instance,
the length $\ell\left(  \sigma\right)  $ of a permutation $\sigma$ is the
smallest length of a path from $\operatorname*{id}\in S_{n}$ to $\sigma$ on
the $n$-th right Bruhat graph (and this holds no matter whether the graph is
considered to be directed or not). For the undirected Bruhat graphs, we have
something more general:

\begin{exercise}
\label{exe.ps4.0}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ and $\tau\in
S_{n}$. Show that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest
length of a path between $\sigma$ and $\tau$ on the (undirected) $n$-th right
Bruhat graph.
\end{exercise}

(Recall that the length of a path in a graph is defined as the number of edges
along this path.)

How many permutations in $S_{n}$ have a given length? The number is not easy
to compute directly; however, its generating function is nice. Namely,%
\[
\sum_{w\in S_{n}}q^{\ell\left(  w\right)  }=\left(  1+q\right)  \left(
1+q+q^{2}\right)  \cdots\left(  1+q+q^{2}+\cdots+q^{n-1}\right)
\]
(where $q$ is an indeterminate). See \cite[Corollary 1.3.13]{Stanley-EC1} for
a proof (but notice that Stanley denotes $S_{n}$ by $\mathfrak{S}_{n}$, and
$\ell\left(  w\right)  $ by $\operatorname*{inv}\left(  w\right)  $).

\begin{remark}
Much more can be said. Let me briefly mention (without proof) two other
related results.

We can ask ourselves in what different ways a permutation can be written as a
composition of $N$ permutations of the form $s_{k}$. For instance, the
permutation $w_{0}\in S_{3}$ which sends $1$, $2$ and $3$ to $3$, $2$ and $1$,
respectively (that is, $w_{0}=\left(  3,2,1\right)  $ in one-line notation)
can be written as a product of three $s_{k}$'s in the two forms%
\begin{equation}
w_{0}=s_{1}\circ s_{2}\circ s_{1},\ \ \ \ \ \ \ \ \ \ w_{0}=s_{2}\circ
s_{1}\circ s_{2}, \label{eq.tits.example}%
\end{equation}
but can also be written as a product of five $s_{k}$'s (e.g., as $w_{0}%
=s_{1}\circ s_{2}\circ s_{1}\circ s_{2}\circ s_{2}$) or seven $s_{k}$'s or
nine $s_{k}$'s, etc. Are the different representations of $w_{0}$ related?

Clearly, the two representations in (\ref{eq.tits.example}) are connected to
each other by the equality $s_{1}\circ s_{2}\circ s_{1}=s_{2}\circ s_{1}\circ
s_{2}$, which is a particular case of (\ref{eq.perms.braid3}). Also, the
representation $w_{0}=s_{1}\circ s_{2}\circ s_{1}\circ s_{2}\circ s_{2}$
reduces to $w_{0}=s_{1}\circ s_{2}\circ s_{1}$ by \textquotedblleft
cancelling\textquotedblright\ the two adjacent $s_{2}$'s at the end (recall
that $s_{i}\circ s_{i}=s_{i}^{2}=\operatorname*{id}$ for every $i$).

Interestingly, this generalizes. Let $n\in\mathbb{N}$ and $\sigma\in S_{n}$. A
\textit{reduced expression} for $\sigma$ will mean a representation of
$\sigma$ as a composition of $\ell\left(  \sigma\right)  $ permutations of the
form $s_{k}$. (As we know, less than $\ell\left(  \sigma\right)  $ such
permutations do not suffice; thus the name \textquotedblleft
reduced\textquotedblright.) Then, (one of the many versions of)
\textit{Matsumoto's theorem} states that any two reduced expressions of
$\sigma$ can be obtained from one another by a rewriting process, each step of
which is either an application of (\ref{eq.perms.braid3}) (i.e., you pick an
\textquotedblleft$s_{i}\circ s_{i+1}\circ s_{i}$\textquotedblright\ in the
expression and replace it by \textquotedblleft$s_{i+1}\circ s_{i}\circ
s_{i+1}$\textquotedblright, or vice versa) or an application of
(\ref{eq.perms.braid2}) (i.e., you pick an \textquotedblleft$s_{i}\circ s_{j}%
$\textquotedblright\ with $\left\vert i-j\right\vert >1$ and replace it by
\textquotedblleft$s_{j}\circ s_{i}$\textquotedblright, or vice versa). For
instance, for $n=4$ and $\sigma=\left(  4,3,1,2,5\right)  $ (in one-line
notation), the two reduced expressions $\sigma=s_{1}\circ s_{2}\circ
s_{3}\circ s_{1}\circ s_{2}$ and $\sigma=s_{2}\circ s_{3}\circ s_{1}\circ
s_{2}\circ s_{3}$ can be obtained from one another by the following rewriting
process:%
\begin{align*}
s_{1}\circ s_{2}\circ\underbrace{s_{3}\circ s_{1}}_{\substack{=s_{1}\circ
s_{3}\\\text{(by (\ref{eq.perms.braid2}))}}}\circ s_{2}  &  =\underbrace{s_{1}%
\circ s_{2}\circ s_{1}}_{\substack{=s_{2}\circ s_{1}\circ s_{2}\\\text{(by
(\ref{eq.perms.braid3}))}}}\circ s_{3}\circ s_{2}=s_{2}\circ s_{1}%
\circ\underbrace{s_{2}\circ s_{3}\circ s_{2}}_{\substack{=s_{3}\circ
s_{2}\circ s_{3}\\\text{(by (\ref{eq.perms.braid3}))}}}\\
&  =s_{2}\circ\underbrace{s_{1}\circ s_{3}}_{\substack{=s_{3}\circ
s_{1}\\\text{(by (\ref{eq.perms.braid2}))}}}\circ s_{2}\circ s_{3}=s_{2}\circ
s_{3}\circ s_{1}\circ s_{2}\circ s_{3}.
\end{align*}
See, e.g., Williamson's thesis \cite[Corollary 1.2.3]{William03} or Knutson's
notes \cite[\S 2.3]{Knutson} for a proof of this fact. (Knutson, instead of
saying that \textquotedblleft$\sigma=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}$ is a reduced expression for $\sigma$\textquotedblright, says that
\textquotedblleft$k_{1}k_{2}\cdots k_{p}$ is a reduced word for $\sigma
$\textquotedblright.)

Something subtler holds for \textquotedblleft non-reduced\textquotedblright%
\ expressions. Namely, if we have a representation of $\sigma$ as a
composition of some number of permutations of the form $s_{k}$ (not
necessarily $\ell\left(  \sigma\right)  $ of them), then we can transform it
into a reduced expression by a rewriting process which consists of
applications of (\ref{eq.perms.braid3}) and (\ref{eq.perms.braid2}) as before
and also of cancellation steps (i.e., picking an \textquotedblleft$s_{i}\circ
s_{i}$\textquotedblright\ in the expression and removing it). This follows
from \cite[Proposition (2.6)]{LLPT95}\footnotemark, and can also easily be
derived from \cite[Corollary 1.2.3 and Corollary 1.1.6]{William03}.

This all is stated and proven in greater generality in good books on Coxeter
groups, such as \cite{BjoBre05}. We won't need these results in the following,
but they are an example of what one can see if one looks at permutations closely.
\end{remark}

\footnotetext{What the authors of \cite{LLPT95} call a \textquotedblleft
presentation\textquotedblright\ of a permutation $\sigma\in S_{n}$ is a finite
list $\left(  s_{k_{1}},s_{k_{2}},\ldots,s_{k_{p}}\right)  $ of elements of
$\left\{  s_{1},s_{2},\ldots,s_{n-1}\right\}  $ satisfying $\sigma=s_{k_{1}%
}\circ s_{k_{2}}\circ\cdots\circ s_{k_{p}}$. What the authors of \cite{LLPT95}
call a \textquotedblleft minimal presentation\textquotedblright\ of $\sigma$
is what we call a reduced expression of $\sigma$.}

\subsection{More on signs of permutations}

In Section \ref{sect.sign}, we have defined the sign $\left(  -1\right)
^{\sigma}=\operatorname*{sign}\sigma=\operatorname*{sgn}\sigma$ of a
permutation $\sigma$. We recall the most important facts about it:

\begin{itemize}
\item We have $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }$ for every $\sigma\in S_{n}$. (This is the definition of
$\left(  -1\right)  ^{\sigma}$.) Thus, for every $\sigma\in S_{n}$, we have
$\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)
}\in\left\{  1,-1\right\}  $.

\item The permutation $\sigma\in S_{n}$ is said to be \textit{even} if
$\left(  -1\right)  ^{\sigma}=1$, and is said to be \textit{odd} if $\left(
-1\right)  ^{\sigma}=-1$.

\item The sign of the identity permutation $\operatorname*{id}\in S_{n}$ is
$\left(  -1\right)  ^{\operatorname*{id}}=1$.

\item For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the sign of the
permutation $s_{k}\in S_{n}$ is $\left(  -1\right)  ^{s_{k}}=-1$.

\item If $\sigma$ and $\tau$ are two permutations in $S_{n}$, then
\begin{equation}
\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(
-1\right)  ^{\tau}. \label{eq.sign.prod}%
\end{equation}


\item If $\sigma\in S_{n}$, then%
\begin{equation}
\left(  -1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\sigma}.
\label{eq.sign.inverse}%
\end{equation}

\end{itemize}

A simple consequence of the above facts is the following proposition:

\begin{proposition}
\label{prop.sign.prod-of-many}Let $n\in\mathbb{N}$ and $k\in\mathbb{N}$. Let
$\sigma_{1},\sigma_{2},\ldots,\sigma_{k}$ be $k$ permutations in $S_{n}$.
Then,%
\begin{equation}
\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{k}%
}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}%
\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{k}}. \label{eq.sign.prod-of-many}%
\end{equation}

\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.sign.prod-of-many}.]Straightforward induction
over $k$. The induction base (i.e., the case when $k=0$) follows from the fact
that $\left(  -1\right)  ^{\operatorname*{id}}=1$ (since the composition of
$0$ permutations is $\operatorname*{id}$). The induction step is easily done
using (\ref{eq.sign.prod}).
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.sign.prod-of-many}.]We will prove
(\ref{eq.sign.prod-of-many}) by induction over $k$:

\textit{Induction base:} Assume that $k=0$. Then, $\sigma_{1}\circ\sigma
_{2}\circ\cdots\circ\sigma_{k}=\sigma_{1}\circ\sigma_{2}\circ\cdots\circ
\sigma_{0}=\operatorname*{id}$ (since the composition of $0$ permutations is
$\operatorname*{id}$). Hence, $\left(  -1\right)  ^{\sigma_{1}\circ\sigma
_{2}\circ\cdots\circ\sigma_{k}}=\left(  -1\right)  ^{\operatorname*{id}}=1$.
On the other hand, from $k=0$, we obtain $\left(  -1\right)  ^{\sigma_{1}%
}\cdot\left(  -1\right)  ^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)
^{\sigma_{k}}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)
^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{0}}=1$ (since the
product of $0$ integers is $1$). Compared with $\left(  -1\right)
^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{k}}=1$, this yields
$\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{k}%
}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}%
\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{k}}$. Thus,
(\ref{eq.sign.prod-of-many}) is proven for $k=0$. The induction base is thus complete.

\textit{Induction step:} Let $K\in\mathbb{N}$. Assume that
(\ref{eq.sign.prod-of-many}) is proven for $k=K$. We need to prove
(\ref{eq.sign.prod-of-many}) for $k=K+1$.

Let $\sigma_{1},\sigma_{2},\ldots,\sigma_{K+1}$ be $K+1$ permutations in
$S_{n}$. We have assumed that (\ref{eq.sign.prod-of-many}) is proven for
$k=K$. Thus, we can apply (\ref{eq.sign.prod-of-many}) to $k=K$. We thus
conclude $\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ
\sigma_{K}}=\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)
^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{K}}$.

But $\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K+1}=\left(  \sigma
_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K}\right)  \circ\sigma_{K+1}$, so
that%
\begin{align*}
\left(  -1\right)  ^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K+1}}
&  =\left(  -1\right)  ^{\left(  \sigma_{1}\circ\sigma_{2}\circ\cdots
\circ\sigma_{K}\right)  \circ\sigma_{K+1}}=\underbrace{\left(  -1\right)
^{\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K}}}_{=\left(  -1\right)
^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}\cdot\cdots\cdot\left(
-1\right)  ^{\sigma_{K}}}\cdot\left(  -1\right)  ^{\sigma_{K+1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\sigma=\sigma_{1}\circ\sigma_{2}\circ\cdots\circ\sigma_{K}\text{ and }%
\tau=\sigma_{K+1}\right) \\
&  =\left(  \left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)
^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{K}}\right)
\cdot\left(  -1\right)  ^{\sigma_{K+1}}\\
&  =\left(  -1\right)  ^{\sigma_{1}}\cdot\left(  -1\right)  ^{\sigma_{2}}%
\cdot\cdots\cdot\left(  -1\right)  ^{\sigma_{K+1}}.
\end{align*}


Let us now forget that we fixed $\sigma_{1},\sigma_{2},\ldots,\sigma_{K+1}$.
We thus have shown that if $\sigma_{1},\sigma_{2},\ldots,\sigma_{K+1}$ are
$K+1$ permutations in $S_{n}$, then $\left(  -1\right)  ^{\sigma_{1}%
\circ\sigma_{2}\circ\cdots\circ\sigma_{K+1}}=\left(  -1\right)  ^{\sigma_{1}%
}\cdot\left(  -1\right)  ^{\sigma_{2}}\cdot\cdots\cdot\left(  -1\right)
^{\sigma_{K+1}}$. In other words, we have proven (\ref{eq.sign.prod-of-many})
for $k=K+1$. This completes the induction step, and so the induction proof of
(\ref{eq.sign.prod-of-many}) is complete. In other words, Proposition
\ref{prop.sign.prod-of-many} is proven.
\end{proof}
\end{verlong}

We state a few more properties, which should not be difficult by now:

\begin{definition}
\label{def.transpos}Let $n\in\mathbb{N}$. Let $i$ and $j$ be two distinct
elements of $\left\{  1,2,\ldots,n\right\}  $. We let $t_{i,j}$ be the
permutation in $S_{n}$ which switches $i$ with $j$ while leaving all other
elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged. Such a permutation is
called a \textit{transposition} (and is often denoted by $\left(  i,j\right)
$ in literature; but we prefer not to do so, since it is too similar to
one-line notation).
\end{definition}

Notice that the permutations $s_{1},s_{2},\ldots,s_{n-1}$ are transpositions
(namely, $s_{i}=t_{i,i+1}$ for every $i\in\left\{  1,2,\ldots,n-1\right\}  $),
but they are not the only transpositions (when $n\geq3$).

\begin{exercise}
\label{exe.ps4.1ab}Let $n\in\mathbb{N}$. Let $i$ and $j$ be two distinct
elements of $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} Find $\ell\left(  t_{i,j}\right)  $.

\textbf{(b)} Show that $\left(  -1\right)  ^{t_{i,j}}=-1$.
\end{exercise}

\begin{exercise}
\label{exe.ps4.1c}Let $n\in\mathbb{N}$. Let $w_{0}$ denote the permutation in
$S_{n}$ which sends each $k\in\left\{  1,2,\ldots,n\right\}  $ to $n+1-k$.
Compute $\ell\left(  w_{0}\right)  $ and $\left(  -1\right)  ^{w_{0}}$.
\end{exercise}

\begin{exercise}
\label{exe.ps4.2}Let $X$ be a finite set. We want to define the sign of any
permutation of $X$. (We have sketched this definition before (see
(\ref{eq.ps2.S(X).sign.teaser})), but now we shall do it in detail.)

Fix a bijection $\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. (Such a bijection always exists. Indeed, constructing such a
bijection is tantamount to writing down a list of all elements of $X$, with no
duplicates.) For every permutation $\sigma$ of $X$, set%
\[
\left(  -1\right)  _{\phi}^{\sigma}=\left(  -1\right)  ^{\phi\circ\sigma
\circ\phi^{-1}}.
\]
Here, the right hand side is well-defined because $\phi\circ\sigma\circ
\phi^{-1}$ is a permutation of $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} Prove that $\left(  -1\right)  _{\phi}^{\sigma}$ depends only on
the permutation $\sigma$ of $X$, but not on the bijection $\phi$. (In other
words, for a given $\sigma$, any two different choices of $\phi$ will lead to
the same $\left(  -1\right)  _{\phi}^{\sigma}$.)

This allows us to define the \textit{sign} of the permutation $\sigma$ to be
$\left(  -1\right)  _{\phi}^{\sigma}$ for any choice of bijection
$\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $. We denote this sign simply
by $\left(  -1\right)  ^{\sigma}$. (When $X=\left\{  1,2,\ldots,n\right\}  $,
then this sign is clearly the same as the sign $\left(  -1\right)  ^{\sigma}$
we defined before, because we can pick the bijection $\phi=\operatorname*{id}$.)

\textbf{(b)} Show that the permutation $\operatorname*{id}:X\rightarrow X$
satisfies $\left(  -1\right)  ^{\operatorname*{id}}=1$.

\textbf{(c)} Show that $\left(  -1\right)  ^{\sigma\circ\tau}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ for any two permutations
$\sigma$ and $\tau$ of $X$.
\end{exercise}

\begin{remark}
Let $n\in\mathbb{N}$. Recall that a \textit{transposition} in $S_{n}$ means a
permutation of the form $t_{i,j}$, where $i$ and $j$ are two distinct elements
of $\left\{  1,2,\ldots,n\right\}  $. Therefore, if $\tau$ is a transposition
in $S_{n}$, then
\begin{equation}
\left(  -1\right)  ^{\tau}=-1. \label{eq.sign.transposition}%
\end{equation}
(In fact, if $\tau$ is a transposition in $S_{n}$, then $\tau$ can be written
in the form $\tau=t_{i,j}$ for two distinct elements $i$ and $j$ of $\left\{
1,2,\ldots,n\right\}  $; and therefore, for these two elements $i$ and $j$, we
have $\left(  -1\right)  ^{\tau}=\left(  -1\right)  ^{t_{i,j}}=-1$ (according
to Exercise \ref{exe.ps4.1ab} \textbf{(b)}). This proves
(\ref{eq.sign.transposition}).)

Now, let $\sigma\in S_{n}$ be any permutation. Assume that $\sigma$ is written
in the form $\sigma=\tau_{1}\circ\tau_{2}\circ\cdots\circ\tau_{k}$ for some
transpositions $\tau_{1},\tau_{2},\ldots,\tau_{k}$ in $S_{n}$. Then,%
\begin{align}
\left(  -1\right)  ^{\sigma}  &  =\left(  -1\right)  ^{\tau_{1}\circ\tau
_{2}\circ\cdots\circ\tau_{k}}=\underbrace{\left(  -1\right)  ^{\tau_{1}}%
}_{\substack{=-1\\\text{(by (\ref{eq.sign.transposition}))}}}\cdot
\underbrace{\left(  -1\right)  ^{\tau_{2}}}_{\substack{=-1\\\text{(by
(\ref{eq.sign.transposition}))}}}\cdot\cdots\cdot\underbrace{\left(
-1\right)  ^{\tau_{k}}}_{\substack{=-1\\\text{(by (\ref{eq.sign.transposition}%
))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod-of-many}), applied
to }\sigma_{i}=\tau_{i}\right) \nonumber\\
&  =\underbrace{\left(  -1\right)  \cdot\left(  -1\right)  \cdot\cdots
\cdot\left(  -1\right)  }_{k\text{ factors}}=\left(  -1\right)  ^{k}.
\label{eq.sign.prod-of-transpositions}%
\end{align}
Since many permutations can be written as products of transpositions in a
simple way, this formula gives a useful method for computing signs.
\end{remark}

\begin{remark}
Let $n\in\mathbb{N}$. It is not hard to prove that
\begin{equation}
\left(  -1\right)  ^{\sigma}=\prod_{1\leq i<j\leq n}\dfrac{\sigma\left(
i\right)  -\sigma\left(  j\right)  }{i-j}\ \ \ \ \ \ \ \ \ \ \text{for every
}\sigma\in S_{n}. \label{eq.sign.pseudoexplicit}%
\end{equation}
(Of course, it is no easier to compute $\left(  -1\right)  ^{\sigma}$ using
this seemingly explicit formula than by counting inversions.)

We shall prove (\ref{eq.sign.pseudoexplicit}) in Exercise
\ref{exe.perm.sign.pseudoexplicit} \textbf{(c)}.
\end{remark}

\begin{remark}
The sign of a permutation is also called its \textit{signum} or its
\textit{signature}. Different authors define the sign of a permutation
$\sigma$ in different ways. Some (e.g., Hefferon in \cite[Chapter Four,
Definition 4.7]{Hefferon}) define it as we do; others (e.g., Conrad in
\cite{Conrad} or Hoffman and Kunze in \cite[p. 152]{HoffmanKunze}) define it
using (\ref{eq.sign.prod-of-transpositions}); yet others define it using
something called the \textit{cycle decomposition} of a permutation; some even
define it using (\ref{eq.sign.pseudoexplicit}), or using a similar ratio of
two polynomials. However, it is not hard to check that all of these
definitions are equivalent. (We already know that the first two of them are equivalent.)
\end{remark}

\begin{exercise}
\label{exe.perm.sign.pseudoexplicit}Let $n\in\mathbb{N}$. Let $\sigma\in
S_{n}$.

\textbf{(a)} If $x_{1},x_{2},\ldots,x_{n}$ are $n$ elements of $\mathbb{C}$,
then prove that%
\[
\prod_{1\leq i<j\leq n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(
j\right)  }\right)  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(b)} More generally: For every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$, let $a_{\left(  i,j\right)  }$ be an element of
$\mathbb{C}$. Assume that%
\begin{equation}
a_{\left(  j,i\right)  }=-a_{\left(  i,j\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}. \label{eq.exe.perm.sign.pseudoexplicit.b.skew}%
\end{equation}
Prove that%
\[
\prod_{1\leq i<j\leq n}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }=\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}a_{\left(  i,j\right)  }.
\]


\textbf{(c)} Prove (\ref{eq.sign.pseudoexplicit}).

\textbf{(d)} Use Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} to
give a new solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}.
\end{exercise}

\begin{exercise}
\label{exe.Ialbe}Let $n\in\mathbb{N}$. Let $I$ be a subset of $\left\{
1,2,\ldots,n\right\}  $. Let $k=\left\vert I\right\vert $. Let $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ be the list of all elements of $I$ in
increasing order (with no repetitions). Let $\left(  b_{1},b_{2}%
,\ldots,b_{n-k}\right)  $ be the list of all elements of $\left\{
1,2,\ldots,n\right\}  \setminus I$ in increasing order (with no repetitions).
Let $\alpha\in S_{k}$ and $\beta\in S_{n-k}$. Prove the following:

\textbf{(a)} There exists a unique $\sigma\in S_{n}$ satisfying%
\[
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\]
Denote this $\sigma$ by $\sigma_{I,\alpha,\beta}$.

\textbf{(b)} Let $\sum I$ denote the sum of all elements of $I$. (Thus, $\sum
I=\sum_{i\in I}i$.) We have%
\[
\ell\left(  \sigma_{I,\alpha,\beta}\right)  =\ell\left(  \alpha\right)
+\ell\left(  \beta\right)  +\sum I-\left(  1+2+\cdots+k\right)
\]
and%
\[
\left(  -1\right)  ^{\sigma_{I,\alpha,\beta}}=\left(  -1\right)  ^{\alpha
}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum I-\left(
1+2+\cdots+k\right)  }.
\]


\textbf{(c)} Forget that we fixed $\alpha$ and $\beta$. We thus have defined
an element $\sigma_{I,\alpha,\beta}\in S_{n}$ for every $\alpha\in S_{k}$ and
every $\beta\in S_{n-k}$. The map%
\begin{align*}
S_{k}\times S_{n-k}  &  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =I\right\}  ,\\
\left(  \alpha,\beta\right)   &  \mapsto\sigma_{I,\alpha,\beta}%
\end{align*}
is well-defined and a bijection.
\end{exercise}

\subsection{Cycles}

Next, we shall discuss another specific class of permutations: the
\textit{cycles}.

\begin{definition}
\label{def.perm.cycles}Let $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $.

Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $i_{1},i_{2},\ldots,i_{k}$ be
$k$ distinct elements of $\left[  n\right]  $. We define $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ to be the permutation in $S_{n}$ which
sends $i_{1},i_{2},\ldots,i_{k}$ to $i_{2},i_{3},\ldots,i_{k},i_{1}$,
respectively, while leaving all other elements of $\left[  n\right]  $ fixed.
In other words, we define $\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}$ to be the permutation in $S_{n}$ given by%
\[
\left(
\begin{array}
[c]{r}%
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  p\right)  =%
\begin{cases}
i_{j+1}, & \text{if }p=i_{j}\text{ for some }j\in\left\{  1,2,\ldots
,k\right\}  ;\\
p, & \text{otherwise}%
\end{cases}
\\
\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left[  n\right]
\end{array}
\right)  ,
\]
where $i_{k+1}$ means $i_{1}$.

(Again, the notation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$
conceals the parameter $n$, which will hopefully not cause any confusion.)

A permutation of the form $\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}$ is said to be a $k$\textit{-cycle} (or sometimes just a
\textit{cycle}, or a \textit{cyclic permutation}). Of course, the name stems
from the fact that it \textquotedblleft cycles\textquotedblright\ through the
elements $i_{1},i_{2},\ldots,i_{k}$ (by sending each of them to the next one
and the last one back to the first) and leaves all other elements unchanged.
\end{definition}

\begin{example}
\label{exa.perm.cycles}Let $n\in\mathbb{N}$. The following facts follow easily
from Definition \ref{def.perm.cycles}:

\textbf{(a)} For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have
$\operatorname*{cyc}\nolimits_{i}=\operatorname*{id}$. In other words, any
$1$-cycle is the identity permutation $\operatorname*{id}$.

\textbf{(b)} If $i$ and $j$ are two distinct elements of $\left\{
1,2,\ldots,n\right\}  $, then $\operatorname*{cyc}\nolimits_{i,j}=t_{i,j}$.
(See Definition \ref{def.transpos} for the definition of $t_{i,j}$.)

\textbf{(c)} If $k\in\left\{  1,2,\ldots,n-1\right\}  $, then
$\operatorname*{cyc}\nolimits_{k,k+1}=s_{k}$.

\textbf{(d)} If $n=5$, then $\operatorname*{cyc}\nolimits_{2,5,3}$ is the
permutation which sends $1$ to $1$, $2$ to $5$, $3$ to $2$, $4$ to $4$, and
$5$ to $3$. (In other words, it is the permutation which is $\left(
1,5,2,4,3\right)  $ in one-line notation.)

\textbf{(e)} If $k\in\left\{  1,2,\ldots,n\right\}  $, and if $i_{1}%
,i_{2},\ldots,i_{k}$ are $k$ pairwise distinct elements of $\left[  n\right]
$, then%
\[
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}=\operatorname*{cyc}%
\nolimits_{i_{2},i_{3},\ldots,i_{k},i_{1}}=\operatorname*{cyc}\nolimits_{i_{3}%
,i_{4},\ldots,i_{k},i_{1},i_{2}}=\cdots=\operatorname*{cyc}\nolimits_{i_{k}%
,i_{1},i_{2},\ldots,i_{k-1}}.
\]
(In less formal words: The $k$-cycle $\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}$ does not change when we cyclically rotate the list
$\left(  i_{1},i_{2},\ldots,i_{k}\right)  $.)
\end{example}

\begin{remark}
What we called $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ in
Definition \ref{def.perm.cycles} is commonly denoted by $\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  $ in the literature. But this latter notation
$\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ would clash with one-line notation
for permutations (the cycle $\operatorname*{cyc}\nolimits_{1,2,3}\in S_{3}$ is
not the same as the permutation which is $\left(  1,2,3\right)  $ in one-line
notation) and also with the standard notation for $k$-tuples. This is why we
prefer to use the notation $\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}$. (That said, we are not going to use $k$-cycles very often.)
\end{remark}

The following exercise gathers some properties of cycles. Parts \textbf{(a)}
and \textbf{(d)} and, to a lesser extent, \textbf{(b)} are fairly important
and you should make sure you know how to solve them. The significantly more
difficult part \textbf{(c)} is more of a curiosity with an interesting proof
(I have not found an application of it so far; skip it if you do not want to
spend time on what is essentially a contest problem).

\begin{exercise}
\label{exe.perm.cycles}Let $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Let $k\in\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} For every $\sigma\in S_{n}$ and every $k$ distinct elements
$i_{1},i_{2},\ldots,i_{k}$ of $\left[  n\right]  $, prove that
\[
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}=\operatorname*{cyc}\nolimits_{\sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  }.
\]


\textbf{(b)} For every $p\in\left\{  0,1,\ldots,n-k\right\}  $, prove that%
\[
\ell\left(  \operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\right)  =k-1.
\]


\textbf{(c)} For every $k$ distinct elements $i_{1},i_{2},\ldots,i_{k}$ of
$\left[  n\right]  $, prove that
\[
\ell\left(  \operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\right)
\geq k-1.
\]


\textbf{(d)} For every $k$ distinct elements $i_{1},i_{2},\ldots,i_{k}$ of
$\left[  n\right]  $, prove that
\[
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
}=\left(  -1\right)  ^{k-1}.
\]

\end{exercise}

\begin{remark}
Exercise \ref{exe.perm.cycles} \textbf{(d)} shows that every $k$-cycle in
$S_{n}$ has sign $\left(  -1\right)  ^{k-1}$. However, the length of a
$k$-cycle need not be $k-1$. Exercise \ref{exe.perm.cycles} \textbf{(c)} shows
that this length is always $\geq k-1$, but it can take other values as well.
For instance, in $S_{4}$, the length of the $3$-cycle $\operatorname*{cyc}%
\nolimits_{1,4,3}$ is $4$. (Another example are the transpositions $t_{i,j}$
from Definition \ref{def.transpos}; these are $2$-cycles but can have length
$>1$.)

I don't know a simple way to describe when equality holds in Exercise
\ref{exe.perm.cycles} \textbf{(c)}. It holds whenever $i_{1},i_{2}%
,\ldots,i_{k}$ are consecutive integers (due to Exercise \ref{exe.perm.cycles}
\textbf{(b)}), but also in some other cases; for example, the $4$-cycle
$\operatorname*{cyc}\nolimits_{1,3,4,2}$ in $S_{4}$ has length $3$.
\end{remark}

\begin{remark}
\label{rmk.perm.cycles.decompose}The main reason why cycles are useful is
that, essentially, every permutation can be \textquotedblleft
decomposed\textquotedblright\ into cycles. We shall not use this fact, but
since it is generally important, let us briefly explain what it means. (You
will probably learn more about it in any standard course on abstract algebra.)

Fix $n\in\mathbb{N}$. Let $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}
$. Two cycles $\alpha$ and $\beta$ in $S_{n}$ are said to be \textit{disjoint}
if they can be written as $\alpha=\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}$ and $\beta=\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{\ell}}$ for $k+\ell$ distinct elements $i_{1},i_{2}%
,\ldots,i_{k},j_{1},j_{2},\ldots,j_{\ell}$ of $\left[  n\right]  $. For
example, the two cycles $\operatorname*{cyc}\nolimits_{1,3}$ and
$\operatorname*{cyc}\nolimits_{2,6,7}$ in $S_{8}$ are disjoint, but the two
cycles $\operatorname*{cyc}\nolimits_{1,4}$ and $\operatorname*{cyc}%
\nolimits_{2,4}$ are not. It is easy to see that any two disjoint cycles
$\alpha$ and $\beta$ commute (i.e., satisfy $\alpha\circ\beta=\beta\circ
\alpha$). Therefore, when you see a composition $\alpha_{1}\circ\alpha
_{2}\circ\cdots\circ\alpha_{p}$ of several pairwise disjoint cycles, you can
reorder its factors arbitrarily without changing the result (for example,
$\alpha_{3}\circ\alpha_{1}\circ\alpha_{4}\circ\alpha_{2}=\alpha_{1}\circ
\alpha_{2}\circ\alpha_{3}\circ\alpha_{4}$ if $p=4$).

Now, the fact I am talking about says the following: Every permutation in
$S_{n}$ can be written as a composition of several pairwise disjoint cycles.
For example, let $n=9$, and let $\sigma\in S_{9}$ be the permutation which is
written $\left(  4,6,1,3,5,2,9,8,7\right)  $ in one-line notation (i.e., we
have $\sigma\left(  1\right)  =4$, $\sigma\left(  2\right)  =6$, etc.). Then,
$\sigma$ can be written as a composition of several pairwise disjoint cycles
as follows:%
\begin{equation}
\sigma=\operatorname*{cyc}\nolimits_{1,4,3}\circ\operatorname*{cyc}%
\nolimits_{7,9}\circ\operatorname*{cyc}\nolimits_{2,6}.
\label{eq.rmk.perm.cycles.decompose.1}%
\end{equation}
Indeed, here is how such a decomposition can be found: Let us draw a directed
graph whose vertices are $1,2,\ldots,n$, and which has an arc $i\rightarrow
\sigma\left(  i\right)  $ for every $i\in\left[  n\right]  $. (Thus, it has
$n$ arcs altogether; some of them can be loops.) For our permutation
$\sigma\in S_{9}$, this graph looks as follows:
\begin{equation}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm, thick,main node/.style={circle,fill=blue!20,draw}] \node[main node] (1) {1}; \node[main node] (3) [below right of=1] {3}; \node[main node] (4) [above right of=1] {4}; \node[main node] (2) [above left of=1] {2}; \node[main node] (6) [below left of=1] {6}; \node[main node] (5) [below right of=4] {5}; \node[main node] (7) [above right of=5] {7}; \node[main node] (9) [below right of=5] {9}; \node[main node] (8) [above right of=9] {8}; \path[every node/.style={font=\sffamily\small}] (1) edge (4) (2) edge [bend right] (6) (3) edge (1) (4) edge (3) (5) edge [loop left] (5) (6) edge [bend right] (2) (7) edge [bend right] (9) (8) edge [loop left] (8) (9) edge [bend right] (7); \end{tikzpicture}.
\label{eq.rmk.perm.cycles.decompose.2}%
\end{equation}
Obviously, at each vertex $i$ of this graph, exactly one arc begins (namely,
the arc $i\rightarrow\sigma\left(  i\right)  $). Moreover, since $\sigma$ is
invertible, it is also clear that at each vertex $i$ of this graph, exactly
one arc ends (namely, the arc $\sigma^{-1}\left(  i\right)  \rightarrow i$).
Due to the way we constructed this graph, it is clear that it completely
describes our permutation $\sigma$: Namely, if we want to find $\sigma\left(
i\right)  $ for a given $i\in\left[  n\right]  $, we should just locate the
vertex $i$ on the graph, and follow the arc that begins at this vertex; the
endpoint of this arc will be $\sigma\left(  i\right)  $.

Now, a look at this graph reveals five directed cycles (in the sense of
\textquotedblleft paths which end at the same vertex at which they
begin\textquotedblright, not yet in the sense of \textquotedblleft cyclic
permutations\textquotedblright). The first one passes through the vertices $2$
and $6$; the second passes through the vertices $3$, $1$ and $4$; the third,
through the vertex $5$ (it is what is called a \textquotedblleft trivial
cycle\textquotedblright), and so on. To each of these cycles we can assign a
cyclic permutation in $S_{n}$: namely, if the cycle passes through the
vertices $i_{1},i_{2},\ldots,i_{k}$ (in this order, and with no repetitions),
then we assign to it the cyclic permutation $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\in S_{n}$. The cyclic permutations
assigned to all five directed cycles are pairwise disjoint, and their
composition is%
\[
\operatorname*{cyc}\nolimits_{2,6}\circ\operatorname*{cyc}\nolimits_{3,1,4}%
\circ\operatorname*{cyc}\nolimits_{5}\circ\operatorname*{cyc}\nolimits_{7,9}%
\circ\operatorname*{cyc}\nolimits_{8}.
\]
But this composition must be $\sigma$ (because if we apply this composition to
an element $i\in\left[  n\right]  $, then we obtain the \textquotedblleft next
vertex after $i$\textquotedblright\ on the directed cycle which passes through
$i$; but due to how we constructed our graph, this \textquotedblleft next
vertex\textquotedblright\ will be precisely $\sigma\left(  i\right)  $).
Hence, we have%
\begin{equation}
\sigma=\operatorname*{cyc}\nolimits_{2,6}\circ\operatorname*{cyc}%
\nolimits_{3,1,4}\circ\operatorname*{cyc}\nolimits_{5}\circ\operatorname*{cyc}%
\nolimits_{7,9}\circ\operatorname*{cyc}\nolimits_{8}.
\label{eq.rmk.perm.cycles.decompose.3}%
\end{equation}
Thus, we have found a way to write $\sigma$ as a composition of several
pairwise disjoint cycles. We can rewrite (and even simplify) this
representation a bit: Namely, we can simplify
(\ref{eq.rmk.perm.cycles.decompose.3}) by removing the factors
$\operatorname*{cyc}\nolimits_{5}$ and $\operatorname*{cyc}\nolimits_{8}$
(because both of these factors equal $\operatorname*{id}$); thus we obtain
$\sigma=\operatorname*{cyc}\nolimits_{2,6}\circ\operatorname*{cyc}%
\nolimits_{3,1,4}\circ\operatorname*{cyc}\nolimits_{7,9}$. We can furthermore
switch $\operatorname*{cyc}\nolimits_{2,6}$ with $\operatorname*{cyc}%
\nolimits_{3,1,4}$ (since disjoint cycles commute), therefore obtaining
$\sigma=\operatorname*{cyc}\nolimits_{3,1,4}\circ\operatorname*{cyc}%
\nolimits_{2,6}\circ\operatorname*{cyc}\nolimits_{7,9}$. Next, we can switch
$\operatorname*{cyc}\nolimits_{2,6}$ with $\operatorname*{cyc}\nolimits_{7,9}%
$, obtaining $\sigma=\operatorname*{cyc}\nolimits_{3,1,4}\circ
\operatorname*{cyc}\nolimits_{7,9}\circ\operatorname*{cyc}\nolimits_{2,6}$.
Finally, we can rewrite $\operatorname*{cyc}\nolimits_{3,1,4}$ as
$\operatorname*{cyc}\nolimits_{1,4,3}$, and we obtain
(\ref{eq.rmk.perm.cycles.decompose.1}).

In general, for every $n\in\mathbb{N}$, every permutation $\sigma\in S_{n}$
can be represented as a composition of several pairwise disjoint cycles (which
can be found by drawing a directed graph as in our example above). This
representation is not literally unique, because we can modify it by:

\begin{itemize}
\item adding or removing trivial factors (i.e., factors of the form
$\operatorname*{cyc}\nolimits_{i}=\operatorname*{id}$);

\item switching different cycles;

\item rewriting $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ as
$\operatorname*{cyc}\nolimits_{i_{2},i_{3},\ldots,i_{k},i_{1}}$.
\end{itemize}

However, it is unique \textbf{up to these modifications}; in other words, any
two representations of $\sigma$ as a composition of several pairwise disjoint
cycles can be transformed into one another by such modifications.

The proofs of all these statements are fairly easy. (One does have to check
certain things, e.g., that the directed graph really consists of disjoint
directed cycles.)

Representing a permutation $\sigma\in S_{n}$ as a composition of several
pairwise disjoint cycles can be done very quickly, and thus gives a quick way
to find $\left(  -1\right)  ^{\sigma}$ (because Exercise \ref{exe.perm.cycles}
\textbf{(d)} tells us how to find the sign of a $k$-cycle). This is
significantly faster than counting inversions of $\sigma$.
\end{remark}

\subsection{Additional exercises}

Permutations and symmetric groups are a staple of combinatorics; there are
countless results involving them. For an example, B\'{o}na's book
\cite{Bona12}, as well as significant parts of Stanley's \cite{Stanley-EC1}
and \cite{Stanley-EC2} are devoted to them. In this section, I shall only give
a haphazard selection of exercises, which are not relevant to the rest of
these notes (thus can be skipped at will). I am not planning to provide
solutions for all of them.

\begin{addexercise}
\label{exeadd.perm.order}Let $n\in\mathbb{N}$. Let $d=\operatorname{lcm}%
\left(  1,2,\ldots,n\right)  $.

\textbf{(a)} Show that $\pi^{d}=\operatorname*{id}$ for every $\pi\in S_{n}$.

\textbf{(b)} Let $k$ be an integer such that every $\pi\in S_{n}$ satisfies
$\pi^{k}=\operatorname*{id}$. Show that $d\mid k$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.perm.sigmacrosstau}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $\sigma\in S_{n}$ and $\tau\in S_{m}$. We define a permutation
$\sigma\times\tau$ of the set $\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $ by setting%
\[
\left(  \sigma\times\tau\right)  \left(  a,b\right)  =\left(  \sigma\left(
a\right)  ,\tau\left(  b\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}\left(  a,b\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  .
\]


\textbf{(a)} Prove that $\sigma\times\tau$ is a well-defined permutation.

\textbf{(b)} Prove that $\sigma\times\tau=\left(  \sigma\times
\operatorname*{id}\right)  \circ\left(  \operatorname*{id}\times\tau\right)  $.

\textbf{(c)} According to Exercise \ref{exe.ps4.2} (applied to $X=\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $), the permutation
$\sigma\times\tau$ has a well-defined sign $\left(  -1\right)  ^{\sigma
\times\tau}$. Prove that $\left(  -1\right)  ^{\sigma\times\tau}=\left(
\left(  -1\right)  ^{\sigma}\right)  ^{m}\left(  \left(  -1\right)  ^{\tau
}\right)  ^{n}$.
\end{addexercise}

The next two additional exercises concern the inversions of a permutation.
They use the following definition:

\begin{definition}
\label{def.Inv}Let $n\in\mathbb{N}$. For every $\sigma\in S_{n}$, we let
$\operatorname*{Inv}\sigma$ denote the set of all inversions of $\sigma$.
\end{definition}

Exercise \ref{exe.ps2.2.5} \textbf{(c)} shows that any $n\in\mathbb{N}$ and
any two permutations $\sigma$ and $\tau$ in $S_{n}$ satisfy the inequality
$\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  $. In the following exercise, we will see when this
inequality becomes an equality:

\begin{addexercise}
\label{exeadd.perm.Inv.sub}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ and
$\tau\in S_{n}$.

\textbf{(a)} Prove that $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $.

\textbf{(b)} Prove that $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if
$\operatorname*{Inv}\left(  \sigma^{-1}\right)  \subseteq\operatorname*{Inv}%
\left(  \tau^{-1}\circ\sigma^{-1}\right)  $.

\textbf{(c)} Prove that $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}%
\tau$ holds if and only if $\ell\left(  \tau\right)  =\ell\left(  \tau
\circ\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $.

\textbf{(d)} Prove that if $\operatorname*{Inv}\sigma=\operatorname*{Inv}\tau
$, then $\sigma=\tau$.

\textbf{(e)} Prove that $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if $\left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  =\varnothing$.
\end{addexercise}

Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(d)} shows that if two
permutations in $S_{n}$ have the same set of inversions, then they are equal.
In other words, a permutation in $S_{n}$ is uniquely determined by its set of
inversions. The next additional exercise shows what set of inversions a
permutation can have:

\begin{addexercise}
\label{exeadd.perm.invset}Let $n\in\mathbb{N}$. Let $G=\left\{  \left(
i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  $.

A subset $U$ of $G$ is said to be \textit{transitive} if every $a,b,c\in
\left\{  1,2,\ldots,n\right\}  $ satisfying $\left(  a,b\right)  \in U$ and
$\left(  b,c\right)  \in U$ also satisfy $\left(  a,c\right)  \in U$.

A subset $U$ of $G$ is said to be \textit{inversive} if there exists a
$\sigma\in S_{n}$ such that $U=\operatorname*{Inv}\sigma$.

Let $U$ be a subset of $G$. Prove that $U$ is inversive if and only if both
$U$ and $G\setminus U$ are transitive.
\end{addexercise}

\section{\label{chp.det}An introduction to determinants}

In this chapter, we will define and study determinants in a combinatorial way
(in the spirit of Hefferon's book \cite{Hefferon}, Gill Williamson's notes
\cite[Chapter 3]{Gill}, Laue's notes \cite{Laue-det} and Zeilberger's paper
\cite{Zeilbe}\footnote{My notes differ from these sources in the following:
\par
\begin{itemize}
\item Hefferon's book \cite{Hefferon} is an introductory textbook for a first
course in Linear Algebra, and so treats rather little of the theory of
determinants (noticeably less than what we do). It is, however, a good
introduction into the \textquotedblleft other part\textquotedblright\ of
linear algebra (i.e., the theory of vector spaces and linear maps), and puts
determinants into the context of that other part, which makes some of their
properties appear less mysterious. (Like many introductory textbooks, it only
discusses matrices over fields, not over commutative rings; it also uses more
handwaving in the proofs.)
\par
\item Zeilberger's paper \cite{Zeilbe} mostly proves advanced results (apart
from its Section 5, which proves our Theorem \ref{thm.det(AB)}). I would
recommend reading it after reading this chapter.
\par
\item Laue's notes \cite{Laue-det} are a brief introduction to determinants
that prove the main results in just 14 pages (although at the cost of terser
writing and stronger assumptions on the reader's preknowledge). If you read
these notes, make sure to pay attention to the \textquotedblleft Prerequisites
and some Terminology\textquotedblright\ section, as it explains the (unusual)
notations used in these notes.
\par
\item Gill Williamson's \cite[Chapter 3]{Gill} probably comes the closest to
what I am doing below (and is highly recommended, not least because it goes
much further into various interesting directions!). My notes are more
elementary and more detailed in what they do.
\end{itemize}
}). Nowadays, students usually learn about determinants in the context of
linear algebra, after having made the acquaintance of vector spaces, matrices,
linear transformations, Gaussian elimination etc.; this approach to
determinants (which I like to call the \textquotedblleft linear-algebraic
approach\textquotedblright) has certain advantages and certain disadvantages
compared to our combinatorial approach\footnote{Its main advantage is that it
gives more motivation and context. However, the other (combinatorial) approach
requires less preknowledge and involves less technical subtleties (for
example, it defines the determinant directly by an explicit formula, while the
linear-algebraic approach defines it implicitly by a list of conditions which
happen to determine it uniquely), which is why I have chosen it. (Of course,
it helped that the combinatorial approach is, well, combinatorial.)}.

We shall study determinants of matrices over \textit{commutative
rings}.\footnote{This is a rather general setup, which includes determinants
of matrices with real entries, of matrices with complex entries, of matrices
with polynomial entries, and many other situations. One benefit of working
combinatorially is that studying determinants in this general setup is no more
difficult than studying them in more restricted settings.} First, let us
define what these words (\textquotedblleft commutative ring\textquotedblright,
\textquotedblleft matrix\textquotedblright\ and \textquotedblleft
determinant\textquotedblright) mean.

\subsection{\label{sect.commring}Commutative rings}

\begin{definition}
If $\mathbb{K}$ is a set, then a \textit{binary operation} on $\mathbb{K}$
means a map from $\mathbb{K}\times\mathbb{K}$ to $\mathbb{K}$. (In other
words, it means a function which takes two elements of $\mathbb{K}$ as input,
and returns an element of $\mathbb{K}$ as output.) For instance, the map from
$\mathbb{Z}\times\mathbb{Z}$ to $\mathbb{Z}$ which sends every pair $\left(
a,b\right)  \in\mathbb{Z}\times\mathbb{Z}$ to $3a-b$ is a binary operation on
$\mathbb{Z}$.

Sometimes, a binary operation $f$ on a set $\mathbb{K}$ will be
\textit{written infix}. This means that the image of $\left(  a,b\right)
\in\mathbb{K}\times\mathbb{K}$ under $f$ will be denoted by $afb$ instead of
$f\left(  a,b\right)  $. For instance, the binary operation $+$ on the set
$\mathbb{Z}$ (which sends a pair $\left(  a,b\right)  $ of integers to their
sum $a+b$) is commonly written infix, because one writes $a+b$ and not
$+\left(  a,b\right)  $ for the sum of $a$ and $b$.
\end{definition}

\begin{definition}
\label{def.commring}A \textit{commutative ring} means a set $\mathbb{K}$
endowed with

\begin{itemize}
\item two binary operations called \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright, and denoted by $+$ and $\cdot$,
respectively, and both written infix\footnotemark, and

\item two elements called $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$
\end{itemize}

such that the following axioms are satisfied:

\begin{itemize}
\item \textit{Commutativity of addition:} We have $a+b=b+a$ for all
$a\in\mathbb{K}$ and $b\in\mathbb{K}$.

\item \textit{Commutativity of multiplication:} We have $ab=ba$ for all
$a\in\mathbb{K}$ and $b\in\mathbb{K}$. Here and in the following, $ab$ is
shorthand for $a\cdot b$ (as is usual for products of numbers).

\item \textit{Associativity of addition:} We have $a+\left(  b+c\right)
=\left(  a+b\right)  +c$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$ and
$c\in\mathbb{K}$.

\item \textit{Associativity of multiplication:} We have $a\left(  bc\right)
=\left(  ab\right)  c$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$ and
$c\in\mathbb{K}$.

\item \textit{Neutrality of }$0$\textit{:} We have $a+0_{\mathbb{K}%
}=0_{\mathbb{K}}+a=a$ for all $a\in\mathbb{K}$.

\item \textit{Existence of additive inverses:} For every $a\in\mathbb{K}$,
there exists an element $a^{\prime}\in\mathbb{K}$ such that $a+a^{\prime
}=a^{\prime}+a=0_{\mathbb{K}}$. This $a^{\prime}$ is commonly denoted by $-a$
and called the \textit{additive inverse} of $a$. (It is easy to check that it
is unique.)

\item \textit{Unitality (a.k.a. neutrality of }$1$\textit{):} We have
$1_{\mathbb{K}}a=a1_{\mathbb{K}}=a$ for all $a\in\mathbb{K}$.

\item \textit{Annihilation:} We have $0_{\mathbb{K}}a=a0_{\mathbb{K}%
}=0_{\mathbb{K}}$ for all $a\in\mathbb{K}$.

\item \textit{Distributivity:} We have $a\left(  b+c\right)  =ab+ac$ and
$\left(  a+b\right)  c=ac+bc$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$ and
$c\in\mathbb{K}$.
\end{itemize}
\end{definition}

\footnotetext{i.e., we write $a+b$ for the image of $\left(  a,b\right)
\in\mathbb{K}\times\mathbb{K}$ under the binary operation called
\textquotedblleft addition\textquotedblright, and we write $a\cdot b$ for the
image of $\left(  a,b\right)  \in\mathbb{K}\times\mathbb{K}$ under the binary
operation called \textquotedblleft multiplication\textquotedblright}(Some of
these axioms are redundant, in the sense that they can be derived from others.
For instance, the equality $\left(  a+b\right)  c=ac+bc$ can be derived from
the axiom $a\left(  b+c\right)  =ab+ac$ using commutativity of multiplication.
Also, annihilation follows from the other axioms\footnote{In fact, let
$a\in\mathbb{K}$. Distributivity yields $\left(  0_{\mathbb{K}}+0_{\mathbb{K}%
}\right)  a=0_{\mathbb{K}}a+0_{\mathbb{K}}a$, so that $0_{\mathbb{K}%
}a+0_{\mathbb{K}}a=\underbrace{\left(  0_{\mathbb{K}}+0_{\mathbb{K}}\right)
}_{\substack{=0_{\mathbb{K}}\\\text{(by neutrality of }0_{\mathbb{K}}\text{)}%
}}a=0_{\mathbb{K}}a$. Adding $-\left(  0_{\mathbb{K}}a\right)  $ on the left,
we obtain $-\left(  0_{\mathbb{K}}a\right)  +\left(  0_{\mathbb{K}%
}a+0_{\mathbb{K}}a\right)  =-\left(  0_{\mathbb{K}}a\right)  +0_{\mathbb{K}}%
a$. But $-\left(  0_{\mathbb{K}}a\right)  +0_{\mathbb{K}}a=0_{\mathbb{K}}$ (by
the definition of $-\left(  0_{\mathbb{K}}a\right)  $), and associativity of
addition shows that $-\left(  0_{\mathbb{K}}a\right)  +\left(  0_{\mathbb{K}%
}a+0_{\mathbb{K}}a\right)  =\underbrace{\left(  -\left(  0_{\mathbb{K}%
}a\right)  +0_{\mathbb{K}}a\right)  }_{=0_{\mathbb{K}}}+0_{\mathbb{K}%
}a=0_{\mathbb{K}}+0_{\mathbb{K}}a=0_{\mathbb{K}}a$ (by neutrality of
$0_{\mathbb{K}}$), so that $0_{\mathbb{K}}a=-\left(  0_{\mathbb{K}}a\right)
+\left(  0_{\mathbb{K}}a+0_{\mathbb{K}}a\right)  =-\left(  0_{\mathbb{K}%
}a\right)  +0_{\mathbb{K}}a=0_{\mathbb{K}}$. Thus, $0_{\mathbb{K}%
}a=0_{\mathbb{K}}$ is proven. Similarly one can show $a0_{\mathbb{K}%
}=0_{\mathbb{K}}$. Therefore, annihilation follows from the other axioms.}.
The reasons why we have chosen these axioms and not fewer (or more, or others)
are somewhat a matter of taste. For example, I like to explicitly require
annihilation, because it is an important axiom in the definition of a
\textit{semiring}, where it no longer follows from the others.)

\begin{definition}
As we have seen in Definition \ref{def.commring}, a commutative ring consists
of a set $\mathbb{K}$, two binary operations on this set named $+$ and $\cdot
$, and two elements of this set named $0$ and $1$. Thus, formally speaking, we
should encode a commutative ring as the $5$-tuple $\left(  \mathbb{K}%
,+,\cdot,0_{\mathbb{K}},1_{\mathbb{K}}\right)  $. Sometimes we will actually
do so; but most of the time, we will refer to the commutative ring just as the
\textquotedblleft commutative ring $\mathbb{K}$\textquotedblright, hoping that
the other four entries of the $5$-tuple (namely, $+$, $\cdot$, $0_{\mathbb{K}%
}$ and $1_{\mathbb{K}}$) are clear from the context. This kind of abbreviation
is commonplace in mathematics; it is called \textquotedblleft\textit{pars pro
toto}\textquotedblright\ (because we are referring to a large structure by the
same symbol as for a small part of it, and hoping that the rest can be
inferred from the context). It is an example of what is called
\textquotedblleft abuse of notation\textquotedblright.

The elements $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$ of a commutative ring
$\mathbb{K}$ are called the \textit{zero} and the \textit{unity}%
\footnotemark\ of $\mathbb{K}$. They are usually denoted by $0$ and $1$
(without the subscript $\mathbb{K}$) when this can cause no confusion (and,
unfortunately, often also when it can). They are not always identical with the
actual integers $0$ and $1$.

The binary operations $+$ and $\cdot$ in Definition \ref{def.commring} are
also usually not identical with the binary operations $+$ and $\cdot$ on the
set of integers, and are denoted by $+_{\mathbb{K}}$ and $\cdot_{\mathbb{K}}$
when confusion can arise.

The set $\mathbb{K}$ is called the \textit{underlying set} of the commutative
ring $\mathbb{K}$. Let us again remind ourselves that the underlying set of a
commutative ring $\mathbb{K}$ is just a part of the data of $\mathbb{K}$.
\end{definition}

\footnotetext{Some people say \textquotedblleft unit\textquotedblright%
\ instead of \textquotedblleft unity\textquotedblright, but other people use
the word \textquotedblleft unit\textquotedblright\ for something different,
which makes every use of this word a potential pitfall.}Here are some examples
and non-examples of rings:\footnote{The following list of examples is long,
and some of these examples rely on knowledge that you might not have yet. As
usual with examples, you need not understand them all. When I say that Laurent
polynomial rings are examples of commutative rings, I do not assume that you
know what Laurent polynomials are; I merely want to ensure that, \textbf{if}
you have already encountered Laurent polynomials, then you get to know that
they form a commutative ring.}

\begin{itemize}
\item The sets $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$
(endowed with the usual addition, the usual multiplication, the usual $0$ and
the usual $1$) are commutative rings. (Notice that existence of
\textbf{multiplicative} inverses is not required\footnote{A
\textit{multiplicative inverse} of an element $a\in\mathbb{K}$ means an
element $a^{\prime}\in\mathbb{K}$ such that $aa^{\prime}=a^{\prime
}a=1_{\mathbb{K}}$. (This is analogous to an additive inverse, except that
addition is replaced by multiplication, and $0_{\mathbb{K}}$ is replaced by
$1_{\mathbb{K}}$.) In a commutative ring, every element is required to have an
additive inverse (by the definition of a commutative ring), but not every
element is guaranteed to have a multiplicative inverse. (For instance, $2$ has
no multiplicative inverse in $\mathbb{Z}$, and $0$ has no multiplicative
inverse in any of the rings $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$ and
$\mathbb{C}$.)
\par
We shall study multiplicative inverses more thoroughly in Section
\ref{sect.invertible} (where we will just call them \textquotedblleft
inverses\textquotedblright).}!)

\item The set $\mathbb{N}$ of nonnegative integers (again endowed with the
usual addition, the usual multiplication, the usual $0$ and the usual $1$) is
\textbf{not} a commutative ring. It fails the existence of additive inverses.
(Of course, negative numbers exist, but this does not count because they don't
lie in $\mathbb{N}$.)

\item We can define a commutative ring $\mathbb{Z}^{\prime}$ as follows: We
define a binary operation $\widetilde{\times}$ on $\mathbb{Z}$ (written infix)
by
\[
\left(  a\widetilde{\times}b=-ab\ \ \ \ \ \ \ \ \ \ \text{for all }\left(
a,b\right)  \in\mathbb{Z}\times\mathbb{Z}\right)  .
\]
Now, let $\mathbb{Z}^{\prime}$ be the \textbf{set} $\mathbb{Z}$, endowed with
the usual addition $+$ and the (unusual) multiplication $\widetilde{\times}$,
with the zero $0_{\mathbb{Z}^{\prime}}=0$ and with the unity $1_{\mathbb{Z}%
^{\prime}}=-1$. It is easy to check that $\mathbb{Z}^{\prime}$ is a
commutative ring\footnote{Notice that we have named this new commutative ring
$\mathbb{Z}^{\prime}$, not $\mathbb{Z}$ (despite having $\mathbb{Z}^{\prime
}=\mathbb{Z}$ as sets). The reason is that if we had named it $\mathbb{Z}$,
then we could no longer speak of \textquotedblleft the commutative ring
$\mathbb{Z}$\textquotedblright\ without being ambiguous (we would have to
specify every time whether we mean the usual multiplication or the unusual
one).}; it is an example of a commutative ring whose unity is clearly
\textbf{not} equal to the integer $1$ (which is why it is important to never
omit the subscript $\mathbb{Z}^{\prime}$ in $1_{\mathbb{Z}^{\prime}}$ here).

That said, $\mathbb{Z}^{\prime}$ is not a very interesting ring: It is
essentially \textquotedblleft a copy of $\mathbb{Z}$, except that every
integer $n$ has been renamed as $-n$\textquotedblright. To formalize this
intuition, we would need to introduce the notion of a \textit{ring
isomorphism}, which we don't want to do right here; but the idea is that the
bijection%
\[
\varphi:\mathbb{Z}\rightarrow\mathbb{Z}^{\prime},\ \ \ \ \ \ \ \ \ \ n\mapsto
-n
\]
satisfies%
\begin{align*}
\varphi\left(  a+b\right)   &  =\varphi\left(  a\right)  +\varphi\left(
b\right)  \ \ \ \ \ \ \ \ \ \ \text{for all }\left(  a,b\right)  \in
\mathbb{Z}\times\mathbb{Z};\\
\varphi\left(  a\cdot b\right)   &  =\varphi\left(  a\right)
\widetilde{\times}\varphi\left(  b\right)  \ \ \ \ \ \ \ \ \ \ \text{for all
}\left(  a,b\right)  \in\mathbb{Z}\times\mathbb{Z};\\
\varphi\left(  0\right)   &  =0_{\mathbb{Z}^{\prime}};\\
\varphi\left(  1\right)   &  =1_{\mathbb{Z}^{\prime}},
\end{align*}
and thus the ring $\mathbb{Z}^{\prime}$ can be viewed as the ring $\mathbb{Z}$
with its elements \textquotedblleft relabelled\textquotedblright\ using this bijection.

\item The polynomial rings $\mathbb{Z}\left[  x\right]  $, $\mathbb{Q}\left[
a,b\right]  $, $\mathbb{C}\left[  z_{1},z_{2},\ldots,z_{n}\right]  $ are
commutative rings. Laurent polynomial rings are also commutative rings. (Do
not worry if you have not seen these rings yet.)

\item The set of all functions $\mathbb{Q}\rightarrow\mathbb{Q}$ is a
commutative ring, where addition and multiplication are defined pointwise
(i.e., addition is defined by $\left(  f+g\right)  \left(  x\right)  =f\left(
x\right)  +g\left(  x\right)  $ for all $x\in\mathbb{Q}$, and multiplication
is defined by $\left(  fg\right)  \left(  x\right)  =f\left(  x\right)  \cdot
g\left(  x\right)  $ for all $x\in\mathbb{Q}$), where the zero is the
\textquotedblleft constant-$0$\textquotedblright\ function (sending every
$x\in\mathbb{Q}$ to $0$), and where the unity is the \textquotedblleft
constant-$1$\textquotedblright\ function (sending every $x\in\mathbb{Q}$ to
$1$). Of course, the same construction works if we consider functions
$\mathbb{R}\rightarrow\mathbb{C}$, or functions $\mathbb{C}\rightarrow
\mathbb{Q}$, or functions $\mathbb{N}\rightarrow\mathbb{Q}$, instead of
functions $\mathbb{Q}\rightarrow\mathbb{Q}$.\ \ \ \ \footnote{But not if we
consider functions $\mathbb{Q}\rightarrow\mathbb{N}$; such functions might
fail the existence of additive inverses.
\par
Generally, if $X$ is any set and $\mathbb{K}$ is any commutative ring, then
the set of all functions $X\rightarrow\mathbb{K}$ is a commutative ring, where
addition and multiplication are defined pointwise, where the zero is the
\textquotedblleft constant-$0_{\mathbb{K}}$\textquotedblright\ function, and
where the unity is the \textquotedblleft constant-$1_{\mathbb{K}}%
$\textquotedblright\ function.}

\item The set $\mathbb{S}$ of all real numbers of the form $a+b\sqrt{5}$ with
$a,b\in\mathbb{Q}$ (endowed with the usual notions of \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright\ defined on $\mathbb{R}$) is a commutative
ring\footnote{To prove this, we argue as follows:
\par
\begin{itemize}
\item Addition and multiplication are indeed two binary operations on
$\mathbb{S}$. This is because the sum of two elements of $\mathbb{S}$ is an
element of $\mathbb{S}$ (namely, $\left(  a+b\sqrt{5}\right)  +\left(
c+d\sqrt{5}\right)  =\left(  a+c\right)  +\left(  b+d\right)  \sqrt{5}$), and
so is their product (namely, $\left(  a+b\sqrt{5}\right)  \cdot\left(
c+d\sqrt{5}\right)  =\left(  ac+5bd\right)  +\left(  bc+ad\right)  \sqrt{5}$).
\par
\item All axioms of a commutative ring are satisfied for $\mathbb{S}$, except
maybe the existence of additive inverses. This is simply because the addition
and the multiplication in $\mathbb{S}$ are \textquotedblleft
inherited\textquotedblright\ from $\mathbb{R}$, and clearly all these axioms
come with the inheritance.
\par
\item Existence of additive inverses also holds in $\mathbb{S}$, because the
additive inverse of $a+b\sqrt{5}$ is $\left(  -a\right)  +\left(  -b\right)
\sqrt{5}$.
\end{itemize}
}.

\item We could define a different ring structure on the set $\mathbb{S}$ (that
is, a commutative ring which, as a set, is identical with $\mathbb{S}$, but
has a different choice of operations) as follows: We define a binary operation
$\ast$ on $\mathbb{S}$ by setting%
\[
\left(  a+b\sqrt{5}\right)  \ast\left(  c+d\sqrt{5}\right)  =ac+bd\sqrt
{5}\ \ \ \ \ \ \ \ \ \ \text{for all }\left(  a,b\right)  \in\mathbb{Q}%
\times\mathbb{Q}\text{ and }\left(  c,d\right)  \in\mathbb{Q}\times
\mathbb{Q}.
\]
\footnote{This is well-defined, because every element of $\mathbb{S}$ can be
written in the form $a+b\sqrt{5}$ for a \textbf{unique} pair $\left(
a,b\right)  \in\mathbb{Q}\times\mathbb{Q}$. This is a consequence of the
irrationality of $\sqrt{5}$.} Now, let $\mathbb{S}^{\prime}$ be the set
$\mathbb{S}$, endowed with the usual addition $+$ and the (unusual)
multiplication $\ast$, with the zero $0_{\mathbb{S}^{\prime}}=0$ and with the
unity $1_{\mathbb{S}^{\prime}}=1+\sqrt{5}$ (not the integer $1$). It is easy
to check that $\mathbb{S}^{\prime}$ is a commutative ring\footnote{Again, we
do not call it $\mathbb{S}$, in order to be able to distinguish between
different ring structures.}. The \textbf{sets} $\mathbb{S}$ and $\mathbb{S}%
^{\prime}$ are identical, but the \textbf{commutative rings} $\mathbb{S}$ and
$\mathbb{S}^{\prime}$ are not\footnote{Keep in mind that, due to our
\textquotedblleft pars pro toto\textquotedblright\ notation, \textquotedblleft
commutative ring $\mathbb{S}$\textquotedblright\ means more than
\textquotedblleft set $\mathbb{S}$\textquotedblright.}: For example, the ring
$\mathbb{S}^{\prime}$ has two nonzero elements whose product is $0$ (namely,
$1\ast\sqrt{5}=0$), whereas the ring $\mathbb{S}$ has no such things. This
shows that not only do we have $\mathbb{S}^{\prime}\neq\mathbb{S}$ as
commutative rings, but there is also no way to regard $\mathbb{S}^{\prime}$ as
\textquotedblleft a copy of $\mathbb{S}$ with its elements
renamed\textquotedblright\ (in the same way as we have regarded $\mathbb{Z}%
^{\prime}$ as \textquotedblleft a copy of $\mathbb{Z}$ with its elements
renamed\textquotedblright). This example should stress the point that a
commutative ring $\mathbb{K}$ is not just a set; it is a set endowed with two
operations ($+$ and $\cdot$) and two elements ($0_{\mathbb{K}}$ and
$1_{\mathbb{K}}$), and these operations and elements are no less important
than the set.

\item The set $\mathbb{S}_{3}$ of all real numbers of the form $a+b\sqrt[3]%
{5}$ with $a,b\in\mathbb{Q}$ (endowed with the usual addition, the usual
multiplication, the usual $0$ and the usual $1$) is \textbf{not} a commutative
ring. Indeed, multiplication is not a binary operation on this set
$\mathbb{S}_{3}$: It does not always send two elements of $\mathbb{S}_{3}$ to
an element of $\mathbb{S}_{3}$. For instance, $\left(  1+1\sqrt[3]{5}\right)
\left(  1+1\sqrt[3]{5}\right)  =1+2\sqrt[3]{5}+\left(  \sqrt[3]{5}\right)
^{2}$ is not in $\mathbb{S}_{3}$.

\item The set of all $2\times2$-matrices over $\mathbb{Q}$ is \textbf{not} a
commutative ring, because commutativity of multiplication does not hold for
this set. (In general, $AB\neq BA$ for matrices.)

\item If you like the empty set, you will enjoy the \textit{zero ring}. This
is the commutative ring which is defined as the one-element set $\left\{
0\right\}  $, with zero and unity both being $0$ (nobody said that they have
to be distinct!), with addition given by $0+0=0$ and with multiplication given
by $0\cdot0=0$. Of course, it is not an empty set\footnote{A commutative ring
cannot be empty, as it contains at least one element (namely, $0$).}, but it
plays a similar role in the world of commutative rings as the empty set does
in the world of sets: It carries no information itself, but things would break
if it were to be excluded\footnote{Some authors \textbf{do} prohibit the zero
ring from being a commutative ring (by requiring every commutative ring to
satisfy $0\neq1$). I think most of them run into difficulties from this
decision sooner or later.}.

Notice that the zero and the unity of the zero ring are identical, i.e., we
have $0_{\mathbb{K}}=1_{\mathbb{K}}$. This shows why it is dangerous to omit
the subscripts and just denote the zero and the unity by $0$ and $1$; in fact,
you don't want to rewrite the equality $0_{\mathbb{K}}=1_{\mathbb{K}}$ as
\textquotedblleft$0=1$\textquotedblright! (Most algebraists make a compromise
between wanting to omit the subscripts and having to clarify what $0$ and $1$
mean: They say that \textquotedblleft$0=1$ in $\mathbb{K}$\textquotedblright%
\ to mean \textquotedblleft$0_{\mathbb{K}}=1_{\mathbb{K}}$\textquotedblright.)

Generally, a \textit{trivial ring} is defined to be a commutative ring
containing only one element (which then necessarily is both the zero and the
unity of this ring). The addition and the multiplication of a trivial ring are
uniquely determined (since there is only one possible value that a sum or a
product could take). Every trivial ring can be viewed as the zero ring with
its element $0$ relabelled.\footnote{In more formal terms, the preceding
statement would say that \textquotedblleft every trivial ring is isomorphic to
the zero ring\textquotedblright.}

\item In set theory, the \textit{symmetric difference} of two sets $A$ and $B$
is defined to be the set $\left(  A\cup B\right)  \setminus\left(  A\cap
B\right)  =\left(  A\setminus B\right)  \cup\left(  B\setminus A\right)  $.
This symmetric difference is denoted by $A\bigtriangleup B$. Now, let $S$ be
any set. Let $\mathcal{P}\left(  S\right)  $ denote the powerset of $S$ (that
is, the set of all subsets of $S$). It is easy to check that the following ten
properties hold:%
\begin{align*}
A\bigtriangleup B  &  =B\bigtriangleup A\ \ \ \ \ \ \ \ \ \ \text{for any sets
}A\text{ and }B\text{;}\\
A\cap B  &  =B\cap A\ \ \ \ \ \ \ \ \ \ \text{for any sets }A\text{ and
}B\text{;}\\
\left(  A\bigtriangleup B\right)  \bigtriangleup C  &  =A\bigtriangleup\left(
B\bigtriangleup C\right)  \ \ \ \ \ \ \ \ \ \ \text{for any sets }A\text{,
}B\text{ and }C\text{;}\\
\left(  A\cap B\right)  \cap C  &  =A\cap\left(  B\cap C\right)
\ \ \ \ \ \ \ \ \ \ \text{for any sets }A\text{, }B\text{ and }C\text{;}\\
A\bigtriangleup\varnothing &  =\varnothing\bigtriangleup
A=A\ \ \ \ \ \ \ \ \ \ \text{for any set }A\text{;}\\
A\bigtriangleup A  &  =\varnothing\ \ \ \ \ \ \ \ \ \ \text{for any set
}A\text{;}\\
A\cap S  &  =S\cap A=A\ \ \ \ \ \ \ \ \ \ \text{for any subset }A\text{ of
}S\text{;}\\
\varnothing\cap A  &  =A\cap\varnothing=\varnothing
\ \ \ \ \ \ \ \ \ \ \text{for any set }A\text{;}\\
A\cap\left(  B\bigtriangleup C\right)   &  =\left(  A\cap B\right)
\bigtriangleup\left(  A\cap C\right)  \ \ \ \ \ \ \ \ \ \ \text{for any sets
}A\text{, }B\text{ and }C\text{;}\\
\left(  A\bigtriangleup B\right)  \cap C  &  =\left(  A\cap C\right)
\bigtriangleup\left(  B\cap C\right)  \ \ \ \ \ \ \ \ \ \ \text{for any sets
}A\text{, }B\text{ and }C\text{.}%
\end{align*}
Therefore, $\mathcal{P}\left(  S\right)  $ becomes a commutative ring, where
the addition is defined to be the operation $\bigtriangleup$, the
multiplication is defined to be the operation $\cap$, the zero is defined to
be the set $\varnothing$, and the unity is defined to be the set
$S$.\ \ \ \ \footnote{The ten properties listed above show that the axioms of
a commutative ring are satisfied for $\left(  \mathcal{P}\left(  S\right)
,\bigtriangleup,\cap,\varnothing,S\right)  $. In particular, the sixth
property shows that every subset $A$ of $S$ has an additive inverse -- namely,
itself. Of course, it is unusual for an element of a commutative ring to be
its own additive inverse, but in this example it happens all the time!}

The commutative ring $\mathcal{P}\left(  S\right)  $ has the property that
$a^{2}=a$ for every $a\in\mathcal{P}\left(  S\right)  $. (This simply means
that $A\cap A=A$ for every $A\subseteq S$.) Commutative rings that have this
property are called
\href{https://en.wikipedia.org/wiki/Boolean_ring}{\textit{Boolean rings}}. (Of
course, $\mathcal{P}\left(  S\right)  $ is the eponymic example for a Boolean
ring; but there are also others.)

\item For every positive integer $n$, the residue classes of integers modulo
$n$ form a commutative ring, which is called $\mathbb{Z}/n\mathbb{Z}$ or
$\mathbb{Z}_{n}$ (depending on the author). This ring has $n$ elements (often
called \textquotedblleft integers modulo $n$\textquotedblright). When $n$ is a
composite number (e.g., $n=6$), this ring has the property that products of
nonzero\footnote{An element $a$ of a commutative ring $\mathbb{K}$ is said to
be \textit{nonzero} if $a\neq0_{\mathbb{K}}$. (This is not the same as saying
that $a$ is not the integer $0$, because the integer $0$ might not be
$0_{\mathbb{K}}$.)} elements can be zero (e.g., we have $2\cdot3\equiv
0\operatorname{mod}6$); this means that there is no way to define division by
all nonzero elements in this ring (even if we are allowed to create
fractions). Notice that $\mathbb{Z}/1\mathbb{Z}$ is a trivial ring.

We notice that if $n$ is a positive integer, and if $\mathbb{K}$ is the
commutative ring $\mathbb{Z}/n\mathbb{Z}$, then $\underbrace{1_{\mathbb{K}%
}+1_{\mathbb{K}}+\cdots+1_{\mathbb{K}}}_{n\text{ times}}=0_{\mathbb{K}}$
(because the left hand side of this equality is the residue class of $n$
modulo $n$, while the right hand side is the residue class of $0$ modulo $n$,
and these two residue classes are clearly equal).

\item Let us try to define \textquotedblleft division by
zero\textquotedblright. So, we introduce a new symbol $\infty$, and we try to
extend the addition on $\mathbb{Q}$ to the set $\mathbb{Q}\cup\left\{
\infty\right\}  $ by setting $a+\infty=\infty$ for all $a\in\mathbb{Q}%
\cup\left\{  \infty\right\}  $. We might also try to extend the multiplication
in some way, and perhaps to add some more elements (such as another symbol
$-\infty$ to serve as the product $\left(  -1\right)  \infty$). I claim that
(whatever we do with the multiplication, and whatever new elements we add) we
do not get a commutative ring. Indeed, assume the contrary. Thus, there exists
a commutative ring $\mathbb{W}$ which contains $\mathbb{Q}\cup\left\{
\infty\right\}  $ as a subset, and which has $a+\infty=\infty$ for all
$a\in\mathbb{Q}$. Thus, in $\mathbb{W}$, we have $1+\infty=\infty=0+\infty$.
Adding $\left(  -1\right)  \infty$ to both sides of this equality, we obtain
$1+\infty+\left(  -1\right)  \infty=0+\infty+\left(  -1\right)  \infty$, so
that $1=0$\ \ \ \ \footnote{because $\infty+\left(  -1\right)  \infty
=1\infty+\left(  -1\right)  \infty=\underbrace{\left(  1+\left(  -1\right)
\right)  }_{=0}\infty=0\infty=0$}; but this is absurd. Hence, we have found a
contradiction. This is why \textquotedblleft division by zero is
impossible\textquotedblright: One can define objects that behave like
\textquotedblleft infinity\textquotedblright\ (and they \textbf{are} useful),
but they break various standard rules such as the axioms of a commutative
ring. In contrast to this, adding a \textquotedblleft number\textquotedblright%
\ $i$ satisfying $i^{2}=-1$ to the real numbers is harmless: The complex
numbers $\mathbb{C}$ are still a commutative ring.

\item Here is an \textquotedblleft almost-ring\textquotedblright\ beloved to
many combinatorialists: the \textit{max-plus semiring} $\mathbb{T}$ (also
called the \textit{tropical semiring}\footnote{Caution: Both of these names
mean many other things as well.}). We create a new symbol $-\infty$, and we
set $\mathbb{T}=\mathbb{Z}\cup\left\{  -\infty\right\}  $ as sets, but we do
\textbf{not} \textquotedblleft inherit\textquotedblright\ the addition and the
multiplication from $\mathbb{Z}$. Instead, we denote the \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright\ operations on $\mathbb{Z}$ by $+_{\mathbb{Z}%
}$ and $\cdot_{\mathbb{Z}}$, and we define two new \textquotedblleft
addition\textquotedblright\ and \textquotedblleft
multiplication\textquotedblright\ operations $+_{\mathbb{T}}$ and
$\cdot_{\mathbb{T}}$ on $\mathbb{T}$ as follows:%
\begin{align*}
a+_{\mathbb{T}}b  &  =\max\left\{  a,b\right\}  ;\\
a\cdot_{\mathbb{T}}b  &  =a+_{\mathbb{Z}}b.
\end{align*}
(Here, we set $\max\left\{  -\infty,n\right\}  =\max\left\{  n,-\infty
\right\}  =n$ and $\left(  -\infty\right)  +_{\mathbb{Z}}n=n+_{\mathbb{Z}%
}\left(  -\infty\right)  =-\infty$ for every $n\in\mathbb{T}$.)

It turns out that the set $\mathbb{T}$ endowed with the two operations
$+_{\mathbb{T}}$ and $\cdot_{\mathbb{T}}$, the zero $0_{\mathbb{T}}=-\infty$
and the unity $1_{\mathbb{T}}=0$ comes rather close to being a commutative
ring. It satisfies all axioms of a commutative ring except for the existence
of additive inverses. Such a structure is called a \textit{semiring}. Other
examples of semirings are $\mathbb{N}$ and a reasonably defined $\mathbb{N}%
\cup\left\{  \infty\right\}  $ (with $0\infty=0$ and $a\infty=\infty$ for all
$a>0$).
\end{itemize}

If $\mathbb{K}$ is a commutative ring, then we can define a subtraction in
$\mathbb{K}$, even though we have not required a subtraction operation as part
of the definition of a commutative ring $\mathbb{K}$. Namely, the
\textit{subtraction} of a commutative ring $\mathbb{K}$ is the binary
operation $-$ on $\mathbb{K}$ (again written infix) defined as follows: For
every $a \in\mathbb{K}$ and $b \in\mathbb{K}$, set $a-b = a+b^{\prime}$, where
$b^{\prime}$ is the additive inverse of $b$. It is not hard to check that
$a-b$ is the unique element $c$ of $\mathbb{K}$ satisfying $a = b+c$; thus,
subtraction is ``the undoing of addition'' just as in the classical situation
of integers. Again, the notation $-$ for the subtraction of $\mathbb{K}$ is
denoted by $-_{\mathbb{K}}$ whenever a confusion with the subtraction of
integers could arise.

Whenever $a$ is an element of a commutative ring $\mathbb{K}$, we write $-a$
for the additive inverse of $a$. This is the same as $0_{\mathbb{K}} - a$.

The intuition for commutative rings is essentially that all computations that
can be made with the operations $+$, $-$ and $\cdot$ on integers can be
similarly made in a commutative ring. For instance, if $a_{1},a_{2}%
,\ldots,a_{n}$ are $n$ elements of a commutative ring, then the sum
$a_{1}+a_{2}+\cdots+a_{n}$ is well-defined, and can be computed by adding the
elements $a_{1},a_{2},\ldots,a_{n}$ to each other in any order\footnote{For
instance, we can compute the sum $a+b+c+d$ of four elements $a,b,c,d$ in many
ways: For example, we can first add $a$ and $b$, then add $c$ and $d$, and
finally add the two results; alternatively, we can first add $a$ and $b$, then
add $d$ to the result, then add $c$ to the result. In a commutative ring, all
such ways lead to the same result. To prove this is a slightly tedious
induction argument that uses commutativity and associativity.}. The same holds
for products. If $n$ is an integer and $a$ is an element of a commutative ring
$\mathbb{K}$, then we define an element $na$ of $\mathbb{K}$ by%
\[
na=%
\begin{cases}
\underbrace{a+a+\cdots+a}_{n\text{ addends}}, & \text{if }n\geq0;\\
-\underbrace{a+a+\cdots+a}_{-n\text{ addends}}, & \text{if }n<0
\end{cases}
.
\]
\footnote{Notice that this definition of $na$ is \textbf{not} a particular
case of the product of two elements of $\mathbb{K}$, because $n$ is not an
element of $\mathbb{K}$.}

If $n$ is a nonnegative integer and $a$ is an element of a commutative ring
$\mathbb{K}$, then $a^{n}$ is a well-defined element of $\mathbb{K}$ (namely,
$a^{n}=\underbrace{a\cdot a\cdot\cdots\cdot a}_{n\text{ factors}}$). The
following identities hold:%
\begin{align}
-\left(  a+b\right)   &  =\left(  -a\right)  +\left(  -b\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K};\label{eq.rings.-(a+b)}\\
-\left(  -a\right)   &  =a\ \ \ \ \ \ \ \ \ \ \text{for }a\in\mathbb{K}%
;\label{eq.rings.-(-a)}\\
-\left(  ab\right)   &  =\left(  -a\right)  b=a\left(  -b\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K};\label{eq.rings.-(ab)}\\
-\left(  na\right)   &  =\left(  -n\right)  a=n\left(  -a\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a\in\mathbb{K}\text{ and }n\in\mathbb{Z}%
;\label{eq.rings.-(na)}\\
n\left(  ab\right)   &  =\left(  na\right)  b=a\left(  nb\right)
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K}\text{ and }n\in\mathbb{Z}%
;\label{eq.rings.nab}\\
\left(  nm\right)  a  &  =n\left(  ma\right)  \ \ \ \ \ \ \ \ \ \ \text{for
}a\in\mathbb{K}\text{ and }n,m\in\mathbb{Z};\label{eq.rings.nma}\\
0^{n}  &  =%
\begin{cases}
0, & \text{if }n>0;\\
1, & \text{if }n=0
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for }n\in\mathbb{N};\label{eq.rings.0**n}\\
a^{n+m}  &  =a^{n}a^{m}\ \ \ \ \ \ \ \ \ \ \text{for }a\in\mathbb{K}\text{ and
}n,m\in\mathbb{N};\label{eq.rings.a**(n+m)}\\
\left(  ab\right)  ^{n}  &  =a^{n}b^{n}\ \ \ \ \ \ \ \ \ \ \text{for }%
a,b\in\mathbb{K}\text{ and }n\in\mathbb{N};\label{eq.rings.-(ab)**n}\\
\left(  a+b\right)  ^{n}  &  =\sum_{k=0}^{n}\dbinom{n}{k}a^{k}b^{n-k}%
\ \ \ \ \ \ \ \ \ \ \text{for }a,b\in\mathbb{K}\text{ and }n\in\mathbb{N}.
\label{eq.rings.(a+b)**n}%
\end{align}
Here, we are using the standard notations $+$, $\cdot$, $0$ and $1$ for the
addition, the multiplication, the zero and the unity of $\mathbb{K}$, because
confusion (e.g., confusion of the $0$ with the integer $0$) is rather
unlikely.\footnote{For instance, in the statement \textquotedblleft$-\left(
a+b\right)  =\left(  -a\right)  +\left(  -b\right)  $ for $a,b\in\mathbb{K}%
$\textquotedblright, it is clear that the $+$ can only stand for the addition
of $\mathbb{K}$ and not (say) for the addition of integers (since $a$, $b$,
$-a$ and $-b$ are elements of $\mathbb{K}$, not (generally) integers). The
only statement whose meaning is ambiguous is \textquotedblleft$0^{n}=%
\begin{cases}
0, & \text{if }n>0;\\
1, & \text{if }n=0
\end{cases}
$ for $n\in\mathbb{N}$\textquotedblright. In this statement, the
\textquotedblleft$0$\textquotedblright\ in \textquotedblleft$n>0$%
\textquotedblright\ and the \textquotedblleft$0$\textquotedblright\ in
\textquotedblleft$n=0$\textquotedblright\ clearly mean the integer $0$ (since
they are being compared with the integer $n$), but the other two appearances
of \textquotedblleft$0$\textquotedblright\ and the \textquotedblleft%
$1$\textquotedblright\ are ambiguous. I hope that the context makes it clear
enough that they mean the zero and the unity of $\mathbb{K}$ (and not the
integers $0$ and $1$).} We shall keep doing so in the following, apart from
situations where confusion can realistically occur.\footnote{Notice that the
equalities (\ref{eq.rings.nab}) and (\ref{eq.rings.nma}) are \textbf{not}
particular cases of the associativity of multiplication which we required to
hold for $\mathbb{K}$. Indeed, the latter associativity says that $a\left(
bc\right)  =\left(  ab\right)  c$ for all $a\in\mathbb{K}$, $b\in\mathbb{K}$
and $c\in\mathbb{K}$. But in (\ref{eq.rings.nab}) and (\ref{eq.rings.nma}),
the $n$ is an integer, not an element of $\mathbb{K}$.}

The identities listed above are not hard to prove. Indeed, they are
generalizations of well-known identities holding for rational numbers; and
some of them (for example, (\ref{eq.rings.a**(n+m)}), (\ref{eq.rings.-(ab)**n}%
) and (\ref{eq.rings.(a+b)**n})) can be proved in exactly the same way as
those identities for rational numbers.\footnote{For instance, we can obtain a
proof of (\ref{eq.rings.(a+b)**n}) by re-reading the solution to Exercise
\ref{exe.prop.binom.binomial}, while replacing every $X$ by an $a$ and
replacing every $Y$ by a $b$. Another proof of (\ref{eq.rings.(a+b)**n}) is
given in the solution to Exercise \ref{exe.prod(ai+bi)} \textbf{(b)}.}

Furthermore, finite sums such as $\sum_{s\in S}a_{s}$ (where $S$ is a finite
set, and $a_{s}\in\mathbb{K}$ for every $s\in S$), and finite products such as
$\prod_{s\in S}a_{s}$ (where $S$ is a finite set, and $a_{s}\in\mathbb{K}$ for
every $s\in S$) are defined whenever $\mathbb{K}$ is a commutative ring.
Again, the definition is the same as for numbers, and these sums and products
behave as they do for numbers. For example, Exercise
\ref{exe.perm.sign.pseudoexplicit} still holds if we replace \textquotedblleft%
$\mathbb{C}$\textquotedblright\ by \textquotedblleft$\mathbb{K}$%
\textquotedblright\ in it (and the same solution proves it) whenever
$\mathbb{K}$ is a commutative ring.

\begin{remark}
The notion of a \textquotedblleft commutative ring\textquotedblright\ is not
fully standardized; there exist several competing definitions:

For some people, a \textquotedblleft commutative ring\textquotedblright\ is
\textit{not} endowed with an element $1$ (although it \textbf{can} have such
an element), and, consequently, does not have to satisfy the unitality axiom.
According to their definition, for example, the set $\left\{  0,2,4,6,\ldots
\right\}  =\left\{  2n\ \mid\ n\in\mathbb{N}\right\}  $ is a commutative ring
(with the usual addition and multiplication). (In contrast, our definition of
a \textquotedblleft commutative ring\textquotedblright\ does not accept
$\left\{  0,2,4,6,\ldots\right\}  $ as a commutative ring, because it does not
contain any element which would fill the role of $1$.) These people tend to
use the notation \textquotedblleft commutative ring with
unity\textquotedblright\ (or \textquotedblleft commutative ring with
$1$\textquotedblright) to mean a commutative ring which is endowed with a $1$
and satisfies the unitality axiom (i.e., what we call a \textquotedblleft
commutative ring\textquotedblright).

On the other hand, there are authors who use the word \textquotedblleft
ring\textquotedblright\ for what we call \textquotedblleft commutative
ring\textquotedblright. These are mostly the authors who work with commutative
rings all the time and find the name \textquotedblleft commutative
ring\textquotedblright\ too long.

When you are reading about rings, it is important to know which meaning of
\textquotedblleft ring\textquotedblright\ the author is subscribing to. (Often
this can be inferred from the examples given.)
\end{remark}

\begin{exercise}
\label{exe.prod(ai+bi)}Let $\mathbb{K}$ be a commutative ring. For every
$n\in\mathbb{N}$, let $\left[  n\right]  $ denote the set $\left\{
1,2,\ldots,n\right\}  $.

\textbf{(a)} Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$
elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n}$ be $n$ further
elements of $\mathbb{K}$. Prove that%
\[
\prod_{i=1}^{n}\left(  a_{i}+b_{i}\right)  =\sum_{I\subseteq\left[  n\right]
}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  n\right]
\setminus I}b_{i}\right)  .
\]
(Here, as usual, the summation sign $\sum_{I\subseteq\left[  n\right]  }$
means $\sum_{I\in\mathcal{P}\left(  \left[  n\right]  \right)  }$, where
$\mathcal{P}\left(  \left[  n\right]  \right)  $ denotes the powerset of
$\left[  n\right]  $.)

\textbf{(b)} Use Exercise \ref{exe.prod(ai+bi)} to give a new proof of
(\ref{eq.rings.(a+b)**n}).
\end{exercise}

\begin{exercise}
\label{exe.multinom2}For each $m\in\mathbb{N}$ and $\left(  k_{1},k_{2}%
,\ldots,k_{m}\right)  \in\mathbb{N}^{m}$, let us define a positive integer
$\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{m}\right)  $ by $\mathbf{m}\left(
k_{1},k_{2},\ldots,k_{m}\right)  =\dfrac{\left(  k_{1}+k_{2}+\cdots
+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}!}$. (This is indeed a positive
integer, because Exercise \ref{exe.multinom1} says so.)

Let $\mathbb{K}$ be a commutative ring. Let $m\in\mathbb{N}$. Let $a_{1}%
,a_{2},\ldots,a_{m}$ be $m$ elements of $\mathbb{K}$. Let $n\in\mathbb{N}$.
Prove that%
\[
\left(  a_{1}+a_{2}+\cdots+a_{m}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}^{m};\\k_{1}+k_{2}+\cdots
+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{m}\right)  \prod_{i=1}%
^{m}a_{i}^{k_{i}}.
\]
(This is called the \textit{multinomial formula}.)
\end{exercise}

\subsection{Matrices}

We have briefly defined determinants in Definition \ref{def.det.old}, but we
haven't done much with them. This will be amended now. But let us first recall
the definitions of basic notions in matrix algebra.

In the following, we fix a commutative ring $\mathbb{K}$. The elements of
$\mathbb{K}$ will be called \textit{scalars} (to distinguish them from
\textit{vectors} and \textit{matrices}, which we will soon discuss, and which
are structures containing several elements of $\mathbb{K}$).

If you feel uncomfortable with commutative rings, you are free to think that
$\mathbb{K}=\mathbb{Q}$ or $\mathbb{K}=\mathbb{C}$ in the following; but
everything I am doing works for any commutative ring unless stated otherwise.

Given two nonnegative integers $n$ and $m$, an $n\times m$\textit{-matrix}
(or, more precisely, $n\times m$\textit{-matrix over} $\mathbb{K}$) means a
rectangular table with $n$ rows and $m$ columns whose entries are elements of
$\mathbb{K}$.\ \ \ \ \footnote{Formally speaking, this means that an $n\times
m$-matrix is a map from $\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $ to $\mathbb{K}$. We represent such a map as a
rectangular table by writing the image of $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $ into the cell in
the $i$-th row and the $j$-th column.} For instance, when $\mathbb{K}%
=\mathbb{Q}$, the table $\left(
\begin{array}
[c]{ccc}%
1 & -2/5 & 4\\
1/3 & -1/2 & 0
\end{array}
\right)  $ is a $2\times3$-matrix. A \textit{matrix} simply means an $n\times
m$-matrix for some $n\in\mathbb{N}$ and $m\in\mathbb{N}$. These $n$ and $m$
are said to be the \textit{dimensions} of the matrix.

If $A$ is an $n\times m$-matrix, and if $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,m\right\}  $, then the $\left(  i,j\right)
$\textit{-th entry of }$A$ means the entry of $A$ in row $i$ and column $j$.
For instance, the $\left(  1,2\right)  $-th entry of the matrix $\left(
\begin{array}
[c]{ccc}%
1 & -2/5 & 4\\
1/3 & -1/2 & 0
\end{array}
\right)  $ is $-2/5$.

If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, and if we are given an element
$a_{i,j}\in\mathbb{K}$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $, then we use the
notation $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ for the
$n\times m$-matrix whose $\left(  i,j\right)  $-th entry is $a_{i,j}$ for all
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $. Thus,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m}%
\end{array}
\right)  .
\]
The letters $i$ and $j$ are not set in stone; they are bound variables like
the $k$ in \textquotedblleft$\sum_{k=1}^{n}k$\textquotedblright. Thus, you are
free to write $\left(  a_{x,y}\right)  _{1\leq x\leq n,\ 1\leq y\leq m}$ or
$\left(  a_{j,i}\right)  _{1\leq j\leq n,\ 1\leq i\leq m}$ instead of $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ (and we will use this
freedom eventually).\footnote{Many authors love to abbreviate
\textquotedblleft$a_{i,j}$\textquotedblright\ by \textquotedblleft$a_{ij}%
$\textquotedblright\ (hoping that the reader will not mistake the subscript
\textquotedblleft$ij$\textquotedblright\ for a product or (in the case where
$i$ and $j$ are single-digit numbers) for a two-digit number). The only
advantage of this abbreviation that I am aware of is that it saves you a
comma; I do not understand why it is so popular. But you should be aware of it
in case you are reading other texts.}

Matrices can be added if they share the same dimensions: If $n$ and $m$ are
two nonnegative integers, and if $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$ are two $n\times m$-matrices, then $A+B$ means the $n\times
m$-matrix $\left(  a_{i,j}+b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.
Thus, matrices are added \textquotedblleft entry by entry\textquotedblright;
for example, $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  +\left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & c^{\prime}\\
d^{\prime} & e^{\prime} & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a+a^{\prime} & b+b^{\prime} & c+c^{\prime}\\
d+d^{\prime} & e+e^{\prime} & f+f^{\prime}%
\end{array}
\right)  $. Similarly, subtraction is defined: If $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$, then $A-B=\left(  a_{i,j}-b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$.

Similarly, one can define the product of a scalar $\lambda\in\mathbb{K}$ with
a matrix $A$: If $\lambda\in\mathbb{K}$ is a scalar, and if $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ is an $n\times m$-matrix,
then $\lambda A$ means the $n\times m$-matrix $\left(  \lambda a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$.

Defining the product of two matrices is trickier. Matrices are \textbf{not}
multiplied \textquotedblleft entry by entry\textquotedblright; this would not
be a very interesting definition. Instead, their product is defined as
follows: If $n$, $m$ and $\ell$ are three nonnegative integers, then the
product $AB$ of an $n\times m$-matrix $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ with an $m\times\ell$-matrix $B=\left(  b_{i,j}\right)
_{1\leq i\leq m,\ 1\leq j\leq\ell}$ means the $n\times\ell$-matrix%
\[
\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
\ell}.
\]
This definition looks somewhat counterintuitive, so let me comment on it.
First of all, for $AB$ to be defined, $A$ and $B$ are \textbf{not} required to
have the same dimensions; instead, $A$ must have as many columns as $B$ has
rows. The resulting matrix $AB$ then has as many rows as $A$ and as many
columns as $B$. Every entry of $AB$ is a sum of products of an entry of $A$
with an entry of $B$ (not a single such product). More precisely, the $\left(
i,j\right)  $-th entry of $AB$ is a sum of products of an entry in the $i$-th
row of $A$ with the respective entry in the $j$-th column of $B$. For example,%
\[
\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & d^{\prime} & g^{\prime}\\
b^{\prime} & e^{\prime} & h^{\prime}\\
c^{\prime} & f^{\prime} & i^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime}+bb^{\prime}+cc^{\prime} & ad^{\prime}+be^{\prime}+cf^{\prime} &
ag^{\prime}+bh^{\prime}+ci^{\prime}\\
da^{\prime}+eb^{\prime}+fc^{\prime} & dd^{\prime}+ee^{\prime}+ff^{\prime} &
dg^{\prime}+eh^{\prime}+fi^{\prime}%
\end{array}
\right)  .
\]


The multiplication of matrices is not commutative! It is easy to find examples
of two matrices $A$ and $B$ for which the products $AB$ and $BA$ are distinct,
or one of them is well-defined but the other is not\footnote{This happens if
$A$ has as many columns as $B$ has rows, but $B$ does not have as many columns
as $A$ has rows.}.

A sum $\sum_{i\in I}A_{i}$ of finitely many matrices $A_{i}$ is defined in the
same way as a sum of numbers or of elements of a commutative ring. However, a
product $\prod_{i\in I}A_{i}$ of finitely many matrices $A_{i}$ (in general)
cannot be defined, because the result would depend on the order of multiplication.

For every $n\in\mathbb{N}$, we let $I_{n}$ denote the $n\times n$-matrix
$\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, where
$\delta_{i,j}$ is defined to be $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.\ \ \ \ \footnote{Here, $0$ and $1$ mean the zero and the unity of
$\mathbb{K}$ (which may and may not be the integers $0$ and $1$).} This matrix
$I_{n}$ looks as follows:%
\[
I_{n}=\left(
\begin{array}
[c]{cccc}%
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  .
\]
It has the property that $I_{n}B=B$ for every $m\in\mathbb{N}$ and every
$n\times m$-matrix $B$; also, $AI_{n}=A$ for every $k\in\mathbb{N}$ and every
$k\times n$-matrix $A$. (Proving this is a good way to check that you
understand how matrices are multiplied.) The matrix $I_{n}$ is called the
$n\times n$ \textit{identity matrix}. (Some call it $E_{n}$ or just $I$, when
the value of $n$ is clear from the context.)

Matrix multiplication is associative: If $n,m,k,\ell\in\mathbb{N}$, and if $A$
is an $n\times m$-matrix, $B$ is an $m\times k$-matrix, and $C$ is a
$k\times\ell$-matrix, then $A\left(  BC\right)  =\left(  AB\right)  C$. The
proof of this is straightforward using our definition of products of
matrices\footnote{Check that $A\left(  BC\right)  $ and $\left(  AB\right)  C$
both are equal to the matrix $\left(  \sum_{u=1}^{m}\sum_{v=1}^{k}%
a_{i,u}b_{u,v}c_{v,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}$.}. This
associativity allows us to write products like $ABC$ without parentheses. By
induction, we can see that longer products such as $A_{1}A_{2}\cdots A_{k}$
for arbitrary $k\in\mathbb{N}$ can also be bracketed at will, because all
bracketings lead to the same result (e.g., for four matrices $A$, $B$, $C$ and
$D$, we have $A\left(  B\left(  CD\right)  \right)  =A\left(  \left(
BC\right)  D\right)  =\left(  AB\right)  \left(  CD\right)  =\left(  A\left(
BC\right)  \right)  D=\left(  \left(  AB\right)  C\right)  D$, provided that
the dimensions of the matrices are appropriate to make sense of the products).
We define an empty product of $n\times n$-matrices to be the $n\times n$
identity matrix $I_{n}$.

For every $n\times n$-matrix $A$ and every $k\in\mathbb{N}$, we can thus
define an $n\times n$-matrix $A^{k}$ by $A^{k}=\underbrace{AA\cdots
A}_{k\text{ factors}}$. In particular, $A^{0}=I_{n}$ (since we defined an
empty product of $n\times n$-matrices to be $I_{n}$).

Further properties of matrix multiplication are easy to state and to prove:

\begin{itemize}
\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$, $k\in\mathbb{N}$ and
$\lambda\in\mathbb{K}$, every $n\times m$-matrix $A$ and every $m\times
k$-matrix $B$, we have $\lambda\left(  AB\right)  =\left(  \lambda A\right)
B=A\left(  \lambda B\right)  $. (This allows us to write $\lambda AB$ for each
of the matrices $\lambda\left(  AB\right)  $, $\left(  \lambda A\right)  B$
and $A\left(  \lambda B\right)  $.)

\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$ and $k\in\mathbb{N}$, every
two $n\times m$-matrices $A$ and $B$, and every $m\times k$-matrix $C$, we
have $\left(  A+B\right)  C=AC+BC$.

\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$ and $k\in\mathbb{N}$, every
$n\times m$-matrix $A$, and every two $m\times k$-matrices $B$ and $C$, we
have $A\left(  B+C\right)  =AB+AC$.

\item For every $n\in\mathbb{N}$, $m\in\mathbb{N}$, $\lambda\in\mathbb{K}$ and
$\mu\in\mathbb{K}$, and every $n\times m$-matrix $A$, we have $\lambda\left(
\mu A\right)  =\left(  \lambda\mu\right)  A$. (This allows us to write
$\lambda\mu A$ for both $\lambda\left(  \mu A\right)  $ and $\left(
\lambda\mu\right)  A$.)
\end{itemize}

For given $n\in\mathbb{N}$ and $m\in\mathbb{N}$, we let $\mathbb{K}^{n\times
m}$ denote the set of all $n\times m$-matrices. (This is one of the two
standard notations for this set; the other is $\operatorname*{M}%
\nolimits_{n,m}\left(  \mathbb{K}\right)  $.)

For given $n\in\mathbb{N}$ and $m\in\mathbb{N}$, we define the $n\times
m$\textit{ zero matrix} to be the $n\times m$-matrix whose all entries are $0$
(that is, the $n\times m$-matrix $\left(  0\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$). We denote this matrix by $0_{n\times m}$. If $A$ is any $n\times
m$-matrix, then the $n\times m$-matrix $-A$ is defined to be $0_{n\times m}-A$.

A \textit{square matrix} is a matrix which has as many rows as it has columns;
in other words, a square matrix is an $n\times n$-matrix for some
$n\in\mathbb{N}$. If $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ is a square matrix, then the $n$-tuple $\left(  a_{1,1},a_{2,2}%
,\ldots,a_{n,n}\right)  $ is called the \textit{diagonal} of $A$. (Some
authors abbreviate $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
by $\left(  a_{i,j}\right)  _{1\leq i,j\leq n}$; this notation has some mild
potential for confusion, though\footnote{The comma between \textquotedblleft%
$i$\textquotedblright\ and \textquotedblleft$j$\textquotedblright\ in
\textquotedblleft$1\leq i,j\leq n$\textquotedblright\ can be understood either
to separate $i$ from $j$, or to separate the inequality $1\leq i$ from the
inequality $j\leq n$. I remember seeing this ambiguity causing a real
misunderstanding.}.) The entries of the diagonal of $A$ are called the
\textit{diagonal entries} of $A$.

For a given $n\in\mathbb{N}$, the product of two $n\times n$-matrices is
always well-defined, and is an $n\times n$-matrix again. The set
$\mathbb{K}^{n\times n}$ satisfies all the axioms of a commutative ring except
for commutativity of multiplication. This makes it into what is commonly
called a \textit{noncommutative ring}\footnote{A \textit{noncommutative ring}
is defined in the same way as we defined a commutative ring, except for the
fact that commutativity of multiplication is removed from the list of axioms.
(The words \textquotedblleft noncommutative ring\textquotedblright\ do not
imply that commutativity of multiplication must be false for this ring; they
merely say that commutativity of multiplication is \textbf{not required} to
hold for it. For example, the noncommutative ring $\mathbb{K}^{n\times n}$ is
actually commutative when $n\leq1$ or when $\mathbb{K}$ is a trivial ring.)
\par
Instead of saying \textquotedblleft noncommutative ring\textquotedblright,
many algebraists just say \textquotedblleft ring\textquotedblright. We shall,
however, keep the word \textquotedblleft noncommutative\textquotedblright\ in
order to avoid confusion.}. We shall study noncommutative rings later (in
Section \ref{sect.noncommring}).

\subsection{Determinants}

Square matrices have determinants. Let us recall how determinants are defined:

\begin{definition}
\label{def.det}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. The \textit{determinant}
$\det A$ of $A$ is defined as%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)
}a_{2,\sigma\left(  2\right)  }\cdots a_{n,\sigma\left(  n\right)  }.
\label{eq.det}%
\end{equation}
In other words,%
\begin{align}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{a_{1,\sigma\left(  1\right)  }a_{2,\sigma\left(  2\right)  }\cdots
a_{n,\sigma\left(  n\right)  }}_{=\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}}\label{eq.det.eq.1}\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }. \label{eq.det.eq.2}%
\end{align}

\end{definition}

For example, the determinant of a $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
a_{1,1}%
\end{array}
\right)  $ is%
\begin{align}
\det\left(
\begin{array}
[c]{c}%
a_{1,1}%
\end{array}
\right)   &  =\sum_{\sigma\in S_{1}}\left(  -1\right)  ^{\sigma}%
a_{1,\sigma\left(  1\right)  }=\underbrace{\left(  -1\right)
^{\operatorname*{id}}}_{=1}a_{1,1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the only permutation }\sigma\in
S_{1}\text{ is }\operatorname*{id}\right) \nonumber\\
&  =a_{1,1}. \label{eq.det.small.1x1}%
\end{align}
The determinant of a $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  $ is%
\begin{align*}
\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)   &  =\sum_{\sigma\in S_{2}}\left(  -1\right)  ^{\sigma}%
a_{1,\sigma\left(  1\right)  }a_{2,\sigma\left(  2\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}%
\underbrace{a_{1,\operatorname*{id}\left(  1\right)  }}_{=a_{1,1}%
}\underbrace{a_{2,\operatorname*{id}\left(  2\right)  }}_{=a_{2,2}%
}+\underbrace{\left(  -1\right)  ^{s_{1}}}_{=-1}\underbrace{a_{1,s_{1}\left(
1\right)  }}_{=a_{1,2}}\underbrace{a_{2,s_{1}\left(  2\right)  }}_{=a_{2,1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the only permutations }\sigma\in
S_{2}\text{ are }\operatorname*{id}\text{ and }s_{1}\right) \\
&  =a_{1,1}a_{2,2}-a_{1,2}a_{2,1}.
\end{align*}
Similarly, for a $3\times3$-matrix, the formula is%
\begin{align}
\det\left(
\begin{array}
[c]{ccc}%
a_{1,1} & a_{1,2} & a_{1,3}\\
a_{2,1} & a_{2,2} & a_{2,3}\\
a_{3,1} & a_{3,2} & a_{3,3}%
\end{array}
\right)   &  =a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}%
a_{2,1}a_{3,2}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}%
-a_{1,3}a_{2,2}a_{3,1}. \label{eq.det.small.3x3}%
\end{align}
Also, the determinant of the $0\times0$-matrix is $1$\ \ \ \ \footnote{In more
details:
\par
There is only one $0\times0$-matrix; it has no rows and no columns and no
entries. According to (\ref{eq.det.eq.2}), its determinant is
\begin{align*}
\sum_{\sigma\in S_{0}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}%
^{0}a_{i,\sigma\left(  i\right)  }}_{=\left(  \text{empty product}\right)
=1}  &  =\sum_{\sigma\in S_{0}}\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since the only }%
\sigma\in S_{0}\text{ is }\operatorname*{id}\right) \\
&  =1.
\end{align*}
}. (This might sound like hairsplitting, but being able to work with
$0\times0$-matrices simplifies some proofs by induction, because it allows one
to take $n=0$ as an induction base.)

The equality (\ref{eq.det.eq.2}) (or, equivalently, (\ref{eq.det.eq.1})) is
known as the \textit{Leibniz formula}. Out of several known ways to define the
determinant, it is probably the most direct. In practice, however, computing a
determinant using (\ref{eq.det.eq.2}) quickly becomes impractical when $n$ is
high (since the sum has $n!$ terms). In most situations that occur both
\href{http://arxiv.org/abs/math/9902004v3}{in mathematics} and
\href{https://en.wikipedia.org/wiki/Determinant#Calculation}{in applications},
determinants can be computed in various simpler ways.

Some authors write $\left\vert A\right\vert $ instead of $\det A$ for the
determinant of a square matrix $A$. I do not like this notation, as it clashes
(in the case of $1\times1$-matrices) with the notation $\left\vert
a\right\vert $ for the absolute value of a real number $a$.

Here is a first example of a determinant which ends up very simple:

\begin{example}
\label{exam.xiyj}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$
elements of $\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$ further
elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(
x_{i}y_{j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. What is $\det A$ ?

For $n=0$, we have $\det A=1$ (since the $0\times0$-matrix has determinant $1$).

For $n=1$, we have $A=\left(
\begin{array}
[c]{c}%
x_{1}y_{1}%
\end{array}
\right)  $ and thus $\det A=x_{1}y_{1}$.

For $n=2$, we have $A=\left(
\begin{array}
[c]{cc}%
x_{1}y_{1} & x_{1}y_{2}\\
x_{2}y_{1} & x_{2}y_{2}%
\end{array}
\right)  $ and thus $\det A=\left(  x_{1}y_{1}\right)  \left(  x_{2}%
y_{2}\right)  -\left(  x_{1}y_{2}\right)  \left(  x_{2}y_{1}\right)  =0$.

What do you expect for greater values of $n$ ? The pattern might not be clear
at this point yet, but if you compute further examples, you will realize that
$\det A=0$ also holds for $n=3$, for $n=4$, for $n=5$... This suggests that
$\det A=0$ for every $n\geq2$. How to prove this?

Let $n\geq2$. Then, (\ref{eq.det.eq.1}) (applied to $a_{i,j}=x_{i}y_{j}$)
yields%
\begin{align}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\left(  x_{1}y_{\sigma\left(  1\right)  }\right)  \left(
x_{2}y_{\sigma\left(  2\right)  }\right)  \cdots\left(  x_{n}y_{\sigma\left(
n\right)  }\right)  }_{=\left(  x_{1}x_{2}\cdots x_{n}\right)  \left(
y_{\sigma\left(  1\right)  }y_{\sigma\left(  2\right)  }\cdots y_{\sigma
\left(  n\right)  }\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  x_{1}x_{2}\cdots
x_{n}\right)  \underbrace{\left(  y_{\sigma\left(  1\right)  }y_{\sigma\left(
2\right)  }\cdots y_{\sigma\left(  n\right)  }\right)  }_{\substack{=y_{1}%
y_{2}\cdots y_{n}\\\text{(since }\sigma\text{ is a permutation)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  x_{1}x_{2}\cdots
x_{n}\right)  \left(  y_{1}y_{2}\cdots y_{n}\right) \nonumber\\
&  =\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\right)  \left(
x_{1}x_{2}\cdots x_{n}\right)  \left(  y_{1}y_{2}\cdots y_{n}\right)  .
\label{eq.exam.xiyj.detA1}%
\end{align}
Now, every $\sigma\in S_{n}$ is either even or odd (but not both), and thus we
have%
\begin{align*}
&  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=1\\\text{(since }%
\sigma\text{ is even)}}}+\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is
odd}}}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=-1\\\text{(since
}\sigma\text{ is odd)}}}\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}1}_{=\left(  \text{the number of even permutations }\sigma\in S_{n}\right)
\cdot1}+\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is odd}%
}}\left(  -1\right)  }_{=\left(  \text{the number of odd permutations }%
\sigma\in S_{n}\right)  \cdot\left(  -1\right)  }\\
&  =\underbrace{\left(  \text{the number of even permutations }\sigma\in
S_{n}\right)  }_{\substack{=n!/2\\\text{(by Exercise \ref{exe.ps2.2.7})}%
}}\cdot1\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the number of odd
permutations }\sigma\in S_{n}\right)  }_{\substack{=n!/2\\\text{(by Exercise
\ref{exe.ps2.2.7})}}}\cdot\left(  -1\right) \\
&  =\left(  n!/2\right)  \cdot1+\left(  n!/2\right)  \cdot\left(  -1\right)
=0.
\end{align*}
Hence, (\ref{eq.exam.xiyj.detA1}) becomes $\det A=\underbrace{\left(
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\right)  }_{=0}\left(
x_{1}x_{2}\cdots x_{n}\right)  \left(  y_{1}y_{2}\cdots y_{n}\right)  =0$, as
we wanted to prove.

We will eventually learn a simpler way to prove this.
\end{example}

\begin{example}
\label{exam.xi+yj}Here is an example similar to Example \ref{exam.xiyj}, but subtler.

Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of
$\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$ further elements of
$\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(  x_{i}+y_{j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. What is $\det A$ ?

For $n=0$, we have $\det A=1$ again.

For $n=1$, we have $A=\left(
\begin{array}
[c]{c}%
x_{1}+y_{1}%
\end{array}
\right)  $ and thus $\det A=x_{1}+y_{1}$.

For $n=2$, we have $A=\left(
\begin{array}
[c]{cc}%
x_{1}+y_{1} & x_{1}+y_{2}\\
x_{2}+y_{1} & x_{2}+y_{2}%
\end{array}
\right)  $ and thus $\det A=\left(  x_{1}+y_{1}\right)  \left(  x_{2}%
+y_{2}\right)  -\left(  x_{1}+y_{2}\right)  \left(  x_{2}+y_{1}\right)
=-\left(  y_{1}-y_{2}\right)  \left(  x_{1}-x_{2}\right)  $.

However, it turns out that for every $n\geq3$, we again have $\det A=0$. This
is harder to prove than the similar claim in Example \ref{exam.xiyj}. We will
eventually see how to do it easily, but as for now let me outline a direct
proof. (I am being rather telegraphic here; do not worry if you do not
understand the following argument, as there will be easier and more detailed
proofs below.)

From (\ref{eq.det.eq.1}), we obtain%
\begin{equation}
\det A=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
x_{1}+y_{\sigma\left(  1\right)  }\right)  \left(  x_{2}+y_{\sigma\left(
2\right)  }\right)  \cdots\left(  x_{n}+y_{\sigma\left(  n\right)  }\right)  .
\label{eq.exam.xi+yj.detA1}%
\end{equation}
If we expand the product $\left(  x_{1}+y_{\sigma\left(  1\right)  }\right)
\left(  x_{2}+y_{\sigma\left(  2\right)  }\right)  \cdots\left(
x_{n}+y_{\sigma\left(  n\right)  }\right)  $, we obtain a sum of $2^{n}$
terms:%
\begin{equation}
\left(  x_{1}+y_{\sigma\left(  1\right)  }\right)  \left(  x_{2}%
+y_{\sigma\left(  2\right)  }\right)  \cdots\left(  x_{n}+y_{\sigma\left(
n\right)  }\right)  =\sum_{I\subseteq\left[  n\right]  }\left(  \prod_{i\in
I}x_{i}\right)  \left(  \prod_{i\in\left[  n\right]  \setminus I}%
y_{\sigma\left(  i\right)  }\right)  \label{eq.exam.xi+yj.detA2}%
\end{equation}
(where $\left[  n\right]  $ means the set $\left\{  1,2,\ldots,n\right\}  $).
(To obtain a fully rigorous proof of (\ref{eq.exam.xi+yj.detA2}), apply
Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} to $a_{i}=x_{i}$ and
$b_{i}=y_{\sigma\left(  i\right)  }$.) Thus, (\ref{eq.exam.xi+yj.detA1})
becomes%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\left(  x_{1}+y_{\sigma\left(  1\right)  }\right)  \left(
x_{2}+y_{\sigma\left(  2\right)  }\right)  \cdots\left(  x_{n}+y_{\sigma
\left(  n\right)  }\right)  }_{\substack{=\sum_{I\subseteq\left[  n\right]
}\left(  \prod_{i\in I}x_{i}\right)  \left(  \prod_{i\in\left[  n\right]
\setminus I}y_{\sigma\left(  i\right)  }\right)  \\\text{(by
(\ref{eq.exam.xi+yj.detA2}))}}}\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{I\subseteq\left[
n\right]  }\left(  \prod_{i\in I}x_{i}\right)  \left(  \prod_{i\in\left[
n\right]  \setminus I}y_{\sigma\left(  i\right)  }\right) \\
&  =\sum_{I\subseteq\left[  n\right]  }\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in I}x_{i}\right)  \left(  \prod
_{i\in\left[  n\right]  \setminus I}y_{\sigma\left(  i\right)  }\right)  .
\end{align*}
We want to prove that this is $0$. In order to do so, it clearly suffices to
show that every $I\subseteq\left[  n\right]  $ satisfies%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in I}%
x_{i}\right)  \left(  \prod_{i\in\left[  n\right]  \setminus I}y_{\sigma
\left(  i\right)  }\right)  =0. \label{eq.exam.xi+yj.detA4}%
\end{equation}
So let us fix $I\subseteq\left[  n\right]  $, and try to prove
(\ref{eq.exam.xi+yj.detA4}). We must be in one of the following two cases:

\begin{description}
\item[Case 1:] The set $\left[  n\right]  \setminus I$ has at least two
elements. In this case, let us pick two distinct elements $a$ and $b$ of this
set, and split the set $S_{n}$ into disjoint two-element subsets by pairing up
every even permutation $\sigma\in S_{n}$ with the odd permutation $\sigma\circ
t_{a,b}$ (where $t_{a,b}$ is as defined in Definition \ref{def.transpos}). The
addends on the left hand side of (\ref{eq.exam.xi+yj.detA4}) corresponding to
two permutations paired up cancel out each other (because the products
$\prod_{i\in\left[  n\right]  \setminus I}y_{\sigma\left(  i\right)  }$ and
$\prod_{i\in\left[  n\right]  \setminus I}y_{\left(  \sigma\circ
t_{a,b}\right)  \left(  i\right)  }$ differ only in the order of their
factors), and thus the whole left hand side of (\ref{eq.exam.xi+yj.detA4}) is
$0$. Thus, (\ref{eq.exam.xi+yj.detA4}) is proven in Case 1.

\item[Case 2:] The set $\left[  n\right]  \setminus I$ has at most one
element. In this case, the set $I$ has at least two elements (it is here that
we use $n\geq3$). Pick two distinct elements $c$ and $d$ of $I$, and split the
set $S_{n}$ into disjoint two-element subsets by pairing up every even
permutation $\sigma\in S_{n}$ with the odd permutation $\sigma\circ t_{c,d}$.
Again, the addends on the left hand side of (\ref{eq.exam.xi+yj.detA4})
corresponding to two permutations paired up cancel out each other (because the
products $\prod_{i\in\left[  n\right]  \setminus I}y_{\sigma\left(  i\right)
}$ and $\prod_{i\in\left[  n\right]  \setminus I}y_{\left(  \sigma\circ
t_{c,d}\right)  \left(  i\right)  }$ are identical), and thus the whole left
hand side of (\ref{eq.exam.xi+yj.detA4}) is $0$. This proves
(\ref{eq.exam.xi+yj.detA4}) in Case 2.
\end{description}

We thus have proven (\ref{eq.exam.xi+yj.detA4}) in both cases. So $\det A=0$
is proven. This was a tricky argument, and shows the limits of the usefulness
of (\ref{eq.det.eq.1}).
\end{example}

We shall now discuss basic properties of the determinant.

\begin{exercise}
\label{exe.ps4.3}Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ be an $n\times n$-matrix. Assume that $a_{i,j}=0$ for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$. Show
that%
\[
\det A=a_{1,1}a_{2,2}\cdots a_{n,n}.
\]

\end{exercise}

\begin{definition}
An $n\times n$-matrix $A$ satisfying the assumption of Exercise
\ref{exe.ps4.3} is said to be \textit{lower-triangular} (because its entries
above the main diagonal are $0$, and thus its nonzero entries are concentrated
in the triangular region southwest of the main diagonal). Exercise
\ref{exe.ps4.3} thus says that the determinant of a lower-triangular matrix is
the product of its diagonal entries. For instance, $\det\left(
\begin{array}
[c]{ccc}%
a & 0 & 0\\
b & c & 0\\
d & e & f
\end{array}
\right)  =acf$.
\end{definition}

\begin{example}
Let $n\in\mathbb{N}$. The $n\times n$ identity matrix $I_{n}$ is
lower-triangular, and its diagonal entries are $1,1,\ldots,1$. Hence, Exercise
\ref{exe.ps4.3} shows that its determinant is $\det\left(  I_{n}\right)
=1\cdot1\cdot\cdots\cdot1=1$.
\end{example}

\begin{definition}
\label{def.transpose}The \textit{transpose} of a matrix $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ is defined to be the matrix
$\left(  a_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. It is denoted by
$A^{T}$. For instance, $\left(
\begin{array}
[c]{ccc}%
1 & 2 & -1\\
4 & 0 & 1
\end{array}
\right)  ^{T}=\left(
\begin{array}
[c]{cc}%
1 & 4\\
2 & 0\\
-1 & 1
\end{array}
\right)  $.
\end{definition}

\begin{remark}
I have seen various other notations for the transpose of a matrix $A$. Some of
them are $A^{t}$ (with a lower case $t$) and $^{T}A$ and $^{t}A$.
\end{remark}

\begin{exercise}
\label{exe.ps4.4}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Show
that $\det\left(  A^{T}\right)  =\det A$.
\end{exercise}

The transpose of a lower-triangular $n\times n$-matrix is an upper-triangular
$n\times n$-matrix (i.e., an $n\times n$-matrix whose entries below the main
diagonal are $0$). Thus, combining Exercise \ref{exe.ps4.3} with Exercise
\ref{exe.ps4.4}, we see that the determinant of an upper-triangular matrix is
the product of its diagonal entries.

Here is yet another simple property of determinants that follows directly from
their definition:

\begin{proposition}
\label{prop.det.scale}Let $n\in\mathbb{N}$ and $\lambda\in\mathbb{K}$. Let $A$
be an $n\times n$-matrix. Then, $\det\left(  \lambda A\right)  =\lambda
^{n}\det A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.det.scale}.]Write $A$ in the form $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, $\lambda A=\left(
\lambda a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition
of $\lambda A$). Hence, (\ref{eq.det.eq.2}) (applied to $\lambda A$ and
$\lambda a_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det\left(  \lambda A\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}\left(  \lambda a_{i,\sigma\left(
i\right)  }\right)  }_{=\lambda^{n}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\lambda^{n}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\lambda^{n}\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}=\lambda^{n}\det A.
\end{align*}
Proposition \ref{prop.det.scale} is thus proven.
\end{proof}

\Needspace{8cm}

\begin{exercise}
\label{exe.ps4.5}Let $a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p$ be elements of
$\mathbb{K}$.

\textbf{(a)} Find a simple formula for the determinant%
\[
\det\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
l & 0 & 0 & e\\
k & 0 & 0 & f\\
j & i & h & g
\end{array}
\right)  .
\]


\textbf{(b)} Find a simple formula for the determinant%
\[
\det\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
f & 0 & 0 & 0 & g\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
\ell & m & n & o & p
\end{array}
\right)  .
\]
(Do not mistake the \textquotedblleft$o$\textquotedblright\ for a
\textquotedblleft$0$\textquotedblright.)

[\textbf{Hint:} Part \textbf{(b)} is simpler than part \textbf{(a)}.]
\end{exercise}

In the next exercises, we shall talk about rows and columns; let us first make
some pedantic remarks about these notions.

If $n\in\mathbb{N}$, then an $n\times1$-matrix is said to be a \textit{column
vector} with $n$ entries\footnote{It is also called a \textit{column vector}
of size $n$.}, whereas a $1\times n$-matrix is said to be a \textit{row
vector} with $n$ entries. Column vectors and row vectors store exactly the
same kind of data (namely, $n$ elements of $\mathbb{K}$), so you might wonder
why I make a difference between them (and also why I distinguish them from
$n$-tuples of elements of $\mathbb{K}$, which also contain precisely the same
kind of data). The reason for this is that column vectors and row vectors
behave differently under matrix multiplication: For example,%
\[
\left(
\begin{array}
[c]{c}%
a\\
b
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
ac & ad\\
bc & bd
\end{array}
\right)
\]
is not the same as%
\[
\left(
\begin{array}
[c]{cc}%
a & b
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
c\\
d
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
ac+bd
\end{array}
\right)  .
\]
If we would identify column vectors with row vectors, then this would cause contradictions.

The reason to distinguish between row vectors and $n$-tuples is subtler: We
have defined row vectors only for a commutative ring $\mathbb{K}$, whereas
$n$-tuples can be made out of elements of any set. As a consequence, the sum
of two row vectors is well-defined (since row vectors are matrices and thus
can be added entry by entry), whereas the sum of two $n$-tuples is not.
Similarly, we can take the product $\lambda v$ of an element $\lambda
\in\mathbb{K}$ with a row vector $v$ (by multiplying every entry of $v$ by
$\lambda$), but such a thing does not make sense for general $n$-tuples. These
differences between row vectors and $n$-tuples, however, cause no clashes of
notation if we use the same notations for both types of object. Thus, we are
often going to identify a row vector $\left(
\begin{array}
[c]{cccc}%
a_{1} & a_{2} & \cdots & a_{n}%
\end{array}
\right)  $ with the $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{K}^{n}$. Thus, $\mathbb{K}^{n}$ becomes the set of all row vectors
with $n$ entries.\ \ \ \ \footnote{Some algebraists, instead, identify column
vectors with $n$-tuples, so that $\mathbb{K}^{n}$ is then the set of all
column vectors with $n$ entries. This is a valid convention as well, but one
must be careful not to use it simultaneously with the other convention (i.e.,
with the convention that row vectors are identified with $n$-tuples); this is
why we will not use it.}

The column vectors with $n$ entries are in 1-to-1 correspondence with the row
vectors with $n$ entries, and this correspondence is given by taking the
transpose: The column vector $v$ corresponds to the row vector $v^{T}$, and
conversely, the row vector $w$ corresponds to the column vector $w^{T}$. In
particular, every column vector $\left(
\begin{array}
[c]{c}%
a_{1}\\
a_{2}\\
\vdots\\
a_{n}%
\end{array}
\right)  $ can be rewritten in the form $\left(
\begin{array}
[c]{cccc}%
a_{1} & a_{2} & \cdots & a_{n}%
\end{array}
\right)  ^{T}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  ^{T}$. We shall often
write it in the latter form, just because it takes up less space on paper.

The rows of a matrix are row vectors; the columns of a matrix are column
vectors. Thus, terms like \textquotedblleft the sum of two rows of a matrix
$A$\textquotedblright\ or \textquotedblleft$-3$ times a column of a matrix
$A$\textquotedblright\ make sense: Rows and columns are vectors, and thus can
be added (when they have the same number of entries) and multiplied by
elements of $\mathbb{K}$.

Let $n\in\mathbb{N}$ and $j\in\left\{  1,2,\ldots,n\right\}  $. If $v$ is a
column vector with $n$ entries (that is, an $n\times1$-matrix), then the
$j$\textit{-th entry of }$v$ means the $\left(  j,1\right)  $-th entry of $v$.
If $v$ is a row vector with $n$ entries (that is, a $1\times n$-matrix), then
the $j$\textit{-th entry of }$v$ means the $\left(  1,j\right)  $-th entry of
$v$. For example, the $2$-nd entry of the row vector $\left(
\begin{array}
[c]{ccc}%
a & b & c
\end{array}
\right)  $ is $b$.

\begin{exercise}
\label{exe.ps4.6}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Prove
the following:

\textbf{(a)} If $B$ is an $n\times n$-matrix obtained from $A$ by switching
two rows, then $\det B=-\det A$. (\textquotedblleft Switching two
rows\textquotedblright\ means \textquotedblleft switching two distinct
rows\textquotedblright, of course.)

\textbf{(b)} If $B$ is an $n\times n$-matrix obtained from $A$ by switching
two columns, then $\det B=-\det A$.

\textbf{(c)} If a row of $A$ consists of zeroes, then $\det A=0$.

\textbf{(d)} If a column of $A$ consists of zeroes, then $\det A=0$.

\textbf{(e)} If $A$ has two equal rows, then $\det A=0$.

\textbf{(f)} If $A$ has two equal columns, then $\det A=0$.

\textbf{(g)} Let $\lambda\in\mathbb{K}$ and $k\in\left\{  1,2,\ldots
,n\right\}  $. If $B$ is the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th row by $\lambda$ (that is, multiplying every entry of
the $k$-th row by $\lambda$), then $\det B=\lambda\det A$.

\textbf{(h)} Let $\lambda\in\mathbb{K}$ and $k\in\left\{  1,2,\ldots
,n\right\}  $. If $B$ is the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th column by $\lambda$, then $\det B=\lambda\det A$.

\textbf{(i)} Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $A^{\prime}$ be an
$n\times n$-matrix whose rows equal the corresponding rows of $A$ except
(perhaps) the $k$-th row. Let $B$ be the $n\times n$-matrix obtained from $A$
by adding the $k$-th row of $A^{\prime}$ to the $k$-th row of $A$ (that is, by
adding every entry of the $k$-th row of $A^{\prime}$ to the corresponding
entry of the $k$-th row of $A$). Then, $\det B=\det A+\det A^{\prime}$.

\textbf{(j)} Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $A^{\prime}$ be an
$n\times n$-matrix whose columns equal the corresponding columns of $A$ except
(perhaps) the $k$-th column. Let $B$ be the $n\times n$-matrix obtained from
$A$ by adding the $k$-th column of $A^{\prime}$ to the $k$-th column of $A$.
Then, $\det B=\det A+\det A^{\prime}$.
\end{exercise}

\begin{example}
Let me visualize Exercise \ref{exe.ps4.6} \textbf{(i)} on an example, as it
has a somewhat daunting statement.

Set $n=3$ and $k=2$. Set $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $. Then, a matrix $A^{\prime}$ satisfying the conditions of Exercise
\ref{exe.ps4.6} \textbf{(i)} has the form $A^{\prime}=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d^{\prime} & e^{\prime} & f^{\prime}\\
g & h & i
\end{array}
\right)  $. For such a matrix $A^{\prime}$, we obtain $B=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d+d^{\prime} & e+e^{\prime} & f+f^{\prime}\\
g & h & i
\end{array}
\right)  $. Exercise \ref{exe.ps4.6} \textbf{(i)} then claims that $\det
B=\det A+\det A^{\prime}$.
\end{example}

Parts \textbf{(a)}, \textbf{(c)}, \textbf{(e)}, \textbf{(g)} and \textbf{(i)}
of Exercise \ref{exe.ps4.6} are often united under the slogan
\textquotedblleft the determinant of a matrix is multilinear and alternating
in its rows\textquotedblright\footnote{Specifically, parts \textbf{(c)},
\textbf{(g)} and \textbf{(i)} say that it is \textquotedblleft
multilinear\textquotedblright, while parts \textbf{(a)} and \textbf{(e)} are
responsible for the \textquotedblleft alternating\textquotedblright.}.
Similarly, parts \textbf{(b)}, \textbf{(d)}, \textbf{(f)}, \textbf{(h)} and
\textbf{(j)} are combined under the slogan \textquotedblleft the determinant
of a matrix is multilinear and alternating in its columns\textquotedblright.
Many texts on linear algebra (for example, \cite{HoffmanKunze}) use these
properties as the \textbf{definition} of the determinant\footnote{More
precisely, they define a \textit{determinant function} to be a function
$F:\mathbb{K}^{n\times n}\rightarrow\mathbb{K}$ which is multilinear and
alternating in the rows of a matrix (i.e., which satisfies parts \textbf{(a)},
\textbf{(c)}, \textbf{(e)}, \textbf{(g)} and \textbf{(i)} of Exercise
\ref{exe.ps4.6} if every appearance of \textquotedblleft$\det$%
\textquotedblright\ is replaced by \textquotedblleft$F$\textquotedblright\ in
this Exercise) and which satisfies $F\left(  I_{n}\right)  =1$. Then, they
show that there is (for each $n\in\mathbb{N}$) exactly one determinant
function $F:\mathbb{K}^{n\times n}\rightarrow\mathbb{K}$. They then denote
this function by $\det$. This is a rather slick definition of a determinant,
but it has the downside that it requires showing that there is exactly one
determinant function (which is often not easier than our approach).}; this is
a valid approach, but I prefer to use Definition \ref{def.det} instead, since
it is more explicit.

\begin{exercise}
\label{exe.ps4.6k}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Prove the following:

\textbf{(a)} If we add a scalar multiple of a row of $A$ to another row of
$A$, then the determinant of $A$ does not change. (A \textit{scalar multiple}
of a row vector $v$ means a row vector of the form $\lambda v$, where
$\lambda\in\mathbb{K}$.)

\textbf{(b)} If we add a scalar multiple of a column of $A$ to another column
of $A$, then the determinant of $A$ does not change. (A \textit{scalar
multiple} of a column vector $v$ means a column vector of the form $\lambda
v$, where $\lambda\in\mathbb{K}$.)
\end{exercise}

\begin{example}
Let us visualize Exercise \ref{exe.ps4.6k} \textbf{(a)}. Set $n=3$ and
$A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $. If we add $-2$ times the second row of $A$ to the first row of
$A$, then we obtain the matrix $\left(
\begin{array}
[c]{ccc}%
a+\left(  -2\right)  d & b+\left(  -2\right)  e & c+\left(  -2\right)  f\\
d & e & f\\
g & h & i
\end{array}
\right)  $. Exercise \ref{exe.ps4.6k} \textbf{(a)} now claims that this new
matrix has the same determinant as $A$ (because $-2$ times the second row of
$A$ is a scalar multiple of the second row of $A$).

Notice the word \textquotedblleft another\textquotedblright\ in Exercise
\ref{exe.ps4.6k}. Adding a scalar multiple of a row of $A$ to \textbf{the
same} row of $A$ will likely change the determinant.
\end{example}

\Needspace{20\baselineskip}

\begin{remark}
\label{exam.xi+yj.2}Exercise \ref{exe.ps4.6k} lets us prove the claim of
Example \ref{exam.xi+yj} in a much simpler way.

Namely, let $n$ and $x_{1},x_{2},\ldots,x_{n}$ and $y_{1},y_{2},\ldots,y_{n}$
and $A$ be as in Example \ref{exam.xi+yj}. Assume that $n\geq3$. We want to
show that $\det A=0$.

The matrix $A$ has at least three rows (since $n\geq3$), and looks as follows:%
\[
A=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & x_{1}+y_{2} & x_{1}+y_{3} & x_{1}+y_{4} & \cdots & x_{1}+y_{n}\\
x_{2}+y_{1} & x_{2}+y_{2} & x_{2}+y_{3} & x_{2}+y_{4} & \cdots & x_{2}+y_{n}\\
x_{3}+y_{1} & x_{3}+y_{2} & x_{3}+y_{3} & x_{3}+y_{4} & \cdots & x_{3}+y_{n}\\
x_{4}+y_{1} & x_{4}+y_{2} & x_{4}+y_{3} & x_{4}+y_{4} & \cdots & x_{4}+y_{n}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & x_{n}+y_{2} & x_{n}+y_{3} & x_{n}+y_{4} & \cdots & x_{n}+y_{n}%
\end{array}
\right)
\]
(where the presence of terms like $x_{4}$ and $y_{4}$ does not mean that the
variables $x_{4}$ and $y_{4}$ exist, in the same way as one can write
\textquotedblleft$x_{1},x_{2},\ldots,x_{k}$\textquotedblright\ even if $k=1$
or $k=0$). Thus, if we subtract the first row of $A$ from the second row of
$A$, then we obtain the matrix%
\[
A^{\prime}=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & x_{1}+y_{2} & x_{1}+y_{3} & x_{1}+y_{4} & \cdots & x_{1}+y_{n}\\
x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & \cdots & x_{2}-x_{1}\\
x_{3}+y_{1} & x_{3}+y_{2} & x_{3}+y_{3} & x_{3}+y_{4} & \cdots & x_{3}+y_{n}\\
x_{4}+y_{1} & x_{4}+y_{2} & x_{4}+y_{3} & x_{4}+y_{4} & \cdots & x_{4}+y_{n}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & x_{n}+y_{2} & x_{n}+y_{3} & x_{n}+y_{4} & \cdots & x_{n}+y_{n}%
\end{array}
\right)
\]
(because $\left(  x_{2}+y_{j}\right)  -\left(  x_{1}+y_{j}\right)
=x_{2}-x_{1}$ for every $j$). The transformation we just did (subtracting a
row from another row) does not change the determinant of the matrix (by
Exercise \ref{exe.ps4.6k} \textbf{(a)}, because subtracting a row from another
row is tantamount to adding the $\left(  -1\right)  $-multiple of the former
row to the latter), and thus we have $\det A^{\prime}=\det A$.

We notice that each entry of the second row of $A^{\prime}$ equals
$x_{2}-x_{1}$.

Next, we subtract the first row of $A^{\prime}$ from the third row of
$A^{\prime}$, and obtain the matrix%
\[
A^{\prime\prime}=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & x_{1}+y_{2} & x_{1}+y_{3} & x_{1}+y_{4} & \cdots & x_{1}+y_{n}\\
x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & x_{2}-x_{1} & \cdots & x_{2}-x_{1}\\
x_{3}-x_{1} & x_{3}-x_{1} & x_{3}-x_{1} & x_{3}-x_{1} & \cdots & x_{3}-x_{1}\\
x_{4}+y_{1} & x_{4}+y_{2} & x_{4}+y_{3} & x_{4}+y_{4} & \cdots & x_{4}+y_{n}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & x_{n}+y_{2} & x_{n}+y_{3} & x_{n}+y_{4} & \cdots & x_{n}+y_{n}%
\end{array}
\right)  .
\]
Again, the determinant is unchanged (because of Exercise \ref{exe.ps4.6k}
\textbf{(a)}), so we have $\det A^{\prime\prime}=\det A^{\prime}=\det A$.

We notice that each entry of the second row of $A^{\prime\prime}$ equals
$x_{2}-x_{1}$ (indeed, these entries have been copied over from $A^{\prime}$),
and that each entry of the third row of $A^{\prime\prime}$ equals $x_{3}%
-x_{1}$.

Next, we subtract the first column of $A^{\prime\prime}$ from each of the
other columns of $A^{\prime\prime}$. This gives us the matrix%
\begin{equation}
A^{\prime\prime\prime}=\left(
\begin{array}
[c]{cccccc}%
x_{1}+y_{1} & y_{2}-y_{1} & y_{3}-y_{1} & y_{4}-y_{1} & \cdots & y_{n}-y_{1}\\
x_{2}-x_{1} & 0 & 0 & 0 & \cdots & 0\\
x_{3}-x_{1} & 0 & 0 & 0 & \cdots & 0\\
x_{4}+y_{1} & y_{2}-y_{1} & y_{3}-y_{1} & y_{4}-y_{1} & \cdots & y_{n}-y_{1}\\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n}+y_{1} & y_{2}-y_{1} & y_{3}-y_{1} & y_{4}-y_{1} & \cdots & y_{n}-y_{1}%
\end{array}
\right)  . \label{eq.exam.xi+yj.2.4}%
\end{equation}
This step, again, has not changed the determinant (because Exercise
\ref{exe.ps4.6k} \textbf{(b)} shows that subtracting a column from another
column does not change the determinant, and what we did was doing $n-1$ such
transformations). Thus, $\det A^{\prime\prime\prime}=\det A^{\prime\prime
}=\det A$.

Now, let us write the matrix $A^{\prime\prime\prime}$ in the form
$A^{\prime\prime\prime}=\left(  a_{i,j}^{\prime\prime\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. (Thus, $a_{i,j}^{\prime\prime\prime}$ is the
$\left(  i,j\right)  $-th entry of $A^{\prime\prime\prime}$ for every $\left(
i,j\right)  $.) Then, (\ref{eq.det.eq.1}) (applied to $A^{\prime\prime\prime}$
instead of $A$) yields%
\begin{equation}
\det A^{\prime\prime\prime}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime}a_{2,\sigma\left(
2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(  n\right)  }%
^{\prime\prime\prime}. \label{eq.exam.xi+yj.2.5}%
\end{equation}
I claim that
\begin{equation}
a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime}a_{2,\sigma\left(
2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(  n\right)  }%
^{\prime\prime\prime}=0\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\label{eq.exam.xi+yj.2.6}%
\end{equation}


\textit{Proof of (\ref{eq.exam.xi+yj.2.6}):} Let $\sigma\in S_{n}$. Then,
$\sigma$ is injective, and thus $\sigma\left(  2\right)  \neq\sigma\left(
3\right)  $. Therefore, at least one of the integers $\sigma\left(  2\right)
$ and $\sigma\left(  3\right)  $ must be $\neq1$ (because otherwise, we would
have $\sigma\left(  2\right)  =1=\sigma\left(  3\right)  $, contradicting
$\sigma\left(  2\right)  \neq\sigma\left(  3\right)  $). We WLOG assume that
$\sigma\left(  2\right)  \neq1$. But a look at (\ref{eq.exam.xi+yj.2.4})
reveals that all entries of the second row of $A^{\prime\prime\prime}$ are
zero except for the first entry. Thus, $a_{2,j}^{\prime\prime\prime}=0$ for
every $j\neq1$. Applied to $j=\sigma\left(  2\right)  $, this yields
$a_{2,\sigma\left(  2\right)  }^{\prime\prime\prime}=0$ (since $\sigma\left(
2\right)  \neq1$). Hence, $a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime
}a_{2,\sigma\left(  2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(
n\right)  }^{\prime\prime\prime}=0$ (because if $0$ appears as a factor in a
product, then the whole product must be $0$). This proves
(\ref{eq.exam.xi+yj.2.6}).

Now, (\ref{eq.exam.xi+yj.2.5}) becomes%
\[
\det A^{\prime\prime\prime}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{a_{1,\sigma\left(  1\right)  }^{\prime\prime\prime}%
a_{2,\sigma\left(  2\right)  }^{\prime\prime\prime}\cdots a_{n,\sigma\left(
n\right)  }^{\prime\prime\prime}}_{\substack{=0\\\text{(by
(\ref{eq.exam.xi+yj.2.6}))}}}=\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}0=0.
\]
Compared with $\det A^{\prime\prime\prime}=\det A$, this yields $\det A=0$.
Thus, $\det A=0$ is proven again.
\end{remark}

\begin{remark}
\label{exam.xmax}Here is another example for the use of Exercise
\ref{exe.ps4.6k}.

Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of
$\mathbb{K}$. Let $A$ be the matrix $\left(  x_{\max\left\{  i,j\right\}
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. (Recall that $\max S$ denotes the
greatest element of a nonempty set $S$.)

For example, if $n=4$, then%
\[
A=\left(
\begin{array}
[c]{cccc}%
x_{1} & x_{2} & x_{3} & x_{4}\\
x_{2} & x_{2} & x_{3} & x_{4}\\
x_{3} & x_{3} & x_{3} & x_{4}\\
x_{4} & x_{4} & x_{4} & x_{4}%
\end{array}
\right)  .
\]


We want to find $\det A$. First, let us subtract the first row of $A$ from
each of the other rows of $A$. Thus we obtain a new matrix $A^{\prime}$. The
determinant has not changed (according to Exercise \ref{exe.ps4.6k}
\textbf{(a)}); i.e., we have $\det A^{\prime}=\det A$. Here is how $A^{\prime
}$ looks like in the case when $n=4$:%
\begin{equation}
A^{\prime}=\left(
\begin{array}
[c]{cccc}%
x_{1} & x_{2} & x_{3} & x_{4}\\
x_{2}-x_{1} & 0 & 0 & 0\\
x_{3}-x_{1} & x_{3}-x_{2} & 0 & 0\\
x_{4}-x_{1} & x_{4}-x_{2} & x_{4}-x_{3} & 0
\end{array}
\right)  . \label{eq.exam.xmax.5}%
\end{equation}
Notice the many zeroes; zeroes are useful when computing determinants. To
generalize the pattern we see on (\ref{eq.exam.xmax.5}), we write the matrix
$A^{\prime}$ in the form $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ (so that $a_{i,j}^{\prime}$ is the $\left(
i,j\right)  $-th entry of $A^{\prime}$ for every $\left(  i,j\right)  $).
Then, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we have%
\begin{equation}
a_{i,j}^{\prime}=\left\{
\begin{array}
[c]{c}%
x_{\max\left\{  i,j\right\}  },\ \ \ \ \ \ \ \ \ \ \text{if }i=1;\\
x_{\max\left\{  i,j\right\}  }-x_{\max\left\{  1,j\right\}  }%
,\ \ \ \ \ \ \ \ \ \ \text{if }i>1
\end{array}
\right.  \label{eq.exam.xmax.4}%
\end{equation}
(since we obtained the matrix $A^{\prime}$ by subtracting the first row of $A$
from each of the other rows of $A$). Hence, for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $1<i\leq j$, we have%
\begin{align}
a_{i,j}^{\prime}  &  =x_{\max\left\{  i,j\right\}  }-x_{\max\left\{
1,j\right\}  }=x_{j}-x_{j}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\max\left\{  i,j\right\}  =j\text{ (because }i\leq j\text{)}\\
\text{and }\max\left\{  1,j\right\}  =j\text{ (because }1<j\text{)}%
\end{array}
\right) \nonumber\\
&  =0. \label{eq.exam.xmax.6}%
\end{align}
This is the general explanation for the six $0$'s in (\ref{eq.exam.xmax.5}).
We notice also that the first row of the matrix $A^{\prime}$ is $\left(
x_{1},x_{2},\ldots,x_{n}\right)  $.

Now, we want to transform $A^{\prime}$ further. Namely, we first switch the
first row with the second row; then we switch the second row (which used to be
the first row) with the third row; then, the third row with the fourth row,
and so on, until we finally switch the $\left(  n-1\right)  $-th row with the
$n$-th row. As a result of these $n-1$ switches, the first row has moved all
the way down to the bottom, past all the other rows. We denote the resulting
matrix by $A^{\prime\prime}$. For instance, if $n=4$, then%
\begin{equation}
A^{\prime\prime}=\left(
\begin{array}
[c]{cccc}%
x_{2}-x_{1} & 0 & 0 & 0\\
x_{3}-x_{1} & x_{3}-x_{2} & 0 & 0\\
x_{4}-x_{1} & x_{4}-x_{2} & x_{4}-x_{3} & 0\\
x_{1} & x_{2} & x_{3} & x_{4}%
\end{array}
\right)  . \label{eq.exam.xmax.9}%
\end{equation}
This is a lower-triangular matrix. To see that this holds in the general case,
we write the matrix $A^{\prime\prime}$ in the form $A^{\prime\prime}=\left(
a_{i,j}^{\prime\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (so that
$a_{i,j}^{\prime\prime}$ is the $\left(  i,j\right)  $-th entry of
$A^{\prime\prime}$ for every $\left(  i,j\right)  $). Then, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, we have%
\begin{equation}
a_{i,j}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
a_{i+1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i<n;\\
a_{1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i=n
\end{array}
\right.  \label{eq.exam.xmax.10}%
\end{equation}
(because the first row of $A^{\prime}$ has become the $n$-th row of
$A^{\prime\prime}$, whereas every other row has moved up one step). In
particular, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}$ satisfying $1\leq i<j\leq n$, we have%
\begin{align*}
a_{i,j}^{\prime\prime}  &  =\left\{
\begin{array}
[c]{c}%
a_{i+1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i<n;\\
a_{1,j}^{\prime},\ \ \ \ \ \ \ \ \ \ \text{if }i=n
\end{array}
\right.  =a_{i+1,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<j\leq
n\right) \\
&  =0\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.exam.xmax.6}), applied to }i+1\text{ instead of }i\\
\text{(because }i<j\text{ yields }i+1\leq j\text{)}%
\end{array}
\right)  .
\end{align*}
This shows that $A^{\prime\prime}$ is indeed lower-triangular. Hence, Exercise
\ref{exe.ps4.3} (applied to $A^{\prime\prime}$ and $a_{i,j}^{\prime\prime}$
instead of $A$ and $a_{i,j}$) shows that $\det A^{\prime\prime}=a_{1,1}%
^{\prime\prime}a_{2,2}^{\prime\prime}\cdots a_{n,n}^{\prime\prime}$.

Using (\ref{eq.exam.xmax.10}) and (\ref{eq.exam.xmax.4}), it is easy to see
that every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
a_{i,i}^{\prime\prime}=\left\{
\begin{array}
[c]{c}%
x_{i+1}-x_{i},\ \ \ \ \ \ \ \ \ \ \text{if }i<n;\\
x_{n},\ \ \ \ \ \ \ \ \ \ \text{if }i=n
\end{array}
\right.  . \label{eq.exam.xmax.15}%
\end{equation}
(This is precisely the pattern you would guess from the diagonal entries in
(\ref{eq.exam.xmax.9}).) Now, multiplying the equalities
(\ref{eq.exam.xmax.15}) for all $i\in\left\{  1,2,\ldots,n\right\}  $, we
obtain $a_{1,1}^{\prime\prime}a_{2,2}^{\prime\prime}\cdots a_{n,n}%
^{\prime\prime}=\left(  x_{2}-x_{1}\right)  \left(  x_{3}-x_{2}\right)
\cdots\left(  x_{n}-x_{n-1}\right)  x_{n}$. Thus,%
\begin{equation}
\det A^{\prime\prime}=a_{1,1}^{\prime\prime}a_{2,2}^{\prime\prime}\cdots
a_{n,n}^{\prime\prime}=\left(  x_{2}-x_{1}\right)  \left(  x_{3}-x_{2}\right)
\cdots\left(  x_{n}-x_{n-1}\right)  x_{n}. \label{eq.exam.xmax.17}%
\end{equation}


But we want $\det A$, not $\det A^{\prime\prime}$. First, let us find $\det
A^{\prime}$. Recall that $A^{\prime\prime}$ was obtained from $A^{\prime}$ by
switching rows, repeatedly -- namely, $n-1$ times. Every time we switch two
rows in a matrix, its determinant gets multiplied by $-1$ (because of Exercise
\ref{exe.ps4.6} \textbf{(a)}). Hence, $n-1$ such switches cause the
determinant to be multiplied by $\left(  -1\right)  ^{n-1}$. Since
$A^{\prime\prime}$ was obtained from $A^{\prime}$ by $n-1$ such switches, we
thus conclude that $\det A^{\prime\prime}=\left(  -1\right)  ^{n-1}\det
A^{\prime}$, so that%
\begin{align*}
\det A^{\prime}  &  =\underbrace{\dfrac{1}{\left(  -1\right)  ^{n-1}}%
}_{=\left(  -1\right)  ^{n-1}}\underbrace{\det A^{\prime\prime}}_{=\left(
x_{2}-x_{1}\right)  \left(  x_{3}-x_{2}\right)  \cdots\left(  x_{n}%
-x_{n-1}\right)  x_{n}}\\
&  =\left(  -1\right)  ^{n-1}\left(  x_{2}-x_{1}\right)  \left(  x_{3}%
-x_{2}\right)  \cdots\left(  x_{n}-x_{n-1}\right)  x_{n}.
\end{align*}


Finally, recall that $\det A^{\prime}=\det A$, so that%
\[
\det A=\det A^{\prime}=\left(  -1\right)  ^{n-1}\left(  x_{2}-x_{1}\right)
\left(  x_{3}-x_{2}\right)  \cdots\left(  x_{n}-x_{n-1}\right)  x_{n}.
\]

\end{remark}

\subsection{$\det\left(  AB\right)  $}

Next, a lemma that will come handy in a more important proof:

\begin{lemma}
\label{lem.det.sigma}Let $n\in\mathbb{N}$. Let $\left[  n\right]  $ denote the
set $\left\{  1,2,\ldots,n\right\}  $. Let $\kappa:\left[  n\right]
\rightarrow\left[  n\right]  $ be a map. Let $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $B_{\kappa}$
be the $n\times n$-matrix $\left(  b_{\kappa\left(  i\right)  ,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

\textbf{(a)} If $\kappa\in S_{n}$, then $\det\left(  B_{\kappa}\right)
=\left(  -1\right)  ^{\kappa}\cdot\det B$.

\textbf{(b)} If $\kappa\notin S_{n}$, then $\det\left(  B_{\kappa}\right)  =0$.
\end{lemma}

\begin{remark}
Lemma \ref{lem.det.sigma} \textbf{(a)} simply says that if we permute the rows
of a square matrix, then its determinant gets multiplied by the sign of the
permutation used. For instance, let $n=3$ and $B=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $. If $\kappa$ is the permutation $\left(  2,3,1\right)  $ (in
one-line notation), then $B_{\kappa}=\left(
\begin{array}
[c]{ccc}%
d & e & f\\
g & h & i\\
a & b & c
\end{array}
\right)  $, and Lemma \ref{lem.det.sigma} \textbf{(a)} says that $\det\left(
B_{\kappa}\right)  =\underbrace{\left(  -1\right)  ^{\kappa}}_{=1}\cdot\det
B=\det B$.

Of course, a similar result holds for permutations of columns.
\end{remark}

\begin{remark}
Exercise \ref{exe.ps4.6} \textbf{(a)} is a particular case of Lemma
\ref{lem.det.sigma} \textbf{(a)}. Indeed, if $B$ is an $n\times n$-matrix
obtained from $A$ by switching the $u$-th and the $v$-th row (where $u$ and
$v$ are two distinct elements of $\left\{  1,2,\ldots,n\right\}  $), then
$B=\left(  a_{t_{u,v}\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ (where $A$ is written in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$).
\end{remark}

\begin{proof}
[Proof of Lemma \ref{lem.det.sigma}.]Recall that $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  $. In other words, $S_{n}$ is
the set of all permutations of $\left[  n\right]  $ (since $\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $). In other words, $S_{n}$ is the set of all
bijective maps $\left[  n\right]  \rightarrow\left[  n\right]  $.

\textbf{(a)} Assume that $\kappa\in S_{n}$. We define a map $\Phi
:S_{n}\rightarrow S_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma\circ\kappa\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in S_{n}.
\]
We also define a map $\Psi:S_{n}\rightarrow S_{n}$ by%
\[
\Psi\left(  \sigma\right)  =\sigma\circ\kappa^{-1}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\]
The maps $\Phi$ and $\Psi$ are mutually inverse\footnote{\textit{Proof.} Every
$\sigma\in S_{n}$ satisfies%
\begin{align*}
\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)   &  =\Psi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\sigma\circ\kappa}\right)
=\Psi\left(  \sigma\circ\kappa\right)  =\sigma\circ\underbrace{\kappa
\circ\kappa^{-1}}_{=\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\Psi\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
Thus, $\Psi\circ\Phi=\operatorname*{id}$. Similarly, $\Phi\circ\Psi
=\operatorname*{id}$. Combined with $\Psi\circ\Phi=\operatorname*{id}$, this
yields that the maps $\Phi$ and $\Psi$ are mutually inverse, qed.}. Hence, the
map $\Phi$ is a bijection.

We have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,
(\ref{eq.det.eq.2}) (applied to $B$ and $b_{i,j}$ instead of $A$ and $a_{i,j}%
$) yields%
\begin{equation}
\det B=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i=1}^{n}}_{=\prod_{i\in\left[  n\right]  }}b_{i,\sigma\left(  i\right)
}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }. \label{pf.lem.det.sigma.a.detB}%
\end{equation}


Now, $B_{\kappa}=\left(  b_{\kappa\left(  i\right)  ,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B_{\kappa}$ and
$b_{\kappa\left(  i\right)  ,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  B_{\kappa}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left[  n\right]  }%
}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\Phi\left(  \sigma\right)
}\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  } \label{pf.lem.det.sigma.a.1}%
\end{align}
(here, we have substituted $\Phi\left(  \sigma\right)  $ for $\sigma$ in the
sum, since $\Phi$ is a bijection).

But every $\sigma\in S_{n}$ satisfies $\left(  -1\right)  ^{\Phi\left(
\sigma\right)  }=\left(  -1\right)  ^{\kappa}\cdot\left(  -1\right)  ^{\sigma
}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $\Phi\left(
\sigma\right)  =\sigma\circ\kappa$, so that%
\begin{align*}
\left(  -1\right)  ^{\Phi\left(  \sigma\right)  }  &  =\left(  -1\right)
^{\sigma\circ\kappa}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)
^{\kappa}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\tau=\kappa\right) \\
&  =\left(  -1\right)  ^{\kappa}\cdot\left(  -1\right)  ^{\sigma},
\end{align*}
qed.} and $\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }$\ \ \ \ \footnote{\textit{Proof.}
Let $\sigma\in S_{n}$. We have $\Phi\left(  \sigma\right)  =\sigma\circ\kappa
$. Thus, for every $i\in\left[  n\right]  $, we have $\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  =\left(  \sigma\circ\kappa\right)
\left(  i\right)  =\sigma\left(  \kappa\left(  i\right)  \right)  $. Hence,
$\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }%
$.
\par
But $\kappa\in S_{n}$. In other words, $\kappa$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, hence a bijection from
$\left[  n\right]  $ to $\left[  n\right]  $. Therefore, we can substitute
$\kappa\left(  i\right)  $ for $i$ in the product $\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }$. We thus obtain $\prod_{i\in\left[
n\right]  }b_{i,\sigma\left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }%
$. Comparing this with $\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)  \left(  i\right)
}=\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(
\kappa\left(  i\right)  \right)  }$, we obtain $\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)
\left(  i\right)  }=\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(  i\right)
}$, qed.}. Thus, (\ref{pf.lem.det.sigma.a.1}) becomes%
\begin{align*}
\det\left(  B_{\kappa}\right)   &  =\sum_{\sigma\in S_{n}}\underbrace{\left(
-1\right)  ^{\Phi\left(  \sigma\right)  }}_{=\left(  -1\right)  ^{\kappa}%
\cdot\left(  -1\right)  ^{\sigma}}\underbrace{\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)
\left(  i\right)  }}_{=\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(
i\right)  }}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\kappa}\cdot\left(
-1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(  i\right)
}\\
&  =\left(  -1\right)  ^{\kappa}\cdot\underbrace{\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{i,\sigma\left(  i\right)
}}_{\substack{=\det B\\\text{(by (\ref{pf.lem.det.sigma.a.detB}))}}}=\left(
-1\right)  ^{\kappa}\cdot\det B.
\end{align*}
This proves Lemma \ref{lem.det.sigma} \textbf{(a)}.

\textbf{(b)} Assume that $\kappa\notin S_{n}$.

The following fact is well-known: If $U$ is a finite set, then every injective
map $U\rightarrow U$ is bijective\footnote{\textit{Proof.} Let $U$ be a finite
set, and let $f$ be an injective map $U\rightarrow U$. We must show that $f$
is bijective.
\par
Since $f$ is injective, we have $\left\vert f\left(  U\right)  \right\vert
=\left\vert U\right\vert $. Thus, $f\left(  U\right)  $ is a subset of $U$
which has size $\left\vert U\right\vert $. But the only such subset is $U$
itself (since $U$ is a finite set). Therefore, $f\left(  U\right)  $ must be
$U$ itself. In other words, the map $f$ is surjective. Hence, $f$ is bijective
(since $f$ is injective and surjective), qed.}. We can apply this to
$U=\left[  n\right]  $, and thus conclude that every injective map $\left[
n\right]  \rightarrow\left[  n\right]  $ is bijective. Therefore, if the map
$\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ were injective, then
$\kappa$ would be bijective and therefore would be an element of $S_{n}$
(since $S_{n}$ is the set of all bijective maps $\left[  n\right]
\rightarrow\left[  n\right]  $); but this would contradict the fact that
$\kappa\notin S_{n}$. Hence, the map $\kappa:\left[  n\right]  \rightarrow
\left[  n\right]  $ cannot be injective. Therefore, there exist two distinct
elements $a$ and $b$ of $\left[  n\right]  $ such that $\kappa\left(
a\right)  =\kappa\left(  b\right)  $. Consider these $a$ and $b$.

Thus, $a$ and $b$ are two distinct elements of $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Hence, a transposition $t_{a,b}\in S_{n}$ is defined
(see Definition \ref{def.transpos} for the definition). This transposition
satisfies $\kappa\circ t_{a,b}=\kappa$\ \ \ \ \footnote{\textit{Proof.} We are
going to show that every $i\in\left[  n\right]  $ satisfies $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
So let $i\in\left[  n\right]  $. We shall show that $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
The definition of $t_{a,b}$ shows that $t_{a,b}$ is the permutation in $S_{n}$
which switches $a$ with $b$ while leaving all other elements of $\left\{
1,2,\ldots,n\right\}  $ unchanged. In other words, we have $t_{a,b}\left(
a\right)  =b$, and $t_{a,b}\left(  b\right)  =a$, and $t_{a,b}\left(
j\right)  =j$ for every $j\in\left[  n\right]  \setminus\left\{  a,b\right\}
$.
\par
Now, we have $i\in\left[  n\right]  $. Thus, we are in one of the following
three cases:
\par
\textit{Case 1:} We have $i=a$.
\par
\textit{Case 2:} We have $i=b$.
\par
\textit{Case 3:} We have $i\in\left[  n\right]  \setminus\left\{  a,b\right\}
$.
\par
Let us first consider Case 1. In this case, we have $i=a$, so that $\left(
\kappa\circ t_{a,b}\right)  \left(  \underbrace{i}_{=a}\right)  =\left(
\kappa\circ t_{a,b}\right)  \left(  a\right)  =\kappa\left(
\underbrace{t_{a,b}\left(  a\right)  }_{=b}\right)  =\kappa\left(  b\right)
$. Compared with $\kappa\left(  \underbrace{i}_{=a}\right)  =\kappa\left(
a\right)  =\kappa\left(  b\right)  $, this yields $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $. Thus, $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ is
proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $i=b$, so that $\left(
\kappa\circ t_{a,b}\right)  \left(  \underbrace{i}_{=b}\right)  =\left(
\kappa\circ t_{a,b}\right)  \left(  b\right)  =\kappa\left(
\underbrace{t_{a,b}\left(  b\right)  }_{=a}\right)  =\kappa\left(  a\right)
=\kappa\left(  b\right)  $. Compared with $\kappa\left(  \underbrace{i}%
_{=b}\right)  =\kappa\left(  b\right)  $, this yields $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $. Thus, $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ is
proven in Case 2.
\par
Let us finally consider Case 3. In this case, we have $i\in\left[  n\right]
\setminus\left\{  a,b\right\}  $. Hence, $t_{a,b}\left(  i\right)  =i$ (since
$t_{a,b}\left(  j\right)  =j$ for every $j\in\left[  n\right]  \setminus
\left\{  a,b\right\}  $). Therefore, $\left(  \kappa\circ t_{a,b}\right)
\left(  i\right)  =\kappa\left(  \underbrace{t_{a,b}\left(  i\right)  }%
_{=i}\right)  =\kappa\left(  i\right)  $. Thus, $\left(  \kappa\circ
t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ is proven in Case
3.
\par
We now have shown $\left(  \kappa\circ t_{a,b}\right)  \left(  i\right)
=\kappa\left(  i\right)  $ in each of the three Cases 1, 2 and 3. Hence,
$\left(  \kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(
i\right)  $ always holds.
\par
Now, let us forget that we fixed $i$. We thus have shown that $\left(
\kappa\circ t_{a,b}\right)  \left(  i\right)  =\kappa\left(  i\right)  $ for
every $i\in\left[  n\right]  $. In other words, $\kappa\circ t_{a,b}=\kappa$,
qed.}. Exercise \ref{exe.ps4.1ab} \textbf{(b)} (applied to $i=a$ and $j=b$)
yields $\left(  -1\right)  ^{t_{a,b}}=-1$.

Let $A_{n}$ be the set of all even permutations in $S_{n}$. Let $C_{n}$ be the
set of all odd permutations in $S_{n}$.

We have $\sigma\circ t_{a,b}\in C_{n}$ for every $\sigma\in A_{n}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in A_{n}$. Then, $\sigma$ is an
even permutation in $S_{n}$ (since $A_{n}$ is the set of all even permutations
in $S_{n}$). Hence, $\left(  -1\right)  ^{\sigma}=1$. Now, (\ref{eq.sign.prod}%
) (applied to $\tau=t_{a,b}$) yields $\left(  -1\right)  ^{\sigma\circ
t_{a,b}}=\underbrace{\left(  -1\right)  ^{\sigma}}_{=1}\cdot
\underbrace{\left(  -1\right)  ^{t_{a,b}}}_{=-1}=-1$. Thus, the permutation
$\sigma\circ t_{a,b}$ is odd. Hence, $\sigma\circ t_{a,b}$ is an odd
permutation in $S_{n}$. In other words, $\sigma\circ t_{a,b}\in C_{n}$ (since
$C_{n}$ is the set of all odd permutations in $S_{n}$), qed.}. Hence, we can
define a map $\Phi:A_{n}\rightarrow C_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma\circ t_{a,b}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in A_{n}.
\]
Consider this map $\Phi$. Furthermore, we have $\sigma\circ\left(
t_{a,b}\right)  ^{-1}\in A_{n}$ for every $\sigma\in C_{n}$%
\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in C_{n}$. Then, $\sigma$ is an
odd permutation in $S_{n}$ (since $C_{n}$ is the set of all odd permutations
in $S_{n}$). Hence, $\left(  -1\right)  ^{\sigma}=-1$.
\par
Applying (\ref{eq.sign.inverse}) to $t_{a,b}$ instead of $\sigma$, we obtain
$\left(  -1\right)  ^{\left(  t_{a,b}\right)  ^{-1}}=\left(  -1\right)
^{t_{a,b}}=-1$. Now, (\ref{eq.sign.prod}) (applied to $\tau=\left(
t_{a,b}\right)  ^{-1}$) yields $\left(  -1\right)  ^{\sigma\circ\left(
t_{a,b}\right)  ^{-1}}=\underbrace{\left(  -1\right)  ^{\sigma}}_{=-1}%
\cdot\underbrace{\left(  -1\right)  ^{\left(  t_{a,b}\right)  ^{-1}}}%
_{=-1}=\left(  -1\right)  \cdot\left(  -1\right)  =1$. Thus, the permutation
$\sigma\circ\left(  t_{a,b}\right)  ^{-1}$ is even. Hence, $\sigma\circ\left(
t_{a,b}\right)  ^{-1}$ is an even permutation in $S_{n}$. In other words,
$\sigma\circ\left(  t_{a,b}\right)  ^{-1}\in A_{n}$ (since $A_{n}$ is the set
of all even permutations in $S_{n}$), qed.}. Thus, we can define a map
$\Psi:C_{n}\rightarrow A_{n}$ by%
\[
\Psi\left(  \sigma\right)  =\sigma\circ\left(  t_{a,b}\right)  ^{-1}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in C_{n}.
\]
Consider this map $\Psi$.

(We could have simplified our life a bit by noticing that $\left(
t_{a,b}\right)  ^{-1}=t_{a,b}$, so that the maps $\Phi$ and $\Psi$ are given
by the same formula, albeit defined on different domains. But I wanted to
demonstrate a use of (\ref{eq.sign.inverse}).)

The maps $\Phi$ and $\Psi$ are mutually inverse\footnote{\textit{Proof.} Every
$\sigma\in A_{n}$ satisfies%
\begin{align*}
\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)   &  =\Psi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\sigma\circ\tau_{a,b}}\right)
=\Psi\left(  \sigma\circ\tau_{a,b}\right)  =\sigma\circ\underbrace{\tau
_{a,b}\circ\left(  \tau_{a,b}\right)  ^{-1}}_{=\operatorname*{id}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Psi\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
Thus, $\Psi\circ\Phi=\operatorname*{id}$. Similarly, $\Phi\circ\Psi
=\operatorname*{id}$. Combined with $\Psi\circ\Phi=\operatorname*{id}$, this
yields that the maps $\Phi$ and $\Psi$ are mutually inverse, qed.}. Hence, the
map $\Psi$ is a bijection. Moreover, every $\sigma\in C_{n}$ satisfies%
\begin{equation}
\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }.
\label{pf.lem.det.sigma.b.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.sigma.b.2}):} Let $\sigma\in
C_{n}$. The map $t_{a,b}$ is a permutation of $\left[  n\right]  $, thus a
bijection $\left[  n\right]  \rightarrow\left[  n\right]  $. Hence, we can
substitute $t_{a,b}\left(  i\right)  $ for $i$ in the product $\prod
_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  i\right)  }$. Thus we obtain%
\[
\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[  n\right]
}b_{\kappa\left(  t_{a,b}\left(  i\right)  \right)  ,\left(  \Psi\left(
\sigma\right)  \right)  \left(  t_{a,b}\left(  i\right)  \right)  }%
=\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }%
\]
(since every $i\in\left[  n\right]  $ satisfies $\kappa\left(  t_{a,b}\left(
i\right)  \right)  =\underbrace{\left(  \kappa\circ t_{a,b}\right)  }%
_{=\kappa}\left(  i\right)  =\kappa\left(  i\right)  $ and
\[
\underbrace{\left(  \Psi\left(  \sigma\right)  \right)  }_{=\sigma\circ\left(
t_{a,b}\right)  ^{-1}}\left(  t_{a,b}\left(  i\right)  \right)  =\left(
\sigma\circ\left(  t_{a,b}\right)  ^{-1}\right)  \left(  t_{a,b}\left(
i\right)  \right)  =\sigma\left(  \underbrace{\left(  t_{a,b}\right)
^{-1}\left(  t_{a,b}\left(  i\right)  \right)  }_{=i}\right)  =\sigma\left(
i\right)
\]
). This proves (\ref{pf.lem.det.sigma.b.2}).}

We have $B_{\kappa}=\left(  b_{\kappa\left(  i\right)  ,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B_{\kappa}$
and $b_{\kappa\left(  i\right)  ,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det\left(  B_{\kappa}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left[  n\right]  }%
}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}}_{\substack{=\sum_{\sigma\in A_{n}}\\\text{(since }A_{n}\text{ is
the}\\\text{set of all even}\\\text{permutations}\\\text{in }S_{n}\text{)}%
}}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=1\\\text{(since
}\sigma\text{ is even)}}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\text{ is odd}}}}_{\substack{=\sum_{\sigma\in C_{n}%
}\\\text{(since }C_{n}\text{ is the}\\\text{set of all odd}%
\\\text{permutations}\\\text{in }S_{n}\text{)}}}\underbrace{\left(  -1\right)
^{\sigma}}_{\substack{=-1\\\text{(since }\sigma\text{ is odd)}}}\prod
_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)
}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every permutation }\sigma\in
S_{n}\text{ is either even or odd, but not both}\right) \\
&  =\sum_{\sigma\in A_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }+\sum_{\sigma\in C_{n}}\left(  -1\right)
\prod_{i\in\left[  n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }\\
&  =\sum_{\sigma\in A_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }-\sum_{\sigma\in C_{n}}\prod_{i\in\left[
n\right]  }b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }=0,
\end{align*}
since%
\begin{align*}
&  \sum_{\sigma\in A_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in C_{n}}\underbrace{\prod_{i\in\left[  n\right]  }%
b_{\kappa\left(  i\right)  ,\left(  \Psi\left(  \sigma\right)  \right)
\left(  i\right)  }}_{\substack{=\prod_{i\in\left[  n\right]  }b_{\kappa
\left(  i\right)  ,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.det.sigma.b.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\Psi\left(
\sigma\right)  \text{ for }\sigma\text{, since the map }\Psi\text{ is a
bijection}\right) \\
&  =\sum_{\sigma\in C_{n}}\prod_{i\in\left[  n\right]  }b_{\kappa\left(
i\right)  ,\sigma\left(  i\right)  }.
\end{align*}
This proves Lemma \ref{lem.det.sigma} \textbf{(b)}.
\end{proof}

Now let us state a basic formula for products of sums in a commutative ring:

\begin{lemma}
\label{lem.prodrule}For every $n\in\mathbb{N}$, let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$. For every $i\in\left[  n\right]  $, let $p_{i,1}%
,p_{i,2},\ldots,p_{i,m_{i}}$ be finitely many elements of $\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m_{1}\right]  \times\left[  m_{2}\right]
\times\cdots\times\left[  m_{n}\right]  }\prod_{i=1}^{n}p_{i,k_{i}}.
\]


(\textbf{Pedantic remark:} If $n=0$, then the Cartesian product $\left[
m_{1}\right]  \times\left[  m_{2}\right]  \times\cdots\times\left[
m_{n}\right]  $ has no factors; it is what is called an \textit{empty
Cartesian product}. It is understood to be a $1$-element set, and its single
element is the $0$-tuple $\left(  {}\right)  $ (also known as the empty list).)
\end{lemma}

I tend to refer to Lemma \ref{lem.prodrule} as the \textit{product rule}
(since it is related to the product rule for joint probabilities); I think it
has no really widespread name. However, it is a fundamental algebraic fact
that is used very often and tacitly (I suspect that most mathematicians have
never thought of it as being a theorem that needs to be proven). The idea
behind Lemma \ref{lem.prodrule} is that if you expand the product%
\begin{align*}
&  \prod_{i=1}^{n}\sum_{k=1}^{m_{i}}p_{i,k}\\
&  =\prod_{i=1}^{n}\left(  p_{i,1}+p_{i,2}+\cdots+p_{i,m_{i}}\right) \\
&  =\left(  p_{1,1}+p_{1,2}+\cdots+p_{1,m_{1}}\right)  \left(  p_{2,1}%
+p_{2,2}+\cdots+p_{2,m_{2}}\right)  \cdots\left(  p_{n,1}+p_{n,2}%
+\cdots+p_{n,m_{n}}\right)  ,
\end{align*}
then you get a sum of $m_{1}m_{2}\cdots m_{n}$ terms, each of which has the
form
\[
p_{1,k_{1}}p_{2,k_{2}}\cdots p_{n,k_{n}}=\prod_{i=1}^{n}p_{i,k_{i}}
\]
for some $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]
\times\left[  m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  $. (More
precisely, it is the sum of all such terms.) A formal proof of Lemma
\ref{lem.prodrule} could be obtained by induction over $n$ using the
distributivity axiom\footnote{and the observation that the $n$-tuples $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]  \times\left[
m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  $ are in bijection with
the pairs $\left(  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)  ,k_{n}\right)  $
of an $\left(  n-1\right)  $-tuple $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)
\in\left[  m_{1}\right]  \times\left[  m_{2}\right]  \times\cdots\times\left[
m_{n-1}\right]  $ and an element $k_{n}\in\left[  m_{n}\right]  $}. For the
details (if you care about them), see the solution to the following exercise:

\begin{exercise}
\label{exe.prodrule}Prove Lemma \ref{lem.prodrule}.
\end{exercise}

\begin{remark}
\label{rmk.prodrule.ai+bi}Lemma \ref{lem.prodrule} can be regarded as a
generalization of Exercise \ref{exe.prod(ai+bi)} \textbf{(a)}. Indeed, let me
sketch how Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} can be derived from
Lemma \ref{lem.prodrule}:

Let $n$, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ and $\left(  b_{1}%
,b_{2},\ldots,b_{n}\right)  $ be as in Exercise \ref{exe.prod(ai+bi)}
\textbf{(a)}. For every $i\in\left[  n\right]  $, set $m_{i}=2$,
$p_{i,1}=a_{i}$ and $p_{i,2}=b_{i}$. Then, Lemma \ref{lem.prodrule} yields%
\begin{align}
\prod_{i=1}^{n}\left(  a_{i}+b_{i}\right)   &  =\underbrace{\sum_{\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\underbrace{\left[  2\right]
\times\left[  2\right]  \times\cdots\times\left[  2\right]  }_{n\text{
factors}}}}_{=\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
2\right]  ^{n}}}\underbrace{\prod_{i=1}^{n}p_{i,k_{i}}}_{=\left(
\prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=1}}a_{i}\right)  \left(
\prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=2}}b_{i}\right)  }\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  2\right]  ^{n}%
}\left(  \prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=1}}a_{i}\right)
\left(  \prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=2}}b_{i}\right)  .
\label{eq.rmk.prodrule.ai+bi.2}%
\end{align}
But there is a bijection between the set $\left[  2\right]  ^{n}$ and the
powerset $\mathcal{P}\left(  \left[  n\right]  \right)  $ of $\left[
n\right]  $: Namely, to every $n$-tuple $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  2\right]  ^{n}$, we can assign the set $\left\{
i\in\left[  n\right]  \ \mid\ k_{i}=1\right\}  \in\mathcal{P}\left(  \left[
n\right]  \right)  $. It is easy to see that this assignment really is a
bijection $\left[  2\right]  ^{n}\rightarrow\mathcal{P}\left(  \left[
n\right]  \right)  $, and that it furthermore has the property that every
$n$-tuple $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  2\right]  ^{n}$
satisfies%
\[
\left(  \prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=1}}a_{i}\right)
\left(  \prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=2}}b_{i}\right)
=\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  n\right]
\setminus I}b_{i}\right)  ,
\]
where $I$ is the image of $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ under
this bijection. Hence,%
\begin{align*}
&  \sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  2\right]  ^{n}%
}\left(  \prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=1}}a_{i}\right)
\left(  \prod_{\substack{i\in\left[  n\right]  ;\\k_{i}=2}}b_{i}\right) \\
&  =\sum_{I\subseteq\left[  n\right]  }\left(  \prod_{i\in I}a_{i}\right)
\left(  \prod_{i\in\left[  n\right]  \setminus I}b_{i}\right)  .
\end{align*}


Hence, (\ref{eq.rmk.prodrule.ai+bi.2}) rewrites as%
\[
\prod_{i=1}^{n}\left(  a_{i}+b_{i}\right)  =\sum_{I\subseteq\left[  n\right]
}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  n\right]
\setminus I}b_{i}\right)  .
\]
But this is precisely the claim of Exercise \ref{exe.prod(ai+bi)} \textbf{(a)}.
\end{remark}

We shall use a corollary of Lemma \ref{lem.prodrule}:

\begin{lemma}
\label{lem.prodrule2}For every $n\in\mathbb{N}$, let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. For every $i\in\left[  n\right]  $,
let $p_{i,1},p_{i,2},\ldots,p_{i,m}$ be $m$ elements of $\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\kappa:\left[  n\right]
\rightarrow\left[  m\right]  }\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.prodrule2}.]For the sake of completeness, let us give
this proof.

Lemma \ref{lem.prodrule} (applied to $m_{i}=m$ for every $i\in\left[
n\right]  $) yields
\begin{equation}
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\underbrace{\left[  m\right]  \times\left[  m\right]
\times\cdots\times\left[  m\right]  }_{n\text{ factors}}}\prod_{i=1}%
^{n}p_{i,k_{i}}. \label{pf.lem.prodrule2.1}%
\end{equation}


Let $\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]  \right)
$ denote the set of all functions from $\left[  n\right]  $ to $\left[
m\right]  $. Now, let $\Phi$ be the map from $\operatorname*{Map}\left(
\left[  n\right]  ,\left[  m\right]  \right)  $ to $\underbrace{\left[
m\right]  \times\left[  m\right]  \times\cdots\times\left[  m\right]
}_{n\text{ factors}}$ given by%
\[
\Phi\left(  \kappa\right)  =\left(  \kappa\left(  1\right)  ,\kappa\left(
2\right)  ,\ldots,\kappa\left(  n\right)  \right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\kappa\in\operatorname*{Map}\left(
\left[  n\right]  ,\left[  m\right]  \right)  .
\]
So the map $\Phi$ takes a function $\kappa$ from $\left[  n\right]  $ to
$\left[  m\right]  $, and outputs the list \newline$\left(  \kappa\left(
1\right)  ,\kappa\left(  2\right)  ,\ldots,\kappa\left(  n\right)  \right)  $
of all its values. Clearly, the map $\Phi$ is injective (since a function
$\kappa\in\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]
\right)  $ can be reconstructed from the list $\left(  \kappa\left(  1\right)
,\kappa\left(  2\right)  ,\ldots,\kappa\left(  n\right)  \right)  =\Phi\left(
\kappa\right)  $) and surjective (since every list of $n$ elements of $\left[
m\right]  $ is the list of values of some function $\kappa\in
\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]  \right)  $).
Thus, $\Phi$ is bijective. Therefore, we can substitute $\Phi\left(
\kappa\right)  $ for $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ in the sum
$\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\underbrace{\left[
m\right]  \times\left[  m\right]  \times\cdots\times\left[  m\right]
}_{n\text{ factors}}}\prod_{i=1}^{n}p_{i,k_{i}}$. In other words, we can
substitute $\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)
,\ldots,\kappa\left(  n\right)  \right)  $ for $\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  $ in this sum (since $\Phi\left(  \kappa\right)
=\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)  ,\ldots
,\kappa\left(  n\right)  \right)  $ for each $\kappa\in\operatorname*{Map}%
\left(  \left[  n\right]  ,\left[  m\right]  \right)  $). We thus obtain%
\begin{align*}
\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\underbrace{\left[
m\right]  \times\left[  m\right]  \times\cdots\times\left[  m\right]
}_{n\text{ factors}}}\prod_{i=1}^{n}p_{i,k_{i}}  &  =\underbrace{\sum
_{\kappa\in\operatorname*{Map}\left(  \left[  n\right]  ,\left[  m\right]
\right)  }}_{=\sum_{\kappa:\left[  n\right]  \rightarrow\left[  m\right]  }%
}\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }\\
&  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[  m\right]  }\prod
_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\end{align*}
Thus, (\ref{pf.lem.prodrule2.1}) becomes%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\underbrace{\left[  m\right]  \times\left[  m\right]
\times\cdots\times\left[  m\right]  }_{n\text{ factors}}}\prod_{i=1}%
^{n}p_{i,k_{i}}=\sum_{\kappa:\left[  n\right]  \rightarrow\left[  m\right]
}\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\]
Lemma \ref{lem.prodrule2} is proven.
\end{proof}

Now we are ready to prove what is probably the most important property of determinants:

\begin{theorem}
\label{thm.det(AB)}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two $n\times
n$-matrices. Then,%
\[
\det\left(  AB\right)  =\det A\cdot\det B.
\]

\end{theorem}

\begin{proof}
[Proof of Theorem \ref{thm.det(AB)}.]Write $A$ and $B$ in the forms $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. The definition of $AB$ thus
yields $AB=\left(  \sum_{k=1}^{n}a_{i,k}b_{k,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Therefore, (\ref{eq.det.eq.2}) (applied to $AB$ and
$\sum_{k=1}^{n}a_{i,k}b_{k,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}\left(  \sum_{k=1}^{n}a_{i,k}b_{k,\sigma\left(
i\right)  }\right)  }_{\substack{=\sum_{\kappa:\left[  n\right]
\rightarrow\left[  n\right]  }\prod_{i=1}^{n}\left(  a_{i,\kappa\left(
i\right)  }b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }\right)
\\\text{(by Lemma \ref{lem.prodrule2}, applied to }m=n\\\text{and }%
p_{i,k}=a_{i,k}b_{k,\sigma\left(  i\right)  }\text{)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\kappa:\left[
n\right]  \rightarrow\left[  n\right]  }\underbrace{\prod_{i=1}^{n}\left(
a_{i,\kappa\left(  i\right)  }b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }\right)  }_{=\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  \left(  \prod_{i=1}^{n}b_{\kappa\left(  i\right)  ,\sigma\left(
i\right)  }\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\kappa:\left[
n\right]  \rightarrow\left[  n\right]  }\left(  \prod_{i=1}^{n}a_{i,\kappa
\left(  i\right)  }\right)  \left(  \prod_{i=1}^{n}b_{\kappa\left(  i\right)
,\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[  n\right]  }\left(
\prod_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)  \left(  \sum_{\sigma\in
S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}b_{\kappa\left(  i\right)
,\sigma\left(  i\right)  }\right)  . \label{pf.thm.det(AB).4}%
\end{align}


Now, for every $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $, we
let $B_{\kappa}$ be the $n\times n$-matrix $\left(  b_{\kappa\left(  i\right)
,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, for every $\kappa:\left[
n\right]  \rightarrow\left[  n\right]  $, the equality (\ref{eq.det.eq.2})
(applied to $B_{\kappa}$ and $b_{\kappa\left(  i\right)  ,j}$ instead of $A$
and $a_{i,j}$) yields%
\begin{equation}
\det\left(  B_{\kappa}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }.
\label{pf.thm.det(AB).5}%
\end{equation}
Thus, (\ref{pf.thm.det(AB).4}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[
n\right]  }\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)
\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}b_{\kappa\left(  i\right)  ,\sigma\left(  i\right)  }\right)
}_{\substack{=\det\left(  B_{\kappa}\right)  \\\text{(by
(\ref{pf.thm.det(AB).5}))}}}\\
&  =\sum_{\kappa:\left[  n\right]  \rightarrow\left[  n\right]  }\left(
\prod_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)  \det\left(  B_{\kappa
}\right) \\
&  =\underbrace{\sum_{\substack{\kappa:\left[  n\right]  \rightarrow\left[
n\right]  ;\\\kappa\in S_{n}}}}_{\substack{=\sum_{\kappa\in S_{n}%
}\\\text{(since every }\kappa\in S_{n}\text{ automatically}\\\text{is a map
}\left[  n\right]  \rightarrow\left[  n\right]  \text{)}}}\left(  \prod
_{i=1}^{n}a_{i,\kappa\left(  i\right)  }\right)  \underbrace{\det\left(
B_{\kappa}\right)  }_{\substack{=\left(  -1\right)  ^{\kappa}\cdot\det
B\\\text{(by Lemma \ref{lem.det.sigma} \textbf{(a)})}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{\kappa:\left[  n\right]
\rightarrow\left[  n\right]  ;\\\kappa\notin S_{n}}}\left(  \prod_{i=1}%
^{n}a_{i,\kappa\left(  i\right)  }\right)  \underbrace{\det\left(  B_{\kappa
}\right)  }_{\substack{=0\\\text{(by Lemma \ref{lem.det.sigma} \textbf{(b)})}%
}}\\
&  =\sum_{\kappa\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  \left(  -1\right)  ^{\kappa}\cdot\det B+\underbrace{\sum
_{\substack{\kappa:\left[  n\right]  \rightarrow\left[  n\right]
;\\\kappa\notin S_{n}}}\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  0}_{=0}\\
&  =\sum_{\kappa\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,\kappa\left(  i\right)
}\right)  \left(  -1\right)  ^{\kappa}\cdot\det B=\sum_{\sigma\in S_{n}%
}\left(  \prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  \left(
-1\right)  ^{\sigma}\cdot\det B\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}\kappa\text{ as }\sigma\right) \\
&  =\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=\det
A\\\text{(by (\ref{eq.det.eq.2}))}}}\cdot\det B=\det A\cdot\det B.
\end{align*}
This proves Theorem \ref{thm.det(AB)}.
\end{proof}

\begin{remark}
The analogue of Theorem \ref{thm.det(AB)} with addition instead of
multiplication does not hold. If $A$ and $B$ are two $n\times n$-matrices for
some $n\in\mathbb{N}$, then $\det\left(  A+B\right)  $ does usually
\textbf{not} equal $\det A+\det B$.
\end{remark}

We shall now show several applications of Theorem \ref{thm.det(AB)}. First, a
simple corollary:

\begin{corollary}
\label{cor.det.product}Let $n\in\mathbb{N}$.

\textbf{(a)} If $B_{1},B_{2},\ldots,B_{k}$ are finitely many $n\times
n$-matrices, then $\det\left(  B_{1}B_{2}\cdots B_{k}\right)  =\prod_{i=1}%
^{k}\det\left(  B_{i}\right)  $.

\textbf{(b)} If $B$ is any $n\times n$-matrix, and $k\in\mathbb{N}$, then
$\det\left(  B^{k}\right)  =\left(  \det B\right)  ^{k}$.
\end{corollary}

\begin{vershort}


\begin{proof}
[Proof of Corollary \ref{cor.det.product}.]Corollary \ref{cor.det.product}
easily follows from Theorem \ref{thm.det(AB)} by induction over $k$. (The
induction base, $k=0$, relies on the fact that the product of $0$ matrices is
$I_{n}$ and has determinant $\det\left(  I_{n}\right)  =1$.) We leave the
details to the reader.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Corollary \ref{cor.det.product}.]\textbf{(a)} Let $B_{1}%
,B_{2},\ldots,B_{k}$ be finitely many $n\times n$-matrices. We shall show that%
\begin{equation}
\det\left(  B_{1}B_{2}\cdots B_{u}\right)  =\prod_{i=1}^{u}\det\left(
B_{i}\right)  \label{pf.cor.det.product.a.1}%
\end{equation}
for every $u\in\left\{  0,1,\ldots,k\right\}  $.

We shall prove (\ref{pf.cor.det.product.a.1}) by induction over $u$:

\textit{Induction base:} We have $\det\left(  \underbrace{B_{1}B_{2}\cdots
B_{0}}_{=\left(  \text{empty product of }n\times n\text{-matrices}\right)
=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$ and $\prod_{i=1}^{0}\det\left(
B_{i}\right)  =\left(  \text{empty product of elements of }\mathbb{K}\right)
=1$. Hence, $\det\left(  B_{1}B_{2}\cdots B_{0}\right)  =1=\prod_{i=1}^{0}%
\det\left(  B_{i}\right)  $. In other words, (\ref{pf.cor.det.product.a.1})
holds for $u=0$. This completes the induction base.

\textit{Induction step:} Let $U\in\left\{  0,1,\ldots,k\right\}  $ be
positive. Assume that (\ref{pf.cor.det.product.a.1}) holds for $u=U-1$. We
need to show that (\ref{pf.cor.det.product.a.1}) holds for $u=U$.

We have assumed that (\ref{pf.cor.det.product.a.1}) holds for $u=U-1$. In
other words,
\[
\det\left(  B_{1}B_{2}\cdots B_{U-1}\right)  =\prod_{i=1}^{U-1}\det\left(
B_{i}\right)  .
\]
Now,%
\begin{align*}
\det\underbrace{\left(  B_{1}B_{2}\cdots B_{U}\right)  }_{=\left(  B_{1}%
B_{2}\cdots B_{U-1}\right)  B_{U}}  &  =\det\left(  \left(  B_{1}B_{2}\cdots
B_{U-1}\right)  B_{U}\right)  =\underbrace{\det\left(  B_{1}B_{2}\cdots
B_{U-1}\right)  }_{=\prod_{i=1}^{U-1}\det\left(  B_{i}\right)  }\cdot
\det\left(  B_{U}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}A=B_{1}B_{2}\cdots B_{U-1}\text{ and }B=B_{U}\right) \\
&  =\left(  \prod_{i=1}^{U-1}\det\left(  B_{i}\right)  \right)  \cdot
\det\left(  B_{U}\right)  =\prod_{i=1}^{U}\det\left(  B_{i}\right)  .
\end{align*}
In other words, (\ref{pf.cor.det.product.a.1}) holds for $u=U$. This completes
the induction step. Thus, (\ref{pf.cor.det.product.a.1}) is proven by induction.

Now, (\ref{pf.cor.det.product.a.1}) (applied to $u=k$) yields $\det\left(
B_{1}B_{2}\cdots B_{k}\right)  =\prod_{i=1}^{k}\det\left(  B_{i}\right)  $.
Corollary \ref{cor.det.product} \textbf{(a)} is thus proven.

\textbf{(b)} Let $B$ be any $n\times n$-matrix. Applying Corollary
\ref{cor.det.product} \textbf{(a)} to $B_{i}=B$, we obtain $\det\left(
\underbrace{BB\cdots B}_{k\text{ factors}}\right)  =\prod_{i=1}^{k}\det
B=\left(  \det B\right)  ^{k}$. Since $\underbrace{BB\cdots B}_{k\text{
factors}}=B^{k}$, this rewrites as $\det\left(  B^{k}\right)  =\left(  \det
B\right)  ^{k}$. Corollary \ref{cor.det.product} \textbf{(b)} is thus proven.
\end{proof}
\end{verlong}

\begin{example}
\label{exam.det(AB).fibo}Recall that the Fibonacci sequence is the sequence
$\left(  f_{0},f_{1},f_{2},\ldots\right)  $ of integers which is defined
recursively by $f_{0}=0$, $f_{1}=1$, and $f_{n}=f_{n-1}+f_{n-2}$ for all
$n\geq2$. We shall prove that%
\begin{equation}
f_{n+1}f_{n-1}-f_{n}^{2}=\left(  -1\right)  ^{n}\ \ \ \ \ \ \ \ \ \ \text{for
every positive integer }n. \label{eq.exam.det(AB).fibo.1}%
\end{equation}
(This is a classical fact known as
\href{https://en.wikipedia.org/wiki/Cassini_and_Catalan_identities}{the
\textit{Cassini identity}} and easy to prove by induction, but we shall prove
it differently to illustrate the use of determinants.)

Let $B$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  $ (over the ring $\mathbb{Z}$). It is easy to see that $\det B=-1$.
But for every positive integer $n$, we have%
\begin{equation}
B^{n}=\left(
\begin{array}
[c]{cc}%
f_{n+1} & f_{n}\\
f_{n} & f_{n-1}%
\end{array}
\right)  . \label{eq.exam.det(AB).fibo.2}%
\end{equation}
Indeed, (\ref{eq.exam.det(AB).fibo.2}) can be easily proven by induction over
$n$: For $n=1$ it is clear by inspection; if it holds for $n=N$, then for
$n=N+1$ it follows from%
\begin{align*}
B^{N+1}  &  =\underbrace{B^{N}}_{\substack{=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  \\\text{(by the induction hypothesis)}}}\underbrace{B}_{=\left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right)  }=\left(
\begin{array}
[c]{cc}%
f_{N+1} & f_{N}\\
f_{N} & f_{N-1}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
1 & 1\\
1 & 0
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
f_{N+1}\cdot1+f_{N}\cdot1 & f_{N+1}\cdot1+f_{N}\cdot0\\
f_{N}\cdot1+f_{N-1}\cdot1 & f_{N}\cdot1+f_{N-1}\cdot0
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of a product of two
matrices}\right) \\
&  =\left(
\begin{array}
[c]{cc}%
f_{N+1}+f_{N} & f_{N+1}\\
f_{N}+f_{N-1} & f_{N}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
f_{N+2} & f_{N+1}\\
f_{N+1} & f_{N}%
\end{array}
\right)
\end{align*}
(since $f_{N+1}+f_{N}=f_{N+2}$ and $f_{N}+f_{N-1}=f_{N+1}$).

Now, let $n$ be a positive integer. Then, (\ref{eq.exam.det(AB).fibo.2})
yields%
\[
\det\left(  B^{n}\right)  =\det\left(
\begin{array}
[c]{cc}%
f_{n+1} & f_{n}\\
f_{n} & f_{n-1}%
\end{array}
\right)  =f_{n+1}f_{n-1}-f_{n}^{2}.
\]
On the other hand, Corollary \ref{cor.det.product} \textbf{(b)} (applied to
$k=n$) yields $\det\left(  B^{n}\right)  =\left(  \underbrace{\det B}%
_{=-1}\right)  ^{n}=\left(  -1\right)  ^{n}$. Hence, $f_{n+1}f_{n-1}-f_{n}%
^{2}=\det\left(  B^{n}\right)  =\left(  -1\right)  ^{n}$. This proves
(\ref{eq.exam.det(AB).fibo.1}).
\end{example}

We can generalize (\ref{eq.exam.det(AB).fibo.1}) as follows:

\begin{exercise}
\label{exe.ps4.det.fibo}Let $a$ and $b$ be two complex numbers. Let $\left(
x_{0},x_{1},x_{2},\ldots\right)  $ be a sequence of complex numbers such that
every $n\geq2$ satisfies%
\begin{equation}
x_{n}=ax_{n-1}+bx_{n-2}. \label{eq.det.fibo.rec}%
\end{equation}
(We called such sequences \textquotedblleft$\left(  a,b\right)  $%
-recurrent\textquotedblright\ in Definition \ref{def.abrec}.) Let
$k\in\mathbb{N}$. Prove that
\begin{equation}
x_{n+1}x_{n-k-1}-x_{n}x_{n-k}=\left(  -b\right)  ^{n-k-1}\left(  x_{k+2}%
x_{0}-x_{k+1}x_{1}\right)  . \label{eq.det.fibo.claim}%
\end{equation}
for every integer $n>k$.
\end{exercise}

We notice that (\ref{eq.exam.det(AB).fibo.1}) can be obtained by applying
(\ref{eq.det.fibo.claim}) to $a=1$, $b=1$, $x_{i}=f_{i}$ and $k=0$. Thus,
(\ref{eq.det.fibo.claim}) is a generalization of (\ref{eq.exam.det(AB).fibo.1}%
). Notice that you could have easily come up with the identity
(\ref{eq.det.fibo.claim}) by trying to generalize the proof of
(\ref{eq.exam.det(AB).fibo.1}) we gave; in contrast, it is not that
straightforward to guess the general formula (\ref{eq.det.fibo.claim}) from
the classical proof of (\ref{eq.exam.det(AB).fibo.1}) by induction. So the
proof of (\ref{eq.exam.det(AB).fibo.1}) using determinants has at least the
advantage of pointing to a generalization.

\begin{example}
\label{exam.xiyj.3}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$
elements of $\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$ further
elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(
x_{i}y_{j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. In Example
\ref{exam.xiyj}, we have shown that $\det A=0$ if $n\geq2$. We can now prove
this in a simpler way.

Namely, let $n\geq2$. Define an $n\times n$-matrix $B$ by $B=\left(
\begin{array}
[c]{ccccc}%
x_{1} & 0 & 0 & \cdots & 0\\
x_{2} & 0 & 0 & \cdots & 0\\
x_{3} & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n} & 0 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first column of $B$ is $\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  ^{T}$, while all other columns are filled with zeroes.)
Define an $n\times n$-matrix $C$ by $C=\left(
\begin{array}
[c]{ccccc}%
y_{1} & y_{2} & y_{3} & \cdots & y_{n}\\
0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first row of $C$ is $\left(  y_{1},y_{2},\ldots
,y_{n}\right)  $, while all other rows are filled with zeroes.)

The second row of $C$ consists of zeroes (and this second row indeed exists,
because $n\geq2$). Thus, Exercise \ref{exe.ps4.6} \textbf{(c)} (applied to $C$
instead of $A$) yields $\det C=0$. Similarly, using Exercise \ref{exe.ps4.6}
\textbf{(d)}, we can show that $\det B=0$. Now, Theorem \ref{thm.det(AB)}
(applied to $B$ and $C$ instead of $A$ and $B$) yields $\det\left(  BC\right)
=\det B\cdot\underbrace{\det C}_{=0}=0$. But what is $BC$ ?

Write $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$, and write $C$ in the form $C=\left(  c_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Then, the definition of $BC$ yields%
\[
BC=\left(  \sum_{k=1}^{n}b_{i,k}c_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Therefore, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}$, the $\left(  i,j\right)  $-th entry of the matrix $BC$ is%
\[
\sum_{k=1}^{n}b_{i,k}c_{k,j}=\underbrace{b_{i,1}}_{=x_{i}}\underbrace{c_{1,j}%
}_{=y_{j}}+\sum_{k=2}^{n}\underbrace{b_{i,k}}_{=0}\underbrace{c_{k,j}}%
_{=0}=x_{i}y_{j}+\underbrace{\sum_{k=2}^{n}0\cdot0}_{=0}=x_{i}y_{j}.
\]
But this is the same as the $\left(  i,j\right)  $-th entry of the matrix $A$.
Thus, every entry of $BC$ equals the corresponding entry of $A$. Hence,
$BC=A$, so that $\det\left(  BC\right)  =\det A$. Thus, $\det A=\det\left(
BC\right)  =0$, just as we wanted to show.
\end{example}

\begin{example}
\label{exam.xi+yj.3}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$, and let $y_{1},y_{2},\ldots,y_{n}$ be $n$
further elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix $\left(
x_{i}+y_{j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. In Example
\ref{exam.xi+yj}, we have shown that $\det A=0$ if $n\geq3$.

We can now prove this in a simpler way. The argument is similar to Example
\ref{exam.xiyj.3}, and so I will be very brief:

Let $n\geq3$. Define an $n\times n$-matrix $B$ by $B=\left(
\begin{array}
[c]{ccccc}%
x_{1} & 1 & 0 & \cdots & 0\\
x_{2} & 1 & 0 & \cdots & 0\\
x_{3} & 1 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n} & 1 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first column of $B$ is $\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  ^{T}$, the second column is $\left(  1,1,\ldots
,1\right)  ^{T}$, while all other columns are filled with zeroes.) Define an
$n\times n$-matrix $C$ by $C=\left(
\begin{array}
[c]{ccccc}%
1 & 1 & 1 & \cdots & 1\\
y_{1} & y_{2} & y_{3} & \cdots & y_{n}\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $. (Thus, the first row of $C$ is $\left(  1,1,\ldots,1\right)  $,
the second row is $\left(  y_{1},y_{2},\ldots,y_{n}\right)  $, while all other
rows are filled with zeroes.) It is now easy to show that $BC=A$ (check
this!), but both $\det B$ and $\det C$ are $0$ (due to having a column or a
row filled with zeroes). Thus, again, we obtain $\det A=0$.
\end{example}

\begin{exercise}
\label{exe.ps4.pascal}Let $n\in\mathbb{N}$. Let $A$ be the $n\times n$-matrix
$\left(  \dbinom{i+j-2}{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(
\begin{array}
[c]{cccc}%
\dbinom{0}{0} & \dbinom{1}{0} & \cdots & \dbinom{n-1}{0}\\
\dbinom{1}{1} & \dbinom{2}{1} & \cdots & \dbinom{n}{1}\\
\vdots & \vdots & \ddots & \vdots\\
\dbinom{n-1}{n-1} & \dbinom{n}{n-1} & \cdots & \dbinom{2n-2}{n-1}%
\end{array}
\right)  $. (This matrix is a piece of Pascal's triangle \textquotedblleft
rotated by $45^{\circ}$\textquotedblright. For example, for $n=4$, we have
$A=\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
1 & 2 & 3 & 4\\
1 & 3 & 6 & 10\\
1 & 4 & 10 & 20
\end{array}
\right)  $.)

Show that $\det A=1$.
\end{exercise}

The matrix $A$ in Exercise \ref{exe.ps4.pascal} is one of the so-called
\textit{Pascal matrices}; see \cite{EdelStrang} for an enlightening exposition
of some of its properties (but beware of the fact that the very first page
reveals a significant part of the solution of Exercise \ref{exe.ps4.pascal}).

\begin{remark}
There exists a more general notion of a matrix, in which the rows and the
columns are indexed not necessarily by integers from $1$ to $n$ (for some
$n\in\mathbb{N}$), but rather by arbitrary objects. For instance, this more
general notion allows us to speak of a matrix with two rows labelled
\textquotedblleft spam\textquotedblright\ and \textquotedblleft
eggs\textquotedblright, and with three columns labelled $0$, $3$ and $\infty$.
(It thus has $6$ entries, such as the $\left(  \text{\textquotedblleft
spam\textquotedblright},3\right)  $-th entry or the $\left(
\text{\textquotedblleft eggs\textquotedblright},\infty\right)  $-th entry.)
This notion of matrices is more general and more flexible than the one used
above (e.g., it allows for infinite matrices), although it has some drawbacks
(e.g., notions such as \textquotedblleft lower-triangular\textquotedblright%
\ are not defined per se, because there might be no canonical way to order the
rows and the columns; also, infinite matrices cannot always be multiplied). We
might want to define the determinant of such a matrix. Of course, this only
makes sense when the rows of the matrix are indexed by the same objects as its
columns (this essentially says that the matrix is a \textquotedblleft square
matrix\textquotedblright\ in a reasonably general sense). So, let $X$ be a
set, and $A$ be a \textquotedblleft generalized matrix\textquotedblright%
\ whose rows and columns are both indexed by the elements of $X$. We want to
define $\det A$. We assume that $X$ is finite (indeed, while $\det A$
sometimes makes sense for infinite $X$, this only happens under some rather
restrictive conditions). Then, we can define $\det A$ by
\[
\det A=\sum_{\sigma\in S_{X}}\left(  -1\right)  ^{\sigma}\prod_{i\in
X}a_{i,\sigma\left(  i\right)  },
\]
where $S_{X}$ denotes the set of all permutations of $X$. This relies on a
definition of $\left(  -1\right)  ^{\sigma}$ for every $\sigma\in S_{X}$;
fortunately, we have provided such a definition in Exercise \ref{exe.ps4.2}.
\end{remark}

We shall see more about determinants later. So far we have barely scratched
the surface. Huge collections of problems and examples on the computation of
determinants can be found in \cite{Prasolov} and \cite{Krattenthaler} (and, if
you can be bothered with 100-years-old notation and level of rigor, in
\cite{Muir} -- one of the most comprehensive collections of \textquotedblleft
forgotten tales\textquotedblright\ in mathematics).

Let us finish this section with a brief remark on the geometrical use of determinants.

\begin{remark}
Let us consider the Euclidean plane $\mathbb{R}^{2}$ with its Cartesian
coordinate system and its origin $0$. If $A=\left(  x_{A},y_{A}\right)  $ and
$B=\left(  x_{B},y_{B}\right)  $ are two points on $\mathbb{R}^{2}$, then the
area of the triangle $0AB$ is $\dfrac{1}{2}\left\vert \det\left(
\begin{array}
[c]{cc}%
x_{A} & x_{B}\\
y_{A} & y_{B}%
\end{array}
\right)  \right\vert $. The absolute value here reflects the fact that
determinants can be negative, while areas must always be $\geq0$ (although
they can be $0$ when $0$, $A$ and $B$ are collinear); however, it makes
working with areas somewhat awkward. This can be circumvented by the notion of
a \textit{signed area}. (The signed area of a triangle $ABC$ is its regular
area if the triangle is \textquotedblleft directed clockwise\textquotedblright%
, and otherwise it is the negative of its area.) The signed area of the
triangle $0AB$ is $\dfrac{1}{2}\det\left(
\begin{array}
[c]{cc}%
x_{A} & x_{B}\\
y_{A} & y_{B}%
\end{array}
\right)  $. (Here, $0$ stands for the origin; i.e., \textquotedblleft the
triangle $0AB$\textquotedblright\ means the triangle with vertices at the
origin, at $A$ and at $B$.)

If $A=\left(  x_{A},y_{A}\right)  $, $B=\left(  x_{B},y_{B}\right)  $ and
$C=\left(  x_{C},y_{C}\right)  $ are three points in $\mathbb{R}^{2}$, then
the signed area of triangle $ABC$ is $\dfrac{1}{2}\det\left(
\begin{array}
[c]{ccc}%
x_{A} & x_{B} & x_{C}\\
y_{A} & y_{B} & y_{C}\\
1 & 1 & 1
\end{array}
\right)  $.

Similar formulas hold for tetrahedra: If $A=\left(  x_{A},y_{A},z_{A}\right)
$, $B=\left(  x_{B},y_{B},z_{B}\right)  $ and $C=\left(  x_{C},y_{C}%
,z_{C}\right)  $ are three points in $\mathbb{R}^{3}$, then the signed volume
of the tetrahedron $0ABC$ is $\dfrac{1}{6}\det\left(
\begin{array}
[c]{ccc}%
x_{A} & x_{B} & x_{C}\\
y_{A} & y_{B} & y_{C}\\
z_{A} & z_{B} & z_{C}%
\end{array}
\right)  $. (Again, take the absolute value for the non-signed volume.) There
is a $4\times4$ determinant formula for the signed volume of a general
tetrahedron $ABCD$.

One can generalize the notion of a triangle in $\mathbb{R}^{2}$ and the notion
of a tetrahedron in $\mathbb{R}^{3}$ to a notion of a \textit{simplex} in
$\mathbb{R}^{n}$. Then, one can try to define a notion of volume for these
objects. Determinants provide a way to do this. (Obviously, they don't allow
you to define the volume of a general \textquotedblleft convex
body\textquotedblright\ like a sphere, and even for simplices it is not
a-priori clear that they satisfy the standard properties that one would expect
them to have -- e.g., that the \textquotedblleft volume\textquotedblright\ of
a simplex does not change when one moves this simplex. But for the algebraic
part of analytic geometry, they are mostly sufficient. To define
\textquotedblleft volumes\textquotedblright\ for general convex bodies, one
needs calculus and the theory of integration in $\mathbb{R}^{n}$; but this
theory, too, uses determinants.)
\end{remark}

\subsection{The Cauchy-Binet formula}

This section is devoted to the Cauchy-Binet formula: a generalization of
Theorem \ref{thm.det(AB)} which is less well-known than the latter, but still
comes useful. This formula appears in the literature in various forms; we
follow \href{http://planetmath.org/cauchybinetformula}{the one on PlanetMath}
(although we use different notations).

First, we introduce a notation for \textquotedblleft picking out some rows of
a matrix and throwing away the rest\textquotedblright\ (and also the analogous
thing for columns):

\begin{definition}
\label{def.rowscols}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be an $n\times m$-matrix.

\textbf{(a)} If $i_{1},i_{2},\ldots,i_{u}$ are some elements of $\left\{
1,2,\ldots,n\right\}  $, then we let $\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A$ denote the $u\times m$-matrix $\left(  a_{i_{x}%
,j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}$. For instance, if $A=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $, then $\operatorname*{rows}\nolimits_{3,1,4}A=\left(
\begin{array}
[c]{ccc}%
c & c^{\prime} & c^{\prime\prime}\\
a & a^{\prime} & a^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $. For every $p\in\left\{  1,2,\ldots,u\right\}  $, we have%
\begin{align}
&  \left(  \text{the }p\text{-th row of }\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right) \nonumber\\
&  =\left(  a_{i_{p},1},a_{i_{p},2},\ldots,a_{i_{p},m}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A=\left(  a_{i_{x},j}\right)  _{1\leq x\leq u,\ 1\leq
j\leq m}\right) \nonumber\\
&  =\left(  \text{the }i_{p}\text{-th row of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right)  . \label{eq.def.rowscols.a.row=row}%
\end{align}
Thus, $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A$ is the
$u\times m$-matrix whose rows (from top to bottom) are the rows labelled
$i_{1},i_{2},\ldots,i_{u}$ of the matrix $A$.

\textbf{(b)} If $j_{1},j_{2},\ldots,j_{v}$ are some elements of $\left\{
1,2,\ldots,m\right\}  $, then we let $\operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}A$ denote the $n\times v$-matrix $\left(  a_{i,j_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}$. For instance, if $A=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  $, then $\operatorname*{cols}\nolimits_{3,2}A=\left(
\begin{array}
[c]{cc}%
a^{\prime\prime} & a^{\prime}\\
b^{\prime\prime} & b^{\prime}\\
c^{\prime\prime} & c^{\prime}%
\end{array}
\right)  $. For every $q\in\left\{  1,2,\ldots,v\right\}  $, we have%
\begin{align}
&  \left(  \text{the }q\text{-th column of }\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right) \nonumber\\
&  =\left(
\begin{array}
[c]{c}%
a_{1,j_{q}}\\
a_{2,j_{q}}\\
\vdots\\
a_{n,j_{q}}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i,j_{y}}\right)  _{1\leq
i\leq n,\ 1\leq y\leq v}\right) \nonumber\\
&  =\left(  \text{the }j_{q}\text{-th column of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right)  . \label{eq.def.rowscols.b.col=col}%
\end{align}
Thus, $\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A$ is the
$n\times v$-matrix whose columns (from left to right) are the columns labelled
$j_{1},j_{2},\ldots,j_{v}$ of the matrix $A$.
\end{definition}

Now we can state the \textit{Cauchy-Binet formula}:

\begin{theorem}
\label{thm.cauchy-binet}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be
an $n\times m$-matrix, and let $B$ be an $m\times n$-matrix. Then,%
\begin{equation}
\det\left(  AB\right)  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
. \label{eq.thm.cauchy-binet}%
\end{equation}

\end{theorem}

\begin{remark}
\label{rmk.cauchy-binet.sumsign}The summation sign $\sum_{1\leq g_{1}%
<g_{2}<\cdots<g_{n}\leq m}$ in (\ref{eq.thm.cauchy-binet}) is an abbreviation
for
\begin{equation}
\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}}}.
\label{eq.rmk.cauchy-binet.real-meaning}%
\end{equation}
In particular, if $n=0$, then it signifies a summation over all $0$-tuples of
elements of $\left\{  1,2,\ldots,m\right\}  $ (because in this case, the chain
of inequalities $g_{1}<g_{2}<\cdots<g_{n}$ is a tautology); such a sum always
has exactly one addend (because there is exactly one $0$-tuple).

When both $n$ and $m$ equal $0$, then the notation $\sum_{1\leq g_{1}%
<g_{2}<\cdots<g_{n}\leq m}$ is slightly confusing: It appears to mean an empty
summation (because $1\leq m$ does not hold). But as we said, we mean this
notation to be an abbreviation for (\ref{eq.rmk.cauchy-binet.real-meaning}),
which signifies a sum with exactly one addend. But this is enough pedantry for
now; for $n>0$, the notation $\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}$
fortunately means exactly what it seems to mean.
\end{remark}

We shall soon give a detailed proof of Theorem \ref{thm.cauchy-binet}; see
\cite[Chapter 29, Theorem]{AigZie} for a different proof\footnote{Note that
the formulation of Theorem \ref{thm.cauchy-binet} in \cite[Chapter 29,
Theorem]{AigZie} is slightly different: In our notations, it says that if $A$
is an $n\times m$-matrix and if $B$ is an $m\times n$-matrix, then%
\begin{equation}
\det\left(  AB\right)  =\sum_{\substack{\mathcal{Z}\subseteq\left\{
1,2,\ldots,m\right\}  ;\\\left\vert \mathcal{Z}\right\vert =n}}\det\left(
\operatorname*{cols}\nolimits_{\mathcal{Z}}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{\mathcal{Z}}B\right)  ,
\label{eq.thm.cauchy-binet.fn1.alt}%
\end{equation}
where the matrices $\operatorname*{cols}\nolimits_{\mathcal{Z}}A$ and
$\operatorname*{rows}\nolimits_{\mathcal{Z}}B$ (for $\mathcal{Z}$ being a
subset of $\left\{  1,2,\ldots,m\right\}  $) are defined as follows: Write the
subset $\mathcal{Z}$ in the form $\left\{  z_{1},z_{2},\ldots,z_{k}\right\}  $
with $z_{1}<z_{2}<\cdots<z_{k}$, and set $\operatorname*{cols}%
\nolimits_{\mathcal{Z}}A=\operatorname*{cols}\nolimits_{z_{1},z_{2}%
,\ldots,z_{k}}A$ and $\operatorname*{rows}\nolimits_{\mathcal{Z}%
}B=\operatorname*{rows}\nolimits_{z_{1},z_{2},\ldots,z_{k}}B$. (Apart from
this, \cite[Chapter 29, Theorem]{AigZie} also requires $n\leq m$; but this
requirement is useless.)
\par
The equalities (\ref{eq.thm.cauchy-binet}) and
(\ref{eq.thm.cauchy-binet.fn1.alt}) are equivalent, because the $n$-tuples
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}
^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$ are in a bijection with the
subsets $\mathcal{Z}$ of $\left\{  1,2,\ldots,m\right\}  $ satisfying
$\left\vert \mathcal{Z}\right\vert =n$. (This bijection sends an $n$-tuple
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ to the subset $\left\{
g_{1},g_{2},\ldots,g_{n}\right\}  $.)
\par
The proof of (\ref{eq.thm.cauchy-binet.fn1.alt}) given in \cite[Chapter
29]{AigZie} uses the \textit{Lindstr\"{o}m-Gessel-Viennot lemma} (which it
calls the \textquotedblleft lemma of Gessel-Viennot\textquotedblright) and is
highly worth reading.}. Before we prove Theorem \ref{thm.cauchy-binet}, let us
give some examples for its use. First, here is a simple fact:

\begin{lemma}
\label{lem.increasing-sequences}Let $n\in\mathbb{N}$.

\textbf{(a)} There exists exactly one $n$-tuple $\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n}$, namely the $n$-tuple $\left(  1,2,\ldots,n\right)
$.

\textbf{(b)} Let $m\in\mathbb{N}$ be such that $m<n$. Then, there exists no
$n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots
,m\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$.
\end{lemma}

As for its intuitive meaning, Lemma \ref{lem.increasing-sequences} can be
viewed as a \textquotedblleft pigeonhole principle\textquotedblright\ for
strictly increasing sequences: Part \textbf{(b)} says (roughly speaking) that
there is no way to squeeze a strictly increasing sequence $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ of $n$ numbers into the set $\left\{
1,2,\ldots,m\right\}  $ when $m<n$; part \textbf{(a)} says (again, informally)
that the only such sequence for $m=n$ is $\left(  1,2,\ldots,n\right)  $.

\begin{exercise}
\label{exe.lem.increasing-sequences} Give a formal proof of Lemma
\ref{lem.increasing-sequences}. (Do not bother doing this if you do not
particularly care about formal proofs and find Lemma
\ref{lem.increasing-sequences} obvious enough.)
\end{exercise}

\begin{example}
\label{exam.cauchy-binet.nxn}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
$n\times n$-matrices. It is easy to check that $\operatorname*{cols}%
\nolimits_{1,2,\ldots,n}A=A$ and $\operatorname*{rows}\nolimits_{1,2,\ldots
,n}B=B$. Now, Theorem \ref{thm.cauchy-binet} (applied to $m=n$) yields%
\begin{equation}
\det\left(  AB\right)  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq n}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
. \label{eq.exam.cauchy-binet.nxn.1}%
\end{equation}
But Lemma \ref{lem.increasing-sequences} \textbf{(a)} yields that there exists
exactly one $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$, namely the
$n$-tuple $\left(  1,2,\ldots,n\right)  $. Hence, the sum on the right hand
side of (\ref{eq.exam.cauchy-binet.nxn.1}) has exactly one addend: namely, the
addend for $\left(  g_{1},g_{2},\ldots,g_{n}\right)  =\left(  1,2,\ldots
,n\right)  $. Therefore, this sum simplifies as follows:%
\begin{align*}
&  \sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq n}\det\left(  \operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \\
&  =\det\left(  \underbrace{\operatorname*{cols}\nolimits_{1,2,\ldots,n}%
A}_{=A}\right)  \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{1,2,\ldots,n}B}_{=B}\right)  =\det A\cdot\det B.
\end{align*}
Hence, (\ref{eq.exam.cauchy-binet.nxn.1}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq n}%
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}%
}B\right) \\
&  =\det A\cdot\det B.
\end{align*}
This, of course, is the statement of Theorem \ref{thm.det(AB)}. Hence, Theorem
\ref{thm.det(AB)} is a particular case of Theorem \ref{thm.cauchy-binet}.
\end{example}

\begin{example}
\label{exam.cauchy-binet.0}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such
that $m<n$. Thus, Lemma \ref{lem.increasing-sequences} \textbf{(b)} shows that
there exists no $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$.

Now, let $A$ be an $n\times m$-matrix, and let $B$ be an $m\times n$-matrix.
Then, Theorem \ref{thm.cauchy-binet} yields%
\begin{align}
\det\left(  AB\right)   &  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}%
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}%
}B\right) \nonumber\\
&  =\left(  \text{empty sum}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since there exists no }n\text{-tuple }\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}\\
\text{satisfying }g_{1}<g_{2}<\cdots<g_{n}%
\end{array}
\right) \nonumber\\
&  =0. \label{eq.exam.cauchy-binet.0}%
\end{align}
This identity allows us to compute $\det A$ in Example \ref{exam.xiyj.3} in a
simpler way: Instead of defining two $n\times n$-matrices $B$ and $C$ by
$B=\left(
\begin{array}
[c]{ccccc}%
x_{1} & 0 & 0 & \cdots & 0\\
x_{2} & 0 & 0 & \cdots & 0\\
x_{3} & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
x_{n} & 0 & 0 & \cdots & 0
\end{array}
\right)  $ and $C=\left(
\begin{array}
[c]{ccccc}%
y_{1} & y_{2} & y_{3} & \cdots & y_{n}\\
0 & 0 & 0 & \cdots & 0\\
0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \cdots & 0
\end{array}
\right)  $, it suffices to define an $n\times1$-matrix $B^{\prime}$ by
$B^{\prime}=\left(  x_{1},x_{2},\ldots,x_{n}\right)  ^{T}$ and a $1\times
n$-matrix $C^{\prime}$ by $C^{\prime}=\left(  y_{1},y_{2},\ldots,y_{n}\right)
$, and argue that $A=B^{\prime}C^{\prime}$. (We leave the details to the
reader.) Similarly, Example \ref{exam.xi+yj.3} could be dealt with.
\end{example}

\begin{remark}
The equality (\ref{eq.exam.cauchy-binet.0}) can also be derived from Theorem
\ref{thm.det(AB)}. Indeed, let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such
that $m<n$. Let $A$ be an $n\times m$-matrix, and let $B$ be an $m\times
n$-matrix. Notice that $n-m>0$ (since $m<n$). Let $A^{\prime}$ be the $n\times
n$-matrix obtained from $A$ by appending $n-m$ new columns to the right of $A$
and filling these columns with zeroes. (For example, if $n=4$ and $m=2$ and
$A=\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}\\
a_{3,1} & a_{3,2}\\
a_{4,1} & a_{4,2}%
\end{array}
\right)  $, then $A^{\prime}=\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & 0 & 0\\
a_{2,1} & a_{2,2} & 0 & 0\\
a_{3,1} & a_{3,2} & 0 & 0\\
a_{4,1} & a_{4,2} & 0 & 0
\end{array}
\right)  $.) Also, let $B^{\prime}$ be the $n\times n$-matrix obtained from
$B$ by appending $n-m$ new rows to the bottom of $B$ and filling these rows
with zeroes. (For example, if $n=4$ and $m=2$ and $B=\left(
\begin{array}
[c]{cccc}%
b_{1,1} & b_{1,2} & b_{1,3} & b_{1,4}\\
b_{2,1} & b_{2,2} & b_{2,3} & b_{2,4}%
\end{array}
\right)  $, then $B^{\prime}=\left(
\begin{array}
[c]{cccc}%
b_{1,1} & b_{1,2} & b_{1,3} & b_{1,4}\\
b_{2,1} & b_{2,2} & b_{2,3} & b_{2,4}\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0
\end{array}
\right)  $.) Then, it is easy to check that $AB=A^{\prime}B^{\prime}$ (in
fact, just compare corresponding entries of $AB$ and $A^{\prime}B^{\prime}$).
But recall that $n-m>0$. Hence, the matrix $A^{\prime}$ has a column
consisting of zeroes (namely, its last column). Thus, Exercise \ref{exe.ps4.6}
\textbf{(d)} (applied to $A^{\prime}$ instead of $A$) shows that $\det\left(
A^{\prime}\right)  =0$. Now,
\begin{align*}
\det\left(  \underbrace{AB}_{=A^{\prime}B^{\prime}}\right)   &  =\det\left(
A^{\prime}B^{\prime}\right)  =\underbrace{\det\left(  A^{\prime}\right)
}_{=0}\cdot\det\left(  B^{\prime}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}A^{\prime}\text{ and }B^{\prime}\text{ instead of }A\text{ and }B\right) \\
&  =0.
\end{align*}
Thus, (\ref{eq.exam.cauchy-binet.0}) is proven again.
\end{remark}

\begin{example}
\label{exam.cauchy-binet.1}Let us see what Theorem \ref{thm.cauchy-binet} says
for $n=1$. Indeed, let $m\in\mathbb{N}$; let $A=\left(  a_{1},a_{2}%
,\ldots,a_{m}\right)  $ be a $1\times m$-matrix (i.e., a row vector with $m$
entries), and let $B=\left(  b_{1},b_{2},\ldots,b_{m}\right)  ^{T}$ be an
$m\times1$-matrix (i.e., a column vector with $m$ entries). Then, $AB$ is the
$1\times1$-matrix $\left(
\begin{array}
[c]{c}%
\sum_{k=1}^{m}a_{k}b_{k}%
\end{array}
\right)  $. Thus,
\begin{equation}
\det\left(  AB\right)  =\det\left(
\begin{array}
[c]{c}%
\sum_{k=1}^{m}a_{k}b_{k}%
\end{array}
\right)  =\sum_{k=1}^{m}a_{k}b_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.det.small.1x1})}\right)  . \label{eq.exam.cauchy-binet.1}%
\end{equation}
What would we obtain if we tried to compute $\det\left(  AB\right)  $ using
Theorem \ref{thm.cauchy-binet}? Theorem \ref{thm.cauchy-binet} (applied to
$n=1$) yields%
\begin{align*}
\det\left(  AB\right)   &  =\underbrace{\sum_{1\leq g_{1}\leq m}}%
_{=\sum_{g_{1}=1}^{m}}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{g_{1}}A}_{=\left(
\begin{array}
[c]{c}%
a_{g_{1}}%
\end{array}
\right)  }\right)  \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{g_{1}}B}_{=\left(
\begin{array}
[c]{c}%
b_{g_{1}}%
\end{array}
\right)  }\right) \\
&  =\sum_{g_{1}=1}^{m}\underbrace{\det\left(
\begin{array}
[c]{c}%
a_{g_{1}}%
\end{array}
\right)  }_{\substack{=a_{g_{1}}\\\text{(by (\ref{eq.det.small.1x1}))}}%
}\cdot\underbrace{\det\left(
\begin{array}
[c]{c}%
b_{g_{1}}%
\end{array}
\right)  }_{\substack{=b_{g_{1}}\\\text{(by (\ref{eq.det.small.1x1}))}}%
}=\sum_{g_{1}=1}^{m}a_{g_{1}}\cdot b_{g_{1}}.
\end{align*}
This is, of course, the same result as that of (\ref{eq.exam.cauchy-binet.1})
(with the summation index $k$ renamed as $g_{1}$). So we did not gain any
interesting insight from applying Theorem \ref{thm.cauchy-binet} to $n=1$.
\end{example}

\begin{example}
\label{exam.cauchy-binet.2}Let us try a slightly less trivial case. Indeed,
let $m\in\mathbb{N}$; let $A=\left(
\begin{array}
[c]{cccc}%
a_{1} & a_{2} & \cdots & a_{m}\\
a_{1}^{\prime} & a_{2}^{\prime} & \cdots & a_{m}^{\prime}%
\end{array}
\right)  $ be a $2\times m$-matrix, and let $B=\left(
\begin{array}
[c]{cc}%
b_{1} & b_{1}^{\prime}\\
b_{2} & b_{2}^{\prime}\\
\vdots & \vdots\\
b_{m} & b_{m}^{\prime}%
\end{array}
\right)  $ be an $m\times2$-matrix. Then, $AB$ is the $2\times2$-matrix
$\left(
\begin{array}
[c]{cc}%
\sum_{k=1}^{m}a_{k}b_{k} & \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\\
\sum_{k=1}^{m}a_{k}^{\prime}b_{k} & \sum_{k=1}^{m}a_{k}^{\prime}b_{k}^{\prime}%
\end{array}
\right)  $. Hence,%
\begin{align}
\det\left(  AB\right)   &  =\det\left(
\begin{array}
[c]{cc}%
\sum_{k=1}^{m}a_{k}b_{k} & \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\\
\sum_{k=1}^{m}a_{k}^{\prime}b_{k} & \sum_{k=1}^{m}a_{k}^{\prime}b_{k}^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{k}b_{k}\right)  \left(  \sum_{k=1}^{m}%
a_{k}^{\prime}b_{k}^{\prime}\right)  -\left(  \sum_{k=1}^{m}a_{k}^{\prime
}b_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\right)  .
\label{eq.exam.cauchy-binet.2.1}%
\end{align}


On the other hand, Theorem \ref{thm.cauchy-binet} (now applied to $n=2$)
yields
\begin{align*}
\det\left(  AB\right)   &  =\sum_{1\leq g_{1}<g_{2}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2}}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2}}B\right) \\
&  =\sum_{1\leq i<j\leq m}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{i,j}A}_{=\left(
\begin{array}
[c]{cc}%
a_{i} & a_{j}\\
a_{i}^{\prime} & a_{j}^{\prime}%
\end{array}
\right)  }\right)  \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{i,j}B}_{=\left(
\begin{array}
[c]{cc}%
b_{i} & b_{i}^{\prime}\\
b_{j} & b_{j}^{\prime}%
\end{array}
\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we renamed the summation indices }g_{1}\text{ and }g_{2}\\
\text{as }i\text{ and }j\text{, since double subscripts are annoying}%
\end{array}
\right) \\
&  =\sum_{1\leq i<j\leq m}\underbrace{\det\left(
\begin{array}
[c]{cc}%
a_{i} & a_{j}\\
a_{i}^{\prime} & a_{j}^{\prime}%
\end{array}
\right)  }_{=a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime}}\cdot\underbrace{\det
\left(
\begin{array}
[c]{cc}%
b_{i} & b_{i}^{\prime}\\
b_{j} & b_{j}^{\prime}%
\end{array}
\right)  }_{=b_{i}b_{j}^{\prime}-b_{j}b_{i}^{\prime}}\\
&  =\sum_{1\leq i<j\leq m}\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime
}\right)  \cdot\left(  b_{i}b_{j}^{\prime}-b_{j}b_{i}^{\prime}\right)  .
\end{align*}
Compared with (\ref{eq.exam.cauchy-binet.2.1}), this yields%
\begin{align}
&  \left(  \sum_{k=1}^{m}a_{k}b_{k}\right)  \left(  \sum_{k=1}^{m}%
a_{k}^{\prime}b_{k}^{\prime}\right)  -\left(  \sum_{k=1}^{m}a_{k}^{\prime
}b_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}b_{k}^{\prime}\right) \nonumber\\
&  =\sum_{1\leq i<j\leq m}\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime
}\right)  \cdot\left(  b_{i}b_{j}^{\prime}-b_{j}b_{i}^{\prime}\right)  .
\label{eq.exam.cauchy-binet.2.binet-cauchy}%
\end{align}
This identity is called
\href{https://en.wikipedia.org/wiki/Binet-Cauchy_identity}{the
\textit{Binet-Cauchy identity}} (I am not kidding -- look it up on the
Wikipedia). It is fairly easy to prove by direct computation; thus, using
Theorem \ref{thm.cauchy-binet} to prove it was quite an overkill. However,
(\ref{eq.exam.cauchy-binet.2.binet-cauchy}) might not be very easy to come up
with, whereas deriving it from Theorem \ref{thm.cauchy-binet} is
straightforward. (And Theorem \ref{thm.cauchy-binet} is easier to memorize
than (\ref{eq.exam.cauchy-binet.2.binet-cauchy}).)

Here is a neat application of (\ref{eq.exam.cauchy-binet.2.binet-cauchy}): If
$a_{1},a_{2},\ldots,a_{m}$ and $a_{1}^{\prime},a_{2}^{\prime},\ldots
,a_{m}^{\prime}$ are real numbers, then
(\ref{eq.exam.cauchy-binet.2.binet-cauchy}) (applied to $b_{k}=a_{k}$ and
$b_{k}^{\prime}=a_{k}^{\prime}$) yields%
\begin{align*}
&  \left(  \sum_{k=1}^{m}a_{k}a_{k}\right)  \left(  \sum_{k=1}^{m}%
a_{k}^{\prime}a_{k}^{\prime}\right)  -\left(  \sum_{k=1}^{m}a_{k}^{\prime
}a_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}a_{k}^{\prime}\right) \\
&  =\sum_{1\leq i<j\leq m}\underbrace{\left(  a_{i}a_{j}^{\prime}-a_{j}%
a_{i}^{\prime}\right)  \cdot\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime
}\right)  }_{=\left(  a_{i}a_{j}^{\prime}-a_{j}a_{i}^{\prime}\right)  ^{2}%
\geq0}\geq\sum_{1\leq i<j\leq m}0=0,
\end{align*}
so that%
\[
\left(  \sum_{k=1}^{m}a_{k}a_{k}\right)  \left(  \sum_{k=1}^{m}a_{k}^{\prime
}a_{k}^{\prime}\right)  \geq\left(  \sum_{k=1}^{m}a_{k}^{\prime}a_{k}\right)
\left(  \sum_{k=1}^{m}a_{k}a_{k}^{\prime}\right)  .
\]
In other words,%
\[
\left(  \sum_{k=1}^{m}a_{k}^{2}\right)  \left(  \sum_{k=1}^{m}\left(
a_{k}^{\prime}\right)  ^{2}\right)  \geq\left(  \sum_{k=1}^{m}a_{k}%
a_{k}^{\prime}\right)  ^{2}.
\]
This is the famous
\href{https://en.wikipedia.org/wiki/Cauchy-Schwarz_inequality}{Cauchy-Schwarz
inequality}.
\end{example}

Let us now prepare for the proof of Theorem \ref{thm.cauchy-binet}. First
comes a fact which should be fairly clear:

\begin{proposition}
\label{prop.sorting}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ integers.

\textbf{(a)} There exists a permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq
a_{\sigma\left(  n\right)  }$.

\textbf{(b)} If $\sigma\in S_{n}$ is such that $a_{\sigma\left(  1\right)
}\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }%
$, then, for every $i\in\left\{  1,2,\ldots,n\right\}  $, the value
$a_{\sigma\left(  i\right)  }$ depends only on $a_{1},a_{2},\ldots,a_{n}$ and
$i$ (but not on $\sigma$).

\textbf{(c)} Assume that the integers $a_{1},a_{2},\ldots,a_{n}$ are distinct.
Then, there is a \textbf{unique} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$.
\end{proposition}

Let me explain why this proposition should be intuitively
obvious.\footnote{See the solution of Exercise \ref{exe.sorting.basics}
further below for a formal proof of this proposition.} Proposition
\ref{prop.sorting} \textbf{(a)} says that any list $\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)  $ of $n$ integers can be sorted in weakly increasing
order by means of a permutation $\sigma\in S_{n}$. Proposition
\ref{prop.sorting} \textbf{(b)} says that the result of this sorting process
is independent of how the sorting happened (although the permutation $\sigma$
will sometimes be non-unique). Proposition \ref{prop.sorting} \textbf{(c)}
says that if the integers $a_{1},a_{2},\ldots,a_{n}$ are distinct, then the
permutation $\sigma\in S_{n}$ which sorts the list $\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)  $ in increasing order is uniquely determined as well. We
required $a_{1},a_{2},\ldots,a_{n}$ to be $n$ integers for the sake of
simplicity, but we could just as well have required them to be elements of any
\textit{totally ordered set} (i.e., any set with a less-than relation
satisfying some standard axioms).

The next fact looks slightly scary, but is still rather simple:

\begin{lemma}
\label{lem.cauchy-binet.EI}For every $n\in\mathbb{N}$, let $\left[  n\right]
$ denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. We let $\mathbf{E}$ be the subset%
\[
\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots,k_{n}\text{ are
distinct}\right\}
\]
of $\left[  m\right]  ^{n}$. We let $\mathbf{I}$ be the subset%
\[
\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}
\]
of $\left[  m\right]  ^{n}$. Then, the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is well-defined and is a bijection.
\end{lemma}

The intuition for Lemma \ref{lem.cauchy-binet.EI} is that every $n$-tuple of
distinct elements of $\left\{  1,2,\ldots,m\right\}  $ can be represented
uniquely as a permuted version of a strictly increasing\footnote{An $n$-tuple
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ is said to be \textit{strictly
increasing} if and only if $k_{1}<k_{2}<\cdots<k_{n}$.} $n$-tuple of elements
of $\left\{  1,2,\ldots,m\right\}  $, and therefore, specifying an $n$-tuple
of distinct elements of $\left\{  1,2,\ldots,m\right\}  $ is tantamount to
specifying a strictly increasing $n$-tuple of elements of $\left\{
1,2,\ldots,m\right\}  $ and a permutation $\sigma\in S_{n}$ which says how
this $n$-tuple is to be permuted.\footnote{For instance, the $4$-tuple
$\left(  4,1,6,2\right)  $ of distinct elements of $\left\{  1,2,\ldots
,7\right\}  $ can be specified by specifying the strictly increasing $4$-tuple
$\left(  1,2,4,6\right)  $ (which is its sorted version) and the permutation
$\pi\in S_{4}$ which sends $1,2,3,4$ to $3,1,4,2$, respectively (that is,
$\pi=\left(  3,1,4,2\right)  $ in one-line notation). In the terminology of
Lemma \ref{lem.cauchy-binet.EI}, the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
sends $\left(  \left(  1,2,4,6\right)  ,\pi\right)  $ to $\left(
4,1,6,2\right)  $.} This is not a formal proof, but this should explain why
Lemma \ref{lem.cauchy-binet.EI} is usually applied throughout mathematics
without even mentioning it as a statement. If desired, a formal proof of Lemma
\ref{lem.cauchy-binet.EI} can be obtained using Proposition \ref{prop.sorting}%
.\footnote{Again, see the solution of Exercise \ref{exe.sorting.basics}
further below for such a proof.}

\begin{exercise}
\label{exe.sorting.basics}Prove Proposition \ref{prop.sorting} and Lemma
\ref{lem.cauchy-binet.EI}. (Ignore this exercise if you find these two facts
sufficiently obvious and are uninterested in the details of their proofs.)
\end{exercise}

Before we return to Theorem \ref{thm.cauchy-binet}, let me make a digression
about sorting:

\begin{exercise}
\label{exe.sorting.nmu}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be such that
$n\geq m$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$ integers. Let $b_{1}%
,b_{2},\ldots,b_{m}$ be $m$ integers. Assume that%
\begin{equation}
a_{i}\leq b_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,m\right\}  . \label{eq.exe.sorting.nmu.ass}%
\end{equation}


Let $\sigma\in S_{n}$ be such that $a_{\sigma\left(  1\right)  }\leq
a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$. Let
$\tau\in S_{m}$ be such that $b_{\tau\left(  1\right)  }\leq b_{\tau\left(
2\right)  }\leq\cdots\leq b_{\tau\left(  m\right)  }$. Then,%
\[
a_{\sigma\left(  i\right)  }\leq b_{\tau\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,m\right\}  .
\]

\end{exercise}

\begin{remark}
\label{rmk.sorting.nmu.interpret}Loosely speaking, Exercise
\ref{exe.sorting.nmu} says the following: If two lists $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ and $\left(  b_{1},b_{2},\ldots,b_{m}\right)  $
of integers have the property that each entry of the first list is $\leq$ to
the corresponding entry of the second list (as long as the latter is
well-defined), then this property still holds after both lists are sorted in
increasing order, provided that we have $n\geq m$ (that is, the first list is
at least as long as the second list).

A consequence of Exercise \ref{exe.sorting.nmu} is the following curious fact,
known as the \textquotedblleft non-messing-up phenomenon\textquotedblright%
\ (\cite[Theorem 1]{Tenner-NMU} and \cite[Example 1]{GalKar71}): If we start
with a matrix filled with integers, then sort the entries of each row of the
matrix in increasing order, and then sort the entries of each column of the
resulting matrix in increasing order, then the final matrix still has sorted
rows (i.e., the entries of each row are still sorted). That is, the sorting of
the columns did not \textquotedblleft mess up\textquotedblright\ the
sortedness of the rows. For example, if we start with the matrix $\left(
\begin{array}
[c]{cccc}%
1 & 3 & 2 & 5\\
2 & 1 & 4 & 2\\
3 & 1 & 6 & 0
\end{array}
\right)  $, then sorting the entries of each row gives us the matrix $\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 5\\
1 & 2 & 2 & 4\\
0 & 1 & 3 & 6
\end{array}
\right)  $, and then sorting the entries of each column results in the matrix
$\left(
\begin{array}
[c]{cccc}%
0 & 1 & 2 & 4\\
1 & 2 & 3 & 5\\
1 & 2 & 3 & 6
\end{array}
\right)  $. The rows of this matrix are still sorted, as the \textquotedblleft
non-messing-up phenomenon\textquotedblright\ predicts. To prove this
phenomenon in general, it suffices to show that any entry in the resulting
matrix is $\leq$ to the entry directly below it (assuming that the latter
exists); but this follows easily from Exercise \ref{exe.sorting.nmu}.
\end{remark}

We are now ready to prove Theorem \ref{thm.cauchy-binet}.

\begin{vershort}


\begin{proof}
[Proof of Theorem \ref{thm.cauchy-binet}.]We shall use the notations of Lemma
\ref{lem.cauchy-binet.EI}.

Write the $n\times m$-matrix $A$ as $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Write the $m\times n$-matrix $B$ as $B=\left(
b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. The definition of $AB$ thus
yields $AB=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Therefore, (\ref{eq.det.eq.2}) (applied to $AB$ and
$\sum_{k=1}^{m}a_{i,k}b_{k,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det\left(  AB\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.short.1}%
\end{equation}
But for every $\sigma\in S_{n}$, we have%
\begin{align*}
\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}}\underbrace{\prod_{i=1}^{n}\left(  a_{i,k_{i}}b_{k_{i}%
,\sigma\left(  i\right)  }\right)  }_{=\left(  \prod_{i=1}^{n}a_{i,k_{i}%
}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)
}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.prodrule}, applied to
}m_{i}=n\text{ and }p_{i,k}=a_{i,k}b_{k,\sigma\left(  i\right)  }\right) \\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }\right)  .
\end{align*}
Hence, (\ref{pf.thm.cauchy-binet.short.1}) rewrites as%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }\right) \nonumber\\
&  =\underbrace{\sum_{\sigma\in S_{n}}\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n}}}_{=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}}\sum_{\sigma\in S_{n}}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(
i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.short.3}%
\end{align}
But every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfies%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }=\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
\label{pf.thm.cauchy-binet.short.detrows}%
\end{equation}
\footnote{\textit{Proof.} Let $\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}$. Recall that $B=\left(  b_{i,j}\right)  _{1\leq
i\leq m,\ 1\leq j\leq n}$. Hence, the definition of $\operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ gives us%
\[
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{i},j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ and
$b_{k_{i},j}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  },
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.short.3}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }_{=\det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  }\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\label{pf.thm.cauchy-binet.short.4}%
\end{align}


But for every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}$ satisfying $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$,
we have%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=0 \label{pf.thm.cauchy-binet.short.6a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.short.6a}):} Let $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ be such that
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$. Then, the
integers $k_{1},k_{2},\ldots,k_{n}$ are not distinct (because $\mathbf{E}$ is
the set of all $n$-tuples in $\left[  m\right]  ^{n}$ whose entries are
distinct). Thus, there exist two distinct elements $p$ and $q$ of $\left[
n\right]  $ such that $k_{p}=k_{q}$. Consider these $p$ and $q$. But
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ is the $n\times
n$-matrix whose rows (from top to bottom) are the rows labelled $k_{1}%
,k_{2},\ldots,k_{n}$ of the matrix $B$. Since $k_{p} = k_{q}$, this shows that
the $p$-th row and the $q$-th row of the matrix $\operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ are equal. Hence, the matrix
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ has two equal rows
(since $p$ and $q$ are distinct). Therefore, Exercise \ref{exe.ps4.6}
\textbf{(e)} (applied to $\operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B$ instead of $A$) yields $\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  =0$, qed.}. Therefore, in the
sum on the right hand side of (\ref{pf.thm.cauchy-binet.short.4}), all the
addends corresponding to $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}$ satisfying $\left(  k_{1},k_{2},\ldots,k_{n}\right)
\notin\mathbf{E}$ evaluate to $0$. We can therefore remove all these addends
from the sum. The remaining addends are those corresponding to $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}$. Therefore,
(\ref{pf.thm.cauchy-binet.short.4}) becomes
\begin{equation}
\det\left(  AB\right)  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\label{pf.thm.cauchy-binet.short.7}%
\end{equation}


On the other hand, Lemma \ref{lem.cauchy-binet.EI} yields that the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is well-defined and is a bijection. Hence, we can substitute $\left(
g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots
,g_{\sigma\left(  n\right)  }\right)  $ for $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  $ in the sum on the right hand side of
(\ref{pf.thm.cauchy-binet.short.7}). We thus obtain%
\begin{align*}
&  \sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}}\left(
\prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  =\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(
i\right)  }}\right)  \det\left(  \operatorname*{rows}\nolimits_{g_{\sigma
\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(
n\right)  }}B\right)  .
\end{align*}
Thus, (\ref{pf.thm.cauchy-binet.short.7}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \nonumber\\
&  =\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(
i\right)  }}\right)  \det\left(  \operatorname*{rows}\nolimits_{g_{\sigma
\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(
n\right)  }}B\right)  . \label{pf.thm.cauchy-binet.short.9}%
\end{align}


But every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
and every $\sigma\in S_{n}$ satisfy%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
\label{pf.thm.cauchy-binet.short.6b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.short.6b}):} Let $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ and $\sigma\in
S_{n}$. We have $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}%
}B=\left(  b_{k_{i},j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (as we have
seen in one of the previous footnotes) and $\operatorname*{rows}%
\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(  2\right)  }%
,\ldots,k_{\sigma\left(  n\right)  }}B=\left(  b_{k_{\sigma\left(  i\right)
},j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (for similar reasons). Hence,
we can apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $\sigma$,
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$, $b_{k_{i},j}$ and
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B$ instead of $\kappa$, $B$,
$b_{i,j}$ and $B_{\kappa}$. As a consequence, we obtain
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\]
This proves (\ref{pf.thm.cauchy-binet.short.6b}).}. Hence,
(\ref{pf.thm.cauchy-binet.short.9}) becomes%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\underbrace{\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)
,\sigma\right)  \in\mathbf{I}\times S_{n}}}_{=\sum_{\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\mathbf{I}}\sum_{\sigma\in S_{n}}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \underbrace{\det\left(
\operatorname*{rows}\nolimits_{g_{\sigma\left(  1\right)  },g_{\sigma\left(
2\right)  },\ldots,g_{\sigma\left(  n\right)  }}B\right)  }%
_{\substack{=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  \\\text{(by
(\ref{pf.thm.cauchy-binet.short.6b}), applied to }k_{i}=g_{i}\text{)}%
}}\nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\sum
_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }%
}\right)  \left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\left(
\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  .
\label{pf.thm.cauchy-binet.short.10}%
\end{align}
But every $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ satisfies
$\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}=\det\left(  \operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\mathbf{I}$. We have $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$. Thus, the definition of $\operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A$ yields%
\[
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A=\left(  a_{i,g_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq n}=\left(  a_{i,g_{j}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $y$ as $j$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A$ and
$a_{i,g_{j}}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}=\sum_{\sigma\in S_{n}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)
^{\sigma},
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.short.10}) becomes%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}%
}\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)  ^{\sigma
}\right)  }_{=\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A\right)  }\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
. \label{pf.thm.cauchy-binet.short.15}%
\end{align}
Finally, we recall that $\mathbf{I}$ was defined as the set
\[
\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}  .
\]
Thus, summing over all $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in
\mathbf{I}$ means the same as summing over all $\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\left[  m\right]  ^{n}$ satisfying $g_{1}%
<g_{2}<\cdots<g_{n}$. In other words,%
\[
\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}=\sum
_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left[  m\right]
^{n};\\g_{1}<g_{2}<\cdots<g_{n}}}=\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}%
\]
(an equality between summation signs -- hopefully its meaning is obvious).
Hence, (\ref{pf.thm.cauchy-binet.short.15}) becomes%
\[
\det\left(  AB\right)  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
.
\]
This proves Theorem \ref{thm.cauchy-binet}.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Theorem \ref{thm.cauchy-binet}.]We shall use the notations of Lemma
\ref{lem.cauchy-binet.EI}. Recall that%
\[
\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots,k_{n}\text{ are
distinct}\right\}
\]
and%
\begin{align*}
\mathbf{I}  &  =\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\} \\
&  =\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left\{
1,2,\ldots,m\right\}  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  m\right]  =\left\{
1,2,\ldots,m\right\}  \right) \\
&  =\left\{  \left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,m\right\}  ^{n}\ \mid\ g_{1}<g_{2}<\cdots<g_{n}\right\}
\end{align*}
(here, we renamed the index $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ as
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  $).

Lemma \ref{lem.cauchy-binet.EI} says that the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is well-defined and is a bijection.

Write the $n\times m$-matrix $A$ as $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Write the $m\times n$-matrix $B$ as $B=\left(
b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$. The definition of $AB$ thus
yields $AB=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Therefore, (\ref{eq.det.eq.2}) (applied to $AB$ and
$\sum_{k=1}^{m}a_{i,k}b_{k,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det\left(  AB\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.1}%
\end{equation}
But for every $\sigma\in S_{n}$, we have%
\begin{align*}
\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(  i\right)
}\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in
\underbrace{\left[  m\right]  \times\left[  m\right]  \times\cdots
\times\left[  m\right]  }_{n\text{ factors}}}\prod_{i=1}^{n}\left(
a_{i,k_{i}}b_{k_{i},\sigma\left(  i\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.prodrule}, applied to
}m_{i}=n\text{ and }p_{i,k}=a_{i,k}b_{k,\sigma\left(  i\right)  }\right) \\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\underbrace{\prod_{i=1}^{n}\left(  a_{i,k_{i}}b_{k_{i},\sigma\left(
i\right)  }\right)  }_{=\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{\left[  m\right]
\times\left[  m\right]  \times\cdots\times\left[  m\right]  }_{n\text{
factors}}=\left[  m\right]  ^{n}\right) \\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  }\right)  .
\end{align*}
Hence, (\ref{pf.thm.cauchy-binet.1}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}\left(  \sum_{k=1}^{m}a_{i,k}b_{k,\sigma\left(
i\right)  }\right)  }_{=\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}}\left(  \prod
_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(
i\right)  }\right) \nonumber\\
&  =\underbrace{\sum_{\sigma\in S_{n}}\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n}}}_{=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}}\sum_{\sigma\in S_{n}}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n}a_{i,k_{i}}\right)  \left(  \prod_{i=1}^{n}b_{k_{i},\sigma\left(
i\right)  }\right) \nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \left(  \sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)
}\right)  . \label{pf.thm.cauchy-binet.3}%
\end{align}
But every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfies $\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}b_{k_{i},\sigma\left(  i\right)  }=\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n}$. The definition of
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ yields
$\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}$ (since $B=\left(  b_{i,j}%
\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$). Thus,%
\[
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{i},j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$ and
$b_{k_{i},j}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
b_{k_{i},\sigma\left(  i\right)  },
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.3}) becomes%
\begin{align}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}b_{k_{i},\sigma\left(  i\right)  }\right)  }_{=\det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  }\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}%
}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  . \label{pf.thm.cauchy-binet.4}%
\end{align}


Next, let us make two observations:

\begin{itemize}
\item Every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}$ satisfying $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$
satisfies%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=0 \label{pf.thm.cauchy-binet.6a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.6a}):} Let $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left[  m\right]  ^{n}$ be such that
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \notin\mathbf{E}$.
\par
Let us first show that there exist two distinct elements $p$ and $q$ of
$\left[  n\right]  $ such that $g_{p}=g_{q}$. Indeed, we assume the contrary.
Thus, there do not exist two distinct elements $p$ and $q$ of $\left[
n\right]  $ such that $g_{p}=g_{q}$. In other words, every two distinct
elements $p$ and $q$ of $\left[  n\right]  $ satisfy $g_{p}\neq g_{q}$. In
other words, the integers $g_{1},g_{2},\ldots,g_{n}$ are distinct. Hence,
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that the integers
$k_{1},k_{2},\ldots,k_{n}$ are distinct (since $\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\left[  m\right]  ^{n}$ and since the integers
$g_{1},g_{2},\ldots,g_{n}$ are distinct). In other words,%
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ \text{the
integers }k_{1},k_{2},\ldots,k_{n}\text{ are distinct}\right\}  =\mathbf{E}.
\]
This contradicts $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \notin\mathbf{E}$.
This contradiction proves that our assumption was wrong. Hence, we have proven
that there exist two distinct elements $p$ and $q$ of $\left[  n\right]  $
such that $g_{p}=g_{q}$. Consider these $p$ and $q$. Now,
(\ref{eq.def.rowscols.a.row=row}) (applied to $m$, $n$, $n$, $g_{r}$, $B$ and
$b_{i,j}$ instead of $n$, $m$, $u$, $i_{r}$, $A$ and $a_{i,j}$) yields%
\begin{align}
&  \left(  \text{the }p\text{-th row of }\operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\left(  \text{the }\underbrace{g_{p}}_{=g_{q}}\text{-th row of }B\right)
=\left(  \text{the }g_{q}\text{-th row of }B\right)  .
\label{pf.thm.cauchy-binet.6a.pf.1}%
\end{align}
On the other hand, (\ref{eq.def.rowscols.a.row=row}) (applied to $m$, $n$,
$n$, $g_{r}$, $B$, $b_{i,j}$ and $q$ instead of $n$, $m$, $u$, $i_{r}$, $A$,
$a_{i,j}$ and $p$) yields%
\begin{equation}
\left(  \text{the }q\text{-th row of }\operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right)  =\left(  \text{the }g_{q}\text{-th row of
}B\right)  .\nonumber
\end{equation}
Compared with (\ref{pf.thm.cauchy-binet.6a.pf.1}), this yields
\[
\left(  \text{the }p\text{-th row of }\operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right)  =\left(  \text{the }q\text{-th row of
}\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  .
\]
Thus, the matrix $\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B$
has two equal rows (namely, the $p$-th row and the $q$-th row), because $p$
and $q$ are distinct. Therefore, Exercise \ref{exe.ps4.6} \textbf{(e)}
(applied to $\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B$
instead of $A$) yields $\det\left(  \operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right)  =0$.
\par
Let us now forget that we fixed $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $.
We thus have shown that every $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left[  m\right]  ^{n}$ satisfying $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \notin\mathbf{E}$ satisfies $\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  =0$. Renaming $\left(
g_{1},g_{2},\ldots,g_{n}\right)  $ as $\left(  k_{1},k_{2},\ldots
,k_{n}\right)  $ in this result, we obtain the following: Every $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ satisfying
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \notin\mathbf{E}$ satisfies
$\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
=0$. This proves (\ref{pf.thm.cauchy-binet.6a}).}.

\item Every $\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]
^{n}$ and every $\sigma\in S_{n}$ satisfy%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  \label{pf.thm.cauchy-binet.6b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cauchy-binet.6b}):} Let $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ and $\sigma\in
S_{n}$. The definition of $\operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B$ yields $\operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B=\left(  b_{k_{x},j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}$
(since $B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$). Thus,%
\[
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{x}%
,j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{i},j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). On the other hand, the definition of
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B$ yields
$\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B=\left(  b_{k_{\sigma\left(
x\right)  },j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}$ (since $B=\left(
b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$). Thus,%
\[
\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)  },k_{\sigma\left(
2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B=\left(  b_{k_{\sigma\left(
x\right)  },j}\right)  _{1\leq x\leq n,\ 1\leq j\leq n}=\left(  b_{k_{\sigma
\left(  i\right)  },j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $x$ as $i$). So we know that $\operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B=\left(  b_{k_{i},j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ and $\operatorname*{rows}\nolimits_{k_{\sigma\left(
1\right)  },k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }%
}B=\left(  b_{k_{\sigma\left(  i\right)  },j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$. Hence, we can apply Lemma \ref{lem.det.sigma} \textbf{(a)} to
$\sigma$, $\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B$,
$b_{k_{i},j}$ and $\operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B$ instead
of $\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$. As a consequence, we obtain
\[
\det\left(  \operatorname*{rows}\nolimits_{k_{\sigma\left(  1\right)
},k_{\sigma\left(  2\right)  },\ldots,k_{\sigma\left(  n\right)  }}B\right)
=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)  .
\]
This proves (\ref{pf.thm.cauchy-binet.6b}).}
\end{itemize}

Now, (\ref{pf.thm.cauchy-binet.4}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right)
\\
&  =\underbrace{\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n};\\\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}}}_{\substack{=\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\mathbf{E}}\\\text{(since }\mathbf{E}\subseteq\left[  m\right]
^{n}\text{)}}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(
\operatorname*{rows}\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in\left[  m\right]  ^{n};\\\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \notin\mathbf{E}}}\left(  \prod_{i=1}^{n}a_{i,k_{i}}\right)
\underbrace{\det\left(  \operatorname*{rows}\nolimits_{k_{1},k_{2}%
,\ldots,k_{n}}B\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cauchy-binet.6a}) (since }\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \notin\mathbf{E}\text{))}}}\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}}\left(
\prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \in\left[  m\right]  ^{n};\\\left(  k_{1},k_{2}%
,\ldots,k_{n}\right)  \notin\mathbf{E}}}\left(  \prod_{i=1}^{n}a_{i,k_{i}%
}\right)  0}_{=0}\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\mathbf{E}}\left(
\prod_{i=1}^{n}a_{i,k_{i}}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{k_{1},k_{2},\ldots,k_{n}}B\right) \\
&  =\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(
i\right)  }}\right)  \det\left(  \operatorname*{rows}\nolimits_{g_{\sigma
\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(
n\right)  }}B\right)
\end{align*}
(here, we have substituted $\left(  g_{\sigma\left(  1\right)  }%
,g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(  n\right)  }\right)  $
for $\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ in the sum, because the map%
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is a bijection). Thus,%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\underbrace{\sum_{\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)
,\sigma\right)  \in\mathbf{I}\times S_{n}}}_{=\sum_{\left(  g_{1},g_{2}%
,\ldots,g_{n}\right)  \in\mathbf{I}}\sum_{\sigma\in S_{n}}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \underbrace{\det\left(
\operatorname*{rows}\nolimits_{g_{\sigma\left(  1\right)  },g_{\sigma\left(
2\right)  },\ldots,g_{\sigma\left(  n\right)  }}B\right)  }%
_{\substack{=\left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  \\\text{(by
(\ref{pf.thm.cauchy-binet.6b}), applied to }k_{i}=g_{i}\text{)}}}\nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\sum
_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }%
}\right)  \left(  -1\right)  ^{\sigma}\cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}\left(
\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)  .
\label{pf.thm.cauchy-binet.10}%
\end{align}
But every $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ satisfies
$\sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)
}}\right)  \left(  -1\right)  ^{\sigma}=\det\left(  \operatorname*{cols}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\mathbf{I}$. Then,%
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $g_{1}<g_{2}<\cdots<g_{n}$.
\par
Now, the definition of $\operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A$ yields $\operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A=\left(  a_{i,g_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq n}$
(since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$). Hence,%
\[
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A=\left(  a_{i,g_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq n}=\left(  a_{i,g_{j}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}%
\]
(here, we renamed the index $y$ as $j$). Hence, (\ref{eq.det.eq.2}) (applied
to $\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A$ and
$a_{i,g_{j}}$ instead of $A$ and $a_{i,j}$) yields%
\[
\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}=\sum_{\sigma\in S_{n}}\left(  \prod
_{i=1}^{n}a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)
^{\sigma},
\]
qed.}. Hence, (\ref{pf.thm.cauchy-binet.10}) becomes%
\begin{align*}
&  \det\left(  AB\right) \\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}%
}\underbrace{\left(  \sum_{\sigma\in S_{n}}\left(  \prod_{i=1}^{n}%
a_{i,g_{\sigma\left(  i\right)  }}\right)  \left(  -1\right)  ^{\sigma
}\right)  }_{=\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}A\right)  }\cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}B\right) \\
&  =\underbrace{\sum_{\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}}%
}_{\substack{=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}%
}}\\\text{(since }\mathbf{I}=\left\{  \left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n}\ \mid\ g_{1}<g_{2}<\cdots
<g_{n}\right\}  \text{)}}}\det\left(  \operatorname*{cols}\nolimits_{g_{1}%
,g_{2},\ldots,g_{n}}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right) \\
&  =\underbrace{\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}}%
}}_{\substack{=\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\\\text{(since we
defined }\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\\\text{to be an
abbreviation for}\\\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,m\right\}  ^{n};\\g_{1}<g_{2}<\cdots<g_{n}}}\text{)}%
}}\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}%
}A\right)  \cdot\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n}}B\right) \\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n}\leq m}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n}}B\right)
.
\end{align*}
This proves Theorem \ref{thm.cauchy-binet}.
\end{proof}
\end{verlong}

\subsection{Prelude to Laplace expansion}

Next we shall show a fact which will allow us to compute some determinants by induction:

\begin{theorem}
\label{thm.laplace.pre}Let $n$ be a positive integer. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Assume that%
\begin{equation}
a_{n,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,\ldots
,n-1\right\}  . \label{eq.thm.laplace.pre.ass}%
\end{equation}
Then, $\det A=a_{n,n}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right)  $.
\end{theorem}

The assumption (\ref{eq.thm.laplace.pre.ass}) says that the last row of the
matrix $A$ consists entirely of zeroes, apart from its last entry $a_{n,n}$
(which may and may not be $0$). Theorem \ref{thm.laplace.pre} states that,
under this assumption, the determinant can be obtained by multiplying this
last entry $a_{n,n}$ with the determinant of the $\left(  n-1\right)
\times\left(  n-1\right)  $-matrix obtained by removing both the last row and
the last column from $A$. For example, for $n=3$, Theorem
\ref{thm.laplace.pre} states that%
\[
\det\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
0 & 0 & g
\end{array}
\right)  =g\det\left(
\begin{array}
[c]{cc}%
a & b\\
d & e
\end{array}
\right)  .
\]


Theorem \ref{thm.laplace.pre} is a particular case of \textit{Laplace
expansion}, which is a general recursive formula for the determinants that we
will encounter further below. But Theorem \ref{thm.laplace.pre} already has
noticeable applications of its own, which is why I have chosen to start with
this particular case.

The proof of Theorem \ref{thm.laplace.pre} essentially relies on the following fact:

\begin{lemma}
\label{lem.laplace.lem}Let $n$ be a positive integer. Let $\left(
a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}$ be an $\left(
n-1\right)  \times\left(  n-1\right)  $-matrix. Then,%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }%
=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.laplace.lem}.]We define a subset $T$ of $S_{n}$ by%
\[
T=\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  .
\]


(In other words, $T$ is the set of all $\tau\in S_{n}$ such that if we write
$\tau$ in one-line notation, then $\tau$ ends with an $n$.)

Now, we shall construct two mutually inverse maps between $S_{n-1}$ and $T$.

\begin{vershort}
For every $\sigma\in S_{n-1}$, we define a map $\widehat{\sigma}:\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $ by setting%
\[
\widehat{\sigma}\left(  i\right)  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n\right\}  .
\]
It is straightforward to see that this map $\widehat{\sigma}$ is well-defined
and belongs to $T$. Thus, we can define a map $\Phi:S_{n-1}\rightarrow T$ by
setting%
\[
\Phi\left(  \sigma\right)  =\widehat{\sigma}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in S_{n-1}.
\]

\end{vershort}

\begin{verlong}
For every $\sigma\in S_{n-1}$, we define a map $\widehat{\sigma}:\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $ by setting%
\[
\left(  \widehat{\sigma}\left(  i\right)  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n\right\}
\right)  .
\]
This map $\widehat{\sigma}$ is well-defined\footnote{\textit{Proof.} Let
$\sigma\in S_{n-1}$. Thus, $\sigma$ is a permutation of the set $\left\{
1,2,\ldots,n-1\right\}  $ (since $S_{n-1}$ is the set of all permutations of
the set $\left\{  1,2,\ldots,n-1\right\}  $). In other words, $\sigma$ is a
bijection $\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots
,n-1\right\}  $.
\par
We have $n\in\left\{  1,2,\ldots,n\right\}  $ (since $n$ is positive). Now,
for every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\in%
\begin{cases}
\left\{  1,2,\ldots,n\right\}  , & \text{if }i<n;\\
\left\{  1,2,\ldots,n\right\}  , & \text{if }i=n
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\sigma\left(  i\right)  \in\left\{  1,2,\ldots,n-1\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  \text{ in the case when }i<n\text{,}\\
\text{and since }n\in\left\{  1,2,\ldots,n\right\}  \text{ in the case when
}i=n
\end{array}
\right) \\
&  =\left\{  1,2,\ldots,n\right\}  .
\end{align*}
In other words, the map $\widehat{\sigma}$ is well-defined.} and belongs to
$T$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Thus, $\sigma$
is a permutation of the set $\left\{  1,2,\ldots,n-1\right\}  $ (since
$S_{n-1}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n-1\right\}  $). In other words, $\sigma$ is a bijection $\left\{
1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots,n-1\right\}  $. Hence,
the map $\sigma$ is injective and surjective.
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  n\right)
=%
\begin{cases}
\sigma\left(  n\right)  , & \text{if }n<n;\\
n, & \text{if }n=n
\end{cases}
=n$ (since $n=n$).
\par
Let us now show that $\widehat{\sigma}$ is injective.
\par
\textit{Proof that }$\widehat{\sigma}$ \textit{is injective:} Let $p$ and $q$
be two elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$\widehat{\sigma}\left(  p\right)  =\widehat{\sigma}\left(  q\right)  $. We
shall prove that $p=q$.
\par
We can WLOG assume that $p\geq q$ (since otherwise, we can simply switch $p$
with $q$). Assume this.
\par
Let us first assume that $p=n$. Then, $\widehat{\sigma}\left(  \underbrace{p}%
_{=n}\right)  =\widehat{\sigma}\left(  n\right)  =n$. Therefore, if we had
$q<n$, then we would have%
\begin{align*}
n  &  =\widehat{\sigma}\left(  p\right)  =\widehat{\sigma}\left(  q\right)  =%
\begin{cases}
\sigma\left(  q\right)  , & \text{if }q<n;\\
q, & \text{if }q=n
\end{cases}
=\sigma\left(  q\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }q<n\right)
\\
&  <n\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\sigma\left(  q\right)
\in\left\{  1,2,\ldots,n-1\right\}  \right)  ,
\end{align*}
which is absurd. Hence, we cannot have $q<n$. Thus, we must have $q\geq n$.
Therefore, $q=n$ (since $q\in\left\{  1,2,\ldots,n\right\}  $), so that
$p=n=q$.
\par
Now, let us forget that we have assumed that $p=n$. We thus have shown that
$p=q$ in the case when $p=n$. Hence, for the rest of this proof of $p=q$, we
can WLOG assume that we don't have $p=n$. Assume this.
\par
We have $p\neq n$ (since we don't have $p=n$). Since $p\in\left\{
1,2,\ldots,n\right\}  $ and $p\neq n$, we have $p\in\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  $,
so that $p<n$. The definition of $\widehat{\sigma}$ yields $\widehat{\sigma
}\left(  p\right)  =%
\begin{cases}
\sigma\left(  p\right)  , & \text{if }p<n;\\
p, & \text{if }p=n
\end{cases}
=\sigma\left(  p\right)  $ (since $p<n$).
\par
Now, $p\geq q$, so that $q\leq p<n$. The definition of $\widehat{\sigma}$
yields $\widehat{\sigma}\left(  q\right)  =%
\begin{cases}
\sigma\left(  q\right)  , & \text{if }q<n;\\
q, & \text{if }q=n
\end{cases}
=\sigma\left(  q\right)  $ (since $q<n$). Also, since $q<n$, we have
$q\in\left\{  1,2,\ldots,n-1\right\}  $.
\par
Now, $\sigma\left(  p\right)  =\widehat{\sigma}\left(  p\right)
=\widehat{\sigma}\left(  q\right)  =\sigma\left(  q\right)  $. Hence, $p=q$
(since the map $\sigma$ is injective). This completes our proof of $p=q$.
\par
Now, let us forget that we fixed $p$ and $q$. We thus have shown that if $p$
and $q$ are two elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$\widehat{\sigma}\left(  p\right)  =\widehat{\sigma}\left(  q\right)  $, then
$p=q$. In other words, the map $\widehat{\sigma}$ is injective.
\par
\textit{Proof that }$\widehat{\sigma}$ \textit{is surjective:} Let
$g\in\left\{  1,2,\ldots,n\right\}  $. We shall show that $g\in\widehat{\sigma
}\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
Indeed, if $g=n$, then $g=n=\widehat{\sigma}\left(  \underbrace{n}%
_{\in\left\{  1,2,\ldots,n\right\}  }\right)  \in\widehat{\sigma}\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. Hence, for the rest of this proof
of $g\in\widehat{\sigma}\left(  \left\{  1,2,\ldots,n\right\}  \right)  $, we
can WLOG assume that we don't have $g=n$. Assume this.
\par
We have $g\neq n$ (since we don't have $g=n$). Since $g\in\left\{
1,2,\ldots,n\right\}  $ and $g\neq n$, we have $g\in\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}
=\sigma\left(  \left\{  1,2,\ldots,n-1\right\}  \right)  $ (since the map
$\sigma$ is surjective). In other words, there exists an $h\in\left\{
1,2,\ldots,n-1\right\}  $ such that $g=\sigma\left(  h\right)  $. Consider
this $h$. We have $h\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $ and $h<n$ (since $h\in\left\{  1,2,\ldots,n-1\right\}
$). The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(
h\right)  =%
\begin{cases}
\sigma\left(  h\right)  , & \text{if }h<n;\\
h, & \text{if }h=n
\end{cases}
=\sigma\left(  h\right)  $ (since $h<n$). Compared with $g=\sigma\left(
h\right)  $, this yields $g=\widehat{\sigma}\left(  \underbrace{h}%
_{\in\left\{  1,2,\ldots,n\right\}  }\right)  \in\widehat{\sigma}\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. Thus, $g\in\widehat{\sigma}\left(
\left\{  1,2,\ldots,n\right\}  \right)  $ is proven.
\par
Now, let us forget that we fixed $g$. We thus have shown that $g\in
\widehat{\sigma}\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ for every
$g\in\left\{  1,2,\ldots,n\right\}  $. In other words, $\left\{
1,2,\ldots,n\right\}  \subseteq\widehat{\sigma}\left(  \left\{  1,2,\ldots
,n\right\}  \right)  $. In other words, the map $\widehat{\sigma}$ is
surjective.
\par
We now know that the map $\widehat{\sigma}$ is both injective and surjective.
In other words, $\widehat{\sigma}$ is bijective. Hence, $\widehat{\sigma}$ is
a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. In other words, $\widehat{\sigma}$ is a permutation
of $\left\{  1,2,\ldots,n\right\}  $. In other words, $\widehat{\sigma}\in
S_{n}$ (since $S_{n}$ is the set of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $).
\par
Thus, $\widehat{\sigma}$ is an element of $S_{n}$ and satisfies
$\widehat{\sigma}\left(  n\right)  =n$. In other words, $\widehat{\sigma}$ is
an element $\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other
words,%
\[
\widehat{\sigma}\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  =T,
\]
qed.}. Thus, we can define a map $\Phi:S_{n-1}\rightarrow T$ by setting%
\[
\left(  \Phi\left(  \sigma\right)  =\widehat{\sigma}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n-1}\right)  .
\]

\end{verlong}

Loosely speaking, for every $\sigma\in S_{n-1}$, the permutation $\Phi\left(
\sigma\right)  =\widehat{\sigma}\in T$ is obtained by writing $\sigma$ in
one-line notation and appending $n$ on its right. For example, if $n=4$ and if
$\sigma\in S_{3}$ is the permutation that is written as $\left(  2,3,1\right)
$ in one-line notation, then $\Phi\left(  \sigma\right)  =\widehat{\sigma}$ is
the permutation that is written as $\left(  2,3,1,4\right)  $ in one-line notation.

\begin{vershort}
On the other hand, for every $\gamma\in T$, we define a map $\overline{\gamma
}:\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots,n-1\right\}
$ by setting
\[
\overline{\gamma}\left(  i\right)  =\gamma\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-1\right\}  .
\]
It is straightforward to see that this map $\overline{\gamma}$ is well-defined
and belongs to $S_{n-1}$. Hence, we can define a map $\Psi:T\rightarrow
S_{n-1}$ by setting%
\[
\Psi\left(  \gamma\right)  =\overline{\gamma}\ \ \ \ \ \ \ \ \ \ \text{for
every }\gamma\in T.
\]

\end{vershort}

\begin{verlong}
On the other hand, for every $\gamma\in T$, we define a map $\overline{\gamma
}:\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots,n-1\right\}
$ by setting
\[
\left(  \overline{\gamma}\left(  i\right)  =\gamma\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-1\right\}
\right)  .
\]
This map $\overline{\gamma}$ is well-defined\footnote{\textit{Proof.} Let
$\gamma\in T$. Thus, $\gamma\in T=\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $. In other words, $\gamma$ is an element $\tau$ of
$S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words, $\gamma$ is an
element of $S_{n}$ and satisfies $\gamma\left(  n\right)  =n$.
\par
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of the
set $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$\gamma$ is a bijection $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Hence, the map $\gamma$ is surjective and injective.
\par
Let $i\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $i<n$ and thus $i\neq n$.
Also, $i\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $. Thus, $\gamma\left(  i\right)  $ is well-defined and belongs to
$\left\{  1,2,\ldots,n\right\}  $. Also, $i\neq n$. Hence, $\gamma\left(
i\right)  \neq\gamma\left(  n\right)  $ (since the map $\gamma$ is injective),
so that $\gamma\left(  i\right)  \neq\gamma\left(  n\right)  =n$. Combined
with $\gamma\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  $, this shows
that $\gamma\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  \setminus
\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  $.
\par
Now, let us forget that we fixed $i$. We thus have shown that $\gamma\left(
i\right)  \in\left\{  1,2,\ldots,n-1\right\}  $ for every $i\in\left\{
1,2,\ldots,n-1\right\}  $. In other words, the map $\overline{\gamma}$ is
well-defined. Qed.} and belongs to $S_{n-1}$\ \ \ \ \footnote{\textit{Proof.}
Let $\gamma\in T$. Thus, $\gamma\in T=\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  n\right)  =n\right\}  $. In other words, $\gamma$ is an element
$\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words,
$\gamma$ is an element of $S_{n}$ and satisfies $\gamma\left(  n\right)  =n$.
\par
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of the
set $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$\gamma$ is a bijection $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Hence, the map $\gamma$ is surjective and injective.
\par
Let us now show that $\overline{\gamma}$ is injective.
\par
\textit{Proof that }$\overline{\gamma}$ \textit{is injective:} Let $p$ and $q$
be two elements of $\left\{  1,2,\ldots,n-1\right\}  $ such that
$\overline{\gamma}\left(  p\right)  =\overline{\gamma}\left(  q\right)  $. We
shall prove that $p=q$.
\par
We have $\overline{\gamma}\left(  p\right)  =\gamma\left(  p\right)  $ (by the
definition of $\overline{\gamma}$) and $\overline{\gamma}\left(  q\right)
=\gamma\left(  q\right)  $ (by the definition of $\overline{\gamma}$). Thus,
$\gamma\left(  p\right)  =\overline{\gamma}\left(  p\right)  =\overline
{\gamma}\left(  q\right)  =\gamma\left(  q\right)  $. Therefore, $p=q$ (since
the map $\gamma$ is injective).
\par
Now, let us forget that we fixed $p$ and $q$. We thus have shown that if $p$
and $q$ are two elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$\overline{\gamma}\left(  p\right)  =\overline{\gamma}\left(  q\right)  $,
then $p=q$. In other words, the map $\overline{\gamma}$ is injective.
\par
\textit{Proof that }$\overline{\gamma}$ \textit{is surjective:} Let
$g\in\left\{  1,2,\ldots,n-1\right\}  $. We shall show that $g\in
\overline{\gamma}\left(  \left\{  1,2,\ldots,n-1\right\}  \right)  $.
\par
Indeed, $g\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  =\gamma\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ (since
the map $\gamma$ is surjective). Hence, there exists an $h\in\left\{
1,2,\ldots,n\right\}  $ such that $g=\gamma\left(  h\right)  $. Consider this
$h$.
\par
We have $g\in\left\{  1,2,\ldots,n-1\right\}  $, thus $g<n$, thus $g\neq n$.
\par
If we had $h=n$, then we would have $g=\gamma\left(  \underbrace{h}%
_{=n}\right)  =\gamma\left(  n\right)  =n$, which would contradict $g\neq n$.
Hence, we cannot have $h=n$. We thus have $h\neq n$. Combined with
$h\in\left\{  1,2,\ldots,n\right\}  $, this shows that $h\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots
,n-1\right\}  $. Thus, $\overline{\gamma}\left(  h\right)  $ is well-defined.
The definition of $\overline{\gamma}$ shows that $\overline{\gamma}\left(
h\right)  =\gamma\left(  h\right)  =g$.
\par
Hence, $g=\overline{\gamma}\left(  \underbrace{h}_{\in\left\{  1,2,\ldots
,n-1\right\}  }\right)  \in\overline{\gamma}\left(  \left\{  1,2,\ldots
,n-1\right\}  \right)  $.
\par
Now, let us forget that we fixed $g$. We thus have shown that $g\in
\overline{\gamma}\left(  \left\{  1,2,\ldots,n-1\right\}  \right)  $ for every
$g\in\left\{  1,2,\ldots,n-1\right\}  $. In other words, $\left\{
1,2,\ldots,n-1\right\}  \subseteq\overline{\gamma}\left(  \left\{
1,2,\ldots,n-1\right\}  \right)  $. In other words, the map $\overline{\gamma
}$ is surjective.
\par
We now know that the map $\overline{\gamma}$ is both injective and surjective.
In other words, $\overline{\gamma}$ is bijective. Hence, $\overline{\gamma}$
is a bijective map $\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{
1,2,\ldots,n-1\right\}  $. In other words, $\overline{\gamma}$ is a
permutation of $\left\{  1,2,\ldots,n-1\right\}  $. In other words,
$\overline{\gamma}\in S_{n-1}$ (since $S_{n-1}$ is the set of all permutations
of the set $\left\{  1,2,\ldots,n-1\right\}  $), qed.}. Hence, we can define a
map $\Psi:T\rightarrow S_{n-1}$ by setting%
\[
\left(  \Psi\left(  \gamma\right)  =\overline{\gamma}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\gamma\in T\right)  .
\]

\end{verlong}

Loosely speaking, for every $\gamma\in T$, the permutation $\Psi\left(
\gamma\right)  =\overline{\gamma}\in S_{n-1}$ is obtained by writing $\gamma$
in one-line notation and removing the $n$ (which is the rightmost entry in the
one-line notation, because $\gamma\left(  n\right)  =n$). For example, if
$n=4$ and if $\gamma\in S_{4}$ is the permutation that is written as $\left(
2,3,1,4\right)  $ in one-line notation, then $\Psi\left(  \gamma\right)
=\overline{\gamma}$ is the permutation that is written as $\left(
2,3,1\right)  $ in one-line notation.

\begin{vershort}
The maps $\Phi$ and $\Psi$ are mutually inverse.\footnote{This should be clear
enough from the descriptions we gave using one-line notation. A formal proof
is straightforward.} Thus, the map $\Phi$ is a bijection.
\end{vershort}

\begin{verlong}
We have $\Phi\circ\Psi=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $\gamma\in T$. Then, $\gamma\in T=\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  n\right)  =n\right\}  $. In other words, $\gamma$ is an element
$\tau$ of $S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words,
$\gamma$ is an element of $S_{n}$ and satisfies $\gamma\left(  n\right)  =n$.
\par
Let $\sigma=\overline{\gamma}$. Then, the definition of $\Phi$ yields
$\Phi\left(  \sigma\right)  =\widehat{\sigma}$. Also, the definition of $\Psi$
yields $\Psi\left(  \gamma\right)  =\overline{\gamma}=\sigma$. Now, $\left(
\Phi\circ\Psi\right)  \left(  \gamma\right)  =\Phi\left(  \underbrace{\Psi
\left(  \gamma\right)  }_{=\sigma}\right)  =\Phi\left(  \sigma\right)
=\widehat{\sigma}$.
\par
Both $\widehat{\sigma}$ and $\gamma$ are elements of $T$, therefore elements
of $S_{n}$ (since $T\subseteq S_{n}$), therefore permutations of the set
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $), therefore
bijective maps $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $.
\par
Let $i\in\left\{  1,2,\ldots,n\right\}  $. We shall show that $\widehat{\sigma
}\left(  i\right)  =\gamma\left(  i\right)  $.
\par
If $i=n$, then%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =n\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i=n\right) \\
&  =\gamma\left(  \underbrace{n}_{=i}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\gamma\left(  n\right)  =n\right) \\
&  =\gamma\left(  i\right)  .
\end{align*}
Hence, $\widehat{\sigma}\left(  i\right)  =\gamma\left(  i\right)  $ is proven
in the case when $i=n$. Therefore, for the rest of the proof of
$\widehat{\sigma}\left(  i\right)  =\gamma\left(  i\right)  $, we can WLOG
assume that we don't have $i=n$. Assume this.
\par
We have $i\neq n$ (since we don't have $i=n$). Combined with $i\in\left\{
1,2,\ldots,n\right\}  $, this shows that $i\in\left\{  1,2,\ldots,n\right\}
\setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  $. Hence,
$i<n$. Now,%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =\underbrace{\sigma}_{=\overline{\gamma}}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<n\right) \\
&  =\overline{\gamma}\left(  i\right)  =\gamma\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\overline{\gamma
}\right)  .
\end{align*}
Thus, $\widehat{\sigma}\left(  i\right)  =\gamma\left(  i\right)  $ is proven.
\par
Let us now forget that we fixed $i$. We thus have proven that $\widehat{\sigma
}\left(  i\right)  =\gamma\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,n\right\}  $. In other words, $\widehat{\sigma}=\gamma$ (since both
$\widehat{\sigma}$ and $\gamma$ are maps $\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}  $). Hence, $\left(  \Phi\circ
\Psi\right)  \left(  \gamma\right)  =\widehat{\sigma}=\gamma
=\operatorname*{id}\left(  \gamma\right)  $.
\par
Let us now forget that we fixed $\gamma$. We have thus proven that $\left(
\Phi\circ\Psi\right)  \left(  \gamma\right)  =\operatorname*{id}\left(
\gamma\right)  $ for every $\gamma\in T$. In other words, $\Phi\circ
\Psi=\operatorname*{id}$, qed.} and $\Psi\circ\Phi=\operatorname*{id}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Let $\varepsilon
=\widehat{\sigma}$. Then, the definition of $\Psi$ yields $\Phi\left(
\sigma\right)  =\widehat{\sigma}=\varepsilon$. Also, the definition of $\Psi$
yields $\Psi\left(  \varepsilon\right)  =\overline{\varepsilon}$. Now,
$\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)  =\Psi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\varepsilon}\right)  =\Psi\left(
\varepsilon\right)  =\overline{\varepsilon}$.
\par
Both $\overline{\varepsilon}$ and $\sigma$ are elements of $S_{n-1}$,
therefore permutations of the set $\left\{  1,2,\ldots,n-1\right\}  $ (since
$S_{n-1}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n-1\right\}  $), therefore bijective maps $\left\{  1,2,\ldots,n-1\right\}
\rightarrow\left\{  1,2,\ldots,n-1\right\}  $.
\par
Let $i\in\left\{  1,2,\ldots,n-1\right\}  $. Thus, $i\leq n-1<n$. Now,%
\begin{align*}
\overline{\varepsilon}\left(  i\right)   &  =\underbrace{\varepsilon
}_{=\widehat{\sigma}}\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\overline{\varepsilon}\right) \\
&  =\widehat{\sigma}\left(  i\right)  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =\sigma\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i<n\right)  .
\end{align*}
\par
Let us now forget that we fixed $i$. We thus have proven that $\overline
{\varepsilon}\left(  i\right)  =\sigma\left(  i\right)  $ for every
$i\in\left\{  1,2,\ldots,n-1\right\}  $. In other words, $\overline
{\varepsilon}=\sigma$ (since both $\overline{\varepsilon}$ and $\sigma$ are
maps $\left\{  1,2,\ldots,n-1\right\}  \rightarrow\left\{  1,2,\ldots
,n-1\right\}  $). Hence, $\left(  \Psi\circ\Phi\right)  \left(  \sigma\right)
=\overline{\varepsilon}=\sigma=\operatorname*{id}\left(  \sigma\right)  $.
\par
Let us now forget that we fixed $\sigma$. We have thus proven that $\left(
\Psi\circ\Phi\right)  \left(  \sigma\right)  =\operatorname*{id}\left(
\sigma\right)  $ for every $\sigma\in S_{n-1}$. In other words, $\Psi\circ
\Phi=\operatorname*{id}$, qed.}. These two equalities show that the maps
$\Phi$ and $\Psi$ are mutually inverse. Hence, the map $\Phi$ is a bijection.
\end{verlong}

\begin{vershort}
It is fairly easy to see that every $\sigma\in S_{n-1}$ satisfies%
\begin{equation}
\left(  -1\right)  ^{\widehat{\sigma}}=\left(  -1\right)  ^{\sigma}
\label{pf.lem.laplace.lem.short.-1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.lem.short.-1}):} Let
$\sigma\in S_{n-1}$. We want to prove that $\left(  -1\right)
^{\widehat{\sigma}}=\left(  -1\right)  ^{\sigma}$. It is clearly sufficient to
show that $\ell\left(  \widehat{\sigma}\right)  =\ell\left(  \sigma\right)  $
(because $\left(  -1\right)  ^{\widehat{\sigma}}=\left(  -1\right)
^{\ell\left(  \widehat{\sigma}\right)  }$ and $\left(  -1\right)  ^{\sigma
}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$). In order to do so, it
is sufficient to show that the inversions of $\widehat{\sigma}$ are precisely
the inversions of $\sigma$ (because $\ell\left(  \widehat{\sigma}\right)  $ is
the number of inversions of $\widehat{\sigma}$, whereas $\ell\left(
\sigma\right)  $ is the number of inversions of $\sigma$).
\par
If $\left(  i,j\right)  $ is an inversion of $\sigma$, then $\left(
i,j\right)  $ is an inversion of $\widehat{\sigma}$ (because if $\left(
i,j\right)  $ is an inversion of $\sigma$, then both $i$ and $j$ are $<n$, and
thus the definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(
i\right)  =\sigma\left(  i\right)  $ and $\widehat{\sigma}\left(  j\right)
=\sigma\left(  j\right)  $). In other words, every inversion of $\sigma$ is an
inversion of $\widehat{\sigma}$.
\par
On the other hand, let $\left(  u,v\right)  $ be an inversion of
$\widehat{\sigma}$. We shall prove that $\left(  u,v\right)  $ is an inversion
of $\sigma$.
\par
Indeed, $\left(  u,v\right)  $ is an inversion of $\widehat{\sigma}$. In other
words, $\left(  u,v\right)  $ is a pair of integers satisfying $1\leq u<v\leq
n$ and $\widehat{\sigma}\left(  u\right)  >\widehat{\sigma}\left(  v\right)
$.
\par
If we had $v=n$, then we would have $\widehat{\sigma}\left(  u\right)
>\widehat{\sigma}\left(  \underbrace{v}_{=n}\right)  =\widehat{\sigma}\left(
n\right)  =n$ (by the definition of $\widehat{\sigma}$), which would
contradict $\widehat{\sigma}\left(  u\right)  \in\left\{  1,2,\ldots
,n\right\}  $. Thus, we cannot have $v=n$. We therefore have $v<n$, so that
$v\leq n-1$. Now, $1\leq u<v\leq n-1$. Thus, both $\sigma\left(  u\right)  $
and $\sigma\left(  v\right)  $ are well-defined. The definition of
$\widehat{\sigma}$ yields $\widehat{\sigma}\left(  u\right)  =\sigma\left(
u\right)  $ (since $u\leq n-1<n$) and $\widehat{\sigma}\left(  v\right)
=\sigma\left(  v\right)  $ (since $v\leq n-1<n$), so that $\sigma\left(
u\right)  =\widehat{\sigma}\left(  u\right)  >\widehat{\sigma}\left(
v\right)  =\sigma\left(  v\right)  $. Thus, $\left(  u,v\right)  $ is a pair
of integers satisfying $1\leq u<v\leq n-1$ and $\sigma\left(  u\right)
>\sigma\left(  v\right)  $. In other words, $\left(  u,v\right)  $ is an
inversion of $\sigma$.
\par
We thus have shown that every inversion of $\widehat{\sigma}$ is an inversion
of $\sigma$. Combining this with the fact that every inversion of $\sigma$ is
an inversion of $\widehat{\sigma}$, we thus conclude that the inversions of
$\widehat{\sigma}$ are precisely the inversions of $\sigma$. As we have
already said, this finishes the proof of (\ref{pf.lem.laplace.lem.short.-1}).}
and%
\begin{equation}
\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(  i\right)  }=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  } \label{pf.lem.laplace.lem.short.prod}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.lem.short.prod}):} Let
$\sigma\in S_{n-1}$. The definition of $\widehat{\sigma}$ yields
$\widehat{\sigma}\left(  i\right)  =\sigma\left(  i\right)  $ for every
$i\in\left\{  1,2,\ldots,n-1\right\}  $. Thus, $a_{i,\widehat{\sigma}\left(
i\right)  }=a_{i,\sigma\left(  i\right)  }$ for every $i\in\left\{
1,2,\ldots,n-1\right\}  $. Hence, $\prod_{i=1}^{n-1}%
\underbrace{a_{i,\widehat{\sigma}\left(  i\right)  }}_{=a_{i,\sigma\left(
i\right)  }}=\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }$, qed.}.
\end{vershort}

\begin{verlong}
For every $m\in\mathbb{N}$ and every $\sigma\in S_{m}$, let
$\operatorname*{Inv}\left(  \sigma\right)  $ be the set of all inversions of
the permutation $\sigma$. Thus, for every $m\in\mathbb{N}$ and $\sigma\in
S_{m}$, we have%
\begin{align}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \nonumber\\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\label{pf.lem.laplace.lem.l}%
\end{align}

\end{verlong}

\begin{verlong}
Moreover, every $\sigma\in S_{n-1}$ satisfies
\[
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  \subseteq
\operatorname*{Inv}\left(  \sigma\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Let $c\in
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $. Thus, $c$ is an
inversion of $\widehat{\sigma}$ (since $\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $ is the set of all inversions of $\widehat{\sigma}%
$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\widehat{\sigma}\left(  i\right)
>\widehat{\sigma}\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion\textquotedblright).
\par
We have $\widehat{\sigma}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}
$, thus $\widehat{\sigma}\left(  i\right)  \leq n$. From $\widehat{\sigma
}\left(  i\right)  >\widehat{\sigma}\left(  j\right)  $, we obtain
$\widehat{\sigma}\left(  j\right)  <\widehat{\sigma}\left(  i\right)  \leq n$
and thus $\widehat{\sigma}\left(  j\right)  \leq n-1$ (since $\widehat{\sigma
}\left(  j\right)  $ and $n$ are integers).
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  n\right)
=%
\begin{cases}
\sigma\left(  n\right)  , & \text{if }n<n;\\
n, & \text{if }n=n
\end{cases}
=n$ (since $n=n$). Hence, $\widehat{\sigma}\left(  n\right)  =n\neq
\widehat{\sigma}\left(  j\right)  $ (since $\widehat{\sigma}\left(  j\right)
<\widehat{\sigma}\left(  n\right)  $), so that $n\neq j$. Hence, $j\neq n$, so
that $j<n$ (since $j\leq n$). Hence, $1\leq i<j\leq n-1$.
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  i\right)
=%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
=\sigma\left(  i\right)  $ (since $i<n$). The definition of $\widehat{\sigma}$
yields $\widehat{\sigma}\left(  j\right)  =%
\begin{cases}
\sigma\left(  j\right)  , & \text{if }j<n;\\
n, & \text{if }j=n
\end{cases}
=\sigma\left(  j\right)  $ (since $j<n$). Now, $\sigma\left(  i\right)
=\widehat{\sigma}\left(  i\right)  >\widehat{\sigma}\left(  j\right)
=\sigma\left(  j\right)  $.
\par
Now, we know that $\left(  i,j\right)  $ is a pair of integers satisfying
$1\leq i<j\leq n-1$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $.
In other words, $\left(  i,j\right)  $ is an inversion of $\sigma$ (by the
definition of an \textquotedblleft inversion\textquotedblright). In other
words, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $
(since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of all
inversions of $\sigma$). Thus, $c=\left(  i,j\right)  \in\operatorname*{Inv}%
\left(  \sigma\right)  $.
\par
Let us now forget that we fixed $c$. We thus have proven that $c\in
\operatorname*{Inv}\left(  \sigma\right)  $ for every $c\in\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  $. In other words, $\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  \subseteq\operatorname*{Inv}\left(
\sigma\right)  $, qed.} and%
\[
\operatorname*{Inv}\left(  \sigma\right)  \subseteq\operatorname*{Inv}\left(
\widehat{\sigma}\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. Let $c\in
\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $c$ is an inversion of
$\sigma$ (since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of all
inversions of $\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $
of integers satisfying $1\leq i<j\leq n-1$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $ (by the definition of an \textquotedblleft
inversion\textquotedblright).
\par
We have $j\leq n-1<n$ and thus $i<j<n$. Since $1\leq i\leq n-1\leq n$, we have
$i\in\left\{  1,2,\ldots,n\right\}  $. Since $1\leq j\leq n-1\leq n$, we have
$j\in\left\{  1,2,\ldots,n\right\}  $.
\par
The definition of $\widehat{\sigma}$ yields $\widehat{\sigma}\left(  i\right)
=%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
=\sigma\left(  i\right)  $ (since $i<n$). The definition of $\widehat{\sigma}$
yields $\widehat{\sigma}\left(  j\right)  =%
\begin{cases}
\sigma\left(  j\right)  , & \text{if }j<n;\\
n, & \text{if }j=n
\end{cases}
=\sigma\left(  j\right)  $ (since $j<n$). Now, $\widehat{\sigma}\left(
i\right)  =\sigma\left(  i\right)  >\sigma\left(  j\right)  =\widehat{\sigma
}\left(  j\right)  $.
\par
Now, we know that $\left(  i,j\right)  $ is a pair of integers satisfying
$1\leq i<j\leq n$ and $\widehat{\sigma}\left(  i\right)  >\widehat{\sigma
}\left(  j\right)  $. In other words, $\left(  i,j\right)  $ is an inversion
of $\widehat{\sigma}$ (by the definition of an \textquotedblleft
inversion\textquotedblright). In other words, $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ (since
$\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ is the set of all
inversions of $\widehat{\sigma}$). Thus, $c=\left(  i,j\right)  \in
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $.
\par
Let us now forget that we fixed $c$. We thus have proven that $c\in
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ for every $c\in
\operatorname*{Inv}\left(  \sigma\right)  $. In other words,
$\operatorname*{Inv}\left(  \sigma\right)  \subseteq\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $, qed.} and%
\[
\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
\widehat{\sigma}\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. We have proven the two
relations $\operatorname*{Inv}\left(  \sigma\right)  \subseteq
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  $ and $\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  \subseteq\operatorname*{Inv}\left(
\sigma\right)  $. Combining these two relations, we obtain
$\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $. Qed.} and%
\[
\ell\left(  \sigma\right)  =\ell\left(  \widehat{\sigma}\right)
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. We have proven that
$\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
\widehat{\sigma}\right)  $. Now, (\ref{pf.lem.laplace.lem.l}) (applied to
$m=n-1$) yields $\ell\left(  \sigma\right)  =\left\vert
\underbrace{\operatorname*{Inv}\left(  \sigma\right)  }_{=\operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  }\right\vert =\left\vert \operatorname*{Inv}%
\left(  \widehat{\sigma}\right)  \right\vert $. On the other hand,
(\ref{pf.lem.laplace.lem.l}) (applied to $n-1$ and $\widehat{\sigma}$ instead
of $m$ and $\sigma$) yields $\ell\left(  \widehat{\sigma}\right)  =\left\vert
\operatorname*{Inv}\left(  \widehat{\sigma}\right)  \right\vert $. Comparing
this with $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}\left(
\widehat{\sigma}\right)  \right\vert $, we obtain $\ell\left(  \sigma\right)
=\ell\left(  \widehat{\sigma}\right)  $, qed.} and%
\begin{equation}
\left(  -1\right)  ^{\widehat{\sigma}}=\left(  -1\right)  ^{\sigma}
\label{pf.lem.laplace.lem.-1}%
\end{equation}
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. We have proven that
$\ell\left(  \sigma\right)  =\ell\left(  \widehat{\sigma}\right)  $. But the
definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(  -1\right)
^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$. Also, the
definition of $\left(  -1\right)  ^{\widehat{\sigma}}$ yields $\left(
-1\right)  ^{\widehat{\sigma}}=\left(  -1\right)  ^{\ell\left(
\widehat{\sigma}\right)  }$. Thus,%
\begin{align*}
\left(  -1\right)  ^{\sigma}  &  =\left(  -1\right)  ^{\ell\left(
\sigma\right)  }=\left(  -1\right)  ^{\ell\left(  \widehat{\sigma}\right)
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\right)
=\ell\left(  \widehat{\sigma}\right)  \right) \\
&  =\left(  -1\right)  ^{\widehat{\sigma}}.
\end{align*}
This proves (\ref{pf.lem.laplace.lem.-1}).} and%
\begin{equation}
\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(  i\right)  }=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  } \label{pf.lem.laplace.lem.prod}%
\end{equation}
\footnote{\textit{Proof.} Let $\sigma\in S_{n-1}$. For every $i\in\left\{
1,2,\ldots,n-1\right\}  $, the element $\widehat{\sigma}\left(  i\right)  $ is
well-defined (since $i\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $) and satisfies%
\begin{align*}
\widehat{\sigma}\left(  i\right)   &  =%
\begin{cases}
\sigma\left(  i\right)  , & \text{if }i<n;\\
n, & \text{if }i=n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widehat{\sigma
}\right) \\
&  =\sigma\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<n\text{
(since }i\leq n-1\text{ (since }i\in\left\{  1,2,\ldots,n-1\right\}
\text{))}\right)  .
\end{align*}
Hence, for every $i\in\left\{  1,2,\ldots,n-1\right\}  $, we have
$a_{i,\widehat{\sigma}\left(  i\right)  }=a_{i,\sigma\left(  i\right)  }$
(since $\widehat{\sigma}\left(  i\right)  =\sigma\left(  i\right)  $). Thus,
$\prod_{i=1}^{n-1}\underbrace{a_{i,\widehat{\sigma}\left(  i\right)  }%
}_{=a_{i,\sigma\left(  i\right)  }}=\prod_{i=1}^{n-1}a_{i,\sigma\left(
i\right)  }$. This proves (\ref{pf.lem.laplace.lem.prod}).}.
\end{verlong}

\begin{vershort}
Now,
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}%
}}_{\substack{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }=\sum_{\sigma\in T}\\\text{(since }\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  =T\text{)}}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in T}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{i,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n-1}}\underbrace{\left(
-1\right)  ^{\Phi\left(  \sigma\right)  }\prod_{i=1}^{n-1}a_{i,\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\left(
-1\right)  ^{\widehat{\sigma}}\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(
i\right)  }\\\text{(since }\Phi\left(  \sigma\right)  =\widehat{\sigma
}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\Phi\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }\Phi:S_{n-1}\rightarrow T\text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{\sigma\in S_{n-1}}\underbrace{\left(  -1\right)  ^{\widehat{\sigma}%
}}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{pf.lem.laplace.lem.short.-1}))}}}\underbrace{\prod_{i=1}^{n-1}%
a_{i,\widehat{\sigma}\left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.laplace.lem.short.prod}))}}}=\sum_{\sigma\in S_{n-1}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }.
\end{align*}
Compared with%
\begin{align*}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)   &  =\sum_{\sigma\in S_{n-1}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.det.eq.2}), applied to }n-1\text{ and}\\
\left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\text{ instead of
}n\text{ and }A
\end{array}
\right)  ,
\end{align*}
this yields%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }%
=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  .
\]
This proves Lemma \ref{lem.laplace.lem}.
\end{vershort}

\begin{verlong}
Now,%
\begin{align}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}%
}}_{\substack{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }=\sum_{\sigma\in T}\\\text{(since }\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  =T\text{)}}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\nonumber\\
&  =\sum_{\sigma\in T}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{i,\sigma\left(  i\right)  }=\sum_{\sigma\in S_{n-1}}\underbrace{\left(
-1\right)  ^{\Phi\left(  \sigma\right)  }\prod_{i=1}^{n-1}a_{i,\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\left(
-1\right)  ^{\widehat{\sigma}}\prod_{i=1}^{n-1}a_{i,\widehat{\sigma}\left(
i\right)  }\\\text{(since }\Phi\left(  \sigma\right)  =\widehat{\sigma
}\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\Phi\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }\Phi:S_{n-1}\rightarrow T\text{ is a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{\sigma\in S_{n-1}}\underbrace{\left(  -1\right)  ^{\widehat{\sigma}%
}}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{pf.lem.laplace.lem.-1}))}}}\underbrace{\prod_{i=1}^{n-1}%
a_{i,\widehat{\sigma}\left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }\\\text{(by (\ref{pf.lem.laplace.lem.prod}%
))}}}\nonumber\\
&  =\sum_{\sigma\in S_{n-1}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }. \label{pf.lem.laplace.lem.almostthere}%
\end{align}
But (\ref{eq.det.eq.2}) (applied to $n-1$ and $\left(  a_{i,j}\right)  _{1\leq
i\leq n-1,\ 1\leq j\leq n-1}$ instead of $n$ and $A$) yields%
\[
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  =\sum_{\sigma\in S_{n-1}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }.
\]
Compared with (\ref{pf.lem.laplace.lem.almostthere}), this yields%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }%
=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  .
\]
This proves Lemma \ref{lem.laplace.lem}.
\end{verlong}
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm.laplace.pre}.]Every permutation $\sigma\in S_{n}$
satisfying $\sigma\left(  n\right)  \neq n$ satisfies
\begin{equation}
a_{n,\sigma\left(  n\right)  }=0 \label{pf.thm.laplace.pre.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.laplace.pre.1}):} Let $\sigma\in
S_{n}$ be a permutation satisfying $\sigma\left(  n\right)  \neq n$. Since
$\sigma\left(  n\right)  \in\left\{  1,2,\ldots,n\right\}  $ and
$\sigma\left(  n\right)  \neq n$, we have $\sigma\left(  n\right)  \in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots
,n-1\right\}  $. Hence, (\ref{eq.thm.laplace.pre.ass}) (applied to
$j=\sigma\left(  n\right)  $) shows that $a_{n,\sigma\left(  n\right)  }=0$,
qed.}.

From (\ref{eq.det.eq.2}), we obtain%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{=\left(
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\right)  a_{n,\sigma\left(
n\right)  }}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\right)  a_{n,\sigma\left(
n\right)  }\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)
}\right)  \underbrace{a_{n,\sigma\left(  n\right)  }}_{\substack{=a_{n,n}%
\\\text{(since }\sigma\left(  n\right)  =n\text{)}}}+\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  \neq n}}\left(  -1\right)  ^{\sigma}\left(
\prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }\right)
\underbrace{a_{n,\sigma\left(  n\right)  }}_{\substack{=0\\\text{(by
(\ref{pf.thm.laplace.pre.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every }\sigma\in S_{n}\text{
satisfies either }\sigma\left(  n\right)  =n\text{ or }\sigma\left(  n\right)
\neq n\text{ (but not both)}\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)
}\right)  a_{n,n}+\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
n\right)  \neq n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n-1}a_{i,\sigma\left(  i\right)  }\right)  0}_{=0}\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i=1}^{n-1}a_{i,\sigma\left(  i\right)
}\right)  a_{n,n}=a_{n,n}\cdot\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  =n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)  \\\text{(by Lemma
\ref{lem.laplace.lem})}}}\\
&  =a_{n,n}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq
j\leq n-1}\right)  .
\end{align*}
This proves Theorem \ref{thm.laplace.pre}.
\end{proof}

Let us finally state an analogue of Theorem \ref{thm.laplace.pre} in which the
last column (rather than the last row) is required to consist mostly of zeroes:

\begin{corollary}
\label{cor.laplace.pre.col}Let $n$ be a positive integer. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Assume that%
\begin{equation}
a_{i,n}=0\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots
,n-1\right\}  . \label{eq.cor.laplace.pre.col.ass}%
\end{equation}
Then, $\det A=a_{n,n}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right)  $.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.laplace.pre.col}.]We have $n-1\in\mathbb{N}$
(since $n$ is a positive integer).

We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, and thus
$A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the
definition of the transpose matrix $A^{T}$). Also, for every $j\in\left\{
1,2,\ldots,n-1\right\}  $, we have $a_{j,n}=0$ (by
(\ref{eq.cor.laplace.pre.col.ass}), applied to $i=j$). Thus, Theorem
\ref{thm.laplace.pre} (applied to $A^{T}$ and $a_{j,i}$ instead of $A$ and
$a_{i,j}$) yields
\begin{equation}
\det\left(  A^{T}\right)  =a_{n,n}\cdot\det\left(  \left(  a_{j,i}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)  .
\label{pf.cor.laplace.pre.col.1}%
\end{equation}


But Exercise \ref{exe.ps4.4} shows that $\det\left(  A^{T}\right)  =\det A$.
Thus, $\det A=\det\left(  A^{T}\right)  $. Also, the definition of the
transpose of a matrix shows that $\left(  \left(  a_{i,j}\right)  _{1\leq
i\leq n-1,\ 1\leq j\leq n-1}\right)  ^{T}=\left(  a_{j,i}\right)  _{1\leq
i\leq n-1,\ 1\leq j\leq n-1}$. Thus,%
\[
\det\left(  \left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  ^{T}\right)  =\det\left(  \left(  a_{j,i}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right)  .
\]
Comparing this with%
\[
\det\left(  \left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  ^{T}\right)  =\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right)
\]
(by Exercise \ref{exe.ps4.4}, applied to $n-1$ and $\left(  a_{i,j}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq n-1}$ instead of $n$ and $A$), we obtain%
\[
\det\left(  \left(  a_{j,i}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  =\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq
j\leq n-1}\right)  .
\]


Now,%
\begin{align*}
\det A  &  =\det\left(  A^{T}\right)  =a_{n,n}\cdot\underbrace{\det\left(
\left(  a_{j,i}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)  }%
_{=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.cor.laplace.pre.col.1})}\right) \\
&  =a_{n,n}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq
j\leq n-1}\right)  .
\end{align*}
This proves Corollary \ref{cor.laplace.pre.col}.
\end{proof}

\subsection{The Vandermonde determinant}

\subsubsection{The statement}

An example for an application of Theorem \ref{thm.laplace.pre} is the famous
\textit{Vandermonde determinant}:

\begin{theorem}
\label{thm.vander-det}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$. Then:

\textbf{(a)} We have%
\[
\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(b)} We have%
\[
\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(c)} We have%
\[
\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\]


\textbf{(d)} We have%
\[
\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\]

\end{theorem}

\begin{remark}
For $n=4$, the four matrices appearing in Theorem \ref{thm.vander-det} are%
\begin{align*}
\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3} & x_{1}^{2} & x_{1} & 1\\
x_{2}^{3} & x_{2}^{2} & x_{2} & 1\\
x_{3}^{3} & x_{3}^{2} & x_{3} & 1\\
x_{4}^{3} & x_{4}^{2} & x_{4} & 1
\end{array}
\right)  ,\\
\left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3} & x_{2}^{3} & x_{3}^{3} & x_{4}^{3}\\
x_{1}^{2} & x_{2}^{2} & x_{3}^{2} & x_{4}^{2}\\
x_{1} & x_{2} & x_{3} & x_{4}\\
1 & 1 & 1 & 1
\end{array}
\right)  ,\\
\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
1 & x_{1} & x_{1}^{2} & x_{1}^{3}\\
1 & x_{2} & x_{2}^{2} & x_{2}^{3}\\
1 & x_{3} & x_{3}^{2} & x_{3}^{3}\\
1 & x_{4} & x_{4}^{2} & x_{4}^{3}%
\end{array}
\right)  ,\\
\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}  &  =\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
x_{1} & x_{2} & x_{3} & x_{4}\\
x_{1}^{2} & x_{2}^{2} & x_{3}^{2} & x_{4}^{2}\\
x_{1}^{3} & x_{2}^{3} & x_{3}^{3} & x_{4}^{3}%
\end{array}
\right)  .
\end{align*}
It is clear that the second of these four matrices is the transpose of the
first; the fourth is the transpose of the third; and the fourth is obtained
from the second by rearranging the rows in opposite order. Thus, the four
parts of Theorem \ref{thm.vander-det} are rather easily seen to be equivalent.
(We shall prove part \textbf{(a)} and derive the others from it.) Nevertheless
it is useful to have seen them all.
\end{remark}

Theorem \ref{thm.vander-det} is a classical result (known as the
\href{https://en.wikipedia.org/wiki/Vandermonde_matrix}{\textit{Vandermonde
determinant}}, although \href{http://arxiv.org/abs/1204.4716}{it is unclear}
whether it has been proven by Vandermonde): Almost all texts on linear algebra
mention it (or, rather, at least one of its four parts), although some only
prove it in lesser generality. It is a fundamental result that has various
applications to abstract algebra, number theory, coding theory, combinatorics
and numerical mathematics.

Theorem \ref{thm.vander-det} has many known proofs\footnote{For four
combinatorial proofs, see \cite{Gessel-Vand}, \cite[\S 5.3]{Aigner07},
\cite[\S 12.9]{Loehr-BC} and \cite{BenDre-Vand}. (Specifically,
\cite{Gessel-Vand} and \cite{BenDre-Vand} prove Theorem \ref{thm.vander-det}
\textbf{(c)}, whereas \cite[\S 5.3]{Aigner07} and \cite[\S 12.9]{Loehr-BC}
prove Theorem \ref{thm.vander-det} \textbf{(b)}. But as we will see, the four
parts of Theorem \ref{thm.vander-det} are easily seen to be equivalent to each
other.)}. My favorite proof (of Theorem \ref{thm.vander-det} \textbf{(c)}
only, but as I said the other parts are easily seen to be equivalent) is given
in \cite[Theorem 1]{GriHyp}. In these notes, I will show two others.

\subsubsection{A proof by induction}

The first proof I shall present has the advantage of demonstrating how Theorem
\ref{thm.laplace.pre} can be used (together with induction) in computing determinants.

\begin{example}
\label{exa.vander-det.3}Let $x,y,z\in\mathbb{K}$. Let $A=\left(
\begin{array}
[c]{ccc}%
1 & x & x^{2}\\
1 & y & y^{2}\\
1 & z & z^{2}%
\end{array}
\right)  $. Then, (\ref{eq.det.small.3x3}) shows that%
\begin{align}
\det A  &  =1yz^{2}+xy^{2}\cdot1+x^{2}\cdot1z-1y^{2}z-x\cdot1z^{2}-x^{2}%
y\cdot1\nonumber\\
&  =yz^{2}+xy^{2}+x^{2}z-y^{2}z-xz^{2}-x^{2}y\nonumber\\
&  =yz\left(  z-y\right)  +zx\left(  x-z\right)  +xy\left(  y-x\right)  .
\label{eq.exa.vander-det.3.1}%
\end{align}
On the other hand, Theorem \ref{thm.vander-det} \textbf{(c)} (applied to
$n=3$, $x_{1}=x$, $x_{2}=y$ and $x_{3}=z$) yields $\det A=\left(  y-x\right)
\left(  z-x\right)  \left(  z-y\right)  $. Compared with
(\ref{eq.exa.vander-det.3.1}), this yields%
\begin{equation}
\left(  y-x\right)  \left(  z-x\right)  \left(  z-y\right)  =yz\left(
z-y\right)  +zx\left(  x-z\right)  +xy\left(  y-x\right)  .
\label{eq.exa.vander-det.3.2}%
\end{equation}
You might have encountered this curious identity as a trick of use in contest
problems. When $x,y,z$ are three distinct complex numbers, we can divide
(\ref{eq.exa.vander-det.3.2}) by $\left(  y-x\right)  \left(  z-x\right)
\left(  z-y\right)  $, and obtain%
\[
1=\dfrac{yz}{\left(  y-x\right)  \left(  z-x\right)  }+\dfrac{zx}{\left(
z-y\right)  \left(  x-y\right)  }+\dfrac{xy}{\left(  x-z\right)  \left(
y-z\right)  }.
\]

\end{example}

Before we prove Theorem \ref{thm.vander-det}, let us see (in greater
generality) what happens to the determinant of a matrix if we rearrange the
rows in opposite order:

\begin{lemma}
\label{lem.vander-det.lem-rearr}Let $n\in\mathbb{N}$. Let $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Then,%
\[
\det\left(  \left(  a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.vander-det.lem-rearr}.]Let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $. Define a permutation $w_{0}$
in $S_{n}$ as in Exercise \ref{exe.ps4.1c}. In the solution of Exercise
\ref{exe.ps4.1c}, we have shown that $\left(  -1\right)  ^{w_{0}}=\left(
-1\right)  ^{n\left(  n-1\right)  /2}$.

\begin{verlong}
Now, $w_{0}\in S_{n}$. In other words, $w_{0}$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$w_{0}$ is a permutation of the set $\left[  n\right]  $ (since $\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $). In other words, $w_{0}$ is a
bijective map $\left[  n\right]  \rightarrow\left[  n\right]  $.
\end{verlong}

Now, we can apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, $w_{0}$ and $\left(
a_{w_{0}\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
instead of $B$, $\kappa$ and $B_{\kappa}$. As a result, we obtain%
\begin{align}
\det\left(  \left(  a_{w_{0}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\underbrace{\left(  -1\right)  ^{w_{0}}%
}_{=\left(  -1\right)  ^{n\left(  n-1\right)  /2}}\cdot\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\label{pf.lem.vander-det.lem-rearr.1}%
\end{align}


\begin{vershort}
But $w_{0}\left(  i\right)  =n+1-i$ for every $i\in\left\{  1,2,\ldots
,n\right\}  $ (by the definition of $w_{0}$). Thus,
(\ref{pf.lem.vander-det.lem-rearr.1}) rewrites as $\det\left(  \left(
a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(
-1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  $. This proves Lemma
\ref{lem.vander-det.lem-rearr}.
\end{vershort}

\begin{verlong}
However, $w_{0}\left(  i\right)  =n+1-i$ for every $i\in\left\{
1,2,\ldots,n\right\}  $ (by the definition of $w_{0}$). Hence, $a_{w_{0}%
\left(  i\right)  ,j}=a_{n+1-i,j}$ for every $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. Thus,%
\[
\det\left(  \left(  \underbrace{a_{w_{0}\left(  i\right)  ,j}}_{=a_{n+1-i,j}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\det\left(  \left(
a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  ,
\]
so that%
\begin{align*}
\det\left(  \left(  a_{n+1-i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)   &  =\det\left(  \left(  a_{w_{0}\left(  i\right)  ,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This proves Lemma \ref{lem.vander-det.lem-rearr}.
\end{verlong}
\end{proof}

\begin{proof}
[First proof of Theorem \ref{thm.vander-det}.]\textbf{(a)} For every
$u\in\left\{  0,1,\ldots,n\right\}  $, let $A_{u}$ be the $u\times u$-matrix
$\left(  x_{i}^{u-j}\right)  _{1\leq i\leq u,\ 1\leq j\leq u}$.

Now, let us show that%
\begin{equation}
\det\left(  A_{u}\right)  =\prod_{1\leq i<j\leq u}\left(  x_{i}-x_{j}\right)
\label{pf.thm.vander-det.a.goal}%
\end{equation}
for every $u\in\left\{  0,1,\ldots,n\right\}  $.

\textit{Proof of (\ref{pf.thm.vander-det.a.goal}):} We will prove
(\ref{pf.thm.vander-det.a.goal}) by induction over $u$:

\textit{Induction base:} The matrix $A_{0}$ is a $0\times0$-matrix and thus
has determinant $\det\left(  A_{0}\right)  =1$. On the other hand, the product
$\prod_{1\leq i<j\leq0}\left(  x_{i}-x_{j}\right)  $ is an empty product
(i.e., a product of $0$ elements of $\mathbb{K}$) and thus equals $1$ as well.
Hence, both $\det\left(  A_{0}\right)  $ and $\prod_{1\leq i<j\leq0}\left(
x_{i}-x_{j}\right)  $ equal $1$. Thus, $\det\left(  A_{0}\right)
=\prod_{1\leq i<j\leq0}\left(  x_{i}-x_{j}\right)  $. In other words,
(\ref{pf.thm.vander-det.a.goal}) holds for $u=0$. The induction base is thus complete.

\textit{Induction step:} Let $U\in\left\{  1,2,\ldots,n\right\}  $. Assume
that (\ref{pf.thm.vander-det.a.goal}) holds for $u=U-1$. We need to prove that
(\ref{pf.thm.vander-det.a.goal}) holds for $u=U$.

Recall that $A_{U}=\left(  x_{i}^{U-j}\right)  _{1\leq i\leq U,\ 1\leq j\leq
U}$ (by the definition of $A_{U}$).

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, define
$b_{i,j}\in\mathbb{K}$ by%
\[
b_{i,j}=%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
.
\]
Let $B$ be the $U\times U$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$. For example, if $U=4$, then%
\begin{align*}
A_{U}  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3} & x_{1}^{2} & x_{1} & 1\\
x_{2}^{3} & x_{2}^{2} & x_{2} & 1\\
x_{3}^{3} & x_{3}^{2} & x_{3} & 1\\
x_{4}^{3} & x_{4}^{2} & x_{4} & 1
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{and}\\
B  &  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3}-x_{4}x_{1}^{2} & x_{1}^{2}-x_{4}x_{1} & x_{1}-x_{4} & 1\\
x_{2}^{3}-x_{4}x_{2}^{2} & x_{2}^{2}-x_{4}x_{2} & x_{2}-x_{4} & 1\\
x_{3}^{3}-x_{4}x_{3}^{2} & x_{3}^{2}-x_{4}x_{3} & x_{3}-x_{4} & 1\\
x_{4}^{3}-x_{4}x_{4}^{2} & x_{4}^{2}-x_{4}x_{4} & x_{4}-x_{4} & 1
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
x_{1}^{3}-x_{4}x_{1}^{2} & x_{1}^{2}-x_{4}x_{1} & x_{1}-x_{4} & 1\\
x_{2}^{3}-x_{4}x_{2}^{2} & x_{2}^{2}-x_{4}x_{2} & x_{2}-x_{4} & 1\\
x_{3}^{3}-x_{4}x_{3}^{2} & x_{3}^{2}-x_{4}x_{3} & x_{3}-x_{4} & 1\\
0 & 0 & 0 & 1
\end{array}
\right)  .
\end{align*}
We claim that $\det B=\det\left(  A_{U}\right)  $. Indeed, here are two ways
to prove this:

\textit{First proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Exercise
\ref{exe.ps4.6k} \textbf{(b)} shows that the determinant of a $U\times
U$-matrix does not change if we subtract a multiple of one of its columns from
another column. Now, let us subtract $x_{U}$ times the $2$-nd column of
$A_{U}$ from the $1$-st column, then subtract $x_{U}$ times the $3$-rd column
of the resulting matrix from the $2$-nd column, and so on, all the way until
we finally subtract $x_{U}$ times the $U$-th column of the matrix from the
$\left(  U-1\right)  $-st column\footnote{So, all in all, we subtract the
$x_{U}$-multiple of each column from its neighbor to its left, but the order
in which we are doing it (namely, from left to right) is important: It means
that the column we are subtracting is unchanged from $A_{U}$. (If we would be
doing these subtractions from right to left instead, then the columns to be
subtracting would be changed by the preceding steps.)}. The resulting matrix
is $B$ (according to our definition of $B$). Thus, $\det B=\det\left(
A_{U}\right)  $ (since our subtractions never change the determinant). This
proves $\det B=\det\left(  A_{U}\right)  $.

\textit{Second proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Here is
another way to prove that $\det B=\det\left(  A_{U}\right)  $, with some less handwaving.

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, we
define $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
-x_{U}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
.
\]
Let $C$ be the $U\times U$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$.

For example, if $U=4$, then%
\[
C=\left(
\begin{array}
[c]{cccc}%
1 & 0 & 0 & 0\\
-x_{4} & 1 & 0 & 0\\
0 & -x_{4} & 1 & 0\\
0 & 0 & -x_{4} & 1
\end{array}
\right)  .
\]


\begin{vershort}
The matrix $C$ is lower-triangular, and thus Exercise \ref{exe.ps4.3} shows
that its determinant is $\det C=\underbrace{c_{1,1}}_{=1}\underbrace{c_{2,2}%
}_{=1}\cdots\underbrace{c_{U,U}}_{=1}=1$.
\end{vershort}

\begin{verlong}
We have $c_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$ be such that
$i<j$. Then, $i\neq j$ (since $i<j$) and $i\neq j+1$ (since $i<j<j+1$). Thus,
we have neither $i=j$ nor $i=j+1$. Now, the definition of $c_{i,j}$ yields
$c_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
-x_{U}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
=0$ (since neither $i=j$ nor $i=j+1$), qed.}. Hence, Exercise \ref{exe.ps4.3}
(applied to $U$, $C$ and $c_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{align*}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{U,U}=\prod_{i=1}^{U}\underbrace{c_{i,i}%
}_{\substack{=%
\begin{cases}
1, & \text{if }i=i;\\
-x_{U}, & \text{if }i=i+1;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }c_{i,i}\text{)}}}=\prod_{i=1}^{U}\underbrace{%
\begin{cases}
1, & \text{if }i=i;\\
-x_{U}, & \text{if }i=i+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=1\\\text{(since }i=i\text{)}}}\\
&  =\prod_{i=1}^{U}1=1.
\end{align*}

\end{verlong}

\begin{vershort}
On the other hand, it is easy to see that $B=A_{U}C$ (check this!). Thus,
Theorem \ref{thm.det(AB)} yields $\det B=\det\left(  A_{U}\right)
\cdot\underbrace{\det C}_{=1}=\det\left(  A_{U}\right)  $. So we have proven
$\det B=\det\left(  A_{U}\right)  $ again.
\end{vershort}

\begin{verlong}
On the other hand, every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U\right\}  ^{2}$ satisfy%
\begin{equation}
b_{i,j}=\sum_{k=1}^{U}x_{i}^{U-k}c_{k,j} \label{pf.thm.vander-det.B=AUC.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.vander-det.B=AUC.pf.1}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$. We must prove
(\ref{pf.thm.vander-det.B=AUC.pf.1}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,U\right\}  $ and $j\in\left\{  1,2,\ldots,U\right\}
$. We are in one of the following two cases:
\par
\textit{Case 1:} We have $j<U$.
\par
\textit{Case 2:} We have $j\geq U$.
\par
Let us first consider Case 1. In this case, we have $j<U$. Hence,
$j\in\left\{  1,2,\ldots,U-1\right\}  $ (since $j\in\left\{  1,2,\ldots
,U\right\}  $). Now, the definition of $b_{i,j}$ yields%
\begin{equation}
b_{i,j}=%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j+1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
=x_{i}^{U-j}-x_{U}x_{i}^{U-j-1} \label{pf.thm.vander-det.B=AUC.pf.1.pf.1}%
\end{equation}
(since $j<U$). On the other hand, $j\in\left\{  1,2,\ldots,U-1\right\}  $, so
that $j+1\in\left\{  2,3,\ldots,U\right\}  \subseteq\left\{  1,2,\ldots
,U\right\}  $. Thus, $j$ and $j+1$ are two distinct elements of $\left\{
1,2,\ldots,U\right\}  $.
\par
The definition of $c_{j,j}$ yields%
\[
c_{j,j}=%
\begin{cases}
1, & \text{if }j=j;\\
-x_{U}, & \text{if }j=j+1;\\
0, & \text{otherwise}%
\end{cases}
=1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=j\right)  .
\]
The definition of $c_{j+1,j}$ yields%
\[
c_{j+1,j}=%
\begin{cases}
1, & \text{if }j+1=j;\\
-x_{U}, & \text{if }j+1=j+1;\\
0, & \text{otherwise}%
\end{cases}
=-x_{U}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1=j+1\right)  .
\]
For every $k\in\left\{  1,2,\ldots,U\right\}  $ satisfying $k\notin\left\{
j,j+1\right\}  $, we have%
\begin{align}
c_{k,j}  &  =%
\begin{cases}
1, & \text{if }k=j;\\
-x_{U}, & \text{if }k=j+1;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }c_{k,j}\right)
\nonumber\\
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }k=j\text{ nor
}k=j+1\text{ (because }k\notin\left\{  j,j+1\right\}  \text{)}\right)
\label{pf.thm.vander-det.B=AUC.pf.1.pf.6}%
\end{align}
\par
Now,%
\begin{align*}
&  \underbrace{\sum_{k=1}^{U}}_{=\sum_{k\in\left\{  1,2,\ldots,U\right\}  }%
}x_{i}^{U-k}c_{k,j}\\
&  =\sum_{k\in\left\{  1,2,\ldots,U\right\}  }x_{i}^{U-k}c_{k,j}\\
&  =x_{i}^{U-j}\underbrace{c_{j,j}}_{=1}+\underbrace{x_{i}^{U-\left(
j+1\right)  }}_{=x_{i}^{U-j-1}}\underbrace{c_{j+1,j}}_{=-x_{U}}+\sum
_{\substack{k\in\left\{  1,2,\ldots,U\right\}  ;\\k\notin\left\{
j,j+1\right\}  }}x_{i}^{U-k}\underbrace{c_{k,j}}_{\substack{=0\\\text{(by
(\ref{pf.thm.vander-det.B=AUC.pf.1.pf.6}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }k=j\text{ and for }k=j+1\text{
from}\\
\text{the sum, because }j\text{ and }j+1\text{ are two distinct elements of
}\left\{  1,2,\ldots,U\right\}
\end{array}
\right) \\
&  =x_{i}^{U-j}1+x_{i}^{U-j-1}\left(  -x_{U}\right)  +\underbrace{\sum
_{\substack{k\in\left\{  1,2,\ldots,U\right\}  ;\\k\notin\left\{
j,j+1\right\}  }}x_{i}^{U-k}0}_{=0}\\
&  =x_{i}^{U-j}1+x_{i}^{U-j-1}\left(  -x_{U}\right)  =x_{i}^{U-j}%
-x_{i}^{U-j-1}x_{U}=x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}.
\end{align*}
Comparing this with (\ref{pf.thm.vander-det.B=AUC.pf.1.pf.1}), we obtain
$b_{i,j}=\sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}$. Thus,
(\ref{pf.thm.vander-det.B=AUC.pf.1}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $j\geq U$. Thus, $j=U$
(since $j\in\left\{  1,2,\ldots,U\right\}  $). Now, the definition of
$b_{i,j}$ yields%
\begin{equation}
b_{i,j}=%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j+1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
=1 \label{pf.thm.vander-det.B=AUC.pf.1.pf.2}%
\end{equation}
(since $j=U$).
\par
Now, let $k\in\left\{  1,2,\ldots,U-1\right\}  $. Then, $k\leq U-1<U=j$ and
thus $k\neq j$. Also, $k<j<j+1$ and thus $k\neq j+1$. Hence, neither $k=j$ nor
$k=j+1$ (since $k\neq j$ and $k\neq j+1$). But the definition of $c_{k,j}$
yields%
\[
c_{k,j}=%
\begin{cases}
1, & \text{if }k=j;\\
-x_{U}, & \text{if }k=j+1;\\
0, & \text{otherwise}%
\end{cases}
=0\ \ \ \ \ \ \ \ \ \ \left(  \text{since neither }k=j\text{ nor
}k=j+1\right)  .
\]
Let us now forget that we fixed $k$. We thus have shown that
\begin{equation}
c_{k,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots
,U-1\right\}  . \label{pf.thm.vander-det.B=AUC.pf.1.pf.3}%
\end{equation}
Now,
\begin{align*}
&  \sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}=\sum_{k=1}^{U-1}x_{i}^{U-k}%
\underbrace{c_{k,j}}_{\substack{=0\\\text{(by
(\ref{pf.thm.vander-det.B=AUC.pf.1.pf.3}))}}}+\underbrace{x_{i}^{U-U}}%
_{=x_{i}^{0}=1}\underbrace{c_{U,j}}_{\substack{=%
\begin{cases}
1, & \text{if }U=j;\\
-x_{U}, & \text{if }U=j+1;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }c_{U,j}\text{)}}}\\
&  =\underbrace{\sum_{k=1}^{U-1}x_{i}^{U-k}0}_{=0}+1\underbrace{%
\begin{cases}
1, & \text{if }U=j;\\
-x_{U}, & \text{if }U=j+1;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=1\\\text{(since }U=j\text{)}}}=0+1\cdot1=1.
\end{align*}
Compared with (\ref{pf.thm.vander-det.B=AUC.pf.1.pf.2}), this yields
$b_{i,j}=\sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}$. Thus,
(\ref{pf.thm.vander-det.B=AUC.pf.1}) is proven in Case 2.
\par
We have now proven (\ref{pf.thm.vander-det.B=AUC.pf.1}) in each of the two
Cases 1 and 2. Since these two Cases cover all possibilities, this yields that
(\ref{pf.thm.vander-det.B=AUC.pf.1}) always holds. Qed.}. Now,%
\[
B=\left(  \underbrace{b_{i,j}}_{\substack{=\sum_{k=1}^{U}x_{i}^{U-k}%
c_{k,j}\\\text{(by (\ref{pf.thm.vander-det.B=AUC.pf.1}))}}}\right)  _{1\leq
i\leq U,\ 1\leq j\leq U}=\left(  \sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}\right)
_{1\leq i\leq U,\ 1\leq j\leq U}.
\]
Compared with%
\begin{align*}
A_{U}C  &  =\left(  \sum_{k=1}^{U}x_{i}^{U-k}c_{k,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of the product }A_{U}C\text{,}\\
\text{since }A_{U}=\left(  x_{i}^{U-j}\right)  _{1\leq i\leq U,\ 1\leq j\leq
U}\text{ and }C=\left(  c_{i,j}\right)  _{1\leq i\leq U,\ 1\leq j\leq U}%
\end{array}
\right)  ,
\end{align*}
this yields $B=A_{U}C$. Hence,%
\begin{align*}
\det\underbrace{B}_{=A_{U}C}  &  =\det\left(  A_{U}C\right)  =\det\left(
A_{U}\right)  \cdot\underbrace{\det C}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)}, applied to }U\text{, }A_{U}\text{ and }C\\
\text{instead of }n\text{, }A\text{ and }B
\end{array}
\right) \\
&  =\det\left(  A_{U}\right)  .
\end{align*}
Thus, $\det B=\det\left(  A_{U}\right)  $ is proven again.
\end{verlong}

[\textit{Remark:} It is instructive to compare the two proofs of $\det
B=\det\left(  A_{U}\right)  $ given above. They are close kin, although they
might look different at first. In the first proof, we argued that $B$ can be
obtained from $A_{U}$ by subtracting multiples of some columns from others; in
the second, we argued that $B=A_{U}C$ for a specific lower-triangular matrix
$C$. But a look at the matrix $C$ makes it clear that multiplying a $U\times
U$-matrix with $C$ on the right (i.e., transforming a $U\times U$-matrix $X$
into the matrix $XC$) is tantamount to subtracting multiples of some columns
from others, in the way we did it to $A_{U}$ to obtain $B$. So the main
difference between the two proofs is that the first proof used a step-by-step
procedure to obtain $B$ from $A_{U}$, whereas the second proof obtained $B$
from $A_{U}$ by a single-step operation (namely, multiplication by a matrix
$C$).]

Next, we observe that for every $j\in\left\{  1,2,\ldots,U-1\right\}  $, we
have%
\begin{align*}
b_{U,j}  &  =%
\begin{cases}
x_{U}^{U-j}-x_{U}x_{U}^{U-j-1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,j}\right) \\
&  =x_{U}^{U-j}-\underbrace{x_{U}x_{U}^{U-j-1}}_{=x_{U}^{\left(  U-j-1\right)
+1}=x_{U}^{U-j}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<U\text{ (since
}j\in\left\{  1,2,\ldots,U-1\right\}  \text{)}\right) \\
&  =x_{U}^{U-j}-x_{U}^{U-j}=0.
\end{align*}
Hence, Theorem \ref{thm.laplace.pre} (applied to $U$, $B$ and $b_{i,j}$
instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{equation}
\det B=b_{U,U}\cdot\det\left(  \left(  b_{i,j}\right)  _{1\leq i\leq
U-1,\ 1\leq j\leq U-1}\right)  . \label{pf.thm.vander-det.detB=prod}%
\end{equation}
Let $B^{\prime}$ denote the $\left(  U-1\right)  \times\left(  U-1\right)
$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$.

The definition of $b_{U,U}$ yields%
\begin{align*}
b_{U,U}  &  =%
\begin{cases}
x_{U}^{U-U}-x_{U}x_{U}^{U-U-1}, & \text{if }U<U;\\
1, & \text{if }U=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,U}\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }U=U\right)  .
\end{align*}
Thus, (\ref{pf.thm.vander-det.detB=prod}) becomes%
\[
\det B=\underbrace{b_{U,U}}_{=1}\cdot\det\left(  \underbrace{\left(
b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}}_{=B^{\prime}}\right)
=\det\left(  B^{\prime}\right)  .
\]
Compared with $\det B=\det\left(  A_{U}\right)  $, this yields%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  .
\label{pf.thm.vander-det.detAU=detB'}%
\end{equation}


Now, let us take a closer look at $B^{\prime}$. Indeed, every $\left(
i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$ satisfies%
\begin{align}
b_{i,j}  &  =%
\begin{cases}
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{i,j}\right)
\nonumber\\
&  =\underbrace{x_{i}^{U-j}}_{=x_{i}x_{i}^{U-j-1}}-x_{U}x_{i}^{U-j-1}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j<U\text{ (since }j\in\left\{  1,2,\ldots,U-1\right\} \\
\text{(since }\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}
^{2}\text{))}%
\end{array}
\right) \nonumber\\
&  =x_{i}x_{i}^{U-j-1}-x_{U}x_{i}^{U-j-1}=\left(  x_{i}-x_{U}\right)
\underbrace{x_{i}^{U-j-1}}_{=x_{i}^{\left(  U-1\right)  -j}}=\left(
x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}.
\label{pf.thm.vander-det.bij-small}%
\end{align}
Hence,%
\begin{equation}
B^{\prime}=\left(  \underbrace{b_{i,j}}_{\substack{=\left(  x_{i}%
-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}\\\text{(by
(\ref{pf.thm.vander-det.bij-small}))}}}\right)  _{1\leq i\leq U-1,\ 1\leq
j\leq U-1}=\left(  \left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}. \label{pf.thm.vander-det.B'}%
\end{equation}
On the other hand, the definition of $A_{U-1}$ yields
\begin{equation}
A_{U-1}=\left(  x_{i}^{\left(  U-1\right)  -j}\right)  _{1\leq i\leq
U-1,\ 1\leq j\leq U-1}. \label{pf.thm.vander-det.AU-1}%
\end{equation}


Now, we claim that%
\begin{equation}
\det\left(  B^{\prime}\right)  =\det\left(  A_{U-1}\right)  \cdot\prod
_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  . \label{pf.thm.vander-det.detB'=}%
\end{equation}
Indeed, here are two ways to prove this:

\textit{First proof of (\ref{pf.thm.vander-det.detB'=}):} Comparing the
formulas (\ref{pf.thm.vander-det.B'}) and (\ref{pf.thm.vander-det.AU-1}), we
see that the matrix $B^{\prime}$ is obtained from the matrix $A_{U-1}$ by
multiplying the first row by $x_{1}-x_{U}$, the second row by $x_{2}-x_{U}$,
and so on, and finally the $\left(  U-1\right)  $-st row by $x_{U-1}-x_{U}$.
But every time we multiply a row of a $\left(  U-1\right)  \times\left(
U-1\right)  $-matrix by some scalar $\lambda\in\mathbb{K}$, the determinant of
the matrix gets multiplied by $\lambda$ (because of Exercise \ref{exe.ps4.6}
\textbf{(g)}). Hence, the determinant of $B^{\prime}$ is obtained from that of
$A_{U-1}$ by first multiplying by $x_{1}-x_{U}$, then multiplying by
$x_{2}-x_{U}$, and so on, and finally multiplying with $x_{U-1}-x_{U}$. In
other words,%
\[
\det\left(  B^{\prime}\right)  =\det\left(  A_{U-1}\right)  \cdot\prod
_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\]
This proves (\ref{pf.thm.vander-det.detB'=}).

\textit{Second proof of (\ref{pf.thm.vander-det.detB'=}):} For every $\left(
i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$, we define $d_{i,j}%
\in\mathbb{K}$ by%
\[
d_{i,j}=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=j;\\
0, & \text{otherwise}%
\end{cases}
.
\]
Let $D$ be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix
$\left(  d_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$.

For example, if $U=4$, then%
\[
D=\left(
\begin{array}
[c]{ccc}%
x_{1}-x_{4} & 0 & 0\\
0 & x_{2}-x_{4} & 0\\
0 & 0 & x_{3}-x_{4}%
\end{array}
\right)  .
\]


\begin{vershort}
The matrix $D$ is lower-triangular (actually, diagonal\footnote{A square
matrix $E=\left(  e_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ is said to
be \textit{diagonal} if every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$ satisfying $i\neq j$ satisfies $e_{i,j}=0$. In other words,
a square matrix is said to be \textit{diagonal} if it is both upper-triangular
and lower-triangular.}), and thus Exercise \ref{exe.ps4.3} shows that its
determinant is $\det D=\left(  x_{1}-x_{U}\right)  \left(  x_{2}-x_{U}\right)
\cdots\left(  x_{U-1}-x_{U}\right)  =\prod_{i=1}^{U-1}\left(  x_{i}%
-x_{U}\right)  $.
\end{vershort}

\begin{verlong}
We have $d_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U-1\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$ be such that
$i<j$. Then, we don't have $i=j$ (since $i<j$). Now, the definition of
$d_{i,j}$ yields $d_{i,j}=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=j;\\
0, & \text{otherwise}%
\end{cases}
=0$ (since we don't have $i=j$), qed.}. Hence, Exercise \ref{exe.ps4.3}
(applied to $U-1$, $D$ and $d_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{align*}
\det D  &  =d_{1,1}d_{2,2}\cdots d_{U-1,U-1}=\prod_{i=1}^{U-1}%
\underbrace{d_{i,i}}_{\substack{=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=i;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }d_{i,i}\text{)}}}\\
&  =\prod_{i=1}^{U-1}\underbrace{%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=i;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=x_{i}-x_{U}\\\text{(since }i=i\text{)}}}=\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}

\end{verlong}

\begin{vershort}
On the other hand, it is easy to see that $B^{\prime}=DA_{U-1}$ (check this!).
Thus, Theorem \ref{thm.det(AB)} yields
\[
\det\left(  B^{\prime}\right)  =\det D\cdot\det\left(  A_{U-1}\right)
=\det\left(  A_{U-1}\right)  \cdot\underbrace{\det D}_{=\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  }=\det\left(  A_{U-1}\right)  \cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\]
Thus, (\ref{pf.thm.vander-det.detB'=}) is proven again.
\end{vershort}

\begin{verlong}
On the other hand, every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,U-1\right\}  ^{2}$ satisfy%
\begin{equation}
b_{i,j}=\sum_{k=1}^{U-1}d_{i,k}x_{k}^{\left(  U-1\right)  -j}
\label{pf.thm.vander-det.B'=DAU-1.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.vander-det.B'=DAU-1.pf.1}):} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$. We must prove
(\ref{pf.thm.vander-det.B'=DAU-1.pf.1}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,U-1\right\}  $ and $j\in\left\{  1,2,\ldots
,U-1\right\}  $. Now,%
\begin{align*}
&  \underbrace{\sum_{k=1}^{U-1}}_{=\sum_{k\in\left\{  1,2,\ldots,U-1\right\}
}}\underbrace{d_{i,k}}_{\substack{=%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=k;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by the definition of }d_{i,k}\text{)}}}x_{k}^{\left(  U-1\right)
-j}\\
&  =\sum_{k\in\left\{  1,2,\ldots,U-1\right\}  }%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=k;\\
0, & \text{otherwise}%
\end{cases}
x_{k}^{\left(  U-1\right)  -j}\\
&  =\underbrace{%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=i;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=x_{i}-x_{U}\\\text{(since }i=i\text{)}}}x_{i}^{\left(
U-1\right)  -j}+\sum_{\substack{k\in\left\{  1,2,\ldots,U-1\right\}  ;\\k\neq
i}}\underbrace{%
\begin{cases}
x_{i}-x_{U}, & \text{if }i=k;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=0\\\text{(since we don't have }i=k\\\text{(since }i\neq k\text{
(since }k\neq i\text{)))}}}x_{k}^{\left(  U-1\right)  -j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=i\text{ from the sum}\right) \\
&  =\left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}%
+\underbrace{\sum_{\substack{k\in\left\{  1,2,\ldots,U-1\right\}  ;\\k\neq
i}}0x_{k}^{\left(  U-1\right)  -j}}_{=0}=\left(  x_{i}-x_{U}\right)
x_{i}^{\left(  U-1\right)  -j}=b_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.vander-det.bij-small})}\right)  ,
\end{align*}
and therefore (\ref{pf.thm.vander-det.B'=DAU-1.pf.1}) is proven.}. Now,%
\[
B^{\prime}=\left(  \underbrace{b_{i,j}}_{\substack{=\sum_{k=1}^{U-1}%
d_{i,k}x_{k}^{\left(  U-1\right)  -j}\\\text{(by
(\ref{pf.thm.vander-det.B'=DAU-1.pf.1}))}}}\right)  _{1\leq i\leq U-1,\ 1\leq
j\leq U-1}=\left(  \sum_{k=1}^{U-1}d_{i,k}x_{k}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}.
\]
Compared with%
\begin{align*}
DA_{U-1}  &  =\left(  \sum_{k=1}^{U-1}d_{i,k}x_{k}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of the product }DA_{U-1}\text{,}\\
\text{since }D=\left(  d_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}\\
\text{and }A_{U-1}=\left(  x_{i}^{\left(  U-1\right)  -j}\right)  _{1\leq
i\leq U-1,\ 1\leq j\leq U-1}%
\end{array}
\right)  ,
\end{align*}
this yields $B^{\prime}=DA_{U-1}$. Hence,%
\begin{align*}
\det\left(  \underbrace{B^{\prime}}_{=DA_{U-1}}\right)   &  =\det\left(
DA_{U-1}\right)  =\det D\cdot\det\left(  A_{U-1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)}, applied to }U-1\text{, }D\text{ and
}A_{U-1}\\
\text{instead of }n\text{, }A\text{ and }B
\end{array}
\right) \\
&  =\det\left(  A_{U-1}\right)  \cdot\underbrace{\det D}_{=\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  }=\det\left(  A_{U-1}\right)  \cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}
Thus, (\ref{pf.thm.vander-det.detB'=}) is proven again.
\end{verlong}

[\textit{Remark:} Again, our two proofs of (\ref{pf.thm.vander-det.detB'=})
are closely related: the first one reveals $B^{\prime}$ as the result of a
step-by-step process applied to $A_{U-1}$, while the second shows how
$B^{\prime}$ can be obtained from $A_{U-1}$ by a single multiplication.
However, here (in contrast to the proofs of $\det B=\det\left(  A_{U}\right)
$), the step-by-step process involves transforming rows (not columns), and the
multiplication is a multiplication from the left (we have $B^{\prime}%
=DA_{U-1}$, not $B^{\prime}=A_{U-1}D$).]

Now, (\ref{pf.thm.vander-det.detAU=detB'}) becomes%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  =\det\left(
A_{U-1}\right)  \cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\label{pf.thm.vander-det.detAU=}%
\end{equation}


But we have assumed that (\ref{pf.thm.vander-det.a.goal}) holds for $u=U-1$.
In other words,%
\[
\det\left(  A_{U-1}\right)  =\underbrace{\prod_{1\leq i<j\leq U-1}}%
_{=\prod_{j=1}^{U-1}\prod_{i=1}^{j-1}}\left(  x_{i}-x_{j}\right)  =\prod
_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  .
\]
Hence, (\ref{pf.thm.vander-det.detAU=}) yields%
\begin{align*}
\det\left(  A_{U}\right)   &  =\underbrace{\det\left(  A_{U-1}\right)
}_{=\prod_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  }\cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right) \\
&  =\left(  \prod_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)
\right)  \cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}
Compared with%
\begin{align*}
\underbrace{\prod_{1\leq i<j\leq U}}_{=\prod_{j=1}^{U}\prod_{i=1}^{j-1}%
}\left(  x_{i}-x_{j}\right)   &  =\prod_{j=1}^{U}\prod_{i=1}^{j-1}\left(
x_{i}-x_{j}\right)  =\left(  \prod_{j=1}^{U-1}\prod_{i=1}^{j-1}\left(
x_{i}-x_{j}\right)  \right)  \cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the factor for
}j=U\text{ from the product}\right)  ,
\end{align*}
this yields $\det\left(  A_{U}\right)  =\prod_{1\leq i<j\leq U}\left(
x_{i}-x_{j}\right)  $. In other words, (\ref{pf.thm.vander-det.a.goal}) holds
for $u=U$. This completes the induction step.

Now, (\ref{pf.thm.vander-det.a.goal}) is proven by induction. Hence, we can
apply (\ref{pf.thm.vander-det.a.goal}) to $u=n$. As the result, we obtain
$\det\left(  A_{n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
$. Since $A_{n}=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A_{n}$), this rewrites as $\det\left(  \left(
x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(a)}.

\textbf{(b)} The definition of the transpose of a matrix yields $\left(
\left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
^{T}=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\det\left(  \underbrace{\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}\right)  =\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(a)}). Compared with%
\[
\det\left(  \left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)
\]
(by Exercise \ref{exe.ps4.4}, applied to $A=\left(  x_{j}^{n-i}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$), this yields%
\[
\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
This proves Theorem \ref{thm.vander-det} \textbf{(b)}.

\textbf{(d)} Applying Lemma \ref{lem.vander-det.lem-rearr} to $a_{i,j}%
=x_{j}^{n-i}$, we obtain%
\begin{align*}
\det\left(  \left(  x_{j}^{n-\left(  n+1-i\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\left(  -1\right)  ^{n\left(  n-1\right)
/2}\underbrace{\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  }_{\substack{=\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by Theorem \ref{thm.vander-det} \textbf{(b)})}}}\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
This rewrites as%
\begin{equation}
\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  \label{pf.thm.vander-det.d.0}%
\end{equation}
(since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies $x_{j}^{n-\left(  n+1-i\right)  }=x_{j}^{i-1}$).

\begin{vershort}
Now, in the solution to Exercise \ref{exe.ps4.1c}, we have shown that the
number of all pairs $\left(  i,j\right)  $ of integers satisfying $1\leq
i<j\leq n$ is $n\left(  n-1\right)  /2$. In other words,%
\begin{equation}
\left(  \text{the number of all }\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}\text{ such that }i<j\right)  =n\left(  n-1\right)
/2. \label{pf.thm.vander-det.d.short.1}%
\end{equation}
Now,
\begin{align*}
\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)   &  =\prod_{1\leq i<j\leq
n}\underbrace{\left(  x_{j}-x_{i}\right)  }_{=\left(  -1\right)  \left(
x_{i}-x_{j}\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we renamed the index }\left(  j,i\right) \\
\text{as }\left(  i,j\right)  \text{ in the product}%
\end{array}
\right) \\
&  =\prod_{1\leq i<j\leq n}\left(  \left(  -1\right)  \left(  x_{i}%
-x_{j}\right)  \right) \\
&  =\underbrace{\left(  -1\right)  ^{\left(  \text{the number of all }\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}\text{ such that
}i<j\right)  }}_{\substack{=\left(  -1\right)  ^{n\left(  n-1\right)
/2}\\\text{(by (\ref{pf.thm.vander-det.d.short.1}))}}}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right) \\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
Compared with (\ref{pf.thm.vander-det.d.0}), this yields $\det\left(  \left(
x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
j<i\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(d)}.
\end{vershort}

\begin{verlong}
Let $G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq
n\right\}  $. Then, $\left\vert G\right\vert =n\left(  n-1\right)  /2$. (This
was proven in the solution to Exercise \ref{exe.ps4.1c}.) Now,%
\begin{align}
\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)   &  =\underbrace{\prod
_{1\leq i<j\leq n}}_{\substack{=\prod_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\1\leq i<j\leq n}}=\prod_{\left(  i,j\right)  \in
G}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}%
\ \mid\ 1\leq i<j\leq n\right\}  \text{)}}}\underbrace{\left(  x_{j}%
-x_{i}\right)  }_{=\left(  -1\right)  \left(  x_{i}-x_{j}\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
j,i\right)  \text{ as }\left(  i,j\right)  \text{ in the product}\right)
\nonumber\\
&  =\prod_{\left(  i,j\right)  \in G}\left(  \left(  -1\right)  \left(
x_{i}-x_{j}\right)  \right) \nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\left\vert G\right\vert }}%
_{\substack{=\left(  -1\right)  ^{n\left(  n-1\right)  /2}\\\text{(since
}\left\vert G\right\vert =n\left(  n-1\right)  /2\text{)}}}\underbrace{\prod
_{\left(  i,j\right)  \in G}}_{\substack{=\prod_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\1\leq i<j\leq n}}\\\text{(since }G=\left\{  \left(
i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  \text{)}%
}}\left(  x_{i}-x_{j}\right) \nonumber\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\underbrace{\prod
_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\1\leq i<j\leq n}}}%
_{=\prod_{1\leq i<j\leq n}}\left(  x_{i}-x_{j}\right) \nonumber\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  . \label{pf.thm.vander-det.d.2}%
\end{align}
Compared with (\ref{pf.thm.vander-det.d.0}), this yields $\det\left(  \left(
x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq
j<i\leq n}\left(  x_{i}-x_{j}\right)  $. This proves Theorem
\ref{thm.vander-det} \textbf{(d)}.
\end{verlong}

\begin{vershort}
\textbf{(c)} We can derive Theorem \ref{thm.vander-det} \textbf{(c)} from
Theorem \ref{thm.vander-det} \textbf{(d)} in the same way as we derived part
\textbf{(b)} from \textbf{(a)}.
\end{vershort}

\begin{verlong}
\textbf{(c)} The definition of the transpose of a matrix yields $\left(
\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
^{T}=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\det\left(  \underbrace{\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}\right)  =\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}%
-x_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(d)}). Compared with%
\[
\det\left(  \left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)
\]
(by Exercise \ref{exe.ps4.4}, applied to $A=\left(  x_{i}^{j-1}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$), this yields%
\[
\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\]
This proves Theorem \ref{thm.vander-det} \textbf{(c)}.
\end{verlong}
\end{proof}

\subsubsection{\label{subsect.vandermonde.factoring}A proof by factoring the
matrix}

Next, I shall outline another proof of Theorem \ref{thm.vander-det}, which
proceeds by writing the matrix $\left(  x_{j}^{i-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ as a product of a lower-triangular matrix with an
upper-triangular matrix. The idea of this proof appears in \cite[Theorem
2.1]{OruPhi}, \cite[Theorem 2]{GohKol} and \cite[proof of Lemma 4.12]{OlvSha}
(although the first two of these three sources use slightly different
arguments, and the third gives no proof).

We will need several lemmas for the proof. The proofs of these lemmas are
relegated to the solution of Exercise \ref{exe.subsect.vandermonde.factoring}.

We begin with a definition that will be used throughout Subsection
\ref{subsect.vandermonde.factoring}:

\begin{definition}
\label{def.vandermonde.factoring.h}Let $k\in\mathbb{Z}$ and $n\in\mathbb{N}$.
Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Then, we
define an element $h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ by%
\[
h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n};\\a_{1}+a_{2}+\cdots
+a_{n}=k}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}.
\]
(Note that the sum on the right hand side of this equality is finite, because
only finitely many $\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}%
^{n}$ satisfy $a_{1}+a_{2}+\cdots+a_{n}=k$.)
\end{definition}

The element $h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ defined in
Definition \ref{def.vandermonde.factoring.h} is often called the
$k$\textit{-th complete homogeneous function of the }$n$ \textit{elements
}$x_{1},x_{2},\ldots,x_{n}$ (although, more often, this notion is reserved for
a different, more abstract object of whom $h_{k}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  $ is just an evaluation). Let us see some examples:

\begin{example}
\label{exam.vandermonde.factoring.h.12}\textbf{(a)} If $n\in\mathbb{N}$, and
if $x_{1},x_{2},\ldots,x_{n}$ are $n$ elements of $\mathbb{K}$, then%
\[
h_{1}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =x_{1}+x_{2}+\cdots+x_{n}%
\]
and%
\[
h_{2}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\left(  x_{1}^{2}+x_{2}%
^{2}+\cdots+x_{n}^{2}\right)  +\sum_{1\leq i<j\leq n}x_{i}x_{j}.
\]
For example, for $n=3$, we obtain $h_{2}\left(  x_{1},x_{2},x_{3}\right)
=\left(  x_{1}^{2}+x_{2}^{2}+x_{3}^{2}\right)  +\left(  x_{1}x_{2}+x_{1}%
x_{3}+x_{2}x_{3}\right)  $.

\textbf{(b)} If $x$ and $y$ are two elements of $\mathbb{K}$, then%
\[
h_{k}\left(  x,y\right)  =\sum_{\substack{\left(  a_{1},a_{2}\right)
\in\mathbb{N}^{2};\\a_{1}+a_{2}=k}}x^{a_{1}}y^{a_{2}}=x^{k}y^{0}+x^{k-1}%
y^{1}+\cdots+x^{0}y^{k}%
\]
for every $k\in\mathbb{N}$.

\textbf{(c)} If $x\in\mathbb{K}$, then $h_{k}\left(  x\right)  =x^{k}$ for
every $k\in\mathbb{N}$.
\end{example}

\begin{lemma}
\label{lem.vandermonde.factoring.h.0}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$.

\textbf{(a)} We have $h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =0$ for
every negative integer $k$.

\textbf{(b)} We have $h_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =1$.
\end{lemma}

As we have said, all lemmas in Subsection \ref{subsect.vandermonde.factoring}
will be proven in the solution to Exercise
\ref{exe.subsect.vandermonde.factoring}.

Three further lemmas on the $h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $
will be of use:

\begin{lemma}
\label{lem.vandermonde.factoring.h.hq1}Let $k$ be a positive integer. Let
$x_{1},x_{2},\ldots,x_{k}$ be $k$ elements of $\mathbb{K}$. Let $q\in
\mathbb{Z}$. Then,%
\[
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)  =\sum_{r=0}^{q}x_{k}^{r}%
h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  .
\]

\end{lemma}

\begin{lemma}
\label{lem.vandermonde.factoring.h.hq2}Let $k$ be a positive integer. Let
$x_{1},x_{2},\ldots,x_{k}$ be $k$ elements of $\mathbb{K}$. Let $q\in
\mathbb{Z}$. Then,%
\[
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)  =h_{q}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  +x_{k}h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
.
\]

\end{lemma}

\begin{lemma}
\label{lem.vandermonde.factoring.h.sum-u}Let $i$ be a positive integer. Let
$x_{1},x_{2},\ldots,x_{i}$ be $i$ elements of $\mathbb{K}$. Let $u\in
\mathbb{K}$. Then,%
\[
\sum_{k=1}^{i}h_{i-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right)  =u^{i-1}.
\]

\end{lemma}

Next, let us introduce two matrices:

\begin{lemma}
\label{lem.vandermonde.factoring.U}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Define an $n\times
n$-matrix $U\in\mathbb{K}^{n\times n}$ by%
\[
U=\left(  \prod_{p=1}^{i-1}\left(  x_{j}-x_{p}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}.
\]
Then, $\det U=\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  $.
\end{lemma}

\begin{example}
If $n=4$, then the matrix $U$ defined in Lemma
\ref{lem.vandermonde.factoring.U} looks as follows:%
\begin{align*}
U  &  =\left(  \prod_{p=1}^{i-1}\left(  x_{j}-x_{p}\right)  \right)  _{1\leq
i\leq4,\ 1\leq j\leq4}\\
&  =\left(
\begin{array}
[c]{cccc}%
\prod_{p=1}^{0}\left(  x_{1}-x_{p}\right)  & \prod_{p=1}^{0}\left(
x_{2}-x_{p}\right)  & \prod_{p=1}^{0}\left(  x_{3}-x_{p}\right)  & \prod
_{p=1}^{0}\left(  x_{4}-x_{p}\right) \\
\prod_{p=1}^{1}\left(  x_{1}-x_{p}\right)  & \prod_{p=1}^{1}\left(
x_{2}-x_{p}\right)  & \prod_{p=1}^{1}\left(  x_{3}-x_{p}\right)  & \prod
_{p=1}^{1}\left(  x_{4}-x_{p}\right) \\
\prod_{p=1}^{2}\left(  x_{1}-x_{p}\right)  & \prod_{p=1}^{2}\left(
x_{2}-x_{p}\right)  & \prod_{p=1}^{2}\left(  x_{3}-x_{p}\right)  & \prod
_{p=1}^{2}\left(  x_{4}-x_{p}\right) \\
\prod_{p=1}^{3}\left(  x_{1}-x_{p}\right)  & \prod_{p=1}^{3}\left(
x_{2}-x_{p}\right)  & \prod_{p=1}^{3}\left(  x_{3}-x_{p}\right)  & \prod
_{p=1}^{3}\left(  x_{4}-x_{p}\right)
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
1 & 1 & 1 & 1\\
0 & x_{2}-x_{1} & x_{3}-x_{1} & x_{4}-x_{1}\\
0 & 0 & \left(  x_{3}-x_{1}\right)  \left(  x_{3}-x_{2}\right)  & \left(
x_{4}-x_{1}\right)  \left(  x_{4}-x_{2}\right) \\
0 & 0 & 0 & \left(  x_{4}-x_{1}\right)  \left(  x_{4}-x_{2}\right)  \left(
x_{4}-x_{3}\right)
\end{array}
\right)  .
\end{align*}
(Here, we have used the fact that if $i>j$, then the product $\prod
_{p=1}^{i-1}\left(  x_{j}-x_{p}\right)  $ contains the factor $x_{j}-x_{j}=0$
and thus equals $0$.)
\end{example}

\begin{lemma}
\label{lem.vandermonde.factoring.L}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Define an $n\times
n$-matrix $L\in\mathbb{K}^{n\times n}$ by%
\[
L=\left(  h_{i-j}\left(  x_{1},x_{2},\ldots,x_{j}\right)  \right)  _{1\leq
i\leq n,\ 1\leq j\leq n}.
\]
Then, $\det L=1$.
\end{lemma}

\begin{example}
If $n=4$, then the matrix $L$ defined in Lemma
\ref{lem.vandermonde.factoring.L} looks as follows:%
\begin{align*}
L  &  =\left(  h_{i-j}\left(  x_{1},x_{2},\ldots,x_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}\\
&  =\left(
\begin{array}
[c]{cccc}%
h_{0}\left(  x_{1}\right)  & h_{-1}\left(  x_{1},x_{2}\right)  & h_{-2}\left(
x_{1},x_{2},x_{3}\right)  & h_{-3}\left(  x_{1},x_{2},x_{3},x_{4}\right) \\
h_{1}\left(  x_{1}\right)  & h_{0}\left(  x_{1},x_{2}\right)  & h_{-1}\left(
x_{1},x_{2},x_{3}\right)  & h_{-2}\left(  x_{1},x_{2},x_{3},x_{4}\right) \\
h_{2}\left(  x_{2}\right)  & h_{1}\left(  x_{1},x_{2}\right)  & h_{0}\left(
x_{1},x_{2},x_{3}\right)  & h_{-1}\left(  x_{1},x_{2},x_{3},x_{4}\right) \\
h_{3}\left(  x_{3}\right)  & h_{2}\left(  x_{1},x_{2}\right)  & h_{1}\left(
x_{1},x_{2},x_{3}\right)  & h_{0}\left(  x_{1},x_{2},x_{3},x_{4}\right)
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cccc}%
1 & 0 & 0 & 0\\
x_{1} & 1 & 0 & 0\\
x_{1}^{2} & x_{1}+x_{2} & 1 & 0\\
x_{1}^{3} & x_{1}^{2}+x_{1}x_{2}+x_{2}^{2} & x_{1}+x_{2}+x_{3} & 1
\end{array}
\right)  .
\end{align*}
(The fact that the diagonal entries are $1$ is a consequence of Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(b)}, and the fact that the
entries above the diagonal are $0$ is a consequence of Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(a)}.)
\end{example}

\begin{lemma}
\label{lem.vandermonde.factoring.A=LU}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $L$ be the $n\times
n$-matrix defined in Lemma \ref{lem.vandermonde.factoring.L}. Let $U$ be the
$n\times n$-matrix defined in Lemma \ref{lem.vandermonde.factoring.U}. Then,%
\[
\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=LU.
\]

\end{lemma}

\begin{exercise}
\label{exe.subsect.vandermonde.factoring}Prove Lemma
\ref{lem.vandermonde.factoring.h.0}, Lemma
\ref{lem.vandermonde.factoring.h.hq1}, Lemma
\ref{lem.vandermonde.factoring.h.hq2}, Lemma
\ref{lem.vandermonde.factoring.h.sum-u}, Lemma
\ref{lem.vandermonde.factoring.U}, Lemma \ref{lem.vandermonde.factoring.L} and
Lemma \ref{lem.vandermonde.factoring.A=LU}.
\end{exercise}

Now, we can prove Theorem \ref{thm.vander-det} again:

\begin{proof}
[Second proof of Theorem \ref{thm.vander-det}.]\textbf{(d)} Let $L$ be the
$n\times n$-matrix defined in Lemma \ref{lem.vandermonde.factoring.L}. Let $U$
be the $n\times n$-matrix defined in Lemma \ref{lem.vandermonde.factoring.U}.
Then, Lemma \ref{lem.vandermonde.factoring.A=LU} yields $\left(  x_{j}%
^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=LU$. Hence,%
\begin{align*}
\det\left(  \underbrace{\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}}_{=LU}\right)   &  =\det\left(  LU\right)  =\underbrace{\det
L}_{\substack{=1\\\text{(by Lemma \ref{lem.vandermonde.factoring.L})}}%
}\cdot\underbrace{\det U}_{\substack{=\prod_{1\leq j<i\leq n}\left(
x_{i}-x_{j}\right)  \\\text{(by Lemma \ref{lem.vandermonde.factoring.U})}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)}, applied to }L\text{ and }U\\
\text{instead of }A\text{ and }B
\end{array}
\right) \\
&  =1\cdot\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  =\prod_{1\leq
j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
This proves Theorem \ref{thm.vander-det} \textbf{(d)}.

\begin{vershort}
Now, it remains to prove parts \textbf{(a)}, \textbf{(b)} and \textbf{(c)} of
Theorem \ref{thm.vander-det}. This is fairly easy: Back in our First proof of
Theorem \ref{thm.vander-det}, we have derived parts \textbf{(b)}, \textbf{(d)}
and \textbf{(c)} from part \textbf{(a)}. By essentially the same arguments
(sometimes done in reverse), we can derive parts \textbf{(a)}, \textbf{(b)}
and \textbf{(c)} from part \textbf{(d)}. (We need to use the fact that
$\left(  \left(  -1\right)  ^{n\left(  n-1\right)  /2}\right)  ^{2}=1$.)
\end{vershort}

\begin{verlong}
\textbf{(b)} Applying Lemma \ref{lem.vander-det.lem-rearr} to $a_{i,j}%
=x_{j}^{i-1}$, we obtain%
\begin{align*}
\det\left(  \left(  x_{j}^{\left(  n+1-i\right)  -1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\left(  -1\right)  ^{n\left(  n-1\right)
/2}\underbrace{\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  }_{\substack{=\prod_{1\leq j<i\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by Theorem \ref{thm.vander-det} \textbf{(d)})}}}\\
&  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq j<i\leq
n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
This rewrites as%
\begin{equation}
\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\prod_{1\leq j<i\leq
n}\left(  x_{i}-x_{j}\right)  \label{pf.thm.vander-det.pf2.b.0}%
\end{equation}
(since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies $x_{j}^{\left(  n+1-i\right)  -1}=x_{j}^{n-i}$).

But we have%
\begin{equation}
\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  =\left(  -1\right)
^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\label{pf.thm.vander-det.pf2.b.1}%
\end{equation}
(Indeed, this is the equality (\ref{pf.thm.vander-det.d.2}) from the First
proof of Theorem \ref{thm.vander-det} above.) Also, the number $n\left(
n-1\right)  /2$ is an integer. Hence, $n\left(  n-1\right)  $ is even. Thus,
$\left(  -1\right)  ^{n\left(  n-1\right)  }=1$. Now,%
\begin{align*}
\left(  \left(  -1\right)  ^{n\left(  n-1\right)  /2}\right)  ^{2}  &
=\left(  -1\right)  ^{\left(  n\left(  n-1\right)  /2\right)  \cdot2}=\left(
-1\right)  ^{n\left(  n-1\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  n\left(  n-1\right)  /2\right)  \cdot2=n\left(  n-1\right)  \right)
\\
&  =1.
\end{align*}
Now, (\ref{pf.thm.vander-det.pf2.b.0}) becomes%
\begin{align*}
\det\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)   &  =\left(  -1\right)  ^{n\left(  n-1\right)  /2}\underbrace{\prod
_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  }_{\substack{=\left(  -1\right)
^{n\left(  n-1\right)  /2}\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\\\text{(by (\ref{pf.thm.vander-det.pf2.b.1}))}}}\\
&  =\underbrace{\left(  -1\right)  ^{n\left(  n-1\right)  /2}\left(
-1\right)  ^{n\left(  n-1\right)  /2}}_{=\left(  \left(  -1\right)  ^{n\left(
n-1\right)  /2}\right)  ^{2}=1}\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right) \\
&  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
This proves Theorem \ref{thm.vander-det} \textbf{(b)}.

\textbf{(a)} The definition of the transpose of a matrix yields $\left(
\left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
^{T}=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\det\left(  \underbrace{\left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}\right)  =\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  .
\]
Hence,%
\begin{align*}
\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)   &  =\det\left(  \left(  \left(  x_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}\right)  =\det\left(  \left(  x_{j}%
^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.ps4.4}, applied to
}A=\left(  x_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\end{align*}
This proves Theorem \ref{thm.vander-det} \textbf{(a)}.

\textbf{(c)} The definition of the transpose of a matrix yields $\left(
\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
^{T}=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\det\left(  \underbrace{\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}\right)  =\det\left(  \left(  x_{j}^{i-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}%
-x_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(d)}). Compared with%
\[
\det\left(  \left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)
\]
(by Exercise \ref{exe.ps4.4}, applied to $A=\left(  x_{i}^{j-1}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$), this yields%
\[
\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}-x_{j}\right)  .
\]
This proves Theorem \ref{thm.vander-det} \textbf{(c)}.
\end{verlong}
\end{proof}

\subsubsection{Remarks and variations}

\begin{remark}
\label{rmk.vander-det.sign}One consequence of Theorem \ref{thm.vander-det} is
a new solution to Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)}:

Namely, let $n\in\mathbb{N}$ and $\sigma\in S_{n}$. Let $x_{1},x_{2}%
,\ldots,x_{n}$ be $n$ elements of $\mathbb{C}$ (or of any commutative ring).
Then, Theorem \ref{thm.vander-det} \textbf{(a)} yields
\[
\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
On the other hand, Theorem \ref{thm.vander-det} \textbf{(a)} (applied to
$x_{\sigma\left(  1\right)  },x_{\sigma\left(  2\right)  },\ldots
,x_{\sigma\left(  n\right)  }$ instead of $x_{1},x_{2},\ldots,x_{n}$) yields%
\begin{equation}
\det\left(  \left(  x_{\sigma\left(  i\right)  }^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{\sigma\left(
i\right)  }-x_{\sigma\left(  j\right)  }\right)  .
\label{exe.rmk.vander-det.sign.2}%
\end{equation}
But Lemma \ref{lem.det.sigma} \textbf{(a)} (applied to $B=\left(  x_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, $\kappa=\sigma$ and
$B_{\kappa}=\left(  x_{\sigma\left(  i\right)  }^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$) yields%
\begin{align*}
\det\left(  \left(  x_{\sigma\left(  i\right)  }^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\left(  -1\right)  ^{\sigma}\cdot
\underbrace{\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  }_{=\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  }\\
&  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  .
\end{align*}
Compared with (\ref{exe.rmk.vander-det.sign.2}), this yields%
\[
\prod_{1\leq i<j\leq n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(
j\right)  }\right)  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}\left(  x_{i}-x_{j}\right)  .
\]
Thus, Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} is solved.
However, Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)} cannot be
solved this way.
\end{remark}

\begin{exercise}
\label{exe.vander-det.s1}Let $n$ be a positive integer. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Prove that%
\[
\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(  x_{1}+x_{2}%
+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
(For example, when $n=4$, this states that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
x_{1}^{4} & x_{1}^{2} & x_{1} & 1\\
x_{2}^{4} & x_{2}^{2} & x_{2} & 1\\
x_{3}^{4} & x_{3}^{2} & x_{3} & 1\\
x_{4}^{4} & x_{4}^{2} & x_{4} & 1
\end{array}
\right) \\
&  =\left(  x_{1}+x_{2}+x_{3}+x_{4}\right)  \left(  x_{1}-x_{2}\right)
\left(  x_{1}-x_{3}\right)  \left(  x_{1}-x_{4}\right)  \left(  x_{2}%
-x_{3}\right)  \left(  x_{2}-x_{4}\right)  \left(  x_{3}-x_{4}\right)  .
\end{align*}
)
\end{exercise}

\begin{remark}
\label{rmk.vander-det.schur}We can try to generalize Vandermonde's
determinant. Namely, let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$
nonnegative integers. Let $A$ be the $n\times n$-matrix
\[
\left(  x_{i}^{a_{j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(
\begin{array}
[c]{cccc}%
x_{1}^{a_{1}} & x_{1}^{a_{2}} & \cdots & x_{1}^{a_{n}}\\
x_{2}^{a_{1}} & x_{2}^{a_{2}} & \cdots & x_{2}^{a_{n}}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n}^{a_{1}} & x_{n}^{a_{2}} & \cdots & x_{n}^{a_{n}}%
\end{array}
\right)  .
\]
What can we say about $\det A$ ?

Theorem \ref{thm.vander-det} says that if $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  =\left(  n-1,n-2,\ldots,0\right)  $, then $\det A=\prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  $.

Exercise \ref{exe.vander-det.s1} says that if $n>0$ and $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  =\left(  n,n-2,n-3,\ldots,0\right)  $, then $\det
A=\left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  $.

This suggests a general pattern: We would suspect that for every $\left(
a_{1},a_{2},\ldots,a_{n}\right)  $, there is a polynomial $P_{\left(
a_{1},a_{2},\ldots,a_{n}\right)  }$ in $n$ indeterminates $X_{1},X_{2}%
,\ldots,X_{n}$ such that%
\[
\det A=P_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  }\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  \cdot\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
.
\]


It turns out that this is true. Moreover, this polynomial $P_{\left(
a_{1},a_{2},\ldots,a_{n}\right)  }$ is:

\begin{itemize}
\item zero if two of $a_{1},a_{2},\ldots,a_{n}$ are equal;

\item homogeneous of degree $a_{1}+a_{2}+\cdots+a_{n}-\dbinom{n}{2}$;

\item symmetric in $X_{1},X_{2},\ldots,X_{n}$.
\end{itemize}

For example,%
\begin{align*}
P_{\left(  n-1,n-2,\ldots,0\right)  }  &  =1;\\
P_{\left(  n,n-2,n-3,\ldots,0\right)  }  &  =\sum_{i=1}^{n}X_{i}=X_{1}%
+X_{2}+\cdots+X_{n};\\
P_{\left(  n,n-1,\ldots,n-k+1,n-k-1,n-k-2,\ldots,0\right)  }  &  =\sum_{1\leq
i_{1}<i_{2}<\cdots<i_{k}\leq n}X_{i_{1}}X_{i_{2}}\cdots X_{i_{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  0,1,\ldots,n\right\}  ;\\
P_{\left(  n+1,n-2,n-3,\ldots,0\right)  }  &  =\sum_{1\leq i\leq j\leq n}%
X_{i}X_{j};\\
P_{\left(  n+1,n-1,n-3,n-4,\ldots,0\right)  }  &  =\sum_{1\leq i<j\leq
n}\left(  X_{i}^{2}X_{j}+X_{i}X_{j}^{2}\right)  +2\sum_{1\leq i<j<k\leq
n}X_{i}X_{j}X_{k}.
\end{align*}


But this polynomial $P_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  }$ can
actually be described rather explicitly for general $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $; it is a so-called \textit{Schur polynomial} (at
least when $a_{1}>a_{2}>\cdots>a_{n}$; otherwise it is either zero or $\pm$ a
Schur polynomial). See \cite[The Bi-Alternant Formula]{Stembridge},
\cite[Theorem 7.15.1]{Stanley-EC1} or \cite{Leeuwen-aS} for the details.
(Notice that \cite{Leeuwen-aS} uses the notation $\varepsilon\left(
\sigma\right)  $ for the sign of a permutation $\sigma$.) The theory of Schur
polynomials shows, in particular, that all coefficients of the polynomial
$P_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  }$ have equal sign (which is
positive if $a_{1}>a_{2}>\cdots>a_{n}$).
\end{remark}

\begin{remark}
\label{rmk.vander-det.secret-ints}There are plenty other variations on the
Vandermonde determinant. For instance, one can try replacing the powers
$x_{i}^{j-1}$ by binomial coefficients $\dbinom{x_{i}}{j-1}$ in Theorem
\ref{thm.vander-det} \textbf{(c)}, at least when these binomial coefficients
are well-defined (e.g., when the $x_{1},x_{2},\ldots,x_{n}$ are complex
numbers). The result is rather nice: If $x_{1},x_{2},\ldots,x_{n}$ are any $n$
complex numbers, then%
\[
\prod_{1\leq i<j\leq n}\dfrac{x_{i}-x_{j}}{i-j}=\det\left(  \left(
\dbinom{x_{i}}{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\]
(This is proven, e.g., in \cite[Corollary 11]{GriHyp} and \cite[\S 9, Example
5]{AndDos}.) This has the surprising consequence that, whenever $x_{1}%
,x_{2},\ldots,x_{n}$ are $n$ integers, the product $\prod_{1\leq i<j\leq
n}\dfrac{x_{i}-x_{j}}{i-j}$ is itself an integer (because it is the
determinant of a matrix whose entries are integers). This is a nontrivial
result! (A more elementary proof appears in \cite[\S 3, Example 8]{AndDos}.)

Another \textquotedblleft secret integer\textquotedblright\ (i.e., rational
number which turns out to be an integer for non-obvious reasons) is%
\begin{equation}
\dfrac{H\left(  a\right)  H\left(  b\right)  H\left(  c\right)  H\left(
a+b+c\right)  }{H\left(  b+c\right)  H\left(  c+a\right)  H\left(  a+b\right)
}, \label{eq.rmk.vander-det.secret-ints.H}%
\end{equation}
where $a,b,c$ are three nonnegative integers, and where $H\left(  n\right)  $
(for $n\in\mathbb{N}$) denotes the \textit{hyperfactorial} of $n$, defined by%
\[
H\left(  n\right)  =\prod_{k=0}^{n-1}k!=0!\cdot1!\cdot\cdots\cdot\left(
n-1\right)  !.
\]
I am aware of two proofs of the fact that
(\ref{eq.rmk.vander-det.secret-ints.H}) gives an integer for every
$a,b,c\in\mathbb{N}$: One proof is combinatorial, and argues that
(\ref{eq.rmk.vander-det.secret-ints.H}) is the number of \textit{plane
partitions inside an }$a\times b\times c$\textit{-box} (see \cite[last
equality in \S 7.21]{Stanley-EC2} for a proof), or, equivalently, the number
of \textit{rhombus tilings of a hexagon with sidelengths }$a,b,c,a,b,c$ (see
\cite{Eisenk-planepartits} for a precise statement). Another proof (see
\cite[Theorem 0]{GriHyp}) exhibits (\ref{eq.rmk.vander-det.secret-ints.H}) as
the determinant of a matrix, again using the Vandermonde determinant!

(None of the references to \cite{GriHyp} makes any claim of precedence;
actually, I am rather sure of the opposite, i.e., that none of my proofs in
\cite{GriHyp} are new.)
\end{remark}

For some more exercises related to Vandermonde determinants, see \cite[Chapter
1, problems 1.12--1.22]{Prasolov}. Here comes one of them:

\begin{exercise}
\label{exe.vander-det.xi+yj}Let $n$ be a positive integer. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $y_{1},y_{2}%
,\ldots,y_{n}$ be $n$ elements of $\mathbb{K}$.

\textbf{(a)} For every $m\in\left\{  0,1,\ldots,n-2\right\}  $, prove that%
\[
\det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0.
\]


\textbf{(b)} Prove that%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}


[\textbf{Hint:} Use the binomial theorem.]

\textbf{(c)} Let $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  \in\mathbb{K}%
^{n}$ be an $n$-tuple of elements of $\mathbb{K}$. Let $P\left(  X\right)
\in\mathbb{K}\left[  X\right]  $ be the polynomial $\sum_{k=0}^{n-1}p_{k}%
X^{k}$. Prove that%
\begin{align*}
&  \det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}

\end{exercise}

Notice how Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)} generalizes
Example \ref{exam.xi+yj} (for $n\geq3$).

\subsection{\label{sect.invertible}Invertible elements in commutative rings,
and fields}

We shall now interrupt our study of determinants for a moment. Let us define
the notion of inverses in $\mathbb{K}$. (Recall that $\mathbb{K}$ is a
commutative ring.)

\begin{definition}
\label{def.rings.inverse}Let $a\in\mathbb{K}$. Then, an element $b\in
\mathbb{K}$ is said to be an \textit{inverse} of $a$ if it satisfies $ab=1$
and $ba=1$.
\end{definition}

Of course, the two conditions $ab=1$ and $ba=1$ in Definition
\ref{def.rings.inverse} are equivalent, since $ab=ba$ for every $a\in
\mathbb{K}$ and $b\in\mathbb{K}$. Nevertheless, we have given both conditions,
because this way the similarity between the inverse of an element of
$\mathbb{K}$ and the inverse of a map becomes particularly clear.

For example, the element $1$ of $\mathbb{Z}$ is its own inverse (since
$1\cdot1=1$), and the element $-1$ of $\mathbb{Z}$ is its own inverse as well
(since $\left(  -1\right)  \cdot\left(  -1\right)  =1$). These elements $1$
and $-1$ are the only elements of $\mathbb{Z}$ which have an inverse in
$\mathbb{Z}$. However, in the larger commutative ring $\mathbb{Q}$, every
nonzero element $a$ has an inverse (namely, $\dfrac{1}{a}$).

\begin{proposition}
\label{prop.rings.inverse-uni}Let $a\in\mathbb{K}$. Then, there exists at most
one inverse of $a$ in $\mathbb{K}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.rings.inverse-uni}.]Let $b$ and $b^{\prime}$
be any two inverses of $a$ in $\mathbb{K}$. Since $b$ is an inverse of $a$ in
$\mathbb{K}$, we have $ab=1$ and $ba=1$ (by the definition of an
\textquotedblleft inverse of $a$\textquotedblright). Since $b^{\prime}$ is an
inverse of $a$ in $\mathbb{K}$, we have $ab^{\prime}=1$ and $b^{\prime}a=1$
(by the definition of an \textquotedblleft inverse of $a$\textquotedblright).
Now, comparing $b\underbrace{ab^{\prime}}_{=1}=b$ with $\underbrace{ba}%
_{=1}b^{\prime}=b^{\prime}$, we obtain $b=b^{\prime}$.

Let us now forget that we fixed $b$ and $b^{\prime}$. We thus have shown that
if $b$ and $b^{\prime}$ are two inverses of $a$ in $\mathbb{K}$, then
$b=b^{\prime}$. In other words, any two inverses of $a$ in $\mathbb{K}$ are
equal. In other words, there exists at most one inverse of $a$ in $\mathbb{K}%
$. This proves Proposition \ref{prop.rings.inverse-uni}.
\end{proof}

\begin{definition}
\label{def.rings.invertible}\textbf{(a)} An element $a\in\mathbb{K}$ is said
to be \textit{invertible} (or, more precisely, \textit{invertible in
}$\mathbb{K}$) if and only if there exists an inverse of $a$ in $\mathbb{K}$.
In this case, this inverse of $a$ is unique (by Proposition
\ref{prop.rings.inverse-uni}), and thus will be called \textit{the inverse of
}$a$ and denoted by $a^{-1}$.

\textbf{(b)} It is clear that the unity $1$ of $\mathbb{K}$ is invertible
(having inverse $1$). Also, the product of any two invertible elements $a$ and
$b$ of $\mathbb{K}$ is again invertible (having inverse $\left(  ab\right)
^{-1}=a^{-1}b^{-1}$).

\textbf{(c)} If $a$ and $b$ are two elements of $\mathbb{K}$ such that $a$ is
invertible (in $\mathbb{K}$), then we write $\dfrac{b}{a}$ (or $b/a$) for the
product $ba^{-1}$. These fractions behave just as fractions of integers
behave: For example, if $a,b,c,d$ are four elements of $\mathbb{K}$ such that
$a$ and $c$ are invertible, then $\dfrac{b}{a}+\dfrac{d}{c}=\dfrac{bc+da}{ac}$
and $\dfrac{b}{a}\cdot\dfrac{d}{c}=\dfrac{bd}{ac}$ (and the product $ac$ is
indeed invertible, so that the fractions $\dfrac{bc+da}{ac}$ and $\dfrac
{bd}{ac}$ actually make sense).
\end{definition}

Of course, the meaning of the word \textquotedblleft
invertible\textquotedblright\ depends on the ring $\mathbb{K}$. For example,
the integer $2$ is invertible in $\mathbb{Q}$ (because $\dfrac{1}{2}$ is an
inverse of $2$ in $\mathbb{Q}$), but not invertible in $\mathbb{Z}$ (since it
has no inverse in $\mathbb{Z}$). Thus, it is important to say
\textquotedblleft invertible in $\mathbb{K}$\textquotedblright\ unless the
context makes it clear what $\mathbb{K}$ is.

One can usually work with invertible elements in commutative rings in the same
way as one works with nonzero rational numbers. For example, if $a$ is an
invertible element of $\mathbb{K}$, then we can define $a^{n}$ not only for
all $n\in\mathbb{N}$, but also for all $n\in\mathbb{Z}$ (by setting
$a^{n}=\left(  a^{-1}\right)  ^{-n}$ for all negative integers $n$). Of
course, when $n=-1$, this is consistent with our notation $a^{-1}$ for the
inverse of $a$.

Next, we define the notion of a \textit{field}\footnote{We are going to use
the following simple fact: A commutative ring $\mathbb{K}$ is a trivial ring
if and only if $0_{\mathbb{K}}=1_{\mathbb{K}}$.
\par
\textit{Proof.} Assume that $\mathbb{K}$ is a trivial ring. Thus, $\mathbb{K}$
has only one element. Hence, both $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$ have
to equal this one element. Therefore, $0_{\mathbb{K}}=1_{\mathbb{K}}$.
\par
Now, forget that we assumed that $\mathbb{K}$ is a trivial ring. We thus have
proven that
\begin{equation}
\text{if }\mathbb{K}\text{ is a trivial ring, then }0_{\mathbb{K}%
}=1_{\mathbb{K}}\text{.} \label{eq.fn.field.trivring.1}%
\end{equation}
\par
Conversely, assume that $0_{\mathbb{K}}=1_{\mathbb{K}}$. Then, every
$a\in\mathbb{K}$ satisfies $a=a\cdot\underbrace{1_{\mathbb{K}}}%
_{=0_{\mathbb{K}}}=a\cdot0_{\mathbb{K}}=0_{\mathbb{K}}\in\left\{
0_{\mathbb{K}}\right\}  $. In other words, $\mathbb{K}\subseteq\left\{
0_{\mathbb{K}}\right\}  $. Combining this with $\left\{  0_{\mathbb{K}%
}\right\}  \subseteq\mathbb{K}$, we obtain $\mathbb{K}=\left\{  0_{\mathbb{K}%
}\right\}  $. Hence, $\mathbb{K}$ has only one element. In other words,
$\mathbb{K}$ is a trivial ring.
\par
Now, forget that we assumed that $0_{\mathbb{K}}=1_{\mathbb{K}}$. We thus have
proven that
\[
\text{if }0_{\mathbb{K}}=1_{\mathbb{K}}\text{, then }\mathbb{K}\text{ is a
trivial ring.}%
\]
Combining this with (\ref{eq.fn.field.trivring.1}), we conclude that
$\mathbb{K}$ is a trivial ring if and only if $0_{\mathbb{K}}=1_{\mathbb{K}}%
$.}.

\begin{definition}
A commutative ring $\mathbb{K}$ is said to be a \textit{field} if it satisfies
the following two properties:

\begin{itemize}
\item We have $0_{\mathbb{K}}\neq1_{\mathbb{K}}$ (that is, $\mathbb{K}$ is not
a trivial ring).

\item Every element of $\mathbb{K}$ is either zero or invertible.
\end{itemize}
\end{definition}

For example, $\mathbb{Q}$, $\mathbb{R}$ and $\mathbb{C}$ are fields, whereas
polynomial rings such as $\mathbb{Q}\left[  x\right]  $ or $\mathbb{R}\left[
a,b\right]  $ are not fields\footnote{For example, the polynomial $x$ is not
invertible in $\mathbb{Q}\left[  x\right]  $.}. For $n$ being a positive
integer, the ring $\mathbb{Z}/n\mathbb{Z}$ (that is, the ring of residue
classes of integers modulo $n$) is a field if and only if $n$ is a prime number.

Linear algebra (i.e., the study of matrices and linear transformations)
becomes much easier (in many aspects) when $\mathbb{K}$ is a
field\footnote{Many properties of a matrix over a field (such as its rank) are
not even well-defined over an arbitrary commutative ring.}. This is one of the
main reasons why most courses on linear algebra work over fields only (or
begin by working over fields and only later move to the generality of
commutative rings). In these notes we are almost completely limiting ourselves
to the parts of matrix theory which work over any commutative ring.
Nevertheless, let us comment on how determinants can be computed fast when
$\mathbb{K}$ is a field.

\Needspace{30\baselineskip}

\begin{remark}
\label{rmk.laplace.pre.gauss-alg}Assume that $\mathbb{K}$ is a field. If $A$
is an $n\times n$-matrix over $\mathbb{K}$, then the determinant of $A$ can be
computed using (\ref{eq.det.eq.1})... but in practice, you probably do not
\textbf{want} to compute it this way, since the right hand side of
(\ref{eq.det.eq.1}) contains a sum of $n!$ terms.

It turns out that there is an algorithm to compute $\det A$, which is
(usually) a lot faster. It is a version of the Gaussian elimination algorithm
commonly used for solving systems of linear equations.

Let us illustrate it on an example: Set%
\[
n=4,\ \ \ \ \ \ \ \ \ \ \mathbb{K}=\mathbb{Q}\ \ \ \ \ \ \ \ \ \ \text{and
}A=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
0 & -1 & 0 & 2\\
2 & 4 & -2 & 3\\
5 & 1 & 3 & 5
\end{array}
\right)  .
\]
We want to find $\det A$.

Exercise \ref{exe.ps4.6k} \textbf{(b)} shows that if we add a scalar multiple
of a column of a matrix to another column of this matrix, then the determinant
of the matrix does not change. Now, by adding appropriate scalar multiples of
the fourth column of $A$ to the first three columns of $A$, we can make sure
that the first three entries of the fourth row of $A$ become zero: Namely, we
have to

\begin{itemize}
\item add $\left(  -1\right)  $ times the fourth column of $A$ to the first
column of $A$;

\item add $\left(  -1/5\right)  $ times the fourth column of $A$ to the second
column of $A$;

\item add $\left(  -3/5\right)  $ times the fourth column of $A$ to the third
column of $A$.
\end{itemize}

These additions can be performed in any order, since none of them
\textquotedblleft interacts\textquotedblright\ with any other (more precisely,
none of them uses any entries that another of them changes). As we know, none
of these additions changes the determinant of the matrix.

Having performed these three additions, we end up with the matrix%
\begin{equation}
A^{\prime}=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
-2 & -7/5 & -6/5 & 2\\
-1 & 17/5 & -19/5 & 3\\
0 & 0 & 0 & 5
\end{array}
\right)  . \label{eq.rmk.laplace.pre.gauss-alg.3}%
\end{equation}
We have $\det\left(  A^{\prime}\right)  =\det A$ (because $A^{\prime}$ was
obtained from $A$ by three operations which do not change the determinant).
Moreover, the fourth row of $A^{\prime}$ contains only one nonzero entry --
namely, its last entry. In other words, if we write $A^{\prime}$ in the form
$A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq4,\ 1\leq j\leq4}$,
then $a_{4,j}^{\prime}=0$ for every $j\in\left\{  1,2,3\right\}  $. Thus,
Theorem \ref{thm.laplace.pre} (applied to $4$, $A^{\prime}$ and $a_{i,j}%
^{\prime}$ instead of $n$, $A$ and $a_{i,j}$) shows that%
\begin{align*}
\det\left(  A^{\prime}\right)   &  =\underbrace{a_{4,4}^{\prime}}_{=5}%
\cdot\det\left(  \underbrace{\left(  a_{i,j}^{\prime}\right)  _{1\leq
i\leq3,\ 1\leq j\leq3}}_{=\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  }\right) \\
&  =5\cdot\det\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  .
\end{align*}
Comparing this with $\det\left(  A^{\prime}\right)  =\det A$, we obtain%
\[
\det A=5\cdot\det\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  .
\]


Thus, we have reduced the problem of computing $\det A$ (the determinant of a
$4\times4$-matrix) to the problem of computing $\det\left(
\begin{array}
[c]{ccc}%
1 & 2 & 3\\
-2 & -7/5 & -6/5\\
-1 & 17/5 & -19/5
\end{array}
\right)  $ (the determinant of a $3\times3$-matrix). Likewise, we can try to
reduce the latter problem to the computation of the determinant of a
$2\times2$-matrix, and then further to the computation of the determinant of a
$1\times1$-matrix. (In our example, we obtain $\det A=-140$ at the end.)

This looks like a viable algorithm (which is, furthermore, fairly fast:
essentially as fast as Gaussian elimination). But does it always work? It
turns out that it \textbf{almost} always works. There are cases in which it
can get \textquotedblleft stuck\textquotedblright, and it needs to be modified
to deal with these cases.

Namely, what can happen is that the $\left(  n,n\right)  $-th entry of the
matrix $A$ could be $0$. Again, let us observe this on an example: Set $n=4$
and $A=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
0 & -1 & 0 & 2\\
2 & 4 & -2 & 3\\
5 & 1 & 3 & 0
\end{array}
\right)  $. Then, we cannot turn the first three entries of the fourth row of
$A$ into zeroes by adding appropriate multiples of the fourth column to the
first three columns. (Whatever multiples we add, the fourth row stays
unchanged.) However, we can now switch the second row of $A$ with the fourth
row. This operation produces the matrix $B=\left(
\begin{array}
[c]{cccc}%
1 & 2 & 3 & 0\\
5 & 1 & 3 & 0\\
2 & 4 & -2 & 3\\
0 & -1 & 0 & 2
\end{array}
\right)  $, which satisfies $\det B=-\det A$ (by Exercise \ref{exe.ps4.6}
\textbf{(a)}). Thus, it suffices to compute $\det B$; and this can be done as above.

The reason why we switched the second row of $A$ with the fourth row is that
the last entry of the second row of $A$ was nonzero. In general, we need to
find a $k\in\left\{  1,2,\ldots,n\right\}  $ such that the last entry of the
$k$-th row of $A$ is nonzero, and switch the $k$-th row of $A$ with the $n$-th
row. But what if no such $k$ exists? In this case, we need another way to
compute $\det A$. It turns out that this is very easy: If there is no
$k\in\left\{  1,2,\ldots,n\right\}  $ such that the last entry of the $k$-th
row of $A$ is nonzero, then the last column of $A$ consists of zeroes, and
thus Exercise \ref{exe.ps4.6} \textbf{(d)} shows that $\det A=0$.

When $\mathbb{K}$ is not a field, this algorithm breaks (or, at least,
\textbf{can} break). Indeed, it relies on the fact that the $\left(
n,n\right)  $-th entry of the matrix $A$ is either zero or invertible. Over a
commutative ring $\mathbb{K}$, it might be neither. For example, if we had
tried to work with $\mathbb{K}=\mathbb{Z}$ (instead of $\mathbb{K}=\mathbb{Q}%
$) in our above example, then we would not be able to add $\left(
-1/5\right)  $ times the fourth column of $A$ to the second column of $A$
(because $-1/5\notin\mathbb{Z}=\mathbb{K}$). Fortunately, of course,
$\mathbb{Z}$ is a subset of $\mathbb{Q}$ (and its operations $+$ and $\cdot$
are consistent with those of $\mathbb{Q}$), so that we can just perform the
whole algorithm over $\mathbb{Q}$ instead of $\mathbb{Z}$. However, we aren't
always in luck: Some commutative rings $\mathbb{K}$ cannot be
\textquotedblleft embedded\textquotedblright\ into fields in the way
$\mathbb{Z}$ is embedded into $\mathbb{Q}$. (For instance, $\mathbb{Z}%
/4\mathbb{Z}$ cannot be embedded into a field.)

Nevertheless, there \textbf{are} reasonably fast algorithms for computing
determinants over any commutative ring; see \cite[\S 2]{Rote}.
\end{remark}

\subsection{The Cauchy determinant}

Now, we can state another classical formula for a determinant: the
\textit{Cauchy determinant}. In one of its many forms, it says the following:

\begin{exercise}
\label{exe.cauchy-det}Let $n\in\mathbb{N}$. Let $x_{1},x_{2},\ldots,x_{n}$ be
$n$ elements of $\mathbb{K}$. Let $y_{1},y_{2},\ldots,y_{n}$ be $n$ elements
of $\mathbb{K}$. Assume that $x_{i}+y_{j}$ is invertible in $\mathbb{K}$ for
every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,
prove that%
\[
\det\left(  \left(  \dfrac{1}{x_{i}+y_{j}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  =\frac{\prod_{1\leq i<j\leq n}\left(  \left(  x_{i}%
-x_{j}\right)  \left(  y_{i}-y_{j}\right)  \right)  }{\prod_{\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}\left(  x_{i}+y_{j}\right)
}.
\]

\end{exercise}

There is a different version of the Cauchy determinant floating around in
literature; it differs from Exercise \ref{exe.cauchy-det} in that each
\textquotedblleft$x_{i}+y_{j}$\textquotedblright\ is replaced by
\textquotedblleft$x_{i}-y_{j}$\textquotedblright, and in that
\textquotedblleft$y_{i}-y_{j}$\textquotedblright\ is replaced by
\textquotedblleft$y_{j}-y_{i}$\textquotedblright. Of course, this version is
nothing else than the result of applying Exercise \ref{exe.cauchy-det} to
$-y_{1},-y_{2},\ldots,-y_{n}$ instead of $y_{1},y_{2},\ldots,y_{n}$.

\begin{exercise}
\label{exe.cauchy-det-lem}Let $n$ be a positive integer. Let $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix
such that $a_{n,n}$ is invertible (in $\mathbb{K}$). Prove that
\begin{align}
&  \det\left(  \left(  a_{i,j}a_{n,n}-a_{i,n}a_{n,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}\right) \nonumber\\
&  =a_{n,n}^{n-2}\cdot\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  . \label{eq.exe.cauchy-det-lem}%
\end{align}

\end{exercise}

Exercise \ref{exe.cauchy-det-lem} is known as the \textit{Chio pivotal
condensation theorem}\footnote{See \cite[footnote 2]{Heinig} and
\cite[\S 2]{Abeles} for some hints about its history.}.

\begin{remark}
Exercise \ref{exe.cauchy-det-lem} gives a way to reduce the computation of an
$n\times n$-determinant (the one on the right hand side of
(\ref{eq.exe.cauchy-det-lem})) to the computation of an $\left(  n-1\right)
\times\left(  n-1\right)  $-determinant (the one on the left hand side),
provided that $a_{n,n}$ is invertible. If this reminds you of Remark
\ref{rmk.laplace.pre.gauss-alg}, you are thinking right...
\end{remark}

\begin{remark}
Exercise \ref{exe.cauchy-det-lem} holds even without the assumption that
$a_{n,n}$ be invertible, as long as we assume (instead) that $n\geq2$. (If we
don't assume that $n\geq2$, then the $a_{n,n}^{n-2}$ on the right hand side of
(\ref{eq.exe.cauchy-det-lem}) will not be defined for non-invertible $a_{n,n}%
$.) Proving this is beyond these notes, though. (A proof of this generalized
version of Exercise \ref{exe.cauchy-det-lem} can be found in \cite{KarZha16}.
It can also be obtained as a particular case of \cite[(4)]{BerBru08}\footnotemark.)
\end{remark}

\footnotetext{In more detail: If we apply \cite[(4)]{BerBru08} to $k=n-1$,
then the right hand side is precisely $\det\left(  \left(  a_{i,j}%
a_{n,n}-a_{i,n}a_{n,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)  $,
and so the formula becomes (\ref{eq.exe.cauchy-det-lem}).}

\subsection{Further determinant equalities}

Next, let us provide an assortment of other exercises on determinants.

\begin{exercise}
\label{exe.det.schur-lem}Let $n\in\mathbb{N}$. Let $\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $b_{1}$,
$b_{2}$, $...$, $b_{n}$ be $n$ elements of $\mathbb{K}$. Prove that%
\[
\sum\limits_{k=1}^{n}\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(  b_{1}+b_{2}+\cdots
+b_{n}\right)  \det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\right)  ,
\]
where $\delta_{j,k}$ means the nonnegative integer $%
\begin{cases}
1, & \text{if }j=k;\\
0, & \text{if }j\neq k
\end{cases}
$. Equivalently (in more reader-friendly terms): Prove that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1}b_{1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1}b_{2} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1}b_{n} & a_{n,2} & \cdots & a_{n,n}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2}b_{1} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2}b_{2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2}b_{n} & \cdots & a_{n,n}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\cdots+\det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,n}b_{1}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}b_{2}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}b_{n}%
\end{array}
\right) \\
&  =\left(  b_{1}+b_{2}+\cdots+b_{n}\right)  \det\left(
\begin{array}
[c]{cccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,n}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,n}%
\end{array}
\right)  .
\end{align*}

\end{exercise}

\begin{exercise}
\label{exe.det.a1a2anx}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ elements of $\mathbb{K}$. Let $x\in\mathbb{K}$. Prove that%
\[
\det\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]

\end{exercise}

\begin{exercise}
\label{exe.det.2diags}Let $n>1$ be an integer. Let $a_{1},a_{2},\ldots,a_{n}$
be $n$ elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n}$ be $n$
elements of $\mathbb{K}$. Let $A$ be the $n\times n$-matrix%
\begin{align*}
&  \left(
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\\
&  =\left(
\begin{array}
[c]{cccccc}%
a_{1} & 0 & 0 & \cdots & 0 & b_{n}\\
b_{1} & a_{2} & 0 & \cdots & 0 & 0\\
0 & b_{2} & a_{3} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{n-1} & 0\\
0 & 0 & 0 & \cdots & b_{n-1} & a_{n}%
\end{array}
\right)  .
\end{align*}
Prove that%
\[
\det A=a_{1}a_{2}\cdots a_{n}+\left(  -1\right)  ^{n-1}b_{1}b_{2}\cdots
b_{n}.
\]

\end{exercise}

\begin{remark}
If we replace \textquotedblleft$i\equiv j+1\operatorname{mod}n$%
\textquotedblright\ by \textquotedblleft$i\equiv j+2\operatorname{mod}%
n$\textquotedblright\ in Exercise \ref{exe.det.2diags}, then the pattern can
break. For instance, for $n=4$ we have%
\[
\det\left(
\begin{array}
[c]{cccc}%
a_{1} & 0 & b_{3} & 0\\
0 & a_{2} & 0 & b_{4}\\
b_{1} & 0 & a_{3} & 0\\
0 & b_{2} & 0 & a_{4}%
\end{array}
\right)  =\left(  a_{2}a_{4}-b_{2}b_{4}\right)  \left(  a_{1}a_{3}-b_{1}%
b_{3}\right)  ,
\]
which is not of the form $a_{1}a_{2}a_{3}a_{4}\pm b_{1}b_{2}b_{3}b_{4}$
anymore. Can you guess for which $d\in\left\{  1,2,\ldots,n-1\right\}  $ we
can replace \textquotedblleft$i\equiv j+1\operatorname{mod}n$%
\textquotedblright\ by \textquotedblleft$i\equiv j+d\operatorname{mod}%
n$\textquotedblright\ in Exercise \ref{exe.det.2diags} and still get a formula
of the form $\det A=a_{1}a_{2}\cdots a_{n}\pm b_{1}b_{2}\cdots b_{n}$ ?
\end{remark}

\subsection{Alternating matrices}

Our next two exercises will concern two special classes of matrices: the
\textit{antisymmetric} and the \textit{alternating matrices}. Let us first
define these classes:

\begin{definition}
\label{def.altern}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.

\textbf{(a)} The matrix $A$ is said to be \textit{antisymmetric} if and only
if $A^{T}=-A$. (Recall that $A^{T}$ is defined as in Definition
\ref{def.transpose}.)

\textbf{(b)} The matrix $A$ is said to be \textit{alternating} if and only if
it satisfies $A^{T}=-A$ and $\left(  a_{i,i}=0\text{ for all }i\in\left\{
1,2,\ldots,n\right\}  \right)  $.
\end{definition}

\begin{example}
\label{exam.altern}A $1\times1$-matrix is alternating if and only if it is the
zero matrix $0_{1\times1}=\left(
\begin{array}
[c]{c}%
0
\end{array}
\right)  $.

A $2\times2$-matrix is alternating if and only if it has the form $\left(
\begin{array}
[c]{cc}%
0 & a\\
-a & 0
\end{array}
\right)  $ for some $a\in\mathbb{K}$.

A $3\times3$-matrix is alternating if and only if it has the form $\left(
\begin{array}
[c]{ccc}%
0 & a & b\\
-a & 0 & c\\
-b & -c & 0
\end{array}
\right)  $ for some $a,b,c\in\mathbb{K}$.

Visually speaking, an $n\times n$-matrix is alternating if and only if its
diagonal entries are $0$ and its entries below the diagonal are the negatives
of their \textquotedblleft mirror-image\textquotedblright\ entries above the diagonal.
\end{example}

\begin{remark}
Clearly, any alternating matrix is antisymmetric. It is easy to see that an
$n\times n$-matrix $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ is antisymmetric if and only if every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies $a_{i,j}=-a_{j,i}$. Thus, if $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ is antisymmetric, then every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies $a_{i,i}=-a_{i,i}$ and thus
$2a_{i,i}=0$. If $\mathbb{K}$ is one of the rings $\mathbb{Z}$, $\mathbb{Q}$,
$\mathbb{R}$ and $\mathbb{C}$, then we can cancel $2$ from this last equality,
and conclude that every antisymmetric $n\times n$-matrix $A$ is alternating.
However, there are commutative rings $\mathbb{K}$ for which this does not hold
(for example, the ring $\mathbb{Z}/2\mathbb{Z}$ of integers modulo $2$).

Antisymmetric matrices are also known as \textit{skew-symmetric} matrices.
\end{remark}

\begin{exercise}
\label{exe.altern.STAS}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be
an alternating $n\times n$-matrix. Let $S$ be an $n\times m$-matrix. Prove
that the $m\times m$-matrix $S^{T}AS$ is alternating.
\end{exercise}

\begin{exercise}
\label{exe.altern.det}Let $n\in\mathbb{N}$ be odd. Let $A$ be an $n\times
n$-matrix. Prove the following:

\textbf{(a)} If $A$ is antisymmetric, then $2\det A=0$.

\textbf{(b)} If $A$ is alternating, then $\det A=0$.
\end{exercise}

\begin{remark}
If $\mathbb{K}$ is one of the rings $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$
and $\mathbb{C}$, then Exercise \ref{exe.altern.det} \textbf{(b)} follows from
Exercise \ref{exe.altern.det} \textbf{(a)} (because any alternating matrix is
antisymmetric, and because we can cancel $2$ from the equality $2\det A=0$).
However, this quick way of solving Exercise \ref{exe.altern.det} \textbf{(b)}
does not work for general $\mathbb{K}$.
\end{remark}

\begin{remark}
Exercise \ref{exe.altern.det} \textbf{(b)} provides a really simple formula
for $\det A$ when $A$ is an alternating $n\times n$-matrix for \textbf{odd}
$n$. One might wonder what can be said about $\det A$ when $A$ is an
alternating $n\times n$-matrix for \textbf{even} $n$. The answer is far less
simple, but more interesting: It turns that $\det A$ is the square of a
certain element of $\mathbb{K}$, called the \textit{Pfaffian} of $A$. See
\cite[(5.5)]{Conrad-Pf} for a short introduction into the Pfaffian (although
at a less elementary level than these notes); see \cite[\S 9.5]{BruRys91} for
a more combinatorial treatment of the Pfaffian (and an application to
matchings of graphs!). For example, the Pfaffian of an alternating $4\times
4$-matrix $A=\left(
\begin{array}
[c]{cccc}%
0 & a & b & c\\
-a & 0 & d & e\\
-b & -d & 0 & f\\
-c & -e & -f & 0
\end{array}
\right)  $ is $af-be+cd$, and it is indeed easy to check that this matrix
satisfies $\det A=\left(  af-be+cd\right)  ^{2}$.
\end{remark}

\subsection{Laplace expansion}

We shall now state Laplace expansion in full. We begin with an example:

\begin{example}
\label{exa.laplace.3x3}Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq3,\ 1\leq
j\leq3}$ be a $3\times3$-matrix. From (\ref{eq.det.small.3x3}), we obtain%
\begin{equation}
\det A=a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}%
a_{3,2}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3}a_{2,2}a_{3,1}.
\label{eq.exa.laplace.3x3.1}%
\end{equation}
On the right hand side of this equality, we have six terms, each of which
contains either $a_{2,1}$ or $a_{2,2}$ or $a_{2,3}$. Let us combine the two
terms containing $a_{2,1}$ and factor out $a_{2,1}$, then do the same with the
two terms containing $a_{2,2}$, and with the two terms containing $a_{2,3}$.
As a result, (\ref{eq.exa.laplace.3x3.1}) becomes%
\begin{align}
&  \det A\nonumber\\
&  =a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}%
-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3}a_{2,2}a_{3,1}\nonumber\\
&  =a_{2,1}\underbrace{\left(  a_{1,3}a_{3,2}-a_{1,2}a_{3,3}\right)  }%
_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,3} & a_{1,2}\\
a_{3,3} & a_{3,2}%
\end{array}
\right)  }+a_{2,2}\underbrace{\left(  a_{1,1}a_{3,3}-a_{1,3}a_{3,1}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  }+a_{2,3}\underbrace{\left(  a_{1,2}a_{3,1}-a_{1,1}a_{3,2}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,1}\\
a_{3,2} & a_{3,1}%
\end{array}
\right)  }\nonumber\\
&  =a_{2,1}\det\left(
\begin{array}
[c]{cc}%
a_{1,3} & a_{1,2}\\
a_{3,3} & a_{3,2}%
\end{array}
\right)  +a_{2,2}\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  +a_{2,3}\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,1}\\
a_{3,2} & a_{3,1}%
\end{array}
\right)  . \label{eq.exa.laplace.3x3.2}%
\end{align}
This is a nice formula with an obvious pattern: The right hand side can be
rewritten as $\sum_{q=1}^{3}a_{2,q}\det\left(  B_{2,q}\right)  $, where
$B_{2,q}=\left(
\begin{array}
[c]{cc}%
a_{1,q+2} & a_{1,q+1}\\
a_{3,q+2} & a_{3,q+1}%
\end{array}
\right)  $ (where we set $a_{i,4}=a_{i,1}$ and $a_{i,5}=a_{i,2}$ for all
$i\in\left\{  1,2,3\right\}  $). Notice the cyclic symmetry (with respect to
the index of the column) in this formula! Unfortunately, in this exact form,
the formula does not generalize to bigger matrices (or even to smaller: the
analogue for a $2\times2$-matrix would be $\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  =-a_{2,1}a_{1,2}+a_{2,2}a_{1,1}$, which has a minus sign unlike
$\sum_{q=1}^{3}a_{2,q}\det\left(  B_{2,q}\right)  $).

However, we can slightly modify our formula, sacrificing the cyclic symmetry
but making it generalize. Namely, let us rewrite $a_{1,3}a_{3,2}%
-a_{1,2}a_{3,3}$ as $-\left(  a_{1,2}a_{3,3}-a_{1,3}a_{3,2}\right)  $ and
$a_{1,2}a_{3,1}-a_{1,1}a_{3,2}$ as $-\left(  a_{1,1}a_{3,2}-a_{1,2}%
a_{3,1}\right)  $; we thus obtain
\begin{align}
&  \det A\nonumber\\
&  =a_{2,1}\underbrace{\left(  a_{1,3}a_{3,2}-a_{1,2}a_{3,3}\right)
}_{=-\left(  a_{1,2}a_{3,3}-a_{1,3}a_{3,2}\right)  }+a_{2,2}\left(
a_{1,1}a_{3,3}-a_{1,3}a_{3,1}\right)  +a_{2,3}\underbrace{\left(
a_{1,2}a_{3,1}-a_{1,1}a_{3,2}\right)  }_{=-\left(  a_{1,1}a_{3,2}%
-a_{1,2}a_{3,1}\right)  }\nonumber\\
&  =-a_{2,1}\underbrace{\left(  a_{1,2}a_{3,3}-a_{1,3}a_{3,2}\right)  }%
_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,3}\\
a_{3,2} & a_{3,3}%
\end{array}
\right)  }+a_{2,2}\underbrace{\left(  a_{1,1}a_{3,3}-a_{1,3}a_{3,1}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  }-a_{2,3}\underbrace{\left(  a_{1,1}a_{3,2}-a_{1,2}a_{3,1}\right)
}_{=\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{3,1} & a_{3,2}%
\end{array}
\right)  }\nonumber\\
&  =-a_{2,1}\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,3}\\
a_{3,2} & a_{3,3}%
\end{array}
\right)  +a_{2,2}\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right)  -a_{2,3}\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{3,1} & a_{3,2}%
\end{array}
\right) \nonumber\\
&  =\sum_{q=1}^{3}\left(  -1\right)  ^{q}a_{2,q}\det\left(  C_{2,q}\right)  ,
\label{eq.exa.laplace.3x3.3}%
\end{align}
where $C_{2,q}$ means the matrix obtained from $A$ by crossing out the $2$-nd
row and the $q$-th column. This formula (unlike (\ref{eq.exa.laplace.3x3.2}))
involves powers of $-1$, but it can be generalized.

How? First, we notice that we can find a similar formula by factoring out
$a_{1,1},a_{1,2},a_{1,3}$ (instead of $a_{2,1},a_{2,2},a_{2,3}$); this formula
will be%
\[
\det A=\sum_{q=1}^{3}\left(  -1\right)  ^{q-1}a_{1,q}\det\left(
C_{1,q}\right)  ,
\]
where $C_{1,q}$ means the matrix obtained from $A$ by crossing out the $1$-st
row and the $q$-th column. This formula, and (\ref{eq.exa.laplace.3x3.3}),
suggest the following generalization: If $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ is an $n\times n$-matrix, and if $p\in\left\{
1,2,\ldots,n\right\}  $, then%
\begin{equation}
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(
C_{p,q}\right)  , \label{eq.exa.laplace.3x3.4}%
\end{equation}
where $C_{p,q}$ means the matrix obtained from $A$ by crossing out the $p$-th
row and the $q$-th column. (The only part of this formula which is not easy to
guess is $\left(  -1\right)  ^{p+q}$; you might need to compute several
particular cases to guess this pattern. Of course, you could also have guessed
$\left(  -1\right)  ^{p-q}$ or $\left(  -1\right)  ^{q-p}$ instead, because
$\left(  -1\right)  ^{p+q}=\left(  -1\right)  ^{p-q}=\left(  -1\right)
^{q-p}$.)

The formula (\ref{eq.exa.laplace.3x3.4}) is what is usually called the Laplace
expansion with respect to the $p$-th row. We will prove it below (Theorem
\ref{thm.laplace.gen} \textbf{(a)}), and we will also prove an analogous
\textquotedblleft Laplace expansion with respect to the $q$-th
column\textquotedblright\ (Theorem \ref{thm.laplace.gen} \textbf{(b)}).
\end{example}

Let us first define a notation:

\begin{definition}
\label{def.submatrix}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be an $n\times m$-matrix.
Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{  1,2,\ldots
,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some elements of $\left\{
1,2,\ldots,m\right\}  $. Then, we define $\operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ to be the $u\times v$-matrix
$\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}$.

When $i_{1}<i_{2}<\cdots<i_{u}$ and $j_{1}<j_{2}<\cdots<j_{v}$, the matrix
$\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A$ can be obtained from $A$ by crossing out all rows other than
the $i_{1}$-th, the $i_{2}$-th, etc., the $i_{u}$-th row and crossing out all
columns other than the $j_{1}$-th, the $j_{2}$-th, etc., the $j_{v}$-th
column. Thus, in this case, $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ is called a \textit{submatrix} of
$A$.
\end{definition}

For example, if $n=3$, $m=4$ and $A=\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
e & f & g & h\\
i & j & k & \ell
\end{array}
\right)  $, then $\operatorname*{sub}\nolimits_{1,3}^{2,3,4}A=\left(
\begin{array}
[c]{ccc}%
b & c & d\\
j & k & \ell
\end{array}
\right)  $ (this is a submatrix of $A$) and $\operatorname*{sub}%
\nolimits_{2,3}^{3,1,1}A=\left(
\begin{array}
[c]{ccc}%
g & e & e\\
k & i & i
\end{array}
\right)  $ (this is not, in general, a submatrix of $A$).

The following properties follow trivially from the definitions:

\begin{proposition}
\label{prop.submatrix.easy}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix. Recall the notations introduced in Definition
\ref{def.rowscols}.

\textbf{(a)} We have $\operatorname*{sub}\nolimits_{1,2,\ldots,n}%
^{1,2,\ldots,m}A=A$.

\textbf{(b)} If $i_{1},i_{2},\ldots,i_{u}$ are some elements of $\left\{
1,2,\ldots,n\right\}  $, then%
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots,m}A.
\]


\textbf{(c)} If $j_{1},j_{2},\ldots,j_{v}$ are some elements of $\left\{
1,2,\ldots,m\right\}  $, then%
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}}A.
\]


\textbf{(d)} Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some elements of
$\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)
=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]


\textbf{(e)} Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some elements of
$\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}A\right)  ^{T}=\operatorname*{sub}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}^{i_{1},i_{2},\ldots,i_{u}}\left(  A^{T}\right)  .
\]

\end{proposition}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.submatrix.easy}.]Write the matrix $A$ in the
form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.

\textbf{(a)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{sub}\nolimits_{1,2,\ldots
,n}^{1,2,\ldots,m}A$ yields%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{1,2,\ldots,m}A  &  =\left(
a_{x,y}\right)  _{1\leq x\leq n,\ 1\leq y\leq m}=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right) \\
&  =A.
\end{align*}
This proves Proposition \ref{prop.submatrix.easy} \textbf{(a)}.

\textbf{(b)} Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $. The definition of $\operatorname*{rows}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}A$ yields $\operatorname*{rows}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(  a_{i_{x},j}\right)  _{1\leq
x\leq u,\ 1\leq j\leq m}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$). On the other hand, we have $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$. Hence, the definition of
$\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots,m}A$
yields
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots
,m}A=\left(  a_{i_{x},y}\right)  _{1\leq x\leq u,\ 1\leq y\leq m}=\left(
a_{i_{x},j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}%
\]
(here, we renamed the index $\left(  x,y\right)  $ as $\left(  x,j\right)  $).
Comparing this with $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}A=\left(  a_{i_{x},j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}$, we obtain
$\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{1,2,\ldots,m}A$. This proves Proposition
\ref{prop.submatrix.easy} \textbf{(b)}.

\textbf{(c)} Let $j_{1},j_{2},\ldots,j_{v}$ be some elements of $\left\{
1,2,\ldots,m\right\}  $. The definition of $\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A$ yields $\operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i,j_{y}}\right)  _{1\leq
i\leq n,\ 1\leq y\leq v}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$). On the other hand, we have $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$. Hence, the definition of
$\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}}A$
yields%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}%
}A=\left(  a_{x,j_{y}}\right)  _{1\leq x\leq n,\ 1\leq y\leq v}=\left(
a_{i,j_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}%
\]
(here, we renamed the index $\left(  x,y\right)  $ as $\left(  i,y\right)  $).
Comparing this with $\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}%
}A=\left(  a_{i,j_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}$, we obtain
$\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,n}^{j_{1},j_{2},\ldots,j_{v}}A$. This proves Proposition
\ref{prop.submatrix.easy} \textbf{(c)}.

\textbf{(d)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ yields $\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}$.

On the other hand, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$. Thus, the definition of $\operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}A$ yields
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i,j_{y}%
}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}=\left(  a_{i,j_{j}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq v}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  i,j\right)
$)\ \ \ \ \footnote{The notation $j_{j}$ might look fishy, since it uses the
letter \textquotedblleft$j$\textquotedblright\ in two unrelated meanings: Its
meaning in \textquotedblleft$j_{1},j_{2},\ldots,j_{v}$\textquotedblright\ has
nothing to do with its meaning with \textquotedblleft$1\leq j\leq
v$\textquotedblright. However, it is easy to distinguish between these two
kinds of \textquotedblleft$j$\textquotedblright, because the former always
appears with a subscript, whereas the latter never does.}. Hence, the
definition of \newline$\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots
,i_{u}}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}%
}A\right)  $ yields%
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}\left(
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)  =\left(
a_{i_{x},j_{j}}\right)  _{1\leq x\leq n,\ 1\leq j\leq v}=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq n,\ 1\leq y\leq v}%
\]
(here, we renamed the index $\left(  x,j\right)  $ as $\left(  x,y\right)  $).
Compared with $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}%
^{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq
u,\ 1\leq y\leq v}$, this yields%
\begin{equation}
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)  .
\label{pf.prop.submatrix.easy.d.1}%
\end{equation}


Furthermore, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A$ yields
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(  a_{i_{x}%
,j}\right)  _{1\leq x\leq u,\ 1\leq j\leq m}=\left(  a_{i_{i},j}\right)
_{1\leq i\leq u,\ 1\leq j\leq m}%
\]
\footnote{The double use of the letter \textquotedblleft$i$\textquotedblright%
\ in \textquotedblleft$i_{i}$\textquotedblright\ might appear confusing. The
first \textquotedblleft$i$\textquotedblright\ is part of the notation $i_{k}$
for $k\in\left\{  1,2,\ldots,u\right\}  $; the second \textquotedblleft%
$i$\textquotedblright\ is an element of $\left\{  1,2,\ldots,u\right\}  $.
These two \textquotedblleft$i$\textquotedblright s are unrelated to each
other. I hope the reader can easily tell them apart by the fact that the
\textquotedblleft$i$\textquotedblright\ that is part of the notation $i_{k}$
always appears with a subscript, whereas the second \textquotedblleft%
$i$\textquotedblright\ never does.} (here, we renamed the index $\left(
x,j\right)  $ as $\left(  i,j\right)  $). Hence, the definition of
\newline$\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  $ yields%
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  =\left(
a_{i_{i},j_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq v}=\left(
a_{i_{x},j_{y}}\right)  _{1\leq x\leq n,\ 1\leq y\leq v}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  x,y\right)  $).
Compared with $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}%
^{j_{1},j_{2},\ldots,j_{v}}A=\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq
u,\ 1\leq y\leq v}$, this yields%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}%
}\left(  \operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]
Combining this with (\ref{pf.prop.submatrix.easy.d.1}), we obtain%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}A\right)
=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]
This proves Proposition \ref{prop.submatrix.easy} \textbf{(d)}.

\textbf{(e)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$. Thus, the definition of $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A$ yields
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}A=\left(  a_{i_{x},j_{y}}\right)  _{1\leq x\leq u,\ 1\leq y\leq
v}=\left(  a_{i_{i},j_{j}}\right)  _{1\leq i\leq u,\ 1\leq j\leq v}%
\]
(here, we have renamed the index $\left(  x,y\right)  $ as $\left(
i,j\right)  $). Therefore, the definition of $\left(  \operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A\right)  ^{T}$
yields
\[
\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}A\right)  ^{T}=\left(  a_{i_{j},j_{i}}\right)  _{1\leq
i\leq v,\ 1\leq j\leq u}=\left(  a_{i_{y},j_{x}}\right)  _{1\leq x\leq
v,\ 1\leq y\leq u}%
\]
(here, we have renamed the index $\left(  i,j\right)  $ as $\left(
x,y\right)  $).

On the other hand, the definition of $A^{T}$ yields $A^{T}=\left(
a_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$ (since $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$). Hence, the definition of
$\operatorname*{sub}\nolimits_{j_{1},j_{2},\ldots,j_{v}}^{i_{1},i_{2}%
,\ldots,i_{u}}\left(  A^{T}\right)  $ yields $\operatorname*{sub}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}^{i_{1},i_{2},\ldots,i_{u}}\left(
A^{T}\right)  =\left(  a_{i_{y},j_{x}}\right)  _{1\leq x\leq v,\ 1\leq y\leq
u}$. Comparing this with $\left(  \operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A\right)  ^{T}=\left(
a_{i_{y},j_{x}}\right)  _{1\leq x\leq v,\ 1\leq y\leq u}$, we obtain%
\[
\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}A\right)  ^{T}=\operatorname*{sub}\nolimits_{j_{1}%
,j_{2},\ldots,j_{v}}^{i_{1},i_{2},\ldots,i_{u}}\left(  A^{T}\right)  .
\]
This proves Proposition \ref{prop.submatrix.easy} \textbf{(e)}.
\end{proof}
\end{verlong}

\begin{definition}
\label{def.hat-omit}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ objects. Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\left(
a_{1},a_{2},\ldots,\widehat{a_{i}},\ldots,a_{n}\right)  $ shall mean the list
$\left(  a_{1},a_{2},\ldots,a_{i-1},a_{i+1},a_{i+2},\ldots,a_{n}\right)  $
(that is, the list $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ with its $i$-th
entry removed). (Thus, the \textquotedblleft hat\textquotedblright\ over the
$a_{i}$ means that this $a_{i}$ is being omitted from the list.)

For example, $\left(  1^{2},2^{2},\ldots,\widehat{5^{2}},\ldots,8^{2}\right)
=\left(  1^{2},2^{2},3^{2},4^{2},6^{2},7^{2},8^{2}\right)  $.
\end{definition}

\begin{definition}
\label{def.submatrix.minor}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times m$-matrix. For every $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,m\right\}  $, we let $A_{\sim i,\sim j}$ be the
$\left(  n-1\right)  \times\left(  m-1\right)  $-matrix $\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{i},\ldots,n}^{1,2,\ldots,\widehat{j},\ldots
,m}A$. (Thus, $A_{\sim i,\sim j}$ is the matrix obtained from $A$ by crossing
out the $i$-th row and the $j$-th column.)

For example, if $n=m=3$ and $A=\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $, then $A_{\sim1,\sim2}=\left(
\begin{array}
[c]{cc}%
d & f\\
g & i
\end{array}
\right)  $ and $A_{\sim3,\sim2}=\left(
\begin{array}
[c]{cc}%
a & c\\
d & f
\end{array}
\right)  $.
\end{definition}

The notation $A_{\sim i,\sim j}$ introduced in Definition
\ref{def.submatrix.minor} is not very standard; but there does not seem to be
a standard one\footnote{For example, Gill Williamson uses the notation
$A\left(  i\mid j\right)  $ in \cite[Chapter 3]{Gill}.}.

Now we can finally state Laplace expansion:

\begin{theorem}
\label{thm.laplace.gen}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.

\textbf{(a)} For every $p\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim
p,\sim q}\right)  .
\]


\textbf{(b)} For every $q\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\det A=\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim
p,\sim q}\right)  .
\]

\end{theorem}

Theorem \ref{thm.laplace.gen} \textbf{(a)} is known as the \textit{Laplace
expansion along the }$p$\textit{-th row} (or \textit{Laplace expansion with
respect to the }$p$\textit{-th row}), whereas Theorem \ref{thm.laplace.gen}
\textbf{(b)} is known as the \textit{Laplace expansion along the }%
$q$\textit{-th column} (or \textit{Laplace expansion with respect to the }%
$q$\textit{-th column}). Notice that Theorem \ref{thm.laplace.gen}
\textbf{(a)} is equivalent to the formula (\ref{eq.exa.laplace.3x3.4}),
because the $A_{\sim p,\sim q}$ in Theorem \ref{thm.laplace.gen} \textbf{(a)}
is precisely what we called $C_{p,q}$ in (\ref{eq.exa.laplace.3x3.4}).

We prepare the field for the proof of Theorem \ref{thm.laplace.gen} with a few lemmas.

\begin{vershort}
\begin{lemma}
\label{lem.laplace.gpshort}For every $n\in\mathbb{N}$, let $\left[  n\right]
$ denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$. For every $p\in\left[  n\right]  $, we define a
permutation $g_{p}\in S_{n}$ by $g_{p}=\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}$ (where we are using the notations of Definition
\ref{def.perm.cycles}).

\textbf{(a)} We have $\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)
,\ldots,g_{p}\left(  n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $ for every $p\in\left[  n\right]  $.

\textbf{(b)} We have $\left(  -1\right)  ^{g_{p}}=\left(  -1\right)  ^{n-p}$
for every $p\in\left[  n\right]  $.

\textbf{(c)} Let $p\in\left[  n\right]  $. We define a map%
\[
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}
\]
by%
\[
\left(  g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n-1\right]  \right)  .
\]
This map $g_{p}^{\prime}$ is well-defined and bijective.

\textbf{(d)} Let $p\in\left[  n\right]  $ and $q\in\left[  n\right]  $. We
define a map
\[
T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\]
by
\[
\left(  T\left(  \sigma\right)  =g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  \right)  .
\]
Then, this map $T$ is well-defined and bijective.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.laplace.gpshort}.]\textbf{(a)} This is trivial.

\textbf{(b)} Let $p\in\left[  n\right]  $. Exercise \ref{exe.perm.cycles}
\textbf{(d)} (applied to $k=p+1$ and $\left(  i_{1},i_{2},\ldots,i_{k}\right)
=\left(  p,p+1,\ldots,n\right)  $) yields%
\[
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}}=\left(
-1\right)  ^{n-\left(  p+1\right)  -1}=\left(  -1\right)  ^{n-p-2}=\left(
-1\right)  ^{n-p}.
\]
Now, $g_{p}=\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}$, so that $\left(
-1\right)  ^{g_{p}}=\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}}=\left(  -1\right)  ^{n-p}$. This proves Lemma
\ref{lem.laplace.gpshort} \textbf{(b)}.

\textbf{(c)} We have $g_{p}\left(  n\right)  =p$ (since $g_{p}%
=\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}$). Also, $g_{p}$ is injective
(since $g_{p}$ is a permutation). Therefore, for every $i\in\left[
n-1\right]  $, we have%
\begin{align*}
g_{p}\left(  i\right)   &  \neq g_{p}\left(  n\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\neq n\text{ (because }i\in\left[
n-1\right]  \text{) and since }g_{p}\text{ is injective}\right) \\
&  =p,
\end{align*}
so that $g_{p}\left(  i\right)  \in\left[  n\right]  \setminus\left\{
p\right\}  $. This shows that the map $g_{p}^{\prime}$ is well-defined.

To prove that $g_{p}^{\prime}$ is bijective, we can construct its inverse.
Indeed, for every $i\in\left[  n\right]  \setminus\left\{  p\right\}  $, we
have%
\[
\left(  g_{p}\right)  ^{-1}\left(  i\right)  \neq n\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\neq p=g_{p}\left(  n\right)  \right)
\]
and thus $\left(  g_{p}\right)  ^{-1}\left(  i\right)  \in\left[  n-1\right]
$. Hence, we can define a map $h:\left[  n\right]  \setminus\left\{
p\right\}  \rightarrow\left[  n-1\right]  $ by%
\[
\left(  h\left(  i\right)  =\left(  g_{p}\right)  ^{-1}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n\right]  \setminus\left\{
p\right\}  \right)  .
\]
It is straightforward to check that the maps $g_{p}^{\prime}$ and $h$ are
mutually inverse. Thus, $g_{p}^{\prime}$ is bijective. Lemma
\ref{lem.laplace.gpshort} \textbf{(c)} is thus proven.

\textbf{(d)} We have $g_{p}\left(  n\right)  =p$ (since $g_{p}%
=\operatorname*{cyc}\nolimits_{p,p+1,\ldots,n}$) and $g_{q}\left(  n\right)
=q$ (similarly). Hence, $\left(  g_{p}\right)  ^{-1}\left(  p\right)  =n$
(since $g_{p}\left(  n\right)  =p$) and $\left(  g_{q}\right)  ^{-1}\left(
q\right)  =n$ (since $g_{q}\left(  n\right)  =q$).

For every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  $, we have $\sigma\left(  n\right)  =n$ and thus
\[
\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\right)  \left(
p\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  p\right)  }_{=n}\right)  \right)  =g_{q}\left(
\underbrace{\sigma\left(  n\right)  }_{=n}\right)  =g_{q}\left(  n\right)  =q
\]
and therefore $g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in\left\{
\tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  $. Thus, the map $T$ is well-defined.

We can also define a map
\[
Q:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\]
by%
\[
\left(  Q\left(  \sigma\right)  =\left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  \right)  .
\]
The well-definedness of $Q$ can be checked similarly to how we proved the
well-definedness of $T$. It is straightforward to verify that the maps $Q$ and
$T$ are mutually inverse. Thus, $T$ is bijective. This completes the proof of
Lemma \ref{lem.laplace.gpshort} \textbf{(d)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{lemma}
\label{lem.laplace.gp}For every $n\in\mathbb{N}$, let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $n\in\mathbb{N}$. We recall that, for each $k\in\left\{  1,2,\ldots
,n-1\right\}  $, we have defined $s_{k}$ to be the permutation in $S_{n}$ that
switches $k$ with $k+1$ but leaves all other numbers unchanged.

For every $p\in\left[  n\right]  $, we define a permutation $g_{p}\in S_{n}$
by%
\[
g_{p}=s_{p}\circ s_{p+1}\circ\cdots\circ s_{n-1}.
\]
(Thus, for $n>0$ and $p=n$, we have $g_{n}=s_{n}\circ s_{n+1}\circ\cdots\circ
s_{n-1}=\left(  \text{a composition of }0\text{ permutations}\right)
=\operatorname*{id}$.)

\textbf{(a)} We have $g_{p}\left(  i\right)  =i$ for every $p\in\left[
n\right]  $ and every $i\in\left[  n\right]  $ satisfying $i<p$.

\textbf{(b)} We have $g_{p}\left(  i\right)  =i+1$ for every $p\in\left[
n\right]  $ and every $i\in\left[  n\right]  $ satisfying $p\leq i<n$.

\textbf{(c)} We have $g_{p}\left(  n\right)  =p$ for every $p\in\left[
n\right]  $.

\textbf{(d)} We have $\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)
,\ldots,g_{p}\left(  n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $ for every $p\in\left[  n\right]  $.

\textbf{(e)} We have $\left(  -1\right)  ^{g_{p}}=\left(  -1\right)  ^{n-p}$
for every $p\in\left[  n\right]  $.

\textbf{(f)} Let $p\in\left[  n\right]  $. We define a map%
\[
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}
\]
by%
\[
\left(  g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n-1\right]  \right)  .
\]
This map $g_{p}^{\prime}$ is well-defined and bijective.

\textbf{(g)} Let $p\in\left[  n\right]  $ and $q\in\left[  n\right]  $. We
define a map
\[
T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\]
by
\[
\left(  T\left(  \sigma\right)  =g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  \right)  .
\]
Then, this map $T$ is well-defined and bijective.
\end{lemma}

\begin{remark}
Let $n\in\mathbb{N}$. Let us use the notations of Lemma \ref{lem.laplace.gp}
and of Definition \ref{def.perm.cycles}. Then, $g_{p}=\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}$ (as follows from parts \textbf{(a)}, \textbf{(b)}
and \textbf{(c)} of Lemma \ref{lem.laplace.gp}). From this viewpoint, it
appears weird that I have not defined $g_{p}$ as $\operatorname*{cyc}%
\nolimits_{p,p+1,\ldots,n}$. The reason is merely that I wanted to avoid using
cycles (an aesthetical choice).

Parts \textbf{(a)}, \textbf{(b)} and \textbf{(c)} of Lemma
\ref{lem.laplace.gp} can be viewed as an analogue of (\ref{sol.ps2.2.4.c.aik}).
\end{remark}

\begin{proof}
[Proof of Lemma \ref{lem.laplace.gp}.]Let us first recall that if $n>0$, then
$g_{n}$ is well-defined (since $n\in\left[  n\right]  $) and satisfies
\begin{align*}
g_{n}  &  =s_{n}\circ s_{n+1}\circ\cdots\circ s_{n-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }g_{n}\right) \\
&  =\left(  \text{a composition of }0\text{ permutations}\right)
=\operatorname*{id}.
\end{align*}
Moreover, every $p\in\left[  n-1\right]  $ satisfies%
\begin{equation}
g_{p}=s_{p}\circ g_{p+1} \label{pf.lem.laplace.gp.recgp}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.gp.recgp}):} Let $p\in\left[
n-1\right]  $. Then, $p\in\left[  n-1\right]  =\left\{  1,2,\ldots
,p-1\right\}  $, so that $p+1\in\left\{  2,3,\ldots,p\right\}  \subseteq
\left\{  1,2,\ldots,p\right\}  =\left[  p\right]  $. Hence, $g_{p+1}$ is
well-defined. The definition of $g_{p+1}$ yields $g_{p+1}=s_{p+1}\circ
s_{\left(  p+1\right)  +1}\circ\cdots\circ s_{n-1}=s_{p+1}\circ s_{p+2}%
\circ\cdots\circ s_{n-1}$. Now, $p\in\left[  n-1\right]  \subseteq\left[
n\right]  $, and thus $g_{p}$ is well-defined. The definition of $g_{p}$
yields%
\[
g_{p}=s_{p}\circ s_{p+1}\circ\cdots\circ s_{n-1}=s_{p}\circ\underbrace{\left(
s_{p+1}\circ s_{p+2}\circ\cdots\circ s_{n-1}\right)  }_{=g_{p+1}}=s_{p}\circ
g_{p+1}.
\]
This proves (\ref{pf.lem.laplace.gp.recgp}).}. Moreover, every $p\in\left[
n-1\right]  $ satisfies
\begin{align}
&  s_{p}\left(  p\right)  =p+1;\label{pf.lem.laplace.gp.sp.1}\\
&  s_{p}\left(  p+1\right)  =p;\label{pf.lem.laplace.gp.sp.2}\\
&  \left(  s_{p}\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left[  n\right]  \setminus\left\{  p,p+1\right\}  \right)  .
\label{pf.lem.laplace.gp.sp.3}%
\end{align}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.gp.sp.1}),
(\ref{pf.lem.laplace.gp.sp.2}) and (\ref{pf.lem.laplace.gp.sp.3}):} Let
$p\in\left[  n-1\right]  $. Recall that $s_{p}$ is defined as the permutation
in $S_{n}$ that switches $p$ with $p+1$ but leaves all other numbers
unchanged. Thus, the permutation $s_{p}$ switches $p$ with $p+1$. In other
words, we have $s_{p}\left(  p\right)  =p+1$ and $s_{p}\left(  p+1\right)
=p$. This proves (\ref{pf.lem.laplace.gp.sp.1}) and
(\ref{pf.lem.laplace.gp.sp.2}). Furthermore, the permutation $s_{p}$ leaves
all other numbers unchanged (where \textquotedblleft other\textquotedblright%
\ means \textquotedblleft other than $p$ and $p+1$\textquotedblright). In
other words, $s_{p}\left(  i\right)  =i$ for every $i\in\left[  n\right]
\setminus\left\{  p,p+1\right\}  $. This proves (\ref{pf.lem.laplace.gp.sp.3}%
).}

\textbf{(a)} Fix $i\in\left[  n\right]  $. Thus, $i\in\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $, so that $1\leq i\leq n$. Let us prove that
\begin{equation}
g_{n-q}\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
0,1,\ldots,n-i-1\right\}  . \label{pf.lem.laplace.gp.a.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.a.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.a.claim}) by induction over $q$:

\textit{Induction base:} If $0\in\left\{  0,1,\ldots,n-i-1\right\}  $, then
$g_{n-0}\left(  i\right)  =i$\ \ \ \ \footnote{\textit{Proof.} Assume that
$0\in\left\{  0,1,\ldots,n-i-1\right\}  $. Thus, $0\leq0\leq n-i-1$, so that
$0\leq n-i-1$ and therefore $1\leq n-\underbrace{i}_{\geq0}\leq n$. Hence,
$n\geq1>0$, and thus $g_{n}=\operatorname*{id}$ (as we know). Hence,
$\underbrace{g_{n-0}}_{=g_{n}=\operatorname*{id}}\left(  i\right)
=\operatorname*{id}\left(  i\right)  =i$, qed.}. In other words,
(\ref{pf.lem.laplace.gp.a.claim}) holds for $q=0$. Thus, the induction base is complete.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,n-i-1\right\}  $ be
positive. Assume that (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q-1$. We
need to show that (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q$.

We have $Q\leq n-i-1$ (since $Q\in\left\{  0,1,\ldots,n-i-1\right\}  $) and
$Q\geq1$ (since $Q$ is positive and belongs to $\left\{  0,1,\ldots
,n-i-1\right\}  $). Hence, $n-\underbrace{Q}_{\substack{\leq
n-i-1<n-1\\\text{(since }i\geq1>0\text{)}}}>n-\left(  n-1\right)  =1$. Hence,
$n-Q\geq1$. Also, $n-\underbrace{Q}_{\geq1}\leq n-1$. Combining $n-Q\geq1$
with $n-Q\leq n-1$, we obtain $1\leq n-Q\leq n-1$ and thus $n-Q\in\left[
n-1\right]  $. Hence, (\ref{pf.lem.laplace.gp.recgp}) (applied to $p=n-Q$)
shows that $g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$.

We have $Q\leq n-i-1$, thus $n-\underbrace{Q}_{\leq n-i-1}-1\geq n-\left(
n-i-1\right)  -1=i$, so that $i\leq n-Q-1<n-Q$ and thus $i\neq n-Q$. Also,
$i<n-Q<n-Q+1$ and thus $i\neq n-Q+1$. Combining $i\neq n-Q$ with $i\neq
n-Q+1$, we obtain $i\notin\left\{  n-Q,n-Q+1\right\}  $. Combined with
$i\in\left[  n\right]  $, this yields $i\in\left[  n\right]  \setminus\left\{
n-Q,n-Q+1\right\}  $. Hence, (\ref{pf.lem.laplace.gp.sp.3}) (applied to
$p=n-Q$) shows that $s_{n-Q}\left(  i\right)  =i$.

But we assumed that (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q-1$. In
other words, we have $g_{n-\left(  Q-1\right)  }\left(  i\right)  =i$. Since
$n-\left(  Q-1\right)  =n-Q+1$, this rewrites as $g_{n-Q+1}\left(  i\right)
=i$. Now,%
\[
\underbrace{g_{n-Q}}_{=s_{n-Q}\circ g_{n-Q+1}}\left(  i\right)  =\left(
s_{n-Q}\circ g_{n-Q+1}\right)  \left(  i\right)  =s_{n-Q}\left(
\underbrace{g_{n-Q+1}\left(  i\right)  }_{=i}\right)  =s_{n-Q}\left(
i\right)  =i.
\]
In other words, (\ref{pf.lem.laplace.gp.a.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.a.claim}) is complete.

Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.gp.a.claim}) for every $i\in\left[  n\right]  $.

Now, let $p\in\left[  n\right]  $ and $i\in\left[  n\right]  $ be such that
$i<p$. Then, $i<p$, so that $p>i$, and thus $p\geq i+1$ (since both $p$ and
$i$ are integers). Hence, $n-\underbrace{p}_{\geq i+1}\leq n-\left(
i+1\right)  =n-i-1$. Also, $p\leq n$ (since $p\in\left[  n\right]  $), so that
$n-p\geq0$. Combining this with $n-p\leq n-i-1$, we obtain $n-p\in\left\{
0,1,\ldots,n-i-1\right\}  $. Hence, we can apply
(\ref{pf.lem.laplace.gp.a.claim}) to $q=n-p$. We thus obtain $g_{n-\left(
n-p\right)  }\left(  i\right)  =i$. Since $n-\left(  n-p\right)  =p$, this
rewrites as $g_{p}\left(  i\right)  =i$. This proves Lemma
\ref{lem.laplace.gp} \textbf{(a)}.

\textbf{(b)} Fix $i\in\left[  n\right]  $ such that $i<n$. Thus, $i\in\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $, so that $i\geq1$. Combined with
$i<n$, this yields $1\leq i<n$.

We have $1\leq i<n$, so that $i\in\left\{  1,2,\ldots,n-1\right\}  $ and thus
$i+1\in\left\{  2,3,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots,n\right\}
=\left[  n\right]  $.

Let us prove that
\begin{equation}
g_{n-q}\left(  i\right)  =i+1\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
n-i,n-i+1,\ldots,n-1\right\}  . \label{pf.lem.laplace.gp.b.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.b.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.b.claim}) by induction over $q$:

\textit{Induction base:} If $n-i\in\left\{  n-i,n-i+1,\ldots,n-1\right\}  $,
then $g_{n-\left(  n-i\right)  }\left(  i\right)  =i+1$%
\ \ \ \ \footnote{\textit{Proof.} Assume that $n-i\in\left\{  n-i,n-i+1,\ldots
,n-1\right\}  $. We have $i+1\in\left[  n\right]  $. Hence, $g_{i+1}$ is
well-defined. Also, $i<i+1$. Hence, Lemma \ref{lem.laplace.gp} \textbf{(a)}
(applied to $p=i+1$) shows that $g_{i+1}\left(  i\right)  =i$.
\par
Now, $i\in\left\{  1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $. Hence,
(\ref{pf.lem.laplace.gp.recgp}) (applied to $p=i$) yields $g_{i}=s_{i}\circ
g_{i+1}$. Hence,%
\[
\underbrace{g_{i}}_{=s_{i}\circ g_{i+1}}\left(  i\right)  =\left(  s_{i}\circ
g_{i+1}\right)  \left(  i\right)  =s_{i}\left(  \underbrace{g_{i+1}\left(
i\right)  }_{=i}\right)  =s_{i}\left(  i\right)  =i+1
\]
(by (\ref{pf.lem.laplace.gp.sp.1}), applied to $p=i$). Since $n-\left(
n-i\right)  =i$, we now have $g_{n-\left(  n-i\right)  }\left(  i\right)
=g_{i}\left(  i\right)  =i+1$. Qed.}. In other words,
(\ref{pf.lem.laplace.gp.b.claim}) holds for $q=n-i$. Thus, the induction base
is complete.

\textit{Induction step:} Let $Q\in\left\{  n-i,n-i+1,\ldots,n-1\right\}  $ be
such that $Q>n-i$. Assume that (\ref{pf.lem.laplace.gp.b.claim}) holds for
$q=Q-1$. We need to show that (\ref{pf.lem.laplace.gp.b.claim}) holds for
$q=Q$.

Now, we have $Q\leq n-1$ (since $Q\in\left\{  n-i,n-i+1,\ldots,n-1\right\}
$). Hence, $n-\underbrace{Q}_{\leq n-1}\geq n-\left(  n-1\right)  =1$. Also,
$n-\underbrace{Q}_{>n-i}<n-\left(  n-i\right)  =i<n$, so that $n-Q\leq n-1$
(since both $n-Q$ and $n$ are integers). Combining $n-Q\geq1$ with $n-Q\leq
n-1$, we obtain $1\leq n-Q\leq n-1$ and thus $n-Q\in\left[  n-1\right]  $.
Hence, (\ref{pf.lem.laplace.gp.recgp}) (applied to $p=n-Q$) shows that
$g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$.

We have $Q>n-i$, thus $n-\underbrace{Q}_{>n-i}<n-\left(  n-i\right)  =i$.
Hence, $i>n-Q$. Adding $1$ to both sides of this inequality, we obtain
$i+1>n-Q+1$, so that $i+1\neq n-Q+1$. Also, $i+1>n-Q+1>n-Q$, so that
$i+1\notin n-Q$. Combining $i+1\neq n-Q$ with $i+1\neq n-Q+1$, we obtain
$i+1\notin\left\{  n-Q,n-Q+1\right\}  $. Combined with $i+1\in\left[
n\right]  $, this yields $i+1\in\left[  n\right]  \setminus\left\{
n-Q,n-Q+1\right\}  $. Hence, (\ref{pf.lem.laplace.gp.sp.3}) (applied to $n-Q$
and $i+1$ instead of $p$ and $i$) shows that $s_{n-Q}\left(  i+1\right)  =i+1$.

But we assumed that (\ref{pf.lem.laplace.gp.b.claim}) holds for $q=Q-1$. In
other words, we have $g_{n-\left(  Q-1\right)  }\left(  i\right)  =i+1$. Since
$n-\left(  Q-1\right)  =n-Q+1$, this rewrites as $g_{n-Q+1}\left(  i\right)
=i+1$. Now,%
\[
\underbrace{g_{n-Q}}_{=s_{n-Q}\circ g_{n-Q+1}}\left(  i\right)  =\left(
s_{n-Q}\circ g_{n-Q+1}\right)  \left(  i\right)  =s_{n-Q}\left(
\underbrace{g_{n-Q+1}\left(  i\right)  }_{=i+1}\right)  =s_{n-Q}\left(
i+1\right)  =i+1.
\]
In other words, (\ref{pf.lem.laplace.gp.b.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.b.claim}) is complete.

Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.gp.b.claim}) for every $i\in\left[  n\right]  $.

Now, let $p\in\left[  n\right]  $ and every $i\in\left[  n\right]  $
satisfying $p\leq i<n$. Combining $n-\underbrace{p}_{\leq i}\geq n-i$ with
$n-\underbrace{p}_{\substack{\geq1\\\text{(since }p\in\left[  n\right]
\text{)}}}\leq n-1$, we obtain $n-i\leq n-p\leq n-1$, so that $n-p\in\left\{
n-i,n-i+1,\ldots,n-1\right\}  $. Hence, we can apply
(\ref{pf.lem.laplace.gp.b.claim}) to $q=n-p$. We thus obtain $g_{n-\left(
n-p\right)  }\left(  i\right)  =i+1$. Since $n-\left(  n-p\right)  =p$, this
rewrites as $g_{p}\left(  i\right)  =i+1$. This proves Lemma
\ref{lem.laplace.gp} \textbf{(b)}.

\textbf{(c)} Let us first show that%
\begin{equation}
g_{n-q}\left(  n\right)  =n-q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
0,1,\ldots,n-1\right\}  . \label{pf.lem.laplace.gp.c.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.c.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.c.claim}) by induction over $q$:

\textit{Induction base:} If $0\in\left\{  0,1,\ldots,n-1\right\}  $, then
$g_{n-0}\left(  n\right)  =n-0$\ \ \ \ \footnote{\textit{Proof.} Assume that
$0\in\left\{  0,1,\ldots,n-1\right\}  $. Thus, $0\leq0\leq n-1$, so that
$0\leq n-1$ and therefore $1\leq n$. Hence, $n\geq1>0$, and thus
$g_{n}=\operatorname*{id}$ (as we know). Hence, $\underbrace{g_{n-0}}%
_{=g_{n}=\operatorname*{id}}\left(  n\right)  =\operatorname*{id}\left(
n\right)  =n=n-0$, qed.}. In other words, (\ref{pf.lem.laplace.gp.c.claim})
holds for $q=0$. Thus, the induction base is complete.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,n-1\right\}  $ be
positive. Assume that (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q-1$. We
need to show that (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q$.

We have $Q\leq n-1$ (since $Q\in\left\{  0,1,\ldots,n-1\right\}  $) and
$Q\geq1$ (since $Q$ is positive and belongs to $\left\{  0,1,\ldots
,n-1\right\}  $). Hence, $n-\underbrace{Q}_{\leq n-1}\geq n-\left(
n-1\right)  =1$. Also, $n-\underbrace{Q}_{\geq1}\leq n-1$. Combining
$n-Q\geq1$ with $n-Q\leq n-1$, we obtain $1\leq n-Q\leq n-1$ and thus
$n-Q\in\left[  n-1\right]  $. Hence, (\ref{pf.lem.laplace.gp.recgp}) (applied
to $p=n-Q$) shows that $g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$.

We have $n-Q\in\left[  n-1\right]  $ (since $1\leq n-Q\leq n-1$). Hence,
$s_{n-Q}\left(  n-Q+1\right)  =n-Q$ (by (\ref{pf.lem.laplace.gp.sp.2}),
applied to $p=n-Q$).

But we assumed that (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q-1$. In
other words, we have $g_{n-\left(  Q-1\right)  }\left(  n\right)  =n-\left(
Q-1\right)  $. Since $n-\left(  Q-1\right)  =n-Q+1$, this rewrites as
$g_{n-Q+1}\left(  n\right)  =n-Q+1$. Now,%
\begin{align*}
\underbrace{g_{n-Q}}_{=s_{n-Q}\circ g_{n-Q+1}}\left(  n\right)   &  =\left(
s_{n-Q}\circ g_{n-Q+1}\right)  \left(  n\right)  =s_{n-Q}\left(
\underbrace{g_{n-Q+1}\left(  n\right)  }_{=n-Q+1}\right) \\
&  =s_{n-Q}\left(  n-Q+1\right)  =n-Q.
\end{align*}
In other words, (\ref{pf.lem.laplace.gp.c.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.c.claim}) is complete.

Now, let $p\in\left[  n\right]  $. Then, $p\in\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $, so that $n-p\in\left\{  0,1,\ldots,n-1\right\}  $.
Thus, we can apply (\ref{pf.lem.laplace.gp.c.claim}) to $q=n-p$. We thus
obtain $g_{n-\left(  n-p\right)  }\left(  n\right)  =n-\left(  n-p\right)  $.
Since $n-\left(  n-p\right)  =p$, this rewrites as $g_{p}\left(  n\right)
=p$. This proves Lemma \ref{lem.laplace.gp} \textbf{(c)}.

\textbf{(d)} Let $p\in\left[  n\right]  $. We have $g_{p}\left(  i\right)  =i$
for every $i\in\left\{  1,2,\ldots,p-1\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,p-1\right\}  $.
Thus, $i\geq1$ and $i\leq p-1$. Combining $i\geq1$ with $i\leq p-1\leq p\leq
n$ (since $p\in\left[  n\right]  $), we obtain $1\leq i\leq n$, so that
$i\in\left[  n\right]  $. Also, $i\leq p-1<p$. Hence, Lemma
\ref{lem.laplace.gp} \textbf{(a)} shows that we have $g_{p}\left(  i\right)
=i$, qed.}. In other words,%
\[
\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
p-1\right)  \right)  =\left(  1,2,\ldots,p-1\right)  .
\]


Also, we have $g_{p}\left(  i\right)  =i+1$ for every $i\in\left\{
p,p+1,\ldots,n-1\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{
p,p+1,\ldots,n-1\right\}  $. Thus, $i\geq p$ and $i\leq n-1$. Combining $i\geq
p\geq1$ (since $p\in\left[  n\right]  $) with $i\leq n-1\leq n$, we obtain
$1\leq i\leq n$, so that $i\in\left[  n\right]  $. Also, $p\leq i$ (since
$i\geq p$) and $i\leq n-1<n$, so that $p\leq i<n$. Hence, Lemma
\ref{lem.laplace.gp} \textbf{(b)} shows that we have $g_{p}\left(  i\right)
=i+1$, qed.}. In other words,%
\begin{align*}
\left(  g_{p}\left(  p\right)  ,g_{p}\left(  p+1\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)   &  =\left(  p+1,\left(  p+1\right)  +1,\ldots,\left(
n-1\right)  +1\right) \\
&  =\left(  p+1,p+2,\ldots,n\right)  .
\end{align*}


Now,%
\begin{align*}
&  \left(  \text{the concatenation of the list }\underbrace{\left(
g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
p-1\right)  \right)  }_{=\left(  1,2,\ldots,p-1\right)  }\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{with the list }\underbrace{\left(
g_{p}\left(  p\right)  ,g_{p}\left(  p+1\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  }_{=\left(  p+1,p+2,\ldots,n\right)  }\right) \\
&  =\left(  \text{the concatenation of the list }\left(  1,2,\ldots
,p-1\right)  \text{ with the list }\left(  p+1,p+2,\ldots,n\right)  \right) \\
&  =\left(  1,2,\ldots,p-1,p+1,p+2,\ldots,n\right)  .
\end{align*}
Comparing this with%
\[
\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  1,2,\ldots
,p-1,p+1,p+2,\ldots,n\right)  ,
\]
we obtain
\begin{align*}
&  \left(  1,2,\ldots,\widehat{p},\ldots,n\right) \\
&  =\left(  \text{the concatenation of the list }\underbrace{\left(
g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
p-1\right)  \right)  }_{=\left(  1,2,\ldots,p-1\right)  }\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{with the list }\underbrace{\left(
g_{p}\left(  p\right)  ,g_{p}\left(  p+1\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  }_{=\left(  p+1,p+2,\ldots,n\right)  }\right) \\
&  =\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots
,g_{p}\left(  n-1\right)  \right)  .
\end{align*}
This proves Lemma \ref{lem.laplace.gp} \textbf{(d)}.

\textbf{(e)} Let us first show that%
\begin{equation}
\left(  -1\right)  ^{g_{n-q}}=\left(  -1\right)  ^{q}%
\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{  0,1,\ldots,n-1\right\}  .
\label{pf.lem.laplace.gp.e.claim}%
\end{equation}


\textit{Proof of (\ref{pf.lem.laplace.gp.e.claim}):} We shall prove
(\ref{pf.lem.laplace.gp.e.claim}) by induction over $q$:

\textit{Induction base:} If $0\in\left\{  0,1,\ldots,n-1\right\}  $, then
$\left(  -1\right)  ^{g_{n-0}}=\left(  -1\right)  ^{0}$%
\ \ \ \ \footnote{\textit{Proof.} Assume that $0\in\left\{  0,1,\ldots
,n-1\right\}  $. Thus, $0\leq0\leq n-1$, so that $0\leq n-1$ and therefore
$1\leq n$. Hence, $n\geq1>0$, and thus $g_{n}=\operatorname*{id}$ (as we
know). Hence, $\left(  -1\right)  ^{g_{n}}=\left(  -1\right)
^{\operatorname*{id}}=1$, so that $\left(  -1\right)  ^{g_{n-0}}=\left(
-1\right)  ^{g_{n}}=1=\left(  -1\right)  ^{0}$, qed.}. In other words,
(\ref{pf.lem.laplace.gp.e.claim}) holds for $q=0$. Thus, the induction base is complete.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,n-1\right\}  $ be
positive. Assume that (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q-1$. We
need to show that (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q$.

We have $Q\leq n-1$ (since $Q\in\left\{  0,1,\ldots,n-1\right\}  $) and
$Q\geq1$ (since $Q$ is positive and belongs to $\left\{  0,1,\ldots
,n-1\right\}  $). Hence, $n-\underbrace{Q}_{\leq n-1}\geq n-\left(
n-1\right)  =1$. Also, $n-\underbrace{Q}_{\geq1}\leq n-1$. Combining
$n-Q\geq1$ with $n-Q\leq n-1$, we obtain $1\leq n-Q\leq n-1$ and thus
$n-Q\in\left\{  1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $. Hence,
(\ref{pf.lem.laplace.gp.recgp}) (applied to $p=n-Q$) shows that $g_{n-Q}%
=s_{n-Q}\circ g_{n-Q+1}$.

Recall that $\left(  -1\right)  ^{s_{k}}=-1$ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. Applying this to $k=n-Q$, we obtain $\left(
-1\right)  ^{s_{n-Q}}=-1$.

But we assumed that (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q-1$. In
other words, we have $\left(  -1\right)  ^{g_{n-\left(  Q-1\right)  }}=\left(
-1\right)  ^{Q-1}$. Since $n-\left(  Q-1\right)  =n-Q+1$, this rewrites as
$\left(  -1\right)  ^{g_{n-Q+1}}=\left(  -1\right)  ^{Q-1}$. Now, from
$g_{n-Q}=s_{n-Q}\circ g_{n-Q+1}$, we obtain%
\begin{align*}
\left(  -1\right)  ^{g_{n-Q}}  &  =\left(  -1\right)  ^{s_{n-Q}\circ
g_{n-Q+1}}=\underbrace{\left(  -1\right)  ^{s_{n-Q}}}_{=-1}\cdot
\underbrace{\left(  -1\right)  ^{g_{n-Q+1}}}_{=\left(  -1\right)  ^{Q-1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\sigma=s_{n-Q}\text{ and }\tau=g_{n-Q+1}\right) \\
&  =\left(  -1\right)  \cdot\left(  -1\right)  ^{Q-1}=\left(  -1\right)
^{\left(  Q-1\right)  +1}=\left(  -1\right)  ^{Q}.
\end{align*}


In other words, (\ref{pf.lem.laplace.gp.e.claim}) holds for $q=Q$. This
completes the induction step. Thus, the induction proof of
(\ref{pf.lem.laplace.gp.e.claim}) is complete.

Now, let $p\in\left[  n\right]  $. Then, $p\in\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $, so that $n-p\in\left\{  0,1,\ldots,n-1\right\}  $.
Thus, we can apply (\ref{pf.lem.laplace.gp.e.claim}) to $q=n-p$. We thus
obtain $\left(  -1\right)  ^{g_{n-\left(  n-p\right)  }}=\left(  -1\right)
^{n-p}$. Since $n-\left(  n-p\right)  =p$, this rewrites as $\left(
-1\right)  ^{g_{p}}=\left(  -1\right)  ^{n-p}$. This proves Lemma
\ref{lem.laplace.gp} \textbf{(e)}.

\textbf{(f)} We have $g_{p}\in S_{n}$. In other words, $g_{p}$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of
all permutations of $\left\{  1,2,\ldots,n\right\}  $). In other words,
$g_{p}$ is a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Thus, the map $g_{p}$ is injective and surjective.

For every $i\in\left[  n-1\right]  $, we have $g_{p}\left(  i\right)
\in\left[  n\right]  \setminus\left\{  p\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left[  n-1\right]  $. Then,
$i\in\left[  n-1\right]  =\left\{  1,2,\ldots,n-1\right\}  $, so that $i\leq
n-1<n$ and thus $i\neq n$.
\par
Also, $i\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $. Hence, $g_{p}\left(  i\right)  $ is well-defined. Moreover,
$i\in\left\{  1,2,\ldots,n-1\right\}  $, so that $i\geq1$.
\par
The map $g_{p}$ is injective. Since $i\neq n$, we therefore have $g_{p}\left(
i\right)  \neq g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}).
\par
Combining $g_{p}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $ with $g_{p}\left(  i\right)  \neq p$, we obtain $g_{p}\left(
i\right)  \in\left[  n\right]  \setminus\left\{  p\right\}  $, qed.}. Thus,
the map $g_{p}^{\prime}$ is well-defined. It remains to show that this map
$g_{p}^{\prime}$ is bijective.

For every $j\in\left[  n\right]  \setminus\left\{  p\right\}  $, we have
$\left(  g_{p}\right)  ^{-1}\left(  j\right)  \in\left[  n-1\right]
$\ \ \ \ \footnote{\textit{Proof.} Let $j\in\left[  n\right]  \setminus
\left\{  p\right\}  $. Thus, $j\in\left[  n\right]  $ and $j\neq p$.
\par
Let $i=\left(  g_{p}\right)  ^{-1}\left(  j\right)  $. (This is clearly
well-defined, since $j\in\left[  n\right]  $.) Now, $g_{p}\left(  i\right)
=j$ (since $i=\left(  g_{p}\right)  ^{-1}\left(  j\right)  $). If we had
$i=n$, then we would have $g_{p}\left(  \underbrace{i}_{=n}\right)
=g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp} \textbf{(c)}),
which would contradict $g_{p}\left(  i\right)  =j\neq p$. Hence, we cannot
have $i=n$. We thus have $i\neq n$.
\par
But $i=\left(  g_{p}\right)  ^{-1}\left(  j\right)  \in\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $. Combining this with $i\neq n$, we obtain
$i\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{
1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $. Thus, $\left(  g_{p}\right)
^{-1}\left(  j\right)  =i\in\left[  n-1\right]  $, qed.}. Therefore, we can
define a map $h:\left[  n\right]  \setminus\left\{  p\right\}  \rightarrow
\left[  n-1\right]  $ by%
\[
\left(  h\left(  j\right)  =\left(  g_{p}\right)  ^{-1}\left(  j\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left[  n\right]  \setminus\left\{
p\right\}  \right)  .
\]
Consider this map $h$.

We have $h\circ g_{p}^{\prime}=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} For every $i\in\left[  n-1\right]  $, we
have%
\begin{align*}
\left(  h\circ g_{p}^{\prime}\right)  \left(  i\right)   &  =h\left(
\underbrace{g_{p}^{\prime}\left(  i\right)  }_{\substack{=g_{p}\left(
i\right)  \\\text{(by the definition of }g_{p}^{\prime}\text{)}}}\right)
=h\left(  g_{p}\left(  i\right)  \right)  =\left(  g_{p}\right)  ^{-1}\left(
g_{p}\left(  i\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }h\right) \\
&  =i=\operatorname*{id}\left(  i\right)  .
\end{align*}
Thus, $h\circ g_{p}^{\prime}=\operatorname*{id}$, qed.} and $g_{p}^{\prime
}\circ h=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} For every
$j\in\left[  n\right]  \setminus\left\{  p\right\}  $, we have%
\begin{align*}
\left(  g_{p}^{\prime}\circ h\right)  \left(  j\right)   &  =g_{p}^{\prime
}\left(  \underbrace{h\left(  j\right)  }_{\substack{=\left(  g_{p}\right)
^{-1}\left(  j\right)  \\\text{(by the definition of }h\text{)}}}\right) \\
&  =g_{p}^{\prime}\left(  \left(  g_{p}\right)  ^{-1}\left(  j\right)
\right)  =g_{p}\left(  \left(  g_{p}\right)  ^{-1}\left(  j\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }g_{p}^{\prime}\right)
\\
&  =j=\operatorname*{id}\left(  j\right)  .
\end{align*}
Thus, $g_{p}^{\prime}\circ h=\operatorname*{id}$, qed.}. Hence, the maps $h$
and $g_{p}^{\prime}$ are mutually inverse. Thus, the map $g_{p}^{\prime}$ is
invertible. In other words, the map $g_{p}^{\prime}$ is bijective. This
completes the proof of Lemma \ref{lem.laplace.gp} \textbf{(f)}.

\textbf{(g)} For every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $, we have%
\[
g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\]
\footnote{\textit{Proof.} Let $\sigma\in\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  n\right)  =n\right\}  $. Thus, $\sigma$ is an element $\tau$ of
$S_{n}$ satisfying $\tau\left(  n\right)  =n$. In other words, $\sigma$ is an
element of $S_{n}$ and satisfies $\sigma\left(  n\right)  =n$.
\par
We have $g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}). Thus, $\left(  g_{p}\right)  ^{-1}\left(  p\right)  =n$.
Moreover, $g_{q}\left(  n\right)  =q$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}, applied to $q$ instead of $p$). Now,%
\[
\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\right)  \left(
p\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  p\right)  }_{=n}\right)  \right)  =g_{q}\left(
\underbrace{\sigma\left(  n\right)  }_{=n}\right)  =g_{q}\left(  n\right)
=q.
\]
\par
Now, we know that $g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in S_{n}$
(since $g_{q}$, $\sigma$ and $\left(  g_{p}\right)  ^{-1}$ all belong to
$S_{n}$) and satisfies $\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)
^{-1}\right)  \left(  p\right)  =q$. In other words, $g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}$ is an element $\tau$ of $S_{n}$ satisfying
$\tau\left(  p\right)  =q$. In other words,%
\[
g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  ,
\]
qed.}. Thus, the map $T$ is well-defined.

Furthermore, for every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $, we have
\[
\left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\]
\footnote{\textit{Proof.} Let $\sigma\in\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  p\right)  =q\right\}  $. Thus, $\sigma$ is an element $\tau$ of
$S_{n}$ satisfying $\tau\left(  p\right)  =q$. In other words, $\sigma$ is an
element of $S_{n}$ and satisfies $\sigma\left(  p\right)  =q$.
\par
We have $g_{p}\left(  n\right)  =p$ (by Lemma \ref{lem.laplace.gp}
\textbf{(c)}). Moreover, $g_{q}\left(  n\right)  =q$ (by Lemma
\ref{lem.laplace.gp} \textbf{(c)}, applied to $q$ instead of $p$), and thus
$\left(  g_{q}\right)  ^{-1}\left(  q\right)  =n$. Now,%
\[
\left(  \left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\right)  \left(
n\right)  =\left(  g_{q}\right)  ^{-1}\left(  \sigma\left(  \underbrace{g_{p}%
\left(  n\right)  }_{=p}\right)  \right)  =\left(  g_{q}\right)  ^{-1}\left(
\underbrace{\sigma\left(  p\right)  }_{=q}\right)  =\left(  g_{q}\right)
^{-1}\left(  q\right)  =n.
\]
\par
Now, we know that $\left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\in S_{n}$
(since $\left(  g_{q}\right)  ^{-1}$, $\sigma$ and $g_{p}$ all belong to
$S_{n}$) and satisfies $\left(  \left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\right)  \left(  n\right)  =n$. In other words, $\left(  g_{q}\right)
^{-1}\circ\sigma\circ g_{p}$ is an element $\tau$ of $S_{n}$ satisfying
$\tau\left(  p\right)  =n$. In other words,%
\[
\left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  ,
\]
qed.}. Hence, we can define a map
\[
Q:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}
\]
by%
\[
\left(  Q\left(  \sigma\right)  =\left(  g_{q}\right)  ^{-1}\circ\sigma\circ
g_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  \right)  .
\]
Consider this map $Q$.

We have $T\circ Q=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Every
$\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  $
satisfies%
\begin{align*}
\left(  T\circ Q\right)  \left(  \sigma\right)   &  =T\left(
\underbrace{Q\left(  \sigma\right)  }_{\substack{=\left(  g_{q}\right)
^{-1}\circ\sigma\circ g_{p}\\\text{(by the definition of }Q\text{)}}}\right)
=T\left(  \left(  g_{q}\right)  ^{-1}\circ\sigma\circ g_{p}\right) \\
&  =\underbrace{g_{q}\circ\left(  g_{q}\right)  ^{-1}}_{=\operatorname*{id}%
}\circ\sigma\circ\underbrace{g_{p}\circ\left(  g_{p}\right)  ^{-1}%
}_{=\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}T\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
In other words, $T\circ Q=\operatorname*{id}$, qed.} and $Q\circ
T=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Every $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $ satisfies%
\begin{align*}
\left(  Q\circ T\right)  \left(  \sigma\right)   &  =Q\left(
\underbrace{T\left(  \sigma\right)  }_{\substack{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}\\\text{(by the definition of }T\text{)}}}\right)
=Q\left(  g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}\right) \\
&  =\underbrace{\left(  g_{q}\right)  ^{-1}\circ\left(  g_{q}\right)
}_{=\operatorname*{id}}\circ\sigma\circ\underbrace{\left(  g_{p}\right)
^{-1}\circ g_{p}}_{=\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }Q\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
In other words, $Q\circ T=\operatorname*{id}$, qed.}. Hence, the maps $T$ and
$Q$ are mutually inverse. Thus, the map $T$ is invertible. In other words, the
map $T$ is bijective. This completes the proof of Lemma \ref{lem.laplace.gp}
\textbf{(g)}.
\end{proof}
\end{verlong}

Our next step towards the proof of Theorem \ref{thm.laplace.gen} is the
following lemma:

\begin{lemma}
\label{lem.laplace.Apq}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $p\in\left\{
1,2,\ldots,n\right\}  $ and $q\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}}\left(
-1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq p}}a_{i,\sigma\left(  i\right)  }=\left(  -1\right)  ^{p+q}%
\det\left(  A_{\sim p,\sim q}\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.laplace.Apq}.]Let us use all notations introduced in
Lemma \ref{lem.laplace.gpshort}.

We have $p\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence,
$g_{p}$ is well-defined. Similarly, $g_{q}$ is well-defined. We have%
\begin{equation}
\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p},\ldots,n\right)
\label{pf.lem.laplace.Apq.short.indices1}%
\end{equation}
(by Lemma \ref{lem.laplace.gpshort} \textbf{(a)}) and
\begin{equation}
\left(  g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{q},\ldots,n\right)
\label{pf.lem.laplace.Apq.short.indices2}%
\end{equation}
(by Lemma \ref{lem.laplace.gpshort} \textbf{(a)}, applied to $q$ instead of
$p$). Now, the definition of $A_{\sim p,\sim q}$ yields%
\begin{align}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  }^{g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots
,g_{q}\left(  n-1\right)  }A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.laplace.Apq.short.indices1}) and
(\ref{pf.lem.laplace.Apq.short.indices2})}\right) \nonumber\\
&  =\left(  a_{g_{p}\left(  x\right)  ,g_{q}\left(  y\right)  }\right)
_{1\leq x\leq n-1,\ 1\leq y\leq n-1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  }^{g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots
,g_{q}\left(  n-1\right)  }A\right) \nonumber\\
&  =\left(  a_{g_{p}\left(  i\right)  ,g_{q}\left(  j\right)  }\right)
_{1\leq i\leq n-1,\ 1\leq j\leq n-1}\label{pf.lem.laplace.Apq.short.A}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right)  .\nonumber
\end{align}


Also, $\left[  n\right]  $ is nonempty (since $p\in\left[  n\right]  $), and
thus we have $n>0$.

Now, let us recall the map $T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $ defined in Lemma \ref{lem.laplace.gpshort}
\textbf{(d)}. Lemma \ref{lem.laplace.gpshort} \textbf{(d)} says that this map
$T$ is well-defined and bijective. Every $\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $ satisfies%
\begin{equation}
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\left(  -1\right)  ^{p+q}%
\cdot\left(  -1\right)  ^{\sigma} \label{pf.lem.laplace.Apq.short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.short.1}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Applying
Lemma \ref{lem.laplace.gpshort} \textbf{(b)} to $q$ instead of $p$, we obtain
$\left(  -1\right)  ^{g_{q}}=\left(  -1\right)  ^{n-q}=\left(  -1\right)
^{n+q}$ (since $n-q\equiv n+q\operatorname{mod}2$).
\par
The definition of $T\left(  \sigma\right)  $ yields $T\left(  \sigma\right)
=g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}$. Thus,%
\[
\underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\circ g_{p}=g_{q}\circ\sigma\circ\underbrace{\left(
g_{p}\right)  ^{-1}\circ g_{p}}_{=\operatorname*{id}}=g_{q}\circ\sigma,
\]
so that%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{g_{q}\circ\sigma}=\underbrace{\left(  -1\right)  ^{g_{q}}%
}_{=\left(  -1\right)  ^{n+q}}\cdot\left(  -1\right)  ^{\sigma}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
g_{q}\text{ and }\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{T\left(  \sigma\right)  }\cdot\underbrace{\left(  -1\right)
^{g_{p}}}_{\substack{=\left(  -1\right)  ^{n-p}\\\text{(by Lemma
\ref{lem.laplace.gpshort} \textbf{(b)})}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sign.prod}), applied to }T\left(  \sigma\right)  \text{ and }%
g_{p}\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p},
\end{align*}
this yields
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p}=\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
We can divide both sides of this equality by $\left(  -1\right)  ^{n-p}$
(since $\left(  -1\right)  ^{n-p}\in\left\{  1,-1\right\}  $ is clearly an
invertible integer), and thus we obtain%
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\dfrac{\left(  -1\right)
^{n+q}\cdot\left(  -1\right)  ^{\sigma}}{\left(  -1\right)  ^{n-p}%
}=\underbrace{\dfrac{\left(  -1\right)  ^{n+q}}{\left(  -1\right)  ^{n-p}}%
}_{\substack{=\left(  -1\right)  ^{\left(  n+q\right)  -\left(  n-p\right)
}=\left(  -1\right)  ^{p+q}\\\text{(since }\left(  n+q\right)  -\left(
n-p\right)  =p+q\text{)}}}\cdot\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{p+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
This proves (\ref{pf.lem.laplace.Apq.short.1}).} and%
\begin{equation}
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }
\label{pf.lem.laplace.Apq.short.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.short.2}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Let us
recall the map $g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[
n\right]  \setminus\left\{  p\right\}  $ introduced in Lemma
\ref{lem.laplace.gpshort} \textbf{(f)}. Lemma \ref{lem.laplace.gpshort}
\textbf{(c)} says that this map $g_{p}^{\prime}$ is well-defined and
bijective. In other words, $g_{p}^{\prime}$ is a bijection.
\par
Let $i\in\left[  n-1\right]  $. Then, $g_{p}^{\prime}\left(  i\right)
=g_{p}\left(  i\right)  $ (by the definition of $g_{p}^{\prime}$). Also, the
definition of $T$ yields $T\left(  \sigma\right)  =g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}$, so that%
\[
\left(  \underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\right)  \left(  \underbrace{g_{p}^{\prime}\left(
i\right)  }_{=g_{p}\left(  i\right)  }\right)  =\left(  g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}\right)  \left(  g_{p}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  g_{p}\left(  i\right)  \right)  }_{=i}\right)  \right)
=g_{q}\left(  \sigma\left(  i\right)  \right)  .
\]
\par
From $g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)  $ and $\left(
T\left(  \sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  i\right)  \right)  $, we obtain%
\begin{equation}
a_{g_{p}^{\prime}\left(  i\right)  ,\left(  T\left(  \sigma\right)  \right)
\left(  g_{p}^{\prime}\left(  i\right)  \right)  }=a_{g_{p}\left(  i\right)
,g_{q}\left(  \sigma\left(  i\right)  \right)  }.
\label{pf.lem.laplace.Apq.short.2.pf.1}%
\end{equation}
\par
Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.Apq.short.2.pf.1}) for every $i\in\left[  n-1\right]  $.
But now, we have%
\begin{align*}
&  \underbrace{\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}}_{\substack{=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
p}}\\\text{(since }\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}%
}}a_{i,\left(  T\left(  \sigma\right)  \right)  \left(  i\right)  }\\
&  =\underbrace{\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}}%
_{=\prod_{i\in\left[  n\right]  \setminus\left\{  p\right\}  }}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[
n\right]  \setminus\left\{  p\right\}  }a_{i,\left(  T\left(  \sigma\right)
\right)  \left(  i\right)  }=\underbrace{\prod_{i\in\left[  n-1\right]  }%
}_{=\prod_{i=1}^{n-1}}\underbrace{a_{g_{p}^{\prime}\left(  i\right)  ,\left(
T\left(  \sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)
\right)  }}_{\substack{=a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(
i\right)  \right)  }\\\text{(by (\ref{pf.lem.laplace.Apq.short.2.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }g_{p}^{\prime}\left(  i\right)  \text{ for
}i\text{, since}\\
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(
i\right)  \right)  }.
\end{align*}
This proves (\ref{pf.lem.laplace.Apq.short.2}).}.

Now,
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}%
}}_{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }}_{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
n\right)  =n}}}\underbrace{\left(  -1\right)  ^{T\left(  \sigma\right)  }%
}_{\substack{=\left(  -1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma
}\\\text{(by (\ref{pf.lem.laplace.Apq.short.1}))}}}\underbrace{\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }\\\text{(by (\ref{pf.lem.laplace.Apq.short.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }T\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\} \\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }\\
&  =\left(  -1\right)  ^{p+q}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  =n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }}_{\substack{=\det\left(  \left(  a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)
\\\text{(by Lemma \ref{lem.laplace.lem}, applied to }a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\text{ instead of }a_{i,j}\text{)}}}\\
&  =\left(  -1\right)  ^{p+q}\det\left(  \underbrace{\left(  a_{g_{p}\left(
i\right)  ,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}}_{\substack{=A_{\sim p,\sim q}\\\text{(by
(\ref{pf.lem.laplace.Apq.short.A}))}}}\right)  =\left(  -1\right)  ^{p+q}%
\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.laplace.Apq}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.laplace.Apq}.]Let us use all notations introduced in
Lemma \ref{lem.laplace.gp}.

We have $p\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence,
$g_{p}$ is well-defined. Similarly, $g_{q}$ is well-defined. We have%
\[
\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{p},\ldots,n\right)
\]
(by Lemma \ref{lem.laplace.gp} \textbf{(d)}) and
\[
\left(  g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(
n-1\right)  \right)  =\left(  1,2,\ldots,\widehat{q},\ldots,n\right)
\]
(by Lemma \ref{lem.laplace.gp} \textbf{(d)}, applied to $q$ instead of $p$).
Now, the definition of $A_{\sim p,\sim q}$ yields%
\begin{align}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{g_{q}\left(  1\right)
,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(  n-1\right)  }A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  =\left(  g_{q}\left(  1\right)  ,g_{q}\left(  2\right)
,\ldots,g_{q}\left(  n-1\right)  \right)  \right) \nonumber\\
&  =\operatorname*{sub}\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(
2\right)  ,\ldots,g_{p}\left(  n-1\right)  }^{g_{q}\left(  1\right)
,g_{q}\left(  2\right)  ,\ldots,g_{q}\left(  n-1\right)  }A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  =\left(  g_{p}\left(  1\right)  ,g_{p}\left(  2\right)
,\ldots,g_{p}\left(  n-1\right)  \right)  \right) \nonumber\\
&  =\left(  a_{g_{p}\left(  x\right)  ,g_{q}\left(  y\right)  }\right)
_{1\leq x\leq n-1,\ 1\leq y\leq n-1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{g_{p}\left(  1\right)  ,g_{p}\left(  2\right)  ,\ldots,g_{p}\left(
n-1\right)  }^{g_{q}\left(  1\right)  ,g_{q}\left(  2\right)  ,\ldots
,g_{q}\left(  n-1\right)  }A\right) \nonumber\\
&  =\left(  a_{g_{p}\left(  i\right)  ,g_{q}\left(  j\right)  }\right)
_{1\leq i\leq n-1,\ 1\leq j\leq n-1}\label{pf.lem.laplace.Apq.A}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right)  .\nonumber
\end{align}


Also, $\left[  n\right]  $ is nonempty (since $p\in\left[  n\right]  $), and
thus we have $n>0$.

Now, let us recall the map $T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $ defined in Lemma \ref{lem.laplace.gp} \textbf{(g)}.
Lemma \ref{lem.laplace.gp} \textbf{(g)} says that this map $T$ is well-defined
and bijective. Every $\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $ satisfies%
\begin{equation}
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\left(  -1\right)  ^{p+q}%
\cdot\left(  -1\right)  ^{\sigma} \label{pf.lem.laplace.Apq.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.1}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Applying
Lemma \ref{lem.laplace.gp} \textbf{(e)} to $q$ instead of $p$, we obtain%
\[
\left(  -1\right)  ^{g_{q}}=\left(  -1\right)  ^{n-q}=\left(  -1\right)
^{n+q}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n-q\equiv n+q\operatorname{mod}%
2\right)  .
\]
\par
The definition of $T\left(  \sigma\right)  $ yields $T\left(  \sigma\right)
=g_{q}\circ\sigma\circ\left(  g_{p}\right)  ^{-1}$. Thus,%
\[
\underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\circ g_{p}=g_{q}\circ\sigma\circ\underbrace{\left(
g_{p}\right)  ^{-1}\circ g_{p}}_{=\operatorname*{id}}=g_{q}\circ\sigma,
\]
so that%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{g_{q}\circ\sigma}=\underbrace{\left(  -1\right)  ^{g_{q}}%
}_{=\left(  -1\right)  ^{n+q}}\cdot\left(  -1\right)  ^{\sigma}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
g_{q}\text{ and }\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{T\left(  \sigma\right)  \circ g_{p}}  &  =\left(
-1\right)  ^{T\left(  \sigma\right)  }\cdot\underbrace{\left(  -1\right)
^{g_{p}}}_{\substack{=\left(  -1\right)  ^{n-p}\\\text{(by Lemma
\ref{lem.laplace.gp} \textbf{(e)})}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.sign.prod}), applied to }T\left(  \sigma\right)  \text{ and }%
g_{p}\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p},
\end{align*}
this yields
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }\cdot\left(  -1\right)
^{n-p}=\left(  -1\right)  ^{n+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
We can divide both sides of this equality by $\left(  -1\right)  ^{n-p}$
(since $\left(  -1\right)  ^{n-p}\in\left\{  1,-1\right\}  $ is clearly an
invertible integer), and thus we obtain%
\[
\left(  -1\right)  ^{T\left(  \sigma\right)  }=\dfrac{\left(  -1\right)
^{n+q}\cdot\left(  -1\right)  ^{\sigma}}{\left(  -1\right)  ^{n-p}%
}=\underbrace{\dfrac{\left(  -1\right)  ^{n+q}}{\left(  -1\right)  ^{n-p}}%
}_{\substack{=\left(  -1\right)  ^{\left(  n+q\right)  -\left(  n-p\right)
}=\left(  -1\right)  ^{p+q}\\\text{(since }\left(  n+q\right)  -\left(
n-p\right)  =p+q\text{)}}}\cdot\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{p+q}\cdot\left(  -1\right)  ^{\sigma}.
\]
This proves (\ref{pf.lem.laplace.Apq.1}).} and%
\begin{equation}
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }
\label{pf.lem.laplace.Apq.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.laplace.Apq.2}):} Let $\sigma
\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $. Let us
recall the map $g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[
n\right]  \setminus\left\{  p\right\}  $ introduced in Lemma
\ref{lem.laplace.gp} \textbf{(f)}. Lemma \ref{lem.laplace.gp} \textbf{(f)}
says that this map $g_{p}^{\prime}$ is well-defined and bijective. In other
words, $g_{p}^{\prime}$ is a bijection.
\par
Let $i\in\left[  n-1\right]  $. Then, $g_{p}^{\prime}\left(  i\right)
=g_{p}\left(  i\right)  $ (by the definition of $g_{p}^{\prime}$). Also, the
definition of $T$ yields $T\left(  \sigma\right)  =g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}$, so that%
\[
\left(  \underbrace{T\left(  \sigma\right)  }_{=g_{q}\circ\sigma\circ\left(
g_{p}\right)  ^{-1}}\right)  \left(  \underbrace{g_{p}^{\prime}\left(
i\right)  }_{=g_{p}\left(  i\right)  }\right)  =\left(  g_{q}\circ\sigma
\circ\left(  g_{p}\right)  ^{-1}\right)  \left(  g_{p}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  \underbrace{\left(  g_{p}\right)
^{-1}\left(  g_{p}\left(  i\right)  \right)  }_{=i}\right)  \right)
=g_{q}\left(  \sigma\left(  i\right)  \right)  .
\]
\par
From $g_{p}^{\prime}\left(  i\right)  =g_{p}\left(  i\right)  $ and $\left(
T\left(  \sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)
\right)  =g_{q}\left(  \sigma\left(  i\right)  \right)  $, we obtain%
\begin{equation}
a_{g_{p}^{\prime}\left(  i\right)  ,\left(  T\left(  \sigma\right)  \right)
\left(  g_{p}^{\prime}\left(  i\right)  \right)  }=a_{g_{p}\left(  i\right)
,g_{q}\left(  \sigma\left(  i\right)  \right)  }.
\label{pf.lem.laplace.Apq.2.pf.1}%
\end{equation}
\par
Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.lem.laplace.Apq.2.pf.1}) for every $i\in\left[  n-1\right]  $. But
now, we have%
\begin{align*}
&  \underbrace{\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}}_{\substack{=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
p}}\\\text{(since }\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}%
}}a_{i,\left(  T\left(  \sigma\right)  \right)  \left(  i\right)  }\\
&  =\underbrace{\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}}%
_{=\prod_{i\in\left[  n\right]  \setminus\left\{  p\right\}  }}a_{i,\left(
T\left(  \sigma\right)  \right)  \left(  i\right)  }=\prod_{i\in\left[
n\right]  \setminus\left\{  p\right\}  }a_{i,\left(  T\left(  \sigma\right)
\right)  \left(  i\right)  }=\underbrace{\prod_{i\in\left[  n-1\right]  }%
}_{\substack{=\prod_{i\in\left\{  1,2,\ldots,n-1\right\}  }\\\text{(since
}\left[  n-1\right]  =\left\{  1,2,\ldots,n-1\right\}  \text{)}}%
}\underbrace{a_{g_{p}^{\prime}\left(  i\right)  ,\left(  T\left(
\sigma\right)  \right)  \left(  g_{p}^{\prime}\left(  i\right)  \right)  }%
}_{\substack{=a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }\\\text{(by (\ref{pf.lem.laplace.Apq.2.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }g_{p}^{\prime}\left(  i\right)  \text{ for
}i\text{, since}\\
g_{p}^{\prime}:\left[  n-1\right]  \rightarrow\left[  n\right]  \setminus
\left\{  p\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\underbrace{\prod_{i\in\left\{  1,2,\ldots,n-1\right\}  }}_{=\prod
_{i=1}^{n-1}}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }=\prod_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(
\sigma\left(  i\right)  \right)  }.
\end{align*}
This proves (\ref{pf.lem.laplace.Apq.2}).}.

Now,
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}%
}}_{=\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  }\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  }}_{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
n\right)  =n}}}\underbrace{\left(  -1\right)  ^{T\left(  \sigma\right)  }%
}_{\substack{=\left(  -1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma
}\\\text{(by (\ref{pf.lem.laplace.Apq.1}))}}}\underbrace{\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\left(  T\left(
\sigma\right)  \right)  \left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)
}\\\text{(by (\ref{pf.lem.laplace.Apq.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }T\left(  \sigma\right)  \text{ for }%
\sigma\text{ in the sum,}\\
\text{since the map }T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}
\end{array}
\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  n\right)  =n}}\left(
-1\right)  ^{p+q}\cdot\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)  \right)  }\\
&  =\left(  -1\right)  ^{p+q}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  n\right)  =n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{g_{p}\left(  i\right)  ,g_{q}\left(  \sigma\left(  i\right)
\right)  }}_{\substack{=\det\left(  \left(  a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right)
\\\text{(by Lemma \ref{lem.laplace.lem}, applied to }a_{g_{p}\left(  i\right)
,g_{q}\left(  j\right)  }\text{ instead of }a_{i,j}\text{)}}}\\
&  =\left(  -1\right)  ^{p+q}\det\left(  \underbrace{\left(  a_{g_{p}\left(
i\right)  ,g_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
n-1}}_{\substack{=A_{\sim p,\sim q}\\\text{(by (\ref{pf.lem.laplace.Apq.A}))}%
}}\right)  =\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.laplace.Apq}.
\end{proof}
\end{verlong}

Now, we can finally prove Theorem \ref{thm.laplace.gen}:

\begin{proof}
[Proof of Theorem \ref{thm.laplace.gen}.]\textbf{(a)} Let $p\in\left\{
1,2,\ldots,n\right\}  $. From (\ref{eq.det.eq.2}), we obtain%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{q\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because for every }\sigma\in S_{n}\text{, there exists}\\
\text{exactly one }q\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying
}\sigma\left(  p\right)  =q
\end{array}
\right) \\
&  =\sum_{q\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{i,\sigma\left(
i\right)  }}_{\substack{=a_{p,\sigma\left(  p\right)  }\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)
}\\\text{(here, we have split off the factor for }i=p\text{ from the
product)}}}\\
&  =\underbrace{\sum_{q\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{q=1}^{n}%
}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}}\left(
-1\right)  ^{\sigma}\underbrace{a_{p,\sigma\left(  p\right)  }}%
_{\substack{=a_{p,q}\\\text{(since }\sigma\left(  p\right)  =q\text{)}}%
}\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}%
}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{q=1}^{n}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
p\right)  =q}}\left(  -1\right)  ^{\sigma}a_{p,q}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}%
_{=a_{p,q}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}\\
&  =\sum_{q=1}^{n}a_{p,q}\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(
i\right)  }}_{\substack{=\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  \\\text{(by Lemma \ref{lem.laplace.Apq})}}}\\
&  =\sum_{q=1}^{n}\underbrace{a_{p,q}\left(  -1\right)  ^{p+q}}_{=\left(
-1\right)  ^{p+q}a_{p,q}}\det\left(  A_{\sim p,\sim q}\right)  =\sum_{q=1}%
^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Theorem \ref{thm.laplace.gen} \textbf{(a)}.

\textbf{(b)} Let $q\in\left\{  1,2,\ldots,n\right\}  $. From
(\ref{eq.det.eq.2}), we obtain%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma^{-1}\left(  q\right)  =p}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because for every }\sigma\in S_{n}\text{, there exists}\\
\text{exactly one }p\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying
}\sigma^{-1}\left(  q\right)  =p
\end{array}
\right) \\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\underbrace{\sum
_{\substack{\sigma\in S_{n};\\\sigma^{-1}\left(  q\right)  =p}}}%
_{\substack{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\\\text{(because for any }\sigma\in S_{n}\text{,}\\\text{the statement
}\left(  \sigma^{-1}\left(  q\right)  =p\right)  \\\text{is equivalent to
the}\\\text{statement }\left(  \sigma\left(  p\right)  =q\right)  \text{)}%
}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }a_{i,\sigma\left(  i\right)  }}_{\substack{=a_{p,\sigma\left(
p\right)  }\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}a_{i,\sigma\left(  i\right)  }\\\text{(here, we have split off
the}\\\text{factor for }i=p\text{ from the product)}}}\\
&  =\underbrace{\sum_{p\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{p=1}^{n}%
}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}}\left(
-1\right)  ^{\sigma}\underbrace{a_{p,\sigma\left(  p\right)  }}%
_{\substack{=a_{p,q}\\\text{(since }\sigma\left(  p\right)  =q\text{)}}%
}\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}%
}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{p=1}^{n}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
p\right)  =q}}\left(  -1\right)  ^{\sigma}a_{p,q}\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}%
_{=a_{p,q}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}\\
&  =\sum_{p=1}^{n}a_{p,q}\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(
i\right)  }}_{\substack{=\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  \\\text{(by Lemma \ref{lem.laplace.Apq})}}}\\
&  =\sum_{p=1}^{n}\underbrace{a_{p,q}\left(  -1\right)  ^{p+q}}_{=\left(
-1\right)  ^{p+q}a_{p,q}}\det\left(  A_{\sim p,\sim q}\right)  =\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim q}\right)  .
\end{align*}
This proves Theorem \ref{thm.laplace.gen} \textbf{(b)}.
\end{proof}

Let me make three simple observations (which can easily be checked by the reader):

\begin{itemize}
\item Theorem \ref{thm.laplace.gen} \textbf{(b)} could be (alternatively)
proven using Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to $A^{T}$
and $a_{q,p}$ instead of $A$ and $a_{p,q}$) and Exercise \ref{exe.ps4.4}.

\item Theorem \ref{thm.laplace.pre} is a particular case of Theorem
\ref{thm.laplace.gen} \textbf{(a)}.

\item Corollary \ref{cor.laplace.pre.col} is a particular case of Theorem
\ref{thm.laplace.gen} \textbf{(b)}.
\end{itemize}

\begin{remark}
Some books use Laplace expansion to define the notion of a determinant. For
example, one can define the determinant of a square matrix recursively, by
setting the determinant of the $0\times0$-matrix to be $1$, and defining the
determinant of an $n\times n$-matrix $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (with $n>0$) to be $\sum_{q=1}^{n}\left(  -1\right)
^{1+q}a_{1,q}\det\left(  A_{\sim1,\sim q}\right)  $ (assuming that
determinants of $\left(  n-1\right)  \times\left(  n-1\right)  $-matrices such
as $A_{\sim1,\sim q}$ are already defined). Of course, this leads to the same
notion of determinant as the one we are using, because of Theorem
\ref{thm.laplace.gen} \textbf{(a)}.
\end{remark}

\subsection{\label{sect.tridiag}Tridiagonal determinants}

In this section, we shall study the so-called \textit{tridiagonal matrices}: a
class of matrices whose all entries are zero everywhere except in the
\textquotedblleft direct proximity\textquotedblright\ of the diagonal (more
specifically: on the diagonal and \textquotedblleft one level below and one
level above\textquotedblright). We shall find recursive formulas for the
determinants of these matrices. These formulas are a simple example of an
application of Laplace expansion, but also interesting in their own right.

\begin{definition}
\label{def.tridiag}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$
elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n-1}$ be $n-1$ elements
of $\mathbb{K}$ (where we take the position that \textquotedblleft$-1$
elements of $\mathbb{K}$\textquotedblright\ means \textquotedblleft no
elements of $\mathbb{K}$\textquotedblright). Let $c_{1},c_{2},\ldots,c_{n-1}$
be $n-1$ elements of $\mathbb{K}$. We now set%
\[
A=\left(
\begin{array}
[c]{ccccccc}%
a_{1} & b_{1} & 0 & \cdots & 0 & 0 & 0\\
c_{1} & a_{2} & b_{2} & \cdots & 0 & 0 & 0\\
0 & c_{2} & a_{3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{n-2} & b_{n-2} & 0\\
0 & 0 & 0 & \cdots & c_{n-2} & a_{n-1} & b_{n-1}\\
0 & 0 & 0 & \cdots & 0 & c_{n-1} & a_{n}%
\end{array}
\right)  .
\]
(More formally,%
\[
A=\left(
\begin{cases}
a_{i}, & \text{if }i=j;\\
b_{i}, & \text{if }i=j-1;\\
c_{j}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
)

The matrix $A$ is called a \textit{tridiagonal matrix}.

We shall keep the notations $n$, $a_{1},a_{2},\ldots,a_{n}$, $b_{1}%
,b_{2},\ldots,b_{n-1}$, $c_{1},c_{2},\ldots,c_{n-1}$ and $A$ fixed for the
rest of Section \ref{sect.tridiag}.
\end{definition}

Playing around with small examples, one soon notices that the determinants of
tridiagonal matrices are too complicated to have neat explicit formulas in
full generality. For $n\in\left\{  0,1,2,3\right\}  $, the determinants look
as follows:%
\begin{align*}
\det A  &  =\det\left(  \text{the }0\times0\text{-matrix}\right)
=1\ \ \ \ \ \ \ \ \ \ \text{if }n=0;\\
\det A  &  =\det\left(
\begin{array}
[c]{c}%
a_{1}%
\end{array}
\right)  =a_{1}\ \ \ \ \ \ \ \ \ \ \text{if }n=1;\\
\det A  &  =\det\left(
\begin{array}
[c]{cc}%
a_{1} & b_{1}\\
c_{1} & a_{2}%
\end{array}
\right)  =a_{1}a_{2}-b_{1}c_{1}\ \ \ \ \ \ \ \ \ \ \text{if }n=2;\\
\det A  &  =\det\left(
\begin{array}
[c]{ccc}%
a_{1} & b_{1} & 0\\
c_{1} & a_{2} & b_{2}\\
0 & c_{2} & a_{3}%
\end{array}
\right)  =a_{1}a_{2}a_{3}-a_{1}b_{2}c_{2}-a_{3}b_{1}c_{1}%
\ \ \ \ \ \ \ \ \ \ \text{if }n=3.
\end{align*}
(And these formulas get more complicated the larger $n$ becomes.) However, the
many zeroes present in a tridiagonal matrix make it easy to find a recursive
formula for its determinant using Laplace expansion:

\begin{proposition}
\label{prop.tridiag.rec}For every two elements $x$ and $y$ of $\left\{
0,1,\ldots,n\right\}  $ satisfying $x\leq y$, we let $A_{x,y}$ be the $\left(
y-x\right)  \times\left(  y-x\right)  $-matrix%
\[
\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-2} & b_{y-2} & 0\\
0 & 0 & 0 & \cdots & c_{y-2} & a_{y-1} & b_{y-1}\\
0 & 0 & 0 & \cdots & 0 & c_{y-1} & a_{y}%
\end{array}
\right)  =\operatorname*{sub}\nolimits_{x+1,x+2,\ldots,y}^{x+1,x+2,\ldots
,y}A.
\]


\textbf{(a)} We have $\det\left(  A_{x,x}\right)  =1$ for every $x\in\left\{
0,1,\ldots,n\right\}  $.

\textbf{(b)} We have $\det\left(  A_{x,x+1}\right)  =a_{x+1}$ for every
$x\in\left\{  0,1,\ldots,n-1\right\}  $.

\textbf{(c)} For every $x\in\left\{  0,1,\ldots,n\right\}  $ and $y\in\left\{
0,1,\ldots,n\right\}  $ satisfying $x\leq y-2$, we have
\[
\det\left(  A_{x,y}\right)  =a_{y}\det\left(  A_{x,y-1}\right)  -b_{y-1}%
c_{y-1}\det\left(  A_{x,y-2}\right)  .
\]


\textbf{(d)} For every $x\in\left\{  0,1,\ldots,n\right\}  $ and $y\in\left\{
0,1,\ldots,n\right\}  $ satisfying $x\leq y-2$, we have
\[
\det\left(  A_{x,y}\right)  =a_{x+1}\det\left(  A_{x+1,y}\right)
-b_{x+1}c_{x+1}\det\left(  A_{x+2,y}\right)  .
\]


\textbf{(e)} We have $A=A_{0,n}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.tridiag.rec}.]\textbf{(e)} The definition of
$A_{0,n}$ yields
\[
A_{0,n}=\left(
\begin{array}
[c]{ccccccc}%
a_{1} & b_{1} & 0 & \cdots & 0 & 0 & 0\\
c_{1} & a_{2} & b_{2} & \cdots & 0 & 0 & 0\\
0 & c_{2} & a_{3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{n-2} & b_{n-2} & 0\\
0 & 0 & 0 & \cdots & c_{n-2} & a_{n-1} & b_{n-1}\\
0 & 0 & 0 & \cdots & 0 & c_{n-1} & a_{n}%
\end{array}
\right)  =A.
\]
This proves Proposition \ref{prop.tridiag.rec} \textbf{(e)}.

\textbf{(a)} Let $x\in\left\{  0,1,\ldots,n\right\}  $. Then, $A_{x,x}$ is an
$\left(  x-x\right)  \times\left(  x-x\right)  $-matrix, thus a $0\times
0$-matrix. Hence, its determinant is $\det\left(  A_{x,x}\right)  =1$. This
proves Proposition \ref{prop.tridiag.rec} \textbf{(a)}.

\textbf{(b)} Let $x\in\left\{  0,1,\ldots,n-1\right\}  $. The definition of
$A_{x,x+1}$ shows that $A_{x,x+1}$ is the $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
a_{x+1}%
\end{array}
\right)  $. Hence, $\det\left(  A_{x,x+1}\right)  =\det\left(
\begin{array}
[c]{c}%
a_{x+1}%
\end{array}
\right)  =a_{x+1}$. This proves Proposition \ref{prop.tridiag.rec}
\textbf{(b)}.

\textbf{(c)} Let $x\in\left\{  0,1,\ldots,n\right\}  $ and $y\in\left\{
0,1,\ldots,n\right\}  $ be such that $x\leq y-2$. We have%
\begin{equation}
A_{x,y}=\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-2} & b_{y-2} & 0\\
0 & 0 & 0 & \cdots & c_{y-2} & a_{y-1} & b_{y-1}\\
0 & 0 & 0 & \cdots & 0 & c_{y-1} & a_{y}%
\end{array}
\right)  . \label{pf.prop.tridiag.rec.c.Axy}%
\end{equation}
This is a $\left(  y-x\right)  \times\left(  y-x\right)  $-matrix. If we cross
out its $\left(  y-x\right)  $-th row (i.e., its last row) and its $\left(
y-x\right)  $-th column (i.e., its last column), then we obtain $A_{x,y-1}$.
In other words, $\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)
,\sim\left(  y-x\right)  }=A_{x,y-1}$.

Let us write the matrix $A_{x,y}$ in the form $A_{x,y}=\left(  u_{i,j}\right)
_{1\leq i\leq y-x,\ 1\leq j\leq y-x}$. Thus,
\begin{align*}
&  \left(  u_{y-x,1},u_{y-x,2},\ldots,u_{y-x,y-x}\right) \\
&  =\left(  \text{the last row of the matrix }A_{x,y}\right)  =\left(
0,0,\ldots,0,c_{y-1},a_{y}\right)  .
\end{align*}
In other words, we have%
\begin{align}
&  \left(  u_{y-x,q}=0\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,y-x-2\right\}  \right)  ,\label{pf.prop.tridiag.rec.c.1}\\
&  u_{y-x,y-x-1}=c_{y-1},\ \ \ \ \ \ \ \ \ \ \text{and}\nonumber\\
&  u_{y-x,y-x}=a_{y}.\nonumber
\end{align}


Now, Laplace expansion along the $\left(  y-x\right)  $-th row (or, more
precisely, Theorem \ref{thm.laplace.gen} \textbf{(a)}, applied to $y-x$,
$A_{x,y}$, $u_{i,j}$ and $y-x$ instead of $n$, $A$, $a_{i,j}$ and $p$) yields%
\begin{align}
\det\left(  A_{x,y}\right)   &  =\sum_{q=1}^{y-x}\left(  -1\right)  ^{\left(
y-x\right)  +q}u_{y-x,q}\det\left(  \left(  A_{x,y}\right)  _{\sim\left(
y-x\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{y-x-2}\left(  -1\right)  ^{\left(  y-x\right)  +q}%
\underbrace{u_{y-x,q}}_{\substack{=0\\\text{(by (\ref{pf.prop.tridiag.rec.c.1}%
))}}}\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim
q}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{\left(  y-x\right)
+\left(  y-x-1\right)  }}_{=-1}\underbrace{u_{y-x,y-x-1}}_{=c_{y-1}}%
\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x-1\right)  }\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{\left(  y-x\right)
+\left(  y-x\right)  }}_{=1}\underbrace{u_{y-x,y-x}}_{=a_{y}}\det\left(
\underbrace{\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x\right)  }}_{=A_{x,y-1}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }y-x\geq2\text{ (since }x\leq
y-2\text{)}\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{y-x-2}\left(  -1\right)  ^{\left(  y-x\right)
+q}0\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim
q}\right)  }_{=0}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -c_{y-1}\det\left(  \left(  A_{x,y}\right)
_{\sim\left(  y-x\right)  ,\sim\left(  y-x-1\right)  }\right)  +a_{y}%
\det\left(  A_{x,y-1}\right) \nonumber\\
&  =-c_{y-1}\det\left(  \left(  A_{x,y}\right)  _{\sim\left(  y-x\right)
,\sim\left(  y-x-1\right)  }\right)  +a_{y}\det\left(  A_{x,y-1}\right)  .
\label{pf.prop.tridiag.rec.c.3}%
\end{align}


Now, let $B=\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x-1\right)  }$. Thus, (\ref{pf.prop.tridiag.rec.c.3}) becomes%
\begin{align}
\det\left(  A_{x,y}\right)   &  =-c_{y-1}\det\left(  \underbrace{\left(
A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(  y-x-1\right)  }}%
_{=B}\right)  +a_{y}\det\left(  A_{x,y-1}\right) \nonumber\\
&  =-c_{y-1}\det B+a_{y}\det\left(  A_{x,y-1}\right)  .
\label{pf.prop.tridiag.rec.c.3a}%
\end{align}


Now,%
\begin{align}
B  &  =\left(  A_{x,y}\right)  _{\sim\left(  y-x\right)  ,\sim\left(
y-x-1\right)  }\nonumber\\
&  =\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-3} & b_{y-3} & 0\\
0 & 0 & 0 & \cdots & c_{y-3} & a_{y-2} & 0\\
0 & 0 & 0 & \cdots & 0 & c_{y-2} & b_{y-1}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{because of
(\ref{pf.prop.tridiag.rec.c.Axy})}\right)  . \label{pf.prop.tridiag.rec.c.5}%
\end{align}


Now, let us write the matrix $B$ in the form $B=\left(  v_{i,j}\right)
_{1\leq i\leq y-x-1,\ 1\leq j\leq y-x-1}$. Thus,
\begin{align*}
&  \left(  v_{1,y-x-1},v_{2,y-x-1},\ldots,v_{y-x-1,y-x-1}\right)  ^{T}\\
&  =\left(  \text{the last column of the matrix }B\right)  =\left(
0,0,\ldots,0,b_{y-1}\right)  ^{T}%
\end{align*}
(because of (\ref{pf.prop.tridiag.rec.c.5})). In other words, we have%
\begin{align}
&  \left(  v_{p,y-x-1}=0\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,y-x-2\right\}  \right)  ,\ \ \ \ \ \ \ \ \ \ \text{and}%
\label{pf.prop.tridiag.rec.c.7}\\
&  v_{y-x-1,y-x-1}=b_{y-1}.\nonumber
\end{align}
Now, Laplace expansion along the $\left(  y-x-1\right)  $-th column (or, more
precisely, Theorem \ref{thm.laplace.gen} \textbf{(b)}, applied to $y-x-1$,
$B$, $v_{i,j}$ and $y-x-1$ instead of $n$, $A$, $a_{i,j}$ and $q$) yields%
\begin{align}
\det B  &  =\sum_{p=1}^{y-x-1}\left(  -1\right)  ^{p+\left(  y-x-1\right)
}v_{p,y-x-1}\det\left(  B_{\sim p,\sim\left(  y-x-1\right)  }\right)
\nonumber\\
&  =\sum_{p=1}^{y-x-2}\left(  -1\right)  ^{p+\left(  y-x-1\right)
}\underbrace{v_{p,y-x-1}}_{\substack{=0\\\text{(by
(\ref{pf.prop.tridiag.rec.c.7}))}}}\det\left(  B_{\sim p,\sim\left(
y-x-1\right)  }\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{\left(  y-x-1\right)
+\left(  y-x-1\right)  }}_{=1}\underbrace{v_{y-x-1,y-x-1}}_{=b_{y-1}}%
\det\left(  B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)  }\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }y-x-1\geq1\text{ (since }x\leq
y-2\text{)}\right) \nonumber\\
&  =\underbrace{\sum_{p=1}^{y-x-2}\left(  -1\right)  ^{p+\left(  y-x-1\right)
}0\det\left(  B_{\sim p,\sim\left(  y-x-1\right)  }\right)  }_{=0}+b_{y-1}%
\det\left(  B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)  }\right)
\nonumber\\
&  =b_{y-1}\det\left(  B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)
}\right)  . \label{pf.prop.tridiag.rec.c.9}%
\end{align}
Finally, a look at (\ref{pf.prop.tridiag.rec.c.5}) reveals that%
\[
B_{\sim\left(  y-x-1\right)  ,\sim\left(  y-x-1\right)  }=\left(
\begin{array}
[c]{ccccccc}%
a_{x+1} & b_{x+1} & 0 & \cdots & 0 & 0 & 0\\
c_{x+1} & a_{x+2} & b_{x+2} & \cdots & 0 & 0 & 0\\
0 & c_{x+2} & a_{x+3} & \cdots & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & a_{y-4} & b_{y-4} & 0\\
0 & 0 & 0 & \cdots & c_{y-4} & a_{y-3} & b_{y-3}\\
0 & 0 & 0 & \cdots & 0 & c_{y-3} & a_{y-2}%
\end{array}
\right)  =A_{x,y-2}.
\]
Hence, (\ref{pf.prop.tridiag.rec.c.9}) becomes%
\[
\det B=b_{y-1}\det\left(  \underbrace{B_{\sim\left(  y-x-1\right)
,\sim\left(  y-x-1\right)  }}_{=A_{x,y-2}}\right)  =b_{y-1}\det\left(
A_{x,y-2}\right)  .
\]
Therefore, (\ref{pf.prop.tridiag.rec.c.3a}) becomes%
\begin{align*}
\det\left(  A_{x,y}\right)   &  =-c_{y-1}\underbrace{\det B}_{=b_{y-1}%
\det\left(  A_{x,y-2}\right)  }+a_{y}\det\left(  A_{x,y-1}\right) \\
&  =-c_{y-1}b_{y-1}\det\left(  A_{x,y-2}\right)  +a_{y}\det\left(
A_{x,y-1}\right) \\
&  =a_{y}\det\left(  A_{x,y-1}\right)  -b_{y-1}c_{y-1}\det\left(
A_{x,y-2}\right)  .
\end{align*}
This proves Proposition \ref{prop.tridiag.rec} \textbf{(c)}.

\textbf{(d)} The proof of Proposition \ref{prop.tridiag.rec} \textbf{(d)} is
similar to the proof of Proposition \ref{prop.tridiag.rec} \textbf{(c)}. The
main difference is that we now have to perform Laplace expansion along the
$1$-st row (instead of the $\left(  y-x\right)  $-th row) and then Laplace
expansion along the $1$-st column (instead of the $\left(  y-x-1\right)  $-th column).
\end{proof}

Proposition \ref{prop.tridiag.rec} gives us two fast recursive algorithms to
compute $\det A$:

The first algorithm proceeds by recursively computing $\det\left(
A_{0,m}\right)  $ for every $m\in\left\{  0,1,\ldots,n\right\}  $. This is
done using Proposition \ref{prop.tridiag.rec} \textbf{(a)} (for $m=0$),
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (for $m=1$) and Proposition
\ref{prop.tridiag.rec} \textbf{(c)} (to find $\det\left(  A_{0,m}\right)  $
for $m\geq2$ in terms of $\det\left(  A_{0,m-1}\right)  $ and $\det\left(
A_{0,m-2}\right)  $). The final value $\det\left(  A_{0,n}\right)  $ is $\det
A$ (by Proposition \ref{prop.tridiag.rec} \textbf{(e)}).

The second algorithm proceeds by recursively computing $\det\left(
A_{m,n}\right)  $ for every $m\in\left\{  0,1,\ldots,n\right\}  $. This
recursion goes backwards: We start with $m=n$ (where we use Proposition
\ref{prop.tridiag.rec} \textbf{(a)}), then turn to $m=n-1$ (using Proposition
\ref{prop.tridiag.rec} \textbf{(b)}), and then go further and further down
(using Proposition \ref{prop.tridiag.rec} \textbf{(d)} to compute $\det\left(
A_{m,n}\right)  $ in terms of $\det\left(  A_{m+1,n}\right)  $ and
$\det\left(  A_{m+2,n}\right)  $).

So we have two different recursive algorithms leading to one and the same
result. Whenever you have such a thing, you can package up the equivalence of
the two algorithms as an exercise, and try to make it less easy by covering up
the actual goal of the algorithms (in our case, computing $\det A$). In our
case, this leads to the following exercise:

\begin{exercise}
\label{exe.tridiag.isl}Let $n\in\mathbb{N}$. Let $a_{1},a_{2},\ldots,a_{n}$ be
$n$ elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{n-1}$ be $n-1$
elements of $\mathbb{K}$.

Define a sequence $\left(  u_{0},u_{1},\ldots,u_{n}\right)  $ of elements of
$\mathbb{K}$ recursively by setting $u_{0}=1$, $u_{1}=a_{1}$ and%
\[
u_{i}=a_{i}u_{i-1}-b_{i-1}u_{i-2}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  2,3,\ldots,n\right\}  .
\]


Define a sequence $\left(  v_{0},v_{1},\ldots,v_{n}\right)  $ of elements of
$\mathbb{K}$ recursively by setting $v_{0}=1$, $v_{1}=a_{n}$ and%
\[
v_{i}=a_{n-i+1}v_{i-1}-b_{n-i+1}v_{i-2}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  2,3,\ldots,n\right\}  .
\]


Prove that $u_{n}=v_{n}$.
\end{exercise}

This exercise generalizes
\href{http://www.artofproblemsolving.com/community/c6h597118p3543340}{IMO
Shortlist 2013 problem A1}\footnote{I have a suspicion that
\href{http://www.artofproblemsolving.com/community/c6h355915}{IMO Shortlist
2009 problem C3} also can be viewed as an equality between two recursive ways
to compute a determinant; but this determinant seems to be harder to find (I
don't think it can be obtained from Proposition \ref{prop.tridiag.rec}).}.

Our recursive algorithms for computing $\det A$ also yield another
observation: The determinant $\det A$ depends not on the $2\left(  n-1\right)
$ elements \newline$b_{1},b_{2},\ldots,b_{n-1},c_{1},c_{2},\ldots,c_{n-1}$ but
only on the products $b_{1}c_{1},b_{2}c_{2},\ldots,b_{n-1}c_{n-1}$.

\begin{exercise}
\label{exe.tridiag.cf}Define $A_{x,y}$ as in Proposition
\ref{prop.tridiag.rec}. Prove that%
\[
\dfrac{\det A}{\det\left(  A_{1,n}\right)  }=a_{1}-\dfrac{b_{1}c_{1}}%
{a_{2}-\dfrac{b_{2}c_{2}}{a_{3}-\dfrac{b_{3}c_{3}}{%
\begin{array}
[c]{ccc}%
a_{4}- &  & \\
& \ddots & \\
&  & -\dfrac{b_{n-2}c_{n-2}}{a_{n-1}-\dfrac{b_{n-1}c_{n-1}}{a_{n}}}%
\end{array}
}}},
\]
provided that all denominators in this equality are invertible.
\end{exercise}

\begin{exercise}
\label{exe.tridiag.fib}Assume that $a_{i}=1$ for all $i\in\left\{
1,2,\ldots,n\right\}  $. Also, assume that $b_{i}=1$ and $c_{i}=-1$ for all
$i\in\left\{  1,2,\ldots,n-1\right\}  $. Let $\left(  f_{0},f_{1},f_{2}%
,\ldots\right)  $ be the Fibonacci sequence (defined as in Chapter
\ref{chp.recur}). Show that $\det A=f_{n+1}$.
\end{exercise}

\begin{remark}
\label{rmk.tridiag.fib-cont}Consider once again the Fibonacci sequence
$\left(  f_{0},f_{1},f_{2},\ldots\right)  $ (defined as in Chapter
\ref{chp.recur}). Let $n$ be a positive integer. Combining the results of
Exercise \ref{exe.tridiag.cf} and Exercise \ref{exe.tridiag.fib} (the details
are left to the reader), we obtain the equality%
\begin{align*}
\dfrac{f_{n+1}}{f_{n}}  &  =1-\dfrac{1\left(  -1\right)  }{1-\dfrac{1\left(
-1\right)  }{1-\dfrac{1\left(  -1\right)  }{%
\begin{array}
[c]{ccc}%
1- &  & \\
& \ddots & \\
&  & -\dfrac{1\left(  -1\right)  }{1-\dfrac{1\left(  -1\right)  }{1}}%
\end{array}
}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{with }n-1\text{ fractions in
total}\right) \\
&  =1+\dfrac{1}{1+\dfrac{1}{1+\dfrac{1}{%
\begin{array}
[c]{ccc}%
1+ &  & \\
& \ddots & \\
&  & +\dfrac{1}{1+\dfrac{1}{1}}%
\end{array}
}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{with }n-1\text{ fractions in
total}\right)  .
\end{align*}
If you know
\href{https://en.wikipedia.org/wiki/Golden_ratio#Alternative_forms}{some
trivia about the golden ratio}, you might recognize this as a part of the
continued fraction for the golden ratio $\varphi$. The whole continued
fraction for $\varphi$ is%
\[
\varphi=1+\dfrac{1}{1+\dfrac{1}{1+\dfrac{1}{%
\begin{array}
[c]{cc}%
1+ & \\
& \ddots
\end{array}
}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{with infinitely many fractions}\right)
.
\]
This hints at the fact that $\lim\limits_{n\rightarrow\infty}\dfrac{f_{n+1}%
}{f_{n}}=\varphi$. (This is easy to prove without continued fractions, of course.)
\end{remark}

\subsection{On block-triangular matrices}

\begin{definition}
\label{def.block2x2}Let $n$, $n^{\prime}$, $m$ and $m^{\prime}$ be four
nonnegative integers.

Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be an
$n\times m$-matrix.

Let $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$ be
an $n\times m^{\prime}$-matrix.

Let $C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$ be
an $n^{\prime}\times m$-matrix.

Let $D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq
m^{\prime}}$ be an $n^{\prime}\times m^{\prime}$-matrix.

Then, $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ will mean the $\left(  n+n^{\prime}\right)  \times\left(
m+m^{\prime}\right)  $-matrix
\[
\left(
\begin{array}
[c]{cccccccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m} & b_{1,1} & b_{1,2} & \cdots &
b_{1,m^{\prime}}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} & b_{2,1} & b_{2,2} & \cdots &
b_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m} & b_{n,1} & b_{n,2} & \cdots &
b_{n,m^{\prime}}\\
c_{1,1} & c_{1,2} & \cdots & c_{1,m} & d_{1,1} & d_{1,2} & \cdots &
d_{1,m^{\prime}}\\
c_{2,1} & c_{2,2} & \cdots & c_{2,m} & d_{2,1} & d_{2,2} & \cdots &
d_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
c_{n^{\prime},1} & c_{n^{\prime},2} & \cdots & c_{n^{\prime},m} &
d_{n^{\prime},1} & d_{n^{\prime},2} & \cdots & d_{n^{\prime},m^{\prime}}%
\end{array}
\right)  .
\]
(Formally speaking, this means that%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\label{eq.def.block2x2.formal}%
\end{equation}
Less formally, we can say that $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ is the matrix obtained by gluing the matrices $A$, $B$, $C$ and $D$
to form one big $\left(  n+n^{\prime}\right)  \times\left(  m+m^{\prime
}\right)  $-matrix, where the right border of $A$ is glued together with the
left border of $B$, the bottom border of $A$ is glued together with the top
border of $C$, etc.)

Do not get fooled by the notation $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $: It is (in general) not a $2\times2$-matrix, but an $\left(
n+n^{\prime}\right)  \times\left(  m+m^{\prime}\right)  $-matrix, and its
entries are not $A$, $B$, $C$ and $D$ but the entries of $A$, $B$, $C$ and $D$.
\end{definition}

\begin{example}
If $A=\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{c}%
b_{1}\\
b_{2}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{cc}%
c_{1} & c_{2}%
\end{array}
\right)  $ and $D=\left(
\begin{array}
[c]{c}%
d
\end{array}
\right)  $, then $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
a_{1,1} & a_{1,2} & b_{1}\\
a_{2,1} & a_{2,2} & b_{2}\\
c_{1} & c_{2} & d
\end{array}
\right)  $.
\end{example}

The notation $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ introduced in Definition \ref{def.block2x2} is a particular case of
a more general notation -- the \textit{block-matrix construction} -- for
gluing together multiple matrices with matching dimensions\footnote{This
construction defines an $\left(  n_{1}+n_{2}+\cdots+n_{x}\right)
\times\left(  m_{1}+m_{2}+\cdots+m_{y}\right)  $-matrix%
\begin{equation}
\left(
\begin{array}
[c]{cccc}%
A_{1,1} & A_{1,2} & \cdots & A_{1,y}\\
A_{2,1} & A_{2,2} & \cdots & A_{2,y}\\
\vdots & \vdots & \ddots & \vdots\\
A_{x,1} & A_{x,2} & \cdots & A_{x,y}%
\end{array}
\right)  \label{eq.block-general}%
\end{equation}
whenever you have given two nonnegative integers $x$ and $y$, an $x$-tuple
$\left(  n_{1},n_{2},\ldots,n_{x}\right)  \in\mathbb{N}^{x}$, a $y$-tuple
$\left(  m_{1},m_{2},\ldots,m_{y}\right)  \in\mathbb{N}^{y}$, and an
$n_{i}\times m_{j}$-matrix $A_{i,j}$ for every $i\in\left\{  1,2,\ldots
,x\right\}  $ and every $j\in\left\{  1,2,\ldots,y\right\}  $. I guess you can
guess the definition of this matrix. So you start with an \textquotedblleft%
$x\times y$-matrix of matrices\textquotedblright\ and glue them together to an
$\left(  n_{1}+n_{2}+\cdots+n_{x}\right)  \times\left(  m_{1}+m_{2}%
+\cdots+m_{y}\right)  $-matrix (provided that the dimensions of these matrices
allow them to be glued -- e.g., you cannot glue a $2\times3$-matrix to a
$4\times6$-matrix along its right border, nor on any other border).
\par
It is called \textquotedblleft block-matrix construction\textquotedblright%
\ because the original matrices $A_{i,j}$ appear as \textquotedblleft
blocks\textquotedblright\ in the big matrix (\ref{eq.block-general}). Most
authors define block matrices to be matrices which are \textquotedblleft
partitioned\textquotedblright\ into blocks as in (\ref{eq.block-general});
this is essentially our construction in reverse: Instead of gluing several
\textquotedblleft small\textquotedblright\ matrices into a big one, they study
big matrices partitioned into many small matrices. Of course, the properties
of their \textquotedblleft block matrices\textquotedblright\ are equivalent to
those of our \textquotedblleft block-matrix construction\textquotedblright.}.
We shall only need the particular case that is Definition \ref{def.block2x2}, however.

\begin{definition}
Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Recall that $\mathbb{K}^{n\times
m}$ is the set of all $n\times m$-matrices.

We use $0_{n\times m}$ (or sometimes just $0$) to denote the $n\times
m$\textit{ zero matrix}. (As we recall, this is the $n\times m$-matrix whose
all entries are $0$; in other words, this is the $n\times m$-matrix $\left(
0\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.)
\end{definition}

\begin{exercise}
\label{exe.block2x2.mult}Let $n$, $n^{\prime}$, $m$, $m^{\prime}$, $\ell$ and
$\ell^{\prime}$ be six nonnegative integers. Let $A\in\mathbb{K}^{n\times m}$,
$B\in\mathbb{K}^{n\times m^{\prime}}$, $C\in\mathbb{K}^{n^{\prime}\times m}$,
$D\in\mathbb{K}^{n^{\prime}\times m^{\prime}}$, $A^{\prime}\in\mathbb{K}%
^{m\times\ell}$, $B^{\prime}\in\mathbb{K}^{m\times\ell^{\prime}}$, $C^{\prime
}\in\mathbb{K}^{m^{\prime}\times\ell}$ and $D^{\prime}\in\mathbb{K}%
^{m^{\prime}\times\ell^{\prime}}$. Then, prove that%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  .
\]

\end{exercise}

\begin{remark}
The intuitive meaning of Exercise \ref{exe.block2x2.mult} is that the product
of two matrices in \textquotedblleft block-matrix notation\textquotedblright%
\ can be computed by applying the usual multiplication rule \textquotedblleft
on the level of blocks\textquotedblright, without having to fall back to
multiplying single entries. However, when applying Exercise
\ref{exe.block2x2.mult}, do not forget to check that its conditions are
satisfied. Let me give an example and a non-example:

\textbf{Example:} If $A=\left(
\begin{array}
[c]{c}%
a_{1}\\
a_{2}\\
a_{3}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{cc}%
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}\\
b_{3,1} & b_{3,2}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{c}%
c
\end{array}
\right)  $, $D=\left(
\begin{array}
[c]{cc}%
d_{1} & d_{2}%
\end{array}
\right)  $, $A^{\prime}=\left(
\begin{array}
[c]{cc}%
a_{1}^{\prime} & a_{2}^{\prime}%
\end{array}
\right)  $, $B^{\prime}=\left(
\begin{array}
[c]{cc}%
b_{1}^{\prime} & b_{2}^{\prime}%
\end{array}
\right)  $, $C^{\prime}=\left(
\begin{array}
[c]{cc}%
c_{1,1}^{\prime} & c_{1,2}^{\prime}\\
c_{2,1}^{\prime} & c_{2,2}^{\prime}%
\end{array}
\right)  $ and $D^{\prime}=\left(
\begin{array}
[c]{cc}%
d_{1,1}^{\prime} & d_{1,2}^{\prime}\\
d_{2,1}^{\prime} & d_{2,2}^{\prime}%
\end{array}
\right)  $, then Exercise \ref{exe.block2x2.mult} can be applied (with $n=3$,
$n^{\prime}=1$, $m=1$, $m^{\prime}=2$, $\ell=2$ and $\ell^{\prime}=2$), and
thus we obtain%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  .
\]


\textbf{Non-example:} If $A=\left(
\begin{array}
[c]{c}%
a_{1}\\
a_{2}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{cc}%
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{c}%
c_{1}\\
c_{2}%
\end{array}
\right)  $, $D=\left(
\begin{array}
[c]{cc}%
d_{1,1} & d_{1,2}\\
d_{2,1} & d_{2,2}%
\end{array}
\right)  $, $A^{\prime}=\left(
\begin{array}
[c]{cc}%
a_{1,1}^{\prime} & a_{1,2}^{\prime}\\
a_{2,1}^{\prime} & a_{2,2}^{\prime}%
\end{array}
\right)  $, $B^{\prime}=\left(
\begin{array}
[c]{cc}%
b_{1,1}^{\prime} & b_{1,2}^{\prime}\\
b_{2,1}^{\prime} & b_{2,2}^{\prime}%
\end{array}
\right)  $, $C^{\prime}=\left(
\begin{array}
[c]{cc}%
c_{1}^{\prime} & c_{2}^{\prime}%
\end{array}
\right)  $ and $D^{\prime}=\left(
\begin{array}
[c]{cc}%
d_{1}^{\prime} & d_{2}^{\prime}%
\end{array}
\right)  $, then Exercise \ref{exe.block2x2.mult} cannot be applied, because
there exist no $n,m,\ell\in\mathbb{N}$ such that $A\in\mathbb{K}^{n\times m}$
and $A^{\prime}\in\mathbb{K}^{m\times\ell}$. (Indeed, the number of columns of
$A$ does not equal the number of rows of $A^{\prime}$, but these numbers would
both have to be $m$.) The matrices $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $ still exist in this case, and can even be multiplied, but their
product is not given by a simple formula such as the one in Exercise
\ref{exe.block2x2.mult}. Thus, beware of seeing Exercise
\ref{exe.block2x2.mult} as a panacea for multiplying matrices blockwise.
\end{remark}

\begin{exercise}
\label{exe.block2x2.tridet}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$
be an $n\times n$-matrix. Let $B$ be an $n\times m$-matrix. Let $D$ be an
$m\times m$-matrix. Prove that%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{m\times n} & D
\end{array}
\right)  =\det A\cdot\det D.
\]

\end{exercise}

\begin{example}
Exercise \ref{exe.block2x2.tridet} (applied to $n=2$ and $m=3$) yields%
\[
\det\left(
\begin{array}
[c]{ccccc}%
a_{1,1} & a_{1,2} & b_{1,1} & b_{1,2} & b_{1,3}\\
a_{2,1} & a_{2,2} & b_{2,1} & b_{2,2} & b_{2,3}\\
0 & 0 & c_{1,1} & c_{1,2} & c_{1,3}\\
0 & 0 & c_{2,1} & c_{2,2} & c_{2,3}\\
0 & 0 & c_{3,1} & c_{3,2} & c_{3,3}%
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{ccc}%
c_{1,1} & c_{1,2} & c_{1,3}\\
c_{2,1} & c_{2,2} & c_{2,3}\\
c_{3,1} & c_{3,2} & c_{3,3}%
\end{array}
\right)  .
\]

\end{example}

\begin{remark}
Not every determinant of the form $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{m\times n} & D
\end{array}
\right)  $ can be computed using Exercise \ref{exe.block2x2.tridet}. In fact,
Exercise \ref{exe.block2x2.tridet} requires $A$ to be an $n\times n$-matrix
and $D$ to be an $m\times m$-matrix; thus, both $A$ and $D$ have to be square
matrices in order for Exercise \ref{exe.block2x2.tridet} to be applicable. For
instance, Exercise \ref{exe.block2x2.tridet} cannot be applied to compute
$\det\left(
\begin{array}
[c]{ccc}%
a_{1} & b_{1,1} & b_{1,2}\\
a_{2} & b_{2,1} & b_{2,2}\\
0 & c_{1} & c_{2}%
\end{array}
\right)  $.
\end{remark}

\begin{remark}
\label{rmk.block2x2.det-gen}You might wonder whether Exercise
\ref{exe.block2x2.tridet} generalizes to a formula for $\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ when $A\in\mathbb{K}^{n\times n}$, $B\in\mathbb{K}^{n\times m}$,
$C\in\mathbb{K}^{m\times n}$ and $D\in\mathbb{K}^{m\times m}$. The general
answer is \textquotedblleft No\textquotedblright. However, when $D$ is
invertible, there exists such a formula (the
\href{https://en.wikipedia.org/wiki/Schur_complement}{Schur complement}
formula shown in Exercise \ref{exe.block2x2.schur} below). Curiously, there is
also a formula for the case when $n=m$ and $CD=DC$ (see \cite[Theorem
3]{Silvest}).
\end{remark}

We notice that Exercise \ref{exe.block2x2.tridet} allows us to solve Exercise
\ref{exe.ps4.5} in a new way.

An analogue of Exercise \ref{exe.block2x2.tridet} exists in which the
$0_{m\times n}$ in the lower-left part of the matrix is replaced by a
$0_{n\times m}$ in the upper-right part:

\begin{exercise}
\label{exe.block2x2.tridet.transposed}Let $n\in\mathbb{N}$ and $m\in
\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let $C$ be an $m\times
n$-matrix. Let $D$ be an $m\times m$-matrix. Prove that%
\[
\det\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  =\det A\cdot\det D.
\]

\end{exercise}

\begin{exercise}
\label{exe.det.creative}Invent and solve an exercise on computing determinants.
\end{exercise}

\subsection{The adjugate matrix}

We start this section with a variation on Theorem \ref{thm.laplace.gen}:

\begin{proposition}
\label{prop.laplace.0}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix. Let $r\in\left\{
1,2,\ldots,n\right\}  $.

\textbf{(a)} For every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfying $p\neq
r$, we have%
\[
0=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\]


\textbf{(b)} For every $q\in\left\{  1,2,\ldots,n\right\}  $ satisfying $q\neq
r$, we have%
\[
0=\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  .
\]

\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.laplace.0}.]\textbf{(a)} Let $p\in\left\{
1,2,\ldots,n\right\}  $ be such that $p\neq r$.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $p$-th
row of $A$ by the $r$-th row of $A$. Thus, the $p$-th and the $r$-th rows of
$C$ are equal. Therefore, the matrix $C$ has two equal rows (since $p\neq r$).
Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$
instead of $A$).

Let us write the $n\times n$-matrix $C$ in the form $C=\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

The $p$-th row of $C$ equals the $r$-th row of $A$ (by the construction of
$C$). In other words,%
\begin{equation}
c_{p,q}=a_{r,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  . \label{pf.prop.laplace.0.short.cpq}%
\end{equation}
On the other hand, the matrix $C$ equals the matrix $A$ in all rows but the
$p$-th one (again, by the construction of $C$). Hence, if we cross out the
$p$-th rows in both $C$ and $A$, then the matrices $C$ and $A$ become equal.
Therefore,%
\begin{equation}
C_{\sim p,\sim q}=A_{\sim p,\sim q}\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left\{  1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.short.CA}%
\end{equation}
(because the construction of $C_{\sim p,\sim q}$ from $C$ involves crossing
out the $p$-th row, and so does the construction of $A_{\sim p,\sim q}$ from
$A$).

Now, $\det C=0$, so that%
\begin{align*}
0  &  =\det C=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\underbrace{c_{p,q}%
}_{\substack{=a_{r,q}\\\text{(by (\ref{pf.prop.laplace.0.short.cpq}))}}%
}\det\left(  \underbrace{C_{\sim p,\sim q}}_{\substack{=A_{\sim p,\sim
q}\\\text{(by (\ref{pf.prop.laplace.0.short.CA}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(a)}, applied to }C\text{ and }c_{i,j}\text{ instead of }A\text{ and
}a_{i,j}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\end{align*}
This proves Proposition \ref{prop.laplace.0} \textbf{(a)}.

\textbf{(b)} This proof is rather similar to the proof of Proposition
\ref{prop.laplace.0} \textbf{(a)}, except that rows are now replaced by
columns. We leave the details to the reader.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.laplace.0}.]\textbf{(a)} Let $p\in\left\{
1,2,\ldots,n\right\}  $ be such that $p\neq r$.

Let $w$ be the $r$-th row of $A$ (regarded, as usual, as a row vector). Thus,
$w=\left(  \text{the }r\text{-th row of }A\right)  $.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $p$-th
row of $A$ by the row vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }C\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{pf.prop.laplace.0.Cu}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq p\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }p\text{-th row of }C\right)  =w.
\label{pf.prop.laplace.0.Cp}%
\end{equation}
The matrix $C$ has two equal rows\footnote{\textit{Proof.} We have $r\neq p$
(since $p\neq r$). Hence, (\ref{pf.prop.laplace.0.Cu}) (applied to $u=r$)
yields%
\begin{align*}
\left(  \text{the }r\text{-th row of }C\right)   &  =\left(  \text{the
}r\text{-th row of }A\right)  =w\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}w=\left(  \text{the }r\text{-th row of }A\right)  \right) \\
&  =\left(  \text{the }p\text{-th row of }C\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.Cp})}\right)  .
\end{align*}
In other words, the $r$-th row of $C$ and the $p$-th row of $C$ are equal.
Since $r\neq p$, this shows that the matrix $C$ has two equal rows. Qed.}.
Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$
instead of $A$).

Let us write the $n\times n$-matrix $C$ in the form $C=\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every $u\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th row of }C\right)  =\left(  c_{u,1}%
,c_{u,2},\ldots,c_{u,n}\right)  . \label{pf.prop.laplace.0.u-th-row}%
\end{equation}
Applying this to $u=p$, we obtain%
\[
\left(  \text{the }p\text{-th row of }C\right)  =\left(  c_{p,1}%
,c_{p,2},\ldots,c_{p,n}\right)  ,
\]
so that%
\begin{align*}
\left(  c_{p,1},c_{p,2},\ldots,c_{p,n}\right)   &  =\left(  \text{the
}p\text{-th row of }C\right)  =w=\left(  \text{the }r\text{-th row of
}A\right) \\
&  =\left(  a_{r,1},a_{r,2},\ldots,a_{r,n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  .
\end{align*}
In other words,%
\begin{equation}
c_{p,q}=a_{r,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  . \label{pf.prop.laplace.0.cpq}%
\end{equation}


On the other hand,%
\begin{equation}
c_{u,q}=a_{u,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  \text{ and }u\in\left\{  1,2,\ldots,n\right\}  \text{
satisfying }u\neq p \label{pf.prop.laplace.0.cuq}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.laplace.0.cuq}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ be such that $u\neq p$. Thus,%
\begin{align*}
\left(  c_{u,1},c_{u,2},\ldots,c_{u,n}\right)   &  =\left(  \text{the
}u\text{-th row of }C\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.laplace.0.u-th-row})}\right) \\
&  =\left(  \text{the }u\text{-th row of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.Cu})}\right) \\
&  =\left(  a_{u,1},a_{u,2},\ldots,a_{u,n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  .
\end{align*}
In other words, $c_{u,q}=a_{u,q}$ for every $q\in\left\{  1,2,\ldots
,n\right\}  $. This proves (\ref{pf.prop.laplace.0.cuq}).}. Now, it is easy to
see that%
\begin{equation}
C_{\sim p,\sim q}=A_{\sim p,\sim q}\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left\{  1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.CA}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.laplace.0.CA}):} Let $q\in\left\{
1,2,\ldots,n\right\}  $.
\par
Let $\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  $. Thus,
$\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $, so that $\left\{  u_{1},u_{2},\ldots,u_{n-1}\right\}
=\left\{  1,2,\ldots,\widehat{p},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  p\right\}  $.
\par
Now, let $x\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $u_{x}\in\left\{
u_{1},u_{2},\ldots,u_{n-1}\right\}  =\left\{  1,2,\ldots,n\right\}
\setminus\left\{  p\right\}  $, so that $u_{x}\neq p$. Hence,%
\begin{equation}
c_{u_{x},q}=a_{u_{x},q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.CA.pf.1}%
\end{equation}
(by (\ref{pf.prop.laplace.0.cuq}), applied to $u=u_{x}$).
\par
Let us now forget that we fixed $x$. We thus have shown that
(\ref{pf.prop.laplace.0.CA.pf.1}) holds for every $x\in\left\{  1,2,\ldots
,n-1\right\}  $.
\par
Let $\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{q},\ldots,n\right)  $. Thus,
$\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  =\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  $.
\par
Now, the definition of $C_{\sim p,\sim q}$ yields%
\begin{align*}
C_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}C=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{v_{1},v_{2},\ldots,v_{n-1}}C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  =\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1}%
,v_{2},\ldots,v_{n-1}}C\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  \right) \\
&  =\left(  \underbrace{c_{u_{x},v_{y}}}_{\substack{=a_{u_{x},v_{y}%
}\\\text{(by (\ref{pf.prop.laplace.0.CA.pf.1}),}\\\text{applied to }%
q=v_{y}\text{)}}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1},v_{2},\ldots,v_{n-1}}C\text{,
since }C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}.
\end{align*}
Compared with%
\begin{align*}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{v_{1},v_{2},\ldots,v_{n-1}}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  =\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1}%
,v_{2},\ldots,v_{n-1}}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  \right) \\
&  =\left(  a_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1},v_{2},\ldots,v_{n-1}}A\text{,
since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  ,
\end{align*}
this yields $C_{\sim p,\sim q}=A_{\sim p,\sim q}$. This proves
(\ref{pf.prop.laplace.0.CA}).}. Now, $\det C=0$, so that%
\begin{align*}
0  &  =\det C=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\underbrace{c_{p,q}%
}_{\substack{=a_{r,q}\\\text{(by (\ref{pf.prop.laplace.0.cpq}))}}}\det\left(
\underbrace{C_{\sim p,\sim q}}_{\substack{=A_{\sim p,\sim q}\\\text{(by
(\ref{pf.prop.laplace.0.CA}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(a)}, applied to }C\text{ and }c_{i,j}\text{ instead of }A\text{ and
}a_{i,j}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{r,q}\det\left(  A_{\sim p,\sim
q}\right)  .
\end{align*}
This proves Proposition \ref{prop.laplace.0} \textbf{(a)}.

\textbf{(b)} This proof is rather similar to the proof of Proposition
\ref{prop.laplace.0} \textbf{(a)}, except that rows are now replaced by
columns. Let me nevertheless show this proof in full detail, for the sake of completeness:

Let $q\in\left\{  1,2,\ldots,n\right\}  $ be such that $q\neq r$.

Let $w$ be the $r$-th column of $A$ (regarded, as usual, as a column vector).
Thus, $w=\left(  \text{the }r\text{-th column of }A\right)  $.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $q$-th
column of $A$ by the column vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th column of }C\right)  =\left(
\text{the }u\text{-th column of }A\right)  \right.
\label{pf.prop.laplace.0.b.Cu}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq q\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }q\text{-th column of }C\right)  =w.
\label{pf.prop.laplace.0.b.Cp}%
\end{equation}
The matrix $C$ has two equal columns\footnote{\textit{Proof.} We have $r\neq
q$ (since $q\neq r$). Hence, (\ref{pf.prop.laplace.0.b.Cu}) (applied to $u=r$)
yields%
\begin{align*}
\left(  \text{the }r\text{-th column of }C\right)   &  =\left(  \text{the
}r\text{-th column of }A\right)  =w\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}w=\left(  \text{the }r\text{-th column of }A\right)  \right) \\
&  =\left(  \text{the }q\text{-th column of }C\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.b.Cp})}\right)  .
\end{align*}
In other words, the $r$-th column of $C$ and the $q$-th column of $C$ are
equal. Since $r\neq q$, this shows that the matrix $C$ has two equal columns.
Qed.}. Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(f)}, applied to
$C$ instead of $A$).

Let us write the $n\times n$-matrix $C$ in the form $C=\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every $u\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th column of }C\right)  =\left(
\begin{array}
[c]{c}%
c_{1,u}\\
c_{2,u}\\
\vdots\\
c_{n,u}%
\end{array}
\right)  . \label{pf.prop.laplace.0.b.u-th-row}%
\end{equation}
Applying this to $u=q$, we obtain%
\[
\left(  \text{the }q\text{-th column of }C\right)  =\left(
\begin{array}
[c]{c}%
c_{1,q}\\
c_{2,q}\\
\vdots\\
c_{n,q}%
\end{array}
\right)  ,
\]
so that%
\begin{align*}
\left(
\begin{array}
[c]{c}%
c_{1,q}\\
c_{2,q}\\
\vdots\\
c_{n,q}%
\end{array}
\right)   &  =\left(  \text{the }q\text{-th column of }C\right)  =w=\left(
\text{the }r\text{-th column of }A\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,r}\\
a_{2,r}\\
\vdots\\
a_{n,r}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
In other words,%
\begin{equation}
c_{p,q}=a_{p,r}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  . \label{pf.prop.laplace.0.b.cpq}%
\end{equation}


On the other hand,%
\begin{equation}
c_{p,u}=a_{p,u}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \text{ and }u\in\left\{  1,2,\ldots,n\right\}  \text{
satisfying }u\neq q \label{pf.prop.laplace.0.b.cuq}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.laplace.0.b.cuq}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ be such that $u\neq q$. Thus,%
\begin{align*}
\left(
\begin{array}
[c]{c}%
c_{1,u}\\
c_{2,u}\\
\vdots\\
c_{n,u}%
\end{array}
\right)   &  =\left(  \text{the }u\text{-th column of }C\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.b.u-th-row}%
)}\right) \\
&  =\left(  \text{the }u\text{-th column of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.laplace.0.b.Cu})}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,u}\\
a_{2,u}\\
\vdots\\
a_{n,u}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
In other words, $c_{p,u}=a_{p,u}$ for every $p\in\left\{  1,2,\ldots
,n\right\}  $. This proves (\ref{pf.prop.laplace.0.b.cuq}).}. Now, it is easy
to see that%
\begin{equation}
C_{\sim p,\sim q}=A_{\sim p,\sim q}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.b.CA}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.laplace.0.b.CA}):} Let $p\in\left\{
1,2,\ldots,n\right\}  $.
\par
Let $\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{q},\ldots,n\right)  $. Thus,
$\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{q}%
,\ldots,n\right)  $, so that $\left\{  u_{1},u_{2},\ldots,u_{n-1}\right\}
=\left\{  1,2,\ldots,\widehat{q},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  q\right\}  $.
\par
Now, let $y\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $u_{y}\in\left\{
u_{1},u_{2},\ldots,u_{n-1}\right\}  =\left\{  1,2,\ldots,n\right\}
\setminus\left\{  q\right\}  $, so that $u_{y}\neq q$. Hence,%
\begin{equation}
c_{p,u_{y}}=a_{p,u_{y}}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \label{pf.prop.laplace.0.b.CA.pf.1}%
\end{equation}
(by (\ref{pf.prop.laplace.0.b.cuq}), applied to $u=u_{y}$).
\par
Let us now forget that we fixed $y$. We thus have shown that
(\ref{pf.prop.laplace.0.b.CA.pf.1}) holds for every $y\in\left\{
1,2,\ldots,n-1\right\}  $.
\par
Let $\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  $. Thus,
$\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $.
\par
Now, the definition of $C_{\sim p,\sim q}$ yields%
\begin{align*}
C_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}C=\operatorname*{sub}%
\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{1,2,\ldots,\widehat{q},\ldots,n}C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  =\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1}%
,u_{2},\ldots,u_{n-1}}C\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{q},\ldots,n\right)  =\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  \right) \\
&  =\left(  \underbrace{c_{v_{x},u_{y}}}_{\substack{=a_{v_{x},u_{y}%
}\\\text{(by (\ref{pf.prop.laplace.0.b.CA.pf.1}),}\\\text{applied to }%
p=v_{x}\text{)}}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1},u_{2},\ldots,u_{n-1}}C\text{,
since }C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{v_{x},u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}.
\end{align*}
Compared with%
\begin{align*}
A_{\sim p,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,n}A=\operatorname*{sub}%
\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{1,2,\ldots,\widehat{q},\ldots,n}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  =\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1}%
,u_{2},\ldots,u_{n-1}}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{q},\ldots,n\right)  =\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  \right) \\
&  =\left(  a_{v_{x},u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
a\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1},u_{2}%
,\ldots,u_{n-1}}A\text{, since }A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ,
\end{align*}
this yields $C_{\sim p,\sim q}=A_{\sim p,\sim q}$. This proves
(\ref{pf.prop.laplace.0.b.CA}).}. Now, $\det C=0$, so that%
\begin{align*}
0  &  =\det C=\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}\underbrace{c_{p,q}%
}_{\substack{=a_{p,r}\\\text{(by (\ref{pf.prop.laplace.0.b.cpq}))}}%
}\det\left(  \underbrace{C_{\sim p,\sim q}}_{\substack{=A_{\sim p,\sim
q}\\\text{(by (\ref{pf.prop.laplace.0.b.CA}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(b)}, applied to }C\text{ and }c_{i,j}\text{ instead of }A\text{ and
}a_{i,j}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  .
\end{align*}
This proves Proposition \ref{prop.laplace.0} \textbf{(b)}.
\end{proof}
\end{verlong}

We now can define the \textquotedblleft adjugate\textquotedblright\ of a matrix:

\begin{definition}
\label{def.adj}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. We
define a new $n\times n$-matrix $\operatorname*{adj}A$ by%
\[
\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]


This matrix $\operatorname*{adj}A$ is called the \textit{adjugate} of the
matrix $A$. (Some authors call it the \textquotedblleft
adjunct\textquotedblright\ or \textquotedblleft adjoint\textquotedblright\ or
\textquotedblleft classical adjoint\textquotedblright\ of $A$ instead.
However, beware of the word \textquotedblleft adjoint\textquotedblright: It
means too many different things; in particular it has a second meaning for a matrix.)
\end{definition}

The appearance of $A_{\sim j,\sim i}$ (not $A_{\sim i,\sim j}$) in Definition
\ref{def.adj} might be surprising, but it is not a mistake. We will soon see
what it is good for.

There is also a related notion, namely that of a \textquotedblleft cofactor
matrix\textquotedblright. The \textit{cofactor matrix} of an $n\times
n$-matrix $A$ is defined to be $\left(  \left(  -1\right)  ^{i+j}\det\left(
A_{\sim i,\sim j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. This is,
of course, the transpose $\left(  \operatorname*{adj}A\right)  ^{T}$ of
$\operatorname*{adj}A$. The entries of this matrix are called the
\textit{cofactors} of $A$.

\begin{example}
The adjugate of the $0\times0$-matrix is the $0\times0$-matrix.

The adjugate of a $1\times1$-matrix $\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)  $ is $\operatorname*{adj}\left(
\begin{array}
[c]{c}%
a
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  $. (Yes, this shows that all $1\times1$-matrices have the same adjugate.)

The adjugate of a $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $ is $\operatorname*{adj}\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  $.

The adjugate of a $3\times3$-matrix $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  $ is $\operatorname*{adj}\left(
\begin{array}
[c]{ccc}%
a & b & c\\
d & e & f\\
g & h & i
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
ei-fh & ch-bi & bf-ce\\
fg-di & ai-cg & cd-af\\
dh-ge & bg-ah & ae-bd
\end{array}
\right)  $.
\end{example}

\begin{proposition}
\label{prop.adj.transpose}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Then, $\operatorname*{adj}\left(  A^{T}\right)  =\left(
\operatorname*{adj}A\right)  ^{T}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.adj.transpose}.]Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. \newline
From $i\in\left\{  1,2,\ldots,n\right\}  $, we obtain $1\leq i\leq n$, so that
$n\geq1$ and thus $n-1\in\mathbb{N}$.

The definition of $A_{\sim i,\sim j}$ yields $A_{\sim i,\sim j}%
=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots,n}^{1,2,\ldots
,\widehat{j},\ldots,n}A$. But the definition of $\left(  A^{T}\right)  _{\sim
j,\sim i}$ yields%
\begin{equation}
\left(  A^{T}\right)  _{\sim j,\sim i}=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{j},\ldots,n}^{1,2,\ldots,\widehat{i},\ldots
,n}\left(  A^{T}\right)  . \label{pf.prop.adj.transpose.1}%
\end{equation}


On the other hand, Proposition \ref{prop.submatrix.easy} \textbf{(e)} (applied
to $m=n$, $u=n-1$, $v=n-1$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(
1,2,\ldots,\widehat{i},\ldots,n\right)  $ and $\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  =\left(  1,2,\ldots,\widehat{j},\ldots,n\right)  $)
yields $\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots
,n}^{1,2,\ldots,\widehat{j},\ldots,n}A\right)  ^{T}=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{j},\ldots,n}^{1,2,\ldots,\widehat{i},\ldots
,n}\left(  A^{T}\right)  $. Compared with (\ref{pf.prop.adj.transpose.1}),
this yields%
\[
\left(  A^{T}\right)  _{\sim j,\sim i}=\left(  \underbrace{\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{i},\ldots,n}^{1,2,\ldots,\widehat{j},\ldots
,n}A}_{=A_{\sim i,\sim j}}\right)  ^{T}=\left(  A_{\sim i,\sim j}\right)
^{T}.
\]
Hence,%
\begin{equation}
\det\left(  \underbrace{\left(  A^{T}\right)  _{\sim j,\sim i}}_{=\left(
A_{\sim i,\sim j}\right)  ^{T}}\right)  =\det\left(  \left(  A_{\sim i,\sim
j}\right)  ^{T}\right)  =\det\left(  A_{\sim i,\sim j}\right)
\label{pf.prop.adj.transpose.4}%
\end{equation}
(by Exercise \ref{exe.ps4.4}, applied to $n-1$ and $A_{\sim i,\sim j}$ instead
of $n$ and $A$).

Let us now forget that we fixed $i$ and $j$. We thus have shown that
(\ref{pf.prop.adj.transpose.4}) holds for every $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $.

Now, $\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(
A_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, and thus
the definition of the transpose of a matrix shows that%
\[
\left(  \operatorname*{adj}A\right)  ^{T}=\left(  \underbrace{\left(
-1\right)  ^{j+i}}_{=\left(  -1\right)  ^{i+j}}\det\left(  A_{\sim i,\sim
j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \left(
-1\right)  ^{i+j}\det\left(  A_{\sim i,\sim j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}.
\]
Compared with%
\begin{align*}
\operatorname*{adj}\left(  A^{T}\right)   &  =\left(  \left(  -1\right)
^{i+j}\underbrace{\det\left(  \left(  A^{T}\right)  _{\sim j,\sim i}\right)
}_{\substack{=\det\left(  A_{\sim i,\sim j}\right)  \\\text{(by
(\ref{pf.prop.adj.transpose.4}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{adj}%
\left(  A^{T}\right)  \right) \\
&  =\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim i,\sim j}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n},
\end{align*}
this yields $\operatorname*{adj}\left(  A^{T}\right)  =\left(
\operatorname*{adj}A\right)  ^{T}$. This proves Proposition
\ref{prop.adj.transpose}.
\end{proof}

The most important property of adjugates, however, is the following fact:

\begin{theorem}
\label{thm.adj.inverse}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Then,%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
(Recall that $I_{n}$ denotes the $n\times n$ identity matrix. Expressions such
as $\operatorname*{adj}A\cdot A$ and $\det A\cdot I_{n}$ have to be understood
as $\left(  \operatorname*{adj}A\right)  \cdot A$ and $\left(  \det A\right)
\cdot I_{n}$, respectively.)
\end{theorem}

\begin{example}
Recall that the adjugate of a $2\times2$-matrix is given by the formula
$\operatorname*{adj}\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  $. Thus, Theorem \ref{thm.adj.inverse} (applied to $n=2$) yields%
\[
\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
d & -b\\
-c & a
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \cdot I_{2}.
\]
(Of course, $\det\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \cdot I_{2}=\left(  ad-bc\right)  \cdot I_{2}=\left(
\begin{array}
[c]{cc}%
ad-bc & 0\\
0 & ad-bc
\end{array}
\right)  $.)
\end{example}

\begin{vershort}
\begin{proof}
[Proof of Theorem \ref{thm.adj.inverse}.]For any two objects $i$ and $j$, we
define $\delta_{i,j}$ to be the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (by the definition of $I_{n}$), and thus%
\begin{equation}
\det A\cdot\underbrace{I_{n}}_{=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}=\det A\cdot\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{pf.thm.adj.inverse.short.R}%
\end{equation}


On the other hand, let us write the matrix $A$ in the form $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, the definition of the
product of two matrices shows that%
\begin{align}
&  A\cdot\operatorname*{adj}A\nonumber\\
&  =\left(  \sum_{k=1}^{n}a_{i,k}\left(  -1\right)  ^{k+j}\det\left(  A_{\sim
j,\sim k}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\\
\text{and }\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(
A_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{q=1}^{n}\underbrace{a_{i,q}\left(  -1\right)  ^{q+j}%
}_{=\left(  -1\right)  ^{q+j}a_{i,q}}\det\left(  A_{\sim j,\sim q}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }q\right) \nonumber\\
&  =\left(  \sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim
j,\sim q}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\label{pf.thm.adj.inverse.short.L}%
\end{align}


Now, we claim that%
\begin{equation}
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =\det A\cdot\delta_{i,j} \label{pf.thm.adj.inverse.short.twise}%
\end{equation}
for any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

\textit{Proof of (\ref{pf.thm.adj.inverse.short.twise}):} Fix $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. We are in one of the
following two cases:

\textit{Case 1:} We have $i=j$.

\textit{Case 2:} We have $i\neq j$.

Let us consider Case 1 first. In this case, we have $i=j$. Hence,
$\delta_{i,j}=1$. Now, Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to
$p=i$) yields%
\[
\det A=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{i+q}}%
_{\substack{=\left(  -1\right)  ^{q+i}=\left(  -1\right)  ^{q+j}\\\text{(since
}i=j\text{)}}}a_{i,q}\det\left(  \underbrace{A_{\sim i,\sim q}}%
_{\substack{=A_{\sim j,\sim q}\\\text{(since }i=j\text{)}}}\right)
=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  .
\]
In view of $\det A\cdot\underbrace{\delta_{i,j}}_{=1}=\det A$, this rewrites
as
\[
\det A\cdot\delta_{i,j}=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}%
\det\left(  A_{\sim j,\sim q}\right)  .
\]
Thus, (\ref{pf.thm.adj.inverse.short.twise}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $i\neq j$. Hence,
$\delta_{i,j}=0$ and $j\neq i$. Now, Proposition \ref{prop.laplace.0}
\textbf{(a)} (applied to $p=j$ and $r=i$) yields%
\[
0=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{j+q}}_{=\left(  -1\right)
^{q+j}}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  =\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  .
\]
In view of $\det A\cdot\underbrace{\delta_{i,j}}_{=0}=0$, this rewrites as%
\[
\det A\cdot\delta_{i,j}=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}%
\det\left(  A_{\sim j,\sim q}\right)  .
\]
Thus, (\ref{pf.thm.adj.inverse.short.twise}) is proven in Case 2.

We have now proven (\ref{pf.thm.adj.inverse.short.twise}) in each of the two
Cases 1 and 2. Thus, (\ref{pf.thm.adj.inverse.short.twise}) is proven.

Now, (\ref{pf.thm.adj.inverse.short.L}) becomes%
\begin{align}
A\cdot\operatorname*{adj}A  &  =\left(  \underbrace{\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  }%
_{\substack{=\det A\cdot\delta_{i,j}\\\text{(by
(\ref{pf.thm.adj.inverse.short.twise}))}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\nonumber\\
&  =\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\det A\cdot I_{n} \label{pf.thm.adj.inverse.short.part1}%
\end{align}
(by (\ref{pf.thm.adj.inverse.short.R})).

It now remains to prove that $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$.
One way to do this is by mimicking the above proof using Theorem
\ref{thm.laplace.gen} \textbf{(b)} and Proposition \ref{prop.laplace.0}
\textbf{(b)} instead of Theorem \ref{thm.laplace.gen} \textbf{(a)} and
Proposition \ref{prop.laplace.0} \textbf{(a)}. However, here is a slicker proof:

Let us forget that we fixed $A$. We thus have shown that
(\ref{pf.thm.adj.inverse.short.part1}) holds for every $n\times n$-matrix $A$.

Now, let $A$ be any $n\times n$-matrix. Then, we can apply
(\ref{pf.thm.adj.inverse.short.part1}) to $A^{T}$ instead of $A$. We thus
obtain%
\begin{equation}
A^{T}\cdot\operatorname*{adj}\left(  A^{T}\right)  =\underbrace{\det\left(
A^{T}\right)  }_{\substack{=\det A\\\text{(by Exercise \ref{exe.ps4.4})}%
}}\cdot I_{n}=\det A\cdot I_{n}. \label{pf.thm.adj.inverse.short.4}%
\end{equation}


But here are four fundamental properties of transposes which are all easy to check:

\begin{itemize}
\item If $u$, $v$ and $w$ are three nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $Q$ is a $v\times w$-matrix, then%
\begin{equation}
\left(  PQ\right)  ^{T}=Q^{T}P^{T}.
\label{pf.thm.adj.inverse.short.tranposes1}%
\end{equation}


\item Every $u\in\mathbb{N}$ satisfies%
\begin{equation}
\left(  I_{u}\right)  ^{T}=I_{u}. \label{pf.thm.adj.inverse.short.tranposes2}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $\lambda\in\mathbb{K}$, then%
\begin{equation}
\left(  \lambda P\right)  ^{T}=\lambda P^{T}.
\label{pf.thm.adj.inverse.short.tranposes3}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, and if $P$ is a $u\times
v$-matrix, then%
\begin{equation}
\left(  P^{T}\right)  ^{T}=P. \label{pf.thm.adj.inverse.short.tranposes4}%
\end{equation}

\end{itemize}

Now, (\ref{pf.thm.adj.inverse.short.tranposes1}) (applied to $u=n$, $v=n$,
$w=n$, $P=\operatorname*{adj}A$ and $Q=A$) shows that%
\[
\left(  \operatorname*{adj}A\cdot A\right)  ^{T}=A^{T}\cdot\underbrace{\left(
\operatorname*{adj}A\right)  ^{T}}_{\substack{=\operatorname*{adj}\left(
A^{T}\right)  \\\text{(by Proposition \ref{prop.adj.transpose})}}}=A^{T}%
\cdot\operatorname*{adj}\left(  A^{T}\right)  =\det A\cdot I_{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.adj.inverse.short.4}%
)}\right)  .
\]
Hence,%
\begin{align*}
\left(  \underbrace{\left(  \operatorname*{adj}A\cdot A\right)  ^{T}}_{=\det
A\cdot I_{n}}\right)  ^{T}  &  =\left(  \det A\cdot I_{n}\right)  ^{T}=\det
A\cdot\underbrace{\left(  I_{n}\right)  ^{T}}_{\substack{=I_{n}\\\text{(by
(\ref{pf.thm.adj.inverse.short.tranposes2}), applied to }u=n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.adj.inverse.short.tranposes3}), applied to }u=n\text{,
}v=n\text{, }P=I_{n}\text{ and }\lambda=\det A\right) \\
&  =\det A\cdot I_{n}.
\end{align*}
Compared with
\[
\left(  \left(  \operatorname*{adj}A\cdot A\right)  ^{T}\right)
^{T}=\operatorname*{adj}A\cdot A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.adj.inverse.short.tranposes4}), applied to }u=n\text{, }v=n\text{
and }P=\operatorname*{adj}A\cdot A\right)  ,
\]
this yields $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$. Combined with
(\ref{pf.thm.adj.inverse.short.part1}), this yields%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
This proves Theorem \ref{thm.adj.inverse}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Theorem \ref{thm.adj.inverse}.]For any two objects $i$ and $j$, we
define $\delta_{i,j}$ to be the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (by the definition of $I_{n}$), and thus%
\begin{equation}
\det A\cdot\underbrace{I_{n}}_{=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}}=\det A\cdot\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{pf.thm.adj.inverse.R}%
\end{equation}


On the other hand, let us write the matrix $A$ in the form $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, the definition of the
product of two matrices shows that%
\begin{align}
A\cdot\operatorname*{adj}A  &  =\left(  \underbrace{\sum_{k=1}^{n}%
a_{i,k}\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
}_{\substack{=\sum_{q=1}^{n}a_{i,q}\left(  -1\right)  ^{q+j}\det\left(
A_{\sim j,\sim q}\right)  \\\text{(here, we renamed the summation index
}k\text{ as }q\text{)}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\\
\text{and }\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(
A_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{q=1}^{n}\underbrace{a_{i,q}\left(  -1\right)  ^{q+j}%
}_{=\left(  -1\right)  ^{q+j}a_{i,q}}\det\left(  A_{\sim j,\sim q}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  =\left(  \sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim
j,\sim q}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\label{pf.thm.adj.inverse.L}%
\end{align}


Now, we claim that%
\begin{equation}
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =\det A\cdot\delta_{i,j} \label{pf.thm.adj.inverse.twise}%
\end{equation}
for any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

\textit{Proof of (\ref{pf.thm.adj.inverse.twise}):} Fix $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Thus, $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We are in one of the
following two cases:

\textit{Case 1:} We have $i=j$.

\textit{Case 2:} We have $i\neq j$.

Let us consider Case 1 first. In this case, we have $i=j$. Hence,
$\delta_{i,j}=1$. Now, Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to
$p=i$) yields%
\[
\det A=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{i+q}}%
_{\substack{=\left(  -1\right)  ^{q+i}=\left(  -1\right)  ^{q+j}\\\text{(since
}i=j\text{)}}}a_{i,q}\det\left(  \underbrace{A_{\sim i,\sim q}}%
_{\substack{=A_{\sim j,\sim q}\\\text{(since }i=j\text{)}}}\right)
=\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  .
\]
Hence,%
\[
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =\det A=\det A\cdot\delta_{i,j}%
\]
(since $\det A\cdot\underbrace{\delta_{i,j}}_{=1}=\det A$). Thus,
(\ref{pf.thm.adj.inverse.twise}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $i\neq j$. Hence,
$\delta_{i,j}=0$ and $j\neq i$. Now, Proposition \ref{prop.laplace.0}
\textbf{(a)} (applied to $p=j$ and $r=i$) yields%
\[
0=\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{j+q}}_{=\left(  -1\right)
^{q+j}}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  =\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  .
\]
Hence,%
\[
\sum_{q=1}^{n}\left(  -1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim
q}\right)  =0=\det A\cdot\delta_{i,j}%
\]
(since $\det A\cdot\underbrace{\delta_{i,j}}_{=0}=0$). Thus,
(\ref{pf.thm.adj.inverse.twise}) is proven in Case 2.

We have now proven (\ref{pf.thm.adj.inverse.twise}) in each of the two Cases 1
and 2. Thus, (\ref{pf.thm.adj.inverse.twise}) always holds. This completes the
proof of (\ref{pf.thm.adj.inverse.twise}).

Now, (\ref{pf.thm.adj.inverse.L}) becomes%
\begin{align}
A\cdot\operatorname*{adj}A  &  =\left(  \underbrace{\sum_{q=1}^{n}\left(
-1\right)  ^{q+j}a_{i,q}\det\left(  A_{\sim j,\sim q}\right)  }%
_{\substack{=\det A\cdot\delta_{i,j}\\\text{(by
(\ref{pf.thm.adj.inverse.twise}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\nonumber\\
&  =\left(  \det A\cdot\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\det A\cdot I_{n} \label{pf.thm.adj.inverse.part1}%
\end{align}
(by (\ref{pf.thm.adj.inverse.R})).

It now remains to prove that $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$.
One way to do this is by mimicking the above proof using Theorem
\ref{thm.laplace.gen} \textbf{(b)} and Proposition \ref{prop.laplace.0}
\textbf{(b)} instead of Theorem \ref{thm.laplace.gen} \textbf{(a)} and
Proposition \ref{prop.laplace.0} \textbf{(a)}. However, here is a slicker proof:

Let us forget that we fixed $A$. We thus have shown that
(\ref{pf.thm.adj.inverse.part1}) holds for every $n\times n$-matrix $A$.

Now, let $A$ be any $n\times n$-matrix. Then, we can apply
(\ref{pf.thm.adj.inverse.part1}) to $A^{T}$ instead of $A$. We thus obtain%
\begin{equation}
A^{T}\cdot\operatorname*{adj}\left(  A^{T}\right)  =\underbrace{\det\left(
A^{T}\right)  }_{\substack{=\det A\\\text{(by Exercise \ref{exe.ps4.4})}%
}}\cdot I_{n}=\det A\cdot I_{n}. \label{pf.thm.adj.inverse.4}%
\end{equation}


But here are four fundamental properties of transposes which are all easy to check:

\begin{itemize}
\item If $u$, $v$ and $w$ are three nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $Q$ is a $v\times w$-matrix, then%
\begin{equation}
\left(  PQ\right)  ^{T}=Q^{T}P^{T}. \label{pf.thm.adj.inverse.tranposes1}%
\end{equation}


\item Every $u\in\mathbb{N}$ satisfies%
\begin{equation}
\left(  I_{u}\right)  ^{T}=I_{u}. \label{pf.thm.adj.inverse.tranposes2}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, if $P$ is a $u\times
v$-matrix, and if $\lambda\in\mathbb{K}$, then%
\begin{equation}
\left(  \lambda P\right)  ^{T}=\lambda P^{T}.
\label{pf.thm.adj.inverse.tranposes3}%
\end{equation}


\item If $u$ and $v$ are two nonnegative integers, and if $P$ is a $u\times
v$-matrix, then%
\begin{equation}
\left(  P^{T}\right)  ^{T}=P. \label{pf.thm.adj.inverse.tranposes4}%
\end{equation}

\end{itemize}

Now, (\ref{pf.thm.adj.inverse.tranposes1}) (applied to $u=n$, $v=n$, $w=n$,
$P=\operatorname*{adj}A$ and $Q=A$) shows that%
\[
\left(  \operatorname*{adj}A\cdot A\right)  ^{T}=A^{T}\cdot\underbrace{\left(
\operatorname*{adj}A\right)  ^{T}}_{\substack{=\operatorname*{adj}\left(
A^{T}\right)  \\\text{(by Proposition \ref{prop.adj.transpose})}}}=A^{T}%
\cdot\operatorname*{adj}\left(  A^{T}\right)  =\det A\cdot I_{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.adj.inverse.4})}\right)  .
\]
Hence,%
\begin{align*}
\left(  \underbrace{\left(  \operatorname*{adj}A\cdot A\right)  ^{T}}_{=\det
A\cdot I_{n}}\right)  ^{T}  &  =\left(  \det A\cdot I_{n}\right)  ^{T}=\det
A\cdot\underbrace{\left(  I_{n}\right)  ^{T}}_{\substack{=I_{n}\\\text{(by
(\ref{pf.thm.adj.inverse.tranposes2}), applied to }u=n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.adj.inverse.tranposes3}),
applied to }u=n\text{, }v=n\text{, }P=I_{n}\text{ and }\lambda=\det A\right)
\\
&  =\det A\cdot I_{n}.
\end{align*}
Compared with
\[
\left(  \left(  \operatorname*{adj}A\cdot A\right)  ^{T}\right)
^{T}=\operatorname*{adj}A\cdot A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.adj.inverse.tranposes4}), applied to }u=n\text{, }v=n\text{ and
}P=\operatorname*{adj}A\cdot A\right)  ,
\]
this yields $\operatorname*{adj}A\cdot A=\det A\cdot I_{n}$. Combined with
(\ref{pf.thm.adj.inverse.part1}), this yields%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
This proves Theorem \ref{thm.adj.inverse}.
\end{proof}
\end{verlong}

The following is a simple consequence of Theorem \ref{thm.adj.inverse}:

\begin{corollary}
\label{cor.adj.kernel}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Let $v$ be a column vector with $n$ entries. If $Av=0_{n\times1}$, then $\det
A\cdot v=0_{n\times1}$.

(Recall that $0_{n\times1}$ denotes the $n\times1$ zero matrix, i.e., the
column vector with $n$ entries whose all entries are $0$.)
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.adj.kernel}.]Assume that $Av=0_{n\times1}$. It is
easy to see that every $m\in\mathbb{N}$ and every $n\times m$-matrix $B$
satisfy $I_{n}B=B$. Applying this to $m=1$ and $B=v$, we obtain $I_{n}v=v$.

It is also easy to see that every $m\in\mathbb{N}$ and every $m\times
n$-matrix $B$ satisfy $B\cdot0_{n\times1}=0_{m\times1}$. Applying this to
$m=n$ and $B=\operatorname*{adj}A$, we obtain $\operatorname*{adj}%
A\cdot0_{n\times1}=0_{n\times1}$.

Now, Theorem \ref{thm.adj.inverse} yields $\operatorname*{adj}A\cdot A=\det
A\cdot I_{n}$. Hence,%
\[
\underbrace{\left(  \operatorname*{adj}A\cdot A\right)  }_{=\det A\cdot I_{n}%
}v=\left(  \det A\cdot I_{n}\right)  v=\det A\cdot\underbrace{\left(
I_{n}v\right)  }_{=v}=\det A\cdot v.
\]
Compared to%
\begin{align*}
\left(  \operatorname*{adj}A\cdot A\right)  v  &  =\operatorname*{adj}%
A\cdot\underbrace{\left(  Av\right)  }_{=0_{n\times1}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since matrix multiplication is
associative}\right) \\
&  =\operatorname*{adj}A\cdot0_{n\times1}=0_{n\times1},
\end{align*}
this yields $\det A\cdot v=0_{n\times1}$. This proves Corollary
\ref{cor.adj.kernel}.
\end{proof}

\begin{exercise}
\label{exe.adj(AB)}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two $n\times
n$-matrices. Prove that%
\[
\operatorname*{adj}\left(  AB\right)  =\operatorname*{adj}B\cdot
\operatorname*{adj}A.
\]

\end{exercise}

Let me end this section with another application of Proposition
\ref{prop.laplace.0}:

\begin{exercise}
\label{exe.vander-hook}Let $n\in\mathbb{N}$. For every $n$ elements
$y_{1},y_{2},\ldots,y_{n}$ of $\mathbb{K}$, we define an element $V\left(
y_{1},y_{2},\ldots,y_{n}\right)  $ of $\mathbb{K}$ by
\[
V\left(  y_{1},y_{2},\ldots,y_{n}\right)  =\prod_{1\leq i<j\leq n}\left(
y_{i}-y_{j}\right)  .
\]


Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let
$t\in\mathbb{K}$. Prove that%
\begin{align*}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =\left(  \dbinom{n}{2}t+\sum_{k=1}^{n}x_{k}\right)  V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  .
\end{align*}


[\textbf{Hint:} Use Theorem \ref{thm.vander-det}, Laplace expansion,
Proposition \ref{prop.laplace.0} and the binomial formula.]
\end{exercise}

Exercise \ref{exe.vander-hook} is part of \cite[\S 4.3, Exercise
10]{Fulton-Young}.

\subsection{Inverting matrices}

We now will study inverses of matrices. We begin with a definition:

\begin{definition}
\label{def.matrices.inverses}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A$ be an $n\times m$-matrix.

\textbf{(a)} A \textit{left inverse} of $A$ means an $m\times n$-matrix $L$
such that $LA=I_{m}$. We say that the matrix $A$ is \textit{left-invertible}
if and only if a left inverse of $A$ exists.

\textbf{(b)} A \textit{right inverse} of $A$ means an $m\times n$-matrix $R$
such that $AR=I_{n}$. We say that the matrix $A$ is \textit{right-invertible}
if and only if a right inverse of $A$ exists.

\textbf{(c)} An \textit{inverse} of $A$ (or \textit{two-sided inverse} of $A$)
means an $m\times n$-matrix $B$ such that $BA=I_{m}$ and $AB=I_{n}$. We say
that the matrix $A$ is \textit{invertible} if and only if an inverse of $A$ exists.

The notions \textquotedblleft left-invertible\textquotedblright,
\textquotedblleft right-invertible\textquotedblright\ and \textquotedblleft
invertible\textquotedblright\ depend on the ring $\mathbb{K}$. We shall
therefore speak of \textquotedblleft left-invertible over $\mathbb{K}%
$\textquotedblright, \textquotedblleft right-invertible over $\mathbb{K}%
$\textquotedblright\ and \textquotedblleft invertible over $\mathbb{K}%
$\textquotedblright\ whenever the context does not unambiguously determine
$\mathbb{K}$.
\end{definition}

The notions of \textquotedblleft left inverse\textquotedblright,
\textquotedblleft right inverse\textquotedblright\ and \textquotedblleft
inverse\textquotedblright\ are not interchangeable (unlike for elements in a
commutative ring). We shall soon see in what cases they are identical; but
first, let us give a few examples.

\begin{example}
\label{exa.matrices.inverses}For this example, set $\mathbb{K}=\mathbb{Z}$.

Let $P$ be the $1\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 2
\end{array}
\right)  $. The matrix $P$ is right-invertible. For instance, $\left(
\begin{array}
[c]{c}%
-1\\
1
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{c}%
3\\
-1
\end{array}
\right)  $ are two right inverses of $P$ (because $P\left(
\begin{array}
[c]{c}%
-1\\
1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  =I_{1}$ and $P\left(
\begin{array}
[c]{c}%
3\\
-1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1
\end{array}
\right)  =I_{1}$). This example shows that the right inverse of a matrix is
not always unique.

The $2\times1$-matrix $P^{T}=\left(
\begin{array}
[c]{c}%
1\\
2
\end{array}
\right)  $ is left-invertible. The left inverses of $P^{T}$ are the transposes
of the right inverses of $P$.

The matrix $P$ is not left-invertible; the matrix $P^{T}$ is not right-invertible.

Let $Q$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & -1\\
3 & -2
\end{array}
\right)  $. The matrix $Q$ is invertible. Its inverse is $\left(
\begin{array}
[c]{cc}%
-2 & 1\\
-3 & 1
\end{array}
\right)  $ (since $\left(
\begin{array}
[c]{cc}%
-2 & 1\\
-3 & 1
\end{array}
\right)  Q=I_{2}$ and $Q\left(
\begin{array}
[c]{cc}%
-2 & 1\\
-3 & 1
\end{array}
\right)  =I_{2}$). It is not hard to see that this is its only inverse.

Let $R$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
1 & 2\\
2 & -1
\end{array}
\right)  $. It can be seen that this matrix is not invertible \textbf{as a
matrix over }$\mathbb{Z}$. On the other hand, if we consider it as a matrix
over $\mathbb{K}=\mathbb{Q}$ instead, then it is invertible, with inverse
$\left(
\begin{array}
[c]{cc}%
1/5 & 2/5\\
2/5 & -1/5
\end{array}
\right)  $.
\end{example}

Of course, any inverse of a matrix $A$ is automatically both a left inverse of
$A$ and a right inverse of $A$. Thus, an invertible matrix $A$ is
automatically both left-invertible and right-invertible.

The following simple fact is an analogue of Proposition
\ref{prop.rings.inverse-uni}:

\begin{proposition}
\label{prop.matrices.inverse-uni}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an $n\times m$-matrix. Let $L$ be a left inverse of $A$. Let $R$ be
a right inverse of $A$.

\textbf{(a)} We have $L=R$.

\textbf{(b)} The matrix $A$ is invertible, and $L=R$ is an inverse of $A$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.matrices.inverse-uni}.]We know that $L$ is a
left inverse of $A$. In other words, $L$ is an $m\times n$-matrix such that
$LA=I_{m}$ (by the definition of a \textquotedblleft left
inverse\textquotedblright).

We know that $R$ is a right inverse of $A$. In other words, $R$ is an $m\times
n$-matrix such that $AR=I_{n}$ (by the definition of a \textquotedblleft right
inverse\textquotedblright).

Now, recall that $I_{m}G=G$ for every $k\in\mathbb{N}$ and every $m\times
k$-matrix $G$. Applying this to $k=n$ and $G=R$, we obtain $I_{m}R=R$.

Also, recall that $GI_{n}=G$ for every $k\in\mathbb{N}$ and every $k\times
n$-matrix $G$. Applying this to $k=m$ and $G=L$, we obtain $LI_{n}=L$. Thus,
$L=L\underbrace{I_{n}}_{=AR}=\underbrace{LA}_{=I_{m}}R=I_{m}R=R$. This proves
Proposition \ref{prop.matrices.inverse-uni} \textbf{(a)}.

\textbf{(b)} We have $LA=I_{m}$ and $A\underbrace{L}_{=R}=AR=I_{n}$. Thus, $L$
is an $m\times n$-matrix such that $LA=I_{m}$ and $AL=I_{n}$. In other words,
$L$ is an inverse of $A$ (by the definition of an \textquotedblleft
inverse\textquotedblright). Thus, $L=R$ is an inverse of $A$ (since $L=R$).
This proves Proposition \ref{prop.matrices.inverse-uni} \textbf{(b)}.
\end{proof}

\begin{corollary}
\label{cor.matrices.inverse-uni.cor}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an $n\times m$-matrix.

\textbf{(a)} If $A$ is left-invertible and right-invertible, then $A$ is invertible.

\textbf{(b)} If $A$ is invertible, then there exists exactly one inverse of
$A$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.matrices.inverse-uni.cor}.]\textbf{(a)} Assume
that $A$ is left-invertible and right-invertible. Thus, $A$ has a left inverse
$L$ (since $A$ is left-invertible). Consider this $L$. Also, $A$ has a right
inverse $R$ (since $A$ is right-invertible). Consider this $R$. Proposition
\ref{prop.matrices.inverse-uni} \textbf{(b)} yields that the matrix $A$ is
invertible, and $L=R$ is an inverse of $A$. Corollary
\ref{cor.matrices.inverse-uni.cor} \textbf{(a)} is proven.

\textbf{(b)} Assume that $A$ is invertible. Let $B$ and $B^{\prime}$ be any
two inverses of $A$. Since $B$ is an inverse of $A$, we know that $B$ is an
$m\times n$-matrix such that $BA=I_{m}$ and $AB=I_{n}$ (by the definition of
an \textquotedblleft inverse\textquotedblright). Thus, in particular, $B$ is
an $m\times n$-matrix such that $BA=I_{m}$. In other words, $B$ is a left
inverse of $A$. Since $B^{\prime}$ is an inverse of $A$, we know that
$B^{\prime}$ is an $m\times n$-matrix such that $B^{\prime}A=I_{m}$ and
$AB^{\prime}=I_{n}$ (by the definition of an \textquotedblleft
inverse\textquotedblright). Thus, in particular, $B^{\prime}$ is an $m\times
n$-matrix such that $AB^{\prime}=I_{n}$. In other words, $B^{\prime}$ is a
right inverse of $A$. Now, Proposition \ref{prop.matrices.inverse-uni}
\textbf{(a)} (applied to $L=B$ and $R=B^{\prime}$) shows that $B=B^{\prime}$.

Let us now forget that we fixed $B$ and $B^{\prime}$. We thus have shown that
if $B$ and $B^{\prime}$ are two inverses of $A$, then $B=B^{\prime}$. In other
words, any two inverses of $A$ are equal. In other words, there exists at most
one inverse of $A$. Since we also know that there exists at least one inverse
of $A$ (since $A$ is invertible), we thus conclude that there exists exactly
one inverse of $A$. This proves Corollary \ref{cor.matrices.inverse-uni.cor}
\textbf{(b)}.
\end{proof}

\begin{definition}
Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be an invertible $n\times
m$-matrix. Corollary \ref{cor.matrices.inverse-uni.cor} \textbf{(b)} shows
that there exists exactly one inverse of $A$. Thus, we can speak of
\textquotedblleft\textit{the inverse of }$A$\textquotedblright. We denote this
inverse by $A^{-1}$.
\end{definition}

In contrast to Definition \ref{def.rings.invertible}, we do \textbf{not}
define the notation $B/A$ for two matrices $B$ and $A$ for which $A$ is
invertible. In fact, the trouble with such a notation would be its ambiguity:
should it mean $BA^{-1}$ or $A^{-1}B$ ? (In general, $BA^{-1}$ and $A^{-1}B$
are not the same.) Some authors do write $B/A$ for the matrices $BA^{-1}$ and
$A^{-1}B$ when these matrices are equal; but we shall not have a reason to do so.

\begin{remark}
\label{rmk.matrices.inverses.AA-1}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an invertible $n\times m$-matrix. Then, the inverse $A^{-1}$ of $A$
is an $m\times n$-matrix and satisfies $AA^{-1}=I_{n}$ and $A^{-1}A=I_{m}$.
This follows from the definition of the inverse of $A$; we are just stating it
once again, because it will later be used without mention.
\end{remark}

Example \ref{exa.matrices.inverses} (and your experiences with a linear
algebra class, if you have taken one) suggest the conjecture that only square
matrices can be invertible. Indeed, this is \textbf{almost} true. There is a
stupid counterexample: If $\mathbb{K}$ is a trivial ring, then every matrix
over $\mathbb{K}$ is invertible\footnote{For example, the $1\times2$-matrix
$\left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)  $ over a trivial ring $\mathbb{K}$ is invertible, having inverse
$\left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}\\
0_{\mathbb{K}}%
\end{array}
\right)  $. If you don't believe me, just check that%
\begin{align*}
\left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}\\
0_{\mathbb{K}}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}\\
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
1_{\mathbb{K}} & 0_{\mathbb{K}}\\
0_{\mathbb{K}} & 1_{\mathbb{K}}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0_{\mathbb{K}}%
=1_{\mathbb{K}}\right) \\
&  =I_{2}%
\end{align*}
and $\left(
\begin{array}
[c]{cc}%
0_{\mathbb{K}} & 0_{\mathbb{K}}%
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}\\
0_{\mathbb{K}}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
0_{\mathbb{K}}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
1_{\mathbb{K}}%
\end{array}
\right)  =I_{1}$.}. It turns out that this is the only case where nonsquare
matrices can be invertible. Indeed, we have the following:

\begin{theorem}
\label{thm.matrices.inverses.oblong}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A$ be an $n\times m$-matrix.

\textbf{(a)} If $A$ is left-invertible and if $n<m$, then $\mathbb{K}$ is a
trivial ring.

\textbf{(b)} If $A$ is right-invertible and if $n>m$, then $\mathbb{K}$ is a
trivial ring.

\textbf{(c)} If $A$ is invertible and if $n\neq m$, then $\mathbb{K}$ is a
trivial ring.
\end{theorem}

\begin{proof}
[Proof of Theorem \ref{thm.matrices.inverses.oblong}.]\textbf{(a)} Assume that
$A$ is left-invertible, and that $n<m$.

The matrix $A$ has a left inverse $L$ (since it is left-invertible). Consider
this $L$.

We know that $L$ is a left inverse of $A$. In other words, $L$ is an $m\times
n$-matrix such that $LA=I_{m}$ (by the definition of a \textquotedblleft left
inverse\textquotedblright). But (\ref{eq.exam.cauchy-binet.0}) (applied to
$m$, $n$, $L$ and $A$ instead of $n$, $m$, $A$ and $B$) yields $\det\left(
LA\right)  =0$ (since $n<m$). Thus, $0=\det\left(  \underbrace{LA}_{=I_{m}%
}\right)  =\det\left(  I_{m}\right)  =1$. Of course, the $0$ and the $1$ in
this equality mean the elements $0_{\mathbb{K}}$ and $1_{\mathbb{K}}$ of
$\mathbb{K}$ (rather than the integers $0$ and $1$); thus, it rewrites as
$0_{\mathbb{K}}=1_{\mathbb{K}}$. In other words, $\mathbb{K}$ is a trivial
ring. This proves Theorem \ref{thm.matrices.inverses.oblong} \textbf{(a)}.

\textbf{(b)} Assume that $A$ is right-invertible, and that $n>m$.

The matrix $A$ has a right inverse $R$ (since it is right-invertible).
Consider this $R$.

We know that $R$ is a right inverse of $A$. In other words, $R$ is an $m\times
n$-matrix such that $AR=I_{n}$ (by the definition of a \textquotedblleft right
inverse\textquotedblright). But (\ref{eq.exam.cauchy-binet.0}) (applied to
$B=R$) yields $\det\left(  AR\right)  =0$ (since $m<n$). Thus, $0=\det\left(
\underbrace{AR}_{=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$. Of course,
the $0$ and the $1$ in this equality mean the elements $0_{\mathbb{K}}$ and
$1_{\mathbb{K}}$ of $\mathbb{K}$ (rather than the integers $0$ and $1$); thus,
it rewrites as $0_{\mathbb{K}}=1_{\mathbb{K}}$. In other words, $\mathbb{K}$
is a trivial ring. This proves Theorem \ref{thm.matrices.inverses.oblong}
\textbf{(b)}.

\textbf{(c)} Assume that $A$ is invertible, and that $n\neq m$. Since $n\neq
m$, we must be in one of the following two cases:

\textit{Case 1:} We have $n<m$.

\textit{Case 2:} We have $n>m$.

Let us first consider Case 1. In this case, we have $n<m$. Now, $A$ is
invertible, and thus left-invertible (since every invertible matrix is
left-invertible). Hence, $\mathbb{K}$ is a trivial ring (according to Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(a)}). Thus, Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(c)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $n>m$. Now, $A$ is
invertible, and thus right-invertible (since every invertible matrix is
right-invertible). Hence, $\mathbb{K}$ is a trivial ring (according to Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(b)}). Thus, Theorem
\ref{thm.matrices.inverses.oblong} \textbf{(c)} is proven in Case 2.

We have thus proven Theorem \ref{thm.matrices.inverses.oblong} \textbf{(c)} in
both Cases 1 and 2. Thus, Theorem \ref{thm.matrices.inverses.oblong}
\textbf{(c)} always holds.
\end{proof}

Theorem \ref{thm.matrices.inverses.oblong} \textbf{(c)} says that the question
whether a matrix is invertible is only interesting for square matrices, unless
the ring $\mathbb{K}$ is given so inexplicitly that we do not know whether it
is trivial or not\footnote{This actually happens rather often in algebra! For
example, rings are often defined by \textquotedblleft generators and
relations\textquotedblright\ (such as \textquotedblleft the ring with
commuting generators $a,b,c$ subject to the relations $a^{2}+b^{2}=c^{2}$ and
$ab=c$\textquotedblright). Sometimes the relations force the ring to become
trivial (for instance, the ring with generator $a$ and relations $a=1$ and
$a^{2}=2$ is clearly the trivial ring, because in this ring we have
$2=a^{2}=1^{2}=1$). Often this is not clear a-priori, and theorems such as
Theorem \ref{thm.matrices.inverses.oblong} can be used to show this. The
triviality of a ring can be a nontrivial statement! (Richman makes this point
in \cite{Richman}.)}. Let us now study the invertibility of a square matrix.
Here, the determinant turns out to be highly useful:

\begin{theorem}
\label{thm.matrices.inverses.square}Let $n\in\mathbb{N}$. Let $A$ be an
$n\times n$-matrix.

\textbf{(a)} The matrix $A$ is invertible if and only if the element $\det A$
of $\mathbb{K}$ is invertible (in $\mathbb{K}$).

\textbf{(b)} If $\det A$ is invertible, then the inverse of $A$ is
$A^{-1}=\dfrac{1}{\det A}\cdot\operatorname*{adj}A$.
\end{theorem}

When $\mathbb{K}$ is a field, the invertible elements of $\mathbb{K}$ are
precisely the nonzero elements of $\mathbb{K}$. Thus, when $\mathbb{K}$ is a
field, the statement of Theorem \ref{thm.matrices.inverses.square}
\textbf{(a)} can be rewritten as \textquotedblleft The matrix $A$ is
invertible if and only if $\det A\neq0$\textquotedblright; this is a
cornerstone of linear algebra. But our statement of Theorem
\ref{thm.matrices.inverses.square} \textbf{(a)} works for an arbitrary
commutative ring $\mathbb{K}$. In particular, it works for $\mathbb{K}%
=\mathbb{Z}$. Here is a consequence:

\begin{corollary}
\label{cor.matrices.inverses.square.ZZ}Let $n\in\mathbb{N}$. Let
$A\in\mathbb{Z}^{n\times n}$ be an $n\times n$-matrix over $\mathbb{Z}$. Then,
the matrix $A$ is invertible if and only if $\det A\in\left\{  1,-1\right\}  $.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.matrices.inverses.square.ZZ}.]If $g$ is an
integer, then $g$ is invertible (in $\mathbb{Z}$) if and only if $g\in\left\{
1,-1\right\}  $. In other words, for every integer $g$, we have the following
equivalence:%
\begin{equation}
\left(  g\text{ is invertible (in }\mathbb{Z}\text{)}\right)
\Longleftrightarrow\left(  g\in\left\{  1,-1\right\}  \right)  .
\label{pf.cor.matrices.inverses.square.ZZ.1}%
\end{equation}


Now, Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} (applied to
$\mathbb{K}=\mathbb{Z}$) yields that the matrix $A$ is invertible if and only
if the element $\det A$ of $\mathbb{Z}$ is invertible (in $\mathbb{Z}$). Thus,
we have the following chain of equivalences:%
\begin{align*}
&  \left(  \text{the matrix }A\text{ is invertible}\right) \\
&  \Longleftrightarrow\ \left(  \det A\text{ is invertible (in }%
\mathbb{Z}\text{)}\right)  \ \Longleftrightarrow\ \left(  \det A\in\left\{
1,-1\right\}  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.cor.matrices.inverses.square.ZZ.1}), applied to }g=\det A\right)  .
\end{align*}
This proves Corollary \ref{cor.matrices.inverses.square.ZZ}.
\end{proof}

Notice that Theorem \ref{thm.matrices.inverses.square} \textbf{(b)} yields an
explicit way to compute the inverse of a square matrix $A$ (provided that we
can compute determinants and the inverse of $\det A$). This is not the fastest
way (at least not when $\mathbb{K}$ is a field), but it is useful for various
theoretical purposes.

\begin{proof}
[Proof of Theorem \ref{thm.matrices.inverses.square}.]\textbf{(a)}
$\Longrightarrow:$\ \ \ \ \footnote{In case you don't know what the notation
\textquotedblleft$\Longrightarrow:$\textquotedblright\ here means:
\par
Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} is an
\textquotedblleft if and only if\textquotedblright\ assertion. In other words,
it asserts that $\mathcal{U}\Longleftrightarrow\mathcal{V}$ for two statements
$\mathcal{U}$ and $\mathcal{V}$. (In our case, $\mathcal{U}$ is the statement
\textquotedblleft the matrix $A$ is invertible\textquotedblright, and
$\mathcal{V}$ is the statement \textquotedblleft the element $\det A$ of
$\mathbb{K}$ is invertible (in $\mathbb{K}$)\textquotedblright.) In order to
prove a statement of the form $\mathcal{U}\Longleftrightarrow\mathcal{V}$, it
is sufficient to prove the implications $\mathcal{U}\Longrightarrow
\mathcal{V}$ and $\mathcal{U}\Longleftarrow\mathcal{V}$. Usually, these two
implications are proven separately (although not always; for instance, in the
proof of Corollary \ref{cor.matrices.inverses.square.ZZ}, we have used a chain
of equivalences to prove $\mathcal{U}\Longleftrightarrow\mathcal{V}$
directly). When writing such a proof, one often uses the abbreviations
\textquotedblleft$\Longrightarrow:$\textquotedblright\ and \textquotedblleft%
$\Longleftarrow:$\textquotedblright\ for \textquotedblleft Here comes the
proof of the implication $\mathcal{U}\Longrightarrow\mathcal{V}$%
:\textquotedblright\ and \textquotedblleft Here comes the proof of the
implication $\mathcal{U}\Longleftarrow\mathcal{V}$:\textquotedblright,
respectively.} Assume that the matrix $A$ is invertible. In other words, an
inverse $B$ of $A$ exists. Consider such a $B$.

The matrix $B$ is an inverse of $A$. In other words, $B$ is an $n\times
n$-matrix such that $BA=I_{n}$ and $AB=I_{n}$ (by the definition of an
\textquotedblleft inverse\textquotedblright). Theorem \ref{thm.det(AB)} yields
$\det\left(  AB\right)  =\det A\cdot\det B$, so that $\det A\cdot\det
B=\det\left(  \underbrace{AB}_{=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$.
Of course, we also have $\det B\cdot\det A=\det A\cdot\det B=1$. Thus, $\det
B$ is an inverse of $\det A$ in $\mathbb{K}$. Therefore, the element $\det A$
is invertible (in $\mathbb{K}$). This proves the $\Longrightarrow$ direction
of Theorem \ref{thm.matrices.inverses.square} \textbf{(a)}.

$\Longleftarrow:$ Assume that the element $\det A$ is invertible (in
$\mathbb{K}$). Thus, its inverse $\dfrac{1}{\det A}$ exists. Theorem
\ref{thm.adj.inverse} yields%
\[
A\cdot\operatorname*{adj}A=\operatorname*{adj}A\cdot A=\det A\cdot I_{n}.
\]
Now, define an $n\times n$-matrix $B$ by $B=\dfrac{1}{\det A}\cdot
\operatorname*{adj}A$. Then,%
\[
A\underbrace{B}_{=\dfrac{1}{\det A}\cdot\operatorname*{adj}A}=A\cdot\left(
\dfrac{1}{\det A}\cdot\operatorname*{adj}A\right)  =\dfrac{1}{\det A}%
\cdot\underbrace{A\cdot\operatorname*{adj}A}_{=\det A\cdot I_{n}%
}=\underbrace{\dfrac{1}{\det A}\cdot\det A}_{=1}\cdot I_{n}=I_{n}%
\]
and%
\[
\underbrace{B}_{=\dfrac{1}{\det A}\cdot\operatorname*{adj}A}A=\dfrac{1}{\det
A}\cdot\underbrace{\operatorname*{adj}A\cdot A}_{=\det A\cdot I_{n}%
}=\underbrace{\dfrac{1}{\det A}\cdot\det A}_{=1}\cdot I_{n}=I_{n}.
\]


Thus, $B$ is an $n\times n$-matrix such that $BA=I_{n}$ and $AB=I_{n}$. In
other words, $B$ is an inverse of $A$ (by the definition of an
\textquotedblleft inverse\textquotedblright). Thus, an inverse of $A$ exists;
in other words, the matrix $A$ is invertible. This proves the $\Longleftarrow$
direction of Theorem \ref{thm.matrices.inverses.square} \textbf{(a)}.

We have now proven both directions of Theorem
\ref{thm.matrices.inverses.square} \textbf{(a)}. Theorem
\ref{thm.matrices.inverses.square} \textbf{(a)} is thus proven.

\textbf{(b)} Assume that $\det A$ is invertible. Thus, its inverse $\dfrac
{1}{\det A}$ exists. We define an $n\times n$-matrix $B$ by $B=\dfrac{1}{\det
A}\cdot\operatorname*{adj}A$. Then, $B$ is an inverse of $A$%
\ \ \ \ \footnote{We have shown this in our proof of the $\Longleftarrow$
direction of Theorem \ref{thm.matrices.inverses.square} \textbf{(a)}.}. In
other words, $B$ is \textbf{the} inverse of $A$. In other words, $B=A^{-1}$.
Hence, $A^{-1}=B=\dfrac{1}{\det A}\cdot\operatorname*{adj}A$. This proves
Theorem \ref{thm.matrices.inverses.square} \textbf{(b)}.
\end{proof}

\begin{corollary}
\label{cor.matrices.inverse.AB}Let $n\in\mathbb{N}$. Let $A$ and $B$ be two
$n\times n$-matrices such that $AB=I_{n}$.

\textbf{(a)} We have $BA=I_{n}$.

\textbf{(b)} The matrix $A$ is invertible, and the matrix $B$ is the inverse
of $A$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.matrices.inverse.AB}.]Theorem \ref{thm.det(AB)}
yields $\det\left(  AB\right)  =\det A\cdot\det B$, so that $\det A\cdot\det
B=\det\left(  \underbrace{AB}_{=I_{n}}\right)  =\det\left(  I_{n}\right)  =1$.
Of course, we also have $\det B\cdot\det A=\det A\cdot\det B=1$. Thus, $\det
B$ is an inverse of $\det A$ in $\mathbb{K}$. Therefore, the element $\det A$
is invertible (in $\mathbb{K}$). Therefore, the matrix $A$ is invertible
(according to the $\Longleftarrow$ direction of Theorem
\ref{thm.matrices.inverses.square} \textbf{(b)}). Thus, the inverse of $A$
exists. Let $C$ be this inverse. Thus, $C$ is a left inverse of $A$ (since
every inverse of $A$ is a left inverse of $A$).

The matrix $B$ is an $n\times n$-matrix satisfying $AB=I_{n}$. In other words,
$B$ is a right inverse of $A$. On the other hand, $C$ is a left inverse of
$A$. Hence, Proposition \ref{prop.matrices.inverse-uni} \textbf{(a)} (applied
to $L=C$ and $R=B$) yields $C=B$. Hence, the matrix $B$ is the inverse of $A$
(since the matrix $C$ is the inverse of $A$). Thus, Corollary
\ref{cor.matrices.inverse.AB} \textbf{(b)} is proven.

Since $B$ is the inverse of $A$, we have $BA=I_{n}$ and $AB=I_{n}$ (by the
definition of an \textquotedblleft inverse\textquotedblright). This proves
Corollary \ref{cor.matrices.inverse.AB} \textbf{(a)}.
\end{proof}

\begin{remark}
Corollary \ref{cor.matrices.inverse.AB} is \textbf{not} obvious! Matrix
multiplication, in general, is not commutative (we have $AB\neq BA$ more often
than not), and there is no reason to expect that $AB=I_{n}$ implies $BA=I_{n}%
$. The fact that this is nevertheless true for square matrices took us quite
some work to prove (we needed, among other things, the notion of an adjugate).
This fact would \textbf{not} hold for rectangular matrices. Nor does it hold
for \textquotedblleft infinite square matrices\textquotedblright: Without
wanting to go into the details of how products of infinite matrices are
defined, I invite you to check that the two infinite matrices $A=\left(
\begin{array}
[c]{cccc}%
0 & 1 & 0 & \cdots\\
0 & 0 & 1 & \cdots\\
0 & 0 & 0 & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{array}
\right)  $ and $B=A^{T}=\left(
\begin{array}
[c]{cccc}%
0 & 0 & 0 & \cdots\\
1 & 0 & 0 & \cdots\\
0 & 1 & 0 & \cdots\\
\vdots & \vdots & \vdots & \ddots
\end{array}
\right)  $ satisfy $AB=I_{\infty}$ but $BA\neq I_{\infty}$. This makes
Corollary \ref{cor.matrices.inverse.AB} \textbf{(a)} all the more interesting.
\end{remark}

Here are some more exercises involving matrices in \textquotedblleft
block-matrix form\textquotedblright:

\begin{exercise}
\label{exe.block2x2.VB+WD}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times n}$, $B\in\mathbb{K}^{n\times m}$, $C\in
\mathbb{K}^{m\times n}$ and $D\in\mathbb{K}^{m\times m}$. Furthermore, let
$W\in\mathbb{K}^{m\times m}$ and $V\in\mathbb{K}^{m\times n}$ be such that
$VA=-WC$. Prove that%
\[
\det W\cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\det A\cdot\det\left(  VB+WD\right)  .
\]


[\textbf{Hint:} Use Exercise \ref{exe.block2x2.mult} to simplify the product
$\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $; then, take determinants.]
\end{exercise}

Exercise \ref{exe.block2x2.VB+WD} can often be used to compute the determinant
of a matrix given in block-matrix form (i.e., determinants of the form
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $) by only computing determinants of smaller matrices (such as $W$,
$A$ and $VB+WD$). It falls short of providing a general method for computing
such determinants\footnote{Indeed, it only gives a formula for $\det
W\cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $, not for $\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $. If $\det W$ is invertible, then it allows for computing
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $; but Exercise \ref{exe.block2x2.VB+WD} gives no hint on how to find
matrices $W$ and $V$ such that $\det W$ is invertible and such that $VA=-WC$.
(Actually, such matrices do not always exist!)}, but it is one of the most
general facts about them. The next two exercises are two special cases of
Exercise \ref{exe.block2x2.VB+WD}:

\begin{exercise}
\label{exe.block2x2.schur}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times n}$, $B\in\mathbb{K}^{n\times m}$, $C\in
\mathbb{K}^{m\times n}$ and $D\in\mathbb{K}^{m\times m}$ be such that the
matrix $A$ is invertible. Prove that%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\det A\cdot\det\left(  D-CA^{-1}B\right)  .
\]

\end{exercise}

Exercise \ref{exe.block2x2.schur} is known as the \textit{Schur complement
formula} (or, at least, it is one of several formulas sharing this name); and
the matrix $D-CA^{-1}B$ appearing on its right hand side is known as the
\textit{Schur complement} of the block $A$ in the matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $.

\begin{exercise}
\label{exe.block2x2.jacobi}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times n}$, $B\in\mathbb{K}^{n\times m}$, $C\in
\mathbb{K}^{m\times n}$ and $D\in\mathbb{K}^{m\times m}$. Let $A^{\prime}%
\in\mathbb{K}^{n\times n}$, $B^{\prime}\in\mathbb{K}^{n\times m}$, $C^{\prime
}\in\mathbb{K}^{m\times n}$ and $D^{\prime}\in\mathbb{K}^{m\times m}$. Assume
that the matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ is invertible, and that its inverse is the matrix $\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $. Prove that%
\[
\det A=\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \cdot\det\left(  D^{\prime}\right)  .
\]

\end{exercise}

Exercise \ref{exe.block2x2.jacobi} can be rewritten in the following more
handy form:

\begin{exercise}
\label{exe.block2x2.jacobi.rewr}We shall use the notations introduced in
Definition \ref{def.submatrix}.

Let $n\in\mathbb{N}$. Let $A\in\mathbb{K}^{n\times n}$ be an invertible
matrix. Let $k\in\left\{  0,1,\ldots,n\right\}  $. Prove that%
\[
\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots
,k}A\right)  =\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}\left(  A^{-1}\right)  \right)
.
\]

\end{exercise}

Exercise \ref{exe.block2x2.jacobi.rewr} is a particular case of the so-called
\textit{Jacobi complementary minor theorem} (Additional exercise
\ref{addexe.jacobi-complement} further below).

\subsection{\label{sect.noncommring}Noncommutative rings}

I think that here is a good place to introduce two other basic notions from
algebra: that of a noncommutative ring, and that of a group.

\begin{definition}
\label{def.ring}The notion of a \textit{noncommutative ring} is defined in the
same way as we have defined a commutative ring (in Definition
\ref{def.commring}), except that we no longer require the \textquotedblleft
Commutativity of multiplication\textquotedblright\ axiom.
\end{definition}

As I have already said, the word \textquotedblleft
noncommutative\textquotedblright\ (in \textquotedblleft noncommutative
ring\textquotedblright) does not mean that commutativity of multiplication has
to be false in this ring; it only means that commutativity of multiplication
is not required. Thus, every commutative ring is a noncommutative ring.
Therefore, each of the examples of a commutative ring given in Section
\ref{sect.commring} is also an example of a noncommutative ring. Of course, it
is more interesting to see some examples of noncommutative rings which
actually fail to obey commutativity of multiplication. Here are some of these examples:

\begin{itemize}
\item If $n\in\mathbb{N}$ and if $\mathbb{K}$ is a commutative ring, then the
set $\mathbb{K}^{n\times n}$ of matrices becomes a noncommutative ring (when
endowed with the addition and multiplication of matrices, with the zero
$0_{n\times n}$ and with the unity $I_{n}$). This is actually a commutative
ring when $\mathbb{K}$ is trivial or when $n\leq1$, but in all
\textquotedblleft interesting\textquotedblright\ cases it is not commutative.

\item If you have heard of \href{https://en.wikipedia.org/wiki/Quaternion}{the
quaternions}, you should realize that they form a noncommutative ring.

\item Given a commutative ring $\mathbb{K}$ and $n$ distinct symbols
$X_{1},X_{2},\ldots,X_{n}$, we can define a \textit{ring of polynomials in the
\textbf{noncommutative} variables} $X_{1},X_{2},\ldots,X_{n}$ over
$\mathbb{K}$. We do not want to go into the details of its definition at this
point, but let us just mention some examples of its elements: For instance,
the ring of polynomials in the noncommutative variables $X$ and $Y$ over
$\mathbb{Q}$ contains elements such as $1+\dfrac{2}{3}X$, $X^{2}+\dfrac{3}%
{2}Y-7XY+YX$, $2XY$, $2YX$ and $5X^{2}Y-6XYX+7Y^{2}X$ (and of course, the
elements $XY$ and $YX$ are not equal).

\item If $n\in\mathbb{N}$ and if $\mathbb{K}$ is a commutative ring, then the
set of all lower-triangular $n\times n$-matrices over $\mathbb{K}$ becomes a
noncommutative ring (with addition, multiplication, zero and unity defined in
the same way as in $\mathbb{K}^{n\times n}$). This is because the sum and the
product of any two lower-triangular $n\times n$-matrices over $\mathbb{K}$ are
again lower-triangular\footnote{Check this! (For the sum, it is clear, but for
the product, it is an instructive exercise.)}, and because the matrices
$0_{n\times n}$ and $I_{n}$ are lower-triangular.

\item In contrast, the set of all invertible $2\times2$-matrices over
$\mathbb{K}$ is \textbf{not} a noncommutative ring (for example, because the
sum of the two invertible matrices $I_{2}$ and $-I_{2}$ is not
invertible\footnote{unless the ring $\mathbb{K}$ is trivial}).

\item If $\mathbb{K}$ is a commutative ring, then the set of all $3\times
3$-matrices (over $\mathbb{K}$) of the form $\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & d & 0\\
0 & 0 & f
\end{array}
\right)  $ (with $a,b,c,d,f\in\mathbb{K}$) is a noncommutative ring (again,
with the same addition, multiplication, zero and unity as for $\mathbb{K}%
^{n\times n}$).\ \ \ \ \footnote{To check this, one needs to prove that the
matrices $0_{3\times3}$ and $I_{3}$ have this form, and that the sum and the
product of any two matrices of this form is again a matrix of this form. All
of this is clear, except for the claim about the product. The latter claim
follows from the computation%
\[
\left(
\begin{array}
[c]{ccc}%
a & b & c\\
0 & d & 0\\
0 & 0 & f
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & c^{\prime}\\
0 & d^{\prime} & 0\\
0 & 0 & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime} & bd^{\prime}+ab^{\prime} & cf^{\prime}+ac^{\prime}\\
0 & dd^{\prime} & 0\\
0 & 0 & ff^{\prime}%
\end{array}
\right)  .
\]
}

\item On the other hand, if $\mathbb{K}$ is a commutative ring, then the set
of all $3\times3$-matrices (over $\mathbb{K}$) of the form $\left(
\begin{array}
[c]{ccc}%
a & b & 0\\
0 & c & d\\
0 & 0 & f
\end{array}
\right)  $ (with $a,b,c,d,f\in\mathbb{K}$) is \textbf{not} a noncommutative
ring (unless $\mathbb{K}$ is trivial), because products of matrices in this
set are not always in this set\footnote{Indeed, $\left(
\begin{array}
[c]{ccc}%
a & b & 0\\
0 & c & d\\
0 & 0 & f
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
a^{\prime} & b^{\prime} & 0\\
0 & c^{\prime} & d^{\prime}\\
0 & 0 & f^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}%
aa^{\prime} & ab^{\prime}+bc^{\prime} & bd^{\prime}\\
0 & cc^{\prime} & cd^{\prime}+df^{\prime}\\
0 & 0 & ff^{\prime}%
\end{array}
\right)  $ can have $bd^{\prime}\neq0$.}.
\end{itemize}

For the rest of this section, we let $\mathbb{L}$ be a \textbf{noncommutative}
ring. What can we do with elements of $\mathbb{L}$ ? We can do some of the
things that we can do with a commutative ring, but not all of them. For
example, we can still define the sum $a_{1}+a_{2}+\cdots+a_{n}$ and the
product $a_{1}a_{2}\cdots a_{n}$ of $n$ elements of a noncommutative ring. But
we cannot arbitrarily reorder the factors of a product and expect to always
get the same result! (With a sum, we can do this.) We can still define $na$
for any $n\in\mathbb{Z}$ and $a\in\mathbb{L}$ (in the same way as we defined
$na$ for $n\in\mathbb{Z}$ and $a\in\mathbb{K}$ when $\mathbb{K}$ was a
commutative ring). We can still define $a^{n}$ for any $n\in\mathbb{N}$ and
$a\in\mathbb{L}$ (again, in the same fashion as for commutative rings). The
identities (\ref{eq.rings.-(a+b)}), (\ref{eq.rings.-(-a)}),
(\ref{eq.rings.-(ab)}), (\ref{eq.rings.-(na)}), (\ref{eq.rings.nab}),
(\ref{eq.rings.nma}), (\ref{eq.rings.0**n}) and (\ref{eq.rings.a**(n+m)})
still hold when the commutative ring $\mathbb{K}$ is replaced by the
noncommutative ring $\mathbb{L}$; but the identities (\ref{eq.rings.-(ab)**n})
and (\ref{eq.rings.(a+b)**n}) may not (although they \textbf{do} hold if we
additionally assume that $ab=ba$). Finite sums such as $\sum_{s\in S}a_{s}$
(where $S$ is a finite set, and $a_{s}\in\mathbb{L}$ for every $s\in S$) are
well-defined, but finite products such as $\prod_{s\in S}a_{s}$ are not
(unless we specify the order in which their factors are to be multiplied).

We can define matrices over $\mathbb{L}$ in the same way as we have defined
matrices over $\mathbb{K}$. We can even define the determinant of a square
matrix over $\mathbb{L}$ using the formula (\ref{eq.det.eq.1}); however, this
determinant lacks many of the important properties that determinants over
$\mathbb{K}$ have (for instance, it satisfies neither Exercise \ref{exe.ps4.4}
nor Theorem \ref{thm.det(AB)}), and is therefore usually not
studied.\footnote{Some algebraists have come up with subtler notions of
determinants for matrices over noncommutative rings. But I don't want to go in
that direction here.}

We define the notion of an \textit{inverse} of an element $a\in\mathbb{L}$; in
order to do so, we simply replace $\mathbb{K}$ by $\mathbb{L}$ in Definition
\ref{def.rings.inverse}. (Now it suddenly matters that we required both $ab=1$
and $ba=1$ in Definition \ref{def.rings.inverse}.) Proposition
\ref{prop.rings.inverse-uni} still holds (and its proof still works) when
$\mathbb{K}$ is replaced by $\mathbb{L}$.

We define the notion of an \textit{invertible element} of $\mathbb{L}$; in
order to do so, we simply replace $\mathbb{K}$ by $\mathbb{L}$ in Definition
\ref{def.rings.invertible} \textbf{(a)}. We cannot directly replace
$\mathbb{K}$ by $\mathbb{L}$ in Definition \ref{def.rings.invertible}
\textbf{(b)}, because for two invertible elements $a$ and $b$ of $\mathbb{L}$
we do not necessarily have $\left(  ab\right)  ^{-1}=a^{-1}b^{-1}$; but
something very similar holds (namely, $\left(  ab\right)  ^{-1}=b^{-1}a^{-1}%
$). Trying to generalize Definition \ref{def.rings.invertible} \textbf{(c)} to
noncommutative rings is rather hopeless: In general, we cannot bring a
\textquotedblleft noncommutative fraction\textquotedblright\ of the form
$ba^{-1}+dc^{-1}$ to a \textquotedblleft common denominator\textquotedblright.

\begin{example}
\label{exa.rings.invertible-matrices}Let $\mathbb{K}$ be a commutative ring.
Let $n\in\mathbb{N}$. As we know, $\mathbb{K}^{n\times n}$ is a noncommutative
ring. The invertible elements of this ring are exactly the invertible $n\times
n$-matrices. (To see this, just compare the definition of an invertible
element of $\mathbb{K}^{n\times n}$ with the definition of an invertible
$n\times n$-matrix. These definitions are clearly equivalent.)
\end{example}

\subsection{Groups, and the group of units}

Let me finally define the notion of a \textit{group}.

\begin{definition}
\label{def.group}A \textit{group} means a set $G$ endowed with

\begin{itemize}
\item a binary operation called \textquotedblleft
multiplication\textquotedblright\ (or \textquotedblleft
composition\textquotedblright, or just \textquotedblleft binary
operation\textquotedblright), and denoted by $\cdot$, and written infix, and

\item an element called $1_{G}$ (or $e_{G}$)
\end{itemize}

such that the following axioms are satisfied:

\begin{itemize}
\item \textit{Associativity:} We have $a\left(  bc\right)  =\left(  ab\right)
c$ for all $a\in G$, $b\in G$ and $c\in G$. Here and in the following, $ab$ is
shorthand for $a\cdot b$ (as is usual for products of numbers).

\item \textit{Neutrality of }$1$\textit{:} We have $a1_{G}=1_{G}a=a$ for all
$a\in G$.

\item \textit{Existence of inverses:} For every $a\in G$, there exists an
element $a^{\prime}\in G$ such that $aa^{\prime}=a^{\prime}a=1_{G}$. This
$a^{\prime}$ is commonly denoted by $a^{-1}$ and called the \textit{inverse}
of $a$. (It is easy to check that it is unique.)
\end{itemize}
\end{definition}

\begin{definition}
The element $1_{G}$ of a group $G$ is denoted the \textit{neutral element} (or
the \textit{identity}) of $G$.

The binary operation $\cdot$ in Definition \ref{def.group} is usually not
identical with the binary operation $\cdot$ on the set of integers, and is
denoted by $\cdot_{G}$ when confusion can arise.
\end{definition}

The definition of a group has similarities with that of a noncommutative ring.
Viewed from a distance, it may look as if a noncommutative ring would
\textquotedblleft consist\textquotedblright\ of two groups with the same
underlying set. This is not quite correct, though, because the multiplication
in a nontrivial ring does not satisfy the \textquotedblleft existence of
inverses\textquotedblright\ axiom. But it is true that there are two groups in
every noncommutative ring:

\begin{proposition}
\label{prop.ring.groups}Let $\mathbb{L}$ be a noncommutative ring.

\textbf{(a)} The set $\mathbb{L}$, endowed with the \textbf{addition}
$+_{\mathbb{L}}$ (as multiplication) and the element $0_{\mathbb{L}}$ (as
neutral element), is a group. This group is called the \textit{additive group}
of $\mathbb{L}$, and denoted by $\mathbb{L}^{+}$.

\textbf{(b)} Let $\mathbb{L}^{\times}$ denote the set of all invertible
elements of $\mathbb{L}$. Then, the product of two elements of $\mathbb{L}%
^{\times}$ again belongs to $\mathbb{L}^{\times}$. Thus, we can define a
binary operation $\cdot_{\mathbb{L}^{\times}}$ on the set $\mathbb{L}^{\times
}$ (written infix) by
\[
a\cdot_{\mathbb{L}^{\times}}b=ab\ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathbb{L}^{\times}\text{ and }b\in\mathbb{L}^{\times}.
\]


The set $\mathbb{L}^{\times}$, endowed with the multiplication $\cdot
_{\mathbb{L}^{\times}}$ (as multiplication) and the element $1_{\mathbb{L}}$
(as neutral element), is a group. This group is called the \textit{group of
units} of $\mathbb{L}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.ring.groups}.]\textbf{(a)} The addition
$+_{\mathbb{L}}$ is clearly a binary operation on $\mathbb{L}$, and the
element $0_{\mathbb{L}}$ is clearly an element of $\mathbb{L}$. The three
axioms in Definition \ref{def.group} are clearly satisfied for the binary
operation $+_{\mathbb{L}}$ and the element $0_{\mathbb{L}}$%
\ \ \ \ \footnote{In fact, they boil down to the \textquotedblleft
associativity of addition\textquotedblright, \textquotedblleft neutrality of
$0$\textquotedblright\ and \textquotedblleft existence of additive
inverses\textquotedblright\ axioms in the definition of a noncommutative
ring.}. Therefore, the set $\mathbb{L}$, endowed with the addition
$+_{\mathbb{L}}$ (as multiplication) and the element $0_{\mathbb{L}}$ (as
neutral element), is a group. This proves Proposition \ref{prop.ring.groups}
\textbf{(a)}.

\textbf{(b)} If $a\in\mathbb{L}^{\times}$ and $b\in\mathbb{L}^{\times}$, then
$ab\in\mathbb{L}^{\times}$\ \ \ \ \footnote{\textit{Proof.} Let $a\in
\mathbb{L}^{\times}$ and $b\in\mathbb{L}^{\times}$. We have $a\in
\mathbb{L}^{\times}$; in other words, $a$ is an invertible element of
$\mathbb{L}$ (because $\mathbb{L}^{\times}$ is the set of all invertible
elements of $\mathbb{L}$). Thus, the inverse $a^{-1}$ of $a$ is well-defined.
Similarly, the inverse $b^{-1}$ of $b$ is well-defined. Now, since we have%
\[
\left(  b^{-1}a^{-1}\right)  \left(  ab\right)  =b^{-1}\underbrace{a^{-1}%
a}_{=1_{\mathbb{L}}}b=b^{-1}b=1_{\mathbb{L}}%
\]
and%
\[
\left(  ab\right)  \left(  b^{-1}a^{-1}\right)  =a\underbrace{bb^{-1}%
}_{=1_{\mathbb{L}}}a^{-1}=aa^{-1}=1_{\mathbb{L}},
\]
we see that the element $b^{-1}a^{-1}$ of $\mathbb{L}$ is an inverse of $ab$.
Thus, the element $ab$ has an inverse. In other words, $ab$ is invertible. In
other words, $ab\in\mathbb{L}^{\times}$ (since $\mathbb{L}^{\times}$ is the
set of all invertible elements of $\mathbb{L}$), qed.}. In other words, the
product of two elements of $\mathbb{L}^{\times}$ again belongs to
$\mathbb{L}^{\times}$. Thus, we can define a binary operation $\cdot
_{\mathbb{L}^{\times}}$ on the set $\mathbb{L}^{\times}$ (written infix) by
\[
a\cdot_{\mathbb{L}^{\times}}b=ab\ \ \ \ \ \ \ \ \ \ \text{for all }%
a\in\mathbb{L}^{\times}\text{ and }b\in\mathbb{L}^{\times}.
\]
Also, $1_{\mathbb{L}}$ is an invertible element of $\mathbb{L}$ (indeed, its
inverse is $1_{\mathbb{L}}$), and thus an element of $\mathbb{L}^{\times}$.

Now, we need to prove that the set $\mathbb{L}^{\times}$, endowed with the
multiplication $\cdot_{\mathbb{L}^{\times}}$ (as multiplication) and the
element $1_{\mathbb{L}}$ (as neutral element), is a group. In order to do so,
we need to check that the \textquotedblleft associativity\textquotedblright,
\textquotedblleft neutrality of $1$\textquotedblright\ and \textquotedblleft
existence of inverses\textquotedblright\ axioms are satisfied.

The \textquotedblleft associativity\textquotedblright\ axiom follows from the
\textquotedblleft associativity of multiplication\textquotedblright\ axiom in
the definition of a noncommutative ring. The \textquotedblleft neutrality of
$1$\textquotedblright\ axiom follows from the \textquotedblleft
unitality\textquotedblright\ axiom in the definition of a noncommutative ring.
It thus remains to prove that the \textquotedblleft existence of
inverses\textquotedblright\ axiom holds.

Thus, let $a\in\mathbb{L}^{\times}$. We need to show that there exists an
$a^{\prime}\in\mathbb{L}^{\times}$ such that $a\cdot_{\mathbb{L}^{\times}%
}a^{\prime}=a^{\prime}\cdot_{\mathbb{L}^{\times}}a=1_{\mathbb{L}}$ (since
$1_{\mathbb{L}}$ is the neutral element of $\mathbb{L}^{\times}$).

We know that $a$ is an invertible element of $\mathbb{L}$ (sin${}$ce
$a\in\mathbb{L}^{\times}$); it thus has an inverse $a^{-1}$. Now, $a$ itself
is an inverse of $a^{-1}$ (since $aa^{-1}=1_{\mathbb{L}}$ and $a^{-1}%
a=1_{\mathbb{L}}$), and thus the element $a^{-1}$ of $\mathbb{L}$ has an
inverse. In other words, $a^{-1}$ is invertible, so that $a^{-1}\in
\mathbb{L}^{\times}$. The definition of the operation $\cdot_{\mathbb{L}%
^{\times}}$ shows that $a\cdot_{\mathbb{L}^{\times}}a^{-1}=aa^{-1}%
=1_{\mathbb{L}}$ and that $a^{-1}\cdot_{\mathbb{L}^{\times}}a=a^{-1}%
a=1_{\mathbb{L}}$. Hence, there exists an $a^{\prime}\in\mathbb{L}^{\times}$
such that $a\cdot_{\mathbb{L}^{\times}}a^{\prime}=a^{\prime}\cdot
_{\mathbb{L}^{\times}}a=1_{\mathbb{L}}$ (namely, $a^{\prime}=a^{-1}$). Thus we
have proven that the \textquotedblleft existence of inverses\textquotedblright%
\ axiom holds. The proof of Proposition \ref{prop.ring.groups} \textbf{(b)} is
thus complete.
\end{proof}

We now have a plentitude of examples of groups: For every noncommutative ring
$\mathbb{L}$, we have the two groups $\mathbb{L}^{+}$ and $\mathbb{L}^{\times
}$ defined in Proposition \ref{prop.ring.groups}. Another example, for every
set $X$, is the symmetric group of $X$ (endowed with the composition of
permutations as multiplication, and the identity permutation
$\operatorname*{id}:X\rightarrow X$ as the neutral element). (Many other
examples can be found in textbooks on algebra, such as \cite{Artin}.)

\begin{remark}
Throwing all notational ballast aside, we can restate Proposition
\ref{prop.ring.groups} \textbf{(b)} as follows: The set of all invertible
elements of a noncommutative ring $\mathbb{L}$ is a group (where the binary
operation is multiplication). We can apply this to the case where
$\mathbb{L}=\mathbb{K}^{n\times n}$ for a commutative ring $\mathbb{K}$ and an
integer $n\in\mathbb{N}$. Thus, we obtain that the set of all invertible
elements of $\mathbb{K}^{n\times n}$ is a group. Since we know that the
invertible elements of $\mathbb{K}^{n\times n}$ are exactly the invertible
$n\times n$-matrices (by Example \ref{exa.rings.invertible-matrices}), we thus
have shown that the set of all invertible $n\times n$-matrices is a group.
This group is commonly denoted by $\operatorname*{GL}\nolimits_{n}\left(
\mathbb{K}\right)  $; it is called the \textit{general linear group of degree
}$n$ over $\mathbb{K}$.
\end{remark}

\subsection{Cramer's rule}

Let us return to the classical properties of determinants. We have already
proven many, but here is one more: It is an application of determinants to
solving systems of linear equations.

\begin{theorem}
\label{thm.cramer}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let
$b=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}$ be a column vector with $n$
entries (that is, an $n\times1$-matrix).\footnotemark

For every $j\in\left\{  1,2,\ldots,n\right\}  $, let $A_{j}^{\#}$ be the
$n\times n$-matrix obtained from $A$ by replacing the $j$-th column of $A$
with the vector $b$.

\textbf{(a)} We have $A\cdot\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}=\det A\cdot b$.

\textbf{(b)} Assume that the matrix $A$ is invertible. Then,%
\[
A^{-1}b=\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}.
\]

\end{theorem}

\footnotetext{The reader should keep in mind that $\left(  b_{1},b_{2}%
,\ldots,b_{n}\right)  ^{T}$ is just a space-saving way to write $\left(
\begin{array}
[c]{c}%
b_{1}\\
b_{2}\\
\vdots\\
b_{n}%
\end{array}
\right)  $.}Theorem \ref{thm.cramer} (or either part of it) is known as
\textit{Cramer's rule}.

\begin{remark}
A system of $n$ linear equations in $n$ variables $x_{1},x_{2},\ldots,x_{n}$
can be written in the form $Ax=b$, where $A$ is a fixed $n\times n$-matrix and
$b$ is a column vector with $n$ entries (and where $x$ is the column vector
$\left(  x_{1},x_{2},\ldots,x_{n}\right)  ^{T}$ containing all the variables).
When the matrix $A$ is invertible, it thus has a unique solution: namely,
$x=A^{-1}b$ (just multiply the equation $Ax=b$ from the left with $A^{-1}$ to
see this), and this solution can be computed using Theorem \ref{thm.cramer}.
This looks nice, but isn't actually all that useful for solving systems of
linear equations: For one thing, this does not immediately help us solve
systems of fewer or more than $n$ equations in $n$ variables; and even in the
case of exactly $n$ equations, the matrix $A$ coming from a system of linear
equations will not always be invertible (and in the more interesting cases, it
will not be). For another thing, at least when $\mathbb{K}$ is a field, there
are faster ways to solve a system of linear equations than anything that
involves computing $n+1$ determinants of $n\times n$-matrices. Theorem
\ref{thm.cramer} nevertheless turns out to be useful in proofs.
\end{remark}

\begin{vershort}
\begin{proof}
[Proof of Theorem \ref{thm.cramer}.]\textbf{(a)} Fix $j\in\left\{
1,2,\ldots,n\right\}  $. Let $C=A_{j}^{\#}$. Thus, $C=A_{j}^{\#}$ is the
$n\times n$-matrix obtained from $A$ by replacing the $j$-th column of $A$
with the vector $b$. In particular, the $j$-th column of $C$ is the vector
$b$. In other words, we have
\begin{equation}
c_{p,j}=b_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,n\right\}  . \label{pf.thm.cramer.short.cpj}%
\end{equation}


Furthermore, the matrix $C$ is equal to the matrix $A$ in all columns but its
$j$-th column (because it is obtained from $A$ by replacing the $j$-th column
of $A$ with the vector $b$). Thus, if we cross out the $j$-th columns in both
matrices $C$ and $A$, then these two matrices become equal. Consequently,%
\begin{equation}
C_{\sim p,\sim j}=A_{\sim p,\sim j}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  \label{pf.thm.cramer.short.CA}%
\end{equation}
(because the matrices $C_{\sim p,\sim j}$ and $A_{\sim p,\sim j}$ are obtained
by crossing out the $p$-th row and the $j$-th column in the matrices $C$ and
$A$, respectively). Now,%
\begin{align}
\det\left(  \underbrace{A_{j}^{\#}}_{=C}\right)   &  =\det C=\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+j}\underbrace{c_{p,j}}_{\substack{=b_{p}\\\text{(by
(\ref{pf.thm.cramer.short.cpj}))}}}\det\left(  \underbrace{C_{\sim p,\sim j}%
}_{\substack{=A_{\sim p,\sim j}\\\text{(by (\ref{pf.thm.cramer.short.CA}))}%
}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.laplace.gen} \textbf{(b)}, applied}\\
\text{to }C\text{, }c_{i,j}\text{ and }j\text{ instead of }A\text{, }%
a_{i,j}\text{ and }q
\end{array}
\right) \nonumber\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}\det\left(  A_{\sim p,\sim
j}\right)  . \label{pf.thm.cramer.short.detAshj}%
\end{align}
Let us now forget that we fixed $j$. We thus have proven
(\ref{pf.thm.cramer.short.detAshj}) for every $j\in\left\{  1,2,\ldots
,n\right\}  $. Now, fix $i\in\left\{  1,2,\ldots,n\right\}  $. Then, for every
$p\in\left\{  1,2,\ldots,n\right\}  $ satisfying $p\neq i$, we have%
\begin{equation}
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =0 \label{pf.thm.cramer.short.termkiller1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.short.termkiller1}):} Let
$p\in\left\{  1,2,\ldots,n\right\}  $ be such that $p\neq i$. Hence,
Proposition \ref{prop.laplace.0} \textbf{(a)} (applied to $r=i$) shows that%
\begin{equation}
0=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  . \label{pf.thm.cramer.short.termkiller1.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =b_{p}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}%
a_{i,q}\det\left(  A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.short.termkiller1.pf.1}))}}}=0.
\]
Thus, (\ref{pf.thm.cramer.short.termkiller1}) is proven.}. Also, we have%
\begin{equation}
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =\det A\cdot b_{i} \label{pf.thm.cramer.short.termkiller2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.short.termkiller2}):} Applying
Theorem \ref{thm.laplace.gen} \textbf{(a)} to $p=i$, we obtain%
\begin{equation}
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim
i,\sim q}\right)  . \label{pf.thm.cramer.short.termkiller2.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =b_{i}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}%
a_{i,q}\det\left(  A_{\sim i,\sim q}\right)  }_{\substack{=\det A\\\text{(by
(\ref{pf.thm.cramer.short.termkiller2.pf.1}))}}}=b_{i}\det A=\det A\cdot
b_{i}.
\]
This proves (\ref{pf.thm.cramer.short.termkiller2}).}. Now,%
\begin{align}
&  \sum_{k=1}^{n}a_{i,k}\underbrace{\det\left(  A_{k}^{\#}\right)
}_{\substack{=\sum_{p=1}^{n}\left(  -1\right)  ^{p+k}b_{p}\det\left(  A_{\sim
p,\sim k}\right)  \\\text{(by (\ref{pf.thm.cramer.short.detAshj}), applied to
}j=k\text{)}}}\nonumber\\
&  =\sum_{k=1}^{n}a_{i,k}\sum_{p=1}^{n}\left(  -1\right)  ^{p+k}b_{p}%
\det\left(  A_{\sim p,\sim k}\right)  =\sum_{q=1}^{n}a_{i,q}\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+q}b_{p}\det\left(  A_{\sim p,\sim q}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }q\text{ in the first sum}\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{n}\sum_{p=1}^{n}}_{\substack{=\sum_{p=1}^{n}%
\sum_{q=1}^{n}\\=\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{q=1}^{n}%
}}\underbrace{a_{i,q}\left(  -1\right)  ^{p+q}b_{p}}_{=b_{p}\left(  -1\right)
^{p+q}a_{i,q}}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{q=1}^{n}b_{p}\left(
-1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq i}%
}\underbrace{\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.short.termkiller1}))}}}+\underbrace{\sum_{q=1}^{n}%
b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim q}\right)
}_{\substack{=\det A\cdot b_{i}\\\text{(by
(\ref{pf.thm.cramer.short.termkiller2}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}p=i\text{ from the sum}\right) \nonumber\\
&  =\underbrace{\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq
i}}0}_{=0}+\det A\cdot b_{i}=\det A\cdot b_{i}.
\label{pf.thm.cramer.short.almostthere}%
\end{align}
Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.thm.cramer.short.almostthere}) for every $i\in\left\{  1,2,\ldots
,n\right\}  $. Now, let $d$ be the vector $\left(  \det\left(  A_{1}%
^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}%
^{\#}\right)  \right)  ^{T}$. Thus,%
\begin{align*}
d  &  =\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\left(
\begin{array}
[c]{c}%
\det\left(  A_{1}^{\#}\right) \\
\det\left(  A_{2}^{\#}\right) \\
\vdots\\
\det\left(  A_{n}^{\#}\right)
\end{array}
\right) \\
&  =\left(  \det\left(  A_{i}^{\#}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq1}.
\end{align*}
The definition of the product of two matrices shows that%
\begin{align*}
A\cdot d  &  =\left(  \underbrace{\sum_{k=1}^{n}a_{i,k}\det\left(  A_{k}%
^{\#}\right)  }_{\substack{=\det A\cdot b_{i}\\\text{(by
(\ref{pf.thm.cramer.short.almostthere}))}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }d=\left(  \det\left(  A_{i}^{\#}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\right) \\
&  =\left(  \det A\cdot b_{i}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}=\left(
\det A\cdot b_{1},\det A\cdot b_{2},\ldots,\det A\cdot b_{n}\right)  ^{T}.
\end{align*}
Comparing this with%
\[
\det A\cdot\underbrace{b}_{=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}%
}=\det A\cdot\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}=\left(  \det A\cdot
b_{1},\det A\cdot b_{2},\ldots,\det A\cdot b_{n}\right)  ^{T},
\]
we obtain $A\cdot d=\det A\cdot b$. Since $d=\left(  \det\left(  A_{1}%
^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}%
^{\#}\right)  \right)  ^{T}$, we can rewrite this as $A\cdot\left(
\det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots
,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\det A\cdot b$. This proves
Theorem \ref{thm.cramer} \textbf{(a)}.

\textbf{(b)} Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} shows
that the matrix $A$ is invertible if and only if the element $\det A$ of
$\mathbb{K}$ is invertible (in $\mathbb{K}$). Hence, the element $\det A$ of
$\mathbb{K}$ is invertible (since the matrix $A$ is invertible). Thus,
$\dfrac{1}{\det A}$ is well-defined. Clearly, $\underbrace{\dfrac{1}{\det
A}\cdot\det A}_{=1}\cdot b=b$, so that%
\begin{align*}
b  &  =\dfrac{1}{\det A}\cdot\underbrace{\det A\cdot b}_{\substack{=A\cdot
\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}\\\text{(by Theorem
\ref{thm.cramer} \textbf{(a)})}}}\\
&  =\dfrac{1}{\det A}\cdot A\cdot\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}\\
&  =A\cdot\underbrace{\left(  \dfrac{1}{\det A}\cdot\left(  \det\left(
A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(
A_{n}^{\#}\right)  \right)  ^{T}\right)  }_{\substack{=\left(  \dfrac{1}{\det
A}\det\left(  A_{1}^{\#}\right)  ,\dfrac{1}{\det A}\det\left(  A_{2}%
^{\#}\right)  ,\ldots,\dfrac{1}{\det A}\det\left(  A_{n}^{\#}\right)  \right)
^{T}\\=\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}}}\\
&  =A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
Therefore,
\begin{align*}
&  A^{-1}\underbrace{b}_{=A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)
}{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac
{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}}\\
&  =\underbrace{A^{-1}A}_{=I_{n}}\cdot\left(  \dfrac{\det\left(  A_{1}%
^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A}%
,\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}\\
&  =I_{n}\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A}%
,\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(
A_{n}^{\#}\right)  }{\det A}\right)  ^{T}=\left(  \dfrac{\det\left(
A_{1}^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det
A},\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
This proves Theorem \ref{thm.cramer} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Theorem \ref{thm.cramer}.]\textbf{(a)} Fix $j\in\left\{
1,2,\ldots,n\right\}  $. Let $C=A_{j}^{\#}$.

We know that $A_{j}^{\#}$ is the $n\times n$-matrix obtained from $A$ by
replacing the $j$-th column of $A$ with the vector $b$. In other words, $C$ is
the $n\times n$-matrix obtained from $A$ by replacing the $j$-th column of $A$
with the vector $b$ (because $C=A_{j}^{\#}$). In other words, we have%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th column of }C\right)  =\left(
\text{the }u\text{-th column of }A\right)  \right.  \label{pf.thm.cramer.C.1}%
\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq j\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }j\text{-th column of }C\right)  =b.
\label{pf.thm.cramer.C.2}%
\end{equation}


Let us write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every $u\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th column of }A\right)  =\left(  a_{1,u}%
,a_{2,u},\ldots,a_{n,u}\right)  ^{T}. \label{pf.thm.cramer.u-th-col-A}%
\end{equation}


On the other hand, let us write the $n\times n$-matrix $C$ in the form
$C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, for every
$u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th column of }C\right)  =\left(  c_{1,u}%
,c_{2,u},\ldots,c_{n,u}\right)  ^{T}. \label{pf.thm.cramer.u-th-col-C}%
\end{equation}


Now, it is easy to see that
\begin{equation}
c_{p,j}=b_{p}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,n\right\}  \label{pf.thm.cramer.cpj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.cpj}):} Applying
(\ref{pf.thm.cramer.u-th-col-C}) to $u=j$, we obtain
\[
\left(  \text{the }j\text{-th column of }C\right)  =\left(  c_{1,j}%
,c_{2,j},\ldots,c_{n,j}\right)  ^{T}.
\]
Thus,%
\begin{align*}
\left(  c_{1,j},c_{2,j},\ldots,c_{n,j}\right)  ^{T}  &  =\left(  \text{the
}j\text{-th column of }C\right)  =b\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.thm.cramer.C.2})}\right) \\
&  =\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}.
\end{align*}
Thus,%
\[
\left(  c_{1,j},c_{2,j},\ldots,c_{n,j}\right)  =\left(  \underbrace{\left(
c_{1,j},c_{2,j},\ldots,c_{n,j}\right)  ^{T}}_{=\left(  b_{1},b_{2}%
,\ldots,b_{n}\right)  ^{T}}\right)  ^{T}=\left(  \left(  b_{1},b_{2}%
,\ldots,b_{n}\right)  ^{T}\right)  ^{T}=\left(  b_{1},b_{2},\ldots
,b_{n}\right)  .
\]
In other words, $c_{p,j}=b_{p}$ for every $p\in\left\{  1,2,\ldots,n\right\}
$. This proves (\ref{pf.thm.cramer.cpj}).}. Furthermore,%
\begin{equation}
c_{p,q}=a_{p,q}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \text{ and }q\in\left\{  1,2,\ldots,n\right\}  \text{
satisfying }q\neq j \label{pf.thm.cramer.cpq}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.cpq}):} Let $q\in\left\{
1,2,\ldots,n\right\}  $ be such that $q\neq j$. We need to prove that
$c_{p,q}=a_{p,q}$ for every $p\in\left\{  1,2,\ldots,n\right\}  $.
\par
Applying (\ref{pf.thm.cramer.u-th-col-C}) to $u=q$, we obtain
\[
\left(  \text{the }q\text{-th column of }C\right)  =\left(  c_{1,q}%
,c_{2,q},\ldots,c_{n,q}\right)  ^{T}.
\]
Thus,%
\begin{align*}
\left(  c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  ^{T}  &  =\left(  \text{the
}q\text{-th column of }C\right) \\
&  =\left(  \text{the }q\text{-th column of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.cramer.C.1}), applied to
}u=q\right) \\
&  =\left(  a_{1,q},a_{2,q},\ldots,a_{n,q}\right)  ^{T}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.cramer.u-th-col-A}), applied
to }u=q\right)  .
\end{align*}
Thus,%
\[
\left(  c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  =\left(  \underbrace{\left(
c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  ^{T}}_{=\left(  a_{1,q},a_{2,q}%
,\ldots,a_{n,q}\right)  ^{T}}\right)  ^{T}=\left(  \left(  a_{1,q}%
,a_{2,q},\ldots,a_{n,q}\right)  ^{T}\right)  ^{T}=\left(  a_{1,q}%
,a_{2,q},\ldots,a_{n,q}\right)  .
\]
In other words, $c_{p,q}=a_{p,q}$ for every $p\in\left\{  1,2,\ldots
,n\right\}  $. This proves (\ref{pf.thm.cramer.cpq}).}. Now, it is easy to see
that%
\begin{equation}
C_{\sim p,\sim j}=A_{\sim p,\sim j}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  \label{pf.thm.cramer.CA}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.CA}):} Let $p\in\left\{
1,2,\ldots,n\right\}  $.
\par
Let $\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{j},\ldots,n\right)  $. Thus,
$\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{j}%
,\ldots,n\right)  $, so that $\left\{  u_{1},u_{2},\ldots,u_{n-1}\right\}
=\left\{  1,2,\ldots,\widehat{j},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  j\right\}  $.
\par
Now, let $y\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $u_{y}\in\left\{
u_{1},u_{2},\ldots,u_{n-1}\right\}  =\left\{  1,2,\ldots,n\right\}
\setminus\left\{  j\right\}  $, so that $u_{y}\neq j$. Hence,%
\begin{equation}
c_{p,u_{y}}=a_{p,u_{y}}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \label{pf.thm.cramer.CA.pf.1}%
\end{equation}
(by (\ref{pf.thm.cramer.cpq}), applied to $q=u_{y}$).
\par
Let us now forget that we fixed $y$. We thus have shown that
(\ref{pf.thm.cramer.CA.pf.1}) holds for every $y\in\left\{  1,2,\ldots
,n-1\right\}  $.
\par
Let $\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  $ denote the $\left(
n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{p},\ldots,n\right)  $. Thus,
$\left(  v_{1},v_{2},\ldots,v_{n-1}\right)  =\left(  1,2,\ldots,\widehat{p}%
,\ldots,n\right)  $.
\par
Now, the definition of $C_{\sim p,\sim j}$ yields%
\begin{align*}
C_{\sim p,\sim j}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{j},\ldots,n}C=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{u_{1},u_{2},\ldots,u_{n-1}}C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{j}%
,\ldots,n\right)  =\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1}%
,u_{2},\ldots,u_{n-1}}C\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  v_{1},v_{2},\ldots
,v_{n-1}\right)  \right) \\
&  =\left(  \underbrace{c_{v_{x},u_{y}}}_{\substack{=a_{v_{x},u_{y}%
}\\\text{(by (\ref{pf.thm.cramer.CA.pf.1}),}\\\text{applied to }%
p=v_{x}\text{)}}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1},u_{2},\ldots,u_{n-1}}C\text{,
since }C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{v_{x},u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}.
\end{align*}
Compared with%
\begin{align*}
A_{\sim p,\sim j}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,n}^{1,2,\ldots,\widehat{j},\ldots,n}A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,n}^{u_{1},u_{2},\ldots,u_{n-1}}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{j}%
,\ldots,n\right)  =\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1}%
,u_{2},\ldots,u_{n-1}}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
1,2,\ldots,\widehat{p},\ldots,n\right)  =\left(  v_{1},v_{2},\ldots
,v_{n-1}\right)  \right) \\
&  =\left(  a_{v_{x},u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq n-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{v_{1},v_{2},\ldots,v_{n-1}}^{u_{1},u_{2},\ldots,u_{n-1}}A\text{,
since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  ,
\end{align*}
this yields $C_{\sim p,\sim j}=A_{\sim p,\sim j}$. This proves
(\ref{pf.thm.cramer.CA}).}.

Now,%
\begin{align}
\det\left(  \underbrace{A_{j}^{\#}}_{=C}\right)   &  =\det C=\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+j}\underbrace{c_{p,j}}_{\substack{=b_{p}\\\text{(by
(\ref{pf.thm.cramer.cpj}))}}}\det\left(  \underbrace{C_{\sim p,\sim j}%
}_{\substack{=A_{\sim p,\sim j}\\\text{(by (\ref{pf.thm.cramer.CA}))}}}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.laplace.gen} \textbf{(b)}, applied to }C\text{,
}c_{i,j}\text{ and }j\\
\text{instead of }A\text{, }a_{i,j}\text{ and }q
\end{array}
\right) \nonumber\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}\det\left(  A_{\sim p,\sim
j}\right)  . \label{pf.thm.cramer.detAshj}%
\end{align}


Let us now forget that we fixed $j$. We thus have proven
(\ref{pf.thm.cramer.detAshj}) for every $j\in\left\{  1,2,\ldots,n\right\}  $.

Now, fix $i\in\left\{  1,2,\ldots,n\right\}  $. Then, for every $p\in\left\{
1,2,\ldots,n\right\}  $ satisfying $p\neq i$, we have%
\begin{equation}
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =0 \label{pf.thm.cramer.termkiller1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.termkiller1}):} Let
$p\in\left\{  1,2,\ldots,n\right\}  $ be such that $p\neq i$. Hence,
Proposition \ref{prop.laplace.0} \textbf{(a)} (applied to $r=i$) shows that%
\begin{equation}
0=\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  . \label{pf.thm.cramer.termkiller1.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right)  =b_{p}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}%
a_{i,q}\det\left(  A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.termkiller1.pf.1}))}}}=0.
\]
Thus, (\ref{pf.thm.cramer.termkiller1}) is proven.}. Also, we have%
\begin{equation}
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =\det A\cdot b_{i} \label{pf.thm.cramer.termkiller2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.cramer.termkiller2}):} Applying
Theorem \ref{thm.laplace.gen} \textbf{(a)} to $p=i$, we obtain%
\begin{equation}
\det A=\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim
i,\sim q}\right)  . \label{pf.thm.cramer.termkiller2.pf.1}%
\end{equation}
Now,%
\[
\sum_{q=1}^{n}b_{i}\left(  -1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim
q}\right)  =b_{i}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{i+q}%
a_{i,q}\det\left(  A_{\sim i,\sim q}\right)  }_{\substack{=\det A\\\text{(by
(\ref{pf.thm.cramer.termkiller2.pf.1}))}}}=b_{i}\det A=\det A\cdot b_{i}.
\]
This proves (\ref{pf.thm.cramer.termkiller2}).}. Now,%

\begin{align}
&  \sum_{k=1}^{n}a_{i,k}\det\left(  A_{k}^{\#}\right) \nonumber\\
&  =\sum_{j=1}^{n}a_{i,j}\underbrace{\det\left(  A_{j}^{\#}\right)
}_{\substack{=\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}\det\left(  A_{\sim
p,\sim j}\right)  \\\text{(by (\ref{pf.thm.cramer.detAshj}))}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}k\text{ as }j\right) \nonumber\\
&  =\sum_{j=1}^{n}a_{i,j}\sum_{p=1}^{n}\left(  -1\right)  ^{p+j}b_{p}%
\det\left(  A_{\sim p,\sim j}\right)  =\sum_{q=1}^{n}a_{i,q}\sum_{p=1}%
^{n}\left(  -1\right)  ^{p+q}b_{p}\det\left(  A_{\sim p,\sim q}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}j\text{ as }q\text{ in the first sum}\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{n}\sum_{p=1}^{n}}_{=\sum_{p=1}^{n}\sum_{q=1}^{n}%
}\underbrace{a_{i,q}\left(  -1\right)  ^{p+q}b_{p}}_{=b_{p}\left(  -1\right)
^{p+q}a_{i,q}}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\underbrace{\sum_{p=1}^{n}}_{=\sum_{p\in\left\{  1,2,\ldots,n\right\}  }%
}\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim
q}\right) \nonumber\\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\sum_{q=1}^{n}b_{p}\left(
-1\right)  ^{p+q}a_{i,q}\det\left(  A_{\sim p,\sim q}\right) \nonumber\\
&  =\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq i}%
}\underbrace{\sum_{q=1}^{n}b_{p}\left(  -1\right)  ^{p+q}a_{i,q}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.thm.cramer.termkiller1}))}}}+\underbrace{\sum_{q=1}^{n}b_{i}\left(
-1\right)  ^{i+q}a_{i,q}\det\left(  A_{\sim i,\sim q}\right)  }%
_{\substack{=\det A\cdot b_{i}\\\text{(by (\ref{pf.thm.cramer.termkiller2}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}p=i\text{ from the sum}\right) \nonumber\\
&  =\underbrace{\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq
i}}0}_{=0}+\det A\cdot b_{i}=\det A\cdot b_{i}.
\label{pf.thm.cramer.almostthere}%
\end{align}


Now, let us forget that we fixed $i$. We thus have proven
(\ref{pf.thm.cramer.almostthere}) for every $i\in\left\{  1,2,\ldots
,n\right\}  $.

Now, let $d$ be the vector $\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}$. Thus,%
\begin{align*}
d  &  =\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\left(
\begin{array}
[c]{c}%
\det\left(  A_{1}^{\#}\right) \\
\det\left(  A_{2}^{\#}\right) \\
\vdots\\
\det\left(  A_{n}^{\#}\right)
\end{array}
\right) \\
&  =\left(  \det\left(  A_{i}^{\#}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq1}.
\end{align*}


The definition of the product of two matrices shows that%
\begin{align*}
A\cdot d  &  =\left(  \underbrace{\sum_{k=1}^{n}a_{i,k}\det\left(  A_{k}%
^{\#}\right)  }_{\substack{=\det A\cdot b_{i}\\\text{(by
(\ref{pf.thm.cramer.almostthere}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }d=\left(  \det\left(  A_{i}^{\#}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\right) \\
&  =\left(  \det A\cdot b_{i}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\end{align*}
Comparing this with%
\begin{align*}
\det A\cdot\underbrace{b}_{=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}}  &
=\det A\cdot\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}\\
&  =\left(  \underbrace{\det A\cdot\left(  b_{1},b_{2},\ldots,b_{n}\right)
}_{\substack{=\left(  \det A\cdot b_{1},\det A\cdot b_{2},\ldots,\det A\cdot
b_{n}\right)  \\=\left(  \det A\cdot b_{j}\right)  _{1\leq i\leq1,\ 1\leq
j\leq n}}}\right)  ^{T}\\
&  =\left(  \left(  \det A\cdot b_{j}\right)  _{1\leq i\leq1,\ 1\leq j\leq
n}\right)  ^{T}=\left(  \det A\cdot b_{i}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of the transpose of a
matrix}\right)  ,
\end{align*}
we obtain $A\cdot d=\det A\cdot b$. Since $d=\left(  \det\left(  A_{1}%
^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}%
^{\#}\right)  \right)  ^{T}$, we can rewrite this as $A\cdot\left(
\det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots
,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\det A\cdot b$. This proves
Theorem \ref{thm.cramer} \textbf{(a)}.

\textbf{(b)} Theorem \ref{thm.matrices.inverses.square} \textbf{(a)} shows
that the matrix $A$ is invertible if and only if the element $\det A$ of
$\mathbb{K}$ is invertible (in $\mathbb{K}$). Hence, the element $\det A$ of
$\mathbb{K}$ is invertible (since the matrix $A$ is invertible). Thus,
$\dfrac{1}{\det A}$ is well-defined. Clearly, $\underbrace{\dfrac{1}{\det
A}\cdot\det A}_{=1}\cdot b=b$, so that%
\begin{align*}
b  &  =\dfrac{1}{\det A}\cdot\underbrace{\det A\cdot b}_{\substack{=A\cdot
\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}\\\text{(by Theorem
\ref{thm.cramer} \textbf{(a)})}}}\\
&  =\dfrac{1}{\det A}\cdot A\cdot\left(  \det\left(  A_{1}^{\#}\right)
,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)
\right)  ^{T}\\
&  =A\cdot\underbrace{\left(  \dfrac{1}{\det A}\cdot\left(  \det\left(
A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots,\det\left(
A_{n}^{\#}\right)  \right)  ^{T}\right)  }_{\substack{=\left(  \dfrac{1}{\det
A}\det\left(  A_{1}^{\#}\right)  ,\dfrac{1}{\det A}\det\left(  A_{2}%
^{\#}\right)  ,\ldots,\dfrac{1}{\det A}\det\left(  A_{n}^{\#}\right)  \right)
^{T}\\=\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}}}\\
&  =A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A},\dfrac
{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(  A_{n}%
^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
Therefore,
\begin{align*}
&  A^{-1}\underbrace{b}_{=A\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)
}{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac
{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}}\\
&  =\underbrace{A^{-1}A}_{=I_{n}}\cdot\left(  \dfrac{\det\left(  A_{1}%
^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A}%
,\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}\\
&  =I_{n}\cdot\left(  \dfrac{\det\left(  A_{1}^{\#}\right)  }{\det A}%
,\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det A},\ldots,\dfrac{\det\left(
A_{n}^{\#}\right)  }{\det A}\right)  ^{T}=\left(  \dfrac{\det\left(
A_{1}^{\#}\right)  }{\det A},\dfrac{\det\left(  A_{2}^{\#}\right)  }{\det
A},\ldots,\dfrac{\det\left(  A_{n}^{\#}\right)  }{\det A}\right)  ^{T}.
\end{align*}
This proves Theorem \ref{thm.cramer} \textbf{(b)}.
\end{proof}
\end{verlong}

\subsection{\label{sect.desnanot}The Desnanot-Jacobi identity}

We now move towards more exotic places. In this section\footnote{which,
unfortunately, has become more technical and tedious than it promised to be --
for which I apologize}, we shall prove the \textit{Desnanot-Jacobi identity},
also known as \textit{Lewis Carroll identity}\footnote{See \cite[\S 3.5]%
{Bresso99} for the history of this identity (as well as for a proof different
from ours, and for an application). In a nutshell: Desnanot discovered it in
1819; Jacobi proved it in 1833 (and again in 1841); in 1866, Charles Lutwidge
Dodgson (better known as Lewis Carroll, although his mathematical works were
not printed under this pen name) popularized it by publishing an algorithm for
evaluating determinants that made heavy use of this identity.}. We will need
some notations to state this identity in the generality I want; but first I
shall state the two best known particular cases (from which the general
version can actually be easily derived, although this is not the way I will
take):\footnote{We shall use the notations of Definition
\ref{def.submatrix.minor} throughout this section.}

\begin{proposition}
\label{prop.desnanot.1n}Let $n\in\mathbb{N}$ be such that $n\geq2$. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times
n$-matrix. Let $A^{\prime}$ be the $\left(  n-2\right)  \times\left(
n-2\right)  $-matrix $\left(  a_{i+1,j+1}\right)  _{1\leq i\leq n-2,\ 1\leq
j\leq n-2}$. (In other words, $A^{\prime}$ is what remains of the matrix $A$
when we remove the first row, the last row, the first column and the last
column.) Then,%
\begin{align*}
&  \det A\cdot\det\left(  A^{\prime}\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim n,\sim
n}\right)  -\det\left(  A_{\sim1,\sim n}\right)  \cdot\det\left(  A_{\sim
n,\sim1}\right)  .
\end{align*}

\end{proposition}

\begin{proposition}
\label{prop.desnanot.12}Let $n\in\mathbb{N}$ be such that $n\geq2$. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times
n$-matrix. Let $\widetilde{A}$ be the $\left(  n-2\right)  \times\left(
n-2\right)  $-matrix $\left(  a_{i+2,j+2}\right)  _{1\leq i\leq n-2,\ 1\leq
j\leq n-2}$. (In other words, $\widetilde{A}$ is what remains of the matrix
$A$ when we remove the first two rows and the first two columns.) Then,%
\begin{align*}
&  \det A\cdot\det\widetilde{A}\\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim2,\sim
2}\right)  -\det\left(  A_{\sim1,\sim2}\right)  \cdot\det\left(  A_{\sim
2,\sim1}\right)  .
\end{align*}

\end{proposition}

\begin{example}
For this example, set $n=4$ and $A=\left(
\begin{array}
[c]{cccc}%
a_{1} & b_{1} & c_{1} & d_{1}\\
a_{2} & b_{2} & c_{2} & d_{2}\\
a_{3} & b_{3} & c_{3} & d_{3}\\
a_{4} & b_{4} & c_{4} & d_{4}%
\end{array}
\right)  $. Then, Proposition \ref{prop.desnanot.1n} says that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
a_{1} & b_{1} & c_{1} & d_{1}\\
a_{2} & b_{2} & c_{2} & d_{2}\\
a_{3} & b_{3} & c_{3} & d_{3}\\
a_{4} & b_{4} & c_{4} & d_{4}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
b_{2} & c_{2}\\
b_{3} & c_{3}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{ccc}%
b_{2} & c_{2} & d_{2}\\
b_{3} & c_{3} & d_{3}\\
b_{4} & c_{4} & d_{4}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{ccc}%
a_{1} & b_{1} & c_{1}\\
a_{2} & b_{2} & c_{2}\\
a_{3} & b_{3} & c_{3}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{ccc}%
a_{2} & b_{2} & c_{2}\\
a_{3} & b_{3} & c_{3}\\
a_{4} & b_{4} & c_{4}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{ccc}%
b_{1} & c_{1} & d_{1}\\
b_{2} & c_{2} & d_{2}\\
b_{3} & c_{3} & d_{3}%
\end{array}
\right)  .
\end{align*}
Meanwhile, Proposition \ref{prop.desnanot.12} says that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
a_{1} & b_{1} & c_{1} & d_{1}\\
a_{2} & b_{2} & c_{2} & d_{2}\\
a_{3} & b_{3} & c_{3} & d_{3}\\
a_{4} & b_{4} & c_{4} & d_{4}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
c_{3} & d_{3}\\
c_{4} & d_{4}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{ccc}%
b_{2} & c_{2} & d_{2}\\
b_{3} & c_{3} & d_{3}\\
b_{4} & c_{4} & d_{4}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{ccc}%
a_{1} & c_{1} & d_{1}\\
a_{3} & c_{3} & d_{3}\\
a_{4} & c_{4} & d_{4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{ccc}%
a_{2} & c_{2} & d_{2}\\
a_{3} & c_{3} & d_{3}\\
a_{4} & c_{4} & d_{4}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{ccc}%
b_{1} & c_{1} & d_{1}\\
b_{3} & c_{3} & d_{3}\\
b_{4} & c_{4} & d_{4}%
\end{array}
\right)  .
\end{align*}

\end{example}

Proposition \ref{prop.desnanot.1n} occurs (for instance) in
\cite[\textit{(Alice)}]{zeilberger-twotime}, in \cite[Theorem 3.12]{Bresso99},
in \cite[Example 3.3]{Gill15} and in \cite[Proposition 10]{Krattenthaler}
(without a proof, but with a brief list of applications). Proposition
\ref{prop.desnanot.12} occurs (among other places) in \cite[(1)]{BerBru08}
(with a generalization). The reader can easily see that Proposition
\ref{prop.desnanot.12} is equivalent to Proposition \ref{prop.desnanot.1n}%
\footnote{Indeed, one is easily obtained from the other by switching the
$2$-nd and the $n$-th rows of the matrix $A$ and switching the $2$-nd and the
$n$-th columns of the matrix $A$, applying parts \textbf{(a)} and \textbf{(b)}
of Exercise \ref{exe.ps4.6} and checking that all signs cancel.}; we shall
prove a generalization of both.

Let me now introduce some notations:\footnote{Recall that we are using the
notations of Definition \ref{def.rowscols}, of Definition \ref{def.hat-omit},
and of Definition \ref{def.submatrix.minor}.}

\begin{definition}
\label{def.unrows2}Let $n\in\mathbb{N}$. Let $r$ and $s$ be two elements of
$\left\{  1,2,\ldots,n\right\}  $ such that $r<s$. Then, $\left(
1,2,\ldots,\widehat{r},\ldots,\widehat{s},\ldots,n\right)  $ will denote the
$\left(  n-2\right)  $-tuple
\[
\left(  \underbrace{1,2,\ldots,r-1}_{\substack{\text{all integers}\\\text{from
}1\text{ to }r-1}},\underbrace{r+1,r+2,\ldots,s-1}_{\substack{\text{all
integers}\\\text{from }r+1\text{ to }s-1}},\underbrace{s+1,s+2,\ldots
,n}_{\substack{\text{all integers}\\\text{from }s+1\text{ to }n}}\right)  .
\]
In other words, $\left(  1,2,\ldots,\widehat{r},\ldots,\widehat{s}%
,\ldots,n\right)  $ will denote the result of removing the entries $r$ and $s$
from the $n$-tuple $\left(  1,2,\ldots,n\right)  $.
\end{definition}

We can now state a more general version of the Desnanot-Jacobi identity:

\begin{theorem}
\label{thm.desnanot}Let $n\in\mathbb{N}$ be such that $n\geq2$. Let $p$, $q$,
$u$ and $v$ be four elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$p<q$ and $u<v$. Let $A$ be an $n\times n$-matrix. Then,%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{p},\ldots,\widehat{q},\ldots,n}^{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}A\right) \\
&  =\det\left(  A_{\sim p,\sim u}\right)  \cdot\det\left(  A_{\sim q,\sim
v}\right)  -\det\left(  A_{\sim p,\sim v}\right)  \cdot\det\left(  A_{\sim
q,\sim u}\right)  .
\end{align*}

\end{theorem}

\begin{example}
If we set $n=3$, $p=1$, $q=2$, $u=1$, $v=3$ and $A=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  $, then Theorem \ref{thm.desnanot} says that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
c^{\prime}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{cc}%
b^{\prime} & b^{\prime\prime}\\
c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  -\det\left(
\begin{array}
[c]{cc}%
b & b^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
a^{\prime} & a^{\prime\prime}\\
c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  .
\end{align*}

\end{example}

Before we prove this theorem, let me introduce some more notations:

\begin{definition}
\label{def.unrows}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times m}$ be an $n\times m$-matrix.

\textbf{(a)} For every $u\in\left\{  1,2,\ldots,n\right\}  $, we let
$A_{u,\bullet}$ be the $u$-th row of the matrix $A$. This $A_{u,\bullet}$ is
thus a row vector with $m$ entries, i.e., a $1\times m$-matrix.

\textbf{(b)} For every $v\in\left\{  1,2,\ldots,m\right\}  $, we let
$A_{\bullet,v}$ be the $v$-th column of the matrix $A$. This $A_{\bullet,v}$
is thus a column vector with $n$ entries, i.e., an $n\times1$-matrix.

\textbf{(c)} For every $u\in\left\{  1,2,\ldots,n\right\}  $, we set $A_{\sim
u,\bullet}=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A$.
This $A_{\sim u,\bullet}$ is thus an $\left(  n-1\right)  \times m$-matrix.
(In more intuitive terms, the definition of $A_{\sim u,\bullet}$ rewrites as
follows: $A_{\sim u,\bullet}$ is the matrix obtained from the matrix $A$ by
removing the $u$-th row.)

\textbf{(d)} For every $v\in\left\{  1,2,\ldots,m\right\}  $, we set
$A_{\bullet,\sim v}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v}%
,\ldots,m}A$. This $A_{\bullet,\sim v}$ is thus an $n\times\left(  m-1\right)
$-matrix. (In more intuitive terms, the definition of $A_{\bullet,\sim v}$
rewrites as follows: $A_{\bullet,\sim v}$ is the matrix obtained from the
matrix $A$ by removing the $v$-th column.)
\end{definition}

\begin{example}
\label{exam.unrows}If $n=3$, $m=4$ and $A=\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
a^{\prime} & b^{\prime} & c^{\prime} & d^{\prime}\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime} & d^{\prime\prime}%
\end{array}
\right)  $, then%
\begin{align*}
A_{2,\bullet}  &  =\left(
\begin{array}
[c]{cccc}%
a^{\prime} & b^{\prime} & c^{\prime} & d^{\prime}%
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ A_{\bullet,2}=\left(
\begin{array}
[c]{c}%
b\\
b^{\prime}\\
b^{\prime\prime}%
\end{array}
\right)  ,\\
A_{\sim2,\bullet}  &  =\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
a^{\prime\prime} & b^{\prime\prime} & c^{\prime\prime} & d^{\prime\prime}%
\end{array}
\right)  ,\ \ \ \ \ \ \ \ \ \ A_{\bullet,\sim2}=\left(
\begin{array}
[c]{ccc}%
a & c & d\\
a^{\prime} & c^{\prime} & d^{\prime}\\
a^{\prime\prime} & c^{\prime\prime} & d^{\prime\prime}%
\end{array}
\right)  .
\end{align*}

\end{example}

Here are some simple properties of the notations introduced in Definition
\ref{def.unrows}:

\begin{proposition}
\label{prop.unrows.basics}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times m}$ be an $n\times m$-matrix.

\textbf{(a)} For every $u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
A_{u,\bullet}=\left(  \text{the }u\text{-th row of the matrix }A\right)
=\operatorname*{rows}\nolimits_{u}A.
\]
(Here, the notation $\operatorname*{rows}\nolimits_{u}A$ is a particular case
of Definition \ref{def.rowscols} \textbf{(a)}.)

\textbf{(b)} For every $v\in\left\{  1,2,\ldots,m\right\}  $, we have%
\[
A_{\bullet,v}=\left(  \text{the }v\text{-th column of the matrix }A\right)
=\operatorname*{cols}\nolimits_{v}A.
\]


\textbf{(c)} For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,m\right\}  $, we have%
\[
\left(  A_{\bullet,\sim v}\right)  _{\sim u,\bullet}=\left(  A_{\sim
u,\bullet}\right)  _{\bullet,\sim v}=A_{\sim u,\sim v}.
\]


\textbf{(d)} For every $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
1,2,\ldots,v-1\right\}  $, we have $\left(  A_{\bullet,\sim v}\right)
_{\bullet,w}=A_{\bullet,w}$.

\textbf{(e)} For every $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
v,v+1,\ldots,m-1\right\}  $, we have $\left(  A_{\bullet,\sim v}\right)
_{\bullet,w}=A_{\bullet,w+1}$.

\textbf{(f)} For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
1,2,\ldots,u-1\right\}  $, we have $\left(  A_{\sim u,\bullet}\right)
_{w,\bullet}=A_{w,\bullet}$.

\textbf{(g)} For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
u,u+1,\ldots,n-1\right\}  $, we have $\left(  A_{\sim u,\bullet}\right)
_{w,\bullet}=A_{w+1,\bullet}$.

\textbf{(h)} For every $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
1,2,\ldots,v-1\right\}  $, we have%
\[
\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v},\ldots,m}A.
\]


\textbf{(i)} For every $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
v,v+1,\ldots,m-1\right\}  $, we have%
\[
\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w+1},\ldots,m}A.
\]


\textbf{(j)} For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
1,2,\ldots,u-1\right\}  $, we have%
\[
\left(  A_{\sim u,\bullet}\right)  _{\sim w,\bullet}=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{u},\ldots,n}A.
\]


\textbf{(k)} For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
u,u+1,\ldots,n-1\right\}  $, we have%
\[
\left(  A_{\sim u,\bullet}\right)  _{\sim w,\bullet}=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{w+1},\ldots,n}A.
\]


\textbf{(l)} For every $v\in\left\{  1,2,\ldots,n\right\}  $, $u\in\left\{
1,2,\ldots,n\right\}  $ and $q\in\left\{  1,2,\ldots,m\right\}  $ satisfying
$u<v$, we have%
\[
\left(  A_{\sim v,\bullet}\right)  _{\sim u,\sim q}=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
A_{\bullet,\sim q}\right)  .
\]

\end{proposition}

\begin{proposition}
\label{prop.unrows.basics-I}Let $n\in\mathbb{N}$. Let $u$ and $v$ be two
elements of $\left\{  1,2,\ldots,n\right\}  $ such that $u<v$. Then, $\left(
\left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}=\left(
I_{n-1}\right)  _{\bullet,u}$. (Recall that $I_{m}$ denotes the $m\times m$
identity matrix for each $m\in\mathbb{N}$.)
\end{proposition}

\begin{exercise}
\label{exe.unrows.basics}\textbf{(a)} Prove Proposition
\ref{prop.unrows.basics} and Proposition \ref{prop.unrows.basics-I}.

\textbf{(b)} Derive Proposition \ref{prop.desnanot.12} and Proposition
\ref{prop.desnanot.1n} from Theorem \ref{thm.desnanot}.
\end{exercise}

Here comes another piece of notation:

\begin{definition}
\label{def.addcol}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times m}$ be an $n\times m$-matrix. Let $v\in
\mathbb{K}^{n\times1}$ be a column vector with $n$ entries. Then, $\left(
A\mid v\right)  $ will denote the $n\times\left(  m+1\right)  $-matrix whose
$m+1$ columns are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$ (from
left to right). (Informally speaking, $\left(  A\mid v\right)  $ is the matrix
obtained when the column vector $v$ is \textquotedblleft
attached\textquotedblright\ to $A$ at the right edge.)
\end{definition}

\begin{example}
\label{exam.addcol}We have $\left(  \left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  \mid\left(
\begin{array}
[c]{c}%
p\\
q
\end{array}
\right)  \right)  =\left(
\begin{array}
[c]{ccc}%
a & b & p\\
c & d & q
\end{array}
\right)  $.
\end{example}

The following properties of the notation introduced in Definition
\ref{def.addcol} are not too hard to see (see the solution of Exercise
\ref{exe.prop.addcol.props} for their proofs), and will be used below:

\begin{proposition}
\label{prop.addcol.props1}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times m}$ be an $n\times m$-matrix. Let $v\in
\mathbb{K}^{n\times1}$ be a column vector with $n$ entries.

\textbf{(a)} Every $q\in\left\{  1,2,\ldots,m\right\}  $ satisfies $\left(
A\mid v\right)  _{\bullet,q}=A_{\bullet,q}$.

\textbf{(b)} We have $\left(  A\mid v\right)  _{\bullet,m+1}=v$.

\textbf{(c)} Every $q\in\left\{  1,2,\ldots,m\right\}  $ satisfies $\left(
A\mid v\right)  _{\bullet,\sim q}=\left(  A_{\bullet,\sim q}\mid v\right)  $.

\textbf{(d)} We have $\left(  A\mid v\right)  _{\bullet,\sim\left(
m+1\right)  }=A$.

\textbf{(e)} We have $\left(  A\mid v\right)  _{\sim p,\bullet}=\left(
A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  $ for every $p\in\left\{
1,2,\ldots,n\right\}  $.

\textbf{(f)} We have $\left(  A\mid v\right)  _{\sim p,\sim\left(  m+1\right)
}=A_{\sim p,\bullet}$ for every $p\in\left\{  1,2,\ldots,n\right\}  $.
\end{proposition}

\begin{proposition}
\label{prop.addcol.props2}Let $n$ be a positive integer. Let $A\in
\mathbb{K}^{n\times\left(  n-1\right)  }$.

\textbf{(a)} For every $v=\left(  v_{1},v_{2},\ldots,v_{n}\right)  ^{T}%
\in\mathbb{K}^{n\times1}$, we have%
\[
\det\left(  A\mid v\right)  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}v_{i}%
\det\left(  A_{\sim i,\bullet}\right)  .
\]


\textbf{(b)} For every $p\in\left\{  1,2,\ldots,n\right\}  $, we have
\[
\det\left(  A\mid\left(  I_{n}\right)  _{\bullet,p}\right)  =\left(
-1\right)  ^{n+p}\det\left(  A_{\sim p,\bullet}\right)  .
\]
(Notice that $\left(  I_{n}\right)  _{\bullet,p}$ is the $p$-th column of the
$n\times n$ identity matrix, i.e., the column vector $\left(
\underbrace{0,0,\ldots,0}_{p-1\text{ zeroes}},1,\underbrace{0,0,\ldots
,0}_{n-p\text{ zeroes}}\right)  ^{T}$.)
\end{proposition}

\begin{proposition}
\label{prop.addcol.props3}Let $n\in\mathbb{N}$. Let $A\in\mathbb{K}^{n\times
n}$.

\textbf{(a)} If $n>0$, then $\left(  A_{\bullet,\sim n}\mid A_{\bullet
,n}\right)  =A$.

\textbf{(b)} For every $q\in\left\{  1,2,\ldots,n\right\}  $, we have
$\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,q}\right)  =\left(  -1\right)
^{n+q}\det A$.

\textbf{(c)} If $r$ and $q$ are two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $r\neq q$, then $\det\left(  A_{\bullet,\sim q}\mid
A_{\bullet,r}\right)  =0$.

\textbf{(d)} For every $p\in\left\{  1,2,\ldots,n\right\}  $ and $q\in\left\{
1,2,\ldots,n\right\}  $, we have $\det\left(  A_{\bullet,\sim q}\mid\left(
I_{n}\right)  _{\bullet,p}\right)  =\left(  -1\right)  ^{n+p}\det\left(
A_{\sim p,\sim q}\right)  $.

\textbf{(e)} If $u$ and $v$ are two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $u<v$, and if $r$ is an element of $\left\{
1,2,\ldots,n-1\right\}  $ satisfying $r\neq u$, then $\det\left(
A_{\bullet,\sim u}\mid\left(  A_{\bullet,\sim v}\right)  _{\bullet,r}\right)
=0$.

\textbf{(f)} If $u$ and $v$ are two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $u<v$, then $\left(  -1\right)  ^{u}\det\left(
A_{\bullet,\sim u}\mid\left(  A_{\bullet,\sim v}\right)  _{\bullet,u}\right)
=\left(  -1\right)  ^{n}\det A$.
\end{proposition}

\begin{exercise}
\label{exe.prop.addcol.props}Prove Proposition \ref{prop.addcol.props1},
Proposition \ref{prop.addcol.props2} and Proposition \ref{prop.addcol.props3}.
\end{exercise}

Now, we can state a slightly more interesting identity:

\begin{proposition}
\label{prop.desnanot.AC}Let $n$ be a positive integer. Let $A\in
\mathbb{K}^{n\times\left(  n-1\right)  }$ and $C\in\mathbb{K}^{n\times n}$.
Let $v\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\[
\det\left(  A_{\sim v,\bullet}\right)  \det C=\sum_{q=1}^{n}\left(  -1\right)
^{n+q}\det\left(  A\mid C_{\bullet,q}\right)  \det\left(  C_{\sim v,\sim
q}\right)  .
\]

\end{proposition}

\begin{example}
\label{exam.desnanot.AC}If we set $n=3$, $A=\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  $, $C=\left(
\begin{array}
[c]{ccc}%
x & x^{\prime} & x^{\prime\prime}\\
y & y^{\prime} & y^{\prime\prime}\\
z & z^{\prime} & z^{\prime\prime}%
\end{array}
\right)  $ and $v=2$, then Proposition \ref{prop.desnanot.AC} states that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{ccc}%
x & x^{\prime} & x^{\prime\prime}\\
y & y^{\prime} & y^{\prime\prime}\\
z & z^{\prime} & z^{\prime\prime}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x\\
b & b^{\prime} & y\\
c & c^{\prime} & z
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
x^{\prime} & x^{\prime\prime}\\
z^{\prime} & z^{\prime\prime}%
\end{array}
\right)  -\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x^{\prime}\\
b & b^{\prime} & y^{\prime}\\
c & c^{\prime} & z^{\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
x & x^{\prime\prime}\\
z & z^{\prime\prime}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x^{\prime\prime}\\
b & b^{\prime} & y^{\prime\prime}\\
c & c^{\prime} & z^{\prime\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
x & x^{\prime}\\
z & z^{\prime}%
\end{array}
\right)  .
\end{align*}

\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.desnanot.AC}.]Write the $n\times n$-matrix $C$
in the form $C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

Fix $q\in\left\{  1,2,\ldots,n\right\}  $. Then, $C_{\bullet,q}$ is the $q$-th
column of the matrix $C$ (by the definition of $C_{\bullet,q}$). Thus,%
\begin{align*}
C_{\bullet,q}  &  =\left(  \text{the }q\text{-th column of the matrix
}C\right) \\
&  =\left(
\begin{array}
[c]{c}%
c_{1,q}\\
c_{2,q}\\
\vdots\\
c_{n,q}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }C=\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  c_{1,q},c_{2,q},\ldots,c_{n,q}\right)  ^{T}.
\end{align*}
Now, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to
$C_{\bullet,q}$ and $c_{i,q}$ instead of $v$ and $v_{i}$) yields
\begin{align}
\det\left(  A\mid C_{\bullet,q}\right)   &  =\sum_{i=1}^{n}\left(  -1\right)
^{n+i}c_{i,q}\det\left(  A_{\sim i,\bullet}\right) \nonumber\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{n+p}c_{p,q}\det\left(  A_{\sim
p,\bullet}\right)  \label{pf.prop.desnanot.AC.2}%
\end{align}
(here, we have renamed the summation index $i$ as $p$).

Now, forget that we fixed $q$. We thus have proven
(\ref{pf.prop.desnanot.AC.2}) for each $q\in\left\{  1,2,\ldots,n\right\}  $.

We have%
\begin{equation}
\sum_{q=1}^{n}\left(  -1\right)  ^{v+q}c_{v,q}\det\left(  C_{\sim v,\sim
q}\right)  =\det C \label{pf.prop.desnanot.AC.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.desnanot.AC.4}):} Theorem
\ref{thm.laplace.gen} (applied to $C$, $c_{i,j}$ and $v$ instead of $A$,
$a_{i,j}$ and $p$) yields%
\begin{equation}
\det C=\sum_{q=1}^{n}\left(  -1\right)  ^{v+q}c_{v,q}\det\left(  C_{\sim
v,\sim q}\right)  . \label{pf.prop.desnanot.AC.4.pf.1}%
\end{equation}
This proves (\ref{pf.prop.desnanot.AC.4}).}.

On the other hand, every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$p\neq v$ satisfies%
\begin{equation}
\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}c_{p,q}\det\left(  C_{\sim v,\sim
q}\right)  =0 \label{pf.prop.desnanot.AC.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.desnanot.AC.3}):} Let $p\in\left\{
1,2,\ldots,n\right\}  $ be such that $p\neq v$. Thus, $v\neq p$. Hence,
Proposition \ref{prop.laplace.0} \textbf{(a)} (applied to $C$, $c_{i,j}$, $p$
and $v$ instead of $A$, $a_{i,j}$, $r$ and $p$) yields%
\begin{equation}
0=\sum_{q=1}^{n}\left(  -1\right)  ^{v+q}c_{p,q}\det\left(  C_{\sim v,\sim
q}\right)  . \label{pf.prop.desnanot.AC.3.pf.1}%
\end{equation}
Now, every $q\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align*}
\left(  -1\right)  ^{p+q}  &  =\left(  -1\right)  ^{\left(  p-v\right)
+\left(  v+q\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }p+q=\left(
p-v\right)  +\left(  v+q\right)  \right) \\
&  =\left(  -1\right)  ^{p-v}\left(  -1\right)  ^{v+q}.
\end{align*}
Hence,%
\begin{align*}
&  \sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{p+q}}_{=\left(  -1\right)
^{p-v}\left(  -1\right)  ^{v+q}}c_{p,q}\det\left(  C_{\sim v,\sim q}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p-v}\left(  -1\right)  ^{v+q}%
c_{p,q}\det\left(  C_{\sim v,\sim q}\right)  =\left(  -1\right)
^{p-v}\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{v+q}c_{p,q}\det\left(
C_{\sim v,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.prop.desnanot.AC.3.pf.1}))}}}=0.
\end{align*}
This proves (\ref{pf.prop.desnanot.AC.3}).}.

Now,%
\begin{align*}
&  \sum_{q=1}^{n}\left(  -1\right)  ^{n+q}\underbrace{\det\left(  A\mid
C_{\bullet,q}\right)  }_{\substack{=\sum_{p=1}^{n}\left(  -1\right)
^{n+p}c_{p,q}\det\left(  A_{\sim p,\bullet}\right)  \\\text{(by
(\ref{pf.prop.desnanot.AC.2}))}}}\det\left(  C_{\sim v,\sim q}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{n+q}\left(  \sum_{p=1}^{n}\left(
-1\right)  ^{n+p}c_{p,q}\det\left(  A_{\sim p,\bullet}\right)  \right)
\det\left(  C_{\sim v,\sim q}\right) \\
&  =\underbrace{\sum_{q=1}^{n}\sum_{p=1}^{n}}_{=\sum_{p=1}^{n}\sum_{q=1}^{n}%
}\underbrace{\left(  -1\right)  ^{n+q}\left(  -1\right)  ^{n+p}}%
_{\substack{=\left(  -1\right)  ^{\left(  n+q\right)  +\left(  n+p\right)
}=\left(  -1\right)  ^{p+q}\\\text{(since }\left(  n+q\right)  +\left(
n+p\right)  =2n+p+q\equiv p+q\operatorname{mod}2\text{)}}}c_{p,q}\det\left(
A_{\sim p,\bullet}\right)  \det\left(  C_{\sim v,\sim q}\right) \\
&  =\sum_{p=1}^{n}\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}c_{p,q}\det\left(
A_{\sim p,\bullet}\right)  \det\left(  C_{\sim v,\sim q}\right) \\
&  =\underbrace{\sum_{p=1}^{n}}_{=\sum_{p\in\left\{  1,2,\ldots,n\right\}  }%
}\det\left(  A_{\sim p,\bullet}\right)  \sum_{q=1}^{n}\left(  -1\right)
^{p+q}c_{p,q}\det\left(  C_{\sim v,\sim q}\right) \\
&  =\sum_{p\in\left\{  1,2,\ldots,n\right\}  }\det\left(  A_{\sim p,\bullet
}\right)  \sum_{q=1}^{n}\left(  -1\right)  ^{p+q}c_{p,q}\det\left(  C_{\sim
v,\sim q}\right) \\
&  =\det\left(  A_{\sim v,\bullet}\right)  \underbrace{\sum_{q=1}^{n}\left(
-1\right)  ^{v+q}c_{v,q}\det\left(  C_{\sim v,\sim q}\right)  }%
_{\substack{=\det C\\\text{(by (\ref{pf.prop.desnanot.AC.4}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{p\in\left\{  1,2,\ldots,n\right\}
;\\p\neq v}}\det\left(  A_{\sim p,\bullet}\right)  \underbrace{\sum_{q=1}%
^{n}\left(  -1\right)  ^{p+q}c_{p,q}\det\left(  C_{\sim v,\sim q}\right)
}_{\substack{=0\\\text{(by (\ref{pf.prop.desnanot.AC.3}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }p=v\text{ from the sum,}\\
\text{since }v\in\left\{  1,2,\ldots,n\right\}
\end{array}
\right) \\
&  =\det\left(  A_{\sim v,\bullet}\right)  \det C+\underbrace{\sum
_{\substack{p\in\left\{  1,2,\ldots,n\right\}  ;\\p\neq v}}\det\left(  A_{\sim
p,\bullet}\right)  0}_{=0}=\det\left(  A_{\sim v,\bullet}\right)  \det C.
\end{align*}
This proves Proposition \ref{prop.desnanot.AC}.
\end{proof}

Now, let us show a purely technical lemma (gathering a few equalities for easy
access in a proof further down):

\begin{lemma}
\label{lem.desnanot.AB.tech}Let $n$ be a positive integer. Let $B\in
\mathbb{K}^{n\times\left(  n-1\right)  }$. Let $u$ and $v$ be two elements of
$\left\{  1,2,\ldots,n\right\}  $ such that $u<v$.

Consider the vector $\left(  I_{n}\right)  _{\bullet,u}\in\mathbb{K}%
^{n\times1}$. (This is the $u$-th column of the identity matrix $I_{n}$.\ \ \ \ \footnotemark)

Define an $n\times n$-matrix $C\in\mathbb{K}^{n\times n}$ by
\[
C=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  .
\]
Then, the following holds:

\textbf{(a)} We have%
\[
\det\left(  C_{\sim v,\sim q}\right)  =-\left(  -1\right)  ^{n+u}\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right)
\]
for every $q\in\left\{  1,2,\ldots,n-1\right\}  $.

\textbf{(b)} We have%
\begin{equation}
\left(  -1\right)  ^{n+q}\det\left(  C_{\sim v,\sim q}\right)  =-\left(
-1\right)  ^{q+u}\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  \label{pf.prop.desnanot.AB.1}%
\end{equation}
for every $q\in\left\{  1,2,\ldots,n-1\right\}  $.

\textbf{(c)} We have%
\begin{equation}
C_{\sim v,\sim n}=B_{\sim v,\bullet}. \label{pf.prop.desnanot.AB.2}%
\end{equation}


\textbf{(d)} We have
\begin{equation}
C_{\bullet,q}=B_{\bullet,q} \label{pf.prop.desnanot.AB.3}%
\end{equation}
for every $q\in\left\{  1,2,\ldots,n-1\right\}  $.

\textbf{(e)} Any $A\in\mathbb{K}^{n\times\left(  n-1\right)  }$ satisfies%
\begin{equation}
\det\left(  A\mid C_{\bullet,n}\right)  =\left(  -1\right)  ^{n+u}\det\left(
A_{\sim u,\bullet}\right)  . \label{pf.prop.desnanot.AB.4}%
\end{equation}


\textbf{(f)} We have
\begin{equation}
\det C=\left(  -1\right)  ^{n+u}\det\left(  B_{\sim u,\bullet}\right)  .
\label{pf.prop.desnanot.AB.5}%
\end{equation}

\end{lemma}

\footnotetext{Explicitly,
\[
\left(  I_{n}\right)  _{\bullet,u}=\left(  \underbrace{0,0,\ldots
,0}_{u-1\text{ zeroes}},1,\underbrace{0,0,\ldots,0}_{n-u\text{ zeroes}%
}\right)  ^{T}.
\]
}

\begin{proof}
[Proof of Lemma \ref{lem.desnanot.AB.tech}.]We have $n-1\in\mathbb{N}$ (since
$n$ is a positive integer). Also, $u<v\leq n$ (since $v\in\left\{
1,2,\ldots,n\right\}  $) and thus $u\leq n-1$ (since $u$ and $n$ are
integers). Combining this with $u\geq1$ (since $u\in\left\{  1,2,\ldots
,n\right\}  $), we obtain $u\in\left\{  1,2,\ldots,n-1\right\}  $.

\textbf{(a)} Let $q\in\left\{  1,2,\ldots,n-1\right\}  $. From $C=\left(
B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  $, we obtain%
\[
C_{\bullet,\sim q}=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)
_{\bullet,\sim q}=\left(  B_{\bullet,\sim q}\mid\left(  I_{n}\right)
_{\bullet,u}\right)
\]
(by Proposition \ref{prop.addcol.props1} \textbf{(c)}, applied to $n-1$, $B$
and $\left(  I_{n}\right)  _{\bullet,u}$ instead of $m$, $A$ and $v$). But
Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $n$, $C$,
$v$ and $q$ instead of $n$, $m$, $A$, $u$ and $v$) yields $\left(
C_{\bullet,\sim q}\right)  _{\sim v,\bullet}=\left(  C_{\sim v,\bullet
}\right)  _{\bullet,\sim q}=C_{\sim v,\sim q}$. Hence,%
\begin{equation}
C_{\sim v,\sim q}=\left(  \underbrace{C_{\bullet,\sim q}}_{=\left(
B_{\bullet,\sim q}\mid\left(  I_{n}\right)  _{\bullet,u}\right)  }\right)
_{\sim v,\bullet}=\left(  B_{\bullet,\sim q}\mid\left(  I_{n}\right)
_{\bullet,u}\right)  _{\sim v,\bullet}=\left(  \left(  B_{\bullet,\sim
q}\right)  _{\sim v,\bullet}\mid\left(  \left(  I_{n}\right)  _{\bullet
,u}\right)  _{\sim v,\bullet}\right)  \label{pf.prop.desnanot.AB.1.pf.1}%
\end{equation}
(by Proposition \ref{prop.addcol.props1} \textbf{(e)}, applied to $n$, $n-2$,
$B_{\bullet,\sim q}$, $\left(  I_{n}\right)  _{\bullet,u}$ and $v$ instead of
$n$, $m$, $A$, $v$ and $p$).

On the other hand, Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied
to $n$, $n-1$, $B$, $v$ and $q$ instead of $n$, $m$, $A$, $u$ and $v$) yields
$\left(  B_{\bullet,\sim q}\right)  _{\sim v,\bullet}=\left(  B_{\sim
v,\bullet}\right)  _{\bullet,\sim q}=B_{\sim v,\sim q}$. Now,
(\ref{pf.prop.desnanot.AB.1.pf.1}) becomes%
\[
C_{\sim v,\sim q}=\left(  \underbrace{\left(  B_{\bullet,\sim q}\right)
_{\sim v,\bullet}}_{=\left(  B_{\sim v,\bullet}\right)  _{\bullet,\sim q}}%
\mid\underbrace{\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim
v,\bullet}}_{\substack{=\left(  I_{n-1}\right)  _{\bullet,u}\\\text{(by
Proposition \ref{prop.unrows.basics-I})}}}\right)  =\left(  \left(  B_{\sim
v,\bullet}\right)  _{\bullet,\sim q}\mid\left(  I_{n-1}\right)  _{\bullet
,u}\right)  .
\]
Hence,%
\[
\det\underbrace{\left(  C_{\sim v,\sim q}\right)  }_{=\left(  \left(  B_{\sim
v,\bullet}\right)  _{\bullet,\sim q}\mid\left(  I_{n-1}\right)  _{\bullet
,u}\right)  }=\det\left(  \left(  B_{\sim v,\bullet}\right)  _{\bullet,\sim
q}\mid\left(  I_{n-1}\right)  _{\bullet,u}\right)  =\left(  -1\right)
^{\left(  n-1\right)  +u}\det\left(  \left(  B_{\sim v,\bullet}\right)  _{\sim
u,\sim q}\right)
\]
(by Proposition \ref{prop.addcol.props3} \textbf{(d)}, applied to $n-1$,
$B_{\sim v,\bullet}$ and $u$ instead of $n$, $A$ and $p$). Thus,%
\begin{align*}
\det\left(  C_{\sim v,\sim q}\right)   &  =\underbrace{\left(  -1\right)
^{\left(  n-1\right)  +u}}_{\substack{=\left(  -1\right)  ^{n+u+1}%
\\\text{(since }\left(  n-1\right)  +u=\left(  n+u+1\right)  -2\\\equiv
n+u+1\operatorname{mod}2\text{)}}}\det\left(  \underbrace{\left(  B_{\sim
v,\bullet}\right)  _{\sim u,\sim q}}_{\substack{=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \\\text{(by Proposition \ref{prop.unrows.basics}
\textbf{(l)},}\\\text{applied to }B\text{ and }n-1\text{ instead of }A\text{
and }m\text{)}}}\right) \\
&  =\underbrace{\left(  -1\right)  ^{n+u+1}}_{=-\left(  -1\right)  ^{n+u}}%
\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right) \\
&  =-\left(  -1\right)  ^{n+u}\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \right)  .
\end{align*}
This proves Lemma \ref{lem.desnanot.AB.tech} \textbf{(a)}.

\textbf{(b)} Let $q\in\left\{  1,2,\ldots,n-1\right\}  $. Then,%
\begin{align*}
&  \left(  -1\right)  ^{n+q}\underbrace{\det\left(  C_{\sim v,\sim q}\right)
}_{\substack{=-\left(  -1\right)  ^{n+u}\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \right)  \\\text{(by Lemma
\ref{lem.desnanot.AB.tech} \textbf{(a)})}}}\\
&  =\left(  -1\right)  ^{n+q}\left(  -\left(  -1\right)  ^{n+u}\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right)  \right) \\
&  =-\underbrace{\left(  -1\right)  ^{n+q}\left(  -1\right)  ^{n+u}%
}_{\substack{=\left(  -1\right)  ^{\left(  n+q\right)  +\left(  n+u\right)
}=\left(  -1\right)  ^{q+u}\\\text{(since }\left(  n+q\right)  +\left(
n+u\right)  =2n+q+u\equiv q+u\operatorname{mod}2\text{)}}}\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right) \\
&  =-\left(  -1\right)  ^{q+u}\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \right)  .
\end{align*}
This proves Lemma \ref{lem.desnanot.AB.tech} \textbf{(b)}.

\textbf{(c)} We have $C=\left(  B\mid\left(  I_{n}\right)  _{\bullet
,u}\right)  $. Thus,%
\begin{align*}
C_{\sim v,\sim n}  &  =\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)
_{\sim v,\sim n}=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)
_{\sim v,\sim\left(  \left(  n-1\right)  +1\right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=\left(  n-1\right)  +1\right) \\
&  =B_{\sim v,\bullet}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.addcol.props1} \textbf{(f)}, applied to}\\
n-1\text{, }B\text{ and }\left(  I_{n}\right)  _{\bullet,u}\text{ instead of
}m\text{, }A\text{ and }v
\end{array}
\right)  .
\end{align*}
This proves Lemma \ref{lem.desnanot.AB.tech} \textbf{(c)}.

\textbf{(d)} Let $q\in\left\{  1,2,\ldots,n-1\right\}  $. From $C=\left(
B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  $, we obtain%
\[
C_{\bullet,q}=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)
_{\bullet,q}=B_{\bullet,q}%
\]
(by Proposition \ref{prop.addcol.props1} \textbf{(a)}, applied to $n-1$, $B$
and $\left(  I_{n}\right)  _{\bullet,u}$ instead of $m$, $A$ and $v$). This
proves Lemma \ref{lem.desnanot.AB.tech} \textbf{(d)}.

\textbf{(e)} Let $A\in\mathbb{K}^{\left(  n-1\right)  \times n}$. Proposition
\ref{prop.addcol.props1} \textbf{(b)} (applied to $n-1$, $B$ and $\left(
I_{n}\right)  _{\bullet,u}$ instead of $m$, $A$ and $v$) yields $\left(
B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  _{\bullet,\left(  n-1\right)
+1}=\left(  I_{n}\right)  _{\bullet,u}$. This rewrites as $\left(
B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  _{\bullet,n}=\left(
I_{n}\right)  _{\bullet,u}$ (since $\left(  n-1\right)  +1=n$). Now,
$C=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  $, so that%
\[
C_{\bullet,n}=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)
_{\bullet,n}=\left(  I_{n}\right)  _{\bullet,u}.
\]
Hence,%
\[
\det\left(  A\mid\underbrace{C_{\bullet,n}}_{=\left(  I_{n}\right)
_{\bullet,u}}\right)  =\det\left(  A\mid\left(  I_{n}\right)  _{\bullet
,u}\right)  =\left(  -1\right)  ^{n+u}\det\left(  A_{\sim u,\bullet}\right)
\]
(by Proposition \ref{prop.addcol.props2} \textbf{(b)}, applied to $p=u$). This
proves Lemma \ref{lem.desnanot.AB.tech} \textbf{(e)}.

\textbf{(f)} We have%
\[
\det\underbrace{C}_{=\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)
}=\det\left(  B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  =\left(
-1\right)  ^{n+u}\det\left(  B_{\sim u,\bullet}\right)
\]
(by Proposition \ref{prop.addcol.props2} \textbf{(b)}, applied to $B$ and $u$
instead of $A$ and $p$). This proves Lemma \ref{lem.desnanot.AB.tech}
\textbf{(f)}.
\end{proof}

Next, we claim the following:

\begin{proposition}
\label{prop.desnanot.AB}Let $n$ be a positive integer. Let $A\in
\mathbb{K}^{n\times\left(  n-1\right)  }$ and $B\in\mathbb{K}^{n\times\left(
n-1\right)  }$. Let $u$ and $v$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ such that $u<v$. Then,%
\begin{align*}
&  \sum_{r=1}^{n-1}\left(  -1\right)  ^{r}\det\left(  A\mid B_{\bullet
,r}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim r}\right)
\right) \\
&  =\left(  -1\right)  ^{n}\left(  \det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\det\left(  A_{\sim v,\bullet}\right)
\det\left(  B_{\sim u,\bullet}\right)  \right)  .
\end{align*}

\end{proposition}

\begin{example}
\label{exam.prop.desnanot.AB}If we set $n=3$, $A=\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{cc}%
x & x^{\prime}\\
y & y^{\prime}\\
z & z^{\prime}%
\end{array}
\right)  $, $u=1$ and $v=2$, then Proposition \ref{prop.desnanot.AB} claims
that%
\begin{align*}
&  -\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x\\
b & b^{\prime} & y\\
c & c^{\prime} & z
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
z^{\prime}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x^{\prime}\\
b & b^{\prime} & y^{\prime}\\
c & c^{\prime} & z^{\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
z
\end{array}
\right) \\
&  =\left(  -1\right)  ^{3}\left(  \det\left(
\begin{array}
[c]{cc}%
b & b^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
x & x^{\prime}\\
z & z^{\prime}%
\end{array}
\right)  -\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
y & y^{\prime}\\
z & z^{\prime}%
\end{array}
\right)  \right)  .
\end{align*}

\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.desnanot.AB}.]We have $n-1\in\mathbb{N}$
(since $n$ is a positive integer).

Define an $n\times n$-matrix $C\in\mathbb{K}^{n\times n}$ as in Lemma
\ref{lem.desnanot.AB.tech}.

Proposition \ref{prop.desnanot.AC} yields%
\begin{align*}
&  \det\left(  A_{\sim v,\bullet}\right)  \det C\\
&  =\sum_{q=1}^{n}\underbrace{\left(  -1\right)  ^{n+q}\det\left(  A\mid
C_{\bullet,q}\right)  }_{=\det\left(  A\mid C_{\bullet,q}\right)  \left(
-1\right)  ^{n+q}}\det\left(  C_{\sim v,\sim q}\right)  =\sum_{q=1}^{n}%
\det\left(  A\mid C_{\bullet,q}\right)  \left(  -1\right)  ^{n+q}\det\left(
C_{\sim v,\sim q}\right) \\
&  =\sum_{q=1}^{n-1}\det\left(  A\mid\underbrace{C_{\bullet,q}}%
_{\substack{=B_{\bullet,q}\\\text{(by (\ref{pf.prop.desnanot.AB.3}))}%
}}\right)  \underbrace{\left(  -1\right)  ^{n+q}\det\left(  C_{\sim v,\sim
q}\right)  }_{\substack{=-\left(  -1\right)  ^{q+u}\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right)  \\\text{(by
(\ref{pf.prop.desnanot.AB.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\det\left(  A\mid C_{\bullet,n}\right)
}_{\substack{=\left(  -1\right)  ^{n+u}\det\left(  A_{\sim u,\bullet}\right)
\\\text{(by (\ref{pf.prop.desnanot.AB.4}))}}}\underbrace{\left(  -1\right)
^{n+n}}_{\substack{=1\\\text{(since }n+n=2n\text{ is even)}}}\det\left(
\underbrace{C_{\sim v,\sim n}}_{\substack{=B_{\sim v,\bullet}\\\text{(by
(\ref{pf.prop.desnanot.AB.2}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}q=n\text{ from the sum}\right) \\
&  =\underbrace{\sum_{q=1}^{n-1}\det\left(  A\mid B_{\bullet,q}\right)
\left(  -\left(  -1\right)  ^{q+u}\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \right)  \right)  }_{=-\sum_{q=1}^{n-1}\left(
-1\right)  ^{q+u}\det\left(  A\mid B_{\bullet,q}\right)  \det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{n+u}\det\left(  A_{\sim
u,\bullet}\right)  \det\left(  B_{\sim v,\bullet}\right) \\
&  =-\sum_{q=1}^{n-1}\left(  -1\right)  ^{q+u}\det\left(  A\mid B_{\bullet
,q}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{n+u}\det\left(  A_{\sim
u,\bullet}\right)  \det\left(  B_{\sim v,\bullet}\right)  .
\end{align*}
Adding $\sum_{q=1}^{n-1}\left(  -1\right)  ^{q+u}\det\left(  A\mid
B_{\bullet,q}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  $ to both sides of this equality, we obtain%
\begin{align*}
&  \sum_{q=1}^{n-1}\left(  -1\right)  ^{q+u}\det\left(  A\mid B_{\bullet
,q}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  +\det\left(  A_{\sim v,\bullet}\right)  \det C\\
&  =\left(  -1\right)  ^{n+u}\det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  .
\end{align*}
Subtracting $\det\left(  A_{\sim v,\bullet}\right)  \det C$ from both sides of
this equality, we find%
\begin{align*}
&  \sum_{q=1}^{n-1}\left(  -1\right)  ^{q+u}\det\left(  A\mid B_{\bullet
,q}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right) \\
&  =\left(  -1\right)  ^{n+u}\det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\det\left(  A_{\sim v,\bullet}\right)
\underbrace{\det C}_{\substack{=\left(  -1\right)  ^{n+u}\det\left(  B_{\sim
u,\bullet}\right)  \\\text{(by (\ref{pf.prop.desnanot.AB.5}))}}}\\
&  =\left(  -1\right)  ^{n+u}\det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\underbrace{\det\left(  A_{\sim
v,\bullet}\right)  \left(  -1\right)  ^{n+u}}_{=\left(  -1\right)  ^{n+u}%
\det\left(  A_{\sim v,\bullet}\right)  }\det\left(  B_{\sim u,\bullet}\right)
\\
&  =\left(  -1\right)  ^{n+u}\det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\left(  -1\right)  ^{n+u}\det\left(
A_{\sim v,\bullet}\right)  \det\left(  B_{\sim u,\bullet}\right) \\
&  =\left(  -1\right)  ^{n+u}\left(  \det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\det\left(  A_{\sim v,\bullet}\right)
\det\left(  B_{\sim u,\bullet}\right)  \right)  .
\end{align*}
Multiplying both sides of this equality by $\left(  -1\right)  ^{u}$, we
obtain%
\begin{align*}
&  \left(  -1\right)  ^{u}\sum_{q=1}^{n-1}\left(  -1\right)  ^{q+u}\det\left(
A\mid B_{\bullet,q}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \right) \\
&  =\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)  ^{n+u}%
}_{\substack{=\left(  -1\right)  ^{u+\left(  n+u\right)  }=\left(  -1\right)
^{n}\\\text{(since }u+\left(  n+u\right)  =2u+n\equiv n\operatorname{mod}%
2\text{)}}}\left(  \det\left(  A_{\sim u,\bullet}\right)  \det\left(  B_{\sim
v,\bullet}\right)  -\det\left(  A_{\sim v,\bullet}\right)  \det\left(  B_{\sim
u,\bullet}\right)  \right) \\
&  =\left(  -1\right)  ^{n}\left(  \det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\det\left(  A_{\sim v,\bullet}\right)
\det\left(  B_{\sim u,\bullet}\right)  \right)  ,
\end{align*}
so that%
\begin{align*}
&  \left(  -1\right)  ^{n}\left(  \det\left(  A_{\sim u,\bullet}\right)
\det\left(  B_{\sim v,\bullet}\right)  -\det\left(  A_{\sim v,\bullet}\right)
\det\left(  B_{\sim u,\bullet}\right)  \right) \\
&  =\left(  -1\right)  ^{u}\sum_{q=1}^{n-1}\left(  -1\right)  ^{q+u}%
\det\left(  A\mid B_{\bullet,q}\right)  \det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(
B_{\bullet,\sim q}\right)  \right) \\
&  =\sum_{q=1}^{n-1}\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)
^{q+u}}_{\substack{=\left(  -1\right)  ^{u+\left(  q+u\right)  }=\left(
-1\right)  ^{q}\\\text{(since }u+\left(  q+u\right)  =2u+q\equiv
q\operatorname{mod}2\text{)}}}\det\left(  A\mid B_{\bullet,q}\right)
\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right) \\
&  =\sum_{q=1}^{n-1}\left(  -1\right)  ^{q}\det\left(  A\mid B_{\bullet
,q}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right) \\
&  =\sum_{r=1}^{n-1}\left(  -1\right)  ^{r}\det\left(  A\mid B_{\bullet
,r}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim r}\right)
\right)
\end{align*}
(here, we have renamed the summation index $q$ as $r$). This proves
Proposition \ref{prop.desnanot.AB}.
\end{proof}

Now, we can finally prove Theorem \ref{thm.desnanot}:

\begin{proof}
[Proof of Theorem \ref{thm.desnanot}.]We have $v\in\left\{  1,2,\ldots
,n\right\}  $ and thus $v\leq n$. Hence, $u<v\leq n$, so that $u\leq n-1$
(since $u$ and $n$ are integers). Also, $u\in\left\{  1,2,\ldots,n\right\}  $,
so that $1\leq u$. Combining $1\leq u$ with $u\leq n-1$, we obtain
$u\in\left\{  1,2,\ldots,n-1\right\}  $.

Proposition \ref{prop.submatrix.easy} \textbf{(d)} (applied to $n$, $n-2$,
$\left(  1,2,\ldots,\widehat{p},\ldots,\widehat{q},\ldots,n\right)  $, $n-2$
and $\left(  1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n\right)  $
instead of $m$, $u$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $, $v$ and
$\left(  j_{1},j_{2},\ldots,j_{v}\right)  $) yields%
\begin{align}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}^{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}A  &
=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}A\right) \label{pf.thm.desnanot.1}\\
&  =\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,\widehat{v},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{p},\ldots,\widehat{q},\ldots,n}A\right)  .\nonumber
\end{align}


We have $u<v$, so that $u\leq v-1$ (since $u$ and $v$ are integers). Combining
this with $1\leq u$, we obtain $u\in\left\{  1,2,\ldots,v-1\right\}  $. Thus,
Proposition \ref{prop.unrows.basics} \textbf{(h)} (applied to $m=n$ and $w=u$)
yields $\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim u}%
=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}A$. Thus,%
\begin{align}
&  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}\left(  \underbrace{\left(  A_{\bullet,\sim v}\right)
_{\bullet,\sim u}}_{=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}A}\right) \nonumber\\
&  =\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p},\ldots
,\widehat{q},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}A\right) \nonumber\\
&  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}^{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}%
A\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.thm.desnanot.1})}\right)  .
\label{pf.thm.desnanot.2}%
\end{align}


\begin{vershort}
Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $p$ and $u$
instead of $m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim u}\right)  _{\sim
p,\bullet}=\left(  A_{\sim p,\bullet}\right)  _{\bullet,\sim u}=A_{\sim p,\sim
u}$. Similarly, $\left(  A_{\bullet,\sim v}\right)  _{\sim p,\bullet}=\left(
A_{\sim p,\bullet}\right)  _{\bullet,\sim v}=A_{\sim p,\sim v}$ and $\left(
A_{\bullet,\sim u}\right)  _{\sim q,\bullet}=\left(  A_{\sim q,\bullet
}\right)  _{\bullet,\sim u}=A_{\sim q,\sim u}$ and $\left(  A_{\bullet,\sim
v}\right)  _{\sim q,\bullet}=\left(  A_{\sim q,\bullet}\right)  _{\bullet,\sim
v}=A_{\sim q,\sim v}$.
\end{vershort}

\begin{verlong}
Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $p$ and $u$
instead of $m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim u}\right)  _{\sim
p,\bullet}=\left(  A_{\sim p,\bullet}\right)  _{\bullet,\sim u}=A_{\sim p,\sim
u}$.

Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $p$ and $v$
instead of $m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim v}\right)  _{\sim
p,\bullet}=\left(  A_{\sim p,\bullet}\right)  _{\bullet,\sim v}=A_{\sim p,\sim
v}$.

Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $q$ and $u$
instead of $m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim u}\right)  _{\sim
q,\bullet}=\left(  A_{\sim q,\bullet}\right)  _{\bullet,\sim u}=A_{\sim q,\sim
u}$.

Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $q$ and $v$
instead of $m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim v}\right)  _{\sim
q,\bullet}=\left(  A_{\sim q,\bullet}\right)  _{\bullet,\sim v}=A_{\sim q,\sim
v}$.
\end{verlong}

The integer $n$ is positive (since $n\geq2$). Thus, Proposition
\ref{prop.desnanot.AB} (applied to $A_{\bullet,\sim u}$, $A_{\bullet,\sim v}$,
$p$ and $q$ instead of $A$, $B$, $u$ and $v$) yields%
\begin{align*}
&  \sum_{r=1}^{n-1}\left(  -1\right)  ^{r}\det\left(  A_{\bullet,\sim u}%
\mid\left(  A_{\bullet,\sim v}\right)  _{\bullet,r}\right)  \det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}\left(  \left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim r}\right)
\right) \\
&  =\left(  -1\right)  ^{n}\left(  \det\left(  \underbrace{\left(
A_{\bullet,\sim u}\right)  _{\sim p,\bullet}}_{=A_{\sim p,\sim u}}\right)
\det\left(  \underbrace{\left(  A_{\bullet,\sim v}\right)  _{\sim q,\bullet}%
}_{=A_{\sim q,\sim v}}\right)  \right. \\
&  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \left.  -\det\left(
\underbrace{\left(  A_{\bullet,\sim u}\right)  _{\sim q,\bullet}}_{=A_{\sim
q,\sim u}}\right)  \det\left(  \underbrace{\left(  A_{\bullet,\sim v}\right)
_{\sim p,\bullet}}_{=A_{\sim p,\sim v}}\right)  \right) \\
&  =\left(  -1\right)  ^{n}\left(  \det\left(  A_{\sim p,\sim u}\right)
\det\left(  A_{\sim q,\sim v}\right)  -\det\left(  A_{\sim q,\sim u}\right)
\det\left(  A_{\sim p,\sim v}\right)  \right)  .
\end{align*}
Hence,%
\begin{align*}
&  \left(  -1\right)  ^{n}\left(  \det\left(  A_{\sim p,\sim u}\right)
\det\left(  A_{\sim q,\sim v}\right)  -\det\left(  A_{\sim q,\sim u}\right)
\det\left(  A_{\sim p,\sim v}\right)  \right) \\
&  =\underbrace{\sum_{r=1}^{n-1}}_{=\sum_{r\in\left\{  1,2,\ldots,n-1\right\}
}}\left(  -1\right)  ^{r}\det\left(  A_{\bullet,\sim u}\mid\left(
A_{\bullet,\sim v}\right)  _{\bullet,r}\right)  \det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}\left(  \left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim r}\right)
\right) \\
&  =\sum_{r\in\left\{  1,2,\ldots,n-1\right\}  }\left(  -1\right)  ^{r}%
\det\left(  A_{\bullet,\sim u}\mid\left(  A_{\bullet,\sim v}\right)
_{\bullet,r}\right)  \det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{p},\ldots,\widehat{q},\ldots,n}\left(  \left(  A_{\bullet,\sim
v}\right)  _{\bullet,\sim r}\right)  \right) \\
&  =\sum_{\substack{r\in\left\{  1,2,\ldots,n-1\right\}  ;\\r\neq u}}\left(
-1\right)  ^{r}\underbrace{\det\left(  A_{\bullet,\sim u}\mid\left(
A_{\bullet,\sim v}\right)  _{\bullet,r}\right)  }_{\substack{=0\\\text{(by
Proposition \ref{prop.addcol.props3} \textbf{(e)})}}}\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}\left(  \left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim r}\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{u}\det\left(
A_{\bullet,\sim u}\mid\left(  A_{\bullet,\sim v}\right)  _{\bullet,u}\right)
}_{\substack{=\left(  -1\right)  ^{n}\det A\\\text{(by Proposition
\ref{prop.addcol.props3} \textbf{(f)})}}}\det\left(
\underbrace{\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{p}%
,\ldots,\widehat{q},\ldots,n}\left(  \left(  A_{\bullet,\sim v}\right)
_{\bullet,\sim u}\right)  }_{\substack{=\operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q},\ldots,n}^{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}A\\\text{(by (\ref{pf.thm.desnanot.2}%
))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }r=u\text{ from the sum,}\\
\text{since }u\in\left\{  1,2,\ldots,n-1\right\}
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{r\in\left\{  1,2,\ldots,n-1\right\}  ;\\r\neq
u}}\left(  -1\right)  ^{r}0\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q},\ldots,n}\left(  \left(
A_{\bullet,\sim v}\right)  _{\bullet,\sim r}\right)  \right)  }_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{n}\det A\cdot\det\left(
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q}%
,\ldots,n}^{1,2,\ldots,\widehat{u},\ldots,\widehat{v},\ldots,n}A\right) \\
&  =\left(  -1\right)  ^{n}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{p},\ldots,\widehat{q},\ldots,n}^{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}A\right)  .
\end{align*}
Multiplying both sides of this equality by $\left(  -1\right)  ^{n}$, we
obtain%
\begin{align*}
&  \left(  -1\right)  ^{n}\left(  -1\right)  ^{n}\left(  \det\left(  A_{\sim
p,\sim u}\right)  \det\left(  A_{\sim q,\sim v}\right)  -\det\left(  A_{\sim
q,\sim u}\right)  \det\left(  A_{\sim p,\sim v}\right)  \right) \\
&  =\underbrace{\left(  -1\right)  ^{n}\left(  -1\right)  ^{n}}%
_{\substack{=\left(  -1\right)  ^{n+n}=1\\\text{(since }n+n=2n\text{ is
even)}}}\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{p},\ldots,\widehat{q},\ldots,n}^{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}A\right) \\
&  =\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{p},\ldots,\widehat{q},\ldots,n}^{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}A\right)  .
\end{align*}
Thus,%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{p},\ldots,\widehat{q},\ldots,n}^{1,2,\ldots,\widehat{u}%
,\ldots,\widehat{v},\ldots,n}A\right) \\
&  =\underbrace{\left(  -1\right)  ^{n}\left(  -1\right)  ^{n}}%
_{\substack{=\left(  -1\right)  ^{n+n}=1\\\text{(since }n+n=2n\text{ is
even)}}}\left(  \det\left(  A_{\sim p,\sim u}\right)  \det\left(  A_{\sim
q,\sim v}\right)  -\det\left(  A_{\sim q,\sim u}\right)  \det\left(  A_{\sim
p,\sim v}\right)  \right) \\
&  =\det\left(  A_{\sim p,\sim u}\right)  \det\left(  A_{\sim q,\sim
v}\right)  -\det\left(  A_{\sim q,\sim u}\right)  \det\left(  A_{\sim p,\sim
v}\right)  .
\end{align*}
This proves Theorem \ref{thm.desnanot}.
\end{proof}

Now that Theorem \ref{thm.desnanot} is proven, we conclude that Proposition
\ref{prop.desnanot.12} and Proposition \ref{prop.desnanot.1n} hold as well
(because in Exercise \ref{exe.unrows.basics} \textbf{(b)}, these two
propositions have been derived from Theorem \ref{thm.desnanot}).

\begin{exercise}
\label{exe.desnanot.jaw}Let $n$ be a positive integer. Let $B\in
\mathbb{K}^{n\times\left(  n-1\right)  }$. Fix $q\in\left\{  1,2,\ldots
,n-1\right\}  $. For every $x\in\left\{  1,2,\ldots,n\right\}  $, set
\[
\alpha_{x}=\det\left(  B_{\sim x,\bullet}\right)  \text{.}%
\]
For every two elements $x$ and $y$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $x<y$, set%
\[
\beta_{x,y}=\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{x},\ldots,\widehat{y},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  .
\]
(Note that this depends on $q$, but we do not mention $q$ in the notation
because $q$ is fixed.)

Let $u$, $v$ and $w$ be three elements of $\left\{  1,2,\ldots,n\right\}  $
such that $u<v<w$. Thus, $\beta_{u,v}$, $\beta_{v,w}$ and $\beta_{u,w}$ are
well-defined elements of $\mathbb{K}$. Prove that%
\[
\alpha_{u}\beta_{v,w}+\alpha_{w}\beta_{u,v}=\alpha_{v}\beta_{u,w}.
\]

\end{exercise}

\begin{example}
\label{exam.exe.desnanot.jaw}If we set $n=4$, $B=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $, $q=3$, $u=1$, $v=2$ and $w=3$, then Exercise
\ref{exe.desnanot.jaw} says that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccc}%
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
d & d^{\prime}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
c & c^{\prime}\\
d & d^{\prime}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
b & b^{\prime}\\
d & d^{\prime}%
\end{array}
\right)  .
\end{align*}

\end{example}

\begin{remark}
Exercise \ref{exe.desnanot.jaw} appears in \cite[proof of Theorem 7]%
{KenWil14}, where it (or, rather, a certain transformation of determinant
expressions that relies on it) is called the \textquotedblleft jaw
move\textquotedblright.
\end{remark}

\begin{exercise}
\label{exe.desnanot.skew}Let $n\in\mathbb{N}$. Let $A$ be an alternating
$n\times n$-matrix. (See Definition \ref{def.altern} \textbf{(b)} for what
this means.) Let $S$ be any $n\times n$-matrix. Prove that each entry of the
matrix $\left(  \operatorname*{adj}S\right)  ^{T}A\left(  \operatorname*{adj}%
S\right)  $ is a multiple of $\det S$.
\end{exercise}

\subsection{The Pl\"{u}cker relation}

The following section is devoted to the \textit{Pl\"{u}cker relations}, or,
rather, one of the many things that tend to carry this name in the
literature\footnote{Most of the relevant literature, unfortunately, is not
very elementary, as the Pl\"{u}cker relations are at their most useful in the
algebraic geometry of the Grassmannian and of flag varieties
(\textquotedblleft Schubert calculus\textquotedblright). See \cite{KleLak72},
\cite[\S 3.4]{Jacobs10} and \cite[\S 9.1]{Fulton-Young} for expositions (all
three, however, well above the level of the present notes).}. The proofs will
be fairly short, since we did much of the necessary work in Section
\ref{sect.desnanot} already.

We shall use the notations of Definition \ref{def.unrows} throughout this section.

We begin with the following identity:

\begin{proposition}
\label{prop.pluecker.pre.row}Let $n$ be a positive integer. Let $B\in
\mathbb{K}^{n\times\left(  n-1\right)  }$. Then:

\textbf{(a)} We have%
\[
\sum_{r=1}^{n}\left(  -1\right)  ^{r}\det\left(  B_{\sim r,\bullet}\right)
B_{r,\bullet}=0_{1\times\left(  n-1\right)  }.
\]
(Recall that the product $\det\left(  B_{\sim r,\bullet}\right)  B_{r,\bullet
}$ in this equality is the product of the scalar $\det\left(  B_{\sim
r,\bullet}\right)  \in\mathbb{K}$ with the row vector $B_{r,\bullet}%
\in\mathbb{K}^{1\times\left(  n-1\right)  }$; as all such products, it is
computed entrywise, i.e., by the formula $\lambda\left(  a_{1},a_{2}%
,\ldots,a_{n-1}\right)  =\left(  \lambda a_{1},\lambda a_{2},\ldots,\lambda
a_{n-1}\right)  $.)

\textbf{(b)} Write the matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n-1}$. Then,
\[
\sum_{r=1}^{n}\left(  -1\right)  ^{r}\det\left(  B_{\sim r,\bullet}\right)
b_{r,q}=0
\]
for every $q\in\left\{  1,2,\ldots,n-1\right\}  $.
\end{proposition}

Note that part \textbf{(a)} of Proposition \ref{prop.pluecker.pre.row} claims
an equality between two row vectors (indeed, $B_{r,\bullet}$ is a row vector
with $n-1$ entries for each $r\in\left\{  1,2,\ldots,n\right\}  $), whereas
part \textbf{(b)} claims an equality between two elements of $\mathbb{K}$ (for
each $q\in\left\{  1,2,\ldots,n-1\right\}  $). That said, the two parts are
essentially restatements of one another, and we will derive part \textbf{(a)}
from part \textbf{(b)} soon enough. Let us first illustrate Proposition
\ref{prop.pluecker.pre.row} on an example:

\begin{example}
\label{exam.prop.pluecker.pre.row}For this example, set $n=4$ and $B=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $. Then, Proposition \ref{prop.pluecker.pre.row} \textbf{(a)} says
that%
\begin{align*}
&  -\det\left(
\begin{array}
[c]{ccc}%
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{ccc}%
b & b^{\prime} & b^{\prime\prime}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{ccc}%
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{ccc}%
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right) \\
&  =0_{1\times3}.
\end{align*}
Proposition \ref{prop.pluecker.pre.row} \textbf{(b)} (applied to $q=3$) yields%
\begin{align*}
&  -\det\left(
\begin{array}
[c]{ccc}%
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot a^{\prime\prime}+\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot b^{\prime\prime}\\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  \cdot c^{\prime\prime}+\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}\\
c & c^{\prime} & c^{\prime\prime}%
\end{array}
\right)  \cdot d^{\prime\prime}\\
&  =0.
\end{align*}

\end{example}

\begin{proof}
[Proof of Proposition \ref{prop.pluecker.pre.row}.]\textbf{(b)} Let
$q\in\left\{  1,2,\ldots,n-1\right\}  $.

\begin{vershort}
We shall use the notation introduced in Definition \ref{def.addcol}. We know
that $B$ is an $n\times\left(  n-1\right)  $-matrix (since $B\in
\mathbb{K}^{n\times\left(  n-1\right)  }$). Thus, $\left(  B\mid B_{\bullet
,q}\right)  $ is an $n\times n$-matrix. This $n\times n$-matrix $\left(  B\mid
B_{\bullet,q}\right)  $ is defined as the $n\times\left(  \left(  n-1\right)
+1\right)  $-matrix whose columns are $B_{\bullet,1},B_{\bullet,2}%
,\ldots,B_{\bullet,n-1},B_{\bullet,q}$; thus, it has two equal columns
(indeed, the column vector $B_{\bullet,q}$ appears twice among the columns
$B_{\bullet,1},B_{\bullet,2},\ldots,B_{\bullet,n-1},B_{\bullet,q}$). Thus,
Exercise \ref{exe.ps4.6} \textbf{(f)} (applied to $A=\left(  B\mid
B_{\bullet,q}\right)  $) shows that $\det\left(  B\mid B_{\bullet,q}\right)
=0$.
\end{vershort}

\begin{verlong}
We shall use the notation introduced in Definition \ref{def.addcol}. We know
that $B$ is an $n\times\left(  n-1\right)  $-matrix (since $B\in
\mathbb{K}^{n\times\left(  n-1\right)  }$); hence, $B_{\bullet,q}$ is an
$n\times1$-matrix. Thus, $\left(  B\mid B_{\bullet,q}\right)  $ is an
$n\times\left(  \left(  n-1\right)  +1\right)  $-matrix. In other words,
$\left(  B\mid B_{\bullet,q}\right)  $ is an $n\times n$-matrix (since
$\left(  n-1\right)  +1=n$). We have%
\begin{equation}
\det\left(  B\mid B_{\bullet,q}\right)  =0
\label{pf.prop.pluecker.pre.row.det=0}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.pluecker.pre.row.det=0}):} Let
$A=\left(  B\mid B_{\bullet,q}\right)  $. Thus, $A$ is an $n\times n$-matrix
(since $\left(  B\mid B_{\bullet,q}\right)  $ is an $n\times n$-matrix). The
definition of $A_{\bullet,n}$ yields that $A_{\bullet,n}$ is the $n$-th column
of the matrix $A$. Thus,%
\[
A_{\bullet,n}=\left(  \text{the }n\text{-th column of the matrix }A\right)  .
\]
Hence,%
\begin{align}
&  \left(  \text{the }n\text{-th column of the matrix }A\right) \nonumber\\
&  =A_{\bullet,n}=\left(  B\mid B_{\bullet,q}\right)  _{\bullet,n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  B\mid B_{\bullet,q}\right)
\right) \nonumber\\
&  =\left(  B\mid B_{\bullet,q}\right)  _{\bullet,\left(  \left(  n-1\right)
+1\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=\left(  n-1\right)
+1\right) \nonumber\\
&  =B_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.addcol.props1} \textbf{(b)}, applied to
}n-1\text{, }B\text{ and }B_{\bullet,q}\\
\text{instead of }m\text{, }A\text{ and }v
\end{array}
\right)  . \label{pf.prop.pluecker.pre.row.det=0.pf.1}%
\end{align}
\par
On the other hand, $n\in\left\{  1,2,\ldots,n\right\}  $ (since $n$ is a
positive integer) and $q\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $. Furthermore, $q\in\left\{  1,2,\ldots,n-1\right\}  $,
thus $q\leq n-1<n$. Hence, $q\neq n$. Hence, $q$ and $n$ are two distinct
elements of $\left\{  1,2,\ldots,n\right\}  $.
\par
But $q\in\left\{  1,2,\ldots,n\right\}  $. Hence, the definition of
$A_{\bullet,q}$ yields that $A_{\bullet,q}$ is the $q$-th column of the matrix
$A$. Thus,%
\[
A_{\bullet,q}=\left(  \text{the }q\text{-th column of the matrix }A\right)  .
\]
Hence,%
\begin{align*}
&  \left(  \text{the }q\text{-th column of the matrix }A\right) \\
&  =A_{\bullet,q}=\left(  B\mid B_{\bullet,q}\right)  _{\bullet,q}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  B\mid B_{\bullet,q}\right)
\right) \\
&  =B_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.addcol.props1} \textbf{(a)}, applied to
}n-1\text{, }B\text{ and }B_{\bullet,q}\\
\text{instead of }m\text{, }A\text{ and }v
\end{array}
\right) \\
&  =\left(  \text{the }n\text{-th column of the matrix }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.pluecker.pre.row.det=0.pf.1})}\right)  .
\end{align*}
Hence, there exist two distinct elements $u$ and $v$ of $\left\{
1,2,\ldots,n\right\}  $ such that
\[
\left(  \text{the }u\text{-th column of the matrix }A\right)  =\left(
\text{the }v\text{-th column of the matrix }A\right)
\]
(namely, $u=q$ and $v=n$). In other words, the matrix $A$ has two equal
columns. Thus, $\det A=0$ (by Exercise \ref{exe.ps4.6} \textbf{(f)}). This
rewrites as $\det\left(  B\mid B_{\bullet,q}\right)  =0$ (since $A=\left(
B\mid B_{\bullet,q}\right)  $). This proves
(\ref{pf.prop.pluecker.pre.row.det=0}).}.
\end{verlong}

But $B_{\bullet,q}$ is the $q$-th column of the matrix $B$ (by the definition
of $B_{\bullet,q}$). Thus,%
\begin{align*}
B_{\bullet,q}  &  =\left(  \text{the }q\text{-th column of the matrix
}B\right) \\
&  =\left(
\begin{array}
[c]{c}%
b_{1,q}\\
b_{2,q}\\
\vdots\\
b_{n,q}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n-1}\right) \\
&  =\left(  b_{1,q},b_{2,q},\ldots,b_{n,q}\right)  ^{T}.
\end{align*}
Hence, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to $B$,
$B_{\bullet,q}$ and $b_{i,q}$ instead of $A$, $v$ and $v_{i}$) yields%
\[
\det\left(  B\mid B_{\bullet,q}\right)  =\sum_{i=1}^{n}\left(  -1\right)
^{n+i}b_{i,q}\det\left(  B_{\sim i,\bullet}\right)  .
\]
Comparing this with $\det\left(  B\mid B_{\bullet,q}\right)  =0$, we obtain%
\[
0=\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}b_{i,q}\det\left(  B_{\sim i,\bullet
}\right)  .
\]
Multiplying both sides of this equality by $\left(  -1\right)  ^{n}$, we
obtain%
\begin{align*}
0  &  =\left(  -1\right)  ^{n}\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}%
b_{i,q}\det\left(  B_{\sim i,\bullet}\right) \\
&  =\sum_{i=1}^{n}\underbrace{\left(  -1\right)  ^{n}\left(  -1\right)
^{n+i}}_{\substack{=\left(  -1\right)  ^{n+\left(  n+i\right)  }=\left(
-1\right)  ^{i}\\\text{(since }n+\left(  n+i\right)  =2n+i\equiv
i\operatorname{mod}2\text{)}}}\underbrace{b_{i,q}\det\left(  B_{\sim
i,\bullet}\right)  }_{=\det\left(  B_{\sim i,\bullet}\right)  b_{i,q}}\\
&  =\sum_{i=1}^{n}\left(  -1\right)  ^{i}\det\left(  B_{\sim i,\bullet
}\right)  b_{i,q}=\sum_{r=1}^{n}\left(  -1\right)  ^{r}\det\left(  B_{\sim
r,\bullet}\right)  b_{r,q}%
\end{align*}
(here, we renamed the summation index $i$ as $r$). This proves Proposition
\ref{prop.pluecker.pre.row} \textbf{(b)}.

\textbf{(a)} Write the matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n-1}$. For every $r\in\left\{  1,2,\ldots
,n\right\}  $, we have%
\begin{align*}
B_{r,\bullet}  &  =\left(  \text{the }r\text{-th row of the matrix }B\right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }B_{r,\bullet}\text{ is the }r\text{-th row of the matrix }B\\
\text{(by the definition of }B_{r,\bullet}\text{)}%
\end{array}
\right) \\
&  =\left(  b_{r,1},b_{r,2},\ldots,b_{r,n-1}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }B=\left(  b_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n-1}\right) \\
&  =\left(  b_{r,j}\right)  _{1\leq i\leq1,\ 1\leq j\leq n-1}.
\end{align*}
Thus,%
\begin{align*}
&  \sum_{r=1}^{n}\left(  -1\right)  ^{r}\det\left(  B_{\sim r,\bullet}\right)
\underbrace{B_{r,\bullet}}_{=\left(  b_{r,j}\right)  _{1\leq i\leq1,\ 1\leq
j\leq n-1}}\\
&  =\sum_{r=1}^{n}\underbrace{\left(  -1\right)  ^{r}\det\left(  B_{\sim
r,\bullet}\right)  \left(  b_{r,j}\right)  _{1\leq i\leq1,\ 1\leq j\leq n-1}%
}_{=\left(  \left(  -1\right)  ^{r}\det\left(  B_{\sim r,\bullet}\right)
b_{r,j}\right)  _{1\leq i\leq1,\ 1\leq j\leq n-1}}=\sum_{r=1}^{n}\left(
\left(  -1\right)  ^{r}\det\left(  B_{\sim r,\bullet}\right)  b_{r,j}\right)
_{1\leq i\leq1,\ 1\leq j\leq n-1}\\
&  =\left(  \underbrace{\sum_{r=1}^{n}\left(  -1\right)  ^{r}\det\left(
B_{\sim r,\bullet}\right)  b_{r,j}}_{\substack{=0\\\text{(by Proposition
\ref{prop.pluecker.pre.row} \textbf{(b)}, applied to }q=j\text{)}}}\right)
_{1\leq i\leq1,\ 1\leq j\leq n-1}=\left(  0\right)  _{1\leq i\leq1,\ 1\leq
j\leq n-1}=0_{1\times\left(  n-1\right)  }.
\end{align*}
This proves Proposition \ref{prop.pluecker.pre.row} \textbf{(a)}.
\end{proof}

Let us next state a variant of Proposition \ref{prop.pluecker.pre.row} where
rows are replaced by columns (and $n$ is renamed as $n+1$):

\begin{proposition}
\label{prop.pluecker.pre.col}Let $n\in\mathbb{N}$. Let $B\in\mathbb{K}%
^{n\times\left(  n+1\right)  }$. Then:

\textbf{(a)} We have%
\[
\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim r}\right)
B_{\bullet,r}=0_{n\times1}.
\]


\textbf{(b)} Write the matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n+1}$. Then,
\[
\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim r}\right)
b_{q,r}=0
\]
for every $q\in\left\{  1,2,\ldots,n\right\}  $.
\end{proposition}

\begin{example}
\label{exam.prop.pluecker.pre.col}For this example, set $n=2$ and $B=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}%
\end{array}
\right)  $. Then, Proposition \ref{prop.pluecker.pre.col} \textbf{(a)} says
that%
\begin{align*}
&  -\det\left(
\begin{array}
[c]{cc}%
a^{\prime} & a^{\prime\prime}\\
b^{\prime} & b^{\prime\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{c}%
a\\
b
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime\prime}\\
b & b^{\prime\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{c}%
a^{\prime}\\
b^{\prime}%
\end{array}
\right)  -\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{c}%
a^{\prime\prime}\\
b^{\prime\prime}%
\end{array}
\right) \\
&  =0_{2\times1}.
\end{align*}
Proposition \ref{prop.pluecker.pre.col} \textbf{(b)} (applied to $q=2$) yields%
\[
-\det\left(
\begin{array}
[c]{cc}%
a^{\prime} & a^{\prime\prime}\\
b^{\prime} & b^{\prime\prime}%
\end{array}
\right)  \cdot b+\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime\prime}\\
b & b^{\prime\prime}%
\end{array}
\right)  \cdot b^{\prime}-\det\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}%
\end{array}
\right)  \cdot b^{\prime\prime}=0.
\]

\end{example}

We shall obtain Proposition \ref{prop.pluecker.pre.col} by applying
Proposition \ref{prop.pluecker.pre.row} to $n+1$ and $B^{T}$
\ \ \ \ \footnote{Recall that $B^{T}$ denotes the transpose of the matrix $B$
(see Definition \ref{def.transpose}).} instead of $n$ and $B$. For this, we
shall need a really simple lemma:

\begin{lemma}
\label{lem.unrows.transpose.1}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$r\in\left\{  1,2,\ldots,m\right\}  $. Let $B\in\mathbb{K}^{n\times m}$. Then,
$\left(  B^{T}\right)  _{\sim r,\bullet}=\left(  B_{\bullet,\sim r}\right)
^{T}$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.unrows.transpose.1}.]Taking the transpose of a matrix
turns all its columns into rows. Thus, if we remove the $r$-th \textbf{column}
from $B$ and then take the transpose of the resulting matrix, then we obtain
the same matrix as if we first take the transpose of $B$ and then remove the
$r$-th \textbf{row} from it. Translating this statement into formulas, we
obtain precisely $\left(  B_{\bullet,\sim r}\right)  ^{T}=\left(
B^{T}\right)  _{\sim r,\bullet}$. Thus, Lemma \ref{lem.unrows.transpose.1} is
proven.\footnote{A more formal proof of this could be given using Proposition
\ref{prop.submatrix.easy} \textbf{(e)}.}
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.unrows.transpose.1}.]We shall use the notations
defined in Definition \ref{def.submatrix} and in Definition \ref{def.rowscols}%
. The definition of $B_{\bullet,\sim r}$ yields
\[
B_{\bullet,\sim r}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{r}%
,\ldots,m}B=\operatorname*{sub}\nolimits_{1,2,\ldots,n}^{1,2,\ldots
,\widehat{r},\ldots,m}B
\]
(by Proposition \ref{prop.submatrix.easy} \textbf{(c)}, applied to $A=B$ and
$v=m-1$ and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  =\left(  1,2,\ldots
,\widehat{r},\ldots,m\right)  $). Hence,%
\begin{equation}
\left(  \underbrace{B_{\bullet,\sim r}}_{=\operatorname*{sub}%
\nolimits_{1,2,\ldots,n}^{1,2,\ldots,\widehat{r},\ldots,m}B}\right)
^{T}=\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,n}^{1,2,\ldots
,\widehat{r},\ldots,m}B\right)  ^{T}=\operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{r},\ldots,m}^{1,2,\ldots,n}\left(  B^{T}\right)
\label{pf.lem.unrows.transpose.1.1}%
\end{equation}
(by Proposition \ref{prop.submatrix.easy} \textbf{(e)}, applied to $A=B$,
$u=n$, $v=m-1$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(
1,2,\ldots,n\right)  $ and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  =\left(
1,2,\ldots,\widehat{r},\ldots,m\right)  $).

On the other hand, $B^{T}$ is an $m\times n$-matrix (since $B$ is an $n\times
m$-matrix (since $B\in\mathbb{K}^{n\times m}$)). Hence, the definition of
$\left(  B^{T}\right)  _{\sim r,\bullet}$ yields%
\[
\left(  B^{T}\right)  _{\sim r,\bullet}=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{r},\ldots,m}\left(  B^{T}\right)
=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{r},\ldots,m}^{1,2,\ldots
,n}\left(  B^{T}\right)
\]
(by Proposition \ref{prop.submatrix.easy} \textbf{(b)}, applied to $m$, $n$,
$B^{T}$, $m-1$ and $\left(  1,2,\ldots,\widehat{r},\ldots,m\right)  $ instead
of $n$, $m$, $A$, $u$ and $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $).
Comparing this with (\ref{pf.lem.unrows.transpose.1.1}), we obtain $\left(
B^{T}\right)  _{\sim r,\bullet}=\left(  B_{\bullet,\sim r}\right)  ^{T}$.
Thus, Lemma \ref{lem.unrows.transpose.1} is proven.
\end{proof}
\end{verlong}

\begin{proof}
[Proof of Proposition \ref{prop.pluecker.pre.col}.]\textbf{(b)} Let
$q\in\left\{  1,2,\ldots,n\right\}  $. Thus, \newline$q\in\left\{
1,2,\ldots,n\right\}  =\left\{  1,2,\ldots,\left(  n+1\right)  -1\right\}  $
(since $n=\left(  n+1\right)  -1$).

We have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n+1}$. Thus,
the definition of $B^{T}$ yields%
\[
B^{T}=\left(  b_{j,i}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n}=\left(
b_{j,i}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq\left(  n+1\right)  -1}%
\]
(since $n=\left(  n+1\right)  -1$). Also, $B^{T}=\left(  b_{j,i}\right)
_{1\leq i\leq n+1,\ 1\leq j\leq\left(  n+1\right)  -1}\in\mathbb{K}^{\left(
n+1\right)  \times\left(  \left(  n+1\right)  -1\right)  }$. Thus, Proposition
\ref{prop.pluecker.pre.row} \textbf{(b)} (applied to $n+1$, $B^{T}$ and
$b_{j,i}$ instead of $n$, $B$ and $b_{i,j}$) yields%
\begin{equation}
\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  \left(  B^{T}\right)
_{\sim r,\bullet}\right)  b_{q,r}=0. \label{pf.prop.pluecker.pre.col.b.1}%
\end{equation}
But every $r\in\left\{  1,2,\ldots,n+1\right\}  $ satisfies%
\begin{equation}
\det\left(  \left(  B^{T}\right)  _{\sim r,\bullet}\right)  =\det\left(
B_{\bullet,\sim r}\right)  \label{pf.prop.pluecker.pre.col.b.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.pluecker.pre.col.b.2}):} Let
$r\in\left\{  1,2,\ldots,n+1\right\}  $. Then, $B_{\bullet,\sim r}%
\in\mathbb{K}^{n\times n}$ (since $B\in\mathbb{K}^{n\times\left(  n+1\right)
}$). In other words, $B_{\bullet,\sim r}$ is an $n\times n$-matrix. Thus,
Exercise \ref{exe.ps4.4} (applied to $A=B_{\bullet,\sim r}$) yields
$\det\left(  \left(  B_{\bullet,\sim r}\right)  ^{T}\right)  =\det\left(
B_{\bullet,\sim r}\right)  $.
\par
But Lemma \ref{lem.unrows.transpose.1} (applied to $m=n+1$) yields $\left(
B^{T}\right)  _{\sim r,\bullet}=\left(  B_{\bullet,\sim r}\right)  ^{T}$.
Thus, $\det\left(  \underbrace{\left(  B^{T}\right)  _{\sim r,\bullet}%
}_{=\left(  B_{\bullet,\sim r}\right)  ^{T}}\right)  =\det\left(  \left(
B_{\bullet,\sim r}\right)  ^{T}\right)  =\det\left(  B_{\bullet,\sim
r}\right)  $. This proves (\ref{pf.prop.pluecker.pre.col.b.2}).}. Hence,
(\ref{pf.prop.pluecker.pre.col.b.1}) yields%
\[
0=\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\underbrace{\det\left(  \left(
B^{T}\right)  _{\sim r,\bullet}\right)  }_{\substack{=\det\left(
B_{\bullet,\sim r}\right)  \\\text{(by (\ref{pf.prop.pluecker.pre.col.b.2}))}%
}}b_{q,r}=\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim
r}\right)  b_{q,r}.
\]
This proves Proposition \ref{prop.pluecker.pre.col} \textbf{(b)}.

\textbf{(a)} Write the matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n+1}$. For every $r\in\left\{  1,2,\ldots
,n+1\right\}  $, we have%
\begin{align*}
B_{\bullet,r}  &  =\left(  \text{the }r\text{-th column of the matrix
}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }B_{\bullet,r}\text{ is the }r\text{-th column of the matrix }B\\
\text{(by the definition of }B_{\bullet,r}\text{)}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
b_{1,r}\\
b_{2,r}\\
\vdots\\
b_{n,r}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n+1}\right) \\
&  =\left(  b_{i,r}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\end{align*}
Thus,%
\begin{align*}
&  \sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim
r}\right)  \underbrace{B_{\bullet,r}}_{=\left(  b_{i,r}\right)  _{1\leq i\leq
n,\ 1\leq j\leq1}}\\
&  =\sum_{r=1}^{n+1}\underbrace{\left(  -1\right)  ^{r}\det\left(
B_{\bullet,\sim r}\right)  \left(  b_{i,r}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}}_{=\left(  \left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim
r}\right)  b_{i,r}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}}=\sum_{r=1}%
^{n+1}\left(  \left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim r}\right)
b_{i,r}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\\
&  =\left(  \underbrace{\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(
B_{\bullet,\sim r}\right)  b_{i,r}}_{\substack{=0\\\text{(by Proposition
\ref{prop.pluecker.pre.col} \textbf{(b)}, applied to }q=i\text{)}}}\right)
_{1\leq i\leq n,\ 1\leq j\leq1}=\left(  0\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}=0_{n\times1}.
\end{align*}
This proves Proposition \ref{prop.pluecker.pre.col} \textbf{(a)}.
\end{proof}

\begin{remark}
\label{rmk.pluecker.pre.col.cramer}Proposition \ref{prop.pluecker.pre.col}
\textbf{(a)} can be viewed as a restatement of Cramer's rule (Theorem
\ref{thm.cramer} \textbf{(a)}). More precisely, it is easy to derive one of
these two facts from the other (although neither of the two is difficult to
prove to begin with). Let us sketch one direction of this argument: namely,
let us derive Theorem \ref{thm.cramer} \textbf{(a)} from Proposition
\ref{prop.pluecker.pre.col} \textbf{(a)}.

Indeed, let $n$, $A$, $b=\left(  b_{1},b_{2},\ldots,b_{n}\right)  ^{T}$ and
$A_{j}^{\#}$ be as in Theorem \ref{thm.cramer} \textbf{(a)}. We want to prove
that $A\cdot\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}%
^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\det A\cdot
b$.

Let $B=\left(  A\mid b\right)  $ (using the notations of Definition
\ref{def.addcol}); this is an $n\times\left(  n+1\right)  $-matrix.

Fix $r\in\left\{  1,2,\ldots,n\right\}  $. The matrix $B_{\bullet,\sim r}$
differs from the matrix $A_{r}^{\#}$ only in the order of its columns: More precisely,

\begin{itemize}
\item the matrix $B_{\bullet,\sim r}$ is obtained from the matrix $A$ by
removing the $r$-th column and attaching the column vector $b$ to the right
edge, whereas

\item the matrix $A_{r}^{\#}$ is obtained from the matrix $A$ by replacing the
$r$-th column by the column vector $b$.
\end{itemize}

Thus, the matrix $B_{\bullet,\sim r}$ can be obtained from the matrix
$A_{r}^{\#}$ by first switching the $r$-th and $\left(  r+1\right)  $-th
columns, then switching the $\left(  r+1\right)  $-th and $\left(  r+2\right)
$-th columns, etc., until finally switching the $\left(  n-1\right)  $-th and
$n$-th columns. Each of these switches multiplies the determinant by $-1$ (by
Exercise \ref{exe.ps4.6} \textbf{(b)}); thus, our sequence of switches
multiplies the determinant by $\left(  -1\right)  ^{n-r}=\left(  -1\right)
^{n+r}$. Hence,
\begin{equation}
\det\left(  B_{\bullet,\sim r}\right)  =\left(  -1\right)  ^{n+r}\det\left(
A_{r}^{\#}\right)  . \label{eq.rmk.pluecker.pre.col.cramer.1}%
\end{equation}


Now, forget that we fixed $r$. It is easy to see that every $\left(
v_{1},v_{2},\ldots,v_{n}\right)  ^{T}\in\mathbb{K}^{1\times n}$ satisfies%
\[
A\cdot\left(  v_{1},v_{2},\ldots,v_{n}\right)  ^{T}=\sum_{r=1}^{n}%
v_{r}A_{\bullet,r}.
\]
Applying this to $\left(  v_{1},v_{2},\ldots,v_{n}\right)  ^{T}=\left(
\det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)  ,\ldots
,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}$, we obtain%
\begin{equation}
A\cdot\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}^{\#}\right)
,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}=\sum_{r=1}^{n}\det\left(
A_{r}^{\#}\right)  A_{\bullet,r}. \label{eq.rmk.pluecker.pre.col.cramer.2}%
\end{equation}
But Proposition \ref{prop.pluecker.pre.col} \textbf{(a)} yields%
\begin{align*}
0_{n\times1}  &  =\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(
B_{\bullet,\sim r}\right)  B_{\bullet,r}\\
&  =\sum_{r=1}^{n}\left(  -1\right)  ^{r}\underbrace{\det\left(
B_{\bullet,\sim r}\right)  }_{\substack{=\left(  -1\right)  ^{n+r}\det\left(
A_{r}^{\#}\right)  \\\text{(by (\ref{eq.rmk.pluecker.pre.col.cramer.1}))}%
}}\underbrace{B_{\bullet,r}}_{\substack{=A_{\bullet,r}\\\text{(since
}B=\left(  A\mid b\right)  \text{ and }r\leq n\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{n+1}\det\left(
\underbrace{B_{\bullet,\sim\left(  n+1\right)  }}_{\substack{=A\\\text{(since
}B=\left(  A\mid b\right)  \text{)}}}\right)  \underbrace{B_{\bullet,n+1}%
}_{\substack{=b\\\text{(since }B=\left(  A\mid b\right)  \text{)}}}\\
&  =\sum_{r=1}^{n}\underbrace{\left(  -1\right)  ^{r}\left(  -1\right)
^{n+r}}_{=\left(  -1\right)  ^{n}}\det\left(  A_{r}^{\#}\right)  A_{\bullet
,r}+\underbrace{\left(  -1\right)  ^{n+1}}_{=-\left(  -1\right)  ^{n}}\det
A\cdot b\\
&  =\sum_{r=1}^{n}\left(  -1\right)  ^{n}\det\left(  A_{r}^{\#}\right)
A_{\bullet,r}-\left(  -1\right)  ^{n}\det A\cdot b\\
&  =\left(  -1\right)  ^{n}\left(  \sum_{r=1}^{n}\det\left(  A_{r}%
^{\#}\right)  A_{\bullet,r}-\det A\cdot b\right)  .
\end{align*}
Multiplying both sides of this equality by $\left(  -1\right)  ^{n}$, we
obtain%
\begin{align*}
0_{n\times1}  &  =\underbrace{\left(  -1\right)  ^{n}\left(  -1\right)  ^{n}%
}_{=1}\left(  \sum_{r=1}^{n}\det\left(  A_{r}^{\#}\right)  A_{\bullet,r}-\det
A\cdot b\right) \\
&  =\sum_{r=1}^{n}\det\left(  A_{r}^{\#}\right)  A_{\bullet,r}-\det A\cdot b.
\end{align*}
Hence,
\[
\det A\cdot b=\sum_{r=1}^{n}\det\left(  A_{r}^{\#}\right)  A_{\bullet
,r}=A\cdot\left(  \det\left(  A_{1}^{\#}\right)  ,\det\left(  A_{2}%
^{\#}\right)  ,\ldots,\det\left(  A_{n}^{\#}\right)  \right)  ^{T}%
\]
(by (\ref{eq.rmk.pluecker.pre.col.cramer.2})). Thus, we have derived Theorem
\ref{thm.cramer} \textbf{(a)} from Proposition \ref{prop.pluecker.pre.col}
\textbf{(a)}. Essentially the same argument (but read backwards) can be used
to derive Proposition \ref{prop.pluecker.pre.col} \textbf{(a)} from Theorem
\ref{thm.cramer} \textbf{(a)}.
\end{remark}

Now, we can easily prove the \textit{Pl\"{u}cker identity}:

\begin{theorem}
\label{thm.pluecker.plu}Let $n$ be a positive integer. Let $A\in
\mathbb{K}^{n\times\left(  n-1\right)  }$ and $B\in\mathbb{K}^{n\times\left(
n+1\right)  }$. Then,%
\[
\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  A\mid B_{\bullet,r}\right)
\det\left(  B_{\bullet,\sim r}\right)  =0
\]
(where we are using the notations from Definition \ref{def.addcol} and from
Definition \ref{def.unrows}).
\end{theorem}

\begin{example}
\label{exam.thm.pluecker.plu}If $n=3$, $A=\left(
\begin{array}
[c]{cc}%
a & a^{\prime}\\
b & b^{\prime}\\
c & c^{\prime}%
\end{array}
\right)  $ and $B=\left(
\begin{array}
[c]{cccc}%
x_{1} & x_{2} & x_{3} & x_{4}\\
y_{1} & y_{2} & y_{3} & y_{4}\\
z_{1} & z_{2} & z_{3} & z_{4}%
\end{array}
\right)  $, then Theorem \ref{thm.pluecker.plu} says that%
\begin{align*}
&  -\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x_{1}\\
b & b^{\prime} & y_{1}\\
c & c^{\prime} & z_{1}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{ccc}%
x_{2} & x_{3} & x_{4}\\
y_{2} & y_{3} & y_{4}\\
z_{2} & z_{3} & z_{4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x_{2}\\
b & b^{\prime} & y_{2}\\
c & c^{\prime} & z_{2}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{ccc}%
x_{1} & x_{3} & x_{4}\\
y_{1} & y_{3} & y_{4}\\
z_{1} & z_{3} & z_{4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x_{3}\\
b & b^{\prime} & y_{3}\\
c & c^{\prime} & z_{3}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{ccc}%
x_{1} & x_{2} & x_{4}\\
y_{1} & y_{2} & y_{4}\\
z_{1} & z_{2} & z_{4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & x_{4}\\
b & b^{\prime} & y_{4}\\
c & c^{\prime} & z_{4}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{ccc}%
x_{1} & x_{2} & x_{3}\\
y_{1} & y_{2} & y_{3}\\
z_{1} & z_{2} & z_{3}%
\end{array}
\right) \\
&  =0.
\end{align*}

\end{example}

\begin{proof}
[Proof of Theorem \ref{thm.pluecker.plu}.]Write the matrix $B$ in the form
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n+1}$.

Let $r\in\left\{  1,2,\ldots,n+1\right\}  $. Then,%
\begin{align*}
B_{\bullet,r}  &  =\left(  \text{the }r\text{-th column of the matrix
}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }B_{\bullet,r}\text{ is the }r\text{-th column of the matrix }B\\
\text{(by the definition of }B_{\bullet,r}\text{)}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
b_{1,r}\\
b_{2,r}\\
\vdots\\
b_{n,r}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n+1}\right) \\
&  =\left(  b_{1,r},b_{2,r},\ldots,b_{n,r}\right)  ^{T}.
\end{align*}
Hence, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to
$B_{\bullet,r}$ and $b_{i,r}$ instead of $v$ and $v_{i}$) shows that%
\begin{equation}
\det\left(  A\mid B_{\bullet,r}\right)  =\sum_{i=1}^{n}\left(  -1\right)
^{n+i}b_{i,r}\det\left(  A_{\sim i,\bullet}\right)  .
\label{pf.thm.pluecker.plu.1}%
\end{equation}


Now, forget that we fixed $r$. We thus have proven
(\ref{pf.thm.pluecker.plu.1}) for each $r\in\left\{  1,2,\ldots,n+1\right\}
$. Now,%
\begin{align*}
&  \sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\underbrace{\det\left(  A\mid
B_{\bullet,r}\right)  }_{\substack{=\sum_{i=1}^{n}\left(  -1\right)
^{n+i}b_{i,r}\det\left(  A_{\sim i,\bullet}\right)  \\\text{(by
(\ref{pf.thm.pluecker.plu.1}))}}}\det\left(  B_{\bullet,\sim r}\right) \\
&  =\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\left(  \sum_{i=1}^{n}\left(
-1\right)  ^{n+i}b_{i,r}\det\left(  A_{\sim i,\bullet}\right)  \right)
\det\left(  B_{\bullet,\sim r}\right) \\
&  =\underbrace{\sum_{r=1}^{n+1}\sum_{i=1}^{n}}_{=\sum_{i=1}^{n}\sum
_{r=1}^{n+1}}\underbrace{\left(  -1\right)  ^{r}\left(  -1\right)
^{n+i}b_{i,r}\det\left(  A_{\sim i,\bullet}\right)  \det\left(  B_{\bullet
,\sim r}\right)  }_{=\left(  -1\right)  ^{r}\det\left(  B_{\bullet,\sim
r}\right)  b_{i,r}\cdot\left(  -1\right)  ^{n+i}\det\left(  A_{\sim i,\bullet
}\right)  }\\
&  =\sum_{i=1}^{n}\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(
B_{\bullet,\sim r}\right)  b_{i,r}\cdot\left(  -1\right)  ^{n+i}\det\left(
A_{\sim i,\bullet}\right) \\
&  =\sum_{i=1}^{n}\underbrace{\left(  \sum_{r=1}^{n+1}\left(  -1\right)
^{r}\det\left(  B_{\bullet,\sim r}\right)  b_{i,r}\right)  }%
_{\substack{=0\\\text{(by Proposition \ref{prop.pluecker.pre.col}
\textbf{(b)},}\\\text{applied to }q=i\text{)}}}\left(  -1\right)  ^{n+i}%
\det\left(  A_{\sim i,\bullet}\right) \\
&  =\sum_{i=1}^{n}0\left(  -1\right)  ^{n+i}\det\left(  A_{\sim i,\bullet
}\right)  =0.
\end{align*}
This proves Theorem \ref{thm.pluecker.plu}.
\end{proof}

\begin{remark}
Theorem \ref{thm.pluecker.plu} (at least in the case when $\mathbb{K}$ is a
field) is essentially equivalent to \cite[\S 9.1, Exercise 1]{Fulton-Young},
to \cite[(QR)]{KleLak72}, to \cite[Theorem 3.4.11 (the \textquotedblleft
necessary\textquotedblright\ part)]{Jacobs10}, and to \cite[Proposition 3.3.2
(the \textquotedblleft only if\textquotedblright\ part)]{Lampe}.
\end{remark}

\begin{verlong}
\begin{remark}
\label{rmk.thm.pluecker.plu.GR}Theorem \ref{thm.pluecker.plu} is equivalent to
Proposition 14.4 in the detailed version of the paper\newline[GriRob14] Darij
Grinberg, Tom Roby, \textit{\textit{Iterative properties of birational
rowmotion}}, \href{http://arxiv.org/abs/1402.6178v6}{arXiv:1402.6178v6}.

More precisely, Theorem \ref{thm.pluecker.plu} is exactly Proposition 14.4 in
the detailed version of [GriRob14] (applied to $u=n$ and $w_{i}=B_{\bullet,i}%
$). Conversely, Proposition 14.4 in the detailed version of [GriRob14] is
precisely Theorem \ref{thm.pluecker.plu} (applied to $n=u$ and $B=\left(
w_{1}\mid w_{2}\mid\cdots\mid w_{u+1}\right)  $, where we are using the
notations of [GriRob14]).
\end{remark}
\end{verlong}

\begin{exercise}
\label{exe.pluecker.rederive-AC}Use Theorem \ref{thm.pluecker.plu} to give a
new proof of Proposition \ref{prop.desnanot.AC}.
\end{exercise}

\subsection{\label{sect.laplace}Laplace expansion in multiple rows/columns}

In this section, I shall show a (somewhat unwieldy, but classical and
important) generalization of Theorem \ref{thm.laplace.gen}. First, we shall
need some notations:

\begin{definition}
\label{def.sect.laplace.notations}Throughout Section \ref{sect.laplace}, we
shall use the following notations:

\begin{itemize}
\item If $I$ is a finite set of integers, then $\sum I$ shall denote the sum
of all elements of $I$. (Thus, $\sum I=\sum_{i\in I}i$.)

\item If $I$ is a finite set of integers, then $w\left(  I\right)  $ shall
denote the list of all elements of $I$ in increasing order (with no
repetitions). (For example, $w\left(  \left\{  3,4,8\right\}  \right)
=\left(  3,4,8\right)  $.)
\end{itemize}

We shall also use the notation introduced in Definition \ref{def.submatrix}.
If $n$, $m$, $A$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\left(
j_{1},j_{2},\ldots,j_{v}\right)  $ are as in Definition \ref{def.submatrix},
then we shall use the notation $\operatorname*{sub}\nolimits_{\left(
i_{1},i_{2},\ldots,i_{u}\right)  }^{\left(  j_{1},j_{2},\ldots,j_{v}\right)
}A$ as a synonym for $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}^{j_{1},j_{2},\ldots,j_{v}}A$.
\end{definition}

\begin{theorem}
\label{thm.det.laplace-multi}Let $n\in\mathbb{N}$. Let $A\in\mathbb{K}%
^{n\times n}$. For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let
$\widetilde{I}$ denote the complement $\left\{  1,2,\ldots,n\right\}
\setminus I$ of $I$. (For instance, if $n=4$ and $I=\left\{  1,4\right\}  $,
then $\widetilde{I}=\left\{  2,3\right\}  $.)

\textbf{(a)} For every subset $P$ of $\left\{  1,2,\ldots,n\right\}  $, we
have%
\[
\det A=\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\]


\textbf{(b)} For every subset $Q$ of $\left\{  1,2,\ldots,n\right\}  $, we
have%
\[
\det A=\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\]

\end{theorem}

Theorem \ref{thm.det.laplace-multi} is actually a generalization of Theorem
\ref{thm.laplace.gen}, known as \textquotedblleft Laplace expansion in
multiple rows (resp. columns)\textquotedblright. It appears (for example) in
\cite[Theorem 3.61]{Gill} and in \cite[Theorem 2.4.1]{Prasolov}.\footnote{Of
course, parts \textbf{(a)} and \textbf{(b)} of Theorem
\ref{thm.det.laplace-multi} are easily seen to be equivalent; thus, many
authors confine themselves to only stating one of them. For example, Theorem
\ref{thm.det.laplace-multi} is \cite[Lemma A.1 (f)]{CaSoSp12}.} Theorem
\ref{thm.laplace.gen} \textbf{(a)} can be recovered from Theorem
\ref{thm.det.laplace-multi} \textbf{(a)} by setting $P=\left\{  p\right\}  $;
similarly for the part \textbf{(b)}.

\begin{example}
Let us see what Theorem \ref{thm.det.laplace-multi} \textbf{(a)} says in a
simple case. For this example, set $n=4$ and $A=\left(  a_{i,j}\right)
_{1\leq i\leq4,\ 1\leq j\leq4}$. Also, set $P=\left\{  1,4\right\}
\subseteq\left\{  1,2,3,4\right\}  $; thus, $w\left(  P\right)  =\left(
1,4\right)  $, $\sum P=1+4=5$, $\left\vert P\right\vert =2$, $\widetilde{P}%
=\left\{  2,3\right\}  $ and $w\left(  \widetilde{P}\right)  =\left(
2,3\right)  $. Now, Theorem \ref{thm.det.laplace-multi} \textbf{(a)} says that%
\begin{align*}
&  \det A\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,3,4\right\}  ;\\\left\vert
Q\right\vert =2}}\left(  -1\right)  ^{5+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{1,4}^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{2,3}^{w\left(  \widetilde{Q}\right)  }A\right) \\
&  =\left(  -1\right)  ^{5+\left(  1+2\right)  }\det\left(
\operatorname*{sub}\nolimits_{1,4}^{1,2}A\right)  \det\left(
\operatorname*{sub}\nolimits_{2,3}^{3,4}A\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{5+\left(  1+3\right)  }%
\det\left(  \operatorname*{sub}\nolimits_{1,4}^{1,3}A\right)  \det\left(
\operatorname*{sub}\nolimits_{2,3}^{2,4}A\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{5+\left(  1+4\right)  }%
\det\left(  \operatorname*{sub}\nolimits_{1,4}^{1,4}A\right)  \det\left(
\operatorname*{sub}\nolimits_{2,3}^{2,3}A\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{5+\left(  2+3\right)  }%
\det\left(  \operatorname*{sub}\nolimits_{1,4}^{2,3}A\right)  \det\left(
\operatorname*{sub}\nolimits_{2,3}^{1,4}A\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{5+\left(  2+4\right)  }%
\det\left(  \operatorname*{sub}\nolimits_{1,4}^{2,4}A\right)  \det\left(
\operatorname*{sub}\nolimits_{2,3}^{1,3}A\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{5+\left(  3+4\right)  }%
\det\left(  \operatorname*{sub}\nolimits_{1,4}^{3,4}A\right)  \det\left(
\operatorname*{sub}\nolimits_{2,3}^{1,2}A\right) \\
&  =\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{4,1} & a_{4,2}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
a_{2,3} & a_{2,4}\\
a_{3,3} & a_{3,4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,3}\\
a_{4,1} & a_{4,3}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
a_{2,2} & a_{2,4}\\
a_{3,2} & a_{3,4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,4}\\
a_{4,1} & a_{4,4}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
a_{2,2} & a_{2,3}\\
a_{3,2} & a_{3,3}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,3}\\
a_{4,2} & a_{4,3}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
a_{2,1} & a_{2,4}\\
a_{3,1} & a_{3,4}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{cc}%
a_{1,2} & a_{1,4}\\
a_{4,2} & a_{4,4}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
a_{2,1} & a_{2,3}\\
a_{3,1} & a_{3,3}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{cc}%
a_{1,3} & a_{1,4}\\
a_{4,3} & a_{4,4}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{cc}%
a_{2,1} & a_{2,2}\\
a_{3,1} & a_{3,2}%
\end{array}
\right)  .
\end{align*}

\end{example}

The following lemma will play a crucial role in our proof of Theorem
\ref{thm.det.laplace-multi} (similar to the role that Lemma
\ref{lem.laplace.Apq} played in our proof of Theorem \ref{thm.laplace.gen}):

\begin{lemma}
\label{lem.det.laplace-multi.Apq}Let $n\in\mathbb{N}$. For any subset $I$ of
$\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote the
complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be two $n\times
n$-matrices. Let $P$ and $Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}
$ such that $\left\vert P\right\vert =\left\vert Q\right\vert $. Then,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  .
\end{align*}

\end{lemma}

The proof of Lemma \ref{lem.det.laplace-multi.Apq} is similar (in its spirit)
to the proof of Lemma \ref{lem.laplace.Apq}, but it requires a lot more
bookkeeping (if one wants to make it rigorous). This proof shall be given in
the solution to Exercise \ref{exe.det.laplace-multi}:

\begin{exercise}
\label{exe.det.laplace-multi}Prove Lemma \ref{lem.det.laplace-multi.Apq} and
Theorem \ref{thm.det.laplace-multi}.

[\textbf{Hint:} First, prove Lemma \ref{lem.det.laplace-multi.Apq} in the case
when $P=\left\{  1,2,\ldots,k\right\}  $ for some $k\in\left\{  0,1,\ldots
,n\right\}  $; in order to do so, use the bijection from Exercise
\ref{exe.Ialbe} \textbf{(c)} (applied to $I=Q$). Then, derive the general case
of Lemma \ref{lem.det.laplace-multi.Apq} by permuting the rows of the
matrices. Finally, prove Theorem \ref{thm.det.laplace-multi}.]
\end{exercise}

The following exercise generalizes Proposition \ref{prop.laplace.0} in the
same way as Theorem \ref{thm.det.laplace-multi} generalizes Theorem
\ref{thm.laplace.gen}:

\begin{exercise}
\label{exe.det.laplace-multi.0}Let $n\in\mathbb{N}$. Let $A\in\mathbb{K}%
^{n\times n}$. For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let
$\widetilde{I}$ denote the complement $\left\{  1,2,\ldots,n\right\}
\setminus I$ of $I$. Let $R$ be a subset of $\left\{  1,2,\ldots,n\right\}  $.
Prove the following:

\textbf{(a)} For every subset $P$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $\left\vert P\right\vert =\left\vert R\right\vert $ and $P\neq R$,
we have%
\[
0=\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\]


\textbf{(b)} For every subset $Q$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $\left\vert Q\right\vert =\left\vert R\right\vert $ and $Q\neq R$,
we have%
\[
0=\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\]

\end{exercise}

\subsection{$\det\left(  A+B\right)  $}

As Theorem \ref{thm.det(AB)} shows, the determinant of the product $AB$ of two
square matrices can be easily and neatly expressed through the determinants of
$A$ and $B$. In contrast, the determinant of a sum $A+B$ of two square
matrices cannot be expressed in such a way\footnote{It is easy to find two
$2\times2$-matrices $A_{1}$ and $B_{1}$ and two other $2\times2$-matrices
$A_{2}$ and $B_{2}$ such that $\det\left(  A_{1}\right)  =\det\left(
A_{2}\right)  $ and $\det\left(  B_{1}\right)  =\det\left(  B_{2}\right)  $
but $\det\left(  A_{1}+B_{1}\right)  \neq\det\left(  A_{2}+B_{2}\right)  $.
This shows that $\det\left(  A+B\right)  $ cannot generally be computed from
$\det A$ and $\det B$.}. There is, however, a formula for $\det\left(
A+B\right)  $ in terms of the determinants of submatrices of $A$ and $B$.
While it is rather unwieldy (a far cry from the elegance of Theorem
\ref{thm.det(AB)}), it is nevertheless useful sometimes; let us now show it:

\begin{theorem}
\label{thm.det(A+B)}Let $n\in\mathbb{N}$. For any subset $I$ of $\left\{
1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote the complement
$\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$. (For instance, if $n=4$
and $I=\left\{  1,4\right\}  $, then $\widetilde{I}=\left\{  2,3\right\}  $.)
Let us use the notations introduced in Definition \ref{def.submatrix} and in
Definition \ref{def.sect.laplace.notations}.

Let $A$ and $B$ be two $n\times n$-matrices. Then,%
\[
\det\left(  A+B\right)  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }%
\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\]

\end{theorem}

\begin{example}
For this example, set $n=2$, $A=\left(  a_{i,j}\right)  _{1\leq i\leq2,\ 1\leq
j\leq2}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq2,\ 1\leq j\leq2}$. Then,
Theorem \ref{thm.det(A+B)} says that%
\begin{align*}
&  \det\left(  A+B\right) \\
&  =\sum_{P\subseteq\left\{  1,2\right\}  }\sum_{\substack{Q\subseteq\left\{
1,2\right\}  ;\\\left\vert P\right\vert =\left\vert Q\right\vert }}\left(
-1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}B\right) \\
&  =\left(  -1\right)  ^{\sum\varnothing+\sum\varnothing}\det
\underbrace{\left(  \operatorname*{sub}\nolimits_{{}}^{{}}A\right)
}_{\substack{\text{this is the}\\0\times0\text{-matrix}}}\det\left(
\operatorname*{sub}\nolimits_{1,2}^{1,2}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{\sum\left\{  1\right\}
+\sum\left\{  1\right\}  }\det\left(  \operatorname*{sub}\nolimits_{1}%
^{1}A\right)  \det\left(  \operatorname*{sub}\nolimits_{2}^{2}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{\sum\left\{  1\right\}
+\sum\left\{  2\right\}  }\det\left(  \operatorname*{sub}\nolimits_{1}%
^{2}A\right)  \det\left(  \operatorname*{sub}\nolimits_{2}^{1}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{\sum\left\{  2\right\}
+\sum\left\{  1\right\}  }\det\left(  \operatorname*{sub}\nolimits_{2}%
^{1}A\right)  \det\left(  \operatorname*{sub}\nolimits_{1}^{2}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{\sum\left\{  2\right\}
+\sum\left\{  2\right\}  }\det\left(  \operatorname*{sub}\nolimits_{2}%
^{2}A\right)  \det\left(  \operatorname*{sub}\nolimits_{1}^{1}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{\sum\left\{  1,2\right\}
+\sum\left\{  1,2\right\}  }\det\left(  \operatorname*{sub}\nolimits_{1,2}%
^{1,2}A\right)  \det\underbrace{\left(  \operatorname*{sub}\nolimits_{{}}^{{}%
}B\right)  }_{\substack{\text{this is the}\\0\times0\text{-matrix}}}\\
&  =\underbrace{\det\left(  \text{the }0\times0\text{-matrix}\right)  }%
_{=1}\det\left(
\begin{array}
[c]{cc}%
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{c}%
a_{1,1}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
b_{2,2}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\det\left(
\begin{array}
[c]{c}%
a_{1,2}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
b_{2,1}%
\end{array}
\right)  -\det\left(
\begin{array}
[c]{c}%
a_{2,1}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
b_{1,2}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(
\begin{array}
[c]{c}%
a_{2,2}%
\end{array}
\right)  \det\left(
\begin{array}
[c]{c}%
b_{1,1}%
\end{array}
\right)  +\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  \underbrace{\det\left(  \text{the }0\times0\text{-matrix}\right)
}_{=1}\\
&  =\det\left(
\begin{array}
[c]{cc}%
b_{1,1} & b_{1,2}\\
b_{2,1} & b_{2,2}%
\end{array}
\right)  +a_{1,1}b_{2,2}-a_{1,2}b_{2,1}-a_{2,1}b_{1,2}+a_{2,2}b_{1,1}%
+\det\left(
\begin{array}
[c]{cc}%
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}%
\end{array}
\right)  .
\end{align*}

\end{example}

\begin{exercise}
\label{exe.det(A+B)}Prove Theorem \ref{thm.det(A+B)}.

[\textbf{Hint:} Use Lemma \ref{lem.det.laplace-multi.Apq}.]
\end{exercise}

Theorem \ref{thm.det(A+B)} takes a simpler form in the particular case when
the matrix $B$ is diagonal (i.e., has all entries outside of its diagonal
equal to $0$):

\begin{corollary}
\label{cor.det(A+D)}Let $n\in\mathbb{N}$. For every two objects $i$ and $j$,
define $\delta_{i,j}\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.

Let $A$ be an $n\times n$-matrix. Let $d_{1},d_{2},\ldots,d_{n}$ be $n$
elements of $\mathbb{K}$. Let $D$ be the $n\times n$-matrix $\left(
d_{i}\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then,%
\[
\det\left(  A+D\right)  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}.
\]

\end{corollary}

This corollary can easily be derived from Theorem \ref{thm.det(A+B)} using the
following fact:

\begin{lemma}
\label{lem.diag.minors}Let $n\in\mathbb{N}$. For every two objects $i$ and
$j$, define $\delta_{i,j}\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.

Let $d_{1},d_{2},\ldots,d_{n}$ be $n$ elements of $\mathbb{K}$. Let $D$ be the
$n\times n$-matrix $\left(  d_{i}\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$. Let $P$ and $Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $
such that $\left\vert P\right\vert =\left\vert Q\right\vert $. Then,%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }D\right)  =\delta_{P,Q}\prod_{i\in P}d_{i}.
\]

\end{lemma}

Proving Corollary \ref{cor.det(A+D)} and Lemma \ref{lem.diag.minors} in detail
is part of Exercise \ref{exe.det(A+B).diag} further below.

A particular case of Corollary \ref{cor.det(A+D)} is the following fact:

\begin{corollary}
\label{cor.det(A+X)}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix.
Let $x\in\mathbb{K}$. Then,%
\begin{align}
\det\left(  A+xI_{n}\right)   &  =\sum_{P\subseteq\left\{  1,2,\ldots
,n\right\}  }\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  P\right)  }A\right)  x^{n-\left\vert P\right\vert }%
\label{eq.cor.det(A+X).1}\\
&  =\sum_{k=0}^{n}\left(  \sum_{\substack{P\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert P\right\vert =n-k}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right)  \right)  x^{k}.
\label{eq.cor.det(A+X).2}%
\end{align}

\end{corollary}

\begin{exercise}
\label{exe.det(A+B).diag}Prove Corollary \ref{cor.det(A+D)}, Lemma
\ref{lem.diag.minors} and Corollary \ref{cor.det(A+X)}.
\end{exercise}

\begin{remark}
\label{rmk.charpol}Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix over
the commutative ring $\mathbb{K}$. Consider the commutative ring
$\mathbb{K}\left[  X\right]  $ of polynomials in the indeterminate $X$ over
$\mathbb{K}$ (that is, polynomials in the indeterminate $X$ with coefficients
lying in $\mathbb{K}$). We can then regard $A$ as a matrix over the ring
$\mathbb{K}\left[  X\right]  $ as well (because every element of $\mathbb{K}$
can be viewed as a constant polynomial in $\mathbb{K}\left[  X\right]  $).

Consider the $n\times n$-matrix $A+XI_{n}$ over the commutative ring
$\mathbb{K}\left[  X\right]  $. (For example, if $n=2$ and $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $, then $A+XI_{n}=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  +X\left(
\begin{array}
[c]{cc}%
1 & 0\\
0 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
a+X & b\\
c & d+X
\end{array}
\right)  $. In general, the matrix $A+XI_{n}$ is obtained from $A$ by adding
an $X$ to each diagonal entry.)

The determinant $\det\left(  A+XI_{n}\right)  $ is a polynomial in
$\mathbb{K}\left[  X\right]  $. (For instance, for $n=2$ and $A=\left(
\begin{array}
[c]{cc}%
a & b\\
c & d
\end{array}
\right)  $, we have%
\begin{align*}
\det\left(  A+XI_{n}\right)   &  =\det\left(
\begin{array}
[c]{cc}%
a+X & b\\
c & d+X
\end{array}
\right)  =\left(  a+X\right)  \left(  d+X\right)  -bc\\
&  =X^{2}+\left(  a+d\right)  X+\left(  ad-bc\right)  .
\end{align*}
)

This polynomial $\det\left(  A+XI_{n}\right)  $ is a highly important object;
it is a close relative of what is called the \textit{characteristic
polynomial} of $A$. (More precisely, the characteristic polynomial of $A$ is
either $\det\left(  XI_{n}-A\right)  $ or $\det\left(  A-XI_{n}\right)  $,
depending on the conventions that one is using; thus, the polynomial
$\det\left(  A+XI_{n}\right)  $ is either the characteristic polynomial of
$-A$ or $\left(  -1\right)  ^{n}$ times this characteristic polynomial.) For
more about the characteristic polynomial, see \cite[Section 4.5]{Artin} or
\cite[Chapter Five, Section II, \S 3]{Hefferon} (or various other texts on
linear algebra).

Using Corollary \ref{cor.det(A+X)}, we can explicitly compute the coefficients
of the polynomial $\det\left(  A+XI_{n}\right)  $. In fact,
(\ref{eq.cor.det(A+X).2}) (applied to $\mathbb{K}\left[  X\right]  $ and $X$
instead of $\mathbb{K}$ and $x$) yields%
\[
\det\left(  A+XI_{n}\right)  =\sum_{k=0}^{n}\left(  \sum_{\substack{P\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert P\right\vert =n-k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  P\right)
}A\right)  \right)  X^{k}.
\]
Hence, for every $k\in\left\{  0,1,\ldots,n\right\}  $, the coefficient of
$X^{k}$ in the polynomial $\det\left(  A+XI_{n}\right)  $ is
\[
\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =n-k}}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  P\right)  }A\right)  .
\]
In particular:

\begin{itemize}
\item The coefficient of $X^{n}$ in the polynomial $\det\left(  A+XI_{n}%
\right)  $ is%
\begin{align*}
&  \sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =n-n}}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  P\right)  }A\right) \\
&  =\det\underbrace{\left(  \operatorname*{sub}\nolimits_{w\left(
\varnothing\right)  }^{w\left(  \varnothing\right)  }A\right)  }_{=\left(
\text{the }0\times0\text{-matrix}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the only subset }P\text{ of }\left\{  1,2,\ldots,n\right\} \\
\text{satisfying }\left\vert P\right\vert =n-n\text{ is the empty set
}\varnothing
\end{array}
\right) \\
&  =\det\left(  \text{the }0\times0\text{-matrix}\right)  =1.
\end{align*}


\item The coefficient of $X^{0}$ in the polynomial $\det\left(  A+XI_{n}%
\right)  $ is%
\begin{align*}
&  \sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =n-0}}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  P\right)  }A\right) \\
&  =\det\underbrace{\left(  \operatorname*{sub}\nolimits_{w\left(  \left\{
1,2,\ldots,n\right\}  \right)  }^{w\left(  \left\{  1,2,\ldots,n\right\}
\right)  }A\right)  }_{=\operatorname*{sub}\nolimits_{1,2,\ldots
,n}^{1,2,\ldots,n}A=A}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the only subset }P\text{ of }\left\{  1,2,\ldots,n\right\} \\
\text{satisfying }\left\vert P\right\vert =n-0\text{ is the set }\left\{
1,2,\ldots,n\right\}
\end{array}
\right) \\
&  =\det A.
\end{align*}


\item Write the matrix $A$ as $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Then, the coefficient of $X^{n-1}$ in the polynomial
$\det\left(  A+XI_{n}\right)  $ is%
\begin{align*}
&  \sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =n-\left(  n-1\right)  }}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right) \\
&  =\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =1}}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  P\right)  }A\right)  =\sum_{k=1}^{n}\det\underbrace{\left(
\operatorname*{sub}\nolimits_{w\left(  \left\{  k\right\}  \right)
}^{w\left(  \left\{  k\right\}  \right)  }A\right)  }%
_{\substack{=\operatorname*{sub}\nolimits_{k}^{k}A=\left(
\begin{array}
[c]{c}%
a_{k,k}%
\end{array}
\right)  \\\text{(this is a }1\times1\text{-matrix)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the subsets }P\text{ of }\left\{  1,2,\ldots,n\right\} \\
\text{satisfying }\left\vert P\right\vert =1\text{ are the sets }\left\{
1\right\}  ,\left\{  2\right\}  ,\ldots,\left\{  n\right\}
\end{array}
\right) \\
&  =\sum_{k=1}^{n}\underbrace{\det\left(
\begin{array}
[c]{c}%
a_{k,k}%
\end{array}
\right)  }_{=a_{k,k}}=\sum_{k=1}^{n}a_{k,k}.
\end{align*}
In other words, this coefficient is the sum of all diagonal entries of $A$.
This sum is called the \textit{trace} of $A$, and is denoted by
$\operatorname*{Tr}A$.
\end{itemize}
\end{remark}

\subsection{Additional exercises}

Here are a few more additional exercises, with no importance to the rest of
the text (and mostly no solutions given).\Needspace{8cm}

\begin{addexercise}
\label{exeadd.noncomm.polarization}For every $n\in\mathbb{N}$, let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.

Let $\mathbb{L}$ be a noncommutative ring. (Keep in mind that our definition
of a \textquotedblleft noncommutative ring\textquotedblright\ includes all
commutative rings.)

Let $n\in\mathbb{N}$. The summation sign $\sum_{I\subseteq\left[  n\right]  }$
shall mean $\sum_{I\in\mathcal{P}\left(  \left[  n\right]  \right)  }$, where
$\mathcal{P}\left(  \left[  n\right]  \right)  $ denotes the powerset of
$\left[  n\right]  $.

Let $v_{1},v_{2},\ldots,v_{n}$ be $n$ elements of $\mathbb{L}$.

\textbf{(a)} Prove that%
\[
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{m}=\sum
_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }%
\]
for each $m\in\mathbb{N}$.

\textbf{(b)} Prove that%
\[
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{m}=0
\]
for each $m\in\left\{  0,1,\ldots,n-1\right\}  $.

\textbf{(c)} Prove that%
\[
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{n}=\sum_{\sigma\in S_{n}%
}v_{\sigma\left(  1\right)  }v_{\sigma\left(  2\right)  }\cdots v_{\sigma
\left(  n\right)  }.
\]


\textbf{(d)} Now, assume that $\mathbb{L}$ is a \textbf{commutative} ring.
Prove that%
\[
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{n}=n!\cdot v_{1}v_{2}\cdots
v_{n}.
\]


[\textbf{Hint:} First, generalize Lemma \ref{lem.prodrule2} to the case of a
noncommutative ring $\mathbb{K}$.]
\end{addexercise}

\begin{addexercise}
\label{exeadd.det.anotherpattern}\textbf{(a)} Compute the determinant of the
$7\times7$-matrix%
\[
\left(
\begin{array}
[c]{ccccccc}%
a & 0 & 0 & 0 & 0 & 0 & b\\
0 & a^{\prime} & 0 & 0 & 0 & b^{\prime} & 0\\
0 & 0 & a^{\prime\prime} & 0 & b^{\prime\prime} & 0 & 0\\
0 & 0 & 0 & e & 0 & 0 & 0\\
0 & 0 & c^{\prime\prime} & 0 & d^{\prime\prime} & 0 & 0\\
0 & c^{\prime} & 0 & 0 & 0 & d^{\prime} & 0\\
c & 0 & 0 & 0 & 0 & 0 & d
\end{array}
\right)  ,
\]
where $a,a^{\prime},a^{\prime\prime},b,b^{\prime},b^{\prime\prime}%
,c,c^{\prime},c^{\prime\prime},d,d^{\prime},d^{\prime\prime},e$ are elements
of $\mathbb{K}$.

\textbf{(e)} Compute the determinant of the $6\times6$-matrix%
\[
\left(
\begin{array}
[c]{cccccc}%
a & 0 & 0 & \ell & 0 & 0\\
0 & b & 0 & 0 & m & 0\\
0 & 0 & c & 0 & 0 & n\\
g & 0 & 0 & d & 0 & 0\\
0 & h & 0 & 0 & e & 0\\
0 & 0 & k & 0 & 0 & f
\end{array}
\right)  ,
\]
where $a,b,c,d,e,f,g,h,k,\ell,m,n$ are elements of $\mathbb{K}$.
\end{addexercise}

\begin{addexercise}
\label{exeadd.powerdet.gen}Let $n\in\mathbb{N}$. Let $A=\left(  a_{i,j}%
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.

\textbf{(a)} Prove that%
\[
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \sum_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{k}=0
\]
for each $k\in\left\{  0,1,\ldots,n-2\right\}  $.

\textbf{(b)} Assume that $n\geq1$. Prove that%
\[
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \sum_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{n-1}=\left(  n-1\right)
!\cdot\sum_{p=1}^{n}\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim
p,\sim q}\right)  .
\]
(Here, we are using the notation introduced in Definition
\ref{def.submatrix.minor}.)
\end{addexercise}

\begin{addexercise}
\label{addexe.jacobi-complement}Let $n\in\mathbb{N}$. For any subset $I$ of
$\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote the
complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$. (For instance,
if $n=4$ and $I=\left\{  1,4\right\}  $, then $\widetilde{I}=\left\{
2,3\right\}  $.) Let us use the notations introduced in Definition
\ref{def.submatrix} and in Definition \ref{def.sect.laplace.notations}.

Let $A\in\mathbb{K}^{n\times n}$ be an invertible matrix. Let $P$ and $Q$ be
two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
P\right\vert =\left\vert Q\right\vert $. Prove that%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  =\left(  -1\right)  ^{\sum P+\sum Q}\det A\cdot
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }\left(  A^{-1}\right)  \right)  .
\label{eq.addexe.jacobi-complement.1}%
\end{equation}


[\textbf{Hint:} Apply Exercise \ref{exe.block2x2.jacobi.rewr} to a matrix
obtained from $A$ by permuting the rows and permuting the columns.]
\end{addexercise}

\begin{remark}
The claim of Additional exercise \ref{addexe.jacobi-complement} is the
so-called \textit{Jacobi complementary minor theorem}. It appears, for
example, in \cite[(1)]{Lalonde} and in \cite[Lemma A.1 (e)]{CaSoSp12}, and is
used rather often when working with determinants (for example, it is used in
\cite[Chapter SYM, proof of Proposition (7.5) (5)]{LLPT95} and many times in
\cite{CaSoSp12}).

The determinant of a submatrix of a matrix $A$ is called a \textit{minor} of
$A$. Thus, in the equality (\ref{eq.addexe.jacobi-complement.1}), the
determinant $\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }A\right)  $ on the left hand side is a minor of $A$,
whereas the determinant $\det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)  }\left(
A^{-1}\right)  \right)  $ on the right hand side is a minor of $A^{-1}$. Thus,
roughly speaking, the equality (\ref{eq.addexe.jacobi-complement.1}) says that
any minor of $A$ equals a certain minor of $A^{-1}$ times $\det A$ times a
certain sign.

It is instructive to check the particular case of
(\ref{eq.addexe.jacobi-complement.1}) obtained when both $P$ and $Q$ are sets
of cardinality $n-1$ (so that $\widetilde{P}$ and $\widetilde{Q}$ are
$1$-element sets). This particular case turns out to be the statement of
Theorem \ref{thm.matrices.inverses.square} \textbf{(b)} in disguise.

Exercise \ref{exe.block2x2.jacobi.rewr} is the particular case of Additional
exercise \ref{addexe.jacobi-complement} obtained when $P=\left\{
1,2,\ldots,k\right\}  $ and $Q=\left\{  1,2,\ldots,k\right\}  $.
\end{remark}

\begin{addexercise}
\label{exeadd.det.pluecker.multi}Let $n$ and $k$ be positive integers such
that $k\leq n$. Let $A\in\mathbb{K}^{n\times\left(  n-k\right)  }$ and
$B\in\mathbb{K}^{n\times\left(  n+k\right)  }$.

Let us use the notations from Definition \ref{def.unrows}. For any subset $I$
of $\left\{  1,2,\ldots,n+k\right\}  $, we introduce the following five notations:

\begin{itemize}
\item Let $\sum I$ denote the sum of all elements of $I$. (Thus, $\sum
I=\sum_{i\in I}i$.)

\item Let $\widetilde{I}$ denote the complement $\left\{  1,2,\ldots
,n+k\right\}  \setminus I$.

\item Let $w\left(  I\right)  $ denote the list of all elements of $I$, in
increasing order (with no element occurring twice). (For example, $w\left(
\left\{  3,4,8\right\}  \right)  =\left(  3,4,8\right)  $.)

\item Let $\left(  A\mid B_{\bullet,I}\right)  $ denote the $n\times\left(
n-k+\left\vert I\right\vert \right)  $-matrix whose columns are
$\underbrace{A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,n-k}}_{\text{the
columns of }A},B_{\bullet,i_{1}},B_{\bullet,i_{2}},\ldots,B_{\bullet,i_{\ell}%
}$ (from left to right), where $\left(  i_{1},i_{2},\ldots,i_{\ell}\right)
=w\left(  I\right)  $.

\item Let $B_{\bullet,\sim I}$ denote the $n\times\left(  n+k-\left\vert
I\right\vert \right)  $-matrix whose columns are $B_{\bullet,j_{1}}%
,B_{\bullet,j_{2}},\ldots,B_{\bullet,j_{m}}$ (from left to right), where
$\left(  j_{1},j_{2},\ldots,j_{m}\right)  =w\left(  \widetilde{I}\right)  $.
(Equivalently, $B_{\bullet,\sim I}=\operatorname*{cols}\nolimits_{w\left(
\widetilde{I}\right)  }B$, using the notations of Definition
\ref{def.rowscols}.)
\end{itemize}

Then, prove that%
\[
\sum_{\substack{I\subseteq\left\{  1,2,\ldots,n+k\right\}  ;\\\left\vert
I\right\vert =k}}\left(  -1\right)  ^{\sum I+\left(  1+2+\cdots+k\right)
}\det\left(  A\mid B_{\bullet,I}\right)  \det\left(  B_{\bullet,\sim
I}\right)  =0.
\]
(Note that this generalizes Theorem \ref{thm.pluecker.plu}; indeed, the latter
theorem is the particular case for $k=1$.)
\end{addexercise}

\begin{addexercise}
\label{exeadd.det.delannoy}Recall that the binomial coefficients satisfy the
recurrence relation (\ref{eq.binom.rec.m}), which (visually) says that every
entry of Pascal's triangle is the sum of the two entries left-above it and
right-above it.

Let us now define a variation of Pascal's triangle as follows: Define a
nonnegative integer $\dbinom{m}{n}_{D}$ for every $m\in\mathbb{N}$ and
$n\in\mathbb{N}$ recursively as follows:

\begin{itemize}
\item Set $\dbinom{0}{n}_{D}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n>0
\end{cases}
$ for every $n\in\mathbb{N}$.

\item For every $m\in\mathbb{Z}$ and $n\in\mathbb{Z}$, set $\dbinom{m}{n}%
_{D}=0$ if either $m$ or $n$ is negative.

\item For every positive integer $m$ and every $n\in\mathbb{N}$, set
\[
\dbinom{m}{n}_{D}=\dbinom{m-1}{n-1}_{D}+\dbinom{m-1}{n}_{D}+\dbinom{m-2}%
{n-1}_{D}.
\]

\end{itemize}

(Thus, if we lay these $\dbinom{m}{n}_{D}$ out in the same way as the binomial
coefficients $\dbinom{m}{n}$ in Pascal's triangle, then every entry is the sum
of the three entries left-above it, right-above it, and straight above it.)

The integers $\dbinom{m}{n}_{D}$ are known as the
\href{https://en.wikipedia.org/wiki/Delannoy_number}{Delannoy numbers}.

\textbf{(a)} Show that
\[
\dbinom{n+m}{n}_{D}=\sum_{i=0}^{n}\dbinom{n}{i}\dbinom{m+i}{n}=\sum_{i=0}%
^{n}\dbinom{n}{i}\dbinom{m}{i}2^{i}%
\]
for every $n\in\mathbb{N}$ and $m\in\mathbb{N}$. (The second equality sign
here is a consequence of Proposition \ref{prop.binom.bin-id} \textbf{(e)}.)

\textbf{(b)} Let $n\in\mathbb{N}$. Let $A$ be the $n\times n$-matrix $\left(
\dbinom{i+j-2}{i-1}_{D}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (an analogue
of the matrix $A$ from Exercise \ref{exe.ps4.pascal}). Show that%
\[
\det A=2^{n\left(  n-1\right)  /2}.
\]

\end{addexercise}

\begin{noncompile}
\textbf{(a)} One can prove $\dbinom{n+m}{n}_{D}=\sum_{i=0}^{n}\dbinom{n}%
{i}\dbinom{m+i}{n}$ by strong induction over $n+m$, along the following lines:%
\begin{align*}
&  \sum_{i=0}^{n}\dbinom{n}{i}\underbrace{\dbinom{m+i}{n}}_{=\dbinom
{m+i-1}{n-1}+\dbinom{m+i-1}{n}}\\
&  =\sum_{i=0}^{n}\underbrace{\dbinom{n}{i}}_{=\dbinom{n-1}{i-1}+\dbinom
{n-1}{i}}\dbinom{m+i-1}{n-1}+\underbrace{\sum_{i=0}^{n}\dbinom{n}{i}%
\dbinom{m+i-1}{n}}_{=\dbinom{n+m-1}{n}_{D}}\\
&  =\underbrace{\sum_{i=0}^{n}\dbinom{n-1}{i-1}\dbinom{m+i-1}{n-1}%
}_{\substack{=\sum_{i=0}^{n}\dbinom{n-1}{i-1}\dbinom{m+\left(  i-1\right)
}{n-1}\\=\dbinom{\left(  n-1\right)  +m}{n-1}_{D}}}+\underbrace{\sum_{i=0}%
^{n}\dbinom{n-1}{i}\dbinom{m+i-1}{n-1}}_{\substack{=\sum_{i=0}^{n}\dbinom
{n-1}{i}\dbinom{\left(  m-1\right)  +i}{n-1}\\=\dbinom{\left(  n-1\right)
+\left(  m-1\right)  }{n-1}_{D}}}+\dbinom{n+m-1}{n}_{D}\\
&  =\dbinom{\left(  n-1\right)  +m}{n-1}_{D}+\dbinom{\left(  n-1\right)
+\left(  m-1\right)  }{n-1}_{D}+\dbinom{n+m-1}{n}_{D}=\dbinom{n+m}{n}_{D}.
\end{align*}


\textbf{(b)} Use $\dbinom{n+m}{n}_{D}=\sum_{i=0}^{n}\dbinom{n}{i}\dbinom{m}%
{i}2^{i}$ and the same tactic as in Exercise \ref{exe.ps4.pascal}.
\end{noncompile}

\begin{addexercise}
\label{exeadd.det.rk1upd}Let $n\in\mathbb{N}$. Let $u$ be a column vector with
$n$ entries, and let $v$ be a row vector with $n$ entries. (Thus, $uv$ is an
$n\times n$-matrix, whereas $vu$ is a $1\times1$-matrix.) Let $A$ be an
$n\times n$-matrix. Prove that%
\[
\det\left(  A+uv\right)  =\det A+v\left(  \operatorname*{adj}A\right)  u
\]
(where we regard the $1\times1$-matrix $v\left(  \operatorname*{adj}A\right)
u$ as an element of $\mathbb{K}$).
\end{addexercise}

\begin{addexercise}
\label{exeadd.det.resultant}Let $P=\sum_{k=0}^{d}p_{k}X^{k}$ and $Q=\sum
_{k=0}^{e}q_{k}X^{k}$ be two polynomials over $\mathbb{K}$ (where $p_{0}%
,p_{1},\ldots,p_{d}\in\mathbb{K}$ and $q_{0},q_{1},\ldots,q_{e}\in\mathbb{K}$
are their coefficients). Define a $\left(  d+e\right)  \times\left(
d+e\right)  $-matrix $A$ as follows:

\begin{itemize}
\item For every $k\in\left\{  1,2,\ldots,e\right\}  $, the $k$-th row of $A$
is%
\[
\left(  \underbrace{0,0,\ldots,0}_{k-1\text{ zeroes}},p_{d},p_{d-1}%
,\ldots,p_{1},p_{0},\underbrace{0,0,\ldots,0}_{e-k\text{ zeroes}}\right)  .
\]


\item For every $k\in\left\{  1,2,\ldots,d\right\}  $, the $\left(
e+k\right)  $-th row of $A$ is%
\[
\left(  \underbrace{0,0,\ldots,0}_{k-1\text{ zeroes}},q_{e},q_{e-1}%
,\ldots,q_{1},q_{0},\underbrace{0,0,\ldots,0}_{d-k\text{ zeroes}}\right)  .
\]

\end{itemize}

(For example, if $d=4$ and $e=3$, then%
\[
A=\left(
\begin{array}
[c]{ccccccc}%
p_{4} & p_{3} & p_{2} & p_{1} & p_{0} & 0 & 0\\
0 & p_{4} & p_{3} & p_{2} & p_{1} & p_{0} & 0\\
0 & 0 & p_{4} & p_{3} & p_{2} & p_{1} & p_{0}\\
q_{3} & q_{2} & q_{1} & q_{0} & 0 & 0 & 0\\
0 & q_{3} & q_{2} & q_{1} & q_{0} & 0 & 0\\
0 & 0 & q_{3} & q_{2} & q_{1} & q_{0} & 0\\
0 & 0 & 0 & q_{3} & q_{2} & q_{1} & q_{0}%
\end{array}
\right)  .
\]
)

Assume that the polynomials $P$ and $Q$ have a common root $z$ (that is, there
exists a $z\in\mathbb{K}$ such that $P\left(  z\right)  =0$ and $Q\left(
z\right)  =0$). Show that $\det A=0$.

[\textbf{Hint:} Find a column vector $v$ with $d+e$ entries satisfying
$Av=0_{\left(  d+e\right)  \times1}$; then apply Corollary
\ref{cor.adj.kernel}.]
\end{addexercise}

\begin{remark}
The matrix $A$ in Additional exercise \ref{exeadd.det.resultant} is called the
\href{https://en.wikipedia.org/wiki/Sylvester_matrix}{\textit{Sylvester
matrix}} of the polynomials $P$ and $Q$ (for degrees $d$ and $e$); its
determinant $\det A$ is known as their
\href{https://en.wikipedia.org/wiki/Resultant}{\textit{resultant}} (at least
when $d$ and $e$ are actually the degrees of $P$ and $Q$). According to the
exercise, the condition $\det A=0$ is necessary for $P$ and $Q$ to have a
common root. In the general case, the converse does not hold: For one, you can
always force $\det A$ to be $0$ by taking $d>\deg P$ and $e>\deg Q$ (so
$p_{d}=0$ and $q_{e}=0$, and thus the $1$-st column of $A$ consists of
zeroes). More importantly, the resultant of the two polynomials $X^{3}-1$ and
$X^{2}+X+1$ is $0$, but they only have common roots in $\mathbb{C}$, not in
$\mathbb{R}$. Thus, there is more to common roots than just the vanishing of a determinant.

However, if $\mathbb{K}$ is an algebraically closed field (I won't go into the
details of what this means, but an example of such a field is $\mathbb{C}$),
and if $d=\deg P$ and $e=\deg Q$, then the polynomials $P$ and $Q$ have a
common root \textbf{if and only if} their resultant is $0$.
\end{remark}

\section{\label{chp.quivers}On acyclic quivers and mutations}

\begin{remark}
Chapter \ref{chp.quivers} does not have much to do with the rest of these
notes; it is merely an unrelated exercise. It is also somewhat rough and might
be rewritten at some point.
\end{remark}

In this chapter, we will use the following notations (which come from
\cite[\S 2.1.1]{Lampe}):

\begin{itemize}
\item A \textit{quiver} means a tuple $Q=\left(  Q_{0},Q_{1},s,t\right)  $,
where $Q_{0}$ and $Q_{1}$ are two finite sets and where $s$ and $t$ are two
maps from $Q_{1}$ to $Q_{0}$. We call the elements of $Q_{0}$ the
\textit{vertices} of the quiver $Q$, and we call the elements of $Q_{1}$ the
\textit{arrows} of the quiver $Q$. For every $e\in Q_{1}$, we call $s\left(
e\right)  $ the \textit{starting point} of $e$ (and we say that $e$
\textit{starts at }$s\left(  e\right)  $), and we call $t\left(  e\right)  $
the \textit{terminal point} of $e$ (and we say that $e$ \textit{ends at}
$t\left(  e\right)  $). Furthermore, if $e\in Q_{1}$, then we say that $e$ is
an \textit{arrow from }$s\left(  e\right)  $ \textit{to }$t\left(  e\right)  $.

So the notion of a quiver is one of many different versions of the notion of a
finite directed graph. (Notice that it is a version which allows multiple
arrows, and which distinguishes between them -- i.e., the quiver stores not
just the information of how many arrows there are from a vertex to another,
but it actually has them all as distinguishable objects in $Q_{1}$. Lampe
himself seems to later tacitly switch to a different notion of quivers, where
edges from a given to vertex to another are indistinguishable and only exist
as a number. This does not matter for the next exercise, which works just as
well with either notion of a quiver; but I just wanted to have it mentioned.

\item The \textit{underlying undirected graph} of a quiver $Q=\left(
Q_{0},Q_{1},s,t\right)  $ is defined as the undirected multigraph with vertex
set $Q_{0}$ and edge multiset%
\[
\left\{  \left\{  s\left(  e\right)  ,t\left(  e\right)  \right\}
\ \mid\ e\in Q_{1}\right\}  _{\operatorname*{multiset}}.
\]
(\textquotedblleft Multigraph\textquotedblright\ means that multiple edges are
allowed, but we do not make them distinguishable.)

\item A quiver $Q=\left(  Q_{0},Q_{1},s,t\right)  $ is said to be
\textit{acyclic} if there is no sequence \newline$\left(  a_{0},a_{1}%
,\ldots,a_{n}\right)  $ of elements of $Q_{0}$ such that $n>0$ and
$a_{0}=a_{n}$ and such that $Q$ has an arrow from $a_{i}$ to $a_{i+1}$ for
every $i\in\left\{  0,1,\ldots,n-1\right\}  $. (This is equivalent to
\cite[Definition 2.1.7]{Lampe}.) Notice that this does not mean that the
\textit{undirected} version of $Q$ has no cycles.

\item Let $Q=\left(  Q_{0},Q_{1},s,t\right)  $. Then, a \textit{sink} of $Q$
means a vertex $v\in Q_{0}$ such that no $e\in Q_{1}$ starts at $v$ (in other
words, no arrow of $Q$ starts at $v$). A \textit{source} of $Q$ means a vertex
$v\in Q_{0}$ such that no $e\in Q_{1}$ ends at $v$ (in other words, no arrow
of $Q$ ends at $v$).

\item Let $Q=\left(  Q_{0},Q_{1},s,t\right)  $. If $i\in Q_{0}$ is a sink of
$Q$, then the \textit{mutation} $\mu_{i}\left(  Q\right)  $ of $Q$ at $i$ is
the quiver obtained from $Q$ simply by turning\footnote{To \textit{turn} an
arrow $e$ means to reverse its direction, i.e., to switch the values of
$s\left(  e\right)  $ and $t\left(  e\right)  $. We model this as a change to
the functions $s$ and $t$, not as a change to the arrow itself.} all arrows
ending at $i$. (To be really pedantic: We define $\mu_{i}\left(  Q\right)  $
as the quiver $\left(  Q_{0},Q_{1},s^{\prime},t^{\prime}\right)  $, where%
\begin{align*}
s^{\prime}\left(  e\right)   &  =\left\{
\begin{array}
[c]{c}%
t\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  =i;\\
s\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  \neq i
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for each }e\in Q_{1}\\
\text{and}\ \ \ \ \ \ \ \ \ \ t^{\prime}\left(  e\right)   &  =\left\{
\begin{array}
[c]{c}%
s\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  =i;\\
t\left(  e\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }t\left(  e\right)  \neq i
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for each }e\in Q_{1}.
\end{align*}
) If $i\in Q_{0}$ is a source of $Q$, then the \textit{mutation} $\mu
_{i}\left(  Q\right)  $ of $Q$ at $i$ is the quiver obtained from $Q$ by
turning all arrows starting at $i$. (Notice that if $i$ is both a source and a
sink of $Q$, then these two definitions give the same result; namely, $\mu
_{i}\left(  Q\right)  =Q$ in this case.)

If $Q$ is an acyclic quiver, then $\mu_{i}\left(  Q\right)  $ is acyclic as
well (whenever $i\in Q_{0}$ is a sink or a source of $Q$).

We use the word \textquotedblleft mutation\textquotedblright\ not only for the
quiver $\mu_{i}\left(  Q\right)  $, but also for the operation that transforms
$Q$ into $\mu_{i}\left(  Q\right)  $. (We have defined this operation only if
$i$ is a sink or a source of $Q$. It can be viewed as a particular case of the
more general definition of mutation given in \cite[Definition 2.2.1]{Lampe},
at least if one gives up the ability to distinguish different arrows from one
vertex to another.)
\end{itemize}

\Needspace{15\baselineskip}

\begin{exercise}
\label{exe.ps1.1.1}Let $Q=\left(  Q_{0},Q_{1},s,t\right)  $ be an acyclic quiver.

\textbf{(a)} Let $A$ and $B$ be two subsets of $Q_{0}$ such that $A\cap
B=\varnothing$ and $A\cup B=Q_{0}$. Assume that there exists no arrow of $Q$
that starts at a vertex in $B$ and ends at a vertex in $A$. Then, by turning
all arrows of $Q$ which start at a vertex in $A$ and end at a vertex in $B$,
we obtain a new acyclic quiver $\operatorname*{mut}\nolimits_{A,B}Q$.

(When we say \textquotedblleft turning all arrows of $Q$ which start at a
vertex in $A$ and end at a vertex in $B$\textquotedblright, we mean
\textquotedblleft turning all arrows $e$ of $Q$ which satisfy $s\left(
e\right)  \in A$ and $t\left(  e\right)  \in B$\textquotedblright. We do
\textbf{not} mean that we fix a vertex $a$ in $A$ and a vertex $b$ in $B$, and
only turn the arrows from $a$ to $b$.)

For example, if $Q=%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%3 \ar[r] & 4 \\
%1 \ar[u] \ar[ru] \ar[r] & 2 \ar[u]
%}}}%
%BeginExpansion
\xymatrix{
3 \ar[r] & 4 \\
1 \ar[u] \ar[ru] \ar[r] & 2 \ar[u]
}%
%EndExpansion
$ and $A=\left\{  1,3\right\}  $ and $B=\left\{  2,4\right\}  $, then
\newline$\operatorname*{mut}\nolimits_{A,B}Q=%
%TCIMACRO{\TeXButton{x}{\xymatrix{
%3 & 4 \ar[l] \ar[ld] \\
%1 \ar[u] & 2 \ar[u] \ar[l]
%}}}%
%BeginExpansion
\xymatrix{
3 & 4 \ar[l] \ar[ld] \\
1 \ar[u] & 2 \ar[u] \ar[l]
}%
%EndExpansion
$.

Prove that $\operatorname*{mut}\nolimits_{A,B}Q$ can be obtained from $Q$ by a
sequence of mutations at sinks. (More precisely, there exists a sequence
$\left(  Q^{\left(  0\right)  },Q^{\left(  1\right)  },\ldots,Q^{\left(
\ell\right)  }\right)  $ of acyclic quivers such that $Q^{\left(  0\right)
}=Q$, $Q^{\left(  \ell\right)  }=\operatorname*{mut}\nolimits_{A,B}Q$, and for
every $i\in\left\{  1,2,\ldots,\ell\right\}  $, the quiver $Q^{\left(
i\right)  }$ is obtained from $Q^{\left(  i-1\right)  }$ by mutation at a sink
of $Q^{\left(  i-1\right)  }$.)

[In our above example, we can mutate at $4$ first and then at $2$.]

\textbf{(b)} If $i\in Q_{0}$ is a \textbf{source} of $Q$, then show that the
mutation $\mu_{i}\left(  Q\right)  $ can be obtained from $Q$ by a sequence of
mutations at sinks.

\textbf{(c)} Assume now that the underlying \textbf{undirected} graph of $Q$
is a tree. (In particular, $Q$ cannot have more than one edge between two
vertices, as these would form a cycle in the underlying undirected graph!)
Show that any acyclic quiver which can be obtained from $Q$ by turning some of
its arrows can also be obtained from $Q$ by a sequence of mutations at sinks.
\end{exercise}

\begin{remark}
More general results than those of Exercise \ref{exe.ps1.1.1} are stated (for
directed graphs rather than quivers, but it is easy to translate from one
language into another) in \cite{Pretzel}.
\end{remark}

\section{Solutions}

This section contains solutions (or, sometimes, solution sketches) to some of
the exercises in the text, as well as occasional remarks. I do not recommend
reading them before trying to solve the problem on your own.

\subsection{Solution to Exercise \ref{exe.multinom1}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.multinom1}.]For each $i\in\left\{
0,1,\ldots,m\right\}  $, define an integer $s_{i}\in\mathbb{N}$ by
$s_{i}=k_{1}+k_{2}+\cdots+k_{i}$. We shall prove that%
\begin{equation}
\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}%
!}=\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}. \label{sol.exe.multinom1.short.goal}%
\end{equation}


[\textit{Proof of (\ref{sol.exe.multinom1.short.goal}):} If $m=0$, then
(\ref{sol.exe.multinom1.short.goal}) holds\footnote{\textit{Proof.} This is
just an exercise in understanding empty sums and empty products: Empty sums
such as $k_{1}+k_{2}+\cdots+k_{0}$ are defined to be $0$, and empty products
such as $k_{1}!k_{2}!\cdots k_{0}!$ or $\prod_{i=1}^{0}\dbinom{s_{i}}{k_{i}}$
are defined to be $1$. With this in mind (and remembering that $0!=1$), it
becomes completely straightforward to verify
(\ref{sol.exe.multinom1.short.goal}) when $m=0$.}. Hence, for the rest of the
proof of (\ref{sol.exe.multinom1.short.goal}), we WLOG assume that we don't
have $m=0$.

We have $m\geq1$ (since $m\in\mathbb{N}$ but we don't have $m=0$), so that
$m-1\geq0$.

The definition of $s_{0}$ yields $s_{0}=k_{1}+k_{2}+\cdots+k_{0}=\left(
\text{empty sum}\right)  =0$. Hence, $s_{0}!=0!=1$. Now,%
\begin{align}
\prod_{i=1}^{m}\dfrac{1}{s_{i-1}!}  &  =\prod_{i=0}^{m-1}\dfrac{1}{s_{i}%
!}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}i-1\text{ in the product}\right) \nonumber\\
&  =\underbrace{\dfrac{1}{s_{0}!}}_{\substack{=1\\\text{(since }%
s_{0}!=1\text{)}}}\prod_{i=1}^{m-1}\dfrac{1}{s_{i}!}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m-1\geq0\right) \nonumber\\
&  =\prod_{i=1}^{m-1}\dfrac{1}{s_{i}!}=\dfrac{1}{\prod_{i=1}^{m-1}s_{i}!}.
\label{sol.exe.multinom1.short.goal.pf.2}%
\end{align}


The definition of $s_{m}$ yields $s_{m}=k_{1}+k_{2}+\cdots+k_{m}$.

Each $i\in\left\{  1,2,\ldots,m\right\}  $ satisfies%
\[
\dbinom{s_{i}}{k_{i}}=s_{i}!\cdot\dfrac{1}{k_{i}!}\cdot\dfrac{1}{s_{i-1}!}%
\]
\footnote{\textit{Proof:} Let $i\in\left\{  1,2,\ldots,m\right\}  $. Then,
both $i-1$ and $i$ belong to the set $\left\{  0,1,\ldots,m\right\}  $. Hence,
the definition of $s_{i-1}$ yields $s_{i-1}=k_{1}+k_{2}+\cdots+k_{i-1}%
\in\mathbb{N}$ (since $k_{1},k_{2},\ldots,k_{i-1}$ are elements of
$\mathbb{N}$). Thus, $s_{i-1}\geq0$. Meanwhile, the definition of $s_{i}$
yields
\[
s_{i}=k_{1}+k_{2}+\cdots+k_{i}=\underbrace{\left(  k_{1}+k_{2}+\cdots
+k_{i-1}\right)  }_{=s_{i-1}}+k_{i}=\underbrace{s_{i-1}}_{\geq0}+k_{i}\geq
k_{i}.
\]
Also, $s_{i}=k_{1}+k_{2}+\cdots+k_{i}\in\mathbb{N}$ (since $k_{1},k_{2}%
,\ldots,k_{i}$ are elements of $\mathbb{N}$). From $s_{i}=s_{i-1}+k_{i}$, we
obtain $s_{i}-k_{i}=s_{i-1}$.
\par
Now, (\ref{eq.binom.formula}) (applied to $s_{i}$ and $k_{i}$ instead of $m$
and $n$) yields
\begin{align*}
\dbinom{s_{i}}{k_{i}}  &  =\dfrac{s_{i}!}{k_{i}!\left(  s_{i}-k_{i}\right)
!}=\dfrac{s_{i}!}{k_{i}!s_{i-1}!}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}s_{i}-k_{i}=s_{i-1}\right) \\
&  =s_{i}!\cdot\dfrac{1}{k_{i}!}\cdot\dfrac{1}{s_{i-1}!}.
\end{align*}
}. Hence,%
\begin{align*}
&  \prod_{i=1}^{m}\underbrace{\dbinom{s_{i}}{k_{i}}}_{=s_{i}!\cdot\dfrac
{1}{k_{i}!}\cdot\dfrac{1}{s_{i-1}!}}\\
&  =\prod_{i=1}^{m}\left(  s_{i}!\cdot\dfrac{1}{k_{i}!}\cdot\dfrac{1}%
{s_{i-1}!}\right)  =\underbrace{\left(  \prod_{i=1}^{m}s_{i}!\right)
}_{\substack{=\left(  \prod_{i=1}^{m-1}s_{i}!\right)  s_{m}!\\\text{(since
}m\geq1\text{)}}}\cdot\left(  \prod_{i=1}^{m}\dfrac{1}{k_{i}!}\right)
\cdot\underbrace{\left(  \prod_{i=1}^{m}\dfrac{1}{s_{i-1}!}\right)
}_{\substack{=\dfrac{1}{\prod_{i=1}^{m-1}s_{i}!}\\\text{(by
(\ref{sol.exe.multinom1.short.goal.pf.2}))}}}\\
&  =\left(  \prod_{i=1}^{m-1}s_{i}!\right)  s_{m}!\cdot\left(  \prod_{i=1}%
^{m}\dfrac{1}{k_{i}!}\right)  \cdot\dfrac{1}{\prod_{i=1}^{m-1}s_{i}!}\\
&  =\underbrace{s_{m}}_{=k_{1}+k_{2}+\cdots+k_{m}}!\cdot\underbrace{\left(
\prod_{i=1}^{m}\dfrac{1}{k_{i}!}\right)  }_{=\dfrac{1}{\prod_{i=1}^{m}k_{i}%
!}=\dfrac{1}{k_{1}!k_{2}!\cdots k_{m}!}}\\
&  =\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !\cdot\dfrac{1}{k_{1}%
!k_{2}!\cdots k_{m}!}=\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}%
{k_{1}!k_{2}!\cdots k_{m}!}.
\end{align*}
This proves (\ref{sol.exe.multinom1.short.goal}).]

On the other hand, each $i\in\left\{  1,2,\ldots,m\right\}  $ satisfies
$\dbinom{s_{i}}{k_{i}}\in\mathbb{Z}$ (by Proposition \ref{prop.binom.int}
(applied to $s_{i}$ and $k_{i}$ instead of $m$ and $n$)). In other words, for
each $i\in\left\{  1,2,\ldots,m\right\}  $, the number $\dbinom{s_{i}}{k_{i}}$
is an integer. Hence, $\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}$ is an integer
(since a product of integers always is an integer). In other words,
$\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}%
!}$ is an integer (since $\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)
!}{k_{1}!k_{2}!\cdots k_{m}!}=\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}$). Since
$\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}%
!}$ is clearly positive (because the numbers $\left(  k_{1}+k_{2}+\cdots
+k_{m}\right)  !$, $k_{1}!$, $k_{2}!$, $\ldots$, $k_{m}!$ are all positive),
we can thus conclude that $\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)
!}{k_{1}!k_{2}!\cdots k_{m}!}$ is a positive integer. This solves Exercise
\ref{exe.multinom1}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.multinom1}.]For each $i\in\left\{
0,1,\ldots,m\right\}  $, define an integer $s_{i}\in\mathbb{N}$ by
$s_{i}=k_{1}+k_{2}+\cdots+k_{i}$. We shall prove that%
\begin{equation}
\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}%
!}=\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}. \label{sol.exe.multinom1.goal}%
\end{equation}


[\textit{Proof of (\ref{sol.exe.multinom1.goal}):} If $m=0$, then
(\ref{sol.exe.multinom1.goal}) holds\footnote{\textit{Proof.} Assume that
$m=0$. We must prove that (\ref{sol.exe.multinom1.goal}) holds.
\par
Dividing the equality $\left(  \underbrace{k_{1}+k_{2}+\cdots+k_{0}}_{=\left(
\text{empty sum}\right)  =0}\right)  !=0!=1$ by the equality $k_{1}%
!k_{2}!\cdots k_{0}!=\left(  \text{empty product}\right)  =1$, we obtain
\[
\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{0}\right)  !}{k_{1}!k_{2}!\cdots k_{0}%
!}=\dfrac{1}{1}=1.
\]
Comparing this with $\prod_{i=1}^{0}\dbinom{s_{i}}{k_{i}}=\left(  \text{empty
product}\right)  =1$, we find%
\[
\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{0}\right)  !}{k_{1}!k_{2}!\cdots k_{0}%
!}=\prod_{i=1}^{0}\dbinom{s_{i}}{k_{i}}.
\]
This rewrites as $\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}%
{k_{1}!k_{2}!\cdots k_{m}!}=\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}$ (since
$m=0$). Thus, (\ref{sol.exe.multinom1.goal}) holds. Qed.}. Hence, for the rest
of the proof of (\ref{sol.exe.multinom1.goal}), we can WLOG assume that we
don't have $m=0$. Assume this.

We have $m\neq0$ (since we don't have $m=0$). Thus, $m\geq1$ (since
$m\in\mathbb{N}$), so that $m-1\geq0$.

We have $0\in\left\{  0,1,\ldots,m\right\}  $ (since $m\in\mathbb{N}$). The
definition of $s_{0}$ thus yields $s_{0}=k_{1}+k_{2}+\cdots+k_{0}=\left(
\text{empty sum}\right)  =0$. Hence, $s_{0}!=0!=1$. Now,%
\begin{align}
\prod_{i=1}^{m}\dfrac{1}{s_{i-1}!}  &  =\prod_{i=0}^{m-1}\dfrac{1}{s_{i}%
!}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}i-1\text{ in the product}\right) \nonumber\\
&  =\underbrace{\dfrac{1}{s_{0}!}}_{\substack{=1\\\text{(since }%
s_{0}!=1\text{)}}}\prod_{i=1}^{m-1}\dfrac{1}{s_{i}!}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the factor for }i=0\\
\text{from the product (since }m-1\geq0\text{)}%
\end{array}
\right) \nonumber\\
&  =\prod_{i=1}^{m-1}\dfrac{1}{s_{i}!}=\dfrac{1}{\prod_{i=1}^{m-1}s_{i}!}.
\label{sol.exe.multinom1.goal.pf.2}%
\end{align}


We have $m\in\left\{  0,1,\ldots,m\right\}  $ (since $m\in\mathbb{N}$). The
definition of $s_{m}$ thus yields $s_{m}=k_{1}+k_{2}+\cdots+k_{m}$.

Each $i\in\left\{  1,2,\ldots,m\right\}  $ satisfies%
\[
\dbinom{s_{i}}{k_{i}}=s_{i}!\cdot\dfrac{1}{k_{i}!}\cdot\dfrac{1}{s_{i-1}!}%
\]
\footnote{\textit{Proof:} Let $i\in\left\{  1,2,\ldots,m\right\}  $. Then,
$i-1\in\left\{  0,1,\ldots,m-1\right\}  \subseteq\left\{  0,1,\ldots
,m\right\}  $. Hence, the definition of $s_{i-1}$ yields $s_{i-1}=k_{1}%
+k_{2}+\cdots+k_{i-1}\in\mathbb{N}$ (since $k_{1},k_{2},\ldots,k_{i-1}$ are
elements of $\mathbb{N}$). Thus, $s_{i-1}\geq0$. But $i\in\left\{
1,2,\ldots,m\right\}  \subseteq\left\{  0,1,\ldots,m\right\}  $. Hence, the
definition of $s_{i}$ yields
\[
s_{i}=k_{1}+k_{2}+\cdots+k_{i}=\underbrace{\left(  k_{1}+k_{2}+\cdots
+k_{i-1}\right)  }_{=s_{i-1}}+k_{i}=\underbrace{s_{i-1}}_{\geq0}+k_{i}\geq
k_{i}.
\]
Also, $s_{i}=k_{1}+k_{2}+\cdots+k_{i}\in\mathbb{N}$ (since $k_{1},k_{2}%
,\ldots,k_{i}$ are elements of $\mathbb{N}$). From $s_{i}=s_{i-1}+k_{i}$, we
obtain $s_{i}-k_{i}=s_{i-1}$.
\par
Now, (\ref{eq.binom.formula}) (applied to $s_{i}$ and $k_{i}$ instead of $m$
and $n$) yields
\begin{align*}
\dbinom{s_{i}}{k_{i}}  &  =\dfrac{s_{i}!}{k_{i}!\left(  s_{i}-k_{i}\right)
!}=\dfrac{s_{i}!}{k_{i}!s_{i-1}!}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}s_{i}-k_{i}=s_{i-1}\right) \\
&  =s_{i}!\cdot\dfrac{1}{k_{i}!}\cdot\dfrac{1}{s_{i-1}!}.
\end{align*}
Qed.}. Hence,%
\begin{align*}
&  \prod_{i=1}^{m}\underbrace{\dbinom{s_{i}}{k_{i}}}_{=s_{i}!\cdot\dfrac
{1}{k_{i}!}\cdot\dfrac{1}{s_{i-1}!}}\\
&  =\prod_{i=1}^{m}\left(  s_{i}!\cdot\dfrac{1}{k_{i}!}\cdot\dfrac{1}%
{s_{i-1}!}\right)  =\underbrace{\left(  \prod_{i=1}^{m}s_{i}!\right)
}_{\substack{=\left(  \prod_{i=1}^{m-1}s_{i}!\right)  s_{m}!\\\text{(here, we
have split off the}\\\text{factor for }i=m\text{ from the}\\\text{product
(since }m\geq1\text{))}}}\cdot\left(  \prod_{i=1}^{m}\dfrac{1}{k_{i}!}\right)
\cdot\underbrace{\left(  \prod_{i=1}^{m}\dfrac{1}{s_{i-1}!}\right)
}_{\substack{=\dfrac{1}{\prod_{i=1}^{m-1}s_{i}!}\\\text{(by
(\ref{sol.exe.multinom1.goal.pf.2}))}}}\\
&  =\left(  \prod_{i=1}^{m-1}s_{i}!\right)  s_{m}!\cdot\left(  \prod_{i=1}%
^{m}\dfrac{1}{k_{i}!}\right)  \cdot\dfrac{1}{\prod_{i=1}^{m-1}s_{i}!}\\
&  =\underbrace{s_{m}}_{=k_{1}+k_{2}+\cdots+k_{m}}!\cdot\underbrace{\left(
\prod_{i=1}^{m}\dfrac{1}{k_{i}!}\right)  }_{=\dfrac{1}{\prod_{i=1}^{m}k_{i}%
!}=\dfrac{1}{k_{1}!k_{2}!\cdots k_{m}!}}\\
&  =\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !\cdot\dfrac{1}{k_{1}%
!k_{2}!\cdots k_{m}!}=\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}%
{k_{1}!k_{2}!\cdots k_{m}!}.
\end{align*}
This proves (\ref{sol.exe.multinom1.goal}).]

On the other hand, each $i\in\left\{  1,2,\ldots,m\right\}  $ satisfies
$\dbinom{s_{i}}{k_{i}}\in\mathbb{Z}$ (by Proposition \ref{prop.binom.int}
(applied to $s_{i}$ and $k_{i}$ instead of $m$ and $n$)). In other words, for
each $i\in\left\{  1,2,\ldots,m\right\}  $, the number $\dbinom{s_{i}}{k_{i}}$
is an integer. Hence, $\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}$ is a product of
integers. Thus, $\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}$ is an integer (since a
product of integers always is an integer). In other words, $\dfrac{\left(
k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}!}$ is an integer
(since $\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots
k_{m}!}=\prod_{i=1}^{m}\dbinom{s_{i}}{k_{i}}$). Since $\dfrac{\left(
k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots k_{m}!}$ is clearly
positive (because the numbers $\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !$,
$k_{1}!$, $k_{2}!$, $\ldots$, $k_{m}!$ are all positive), we can thus conclude
that $\dfrac{\left(  k_{1}+k_{2}+\cdots+k_{m}\right)  !}{k_{1}!k_{2}!\cdots
k_{m}!}$ is a positive integer. This solves Exercise \ref{exe.multinom1}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.prop.binom.subsets}}

\begin{vershort}
Exercise \ref{exe.prop.binom.subsets} asks us to prove Proposition
\ref{prop.binom.subsets}.

First, let us handle two trivial cases of Proposition \ref{prop.binom.subsets}:

\begin{lemma}
\label{lem.sol.prop.binom.subsets.n=0}Proposition \ref{prop.binom.subsets}
holds in the case when $n=0$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.binom.subsets.n=0}.]Let $m\in\mathbb{N}$,
and let $S$ be an $m$-element set. Then, the set $S$ has exactly one
$0$-element subset (namely, the empty set $\varnothing$). Thus, the number of
all $0$-element subsets of $S$ is $1$.

On the other hand, $\dbinom{m}{0}=1$ (by (\ref{eq.binom.00})). Thus,
$\dbinom{m}{0}$ is the number of all $0$-element subsets of $S$ (since the
number of all $0$-element subsets of $S$ is $1$).

Now, forget that we fixed $m$ and $S$. We thus have shown that if
$m\in\mathbb{N}$ and if $S$ is an $m$-element set, then $\dbinom{m}{0}$ is the
number of all $0$-element subsets of $S$. In other words, Proposition
\ref{prop.binom.subsets} holds in the case when $n=0$. This proves Lemma
\ref{lem.sol.prop.binom.subsets.n=0}.
\end{proof}

\begin{lemma}
\label{lem.sol.prop.binom.subsets.m=0}Proposition \ref{prop.binom.subsets}
holds in the case when $m=0$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.binom.subsets.m=0}.]Let $m$, $n$ and $S$ be
as in Proposition \ref{prop.binom.subsets}. Assume that $m=0$. We then must
prove that Proposition \ref{prop.binom.subsets} holds for these $m$, $n$ and
$S$.

If $n=0$, then this follows from Lemma \ref{lem.sol.prop.binom.subsets.n=0}.
Hence, for the rest of this proof, we can WLOG assume that $n\neq0$. Assume this.

From $n\neq0$, we conclude that $n$ is a positive integer. Hence, $n>0=m$, so
that $m<n$. Thus, (\ref{eq.binom.0}) yields $\dbinom{m}{n}=0$.

But $S$ is an $m$-element set, i.e., a $0$-element set (since $m=0$). In other
words, $S=\varnothing$. Hence, $S$ has no $n$-element
subsets\footnote{\textit{Proof.} Assume the contrary. Thus, $S$ has an
$n$-element subset. Let $Q$ be such a subset. Thus, $Q\subseteq S$ and
$\left\vert Q\right\vert =n$. Since $Q\subseteq S=\varnothing$, we have
$Q=\varnothing$ and thus $\left\vert Q\right\vert =0$. This contradicts
$\left\vert Q\right\vert =n\neq0$. This contradiction shows that our
assumption was wrong, qed.}. In other words, the number of all $n$-element
subsets of $S$ is $0$. Since $\dbinom{m}{n}$ is also $0$, this shows that
$\dbinom{m}{n}$ is the number of all $n$-element subsets of $S$. Hence,
Proposition \ref{prop.binom.subsets} holds for our $m$, $n$ and $S$. This
proves Lemma \ref{lem.sol.prop.binom.subsets.m=0}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.binom.subsets}.]We shall prove Proposition
\ref{prop.binom.subsets} by induction over $m$:

\textit{Induction base:} Lemma \ref{lem.sol.prop.binom.subsets.m=0} shows that
Proposition \ref{prop.binom.subsets} holds in the case when $m=0$. This
completes the induction base.

\textit{Induction step:} Let $M$ be a positive integer. Assume that
Proposition \ref{prop.binom.subsets} holds for $m=M-1$. We now must prove that
Proposition \ref{prop.binom.subsets} holds for $m=M$.

We have assumed that Proposition \ref{prop.binom.subsets} holds for $m=M-1$.
In other words, if $n\in\mathbb{N}$, and if $S$ is an $\left(  M-1\right)
$-element set, then%
\begin{equation}
\dbinom{M-1}{n}\text{ is the number of all }n\text{-element subsets of }S.
\label{pf.prop.binom.subsets.ihyp}%
\end{equation}


Now, let $n\in\mathbb{N}$, and let $S$ be an $M$-element set. We shall show
that
\begin{equation}
\dbinom{M}{n}\text{ is the number of all }n\text{-element subsets of }S.
\label{pf.prop.binom.subsets.igoal}%
\end{equation}


\textit{Proof of (\ref{pf.prop.binom.subsets.igoal}):} If $n=0$, then
(\ref{pf.prop.binom.subsets.igoal}) follows from Lemma
\ref{lem.sol.prop.binom.subsets.n=0}\footnote{\textit{Proof.} Lemma
\ref{lem.sol.prop.binom.subsets.n=0} yields that Proposition
\ref{prop.binom.subsets} holds for $n=0$. Hence, we can apply Proposition
\ref{prop.binom.subsets} to $n=0$ and $m=M$. We thus obtain that $\dbinom
{M}{0}$ is the number of all $0$-element subsets of $S$. If $n=0$, then this
rewrites as follows: $\dbinom{M}{n}$ is the number of all $n$-element subsets
of $S$. Hence, if $n=0$, then (\ref{pf.prop.binom.subsets.igoal}) holds.}.
Thus, for the rest of this proof of (\ref{pf.prop.binom.subsets.igoal}), we
can WLOG assume that $n\neq0$. Assume this.

Now, $n$ is a positive integer (since $n\in\mathbb{N}$ and $n\neq0$); thus,
$n-1\in\mathbb{N}$.

Now, $S$ is an $M$-element set; thus, $\left\vert S\right\vert =M>0$. Hence,
the set $S$ is nonempty. In other words, there exists an $s\in S$. Fix such an
$s$. Then, $\left\vert S\setminus\left\{  s\right\}  \right\vert
=\underbrace{\left\vert S\right\vert }_{=M}-1=M-1$. In other words,
$S\setminus\left\{  s\right\}  $ is an $\left(  M-1\right)  $-element set.
Hence, (\ref{pf.prop.binom.subsets.ihyp}) (applied to $S\setminus\left\{
s\right\}  $ instead of $S$) yields that%
\begin{equation}
\dbinom{M-1}{n}\text{ is the number of all }n\text{-element subsets of
}S\setminus\left\{  s\right\}  . \label{pf.prop.binom.subsets.ihyp.1}%
\end{equation}
Also, (\ref{pf.prop.binom.subsets.ihyp}) (applied to $n-1$ and $S\setminus
\left\{  s\right\}  $ instead of $n$ and $S$) yields that%
\begin{equation}
\dbinom{M-1}{n-1}\text{ is the number of all }\left(  n-1\right)
\text{-element subsets of }S\setminus\left\{  s\right\}  .
\label{pf.prop.binom.subsets.ihyp.2}%
\end{equation}


Now, the $n$-element subsets of $S$ can be classified into two types: the ones
that contain $s$, and the ones that don't. We can count them separately:

\begin{itemize}
\item The $n$-element subsets of $S$ that contain $s$ are in bijection with
the $\left(  n-1\right)  $-element subsets of $S\setminus\left\{  s\right\}
$. More precisely: To each $\left(  n-1\right)  $-element subset $U$ of
$S\setminus\left\{  s\right\}  $, we can assign a unique $n$-element subset of
$S$ that contains $s$ (namely, $U\cup\left\{  s\right\}  $); and this
assignment is bijective (i.e., each $n$-element subset of $S$ that contains
$s$ gets assigned to exactly one $\left(  n-1\right)  $-element subsets of
$S\setminus\left\{  s\right\}  $).\ \ \ \ \footnote{Let me restate this in
even more formal terms:
\par
Let $\mathbf{A}$ be the set of all $\left(  n-1\right)  $-element subsets of
$S\setminus\left\{  s\right\}  $. Let $\mathbf{B}$ be the set of all
$n$-element subsets of $S$ that contain $s$. Then, the map%
\[
\mathbf{A}\rightarrow\mathbf{B},\ \ \ \ \ \ \ \ \ \ U\mapsto U\cup\left\{
s\right\}
\]
is well-defined and bijective.
\par
(If you want to prove this formally, you need to prove two statements:
\par
\begin{enumerate}
\item The map $\mathbf{A}\rightarrow\mathbf{B},\ U\mapsto U\cup\left\{
s\right\}  $ is well-defined (i.e., we have $U\cup\left\{  s\right\}
\in\mathbf{B}$ for each $U\in\mathbf{A}$).
\par
\item This map is bijective.
\end{enumerate}
\par
Proving the first statement is straightforward. The best way to prove the
second statement is to show that the map $\mathbf{A}\rightarrow\mathbf{B}%
,\ U\mapsto U\cup\left\{  s\right\}  $ has an inverse -- namely, the map
$\mathbf{B}\rightarrow\mathbf{A},\ V\mapsto V\setminus\left\{  s\right\}  $.
Of course, you would also have to show that this latter map is well-defined,
too.)} Hence,
\begin{align}
&  \left(  \text{the number of all }n\text{-element subsets of }S\text{ that
contain }s\right) \nonumber\\
&  =\left(  \text{the number of all }\left(  n-1\right)  \text{-element
subsets of }S\setminus\left\{  s\right\}  \right) \nonumber\\
&  =\dbinom{M-1}{n-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.binom.subsets.ihyp.2})}\right)  .
\label{pf.prop.binom.subsets.i1}%
\end{align}


\item The $n$-element subsets of $S$ that don't contain $s$ are precisely the
$n$-element subsets of $S\setminus\left\{  s\right\}  $ (because the subsets
of $S$ that don't contain $s$ are precisely the subsets of $S\setminus\left\{
s\right\}  $). Hence,%
\begin{align}
&  \left(  \text{the number of all }n\text{-element subsets of }S\text{ that
don't contain }s\right) \nonumber\\
&  =\left(  \text{the number of all }n\text{-element subsets of }%
S\setminus\left\{  s\right\}  \right) \nonumber\\
&  =\dbinom{M-1}{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.binom.subsets.ihyp.1})}\right)  .
\label{pf.prop.binom.subsets.i2}%
\end{align}

\end{itemize}

Now, every $n$-element subset of $S$ either contains $s$ or does not. Hence,%
\begin{align*}
&  \left(  \text{the number of all }n\text{-element subsets of }S\right) \\
&  =\underbrace{\left(  \text{the number of all }n\text{-element subsets of
}S\text{ that contain }s\right)  }_{\substack{=\dbinom{M-1}{n-1}\\\text{(by
(\ref{pf.prop.binom.subsets.i1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the number of all
}n\text{-element subsets of }S\text{ that don't contain }s\right)
}_{\substack{=\dbinom{M-1}{n}\\\text{(by (\ref{pf.prop.binom.subsets.i2}))}%
}}\\
&  =\dbinom{M-1}{n-1}+\dbinom{M-1}{n}.
\end{align*}
Compared with
\[
\dbinom{M}{n}=\dbinom{M-1}{n-1}+\dbinom{M-1}{n}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{eq.binom.rec.m}), applied to }m=M\right)  ,
\]
this yields $\left(  \text{the number of all }n\text{-element subsets of
}S\right)  =\dbinom{M}{n}$. Hence, (\ref{pf.prop.binom.subsets.igoal}) is proven.

Now, forget that we fixed $n$ and $S$. We thus have shown that every
$n\in\mathbb{N}$ and every $M$-element set $S$ satisfy
(\ref{pf.prop.binom.subsets.igoal}). In other words, Proposition
\ref{prop.binom.subsets} holds for $m=M$. This completes the inducton step.
Thus, Proposition \ref{prop.binom.subsets} is proven by induction.
\end{proof}
\end{vershort}

\begin{verlong}
In order to solve Exercise \ref{exe.prop.binom.subsets}, we need to prove
Proposition \ref{prop.binom.subsets}. We shall do this in detail; but first,
let us introduce a notation:

\begin{definition}
\label{def.sol.prop.binom.subsets.Pm}Let $S$ be a set. Let $m\in\mathbb{N}$.
Then, $\mathcal{P}_{m}\left(  S\right)  $ will denote the set of all
$m$-element subsets of $S$.
\end{definition}

\begin{proposition}
\label{prop.sol.prop.binom.subsets.Pm.lem}\textbf{(a)} We have $\mathcal{P}%
_{0}\left(  S\right)  =\left\{  \varnothing\right\}  $ for every set $S$.

\textbf{(b)} For every positive integer $m$, we have $\mathcal{P}_{m}\left(
\varnothing\right)  =\varnothing$.

\textbf{(c)} Let $S$ be a set. Let $s\in S$. Let $m$ be a positive integer.
Then, $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
\subseteq\mathcal{P}_{m}\left(  S\right)  $. Furthermore, the map%
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
is well-defined and a bijection.
\end{proposition}

Let us briefly explain what Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} says. Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(a)} says that the only
$0$-element subset of any set $S$ is $\varnothing$. Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(b)} says that the empty set
$\varnothing$ has no $m$-element subsets when $m$ is a positive integer.
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(c)} says that if
$s$ is an element of a set $S$, and if $m$ is a positive integer, then:

\begin{itemize}
\item all $m$-element subsets of $S\setminus\left\{  s\right\}  $ are
$m$-element subsets of $S$ as well;

\item the $m$-element subsets of $S$ which are \textbf{not} $m$-element
subsets of $S\setminus\left\{  s\right\}  $ are in bijection with the $\left(
m-1\right)  $-element subsets of $S\setminus\left\{  s\right\}  $; this
bijection sends an $\left(  m-1\right)  $-element subset $U$ of $S\setminus
\left\{  s\right\}  $ to the $m$-element subset $U\cup\left\{  s\right\}  $ of
$S$.
\end{itemize}

This restatement should make Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} obvious (or, at least, intuitively
clear). For the sake of completeness, let me also give a formal proof:

\begin{proof}
[Proof of Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem}.]\textbf{(a)}
Let $S$ be a set. The set $\varnothing$ is clearly a $0$-element subset of
$S$, and furthermore is the only $0$-element subset of $S$ (since
$\varnothing$ is the only $0$-element set). Now, the definition of
$\mathcal{P}_{0}\left(  S\right)  $ shows that $\mathcal{P}_{0}\left(
S\right)  $ is the set of all $0$-element subsets of $S$. Thus,%
\[
\mathcal{P}_{0}\left(  S\right)  =\left(  \text{the set of all }%
0\text{-element subsets of }S\right)  =\left\{  \varnothing\right\}
\]
(since $\varnothing$ is the only $0$-element subset of $S$). This proves
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(a)}.

\textbf{(b)} Let $m$ be a positive integer. Then, there exist no $m$-element
subsets of $\varnothing$\ \ \ \ \footnote{\textit{Proof.} Let $U$ be an
$m$-element subset of $\varnothing$. Thus, $U$ is an $m$-element set; hence,
$\left\vert U\right\vert =m>0$ (since $m$ is positive). But $U$ is a subset of
$\varnothing$; hence, $\left\vert U\right\vert \leq\left\vert \varnothing
\right\vert =0$. This contradicts $\left\vert U\right\vert >0$.
\par
Now, forget that we fixed $U$. We thus have found a contradiction for every
$m$-element subset $U$ of $\varnothing$. Hence, there exist no $m$-element
subsets of $\varnothing$. Qed.}. Now, the definition of $\mathcal{P}%
_{m}\left(  \varnothing\right)  $ shows that $\mathcal{P}_{m}\left(
\varnothing\right)  $ is the set of all $m$-element subsets of $\varnothing$.
Hence,
\[
\mathcal{P}_{m}\left(  \varnothing\right)  =\left(  \text{the set of all
}m\text{-element subsets of }\varnothing\right)  =\varnothing
\]
(since there exist no $m$-element subsets of $\varnothing$). This proves
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(b)}.

\textbf{(c)} We have $m-1\in\mathbb{N}$ (since $m$ is a positive integer). We
know that $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)  $ is
the set of all $m$-element subsets of $S\setminus\left\{  s\right\}  $ (by the
definition of $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
$). Also, $\mathcal{P}_{m}\left(  S\right)  $ is the set of all $m$-element
subsets of $S$ (by the definition of $\mathcal{P}_{m}\left(  S\right)  $).
Finally, $\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  $ is
the set of all $\left(  m-1\right)  $-element subsets of $S\setminus\left\{
s\right\}  $ (by the definition of $\mathcal{P}_{m-1}\left(  S\setminus
\left\{  s\right\}  \right)  $).

For every $U\in\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
$, we have $U\in\mathcal{P}_{m}\left(  S\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $U\in\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $. We must show that $U\in
\mathcal{P}_{m}\left(  S\right)  $.
\par
We have $U\in\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)  $.
In other words, $U$ is an $m$-element subset of $S\setminus\left\{  s\right\}
$ (since $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)  $ is
the set of all $m$-element subsets of $S\setminus\left\{  s\right\}  $). Thus,
$U$ is an $m$-element set; hence, $\left\vert U\right\vert =m$. Also, $U$ is a
subset of $S\setminus\left\{  s\right\}  $; thus, $U\subseteq S\setminus
\left\{  s\right\}  \subseteq S$. Therefore, $U$ is a subset of $S$. Hence,
$U$ is an $m$-element subset of $S$. In other words, $U\in\mathcal{P}%
_{m}\left(  S\right)  $ (since $\mathcal{P}_{m}\left(  S\right)  $ is the set
of all $m$-element subsets of $S$). Qed.}. In other words, $\mathcal{P}%
_{m}\left(  S\setminus\left\{  s\right\}  \right)  \subseteq\mathcal{P}%
_{m}\left(  S\right)  $.

For every $U\in\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
$, we have $U\cup\left\{  s\right\}  \in\mathcal{P}_{m}\left(  S\right)
\setminus\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
$\ \ \ \ \footnote{\textit{Proof.} Let $U\in\mathcal{P}_{m-1}\left(
S\setminus\left\{  s\right\}  \right)  $. We must prove that $U\cup\left\{
s\right\}  \in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $.
\par
Let $V=U\cup\left\{  s\right\}  $.
\par
We have $U\in\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
$. In other words, $U$ is an $\left(  m-1\right)  $-element subset of
$S\setminus\left\{  s\right\}  $ (since $\mathcal{P}_{m-1}\left(
S\setminus\left\{  s\right\}  \right)  $ is the set of all $\left(
m-1\right)  $-element subsets of $S\setminus\left\{  s\right\}  $). Thus, $U$
is an $\left(  m-1\right)  $-element set; in other words, $\left\vert
U\right\vert =m-1$. Furthermore, $U$ is a subset of $S\setminus\left\{
s\right\}  $; hence, $U\subseteq S\setminus\left\{  s\right\}  $. Now,
\[
V=\underbrace{U}_{\subseteq S\setminus\left\{  s\right\}  \subseteq S}%
\cup\underbrace{\left\{  s\right\}  }_{\substack{\subseteq S\\\text{(since
}s\in S\text{)}}}\subseteq S\cup S=S.
\]
In other words, $V$ is a subset of $S$.
\par
We have $s\in\left\{  s\right\}  $ and thus $s\notin S\setminus\left\{
s\right\}  $. If we had $s\in U$, then we would have $s\in U\subseteq
S\setminus\left\{  s\right\}  $, which would contradict $s\notin
S\setminus\left\{  s\right\}  $. Thus, we cannot have $s\in U$. In other
words, we have $s\notin U$. Hence, $\left\vert U\cup\left\{  s\right\}
\right\vert =\underbrace{\left\vert U\right\vert }_{=m-1}+1=\left(
m-1\right)  +1=m$. Thus, $\left\vert \underbrace{V}_{=U\cup\left\{  s\right\}
}\right\vert =\left\vert U\cup\left\{  s\right\}  \right\vert =m$. In other
words, $V$ is an $m$-element set. Thus, $V$ is an $m$-element subset of $S$.
In other words, $V\in\mathcal{P}_{m}\left(  S\right)  $ (since $\mathcal{P}%
_{m}\left(  S\right)  $ is the set of all $m$-element subsets of $S$).
\par
Now, we shall prove that $V\notin\mathcal{P}_{m}\left(  S\setminus\left\{
s\right\}  \right)  $. Indeed, assume the contrary (for the sake of
contradiction). Thus, $V\in\mathcal{P}_{m}\left(  S\setminus\left\{
s\right\}  \right)  $. In other words, $V$ is an $m$-element subset of
$S\setminus\left\{  s\right\}  $ (since $\mathcal{P}_{m}\left(  S\setminus
\left\{  s\right\}  \right)  $ is the set of all $m$-element subsets of
$S\setminus\left\{  s\right\}  $). Hence, $V$ is a subset of $S\setminus
\left\{  s\right\}  $. In other words, $V\subseteq S\setminus\left\{
s\right\}  $. Thus, $s\in\left\{  s\right\}  \subseteq U\cup\left\{
s\right\}  =V\subseteq S\setminus\left\{  s\right\}  $, which contradicts
$s\notin S\setminus\left\{  s\right\}  $. This contradiction proves that our
assumption was false. Hence, $V\notin\mathcal{P}_{m}\left(  S\setminus\left\{
s\right\}  \right)  $ is proven.
\par
Combining $V\in\mathcal{P}_{m}\left(  S\right)  $ with $V\notin\mathcal{P}%
_{m}\left(  S\setminus\left\{  s\right\}  \right)  $, we obtain $V\in
\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $. Thus, $U\cup\left\{  s\right\}
=V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $, qed.}. Hence, we can define a map%
\[
\alpha:\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)
\]
by
\begin{equation}
\left(  \alpha\left(  U\right)  =U\cup\left\{  s\right\}
\ \ \ \ \ \ \ \ \ \ \text{for every }U\in\mathcal{P}_{m-1}\left(
S\setminus\left\{  s\right\}  \right)  \right)  .
\label{pf.prop.sol.prop.binom.subsets.Pm.lem.c.alpha()=}%
\end{equation}
Consider this map $\alpha$.

For every $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}%
_{m}\left(  S\setminus\left\{  s\right\}  \right)  $, we have $V\setminus
\left\{  s\right\}  \in\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}
\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $V\in\mathcal{P}_{m}\left(
S\right)  \setminus\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}
\right)  $. We must prove that $V\setminus\left\{  s\right\}  \in
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  $.
\par
Let $W=V\setminus\left\{  s\right\}  $.
\par
We have $V\notin\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
$ (since $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $).
\par
We have $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  \subseteq\mathcal{P}_{m}\left(
S\right)  $. In other words, $V$ is an $m$-element subset of $S$ (since
$\mathcal{P}_{m}\left(  S\right)  $ is the set of all $m$-element subsets of
$S$). Thus, $V$ is an $m$-element set. In other words, $\left\vert
V\right\vert =m$. Also, $V$ is a subset of $S$. In other words, $V\subseteq
S$.
\par
Notice that $W=\underbrace{V}_{\subseteq S}\setminus\left\{  s\right\}
\subseteq S\setminus\left\{  s\right\}  $. In other words, $W$ is a subset of
$S\setminus\left\{  s\right\}  $.
\par
Now, we shall prove that $s\in V$. Indeed, assume the contrary. Thus, $s\notin
V$. Hence, $V\setminus\left\{  s\right\}  =V$, so that $V=V\setminus\left\{
s\right\}  \subseteq S\setminus\left\{  s\right\}  $. Hence, $V$ is a subset
of $S\setminus\left\{  s\right\}  $. Thus, $V$ is an $m$-element subset of
$S\setminus\left\{  s\right\}  $. In other words, $V\in\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $ (since $\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $ is the set of all $m$-element subsets
of $S\setminus\left\{  s\right\}  $). This contradicts $V\notin\mathcal{P}%
_{m}\left(  S\setminus\left\{  s\right\}  \right)  $. This contradiction
proves that our assumption was wrong.
\par
Hence, $s\in V$ is proven. Thus, $\left\vert V\setminus\left\{  s\right\}
\right\vert =\underbrace{\left\vert V\right\vert }_{=m}-1=m-1$. In other
words, $V\setminus\left\{  s\right\}  $ is an $\left(  m-1\right)  $-element
set. In other words, $W$ is an $\left(  m-1\right)  $-element set (since
$W=V\setminus\left\{  s\right\}  $). Thus, $W$ is an $\left(  m-1\right)
$-element subset of $S\setminus\left\{  s\right\}  $. In other words,
$W\in\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  $ (since
$\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  $ is the set
of all $\left(  m-1\right)  $-element subsets of $S\setminus\left\{
s\right\}  $). Hence, $V\setminus\left\{  s\right\}  =W\in\mathcal{P}%
_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  $. Qed.}. Hence, we can
define a map%
\[
\beta:\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  \rightarrow\mathcal{P}_{m-1}\left(
S\setminus\left\{  s\right\}  \right)
\]
by%
\[
\left(  \beta\left(  V\right)  =V\setminus\left\{  s\right\}
\ \ \ \ \ \ \ \ \ \ \text{for every }V\in\mathcal{P}_{m}\left(  S\right)
\setminus\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
\right)  .
\]
Consider this map $\beta$.

We have $\alpha\circ\beta=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $. We shall show that $\left(
\alpha\circ\beta\right)  \left(  V\right)  =\operatorname*{id}\left(
V\right)  $.
\par
We have $V\notin\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
$ (since $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  $).
\par
We have $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  \subseteq\mathcal{P}_{m}\left(
S\right)  $. In other words, $V$ is an $m$-element subset of $S$ (since
$\mathcal{P}_{m}\left(  S\right)  $ is the set of all $m$-element subsets of
$S$). Thus, $V$ is a subset of $S$. In other words, $V\subseteq S$.
\par
Now, we shall prove that $s\in V$. Indeed, assume the contrary. Thus, $s\notin
V$. Hence, $V\setminus\left\{  s\right\}  =V$, so that $V=\underbrace{V}%
_{\subseteq S}\setminus\left\{  s\right\}  \subseteq S\setminus\left\{
s\right\}  $. Hence, $V$ is a subset of $S\setminus\left\{  s\right\}  $.
Thus, $V$ is an $m$-element subset of $S\setminus\left\{  s\right\}  $. In
other words, $V\in\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}
\right)  $ (since $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}
\right)  $ is the set of all $m$-element subsets of $S\setminus\left\{
s\right\}  $). This contradicts $V\notin\mathcal{P}_{m}\left(  S\setminus
\left\{  s\right\}  \right)  $. This contradiction proves that our assumption
was wrong.
\par
Hence, $s\in V$ is proven. Thus, $V\cup\left\{  s\right\}  =V$. Now,
\begin{align*}
\left(  \alpha\circ\beta\right)  \left(  V\right)   &  =\alpha\left(
\beta\left(  V\right)  \right)  =\underbrace{\beta\left(  V\right)
}_{\substack{=V\setminus\left\{  s\right\}  \\\text{(by the definition of
}\beta\text{)}}}\cup\left\{  s\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\alpha\right) \\
&  =\left(  V\setminus\left\{  s\right\}  \right)  \cup\left\{  s\right\}
=V\cup\left\{  s\right\}  =V=\operatorname*{id}\left(  V\right)  .
\end{align*}
\par
Now, forget that we fixed $V$. We thus have shown that $\left(  \alpha
\circ\beta\right)  \left(  V\right)  =\operatorname*{id}\left(  V\right)  $
for every $V\in\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}%
_{m}\left(  S\setminus\left\{  s\right\}  \right)  $. In other words,
$\alpha\circ\beta=\operatorname*{id}$. Qed.} and $\beta\circ\alpha
=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let $U\in\mathcal{P}%
_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  $. We shall prove that
$\left(  \beta\circ\alpha\right)  \left(  U\right)  =\operatorname*{id}\left(
U\right)  $.
\par
We have $U\in\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
$. In other words, $U$ is an $\left(  m-1\right)  $-element subset of
$S\setminus\left\{  s\right\}  $ (since $\mathcal{P}_{m-1}\left(
S\setminus\left\{  s\right\}  \right)  $ is the set of all $\left(
m-1\right)  $-element subsets of $S\setminus\left\{  s\right\}  $). Thus, $U$
is a subset of $S\setminus\left\{  s\right\}  $; hence, $U\subseteq
S\setminus\left\{  s\right\}  $.
\par
We have $s\in\left\{  s\right\}  $ and thus $s\notin S\setminus\left\{
s\right\}  $. If we had $s\in U$, then we would have $s\in U\subseteq
S\setminus\left\{  s\right\}  $, which would contradict $s\notin
S\setminus\left\{  s\right\}  $. Thus, we cannot have $s\in U$. In other
words, we have $s\notin U$. Hence, $U\setminus\left\{  s\right\}  =U$.
\par
Now,%
\begin{align*}
\left(  \beta\circ\alpha\right)  \left(  U\right)   &  =\beta\left(
\alpha\left(  U\right)  \right)  =\underbrace{\alpha\left(  U\right)
}_{\substack{=U\cup\left\{  s\right\}  \\\text{(by the definition of }%
\alpha\text{)}}}\setminus\left\{  s\right\}  \ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\beta\right) \\
&  =\left(  U\cup\left\{  s\right\}  \right)  \setminus\left\{  s\right\}
=U\setminus\left\{  s\right\}  =U=\operatorname*{id}\left(  U\right)  .
\end{align*}
\par
Now, forget that we fixed $U$. Thus, we have shown that $\left(  \beta
\circ\alpha\right)  \left(  U\right)  =\operatorname*{id}\left(  U\right)  $
for each $U\in\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
$. In other words, $\beta\circ\alpha=\operatorname*{id}$, qed.}. These two
equalities show that the maps $\alpha$ and $\beta$ are mutually inverse. Thus,
the map $\alpha$ is invertible, i.e., is a bijection.

But $\alpha$ is the map
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
(because $\alpha$ is the map $\mathcal{P}_{m-1}\left(  S\setminus\left\{
s\right\}  \right)  \rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus
\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)  $ defined by
(\ref{pf.prop.sol.prop.binom.subsets.Pm.lem.c.alpha()=})).

We know that the map $\alpha$ is well-defined and a bijection. In other words,
the map
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
is well-defined and a bijection (since $\alpha$ is the map
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
). This proves Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem}
\textbf{(c)}.
\end{proof}

We can now prove Proposition \ref{prop.binom.subsets} in a slightly rewritten form:

\begin{proposition}
\label{prop.sol.prop.binom.subsets.binom}Let $m\in\mathbb{N}$. Let $S$ be a
finite set. Then,%
\[
\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert =\dbinom{\left\vert
S\right\vert }{m}.
\]

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sol.prop.binom.subsets.binom}.]We shall prove
Proposition \ref{prop.sol.prop.binom.subsets.binom} by induction over
$\left\vert S\right\vert $ (using the fact that $\left\vert S\right\vert
\in\mathbb{N}$ for any finite set $S$):

\textit{Induction base:} We must show that Proposition
\ref{prop.sol.prop.binom.subsets.binom} holds for $\left\vert S\right\vert
=0$. So, let $m\in\mathbb{N}$, and let $S$ be a finite set satisfying
$\left\vert S\right\vert =0$. We must then prove Proposition
\ref{prop.sol.prop.binom.subsets.binom}.

We have $\left\vert S\right\vert =0$ and thus $S=\varnothing$. We now want to
prove that%
\begin{equation}
\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert =\dbinom{\left\vert
S\right\vert }{m}. \label{eq.prop.sol.prop.binom.subsets.binom.ibas.goal}%
\end{equation}


We are in one of the following two cases:

\textit{Case 1:} We have $m=0$.

\textit{Case 2:} We have $m\neq0$.

Let us first consider Case 1. In this case, we have $m=0$. Hence,%
\[
\mathcal{P}_{m}\left(  S\right)  =\mathcal{P}_{0}\left(  S\right)  =\left\{
\varnothing\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(a)}}\right)
\]
and thus $\left\vert \underbrace{\mathcal{P}_{m}\left(  S\right)  }_{=\left\{
\varnothing\right\}  }\right\vert =\left\vert \left\{  \varnothing\right\}
\right\vert =1$. Comparing this with%
\begin{align*}
\dbinom{\left\vert S\right\vert }{m}  &  =\dbinom{\left\vert S\right\vert }%
{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m=0\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.binom.00}), applied to
}\left\vert S\right\vert \text{ instead of }m\right)  ,
\end{align*}
we obtain $\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert
=\dbinom{\left\vert S\right\vert }{m}$. Thus,
(\ref{eq.prop.sol.prop.binom.subsets.binom.ibas.goal}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $m\neq0$. Combined with
$m\in\mathbb{N}$, this yields $m\in\mathbb{N}\setminus\left\{  0\right\}
=\left\{  1,2,3,\ldots\right\}  $. In other words, $m$ is a positive integer.
Thus, $m>0$, so that $0<m$. Hence, (\ref{eq.binom.0}) (applied to $0$ and $m$
instead of $m$ and $n$) yields $\dbinom{0}{m}=0$. But Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(b)} yields $\mathcal{P}%
_{m}\left(  \varnothing\right)  =\varnothing$. Now,
\[
\left\vert \mathcal{P}_{m}\left(  \underbrace{S}_{=\varnothing}\right)
\right\vert =\left\vert \underbrace{\mathcal{P}_{m}\left(  \varnothing\right)
}_{=\varnothing}\right\vert =\left\vert \varnothing\right\vert =0.
\]
Comparing this with%
\begin{align*}
\dbinom{\left\vert S\right\vert }{m}  &  =\dbinom{0}{m}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert S\right\vert =0\right) \\
&  =0,
\end{align*}
we obtain $\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert
=\dbinom{\left\vert S\right\vert }{m}$. Thus,
(\ref{eq.prop.sol.prop.binom.subsets.binom.ibas.goal}) is proven in Case 2.

We now have proven (\ref{eq.prop.sol.prop.binom.subsets.binom.ibas.goal}) in
each of the two Cases 1 and 2. Since these two Cases cover all possibilities,
this yields that (\ref{eq.prop.sol.prop.binom.subsets.binom.ibas.goal}) always
holds. In other words, we have $\left\vert \mathcal{P}_{m}\left(  S\right)
\right\vert =\dbinom{\left\vert S\right\vert }{m}$. In other words,
Proposition \ref{prop.sol.prop.binom.subsets.binom} holds. Thus, Proposition
\ref{prop.sol.prop.binom.subsets.binom} is proven for $\left\vert S\right\vert
=0$. This completes the induction base.

\textit{Induction step:} Let $N$ be a positive integer. Assume that
Proposition \ref{prop.sol.prop.binom.subsets.binom} holds for $\left\vert
S\right\vert =N-1$. We must then show that Proposition
\ref{prop.sol.prop.binom.subsets.binom} holds for $\left\vert S\right\vert =N$.

We have assumed that Proposition \ref{prop.sol.prop.binom.subsets.binom} holds
for $\left\vert S\right\vert =N-1$. In other words, every $m\in\mathbb{N}$ and
every finite set $S$ satisfying $\left\vert S\right\vert =N-1$ satisfy%
\begin{equation}
\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert =\dbinom{\left\vert
S\right\vert }{m}. \label{eq.prop.sol.prop.binom.subsets.binom.istep.hyp}%
\end{equation}


Let now $m\in\mathbb{N}$, and let $S$ be a finite set satisfying $\left\vert
S\right\vert =N$. We shall show that%
\begin{equation}
\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert =\dbinom{\left\vert
S\right\vert }{m}. \label{eq.prop.sol.prop.binom.subsets.binom.istep.goal}%
\end{equation}


\textit{Proof of (\ref{eq.prop.sol.prop.binom.subsets.binom.istep.goal}):} We
have $\left\vert S\right\vert =N>0$ (since $N$ is positive). Hence, the set
$S$ is nonempty. In other words, the set $S$ has at least one element $s$. Fix
such an $s$.

We are in one of the following two cases:

\textit{Case 1:} We have $m=0$.

\textit{Case 2:} We have $m\neq0$.

Let us first consider Case 1. In this case, we have $m=0$. Hence,%
\[
\mathcal{P}_{m}\left(  S\right)  =\mathcal{P}_{0}\left(  S\right)  =\left\{
\varnothing\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(a)}}\right)
\]
and thus $\left\vert \underbrace{\mathcal{P}_{m}\left(  S\right)  }_{=\left\{
\varnothing\right\}  }\right\vert =\left\vert \left\{  \varnothing\right\}
\right\vert =1$. Comparing this with%
\begin{align*}
\dbinom{\left\vert S\right\vert }{m}  &  =\dbinom{\left\vert S\right\vert }%
{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m=0\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.binom.00}), applied to
}\left\vert S\right\vert \text{ instead of }m\right)  ,
\end{align*}
we obtain $\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert
=\dbinom{\left\vert S\right\vert }{m}$. Thus,
(\ref{eq.prop.sol.prop.binom.subsets.binom.istep.goal}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $m\neq0$. Combined with
$m\in\mathbb{N}$, this yields $m\in\mathbb{N}\setminus\left\{  0\right\}
=\left\{  1,2,3,\ldots\right\}  $. In other words, $m$ is a positive integer.
Hence, Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(c)}
yields that $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
\subseteq\mathcal{P}_{m}\left(  S\right)  $, and that the map%
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
is well-defined and a bijection.

So there exists a bijection from $\mathcal{P}_{m-1}\left(  S\setminus\left\{
s\right\}  \right)  $ to $\mathcal{P}_{m}\left(  S\right)  \setminus
\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)  $ (namely, the
map
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
). Hence, $\left\vert \mathcal{P}_{m}\left(  S\right)  \setminus
\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)  \right\vert
=\left\vert \mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
\right\vert $.

But $s\in S$ and thus $\left\vert S\setminus\left\{  s\right\}  \right\vert
=\underbrace{\left\vert S\right\vert }_{=N}-1=N-1$. Hence,
(\ref{eq.prop.sol.prop.binom.subsets.binom.istep.hyp}) (applied to
$S\setminus\left\{  s\right\}  $ instead of $S$) yields
\[
\left\vert \mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
\right\vert =\dbinom{\left\vert S\setminus\left\{  s\right\}  \right\vert }%
{m}=\dbinom{N-1}{m}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert
S\setminus\left\{  s\right\}  \right\vert =N-1\right)  .
\]
Also, $m$ is a positive integer; thus, $m-1\in\mathbb{N}$. Hence,
(\ref{eq.prop.sol.prop.binom.subsets.binom.istep.hyp}) (applied to $m-1$ and
$S\setminus\left\{  s\right\}  $ instead of $m$ and $S$) yields
\[
\left\vert \mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)
\right\vert =\dbinom{\left\vert S\setminus\left\{  s\right\}  \right\vert
}{m-1}=\dbinom{N-1}{m-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert
S\setminus\left\{  s\right\}  \right\vert =N-1\right)  .
\]


Now, recall that $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}
\right)  \subseteq\mathcal{P}_{m}\left(  S\right)  $. Thus,%
\begin{align*}
\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert  &
=\underbrace{\left\vert \mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}
\right)  \right\vert }_{=\dbinom{N-1}{m}}+\underbrace{\left\vert
\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  \right\vert }_{\substack{=\left\vert
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)  \right\vert
\\=\dbinom{N-1}{m-1}}}\\
&  =\dbinom{N-1}{m}+\dbinom{N-1}{m-1}=\dbinom{N-1}{m-1}+\dbinom{N-1}{m}.
\end{align*}
Comparing this with%
\begin{align*}
\dbinom{\left\vert S\right\vert }{m}  &  =\dbinom{N}{m}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert S\right\vert =N\right) \\
&  =\dbinom{N-1}{m-1}+\dbinom{N-1}{m}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.binom.rec.m}), applied to }N\text{ and }m\\
\text{instead of }m\text{ and }n
\end{array}
\right)  ,
\end{align*}
we obtain $\left\vert \mathcal{P}_{m}\left(  S\right)  \right\vert
=\dbinom{\left\vert S\right\vert }{m}$. Thus,
(\ref{eq.prop.sol.prop.binom.subsets.binom.istep.goal}) is proven in Case 2.

We now have proven (\ref{eq.prop.sol.prop.binom.subsets.binom.istep.goal}) in
each of the two Cases 1 and 2. Since these two Cases cover all possibilities,
this yields that (\ref{eq.prop.sol.prop.binom.subsets.binom.istep.goal})
always holds. In other words, we have $\left\vert \mathcal{P}_{m}\left(
S\right)  \right\vert =\dbinom{\left\vert S\right\vert }{m}$.

Now, forget that we fixed $m$ and $S$. We thus have shown that if
$m\in\mathbb{N}$, and if $S$ is a finite set satisfying $\left\vert
S\right\vert =N$, then $\left\vert \mathcal{P}_{m}\left(  S\right)
\right\vert =\dbinom{\left\vert S\right\vert }{m}$. In other words,
Proposition \ref{prop.sol.prop.binom.subsets.binom} is proven for $\left\vert
S\right\vert =N$. This completes the induction step. The induction proof of
Proposition \ref{prop.sol.prop.binom.subsets.binom} is thus complete.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.binom.subsets}.]Let $m\in\mathbb{N}$ and
$n\in\mathbb{N}$. Let $S$ be an $m$-element set. Thus, $S$ is a finite set.
Moreover, $\left\vert S\right\vert =m$ (since $S$ is an $m$-element set).

But $\mathcal{P}_{n}\left(  S\right)  $ is the set of all $n$-element subsets
of $S$ (by the definition of $\mathcal{P}_{n}\left(  S\right)  $). Hence,%
\[
\mathcal{P}_{n}\left(  S\right)  =\left(  \text{the set of all }%
n\text{-element subsets of }S\right)  .
\]
Thus,%
\begin{align*}
&  \left\vert \underbrace{\mathcal{P}_{n}\left(  S\right)  }_{=\left(
\text{the set of all }n\text{-element subsets of }S\right)  }\right\vert \\
&  =\left\vert \left(  \text{the set of all }n\text{-element subsets of
}S\right)  \right\vert \\
&  =\left(  \text{the number of all }n\text{-element subsets of }S\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\left\vert \mathcal{P}_{n}\left(  S\right)  \right\vert  &  =\dbinom
{\left\vert S\right\vert }{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.sol.prop.binom.subsets.binom}, applied to }n\text{ instead of
}m\right) \\
&  =\dbinom{m}{n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\vert
S\right\vert =m\right)  ,
\end{align*}
we obtain%
\[
\dbinom{m}{n}=\left(  \text{the number of all }n\text{-element subsets of
}S\right)  .
\]
In other words, $\dbinom{m}{n}$ is the number of all $n$-element subsets of
$S$. This proves Proposition \ref{prop.binom.subsets}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.prop.binom.binomial}}

In order to solve Exercise \ref{exe.prop.binom.binomial}, we need to prove
Proposition \ref{prop.binom.binomial}.

\begin{proof}
[Proof of Proposition \ref{prop.binom.binomial}.]We shall prove Proposition
\ref{prop.binom.binomial} by induction over $n$:

\textit{Induction base:} We have%
\[
\sum_{k=0}^{0}\dbinom{0}{k}X^{k}Y^{0-k}=\underbrace{\dbinom{0}{0}%
}_{\substack{=1\\\text{(by (\ref{eq.binom.00}), applied to }m=0\text{)}%
}}\underbrace{X^{0}}_{=1}\underbrace{Y^{0-0}}_{=Y^{0}=1}=1.
\]
Comparing this with $\left(  X+Y\right)  ^{0}=1$, we obtain $\left(
X+Y\right)  ^{0}=\sum_{k=0}^{0}\dbinom{0}{k}X^{k}Y^{0-k}$. In other words,
Proposition \ref{prop.binom.binomial} holds for $n=0$. This completes the
induction base.

\textit{Induction step:} Let $N$ be a positive integer. Assume that
Proposition \ref{prop.binom.binomial} holds for $n=N-1$. We must now prove
that Proposition \ref{prop.binom.binomial} holds for $n=N$.

Notice that $N\geq1$ (since $N$ is a positive integer), so that $N-1\geq0$.

We have assumed that Proposition \ref{prop.binom.binomial} holds for $n=N-1$.
In other words, we have%
\begin{equation}
\left(  X+Y\right)  ^{N-1}=\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{\left(
N-1\right)  -k}. \label{pf.prop.binom.binomial.ihyp}%
\end{equation}


Now,%
\begin{align}
&  \left(  X+Y\right)  ^{N}\nonumber\\
&  =\left(  X+Y\right)  \underbrace{\left(  X+Y\right)  ^{N-1}}%
_{\substack{=\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{\left(  N-1\right)
-k}\\\text{(by (\ref{pf.prop.binom.binomial.ihyp}))}}}\nonumber\\
&  =\left(  X+Y\right)  \left(  \sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{\left(
N-1\right)  -k}\right) \nonumber\\
&  =\underbrace{X\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{\left(  N-1\right)
-k}}_{=\sum_{k=0}^{N-1}\dbinom{N-1}{k}XX^{k}Y^{\left(  N-1\right)  -k}%
}+\underbrace{Y\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{\left(  N-1\right)  -k}%
}_{=\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}YY^{\left(  N-1\right)  -k}%
}\nonumber\\
&  =\sum_{k=0}^{N-1}\underbrace{\dbinom{N-1}{k}}_{\substack{=\dbinom
{N-1}{\left(  k+1\right)  -1}\\\text{(since }k=\left(  k+1\right)  -1\text{)}%
}}\underbrace{XX^{k}}_{=X^{k+1}}\underbrace{Y^{\left(  N-1\right)  -k}%
}_{\substack{=Y^{N-\left(  k+1\right)  }\\\text{(since }\left(  N-1\right)
-k=N-\left(  k+1\right)  \text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}%
\underbrace{YY^{\left(  N-1\right)  -k}}_{\substack{=Y^{\left(  \left(
N-1\right)  -k\right)  +1}=Y^{N-k}\\\text{(since }\left(  \left(  N-1\right)
-k\right)  +1=N-k\text{)}}}\nonumber\\
&  =\underbrace{\sum_{k=0}^{N-1}\dbinom{N-1}{\left(  k+1\right)  -1}%
X^{k+1}Y^{N-\left(  k+1\right)  }}_{\substack{=\sum_{k=1}^{N}\dbinom{N-1}%
{k-1}X^{k}Y^{N-k}\\\text{(here, we have substituted }k\text{ for }k+1\text{ in
the sum)}}}+\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}\nonumber\\
&  =\sum_{k=1}^{N}\dbinom{N-1}{k-1}X^{k}Y^{N-k}+\sum_{k=0}^{N-1}\dbinom
{N-1}{k}X^{k}Y^{N-k}. \label{pf.prop.binom.binomial.1}%
\end{align}
But recall that $N-1\geq0$, so that $N-1\in\mathbb{N}$. Clearly, $N-1<N$.
Hence, (\ref{eq.binom.0}) (applied to $m=N-1$ and $n=N$) yields $\dbinom
{N-1}{N}=0$.

On the other hand, $N$ is a positive integer. Hence, $N\in\left\{
1,2,\ldots,N\right\}  $. Thus, we can split off the addend for $k=N$ from the
sum $\sum_{k=1}^{N}\dbinom{N-1}{k}X^{k}Y^{N-k}$. We thus obtain%
\begin{align}
\sum_{k=1}^{N}\dbinom{N-1}{k}X^{k}Y^{N-k}  &  =\sum_{k=1}^{N-1}\dbinom{N-1}%
{k}X^{k}Y^{N-k}+\underbrace{\dbinom{N-1}{N}}_{=0}X^{N}Y^{N-N}\nonumber\\
&  =\sum_{k=1}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}+\underbrace{0X^{N}Y^{N-N}%
}_{=0}\nonumber\\
&  =\sum_{k=1}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}.
\label{pf.prop.binom.binomial.4b1}%
\end{align}


Furthermore, $N-1\geq0$, so that $0\in\left\{  0,1,\ldots,N-1\right\}  $.
Hence, we can split off the addend for $k=0$ from the sum $\sum_{k=0}%
^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}$. We thus obtain%
\begin{align*}
&  \sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}\\
&  =\underbrace{\dbinom{N-1}{0}}_{\substack{=1\\\text{(by (\ref{eq.binom.00}),
applied to }m=N-1\text{)}}}X^{0}Y^{N-0}+\sum_{k=1}^{N-1}\dbinom{N-1}{k}%
X^{k}Y^{N-k}\\
&  =X^{0}Y^{N-0}+\sum_{k=1}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}.
\end{align*}
Comparing this with%
\begin{align*}
&  \underbrace{\dbinom{N}{0}}_{\substack{=1\\\text{(by (\ref{eq.binom.00}),
applied to }m=N\text{)}}}X^{0}Y^{N-0}+\underbrace{\sum_{k=1}^{N}\dbinom
{N-1}{k}X^{k}Y^{N-k}}_{\substack{=\sum_{k=1}^{N-1}\dbinom{N-1}{k}X^{k}%
Y^{N-k}\\\text{(by (\ref{pf.prop.binom.binomial.4b1}))}}}\\
&  =X^{0}Y^{N-0}+\sum_{k=1}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k},
\end{align*}
we obtain%
\begin{equation}
\sum_{k=0}^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}=\dbinom{N}{0}X^{0}Y^{N-0}%
+\sum_{k=1}^{N}\dbinom{N-1}{k}X^{k}Y^{N-k}. \label{pf.prop.binom.binomial.4b}%
\end{equation}
Now, (\ref{pf.prop.binom.binomial.1}) becomes%
\begin{align*}
&  \left(  X+Y\right)  ^{N}\\
&  =\sum_{k=1}^{N}\dbinom{N-1}{k-1}X^{k}Y^{N-k}+\underbrace{\sum_{k=0}%
^{N-1}\dbinom{N-1}{k}X^{k}Y^{N-k}}_{\substack{=\dbinom{N}{0}X^{0}Y^{N-0}%
+\sum_{k=1}^{N}\dbinom{N-1}{k}X^{k}Y^{N-k}\\\text{(by
(\ref{pf.prop.binom.binomial.4b}))}}}\\
&  =\sum_{k=1}^{N}\dbinom{N-1}{k-1}X^{k}Y^{N-k}+\dbinom{N}{0}X^{0}Y^{N-0}%
+\sum_{k=1}^{N}\dbinom{N-1}{k}X^{k}Y^{N-k}\\
&  =\dbinom{N}{0}X^{0}Y^{N-0}+\underbrace{\sum_{k=1}^{N}\dbinom{N-1}{k-1}%
X^{k}Y^{N-k}+\sum_{k=1}^{N}\dbinom{N-1}{k}X^{k}Y^{N-k}}_{=\sum_{k=1}%
^{N}\left(  \dbinom{N-1}{k-1}+\dbinom{N-1}{k}\right)  X^{k}Y^{N-k}}\\
&  =\dbinom{N}{0}X^{0}Y^{N-0}+\sum_{k=1}^{N}\left(  \dbinom{N-1}{k-1}%
+\dbinom{N-1}{k}\right)  X^{k}Y^{N-k}.
\end{align*}
Comparing this with%
\begin{align*}
&  \sum_{k=0}^{N}\dbinom{N}{k}X^{k}Y^{N-k}\\
&  =\dbinom{N}{0}X^{0}Y^{N-0}+\sum_{k=1}^{N}\underbrace{\dbinom{N}{k}%
}_{\substack{=\dbinom{N-1}{k-1}+\dbinom{N-1}{k}\\\text{(by
(\ref{eq.binom.rec.m}), applied to }m=N\text{ and }n=k\text{)}}}X^{k}Y^{N-k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=0\text{ from the sum}\right) \\
&  =\dbinom{N}{0}X^{0}Y^{N-0}+\sum_{k=1}^{N}\left(  \dbinom{N-1}{k-1}%
+\dbinom{N-1}{k}\right)  X^{k}Y^{N-k},
\end{align*}
we obtain $\left(  X+Y\right)  ^{N}=\sum_{k=0}^{N}\dbinom{N}{k}X^{k}Y^{N-k}$.
In other words, Proposition \ref{prop.binom.binomial} holds for $n=N$. Hence,
Proposition \ref{prop.binom.binomial} is proven by induction.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps1.1.2}}

\begin{proof}
[Solution to Exercise \ref{exe.ps1.1.2}.]For every $N\in\mathbb{N}$, we let
$\left[  N\right]  $ denote the $N$-element set \newline$\left\{
1,2,\ldots,N\right\}  $.

For every $i\in\mathbb{N}$ and $j\in\mathbb{N}$, we define a \textit{filled
}$\left(  i,j\right)  $\textit{-set} to mean a subset $S$ of $\left[
i\right]  \times\left[  j\right]  $ satisfying the following two conditions:

\begin{enumerate}
\item For every $k\in\left[  i\right]  $, at least one element of $S$ has its
first coordinate\footnote{The \textit{coordinates} of a pair $\left(
u,v\right)  $ mean the entries $u$ and $v$. Thus, the first coordinate of
$\left(  u,v\right)  $ is $u$.} equal to $k$.

\item For every $\ell\in\left[  j\right]  $, at least one element of $S$ has
its second coordinate equal to $\ell$.
\end{enumerate}

We can visualize subsets $S$ of $\left[  i\right]  \times\left[  j\right]  $
as selections of boxes in a rectangular table that has $i$ rows and $j$
columns\footnote{Namely, for every $\left(  u,v\right)  \in S$, we select the
box in row $u$ and column $v$.}. For instance, for $i=3$ and $j=4$, we can
represent the subset $S=\left\{  \left(  1,1\right)  ,\left(  1,3\right)
,\left(  2,2\right)  ,\left(  3,1\right)  ,\left(  3,3\right)  ,\left(
3,4\right)  \right\}  $ of $\left[  i\right]  \times\left[  j\right]  $ as the
selection%
\begin{equation}%
\begin{tabular}
[c]{|c|c|c|c|}\hline
X &  & X & \\\hline
& X &  & \\\hline
X &  & X & X\\\hline
\end{tabular}
\ \ \label{sol.ps1.1.2.exa1}%
\end{equation}
(where the rows are labelled $1,2,3$ from top to bottom, the columns are
labelled $1,2,3,4$ from left to right, as in a matrix, and where the elements
of $S$ are marked with X'es). Condition 1 then says that every row contains at
least one selected box (i.e., at least one X); and Condition 2 says that every
column contains at least one selected box. Our example (\ref{sol.ps1.1.2.exa1}%
) satisfies these two conditions, but (for instance) the subset%
\[%
\begin{tabular}
[c]{|c|c|c|c|}\hline
X & X &  & X\\\hline
& X & \phantom{X} & \\\hline
X &  &  & X\\\hline
\end{tabular}
\ \
\]
does not (it fails Condition 2).

For every $i\in\mathbb{N}$ and $j\in\mathbb{N}$, we define a nonnegative
integer $c_{i,j}$ as the number of all filled $\left(  i,j\right)  $-sets
which have $n$ elements\footnote{When we say \textquotedblleft have $n$
elements\textquotedblright, we mean \textquotedblleft have exactly $n$
elements\textquotedblright, not \textquotedblleft have at least $n$
elements\textquotedblright.}. We shall now show that (\ref{eq.exe.1.2.claim})
is satisfied.

Indeed, let us first prove that any $x\in\mathbb{N}$ and $y\in\mathbb{N}$
satisfy%
\begin{equation}
\dbinom{xy}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}{i}\dbinom{y}{j}.
\label{sol.ps1.1.2.xyclaim}%
\end{equation}


Keep in mind that (\ref{sol.ps1.1.2.xyclaim}) and (\ref{eq.exe.1.2.claim}) are
different claims: The $x$ and $y$ in (\ref{sol.ps1.1.2.xyclaim}) are
nonnegative integers, while the $X$ and $Y$ in (\ref{eq.exe.1.2.claim}) are indeterminates!

\textit{Proof of (\ref{sol.ps1.1.2.xyclaim}):} Let $x\in\mathbb{N}$ and
$y\in\mathbb{N}$. Recall that $\dbinom{xy}{n}$ is the number of all
$n$-element subsets of a given $xy$-element set. Since $\left[  x\right]
\times\left[  y\right]  $ is an $xy$-element set, we thus conclude that
$\dbinom{xy}{n}$ is the number of all $n$-element subsets of $\left[
x\right]  \times\left[  y\right]  $.

Now, let us find a different way to count all $n$-element subsets of $\left[
x\right]  \times\left[  y\right]  $. As above, we can visualize such subsets
as selections of boxes in a rectangular table that has $x$ rows and $y$
columns; we again mark the selected boxes by X'es. We want to count all ways
to select $n$ boxes in this table, i.e., to place $n$ X'es in the table. We
can place $n$ X'es in the table by means of the following process:

\begin{enumerate}
\item We choose how many rows of the table will have at least one X. This can
be a number from $0$ to $n$ (inclusive)\footnote{I am not saying that any
number from $0$ to $n$ (inclusive) is possible; I am just saying that this
will always be a number from $0$ to $n$ (inclusive). Here is why:
\par
Clearly, the number of rows of the table that will have at least one X is
$\geq0$. But it is also $\leq n$, because we want to place only $n$ X'es in
the table, and these $n$ X'es will clearly occupy at most $n$ rows.}; we
denote it by $i$.

\item We choose how many columns of the table will have at least one X. This
can be a number from $0$ to $n$ (inclusive)\footnote{This follows by a similar
argument as the analogous statement in Step 1.}; we denote it by $j$.

\item We choose the $i$ rows of the table that will have at least one X. This
can be done in $\dbinom{x}{i}$ ways (since there are $x$ rows to choose from).

\item We choose the $j$ columns of the table that will have at least one X.
This can be done in $\dbinom{y}{j}$ ways (since there are $y$ columns to
choose from).

\item It remains to place $n$ X'es in the table in such a way that the rows
that contain at least one X are precisely the $i$ chosen rows, and the columns
that contain at least one X are precisely the $j$ chosen columns. To do so, we
can temporarily remove all the remaining $x-i$ rows and $y-j$ columns. We are
then left with a rectangular table that has $i$ rows and $j$ columns, and now
we need to place $n$ X'es in it in such a way that every row contains at least
one X and every column contains at least one X. As we know, the number of ways
to do this is $c_{i,j}$ (because this is how $c_{i,j}$ was defined).
\end{enumerate}

This process makes it clear that the total number of ways to place $n$ X'es in
the (original) table is $\sum_{i=0}^{n}\sum_{j=0}^{n}\dbinom{x}{i}\dbinom
{y}{j}c_{i,j}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}{i}\dbinom{y}{j}$.
In other words, the number of all $n$-element subsets of $\left[  x\right]
\times\left[  y\right]  $ is $\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}%
{i}\dbinom{y}{j}$.

So we know that this number equals both $\dbinom{xy}{n}$ and $\sum_{i=0}%
^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{x}{i}\dbinom{y}{j}$ at the same time.
Comparing these values, we obtain (\ref{sol.ps1.1.2.xyclaim}).

Now that (\ref{sol.ps1.1.2.xyclaim}) is proven, we can finally solve the
exercise. We define two polynomials $P$ and $Q$ in the indeterminates $X$ and
$Y$ with rational coefficients by setting%
\begin{align*}
P  &  =\dbinom{XY}{n};\\
Q  &  =\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j}\dbinom{X}{i}\dbinom{Y}{j}%
\end{align*}
\footnote{These are both polynomials since $\dbinom{XY}{n}$, $\dbinom{X}{i}$
and $\dbinom{Y}{j}$ are polynomials in $X$ and $Y$.}. The equality
(\ref{sol.ps1.1.2.xyclaim}) (which we have proven) states that $P\left(
x,y\right)  =Q\left(  x,y\right)  $ for all $x\in\mathbb{N}$ and
$y\in\mathbb{N}$. Thus, Lemma \ref{lem.polyeq} \textbf{(d)} yields that $P=Q$.
Recalling how $P$ and $Q$ are defined, we see that this is precisely the
equality (\ref{eq.exe.1.2.claim}).

Hence, (\ref{eq.exe.1.2.claim}) is proven, and Exercise \ref{exe.ps1.1.2} solved.
\end{proof}

\begin{remark}
I learnt the above solution to Exercise \ref{exe.ps1.1.2}
\href{http://www.artofproblemsolving.com/community/c6h299793p1623722}{from
Gjergji Zaimi on AoPS}. The numbers $c_{i,j}$ constructed in the solution do
not appear to be easily computable by a simple closed formula. Nevertheless,
they have some nice properties that can be easily obtained from their
combinatorial definition:

\begin{itemize}
\item We have $c_{i,j}=c_{j,i}$ for all $i\in\mathbb{N}$ and $j\in\mathbb{N}$.

\item We have $c_{0,0}=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n>0
\end{cases}
$, but every positive integer $j$ satisfies $c_{0,j}=0$.

\item We have $c_{1,j}=%
\begin{cases}
1, & \text{if }n=j;\\
0, & \text{if }n\neq j
\end{cases}
$ for all positive integers $j$.

\item We have $c_{2,2}=%
\begin{cases}
0, & \text{if }n\leq1;\\
2, & \text{if }n=2;\\
4, & \text{if }n=3;\\
1, & \text{if }n=4;\\
0, & \text{if }n>4
\end{cases}
$.

\item We have $c_{i,j}=0$ if $n>ij$.

\item For every $j\in\mathbb{N}$, the number $c_{n,j}$ is the number of all
surjective maps $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,j\right\}  $.
\end{itemize}
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps1.1.3}}

\begin{proof}
[Solution to Exercise \ref{exe.ps1.1.3}.]Here is one possible solution:

Exercise \ref{exe.ps1.1.2} shows that, for every $n\in\mathbb{N}$, there exist
\textbf{nonnegative} integers $c_{i,j}$ for all $0\leq i\leq n$ and $0\leq
j\leq n$ such that (\ref{eq.exe.1.2.claim}) holds. We denote these integers
$c_{i,j}$ by $c_{i,j,n}$ (in order to make their dependence on $n$ explicit).
Thus, for every $n\in\mathbb{N}$, the nonnegative integers $c_{i,j,n}$ defined
for all $0\leq i\leq n$ and $0\leq j\leq n$ satisfy%
\begin{equation}
\dbinom{XY}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j,n}\dbinom{X}{i}\dbinom{Y}%
{j}. \label{pf.exe.1.3.1}%
\end{equation}
Substituting $a$ and $X$ for $X$ and $Y$ in this equality, we obtain%
\begin{equation}
\dbinom{aX}{n}=\sum_{i=0}^{n}\sum_{j=0}^{n}c_{i,j,n}\dbinom{a}{i}\dbinom{X}%
{j}. \label{pf.exe.1.3.2}%
\end{equation}


Now, Theorem \ref{thm.vandermonde} (applied to $n=c$) yields%
\[
\dbinom{X+Y}{c}=\sum_{k=0}^{c}\dbinom{X}{k}\dbinom{Y}{c-k}.
\]
Substituting $aX$ and $b$ for $X$ and $Y$ in this equality, we obtain%
\begin{align}
\dbinom{aX+b}{c}  &  =\sum_{k=0}^{c}\underbrace{\dbinom{aX}{k}}%
_{\substack{=\sum_{i=0}^{k}\sum_{j=0}^{k}c_{i,j,k}\dbinom{a}{i}\dbinom{X}%
{j}\\\text{(by (\ref{pf.exe.1.3.2}), applied to }n=k\text{)}}}\dbinom{b}%
{c-k}\nonumber\\
&  =\sum_{k=0}^{c}\left(  \sum_{i=0}^{k}\sum_{j=0}^{k}c_{i,j,k}\dbinom{a}%
{i}\dbinom{X}{j}\right)  \dbinom{b}{c-k}\nonumber\\
&  =\underbrace{\sum_{k=0}^{c}\sum_{i=0}^{k}\sum_{j=0}^{k}}_{=\sum_{j=0}%
^{c}\sum_{k=j}^{c}\sum_{i=0}^{k}}c_{i,j,k}\dbinom{a}{i}\underbrace{\dbinom
{X}{j}\dbinom{b}{c-k}}_{=\dbinom{b}{c-k}\dbinom{X}{j}}\nonumber\\
&  =\sum_{j=0}^{c}\sum_{k=j}^{c}\sum_{i=0}^{k}c_{i,j,k}\dbinom{a}{i}\dbinom
{b}{c-k}\dbinom{X}{j}. \label{pf.exe.1.3.5}%
\end{align}


Now, for every $j\in\left\{  0,1,\ldots,c\right\}  $, define an integer
$d_{j}$ by $d_{j}=\sum_{k=j}^{c}\sum_{i=0}^{k}c_{i,j,k}\dbinom{a}{i}\dbinom
{b}{c-k}$. This $d_{j}$ is clearly a nonnegative integer (since the
$c_{i,j,n}$ are nonnegative, and so are the binomial coefficients $\dbinom
{a}{i}$ and $\dbinom{b}{c-k}$ (due to $a$ and $b$ being nonnegative)). Then,
(\ref{pf.exe.1.3.5}) becomes%
\[
\dbinom{aX+b}{c}=\sum_{j=0}^{c}\underbrace{\sum_{k=j}^{c}\sum_{i=0}%
^{k}c_{i,j,k}\dbinom{a}{i}\dbinom{b}{c-k}}_{=d_{j}}\dbinom{X}{j}=\sum
_{j=0}^{c}d_{j}\dbinom{X}{j}=\sum_{i=0}^{c}d_{i}\dbinom{X}{i}.
\]
Exercise \ref{exe.ps1.1.3} is thus solved.
\end{proof}

\subsection{Solution to Additional exercise \ref{exeadd.AoPS333199}}

We shall prove the following generalization of Additional exercise
\ref{exeadd.AoPS333199}:

\begin{proposition}
\label{prop.AoPS333199.gen}Let $\mathbb{K}$ be a commutative ring. (See
Definition \ref{def.commring} for the definition of a \textquotedblleft
commutative ring\textquotedblright. For example, we can set $\mathbb{K}%
=\mathbb{Z}$ or $\mathbb{K}=\mathbb{R}$ or $\mathbb{K}=\mathbb{Q}\left[
X\right]  $.) Let $x$ and $y$ be two elements of $\mathbb{K}$. For any
$n\in\mathbb{N}$ and $m\in\mathbb{N}$, define $Y_{m,n}\in\mathbb{K}$ by%
\[
Y_{m,n}=\sum_{k=0}^{n}y^{k}\dbinom{n}{k}\left(  x^{n-k}+y\right)  ^{m}.
\]
Then, $Y_{m,n}=Y_{n,m}$ for any $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.AoPS333199.gen}.]For any $n\in\mathbb{N}$ and
$m\in\mathbb{N}$, we have%
\begin{align}
Y_{m,n}  &  =\sum_{k=0}^{n}y^{k}\dbinom{n}{k}\left(  x^{n-k}+y\right)
^{m}=\sum_{\ell=0}^{n}y^{\ell}\dbinom{n}{\ell}\left(  \underbrace{x^{n-\ell
}+y}_{=y+x^{n-\ell}}\right)  ^{m}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}k\text{ as }\ell\right) \nonumber\\
&  =\sum_{\ell=0}^{n}y^{\ell}\dbinom{n}{\ell}\underbrace{\left(  y+x^{n-\ell
}\right)  ^{m}}_{\substack{=\sum_{k=0}^{m}\dbinom{m}{k}y^{k}\left(  x^{n-\ell
}\right)  ^{m-k}\\\text{(by the binomial formula)}}}=\sum_{\ell=0}^{n}y^{\ell
}\dbinom{n}{\ell}\left(  \sum_{k=0}^{m}\dbinom{m}{k}y^{k}\left(  x^{n-\ell
}\right)  ^{m-k}\right) \nonumber\\
&  =\sum_{\ell=0}^{n}\sum_{k=0}^{m}\underbrace{y^{\ell}\dbinom{n}{\ell}%
\dbinom{m}{k}}_{=\dbinom{n}{\ell}\dbinom{m}{k}y^{\ell}}y^{k}\left(  x^{n-\ell
}\right)  ^{m-k}=\sum_{\ell=0}^{n}\sum_{k=0}^{m}\dbinom{n}{\ell}\dbinom{m}%
{k}\underbrace{y^{\ell}y^{k}}_{=y^{\ell+k}}\underbrace{\left(  x^{n-\ell
}\right)  ^{m-k}}_{=x^{\left(  n-\ell\right)  \left(  m-k\right)  }%
}\nonumber\\
&  =\sum_{\ell=0}^{n}\sum_{k=0}^{m}\dbinom{n}{\ell}\dbinom{m}{k}y^{\ell
+k}x^{\left(  n-\ell\right)  \left(  m-k\right)  }.
\label{pf.prop.AoPS333199.gen.1}%
\end{align}
Now, let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Then,
(\ref{pf.prop.AoPS333199.gen.1}) (applied to $m$ and $n$ instead of $n$ and
$m$) shows that%
\begin{align*}
Y_{n,m}  &  =\underbrace{\sum_{\ell=0}^{m}\sum_{k=0}^{n}}_{=\sum_{k=0}^{n}%
\sum_{\ell=0}^{m}}\underbrace{\dbinom{m}{\ell}\dbinom{n}{k}}_{=\dbinom{n}%
{k}\dbinom{m}{\ell}}\underbrace{y^{\ell+k}}_{=y^{k+\ell}}%
\underbrace{x^{\left(  m-\ell\right)  \left(  n-k\right)  }}_{=x^{\left(
n-k\right)  \left(  m-\ell\right)  }}\\
&  =\sum_{k=0}^{n}\sum_{\ell=0}^{m}\dbinom{n}{k}\dbinom{m}{\ell}y^{k+\ell
}x^{\left(  n-k\right)  \left(  m-\ell\right)  }=\sum_{k=0}^{n}\sum_{g=0}%
^{m}\dbinom{n}{k}\dbinom{m}{g}y^{k+g}x^{\left(  n-k\right)  \left(
m-g\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}\ell\text{ as }g\right) \\
&  =\sum_{\ell=0}^{n}\sum_{g=0}^{m}\dbinom{n}{\ell}\dbinom{m}{g}y^{\ell
+g}x^{\left(  n-\ell\right)  \left(  m-g\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}k\text{ as }\ell\right) \\
&  =\sum_{\ell=0}^{n}\sum_{k=0}^{m}\dbinom{n}{\ell}\dbinom{m}{k}y^{\ell
+k}x^{\left(  n-\ell\right)  \left(  m-k\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}g\text{ as }k\right) \\
&  =Y_{m,n}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.AoPS333199.gen.1})}\right)  .
\end{align*}
This proves Proposition \ref{prop.AoPS333199.gen}.
\end{proof}

\begin{proof}
[Solution to Additional exercise \ref{exeadd.AoPS333199}.]Set $\mathbb{K}%
=\mathbb{Z}\left[  X\right]  $, and define two elements $x$ and $y$ of
$\mathbb{K}$ by $x=X$ and $y=-1$. For any $n\in\mathbb{N}$ and $m\in
\mathbb{N}$, define $Y_{m,n}\in\mathbb{K}$ as in Proposition
\ref{prop.AoPS333199.gen}. Then, for any $n\in\mathbb{N}$ and $m\in\mathbb{N}%
$, we have%
\begin{align}
Y_{m,n}  &  =\sum_{k=0}^{n}\underbrace{y^{k}}_{\substack{=\left(  -1\right)
^{k}\\\text{(since }y=-1\text{)}}}\dbinom{n}{k}\left(  \underbrace{x^{n-k}%
}_{\substack{=X^{n-k}\\\text{(since }x=X\text{)}}}+\underbrace{y}%
_{=-1}\right)  ^{m}\nonumber\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\left(
\underbrace{X^{n-k}+\left(  -1\right)  }_{=X^{n-k}-1}\right)  ^{m}=\sum
_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\left(  X^{n-k}-1\right)
^{m}=Z_{m,n}. \label{sol.exeadd.AoPS333199.1}%
\end{align}


Now, fix $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Applying
(\ref{sol.exeadd.AoPS333199.1}) to $m$ and $n$ instead of $n$ and $m$, we
obtain $Y_{n,m}=Z_{n,m}$. But Proposition \ref{prop.AoPS333199.gen} shows that
$Y_{m,n}=Y_{n,m}$. Comparing this with (\ref{sol.exeadd.AoPS333199.1}), we
obtain $Y_{n,m}=Z_{m,n}$. Comparing this with $Y_{n,m}=Z_{n,m}$, we obtain
$Z_{m,n}=Z_{n,m}$. This solves Additional exercise \ref{exeadd.AoPS333199}.
\end{proof}

\begin{remark}
Two solutions to Exercise \ref{exeadd.AoPS333199} are sketched in
\url{http://www.artofproblemsolving.com/community/c6h333199p1782800} . One is
essentially the solution given above (except in lesser generality); the other
is combinatorial.
\end{remark}

\subsection{Solution to Additional exercise \ref{exeadd.AoPS262752}}

We shall give two solutions to Additional exercise \ref{exeadd.AoPS262752}.
The first solution follows the Hint given in the exercise, and illustrates
both the use of Lemma \ref{lem.polyeq} \textbf{(b)} and of \textquotedblleft
generating functions\textquotedblright\ (the strategy of proving identities by
identifying both sides as coefficients in polynomials or power series). The
second solution is of a more classical nature, using no new methods but a
tricky application of Theorem \ref{thm.vandermonde}.

\subsubsection{First solution}

The crux of the first solution is the proof of the following lemma (which
appears in \cite[(5.55)]{GKP}):

\begin{lemma}
\label{lem.AoPS262752.r}Let $n\in\mathbb{N}$ and $x\in\mathbb{N}$. Then,%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{k}\dbinom{x}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]

\end{lemma}

Thus, Lemma \ref{lem.AoPS262752.r} is obtained from Additional exercise
\ref{exeadd.AoPS262752} by substituting a nonnegative integer $x$ for the
indeterminate $X$. It thus is clear that Lemma \ref{lem.AoPS262752.r} follows
from Additional exercise \ref{exeadd.AoPS262752}. However, for us, the
interest lies in the opposite implication: We shall derive Additional exercise
\ref{exeadd.AoPS262752} from Lemma \ref{lem.AoPS262752.r}. Let us, however,
prove Lemma \ref{lem.AoPS262752.r} first. But before we do this, let us state
a version of the binomial theorem:

\begin{proposition}
\label{prop.AoPS262752.binom}Let $x\in\mathbb{N}$. Then,%
\[
\left(  1+X\right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}%
\]
(an equality between polynomials in $\mathbb{Z}\left[  X\right]  $). (The sum
$\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}$ is an infinite sum, but only
finitely many of its addends are nonzero, so it is well-defined.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.AoPS262752.binom}.]We have%
\begin{align*}
\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}  &  =\underbrace{\sum_{\substack{k\in
\mathbb{N};\\k\leq x}}}_{=\sum_{k=0}^{x}}\dbinom{x}{k}X^{k}+\sum
_{\substack{k\in\mathbb{N};\\k>x}}\underbrace{\dbinom{x}{k}}%
_{\substack{=0\\\text{(by (\ref{eq.binom.0}) (applied to }x\text{ and
}k\\\text{instead of }m\text{ and }n\text{) (since }x<k\text{ (since
}k>x\text{)))}}}X^{k}\\
&  =\sum_{k=0}^{x}\dbinom{x}{k}X^{k}+\underbrace{\sum_{\substack{k\in
\mathbb{N};\\k>x}}0X^{k}}_{=0}=\sum_{k=0}^{x}\dbinom{x}{k}X^{k}.
\end{align*}
Comparing this with
\begin{align*}
\left(  \underbrace{1+X}_{=X+1}\right)  ^{x}  &  =\left(  X+1\right)
^{x}=\sum_{k=0}^{x}\dbinom{x}{k}X^{k}\underbrace{1^{x-k}}_{=1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the binomial formula}\right) \\
&  =\sum_{k=0}^{x}\dbinom{x}{k}X^{k},
\end{align*}
we obtain $\left(  1+X\right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}$.
This proves Proposition \ref{prop.AoPS262752.binom}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.AoPS262752.r}.]Proposition
\ref{prop.AoPS262752.binom} yields
\begin{equation}
\left(  1+X\right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}.
\label{pf.lem.AoPS262752.r.1}%
\end{equation}
Substituting $-X$ for $X$ in this equality, we obtain%
\[
\left(  1+\left(  -X\right)  \right)  ^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}%
{k}\underbrace{\left(  -X\right)  ^{k}}_{=\left(  -1\right)  ^{k}X^{k}}%
=\sum_{k\in\mathbb{N}}\dbinom{x}{k}\left(  -1\right)  ^{k}X^{k}=\sum
_{k\in\mathbb{N}}\left(  -1\right)  ^{k}\dbinom{x}{k}X^{k}.
\]
Since $1+\left(  -X\right)  =1-X$, this rewrites as%
\[
\left(  1-X\right)  ^{x}=\sum_{k\in\mathbb{N}}\left(  -1\right)  ^{k}%
\dbinom{x}{k}X^{k}.
\]
Multiplying this equality with (\ref{pf.lem.AoPS262752.r.1}), we obtain%
\begin{align*}
\left(  1-X\right)  ^{x}\left(  1+X\right)  ^{x}  &  =\left(  \sum
_{k\in\mathbb{N}}\left(  -1\right)  ^{k}\dbinom{x}{k}X^{k}\right)  \left(
\sum_{k\in\mathbb{N}}\dbinom{x}{k}X^{k}\right) \\
&  =\sum_{k\in\mathbb{N}}\left(  \sum_{i=0}^{k}\left(  -1\right)  ^{i}%
\dbinom{x}{i}\dbinom{x}{k-i}\right)  X^{k}%
\end{align*}
(according to the definition of the product of two polynomials). Hence,
\begin{align}
&  \left(  \text{the coefficient of }X^{n}\text{ in }\left(  1-X\right)
^{x}\left(  1+X\right)  ^{x}\right) \nonumber\\
&  =\sum_{i=0}^{n}\left(  -1\right)  ^{i}\dbinom{x}{i}\dbinom{x}{n-i}%
=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{k}\dbinom{x}{n-k}
\label{pf.lem.AoPS262752.r.6}%
\end{align}
(here, we have renamed the summation index $i$ as $k$).

On the other hand,%
\begin{align*}
\left(  1-X\right)  ^{x}\left(  1+X\right)  ^{x}  &  =\left(
\underbrace{\left(  1-X\right)  \left(  1+X\right)  }_{=1-X^{2}=1+\left(
-X\right)  ^{2}}\right)  ^{x}=\left(  1+\left(  -X\right)  ^{2}\right)
^{x}=\sum_{k\in\mathbb{N}}\dbinom{x}{k}\underbrace{\left(  -X^{2}\right)
^{k}}_{=\left(  -1\right)  ^{k}\left(  X^{2}\right)  ^{k}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{this follows from substituting }%
-X^{2}\text{ for }X\text{ in (\ref{pf.lem.AoPS262752.r.1})}\right) \\
&  =\sum_{k\in\mathbb{N}}\underbrace{\dbinom{x}{k}\left(  -1\right)  ^{k}%
}_{=\left(  -1\right)  ^{k}\dbinom{x}{k}}\underbrace{\left(  X^{2}\right)
^{k}}_{=X^{2k}}=\sum_{k\in\mathbb{N}}\left(  -1\right)  ^{k}\dbinom{x}%
{k}X^{2k}.
\end{align*}
Hence,%
\begin{align*}
&  \left(  \text{the coefficient of }X^{n}\text{ in }\left(  1-X\right)
^{x}\left(  1+X\right)  ^{x}\right) \\
&  =%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\end{align*}
Comparing this with (\ref{pf.lem.AoPS262752.r.6}), we obtain%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}{k}\dbinom{x}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
This proves Lemma \ref{lem.AoPS262752.r}.
\end{proof}

\begin{proof}
[First solution to Additional exercise \ref{exeadd.AoPS262752}.]Define two
polynomials $P$ and $Q$ (with rational coefficients) by%
\begin{equation}
P=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}
\label{sol.exeadd.AoPS262752.sol1.P}%
\end{equation}
and%
\begin{equation}
Q=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
. \label{sol.exeadd.AoPS262752.sol1.Q}%
\end{equation}
For every $x\in\mathbb{N}$, we have%
\begin{align*}
P\left(  x\right)   &  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{x}%
{k}\dbinom{x}{n-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}P\right) \\
&  =%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{x}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.AoPS262752.r}}\right) \\
&  =Q\left(  x\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}Q\right)  .
\end{align*}
Hence, Lemma \ref{lem.polyeq} \textbf{(b)} shows that $P=Q$. In light of
(\ref{sol.exeadd.AoPS262752.sol1.P}) and (\ref{sol.exeadd.AoPS262752.sol1.Q}),
this rewrites as
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
This solves Additional exercise \ref{exeadd.AoPS262752}.
\end{proof}

\subsubsection{Second solution}

Now let us prepare for the second solution to Additional exercise
\ref{exeadd.AoPS262752}. We shall use the following notation (known as
\href{https://en.wikipedia.org/wiki/Iverson_bracket}{the \textit{Iverson
bracket}}): If $\mathcal{A}$ is any logical statement, then $\left[
\mathcal{A}\right]  $ will mean the integer $%
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}%
\end{cases}
$. For example, $\left[  1+1=2\right]  =1$ (since $1+1=2$ is true), whereas
$\left[  1+1=1\right]  =0$ (since $1+1=1$ is false). Clearly, if $\mathcal{A}$
and $\mathcal{B}$ are two equivalent logical statements, then $\left[
\mathcal{A}\right]  =\left[  \mathcal{B}\right]  $.

We notice that every $n\in\mathbb{N}$ satisfies%
\[
\left[  n=0\right]  =%
\begin{cases}
1, & \text{if }n=0\text{ is true};\\
0, & \text{if }n=0\text{ is false}%
\end{cases}
=%
\begin{cases}
1, & \text{if }n=0;\\
0, & \text{if }n\neq0
\end{cases}
=\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}%
\]
(by Proposition \ref{prop.binom.bin-id} \textbf{(c)}). In other words, every
$n\in\mathbb{N}$ satisfies%
\begin{equation}
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}=\left[  n=0\right]  .
\label{sol.exeadd.AoPS262752.n=0}%
\end{equation}


Next, we state a simple fact:

\begin{lemma}
\label{lem.exeadd.AoPS262752.1}Let $m\in\mathbb{N}$ and $i\in\mathbb{N}$.
Then,%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\left[  m=i\right]  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.exeadd.AoPS262752.1}.]If $m<i$, then Lemma
\ref{lem.exeadd.AoPS262752.1} holds\footnote{\textit{Proof.} Assume that
$m<i$. Then, $m\neq i$. Thus, $\left[  m=i\right]  =0$.
\par
But every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies $k\leq m<i$. Hence,
every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies $\dbinom{k}{i}=0$ (by
(\ref{eq.binom.0}), applied to $k$ and $i$ instead of $m$ and $n$). Now,
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}}_{=0}\dbinom
{m}{k}=\sum_{k=0}^{m}\left(  -1\right)  ^{k}0\dbinom{m}{k}=0=\left[
m=i\right]
\]
(since $\left[  m=i\right]  =0$). In other words, Lemma
\ref{lem.exeadd.AoPS262752.1} holds, qed.}. Hence, for the rest of this proof
of Lemma \ref{lem.exeadd.AoPS262752.1}, we can WLOG assume that we don't have
$m<i$. Assume this.

We have $m\geq i$ (since we don't have $m<i$). Hence, $m\geq i\geq0$, so that%
\begin{align}
&  \sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}\dbinom{m}%
{k}}_{=\dbinom{m}{k}\dbinom{k}{i}}\nonumber\\
&  =\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{m}{k}\dbinom{k}{i}\nonumber\\
&  =\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}\underbrace{\dbinom
{k}{i}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}) (applied to }k\text{ and
}i\\\text{instead of }m\text{ and }n\text{) (since }k<i\text{))}}}+\sum
_{k=i}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{m}{k}\dbinom{k}{i}%
}_{\substack{=\dbinom{m}{i}\dbinom{m-i}{k-i}\\\text{(by
(\ref{eq.binom.trinom-rev.m}) (applied to }k\text{ and }i\\\text{instead of
}i\text{ and }a\text{) (since }k\geq i\text{))}}}\nonumber\\
&  =\underbrace{\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}0}%
_{=0}+\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
=\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
\nonumber\\
&  =\sum_{k=0}^{m-i}\underbrace{\left(  -1\right)  ^{k+i}}_{=\left(
-1\right)  ^{k}\left(  -1\right)  ^{i}}\dbinom{m}{i}\dbinom{m-i}{k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+i\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=0}^{m-i}\left(  -1\right)  ^{k}\left(  -1\right)  ^{i}\dbinom
{m}{i}\dbinom{m-i}{k}=\left(  -1\right)  ^{i}\dbinom{m}{i}\underbrace{\sum
_{k=0}^{m-i}\left(  -1\right)  ^{k}\dbinom{m-i}{k}}_{\substack{=\left[
m-i=0\right]  \\\text{(by (\ref{sol.exeadd.AoPS262752.n=0}) (applied to
}n=m-i\text{))}}}\nonumber\\
&  =\left(  -1\right)  ^{i}\dbinom{m}{i}\left[  \underbrace{m-i=0}%
_{\substack{\text{this is equivalent to}\\m=i}}\right]  =\left(  -1\right)
^{i}\dbinom{m}{i}\left[  m=i\right]  .
\label{pf.lem.exeadd.AoPS262752.1.short.main}%
\end{align}
But it is easy to see that $\dbinom{m}{i}\left[  m=i\right]  =\left[
m=i\right]  $\ \ \ \ \footnote{\textit{Proof.} We have $\underbrace{\dbinom
{i}{i}}_{=1}\left[  i=i\right]  =\left[  i=i\right]  $. In other words, the
equality $\dbinom{m}{i}\left[  m=i\right]  =\left[  m=i\right]  $ holds in the
case when $m=i$. Therefore, in order to prove this equality, we only need to
consider the case when $m\neq i$. So assume that $m\neq i$. Then, $\left[
m=i\right]  =0$, and thus $\dbinom{m}{i}\underbrace{\left[  m=i\right]  }%
_{=0}=0=\left[  m=i\right]  $, qed.}. Hence,
(\ref{pf.lem.exeadd.AoPS262752.1.short.main}) becomes%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\underbrace{\dbinom{m}{i}\left[  m=i\right]  }_{=\left[
m=i\right]  }=\left(  -1\right)  ^{i}\left[  m=i\right]  .
\]
This proves Lemma \ref{lem.exeadd.AoPS262752.1}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.exeadd.AoPS262752.1}.]If $m<i$, then Lemma
\ref{lem.exeadd.AoPS262752.1} holds\footnote{\textit{Proof.} Assume that
$m<i$. Then, $m\neq i$. Hence, the statement $m=i$ is false. Thus, $\left[
m=i\right]  =0$.
\par
But every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies $k\leq m<i$. Hence,
every $k\in\left\{  0,1,\ldots,m\right\}  $ satisfies
\begin{equation}
\dbinom{k}{i}=0 \label{pf.lem.exeadd.AoPS262752.1.fn1.1}%
\end{equation}
(by (\ref{eq.binom.0}), applied to $k$ and $i$ instead of $m$ and $n$). Now,
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}}%
_{\substack{=0\\\text{(by (\ref{pf.lem.exeadd.AoPS262752.1.fn1.1}))}}%
}\dbinom{m}{k}=\sum_{k=0}^{m}\left(  -1\right)  ^{k}0\dbinom{m}{k}=0=\left[
m=i\right]
\]
(since $\left[  m=i\right]  =0$). In other words, Lemma
\ref{lem.exeadd.AoPS262752.1} holds, qed.}. Hence, for the rest of this proof
of Lemma \ref{lem.exeadd.AoPS262752.1}, we can WLOG assume that we don't have
$m<i$. Assume this.

We have $m\geq i$ (since we don't have $m<i$). Hence, $m\geq i\geq0$, so that%
\begin{align}
&  \sum_{k=0}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{k}{i}\dbinom{m}%
{k}}_{=\dbinom{m}{k}\dbinom{k}{i}}\nonumber\\
&  =\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{m}{k}\dbinom{k}{i}\nonumber\\
&  =\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}\underbrace{\dbinom
{k}{i}}_{\substack{=0\\\text{(by (\ref{eq.binom.0}) (applied to }k\text{ and
}i\\\text{instead of }m\text{ and }n\text{) (since }k<i\text{))}}}+\sum
_{k=i}^{m}\left(  -1\right)  ^{k}\underbrace{\dbinom{m}{k}\dbinom{k}{i}%
}_{\substack{=\dbinom{m}{i}\dbinom{m-i}{k-i}\\\text{(by
(\ref{eq.binom.trinom-rev.m}) (applied to }k\text{ and }i\\\text{instead of
}i\text{ and }a\text{) (since }k\geq i\text{))}}}\nonumber\\
&  =\underbrace{\sum_{k=0}^{i-1}\left(  -1\right)  ^{k}\dbinom{m}{k}0}%
_{=0}+\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
=\sum_{k=i}^{m}\left(  -1\right)  ^{k}\dbinom{m}{i}\dbinom{m-i}{k-i}%
\nonumber\\
&  =\sum_{k=0}^{m-i}\underbrace{\left(  -1\right)  ^{k+i}}_{=\left(
-1\right)  ^{k}\left(  -1\right)  ^{i}}\dbinom{m}{i}\underbrace{\dbinom
{m-i}{k+i-i}}_{\substack{=\dbinom{m-i}{k}\\\text{(since }k+i-i=k\text{)}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+i\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=0}^{m-i}\left(  -1\right)  ^{k}\left(  -1\right)  ^{i}\dbinom
{m}{i}\dbinom{m-i}{k}=\left(  -1\right)  ^{i}\dbinom{m}{i}\underbrace{\sum
_{k=0}^{m-i}\left(  -1\right)  ^{k}\dbinom{m-i}{k}}_{\substack{=\left[
m-i=0\right]  \\\text{(by (\ref{sol.exeadd.AoPS262752.n=0}) (applied to
}n=m-i\text{))}}}\nonumber\\
&  =\left(  -1\right)  ^{i}\dbinom{m}{i}\left[  m-i=0\right]  .
\label{pf.lem.exeadd.AoPS262752.1.main}%
\end{align}


If $m=i$, then Lemma \ref{lem.exeadd.AoPS262752.1}
holds\footnote{\textit{Proof.} Assume that $m=i$. Hence, $m-i=0$, so that
$\left[  m-i=0\right]  =1$. Also, $\left[  m=i\right]  =1$ (since $m=i$).
Also, from $m=i$, we obtain $\dbinom{m}{i}=\dbinom{i}{i}=1$ (by
(\ref{eq.binom.mm}), applied to $m=i$). Hence,
(\ref{pf.lem.exeadd.AoPS262752.1.main}) becomes%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\underbrace{\dbinom{m}{i}}_{=1}\underbrace{\left[
m-i=0\right]  }_{=1=\left[  m=i\right]  }=\left(  -1\right)  ^{i}\left[
m=i\right]  .
\]
In other words, Lemma \ref{lem.exeadd.AoPS262752.1} holds, qed.}. Hence, for
the rest of this proof of Lemma \ref{lem.exeadd.AoPS262752.1}, we can WLOG
assume that we don't have $m=i$. Assume this.

We have $m\neq i$ (since we don't have $m=i$). Hence, $\left[  m=i\right]
=0$, so that $\left(  -1\right)  ^{i}\underbrace{\left[  m=i\right]  }_{=0}%
=0$. Also, $m-i\neq0$ (since $m\neq i$) and thus $\left[  m-i=0\right]  =0$.
Now, (\ref{pf.lem.exeadd.AoPS262752.1.main}) becomes%
\[
\sum_{k=0}^{m}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{m}{k}=\left(
-1\right)  ^{i}\dbinom{m}{i}\underbrace{\left[  m-i=0\right]  }_{=0}=0=\left(
-1\right)  ^{i}\left[  m=i\right]
\]
(since $\left(  -1\right)  ^{i}\left[  m=i\right]  =0$). This proves Lemma
\ref{lem.exeadd.AoPS262752.1}.
\end{proof}
\end{verlong}

Here comes one more simple lemma:

\begin{lemma}
\label{lem.exeadd.AoPS262752.2}Let $n\in\mathbb{N}$. Let $a_{0},a_{1}%
,\ldots,a_{n}$ be $n+1$ polynomials in the indeterminate $X$ with rational
coefficients (that is, $n+1$ elements of $\mathbb{Q}\left[  X\right]  $).
Then,%
\[
\sum_{i=0}^{n}a_{i}\left[  n-i=i\right]  =%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.exeadd.AoPS262752.2}.]We have $n\in\mathbb{N}$, so
that $n\geq0$. For every $i\in\left\{  0,1,\ldots,n\right\}  $, we have%
\begin{equation}
\left[  \underbrace{n-i=i}_{\text{this is equivalent to }n=2i}\right]
=\left[  \underbrace{n=2i}_{\text{this is equivalent to }i=n/2}\right]
=\left[  i=n/2\right]  . \label{pf.lem.exeadd.AoPS262752.2.1}%
\end{equation}
Thus,%
\begin{align}
&  \underbrace{\sum_{i=0}^{n}}_{=\sum_{i\in\left\{  0,1,\ldots,n\right\}  }%
}a_{i}\underbrace{\left[  n-i=i\right]  }_{\substack{=\left[  i=n/2\right]
\\\text{(by (\ref{pf.lem.exeadd.AoPS262752.2.1}))}}}\nonumber\\
&  =\sum_{i\in\left\{  0,1,\ldots,n\right\}  }a_{i}\left[  i=n/2\right]
=\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}}a_{i}%
\underbrace{\left[  i=n/2\right]  }_{\substack{=1\\\text{(since }%
i=n/2\text{)}}}+\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i\neq
n/2}}a_{i}\underbrace{\left[  i=n/2\right]  }_{\substack{=0\\\text{(since
}i\neq n/2\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every }i\in\left\{  0,1,\ldots
,n\right\}  \text{ satisfies either }i=n/2\text{ or }i\neq n/2\text{ (but not
both)}\right) \nonumber\\
&  =\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}%
}a_{i}+\underbrace{\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i\neq
n/2}}a_{i}0}_{=0}=\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}
;\\i=n/2}}a_{i}. \label{pf.lem.exeadd.AoPS262752.2.2}%
\end{align}


We must be in one of the following two cases:

\textit{Case 1:} The number $n$ is even.

\textit{Case 2:} The number $n$ is odd.

Let us first consider Case 1. In this case, the number $n$ is even. Hence,
\begin{equation}%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
=a_{n/2}. \label{pf.lem.exeadd.AoPS262752.2.c1.1}%
\end{equation}


On the other hand, $n/2\in\mathbb{Z}$ (since $n$ is even). Combined with
$n/2\geq0$ (since $n\geq0$) and $n/2\leq n$ (for the same reason), this shows
that $n/2\in\left\{  0,1,\ldots,n\right\}  $. Now,
(\ref{pf.lem.exeadd.AoPS262752.2.2}) becomes%
\begin{align*}
&  \sum_{i=0}^{n}a_{i}\left[  n-i=i\right] \\
&  =\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}}a_{i}%
=a_{n/2}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n/2\in\left\{  0,1,\ldots
,n\right\}  \right) \\
&  =%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.exeadd.AoPS262752.2.c1.1}%
)}\right)  .
\end{align*}
Thus, Lemma \ref{lem.exeadd.AoPS262752.2} is proven in Case 1.

Let us now consider Case 2. In this case, the number $n$ is odd. Hence,
\begin{equation}%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
=0. \label{pf.lem.exeadd.AoPS262752.2.c2.1}%
\end{equation}


On the other hand, $n/2\notin\mathbb{Z}$ (since $n$ is odd). Hence,
$n/2\notin\left\{  0,1,\ldots,n\right\}  $ (since $\left\{  0,1,\ldots
,n\right\}  \subseteq\mathbb{Z}$). Now, (\ref{pf.lem.exeadd.AoPS262752.2.2})
becomes%
\begin{align*}
&  \sum_{i=0}^{n}a_{i}\left[  n-i=i\right] \\
&  =\sum_{\substack{i\in\left\{  0,1,\ldots,n\right\}  ;\\i=n/2}}a_{i}=\left(
\text{empty sum}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
n/2\notin\left\{  0,1,\ldots,n\right\}  \right) \\
&  =0=%
\begin{cases}
a_{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.exeadd.AoPS262752.2.c2.1}%
)}\right)  .
\end{align*}
Thus, Lemma \ref{lem.exeadd.AoPS262752.2} is proven in Case 2.

We have now proved Lemma \ref{lem.exeadd.AoPS262752.2} in both Cases 1 and 2.
Since these two Cases cover all possibilities, this shows that Lemma
\ref{lem.exeadd.AoPS262752.2} always holds.
\end{proof}

Now, we are ready to solve Additional exercise \ref{exeadd.AoPS262752} again:

\begin{proof}
[Second solution to Additional exercise \ref{exeadd.AoPS262752}.]Let
$g\in\left\{  0,1,\ldots,n\right\}  $ be arbitrary. Then, Theorem
\ref{thm.vandermonde} (applied to $n-g$ instead of $n$) yields%
\[
\dbinom{X+Y}{n-g}=\sum_{k=0}^{n-g}\dbinom{X}{k}\dbinom{Y}{n-g-k}%
\]
(an equality between two polynomials in $X$ and $Y$). Substituting $g$ and
$X-g$ for $X$ and $Y$ in this equality, we obtain%
\[
\dbinom{g+\left(  X-g\right)  }{n-g}=\sum_{k=0}^{n-g}\dbinom{g}{k}\dbinom
{X-g}{n-g-k}=\sum_{i=0}^{n-g}\dbinom{g}{i}\dbinom{X-g}{n-g-i}%
\]
(here, we have renamed the summation index $k$ as $i$). Since $g+\left(
X-g\right)  =X$, this rewrites as
\begin{equation}
\dbinom{X}{n-g}=\sum_{i=0}^{n-g}\dbinom{g}{i}\dbinom{X-g}{n-g-i}.
\label{sol.exeadd.AoPS262752.sol2.1}%
\end{equation}


Now, let us forget that we fixed $g$. We thus have shown that
(\ref{sol.exeadd.AoPS262752.sol2.1}) holds for every $g\in\left\{
0,1,\ldots,n\right\}  $.

On the other hand, for every $k\in\mathbb{N}$ and $i\in\mathbb{N}$ satisfying
$k+i\leq n$, we have%
\begin{equation}
\dbinom{X}{k}\dbinom{X-k}{n-k-i}=\dbinom{X}{n-i}\dbinom{n-i}{k}
\label{sol.exeadd.AoPS262752.sol2.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exeadd.AoPS262752.sol2.3}):} Let
$k\in\mathbb{N}$ and $i\in\mathbb{N}$ be such that $k+i\leq n$. From $k+i\leq
n$, we obtain $n-i\geq k$. Thus, $n-i\geq k\geq0$, so that $n-i\in\mathbb{N}$.
Hence, (\ref{eq.binom.trinom-rev}) (applied to $n-i$ and $k$ instead of $i$
and $a$) shows that
\[
\dbinom{X}{n-i}\dbinom{n-i}{k}=\dbinom{X}{k}\dbinom{X-k}{\left(  n-i\right)
-k}=\dbinom{X}{k}\dbinom{X-k}{n-k-i}%
\]
(since $\left(  n-i\right)  -k=n-k-i$). This proves
(\ref{sol.exeadd.AoPS262752.sol2.3}).}.

Now,%
\begin{align}
&  \sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\underbrace{\dbinom
{X}{n-k}}_{\substack{=\sum_{i=0}^{n-k}\dbinom{k}{i}\dbinom{X-k}{n-k-i}%
\\\text{(by (\ref{sol.exeadd.AoPS262752.sol2.1}) (applied to }g=k\text{))}%
}}\nonumber\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\left(  \sum_{i=0}%
^{n-k}\dbinom{k}{i}\dbinom{X-k}{n-k-i}\right) \nonumber\\
&  =\underbrace{\sum_{k=0}^{n}\sum_{i=0}^{n-k}}_{\substack{=\sum
_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\k\leq n;\ i\leq n-k}%
}=\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\k\leq n;\ k+i\leq
n}}\\\text{(since the condition }i\leq n-k\text{ is equivalent to }k+i\leq
n\text{)}}}\left(  -1\right)  ^{k}\underbrace{\dbinom{X}{k}\dbinom{k}{i}%
}_{=\dbinom{k}{i}\dbinom{X}{k}}\dbinom{X-k}{n-k-i}\nonumber\\
&  =\underbrace{\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\k\leq
n;\ k+i\leq n}}}_{\substack{=\sum_{\substack{\left(  k,i\right)  \in
\mathbb{N}^{2};\\k+i\leq n}}\\\text{(since the condition }\left(  k\leq
n\text{ and }k+i\leq n\right)  \text{ is equivalent to }k+i\leq
n\\\text{(because the condition }k\leq n\text{ follows from }k+i\leq
n\text{))}}}\left(  -1\right)  ^{k}\dbinom{k}{i}\underbrace{\dbinom{X}%
{k}\dbinom{X-k}{n-k-i}}_{\substack{=\dbinom{X}{n-i}\dbinom{n-i}{k}\\\text{(by
(\ref{sol.exeadd.AoPS262752.sol2.3}))}}}\nonumber\\
&  =\underbrace{\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}%
^{2};\\k+i\leq n}}}_{\substack{=\sum_{\substack{\left(  k,i\right)
\in\mathbb{N}^{2};\\i\leq n;\ k+i\leq n}}\\\text{(since the condition }k+i\leq
n\text{ is equivalent to }\left(  i\leq n\text{ and }k+i\leq n\right)
\\\text{(because the condition }i\leq n\text{ follows from }k+i\leq
n\text{))}}}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{X}{n-i}\dbinom{n-i}%
{k}\nonumber
\end{align}%
\begin{align*}
&  =\underbrace{\sum_{\substack{\left(  k,i\right)  \in\mathbb{N}^{2};\\i\leq
n;\ k+i\leq n}}}_{\substack{=\sum_{\substack{\left(  i,k\right)  \in
\mathbb{N}^{2};\\i\leq n;\ k+i\leq n}}=\sum_{\substack{\left(  i,k\right)
\in\mathbb{N}^{2};\\i\leq n;\ k\leq n-i}}\\\text{(since the condition }k+i\leq
n\text{ is equivalent to }k\leq n-i\text{)}}}\left(  -1\right)  ^{k}\dbinom
{k}{i}\dbinom{X}{n-i}\dbinom{n-i}{k}\\
&  =\underbrace{\sum_{\substack{\left(  i,k\right)  \in\mathbb{N}^{2};\\i\leq
n;\ k\leq n-i}}}_{=\sum_{i=0}^{n}\sum_{k=0}^{n-i}}\left(  -1\right)
^{k}\dbinom{k}{i}\dbinom{X}{n-i}\dbinom{n-i}{k}\\
&  =\sum_{i=0}^{n}\sum_{k=0}^{n-i}\left(  -1\right)  ^{k}\dbinom{k}{i}%
\dbinom{X}{n-i}\dbinom{n-i}{k}=\sum_{i=0}^{n}\dbinom{X}{n-i}\underbrace{\sum
_{k=0}^{n-i}\left(  -1\right)  ^{k}\dbinom{k}{i}\dbinom{n-i}{k}}%
_{\substack{=\left(  -1\right)  ^{i}\left[  n-i=i\right]  \\\text{(by Lemma
\ref{lem.exeadd.AoPS262752.1}}\\\text{(applied to }m=n-i\text{))}}}\\
&  =\sum_{i=0}^{n}\underbrace{\dbinom{X}{n-i}\left(  -1\right)  ^{i}%
}_{=\left(  -1\right)  ^{i}\dbinom{X}{n-i}}\left[  n-i=i\right] \\
&  =\sum_{i=0}^{n}\left(  -1\right)  ^{i}\dbinom{X}{n-i}\left[  n-i=i\right]
=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n-n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.exeadd.AoPS262752.2},
applied to }a_{i}=\left(  -1\right)  ^{i}\dbinom{X}{n-i}\right) \\
&  =%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n-n/2=n/2\right)  .
\end{align*}
This solves Additional exercise \ref{exeadd.AoPS262752}.
\end{proof}

\subsubsection{Addendum}

Let us record a classical result which follows from Additional exercise
\ref{exeadd.AoPS262752}:

\begin{corollary}
\label{cor.AoPS262752.X=n}Let $n\in\mathbb{N}$. Then,%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}^{2}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{n}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]

\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.AoPS262752.X=n}.]Additional exercise
\ref{exeadd.AoPS262752} shows that%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{X}{k}\dbinom{X}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{X}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
Substituting $n$ for $X$ in this equality, we obtain%
\[
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\dbinom{n}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{n}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\]
Now,%
\begin{align*}
\sum_{k=0}^{n}\left(  -1\right)  ^{k}\underbrace{\dbinom{n}{k}^{2}}%
_{=\dbinom{n}{k}\dbinom{n}{k}}  &  =\sum_{k=0}^{n}\left(  -1\right)
^{k}\dbinom{n}{k}\underbrace{\dbinom{n}{k}}_{\substack{=\dbinom{n}%
{n-k}\\\text{(by (\ref{eq.binom.symm}) (applied to }n\text{ and }%
k\\\text{instead of }m\text{ and }n\text{))}}}\\
&  =\sum_{k=0}^{n}\left(  -1\right)  ^{k}\dbinom{n}{k}\dbinom{n}{n-k}=%
\begin{cases}
\left(  -1\right)  ^{n/2}\dbinom{n}{n/2}, & \text{if }n\text{ is even};\\
0, & \text{if }n\text{ is odd}%
\end{cases}
.
\end{align*}
This proves Corollary \ref{cor.AoPS262752.X=n}.
\end{proof}

\subsection{Solution to Additional exercise \ref{exeadd.choose.a/b}}

\subsubsection{First solution}

Before we solve Additional exercise \ref{exeadd.choose.a/b}, let us prove two
basic facts in modular arithmetic:

\begin{proposition}
\label{prop.exeadd.choose.a/b.lem1}Let $b$ and $c$ be two integers such that
$c>0$. Then, there exists an $s\in\mathbb{Z}$ such that $b^{c-1}\equiv
sb^{c}\operatorname{mod}c$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.exeadd.choose.a/b.lem1}.]For every integer
$m$, let $m\%c$ denote the remainder obtained when $m$ is divided by $c$.
Thus, every integer $m$ satisfies%
\begin{equation}
m\%c\in\left\{  0,1,\ldots,c-1\right\}
\label{pf.prop.exeadd.choose.a/b.lem1.rem.1}%
\end{equation}
and%
\begin{equation}
m\%c\equiv m\operatorname{mod}c. \label{pf.prop.exeadd.choose.a/b.lem1.rem.2}%
\end{equation}


We shall now show that two of the $c+1$ integers $b^{0}\%c,b^{1}%
\%c,\ldots,b^{c}\%c$ are equal.

Indeed, assume the contrary (for the sake of contradiction). Thus, the $c+1$
integers $b^{0}\%c,b^{1}\%c,\ldots,b^{c}\%c$ are pairwise distinct. Hence,%
\[
\left\vert \left\{  b^{0}\%c,b^{1}\%c,\ldots,b^{c}\%c\right\}  \right\vert
=c+1.
\]
But we have $b^{i}\%c\in\left\{  0,1,\ldots,c-1\right\}  $ for each
$i\in\left\{  0,1,\ldots,c\right\}  $ (by
(\ref{pf.prop.exeadd.choose.a/b.lem1.rem.1}), applied to $m=b^{i}$). Thus,%
\[
\left\{  b^{0}\%c,b^{1}\%c,\ldots,b^{c}\%c\right\}  \subseteq\left\{
0,1,\ldots,c-1\right\}  .
\]
Therefore,%
\[
\left\vert \left\{  b^{0}\%c,b^{1}\%c,\ldots,b^{c}\%c\right\}  \right\vert
\leq\left\vert \left\{  0,1,\ldots,c-1\right\}  \right\vert =c,
\]
so that%
\[
c\geq\left\vert \left\{  b^{0}\%c,b^{1}\%c,\ldots,b^{c}\%c\right\}
\right\vert =c+1.
\]
This contradicts $c<c+1$. This contradiction shows that our assumption was
wrong. Hence, we have proven that two of the $c+1$ integers $b^{0}%
\%c,b^{1}\%c,\ldots,b^{c}\%c$ are equal. In other words, there exist two
distinct elements $u$ and $v$ of $\left\{  0,1,\ldots,c\right\}  $ such that
$b^{u}\%c=b^{v}\%c$. Consider these $u$ and $v$.

We can WLOG assume that $u\leq v$ (since otherwise, we can simply switch $u$
with $v$, and nothing changes). Assume this. Thus, $u\leq v$, so that $u<v$
(since $u$ and $v$ are distinct).

Now, (\ref{pf.prop.exeadd.choose.a/b.lem1.rem.2}) (applied to $m=b^{u}$)
yields $b^{u}\%c\equiv b^{u}\operatorname{mod}c$. Also,
(\ref{pf.prop.exeadd.choose.a/b.lem1.rem.2}) (applied to $m=b^{v}$) yields
$b^{v}\%c\equiv b^{v}\operatorname{mod}c$. Thus, $b^{u}\equiv b^{u}%
\%c=b^{v}\%c\equiv b^{v}\operatorname{mod}c$.

Also, $u\in\left\{  0,1,\ldots,c\right\}  $, so that $0\leq u$. Also,
$v\in\left\{  0,1,\ldots,c\right\}  $, so that $v\leq c$. Hence,
$c-v\in\mathbb{N}$. Thus, $b^{c-v}$ is a well-defined integer.

Also, $0\leq u<v$, so that $0\leq v-1$ (since $0$ and $v$ are integers). Thus,
$v-1\in\mathbb{N}$, so that $b^{v-1}$ is a well-defined integer.

But $u<v$, and thus $u\leq v-1$ (since $u$ and $v$ are integers). Hence,
$\left(  v-1\right)  -u\in\mathbb{N}$. Thus, $b^{\left(  v-1\right)  -u}$ is a
well-defined integer. Set $t=b^{\left(  v-1\right)  -u}$. Thus, $t$ is an integer.

Now, $v-1=\left(  \left(  v-1\right)  -u\right)  +u$, and thus%
\begin{align*}
b^{v-1}  &  =b^{\left(  \left(  v-1\right)  -u\right)  +u}%
=\underbrace{b^{\left(  v-1\right)  -u}}_{=t}b^{u}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }0\leq u\leq v-1\right) \\
&  =t\underbrace{b^{u}}_{\equiv b^{v}\operatorname{mod}c}\equiv tb^{v}%
\operatorname{mod}c.
\end{align*}


Now, $c-1=\left(  v-1\right)  +\left(  c-v\right)  $, so that%
\begin{align*}
b^{c-1}  &  =b^{\left(  v-1\right)  +\left(  c-v\right)  }=\underbrace{b^{v-1}%
}_{\equiv tb^{v}\operatorname{mod}c}b^{c-v}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }v-1\in\mathbb{N}\text{ and }c-v\in\mathbb{N}\right) \\
&  \equiv t\underbrace{b^{v}b^{c-v}}_{=b^{v+\left(  c-v\right)  }=b^{c}%
}=tb^{c}\operatorname{mod}c.
\end{align*}
Hence, there exists an $s\in\mathbb{Z}$ such that $b^{c-1}\equiv
sb^{c}\operatorname{mod}c$ (namely, $s=t$). This proves Proposition
\ref{prop.exeadd.choose.a/b.lem1}.
\end{proof}

\begin{lemma}
\label{lem.exeadd.choose.a/b.prod-of-congs}Let $n\in\mathbb{N}$. Let $c$ be a
positive integer. Let $u_{0},u_{1},\ldots,u_{n-1}$ be $n$ integers. Let
$v_{0},v_{1},\ldots,v_{n-1}$ be $n$ integers. Let $d$ be an integer. Assume
that%
\begin{equation}
du_{i}\equiv dv_{i}\operatorname{mod}c\ \ \ \ \ \ \ \ \ \ \text{for each }%
i\in\left\{  0,1,\ldots,n-1\right\}  .
\label{eq.lem.exeadd.choose.a/b.prod-of-congs.cond}%
\end{equation}
Then,%
\[
d\prod_{i=0}^{n-1}u_{i}\equiv d\prod_{i=0}^{n-1}v_{i}\operatorname{mod}c.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.exeadd.choose.a/b.prod-of-congs}.]We claim that%
\begin{equation}
d\prod_{i=0}^{k-1}u_{i}\equiv d\prod_{i=0}^{k-1}v_{i}\operatorname{mod}c
\label{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}%
\end{equation}
for each $k\in\left\{  0,1,\ldots,n\right\}  $

\textit{Proof of (\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}):} We
shall prove (\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) by induction
over $c$:

\textit{Induction base:} We have $d\underbrace{\prod_{i=0}^{0-1}u_{i}%
}_{=\left(  \text{empty product}\right)  =1}=d$. Comparing this with
\newline$d\underbrace{\prod_{i=0}^{0-1}v_{i}}_{=\left(  \text{empty
product}\right)  =1}=d$, we obtain $d\prod_{i=0}^{0-1}u_{i}=d\prod_{i=0}%
^{0-1}v_{i}$. Hence, $d\prod_{i=0}^{0-1}u_{i}\equiv d\prod_{i=0}^{0-1}%
v_{i}\operatorname{mod}c$. In other words,
(\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) holds for $k=0$. This
completes the induction base.

\textit{Induction step:} Let $K\in\left\{  0,1,\ldots,n\right\}  $ be
positive. Assume that (\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal})
holds for $k=K-1$. We must show that
(\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) holds for $k=K$.

We have $K\in\left\{  1,2,\ldots,n\right\}  $ (since $K\in\left\{
0,1,\ldots,n\right\}  $ and $K$ is positive). Hence, $K-1\in\left\{
0,1,\ldots,n-1\right\}  $.

We have assumed that (\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) holds
for $k=K-1$. In other words, we have%
\begin{equation}
d\prod_{i=0}^{\left(  K-1\right)  -1}u_{i}\equiv d\prod_{i=0}^{\left(
K-1\right)  -1}v_{i}\operatorname{mod}c.
\label{pf.lem.exeadd.choose.a/b.prod-of-congs.goal.pf.1}%
\end{equation}


Now,%
\begin{align*}
d\underbrace{\prod_{i=0}^{K-1}u_{i}}_{=\left(  \prod_{i=0}^{\left(
K-1\right)  -1}u_{i}\right)  u_{K-1}}  &  =\underbrace{d\left(  \prod
_{i=0}^{\left(  K-1\right)  -1}u_{i}\right)  }_{\substack{\equiv d\prod
_{i=0}^{\left(  K-1\right)  -1}v_{i}\operatorname{mod}c\\\text{(by
(\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal.pf.1}))}}}u_{K-1}\\
&  \equiv\left(  d\prod_{i=0}^{\left(  K-1\right)  -1}v_{i}\right)
u_{K-1}=\underbrace{du_{K-1}}_{\substack{\equiv dv_{K-1}\operatorname{mod}%
c\\\text{(by (\ref{eq.lem.exeadd.choose.a/b.prod-of-congs.cond}), applied to
}i=K-1\text{)}}}\prod_{i=0}^{\left(  K-1\right)  -1}v_{i}\\
&  \equiv dv_{K-1}\prod_{i=0}^{\left(  K-1\right)  -1}v_{i}%
=d\underbrace{\left(  \prod_{i=0}^{\left(  K-1\right)  -1}v_{i}\right)
v_{K-1}}_{=\prod_{i=0}^{K-1}v_{i}}=d\prod_{i=0}^{K-1}v_{i}\operatorname{mod}c.
\end{align*}
In other words, (\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) holds for
$k=K$. This completes the induction step. Thus,
(\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) is proven by induction.

Now, (\ref{pf.lem.exeadd.choose.a/b.prod-of-congs.goal}) (applied to $k=n$)
yields%
\[
d\prod_{i=0}^{n-1}u_{i}\equiv d\prod_{i=0}^{n-1}v_{i}\operatorname{mod}c.
\]
This proves Lemma \ref{lem.exeadd.choose.a/b.prod-of-congs}.
\end{proof}

\begin{proof}
[First solution to Additional exercise \ref{exeadd.choose.a/b}.]Set $c=n!$.
Notice that $c$ is a positive integer (since $c=n!=1\cdot2\cdot\cdots\cdot
n$); hence, $c-1\in\mathbb{N}$.

Proposition \ref{prop.exeadd.choose.a/b.lem1} shows that there exists an
$s\in\mathbb{Z}$ such that $b^{c-1}\equiv sb^{c}\operatorname{mod}c$. Consider
this $s$.

For every $i\in\mathbb{Z}$, we have%
\begin{align}
b^{c-1}\left(  a-bi\right)   &  =\underbrace{b^{c-1}}_{\equiv sb^{c}%
\operatorname{mod}c}a-\underbrace{b^{c-1}b}_{=b^{c}}i\nonumber\\
&  \equiv sb^{c}a-b^{c}i=\underbrace{b^{c}}_{=b^{c-1}b}\left(  sa-i\right)
=b^{c-1}b\left(  sa-i\right)  \operatorname{mod}c.
\label{sol.exeadd.choose.a/b.congr}%
\end{align}
Thus, in particular, (\ref{sol.exeadd.choose.a/b.congr}) holds for every
$i\in\left\{  0,1,\ldots,n-1\right\}  $. Hence, Lemma
\ref{lem.exeadd.choose.a/b.prod-of-congs} (applied to $d=b^{c-1}$,
$u_{i}=a-bi$ and $v_{i}=b\left(  sa-i\right)  $) yields%
\begin{equation}
b^{c-1}\prod_{i=0}^{n-1}\left(  a-bi\right)  \equiv b^{c-1}\prod_{i=0}%
^{n-1}\left(  b\left(  sa-i\right)  \right)  \operatorname{mod}c.
\label{sol.exeadd.choose.a/b.congr.prod}%
\end{equation}


Now, recall that%
\begin{align}
\dbinom{X}{n}  &  =\dfrac{X\left(  X-1\right)  \cdots\left(  X-n+1\right)
}{n!}=\underbrace{\dfrac{1}{n!}}_{\substack{=\dfrac{1}{c}\\\text{(since
}n!=c\text{)}}}\underbrace{X\left(  X-1\right)  \cdots\left(  X-n+1\right)
}_{=\prod_{i=0}^{n-1}\left(  X-i\right)  }\nonumber\\
&  =\dfrac{1}{c}\prod_{i=0}^{n-1}\left(  X-i\right)  .
\label{sol.exeadd.choose.a/b.congr.Xn}%
\end{align}
If we substitute $sa$ for $X$ in this polynomial identity, then we obtain%
\[
\dbinom{sa}{n}=\dfrac{1}{c}\prod_{i=0}^{n-1}\left(  sa-i\right)  =\dfrac
{\prod_{i=0}^{n-1}\left(  sa-i\right)  }{c}.
\]
Thus,%
\[
\dfrac{\prod_{i=0}^{n-1}\left(  sa-i\right)  }{c}=\dbinom{sa}{n}\in\mathbb{Z}%
\]
(by (\ref{eq.binom.int}) (applied to $m=sa$)). In other words, $c\mid
\prod_{i=0}^{n-1}\left(  sa-i\right)  $. In other words,
\begin{equation}
\prod_{i=0}^{n-1}\left(  sa-i\right)  \equiv0\operatorname{mod}c.
\label{sol.exeadd.choose.a/b.congr.prod0}%
\end{equation}
Now, (\ref{sol.exeadd.choose.a/b.congr.prod}) becomes%
\[
b^{c-1}\prod_{i=0}^{n-1}\left(  a-bi\right)  \equiv b^{c-1}\underbrace{\prod
_{i=0}^{n-1}\left(  b\left(  sa-i\right)  \right)  }_{=b^{n}\prod_{i=0}%
^{n-1}\left(  sa-i\right)  }=b^{c-1}b^{n}\underbrace{\prod_{i=0}^{n-1}\left(
sa-i\right)  }_{\substack{\equiv0\operatorname{mod}c\\\text{(by
(\ref{sol.exeadd.choose.a/b.congr.prod0}))}}}\equiv0\operatorname{mod}c.
\]
In other words,
\begin{equation}
c\mid b^{c-1}\prod_{i=0}^{n-1}\left(  a-bi\right)  .
\label{sol.exeadd.choose.a/b.divi}%
\end{equation}


If we substitute $a/b$ for $X$ in the polynomial identity
(\ref{sol.exeadd.choose.a/b.congr.Xn}), then we obtain%
\begin{align*}
\dbinom{a/b}{n}  &  =\dfrac{1}{c}\prod_{i=0}^{n-1}\underbrace{\left(
a/b-i\right)  }_{=\dfrac{1}{b}\left(  a-bi\right)  }=\dfrac{1}{c}%
\underbrace{\prod_{i=0}^{n-1}\left(  \dfrac{1}{b}\left(  a-bi\right)  \right)
}_{=\left(  \dfrac{1}{b}\right)  ^{n}\prod_{i=0}^{n-1}\left(  a-bi\right)  }\\
&  =\dfrac{1}{c}\underbrace{\left(  \dfrac{1}{b}\right)  ^{n}}_{=\dfrac
{1}{b^{n}}}\prod_{i=0}^{n-1}\left(  a-bi\right)  =\dfrac{1}{c}\cdot\dfrac
{1}{b^{n}}\prod_{i=0}^{n-1}\left(  a-bi\right)  .
\end{align*}
Multiplying this equality with $b^{n+\left(  c-1\right)  }$, we obtain%
\begin{align*}
b^{n+\left(  c-1\right)  }\dbinom{a/b}{n}  &  =b^{n+\left(  c-1\right)  }%
\cdot\dfrac{1}{c}\cdot\dfrac{1}{b^{n}}\prod_{i=0}^{n-1}\left(  a-bi\right)
=\dfrac{1}{c}\cdot\underbrace{b^{n+\left(  c-1\right)  }\cdot\dfrac{1}{b^{n}}%
}_{=b^{c-1}}\prod_{i=0}^{n-1}\left(  a-bi\right) \\
&  =\dfrac{1}{c}\cdot b^{c-1}\prod_{i=0}^{n-1}\left(  a-bi\right)
=\dfrac{b^{c-1}\prod_{i=0}^{n-1}\left(  a-bi\right)  }{c}\in\mathbb{Z}%
\end{align*}
(by (\ref{sol.exeadd.choose.a/b.divi})). Hence, there exists some
$N\in\mathbb{N}$ such that $b^{N}\dbinom{a/b}{n}\in\mathbb{Z}$ (namely,
$N=n+\left(  c-1\right)  $). This solves Additional exercise
\ref{exeadd.choose.a/b}.
\end{proof}

\begin{remark}
Our above solution of Additional exercise \ref{exeadd.choose.a/b} shows that
the $N$ in this exercise can be taken to be $n+\left(  n!-1\right)  $. This
is, however, far from being the best possible value of $N$. A much better
value that also works is $\max\left\{  0,2n-1\right\}  $. Proving this,
however, would require a different idea. The second solution below gives a
proof of this better value. (Alternatively, this better value can be obtained
by studying the exponents of primes appearing in $n!$.)
\end{remark}

\subsubsection{Second solution}

We shall now prepare to give a second solution to Additional exercise
\ref{exeadd.choose.a/b}. Our main goal is to prove the following fact:

\begin{theorem}
\label{thm.sol.exeadd.choose.a/b.sol2.main}Let $a$ and $b$ be two integers
such that $b\neq0$. Let $n$ be a positive integer. Then, $b^{2n-1}\dbinom
{a/b}{n}\in\mathbb{Z}$.
\end{theorem}

Clearly, Additional exercise \ref{exeadd.choose.a/b} immediately follows from
Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main} in the case when $n\neq0$.
(In the case when $n=0$, it holds for obvious reasons.)

Before we start proving Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}, let
us show the following lemma:

\begin{lemma}
\label{lem.sol.exeadd.choose.a/b.sol2.lem}Let $b$ and $n$ be positive
integers. Assume that every $k\in\left\{  1,2,\ldots,n-1\right\}  $ and
$c\in\mathbb{Z}$ satisfy%
\begin{equation}
b^{2k-1}\dbinom{c/b}{k}\in\mathbb{Z}.
\label{eq.lem.sol.exeadd.choose.a/b.sol2.lem.ass}%
\end{equation}
Then:

\textbf{(a)} Every $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$ satisfy%
\[
b^{2n-2}\left(  \dbinom{\left(  u+v\right)  /b}{n}-\dbinom{u/b}{n}%
-\dbinom{v/b}{n}\right)  \in\mathbb{Z}.
\]


\textbf{(b)} Every $a\in\mathbb{Z}$ and $h\in\mathbb{N}$ satisfy%
\[
b^{2n-2}\left(  \dbinom{ha/b}{n}-h\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]


\textbf{(c)} Every $a\in\mathbb{Z}$ satisfies%
\[
b^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem}.]\textbf{(a)} Let
$u\in\mathbb{Z}$ and $v\in\mathbb{Z}$. Substituting $u/b$ for $X$ in
Proposition \ref{prop.binom.00} \textbf{(a)}, we obtain $\dbinom{u/b}{0}=1$.
Similarly, $\dbinom{v/b}{0}=1$.

Substituting $u/b$ and $v/b$ for $X$ and $Y$ in Theorem \ref{thm.vandermonde},
we obtain%
\begin{align*}
\dbinom{u/b+v/b}{n}  &  =\sum_{k=0}^{n}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\\
&  =\underbrace{\dbinom{u/b}{0}}_{=1}\underbrace{\dbinom{v/b}{n-0}}%
_{=\dbinom{v/b}{n}}+\sum_{k=1}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}%
+\dbinom{u/b}{n}\underbrace{\dbinom{v/b}{n-n}}_{=\dbinom{v/b}{0}=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }k=0\text{ and}\\
\text{for }k=n\text{ from the sum (since }0\text{ and }n\text{ are two
distinct}\\
\text{elements of }\left\{  0,1,\ldots,n\right\}  \text{)}%
\end{array}
\right) \\
&  =\dbinom{v/b}{n}+\sum_{k=1}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}%
+\dbinom{u/b}{n}.
\end{align*}
Subtracting $\dbinom{u/b}{n}+\dbinom{v/b}{n}$ from this equality, we obtain%
\begin{equation}
\dbinom{u/b+v/b}{n}-\dbinom{u/b}{n}-\dbinom{v/b}{n}=\sum_{k=1}^{n-1}%
\dbinom{u/b}{k}\dbinom{v/b}{n-k}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.a.1}%
\end{equation}


But for every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the number
$b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}$ is an
integer\footnote{\textit{Proof:} Let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
\par
From (\ref{eq.lem.sol.exeadd.choose.a/b.sol2.lem.ass}) (applied to $c=u$), we
obtain $b^{2k-1}\dbinom{u/b}{k}\in\mathbb{Z}$. In other words, the number
$b^{2k-1}\dbinom{u/b}{k}$ is an integer.
\par
But we also have $n-k\in\left\{  1,2,\ldots,n-1\right\}  $ (since
$k\in\left\{  1,2,\ldots,n-1\right\}  $). Thus, from
(\ref{eq.lem.sol.exeadd.choose.a/b.sol2.lem.ass}) (applied to $n-k$ and $v$
instead of $k$ and $c$), we obtain $b^{2\left(  n-k\right)  -1}\dbinom
{v/b}{n-k}\in\mathbb{Z}$. In other words, the number $b^{2\left(  n-k\right)
-1}\dbinom{v/b}{n-k}$ is an integer.
\par
Now, the numbers $b^{2k-1}\dbinom{u/b}{k}$ and $b^{2\left(  n-k\right)
-1}\dbinom{v/b}{n-k}$ are integers. Hence, their product is an integer as
well. In other words, the number $b^{2k-1}\dbinom{u/b}{k}\cdot b^{2\left(
n-k\right)  -1}\dbinom{v/b}{n-k}$ is an integer. Since%
\begin{align*}
&  b^{2k-1}\dbinom{u/b}{k}\cdot b^{2\left(  n-k\right)  -1}\dbinom{v/b}{n-k}\\
&  =\underbrace{b^{2k-1}b^{2\left(  n-k\right)  -1}}_{\substack{=b^{\left(
2k-1\right)  +\left(  2\left(  n-k\right)  -1\right)  }=b^{2n-2}\\\text{(since
}\left(  2k-1\right)  +\left(  2\left(  n-k\right)  -1\right)  =2n-2\text{)}%
}}\dbinom{u/b}{k}\dbinom{v/b}{n-k}=b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k},
\end{align*}
this rewrites as follows: The number $b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}%
{n-k}$ is an integer. Qed.}. Hence, $\sum_{k=1}^{n-1}b^{2n-2}\dbinom{u/b}%
{k}\dbinom{v/b}{n-k}$ is a sum of $n-1$ integers, and thus itself is an
integer. In other words,%
\begin{equation}
\sum_{k=1}^{n-1}b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\in\mathbb{Z}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.a.5}%
\end{equation}


Now,%
\begin{align*}
&  b^{2n-2}\left(  \underbrace{\dbinom{\left(  u+v\right)  /b}{n}%
}_{\substack{=\dbinom{u/b+v/b}{n}\\\text{(since }\left(  u+v\right)
/b=u/b+v/b\text{)}}}-\dbinom{u/b}{n}-\dbinom{v/b}{n}\right) \\
&  =b^{2n-2}\underbrace{\left(  \dbinom{u/b+v/b}{n}-\dbinom{u/b}{n}%
-\dbinom{v/b}{n}\right)  }_{\substack{=\sum_{k=1}^{n-1}\dbinom{u/b}{k}%
\dbinom{v/b}{n-k}\\\text{(by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.a.1}))}}}\\
&  =b^{2n-2}\sum_{k=1}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}=\sum_{k=1}%
^{n-1}b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\in\mathbb{Z}%
\end{align*}
(by (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.a.5})). This proves
Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(a)}.

\textbf{(b)} Let $a\in\mathbb{Z}$ and $h\in\mathbb{N}$. For every
$k\in\left\{  0,1,\ldots,h-1\right\}  $, the number \newline$b^{2n-2}\left(
\dbinom{\left(  k+1\right)  a/b}{n}-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)  $
is an integer\footnote{\textit{Proof:} Let $k\in\left\{  0,1,\ldots
,h-1\right\}  $. Then, Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem}
\textbf{(a)} (applied to $u=ka$ and $v=a$) yields%
\[
b^{2n-2}\left(  \dbinom{\left(  ka+a\right)  /b}{n}-\dbinom{ka/b}{n}%
-\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]
Since $ka+a=\left(  k+1\right)  a$, this rewrites as follows:%
\[
b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom{ka/b}{n}%
-\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]
Qed.}. Hence,
\[
\sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}%
-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)
\]
is a sum of $h$ integers, and thus itself is an integer. In other words,%
\begin{equation}
\sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}%
-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.b.3}%
\end{equation}


But $1-1=0\leq h$. Hence, (\ref{eq.sum.telescope}) (applied to $\mathbb{A}%
=\mathbb{Q}$, $u=1$, $v=h$ and $a_{s}=\dbinom{sa/b}{n}$) yields%
\begin{align*}
\sum_{s=1}^{h}\left(  \dbinom{sa/b}{n}-\dbinom{\left(  s-1\right)  a/b}%
{n}\right)   &  =\dbinom{ha/b}{n}-\underbrace{\dbinom{\left(  1-1\right)
a/b}{n}}_{\substack{=\dbinom{0}{n}\\\text{(since }\left(  1-1\right)
a/b=0\text{)}}}\\
&  =\dbinom{ha/b}{n}-\underbrace{\dbinom{0}{n}}_{\substack{=0\\\text{(by
Proposition \ref{prop.binom.0},}\\\text{applied to }m=0\text{)}}%
}=\dbinom{ha/b}{n}.
\end{align*}
Hence,%
\begin{align}
\dbinom{ha/b}{n}  &  =\sum_{s=1}^{h}\left(  \dbinom{sa/b}{n}-\dbinom{\left(
s-1\right)  a/b}{n}\right) \nonumber\\
&  =\sum_{k=0}^{h-1}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom
{ka/b}{n}\right)  \label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.b.6}%
\end{align}
(here, we have substituted $k+1$ for $s$ in the sum). Now,%
\begin{align}
&  \sum_{k=0}^{h-1}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom
{ka/b}{n}-\dbinom{a/b}{n}\right) \nonumber\\
&  =\underbrace{\sum_{k=0}^{h-1}\left(  \dbinom{\left(  k+1\right)  a/b}%
{n}-\dbinom{ka/b}{n}\right)  }_{\substack{=\dbinom{ha/b}{n}\\\text{(by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.b.6}))}}}-\underbrace{\sum
_{k=0}^{h-1}\dbinom{a/b}{n}}_{=h\dbinom{a/b}{n}}\nonumber\\
&  =\dbinom{ha/b}{n}-h\dbinom{a/b}{n}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.b.7}%
\end{align}


Now,
\begin{align*}
&  \sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}%
-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right) \\
&  =b^{2n-2}\underbrace{\sum_{k=0}^{h-1}\left(  \dbinom{\left(  k+1\right)
a/b}{n}-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)  }_{\substack{=\dbinom
{ha/b}{n}-h\dbinom{a/b}{n}\\\text{(by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.b.7}))}}}\\
&  =b^{2n-2}\left(  \dbinom{ha/b}{n}-h\dbinom{a/b}{n}\right)  .
\end{align*}
Thus,%
\begin{align*}
&  b^{2n-2}\left(  \dbinom{ha/b}{n}-h\dbinom{a/b}{n}\right) \\
&  =\sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}%
{n}-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right) \\
&  \in\mathbb{Z}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.b.3})}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(b)}.

\textbf{(c)} Let $a\in\mathbb{Z}$. Proposition \ref{prop.binom.int} (applied
to $m=a$) yields $\dbinom{a}{n}\in\mathbb{Z}$. In other words, $\dbinom{a}{n}$
is an integer.

Also, $2n-2\in\mathbb{N}$ (since $n$ is a positive integer). Thus, $b^{2n-2}$
is an integer (since $b$ is an integer). Now, the numbers $b^{2n-2}$ and
$\dbinom{a}{n}$ are both integers. Hence, their product must also be an
integer. In other words, $b^{2n-2}\dbinom{a}{n}$ is an integer.

But Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(b)} (applied to
$h=b$) yields%
\[
b^{2n-2}\left(  \dbinom{ba/b}{n}-b\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]
In other words, $b^{2n-2}\left(  \dbinom{ba/b}{n}-b\dbinom{a/b}{n}\right)  $
is an integer. Denote this integer by $z$. Thus,%
\begin{align*}
z  &  =b^{2n-2}\left(  \dbinom{ba/b}{n}-b\dbinom{a/b}{n}\right)
=b^{2n-2}\underbrace{\dbinom{ba/b}{n}}_{=\dbinom{a}{n}}-\underbrace{b^{2n-2}%
b}_{=b^{\left(  2n-2\right)  +1}=b^{2n-1}}\dbinom{a/b}{n}\\
&  =b^{2n-2}\dbinom{a}{n}-b^{2n-1}\dbinom{a/b}{n}.
\end{align*}
Solving this equality for $b^{2n-1}\dbinom{a/b}{n}$, we obtain
\begin{equation}
b^{2n-1}\dbinom{a/b}{n}=b^{2n-2}\dbinom{a}{n}-z.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.c.3}%
\end{equation}


But the numbers $b^{2n-2}\dbinom{a}{n}$ and $z$ are integers. Hence, their
difference is also an integer. In other words, $b^{2n-2}\dbinom{a}{n}-z$ is an
integer. In other words, $b^{2n-2}\dbinom{a}{n}-z\in\mathbb{Z}$. Hence,
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.short.c.3}) becomes $b^{2n-1}%
\dbinom{a/b}{n}=b^{2n-2}\dbinom{a}{n}-z\in\mathbb{Z}$. This proves Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(c)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem}.]The integer $n$ is
positive. Hence, $n-1\in\mathbb{N}$, so that $0\in\left\{  0,1,\ldots
,n-1\right\}  $. Also, $n\in\mathbb{N}$ (since $n$ is a positive integer), so
that $n\in\left\{  0,1,\ldots,n\right\}  $.

\textbf{(a)} Let $u\in\mathbb{Z}$ and $v\in\mathbb{Z}$. Proposition
\ref{prop.binom.00} \textbf{(a)} yields $\dbinom{X}{0}=1$ (an identity between
two polynomials in $X$). Substituting $u/b$ for $X$ in this identity, we
obtain $\dbinom{u/b}{0}=1$. Substituting $v/b$ for $X$ in the identity
$\dbinom{X}{0}=1$, we obtain $\dbinom{v/b}{0}=1$.

Theorem \ref{thm.vandermonde} yields%
\[
\dbinom{X+Y}{n}=\sum_{k=0}^{n}\dbinom{X}{k}\dbinom{Y}{n-k}%
\]
(an identity between polynomials in $X$ and $Y$). Substituting $u/b$ and $v/b$
for $X$ and $Y$ in this identity, we obtain%
\begin{align*}
\dbinom{u/b+v/b}{n}  &  =\sum_{k=0}^{n}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\\
&  =\sum_{k=0}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}+\dbinom{u/b}%
{n}\underbrace{\dbinom{v/b}{n-n}}_{=\dbinom{v/b}{0}=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=n\text{ from}\\
\text{the sum (since }n\in\left\{  0,1,\ldots,n\right\}  \text{)}%
\end{array}
\right) \\
&  =\sum_{k=0}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}+\dbinom{u/b}{n}.
\end{align*}
Subtracting $\dbinom{u/b}{n}$ from this equality, we obtain%
\begin{align*}
&  \dbinom{u/b+v/b}{n}-\dbinom{u/b}{n}\\
&  =\sum_{k=0}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\\
&  =\underbrace{\dbinom{u/b}{0}}_{=1}\underbrace{\dbinom{v/b}{n-0}}%
_{=\dbinom{v/b}{n}}+\sum_{k=1}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\\
&  =\dbinom{v/b}{n}+\sum_{k=1}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}.
\end{align*}
Subtracting $\dbinom{v/b}{n}$ from this equality, we obtain%
\begin{equation}
\dbinom{u/b+v/b}{n}-\dbinom{u/b}{n}-\dbinom{v/b}{n}=\sum_{k=1}^{n-1}%
\dbinom{u/b}{k}\dbinom{v/b}{n-k}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.1}%
\end{equation}


But for every $k\in\left\{  1,2,\ldots,n-1\right\}  $, we have%
\begin{equation}
b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\in\mathbb{Z}
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.3}):}
Let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
\par
From (\ref{eq.lem.sol.exeadd.choose.a/b.sol2.lem.ass}) (applied to $c=u$), we
obtain $b^{2k-1}\dbinom{u/b}{k}\in\mathbb{Z}$. In other words, the number
$b^{2k-1}\dbinom{u/b}{k}$ is an integer.
\par
But we also have $n-k\in\left\{  1,2,\ldots,n-1\right\}  $ (since
$k\in\left\{  1,2,\ldots,n-1\right\}  $). Thus, from
(\ref{eq.lem.sol.exeadd.choose.a/b.sol2.lem.ass}) (applied to $n-k$ and $v$
instead of $k$ and $c$), we obtain $b^{2\left(  n-k\right)  -1}\dbinom
{v/b}{n-k}\in\mathbb{Z}$. In other words, the number $b^{2\left(  n-k\right)
-1}\dbinom{v/b}{n-k}$ is an integer.
\par
Now, the numbers $b^{2k-1}\dbinom{u/b}{k}$ and $b^{2\left(  n-k\right)
-1}\dbinom{v/b}{n-k}$ are integers. Hence, their product is an integer as
well. In other words, $b^{2k-1}\dbinom{u/b}{k}\cdot b^{2\left(  n-k\right)
-1}\dbinom{v/b}{n-k}$ is an integer. In other words, $b^{2k-1}\dbinom{u/b}%
{k}\cdot b^{2\left(  n-k\right)  -1}\dbinom{v/b}{n-k}\in\mathbb{Z}$. Since%
\begin{align*}
&  b^{2k-1}\dbinom{u/b}{k}\cdot b^{2\left(  n-k\right)  -1}\dbinom{v/b}{n-k}\\
&  =\underbrace{b^{2k-1}b^{2\left(  n-k\right)  -1}}_{\substack{=b^{\left(
2k-1\right)  +\left(  2\left(  n-k\right)  -1\right)  }=b^{2n-2}\\\text{(since
}\left(  2k-1\right)  +\left(  2\left(  n-k\right)  -1\right)  =2n-2\text{)}%
}}\dbinom{u/b}{k}\dbinom{v/b}{n-k}=b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k},
\end{align*}
this rewrites as $b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\in\mathbb{Z}$.
Thus, (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.3}) is proven.}. In other
words, for every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the number
$b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}$ is an integer. Hence, $\sum
_{k=1}^{n-1}b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}$ is a sum of $n-1$
integers, and thus itself is an integer. In other words,%
\begin{equation}
\sum_{k=1}^{n-1}b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\in\mathbb{Z}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.5}%
\end{equation}


Now,%
\begin{align*}
&  b^{2n-2}\left(  \underbrace{\dbinom{\left(  u+v\right)  /b}{n}%
}_{\substack{=\dbinom{u/b+v/b}{n}\\\text{(since }\left(  u+v\right)
/b=u/b+v/b\text{)}}}-\dbinom{u/b}{n}-\dbinom{v/b}{n}\right) \\
&  =b^{2n-2}\underbrace{\left(  \dbinom{u/b+v/b}{n}-\dbinom{u/b}{n}%
-\dbinom{v/b}{n}\right)  }_{\substack{=\sum_{k=1}^{n-1}\dbinom{u/b}{k}%
\dbinom{v/b}{n-k}\\\text{(by (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.1}%
))}}}\\
&  =b^{2n-2}\sum_{k=1}^{n-1}\dbinom{u/b}{k}\dbinom{v/b}{n-k}=\sum_{k=1}%
^{n-1}b^{2n-2}\dbinom{u/b}{k}\dbinom{v/b}{n-k}\in\mathbb{Z}%
\end{align*}
(by (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.a.5})). This proves Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(a)}.

\textbf{(b)} Let $a\in\mathbb{Z}$ and $h\in\mathbb{N}$. For every
$k\in\left\{  0,1,\ldots,h-1\right\}  $, we have%
\begin{equation}
b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom{ka/b}{n}%
-\dbinom{a/b}{n}\right)  \in\mathbb{Z}
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.1}):}
Let $k\in\left\{  0,1,\ldots,h-1\right\}  $. Then, Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(a)} (applied to $u=ka$ and
$v=a$) yields%
\[
b^{2n-2}\left(  \dbinom{\left(  ka+a\right)  /b}{n}-\dbinom{ka/b}{n}%
-\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]
Since $ka+a=\left(  k+1\right)  a$, this rewrites as follows:%
\[
b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom{ka/b}{n}%
-\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]
Thus, (\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.1}) is proven.}. In other
words, for every $k\in\left\{  0,1,\ldots,h-1\right\}  $, the number
\newline$b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom{ka/b}%
{n}-\dbinom{a/b}{n}\right)  $ is an integer. Hence,
\[
\sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}%
-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)
\]
is a sum of $h$ integers, and thus itself is an integer. In other words,%
\begin{equation}
\sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}%
-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.3}%
\end{equation}


But $h\in\mathbb{N}$ and thus $h\in\left\{  0,1,\ldots,h\right\}  $. Hence,
\begin{align*}
\sum_{k=0}^{h}\dbinom{ka/b}{n}  &  =\sum_{k=0}^{h-1}\dbinom{ka/b}{n}%
+\dbinom{ha/b}{n}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=h\\
\text{from the sum (since }h\in\left\{  0,1,\ldots,h\right\}  \text{)}%
\end{array}
\right)  .
\end{align*}
Subtracting $\dbinom{ha/b}{n}$ from this equality, we obtain%
\begin{equation}
\sum_{k=0}^{h}\dbinom{ka/b}{n}-\dbinom{ha/b}{n}=\sum_{k=0}^{h-1}\dbinom
{ka/b}{n}. \label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.5a}%
\end{equation}


On the other hand, $n$ is a positive integer. Hence, $0<n$. Thus, Proposition
\ref{prop.binom.0} (applied to $m=0$) yields $\dbinom{0}{n}=0$.

But $h\in\mathbb{N}$ and thus $0\in\left\{  0,1,\ldots,h\right\}  $. Hence,
\begin{align}
\sum_{k=0}^{h}\dbinom{ka/b}{n}  &  =\underbrace{\dbinom{0a/b}{n}%
}_{\substack{=\dbinom{0}{n}\\\text{(since }0a/b=0\text{)}}}+\sum_{k=1}%
^{h}\dbinom{ka/b}{n}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=0\\
\text{from the sum (since }0\in\left\{  0,1,\ldots,h\right\}  \text{)}%
\end{array}
\right) \nonumber\\
&  =\underbrace{\dbinom{0}{n}}_{=0}+\sum_{k=1}^{h}\dbinom{ka/b}{n}=\sum
_{k=1}^{h}\dbinom{ka/b}{n}\nonumber\\
&  =\sum_{k=0}^{h-1}\dbinom{\left(  k+1\right)  a/b}{n}
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.5b}%
\end{align}
(here, we have substituted $k+1$ for $k$ in the sum).

But%
\begin{align}
&  \sum_{k=0}^{h-1}\left(  \dbinom{\left(  k+1\right)  a/b}{n}-\dbinom
{ka/b}{n}-\dbinom{a/b}{n}\right) \nonumber\\
&  =\underbrace{\sum_{k=0}^{h-1}\dbinom{\left(  k+1\right)  a/b}{n}%
}_{\substack{=\sum_{k=0}^{h}\dbinom{ka/b}{n}\\\text{(by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.5b}))}}}-\underbrace{\sum
_{k=0}^{h-1}\dbinom{ka/b}{n}}_{\substack{=\sum_{k=0}^{h}\dbinom{ka/b}%
{n}-\dbinom{ha/b}{n}\\\text{(by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.5a}))}}}-\underbrace{\sum
_{k=0}^{h-1}\dbinom{a/b}{n}}_{=h\dbinom{a/b}{n}}\nonumber\\
&  =\sum_{k=0}^{h}\dbinom{ka/b}{n}-\left(  \sum_{k=0}^{h}\dbinom{ka/b}%
{n}-\dbinom{ha/b}{n}\right)  -h\dbinom{a/b}{n}\nonumber\\
&  =\dbinom{ha/b}{n}-h\dbinom{a/b}{n}.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.7}%
\end{align}


Now,
\begin{align*}
&  \sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}{n}%
-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right) \\
&  =b^{2n-2}\underbrace{\sum_{k=0}^{h-1}\left(  \dbinom{\left(  k+1\right)
a/b}{n}-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right)  }_{\substack{=\dbinom
{ha/b}{n}-h\dbinom{a/b}{n}\\\text{(by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.7}))}}}\\
&  =b^{2n-2}\left(  \dbinom{ha/b}{n}-h\dbinom{a/b}{n}\right)  .
\end{align*}
Thus,%
\begin{align*}
&  b^{2n-2}\left(  \dbinom{ha/b}{n}-h\dbinom{a/b}{n}\right) \\
&  =\sum_{k=0}^{h-1}b^{2n-2}\left(  \dbinom{\left(  k+1\right)  a/b}%
{n}-\dbinom{ka/b}{n}-\dbinom{a/b}{n}\right) \\
&  \in\mathbb{Z}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.b.3})}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(b)}.

\textbf{(c)} Let $a\in\mathbb{Z}$. Proposition \ref{prop.binom.int} (applied
to $m=a$) yields $\dbinom{a}{n}\in\mathbb{Z}$. In other words, $\dbinom{a}{n}$
is an integer.

Also, $n$ is a positive integer; thus, $n\geq1$, so that $n-1\in\mathbb{N}$
and thus $2\left(  n-1\right)  \in\mathbb{N}$. Hence, $2n-2=2\left(
n-1\right)  \in\mathbb{N}$. Thus, $b^{2n-2}$ is an integer (since $b$ is an
integer). Now, the numbers $b^{2n-2}$ and $\dbinom{a}{n}$ are both integers.
Hence, their product must also be an integer. In other words, $b^{2n-2}%
\dbinom{a}{n}$ is an integer.

But Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(b)} (applied to
$h=b$) yields%
\[
b^{2n-2}\left(  \dbinom{ba/b}{n}-b\dbinom{a/b}{n}\right)  \in\mathbb{Z}.
\]
In other words, $b^{2n-2}\left(  \dbinom{ba/b}{n}-b\dbinom{a/b}{n}\right)  $
is an integer. Denote this integer by $z$. Thus,%
\begin{align*}
z  &  =b^{2n-2}\left(  \dbinom{ba/b}{n}-b\dbinom{a/b}{n}\right) \\
&  =b^{2n-2}\underbrace{\dbinom{ba/b}{n}}_{\substack{=\dbinom{a}%
{n}\\\text{(since }ba/b=a\text{)}}}-\underbrace{b^{2n-2}b}%
_{\substack{=b^{\left(  2n-2\right)  +1}=b^{2n-1}\\\text{(since }\left(
2n-2\right)  +1=2n-1\text{)}}}\dbinom{a/b}{n}\\
&  =b^{2n-2}\dbinom{a}{n}-b^{2n-1}\dbinom{a/b}{n}.
\end{align*}
Adding $b^{2n-1}\dbinom{a/b}{n}$ to both sides of this equality, we obtain
$b^{2n-1}\dbinom{a/b}{n}+z=b^{2n-2}\dbinom{a}{n}$. Subtracting $z$ from this
equality, we obtain
\begin{equation}
b^{2n-1}\dbinom{a/b}{n}=b^{2n-2}\dbinom{a}{n}-z.
\label{pf.lem.sol.exeadd.choose.a/b.sol2.lem.c.3}%
\end{equation}


But the numbers $b^{2n-2}\dbinom{a}{n}$ and $z$ are integers. Hence, their
difference is also an integer. In other words, $b^{2n-2}\dbinom{a}{n}-z$ is an
integer. In other words, $b^{2n-2}\dbinom{a}{n}-z\in\mathbb{Z}$. Hence,
(\ref{pf.lem.sol.exeadd.choose.a/b.sol2.lem.c.3}) becomes $b^{2n-1}%
\dbinom{a/b}{n}=b^{2n-2}\dbinom{a}{n}-z\in\mathbb{Z}$. This proves Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(c)}.
\end{proof}
\end{verlong}

Our next lemma is essentially Theorem
\ref{thm.sol.exeadd.choose.a/b.sol2.main}, restricted to the case when $b$ is positive:

\begin{lemma}
\label{lem.sol.exeadd.choose.a/b.sol2.pos}Let $a$ and $b$ be two integers such
that $b>0$. Let $n$ be a positive integer. Then, $b^{2n-1}\dbinom{a/b}{n}%
\in\mathbb{Z}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.pos}.]We shall prove Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.pos} by strong induction on $n$:

\textit{Induction step:} Let $N$ be a positive integer. Assume that Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.pos} holds in the case when $n<N$. We must
show that Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.pos} holds in the case
when $n=N$.

We have assumed that Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.pos} holds in
the case when $n<N$. In other words, the following statement holds:

\begin{statement}
\textit{Statement 1:} Let $a$ and $b$ be two integers such that $b>0$. Let $n$
be a positive integer such that $n<N$. Then, $b^{2n-1}\dbinom{a/b}{n}%
\in\mathbb{Z}$.
\end{statement}

Now, let us prove the following statement:

\begin{statement}
\textit{Statement 2:} Let $a$ and $b$ be two integers such that $b>0$. Then,
$b^{2N-1}\dbinom{a/b}{N}\in\mathbb{Z}$.
\end{statement}

\textit{Proof of Statement 2:} Every $k\in\left\{  1,2,\ldots,N-1\right\}  $
and $c\in\mathbb{Z}$ satisfy%
\[
b^{2k-1}\dbinom{c/b}{k}\in\mathbb{Z}%
\]
\footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots,N-1\right\}  $ and
$c\in\mathbb{Z}$. From $k\in\left\{  1,2,\ldots,N-1\right\}  $, we obtain
$1\leq k\leq N-1$. Now, $k$ is a positive integer (since $1\leq k$) and
satisfies $k<N$ (since $k\leq N-1<N$). Hence, Statement 1 (applied to $k$ and
$c$ instead of $n$ and $a$) yields $b^{2k-1}\dbinom{c/b}{k}\in\mathbb{Z}$.
Qed.}. Hence, Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.lem} \textbf{(c)}
(applied to $n=N$) yields $b^{2N-1}\dbinom{a/b}{N}\in\mathbb{Z}$. Thus,
Statement 2 is proven.

So we have proven Statement 2. In other words, we have proven that Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.pos} holds in the case when $n=N$. This
completes the induction step. Thus, the inductive proof of Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.pos} is complete.
\end{proof}

\begin{vershort}
\begin{proof}
[Proof of Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}.]We must prove
that $b^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}$. If $b>0$, then this follows
immediately from Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.pos}. Hence, for
the rest of this proof, we WLOG assume that we don't have $b>0$. Hence,
$b\leq0$, so that $b<0$ (since $b\neq0$). Therefore, $-b>0$. Thus, Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.lem} (applied to $-a$ and $-b$ instead of
$a$ and $b$) yields $\left(  -b\right)  ^{2n-1}\dbinom{\left(  -a\right)
/\left(  -b\right)  }{n}\in\mathbb{Z}$. Since $\left(  -a\right)  /\left(
-b\right)  =a/b$, this rewrites as $\left(  -b\right)  ^{2n-1}\dbinom{a/b}%
{n}\in\mathbb{Z}$. In other words, $\left(  -b\right)  ^{2n-1}\dbinom{a/b}{n}$
is an integer.

But%
\[
\underbrace{\left(  -b\right)  ^{2n-1}}_{=\left(  -1\right)  ^{2n-1}b^{2n-1}%
}\dbinom{a/b}{n}=\underbrace{\left(  -1\right)  ^{2n-1}}%
_{\substack{=-1\\\text{(since }2n-1\text{ is odd)}}}b^{2n-1}\dbinom{a/b}%
{n}=-b^{2n-1}\dbinom{a/b}{n}.
\]
In other words, the two numbers $\left(  -b\right)  ^{2n-1}\dbinom{a/b}{n}$
and $b^{2n-1}\dbinom{a/b}{n}$ differ only in sign. Since the first of them is
an integer, we thus conclude that so is the second. In other words,
$b^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}$. This proves Theorem
\ref{thm.sol.exeadd.choose.a/b.sol2.main}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}.]We must prove
that $b^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}$. If $b>0$, then this follows
immediately from Lemma \ref{lem.sol.exeadd.choose.a/b.sol2.pos}. Hence, for
the rest of this proof, we can WLOG assume that we don't have $b>0$. Assume this.

We have $b\leq0$ (since we don't have $b>0$). Combining this with $b\neq0$, we
obtain $b<0$. Hence, $-b>0$. Thus, Lemma
\ref{lem.sol.exeadd.choose.a/b.sol2.lem} (applied to $-a$ and $-b$ instead of
$a$ and $b$) yields $\left(  -b\right)  ^{2n-1}\dbinom{\left(  -a\right)
/\left(  -b\right)  }{n}\in\mathbb{Z}$. Since $\left(  -a\right)  /\left(
-b\right)  =a/b$, this rewrites as $\left(  -b\right)  ^{2n-1}\dbinom{a/b}%
{n}\in\mathbb{Z}$. In other words, $\left(  -b\right)  ^{2n-1}\dbinom{a/b}{n}$
is an integer.

Both numbers $\left(  -1\right)  ^{2n-1}$ and $\left(  -b\right)
^{2n-1}\dbinom{a/b}{n}$ are integers. Hence, their product is also an integer.
In other words, $\left(  -1\right)  ^{2n-1}\left(  -b\right)  ^{2n-1}%
\dbinom{a/b}{n}$ is an integer. In other words,
\[
\left(  -1\right)  ^{2n-1}\left(  -b\right)  ^{2n-1}\dbinom{a/b}{n}%
\in\mathbb{Z}.
\]


But $\left(  -1\right)  ^{2n-1}\left(  -b\right)  ^{2n-1}=\left(
\underbrace{\left(  -1\right)  \left(  -b\right)  }_{=b}\right)
^{2n-1}=b^{2n-1}$, so that \newline$b^{2n-1}=\left(  -1\right)  ^{2n-1}\left(
-b\right)  ^{2n-1}$. Multiplying this equality by $\dbinom{a/b}{n}$, we find%
\[
b^{2n-1}\dbinom{a/b}{n}=\left(  -1\right)  ^{2n-1}\left(  -b\right)
^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}.
\]
This proves Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}.
\end{proof}
\end{verlong}

Now, only some trivial bookkeeping remains to be done in order to solve
Additional exercise \ref{exeadd.choose.a/b}. To simplify it, we state the
following corollary from Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}:

\begin{corollary}
\label{cor.sol.exeadd.choose.a/b.sol2.main}Let $a$ and $b$ be two integers
such that $b\neq0$. Let $n\in\mathbb{N}$. Let $m=\max\left\{  0,2n-1\right\}
$. Then, $b^{m}\dbinom{a/b}{n}\in\mathbb{Z}$.
\end{corollary}

\begin{vershort}
\begin{proof}
[Proof of Corollary \ref{cor.sol.exeadd.choose.a/b.sol2.main}.]If $n=0$, then
Corollary \ref{cor.sol.exeadd.choose.a/b.sol2.main}
holds\footnote{\textit{Proof.} Assume that $n=0$. We must show that Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} holds.
\par
We have $m=\max\left\{  0,2n-1\right\}  =0$ (since $2\underbrace{n}%
_{=0}-1=0-1<0$). Hence, $b^{m}=b^{0}=1$ and thus
\[
\underbrace{b^{m}}_{=1}\dbinom{a/b}{n}=\dbinom{a/b}{n}=\dbinom{a/b}%
{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=0\right)  .
\]
\par
But substituting $a/b$ for $X$ in Proposition \ref{prop.binom.00}
\textbf{(a)}, we find $\dbinom{a/b}{0}=1$. Thus, $b^{m}\dbinom{a/b}{n}%
=\dbinom{a/b}{0}=1\in\mathbb{Z}$. Thus, Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} holds.}. Hence, for the rest of this
proof, we WLOG assume that $n\neq0$. Therefore, $n$ is a positive integer
(since $n\in\mathbb{N}$). Therefore, $2n-1>0$, so that $\max\left\{
0,2n-1\right\}  =2n-1$. Now, $m=\max\left\{  0,2n-1\right\}  =2n-1$, so that
$b^{m}=b^{2n-1}$. Multiplying this equality by $\dbinom{a/b}{n}$, we find%
\[
b^{m}\dbinom{a/b}{n}=b^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}%
\]
(by Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}). Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} is thus proven.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Corollary \ref{cor.sol.exeadd.choose.a/b.sol2.main}.]If $n=0$, then
Corollary \ref{cor.sol.exeadd.choose.a/b.sol2.main}
holds\footnote{\textit{Proof.} Assume that $n=0$. We must show that Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} holds.
\par
We have $2\underbrace{n}_{=0}-1=0-1=-1$ and $m=\max\left\{
0,\underbrace{2n-1}_{=-1}\right\}  =\max\left\{  0,-1\right\}  =0$. Hence,
$b^{m}=b^{0}=1$ and thus $\underbrace{b^{m}}_{=1}\dbinom{a/b}{n}=\dbinom
{a/b}{n}=\dbinom{a/b}{0}$ (since $n=0$).
\par
But Proposition \ref{prop.binom.00} \textbf{(a)} yields $\dbinom{X}{0}=1$ (an
identity between two polynomials in $X$). Substituting $a/b$ for $X$ in this
identity, we obtain $\dbinom{a/b}{0}=1$. Thus, $b^{m}\dbinom{a/b}{n}%
=\dbinom{a/b}{0}=1\in\mathbb{Z}$. Thus, Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} holds. Qed.}. Hence, for the rest of
this proof, we can WLOG assume that we don't have $n=0$. Assume this.

We don't have $n=0$. Hence, we have $n\neq0$. Thus, $n$ is a positive integer
(since $n\in\mathbb{N}$). Therefore, $n\geq1$, so that $2\underbrace{n}%
_{\geq1}-1\geq2\cdot1-1=1>0$. Thus, $\max\left\{  0,2n-1\right\}  =2n-1$. Now,
$m=\max\left\{  0,2n-1\right\}  =2n-1$, so that $b^{m}=b^{2n-1}$. Multiplying
this equality by $\dbinom{a/b}{n}$, we find%
\[
b^{m}\dbinom{a/b}{n}=b^{2n-1}\dbinom{a/b}{n}\in\mathbb{Z}%
\]
(by Theorem \ref{thm.sol.exeadd.choose.a/b.sol2.main}). Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} is thus proven.
\end{proof}
\end{verlong}

\begin{proof}
[Second solution to Additional exercise \ref{exeadd.choose.a/b}.]Let
$m=\max\left\{  0,2n-1\right\}  $. Then, \newline$m=\max\left\{
0,2n-1\right\}  \geq0$, so that $m\in\mathbb{N}$. Also, Corollary
\ref{cor.sol.exeadd.choose.a/b.sol2.main} shows that $b^{m}\dbinom{a/b}{n}%
\in\mathbb{Z}$. Hence, there exists some $N\in\mathbb{N}$ such that
$b^{N}\dbinom{a/b}{n}\in\mathbb{Z}$ (namely, $N=m$). This solves Additional
exercise \ref{exeadd.choose.a/b}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.1}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.1}.]We claim that every $n\in\mathbb{N}$
satisfies%
\begin{equation}
x_{n}=\dfrac{1}{2^{n}}\left(  2na^{n-1}x_{1}-\left(  n-1\right)  a^{n}%
x_{0}\right)  \label{sol.ps2.2.1.claim}%
\end{equation}
(where $na^{n-1}$ is to be understood as $0$ when $n=0$\ \ \ \ \footnote{This
needs to be said, because $a^{n-1}$ alone can be undefined for $n=0$ (if
$a=0$).}).

\textit{Proof of (\ref{sol.ps2.2.1.claim}):} We shall prove
(\ref{sol.ps2.2.1.claim}) by strong induction on $n$. So we fix some
$N\in\mathbb{N}$, and we assume that (\ref{sol.ps2.2.1.claim}) is already
proven for every $n<N$. (This is our induction hypothesis.) We now need to
show that (\ref{sol.ps2.2.1.claim}) holds for $n=N$ as well.\footnote{If you
are wondering \textquotedblleft where is the induction base?\textquotedblright%
: It isn't missing. A strong induction needs no induction base. Strong
induction lets you prove that some statement $\mathcal{A}_{n}$ holds for every
$n\in\mathbb{N}$ by means of proving that for every $N\in\mathbb{N}$,%
\begin{equation}
\text{if }\mathcal{A}_{n}\text{ holds for every }n<N\text{, then }%
\mathcal{A}_{N}\text{ holds.} \label{sol.ps2.2.1.strind}%
\end{equation}
This immediately shows that $\mathcal{A}_{0}$ holds: Namely, it is clear that
$\mathcal{A}_{n}$ holds for every $n<0$ (because there exists no $n<0$), and
thus (\ref{sol.ps2.2.1.strind}) (applied to $N=0$) shows that $\mathcal{A}%
_{0}$ holds.
\par
Of course, the proof of (\ref{sol.ps2.2.1.strind}) might involve some case
analysis; in particular, it might argue differently depending on whether $N=0$
or $N\geq1$. (Indeed, our proof will be something like this: it will treat the
cases $N=0$, $N=1$ and $N\geq2$ separately.) So there can be a
\textquotedblleft de-facto induction base\textquotedblright\ (or two, or many)
hidden in the proof of (\ref{sol.ps2.2.1.strind}).} In other words, we need to
prove that
\begin{equation}
x_{N}=\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(  N-1\right)  a^{N}%
x_{0}\right)  . \label{sol.ps2.2.1.goal}%
\end{equation}


We must be in one of the following three cases:

\textit{Case 1:} We have $N=0$.

\textit{Case 2:} We have $N=1$.

\textit{Case 3:} We have $N\geq2$.

Let us first consider Case 1. In this case, we have $N=0$. Hence,%
\begin{align*}
\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(  N-1\right)  a^{N}x_{0}\right)
&  =\underbrace{\dfrac{1}{2^{0}}}_{=1}\left(  \underbrace{2\cdot0a^{0-1}x_{1}%
}_{=0}-\underbrace{\left(  0-1\right)  a^{0}}_{=-1}x_{0}\right) \\
&  =1\left(  0-\left(  -1\right)  x_{0}\right)  =1x_{0}=x_{0}\\
&  =x_{N}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0=N\right)  .
\end{align*}
In other words, $x_{N}=\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(
N-1\right)  a^{N}x_{0}\right)  $. Hence, (\ref{sol.ps2.2.1.goal}) is proven in
Case 1.

The proof of (\ref{sol.ps2.2.1.goal}) in Case 2 is similarly straightforward,
and is left to the reader.

Let us now consider Case 3. In this case, we have $N\geq2$. Hence, both $N-1$
and $N-2$ are nonnegative integers. Moreover, $N-1<N$, so that
(\ref{sol.ps2.2.1.claim}) is already proven for $n=N-1$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{N-1}=\dfrac{1}{2^{N-1}}\left(  2\left(  N-1\right)  a^{N-2}x_{1}-\left(
N-2\right)  a^{N-1}x_{0}\right)  . \label{sol.ps2.2.1.hyp1}%
\end{equation}
Also, $N-2$ is a nonnegative integer such that $N-2<N$. Hence,
(\ref{sol.ps2.2.1.claim}) is already proven for $n=N-2$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{N-2}=\dfrac{1}{2^{N-2}}\left(  2\left(  N-2\right)  a^{N-3}x_{1}-\left(
N-3\right)  a^{N-2}x_{0}\right)  . \label{sol.ps2.2.1.hyp2}%
\end{equation}


Now, recall that $a^{2}+4b=0$, so that $4b=-a^{2}$. Hence, $4b\cdot\left(
N-2\right)  a^{N-3}=\left(  -a^{2}\right)  \cdot\left(  N-2\right)
a^{N-3}=-\left(  N-2\right)  a^{N-1}$. (Don't forget to check that this latter
equality holds also when $N-2=0$; keep in mind that $\left(  N-2\right)
a^{N-3}$ was defined to be $0$ in this case, although $a^{N-3}$ might be
undefined.) From $4b=-a^{2}$, we also deduce $b=-\dfrac{a^{2}}{4}$, so that
$b\left(  N-3\right)  a^{N-2}=\dfrac{-a^{2}}{4}\left(  N-3\right)
a^{N-2}=-\dfrac{1}{4}\left(  N-3\right)  a^{N}$.

But the sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent. Hence,%
\begin{align*}
x_{N}  &  =ax_{N-1}+bx_{N-2}\\
&  =a\cdot\dfrac{1}{2^{N-1}}\left(  2\left(  N-1\right)  a^{N-2}x_{1}-\left(
N-2\right)  a^{N-1}x_{0}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +b\cdot\dfrac{1}{2^{N-2}}\left(  2\left(  N-2\right)
a^{N-3}x_{1}-\left(  N-3\right)  a^{N-2}x_{0}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.1.hyp1}) and
(\ref{sol.ps2.2.1.hyp2})}\right) \\
&  =a\cdot\dfrac{1}{2^{N-1}}2\left(  N-1\right)  a^{N-2}x_{1}-a\cdot\dfrac
{1}{2^{N-1}}\left(  N-2\right)  a^{N-1}x_{0}\\
&  \ \ \ \ \ \ \ \ \ \ +b\cdot\dfrac{1}{2^{N-2}}2\left(  N-2\right)
a^{N-3}x_{1}-b\cdot\dfrac{1}{2^{N-2}}\left(  N-3\right)  a^{N-2}x_{0}\\
&  =\underbrace{\left(  a\cdot\dfrac{1}{2^{N-1}}2\left(  N-1\right)
a^{N-2}+b\cdot\dfrac{1}{2^{N-2}}2\left(  N-2\right)  a^{N-3}\right)
}_{=\dfrac{1}{2^{N-1}}\left(  a\cdot2\left(  N-1\right)  a^{N-2}%
+4b\cdot\left(  N-2\right)  a^{N-3}\right)  }x_{1}\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  a\cdot\dfrac{1}{2^{N-1}}\left(
N-2\right)  a^{N-1}+b\cdot\dfrac{1}{2^{N-2}}\left(  N-3\right)  a^{N-2}%
\right)  }_{=\dfrac{1}{2^{N-1}}\left(  a\cdot\left(  N-2\right)
a^{N-1}+2b\left(  N-3\right)  a^{N-2}\right)  }x_{0}\\
&  =\dfrac{1}{2^{N-1}}\left(  \underbrace{a\cdot2\left(  N-1\right)  a^{N-2}%
}_{=2\left(  N-1\right)  a^{N-1}}+\underbrace{4b\cdot\left(  N-2\right)
a^{N-3}}_{=-\left(  N-2\right)  a^{N-1}}\right)  x_{1}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2^{N-1}}\left(  \underbrace{a\cdot\left(
N-2\right)  a^{N-1}}_{=\left(  N-2\right)  a^{N}}+2\underbrace{b\left(
N-3\right)  a^{N-2}}_{=-\dfrac{1}{4}\left(  N-3\right)  a^{N}}\right)  x_{0}\\
&  =\dfrac{1}{2^{N-1}}\underbrace{\left(  2\left(  N-1\right)  a^{N-1}+\left(
-\left(  N-2\right)  a^{N-1}\right)  \right)  }_{=Na^{N-1}}x_{1}\\
&  \ \ \ \ \ \ \ \ \ \ -\dfrac{1}{2^{N-1}}\underbrace{\left(  \left(
N-2\right)  a^{N}+2\left(  -\dfrac{1}{4}\left(  N-3\right)  \right)
a^{N}\right)  }_{=\dfrac{1}{2}\left(  N-1\right)  a^{N}}x_{0}\\
&  =\dfrac{1}{2^{N-1}}Na^{N-1}x_{1}-\dfrac{1}{2^{N-1}}\cdot\dfrac{1}{2}\left(
N-1\right)  a^{N}x_{0}=\dfrac{1}{2^{N-1}}\left(  Na^{N-1}x_{1}-\dfrac{1}%
{2}\left(  N-1\right)  a^{N}x_{0}\right) \\
&  =\dfrac{1}{2^{N}}\left(  2Na^{N-1}x_{1}-\left(  N-1\right)  a^{N}%
x_{0}\right)  .
\end{align*}
In other words, (\ref{sol.ps2.2.1.goal}) is proven in Case 3.

Thus we have seen that (\ref{sol.ps2.2.1.goal}) holds in each of the three
Cases 1, 2 and 3. Since these three cases cover all possibilities, this
finishes the proof of (\ref{sol.ps2.2.1.goal}). Hence, we have finished our
proof of (\ref{sol.ps2.2.1.claim}) by strong induction.
\end{proof}

\begin{remark}
Proving (\ref{sol.ps2.2.1.claim}) by strong induction is a completely
straightforward task. The main difficulty of the exercise is finding this
identity. Linear algebra (specifically, the theory of the Jordan normal form)
gives a \textquotedblleft conceptual\textquotedblright\ way to derive it, but
it can also be experimentally found by computing $x_{2},x_{3},x_{4}%
,x_{5},x_{6}$ directly (using $a^{2}+4b=0$ to rewrite $b$ as $-\dfrac{a^{2}%
}{4}$, so that only the variable $a$ appears in the expressions) and guessing
the pattern.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps2.2.2}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.2}.]We shall only solve part
\textbf{(c)}, since the other two parts are its particular cases (for $N=2$
and for $N=3$, respectively).

\textbf{(c)} We define a new sequence $\left(  c_{0},c_{1},c_{2}%
,\ldots\right)  $ recursively by
\begin{align*}
c_{0}  &  =2,\\
c_{1}  &  =a,\ \ \ \ \ \ \ \ \ \ \text{and}\\
c_{n}  &  =ac_{n-1}+bc_{n-2}\ \ \ \ \ \ \ \ \ \ \text{for all }n\geq2.
\end{align*}
(So this sequence $\left(  c_{0},c_{1},c_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent. Its first values are $c_{0}=2$, $c_{1}=a$,
$c_{2}=a^{2}+2b$, $c_{3}=a\left(  a^{2}+3b\right)  $ and $c_{4}=a^{4}%
+4a^{2}b+2b^{2}$. We notice that this sequence depends only on $a$ and $b$.)

We now claim that every $N\in\mathbb{N}$ and $m\in\mathbb{N}$ satisfy%
\begin{equation}
x_{m+2N}=c_{N}x_{m+N}+\left(  -1\right)  ^{N-1}b^{N}x_{m}.
\label{sol.ps2.2.2.c.claim}%
\end{equation}
Once this is proven, we will be done: In fact, (\ref{sol.ps2.2.2.c.claim})
shows that, for every nonnegative integers $N$ and $K$, the sequence $\left(
x_{K},x_{N+K},x_{2N+K},x_{3N+K},\ldots\right)  $ is $\left(  c_{N},\left(
-1\right)  ^{N-1}b^{N}\right)  $-recurrent\footnote{\textit{Proof.} Assume
that we have already proven (\ref{sol.ps2.2.2.c.claim}). Now, for every
nonnegative integers $N$ and $K$, for every $u\geq2$, we have%
\begin{align*}
x_{uN+K}  &  =x_{\left(  u-2\right)  N+K+2N}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }uN+K=\left(  u-2\right)  N+K+2N\right) \\
&  =c_{N}\underbrace{x_{\left(  u-2\right)  N+K+N}}_{=x_{\left(  u-1\right)
N+K}}+\left(  -1\right)  ^{N-1}b^{N}x_{\left(  u-2\right)  N+K}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.2.c.claim}), applied to
}m=\left(  u-2\right)  N+K\right) \\
&  =c_{N}x_{\left(  u-1\right)  N+K}+\left(  -1\right)  ^{N-1}b^{N}x_{\left(
u-2\right)  N+K}.
\end{align*}
In other words, for every nonnegative integers $N$ and $K$, the sequence
$\left(  x_{K},x_{N+K},x_{2N+K},x_{3N+K},\ldots\right)  $ is $\left(
c_{N},\left(  -1\right)  ^{N-1}b^{N}\right)  $-recurrent. Qed.}. Thus, in
order to solve Exercise \ref{exe.ps2.2.2} \textbf{(c)}, we only need to prove
(\ref{sol.ps2.2.2.c.claim}).

\textit{Proof of (\ref{sol.ps2.2.2.c.claim}):} We shall prove
(\ref{sol.ps2.2.2.c.claim}) by strong induction over $N$. Thus, we fix some
$n\in\mathbb{N}$, and we assume (as our induction hypothesis) that
(\ref{sol.ps2.2.2.c.claim}) holds for every $N<n$ (and, of course, every
$m\in\mathbb{N}$). We need to prove that (\ref{sol.ps2.2.2.c.claim}) holds for
$N=n$ (and every $m\in\mathbb{N}$). In other words, we need to prove that%
\begin{equation}
x_{m+2n}=c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}
\label{sol.ps2.2.2.c.goal}%
\end{equation}
for every $m\in\mathbb{N}$.

We must be in one of the following three cases:

\textit{Case 1:} We have $n=0$.

\textit{Case 2:} We have $n=1$.

\textit{Case 3:} We have $n\geq2$.

Let us first consider Case 1. In this case, we have $n=0$. Now, let
$m\in\mathbb{N}$. Since $n=0$, we have%
\[
c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}=\underbrace{c_{0}}%
_{=2}\underbrace{x_{m+0}}_{=x_{m}}+\underbrace{\left(  -1\right)  ^{0-1}%
}_{=-1}\underbrace{b^{0}}_{=1}x_{m}=2x_{m}+\left(  -1\right)  x_{m}=x_{m}.
\]
Compared with%
\begin{align*}
x_{m+2n}  &  =x_{m+2\cdot0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=0\right)
\\
&  =x_{m},
\end{align*}
this yields $x_{m+2n}=c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}$.
Hence, (\ref{sol.ps2.2.2.c.goal}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $n=1$. Now, let
$m\in\mathbb{N}$. Since $n=1$, we have%
\[
c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}=\underbrace{c_{1}}%
_{=a}x_{m+1}+\underbrace{\left(  -1\right)  ^{1-1}}_{=1}\underbrace{b^{1}%
}_{=b}x_{m}=ax_{m+1}+bx_{m}.
\]
Compared with%
\begin{align*}
x_{m+2n}  &  =x_{m+2\cdot1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=1\right)
\\
&  =x_{m+2}=ax_{\left(  m+2\right)  -1}+bx_{\left(  m+2\right)  -2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the sequence }\left(  x_{0}%
,x_{1},x_{2},\ldots\right)  \text{ is }\left(  a,b\right)  \text{-recurrent}%
\right) \\
&  =ax_{m+1}+bx_{m},
\end{align*}
this yields $x_{m+2n}=c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}$.
Hence, (\ref{sol.ps2.2.2.c.goal}) is proven in Case 2.

Let us finally consider Case 3. In this case, we have $n\geq2$. Hence, both
$n-1$ and $n-2$ are nonnegative integers. Moreover, $n-1<n$, so that
(\ref{sol.ps2.2.2.c.claim}) is already proven for $N=n-1$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{m+2\left(  n-1\right)  }=c_{n-1}x_{m+\left(  n-1\right)  }+\left(
-1\right)  ^{\left(  n-1\right)  -1}b^{n-1}x_{m} \label{sol.ps2.2.2.c.hyp1}%
\end{equation}
for every $m\in\mathbb{N}$.

Also, $n-2$ is a nonnegative integer such that $n-2<n$. Hence,
(\ref{sol.ps2.2.1.claim}) is already proven for $N=n-2$ (by our induction
hypothesis). In other words, we have%
\begin{equation}
x_{m+2\left(  n-2\right)  }=c_{n-2}x_{m+\left(  n-2\right)  }+\left(
-1\right)  ^{\left(  n-2\right)  -1}b^{n-2}x_{m} \label{sol.ps2.2.2.c.hyp2}%
\end{equation}
for every $m\in\mathbb{N}$.

Now, fix $m\in\mathbb{N}$. We want to prove (\ref{sol.ps2.2.2.c.goal}). The
recursive definition of the sequence $\left(  c_{0},c_{1},c_{2},\ldots\right)
$ yields
\begin{equation}
c_{n}=ac_{n-1}+bc_{n-2}. \label{sol.ps2.2.2.c.1}%
\end{equation}
Since the sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent, we have%
\[
x_{m+2}=a\underbrace{x_{\left(  m+2\right)  -1}}_{=x_{m+1}}%
+b\underbrace{x_{\left(  m+2\right)  -2}}_{=x_{m}}=ax_{m+1}+bx_{m},
\]
so that%
\begin{equation}
ax_{m+1}-x_{m+2}=-bx_{m}. \label{sol.ps2.2.2.c.3}%
\end{equation}
But since the sequence $\left(  x_{0},x_{1},x_{2},\ldots\right)  $ is $\left(
a,b\right)  $-recurrent, we also have
\begin{align*}
x_{m+2n}  &  =a\underbrace{x_{\left(  m+2n\right)  -1}}_{\substack{=x_{\left(
m+1\right)  +2\left(  n-1\right)  }\\\text{(since }\left(  m+2n\right)
-1=\left(  m+1\right)  +2\left(  n-1\right)  \text{)}}%
}+b\underbrace{x_{\left(  m+2n\right)  -2}}_{\substack{=x_{\left(  m+2\right)
+2\left(  n-2\right)  }\\\text{(since }\left(  m+2n\right)  -2=\left(
m+2\right)  +2\left(  n-2\right)  \text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{m}_{\geq
0}+2\underbrace{n}_{\geq2}\geq0+2\cdot2=4\geq2\right) \\
&  =a\underbrace{x_{\left(  m+1\right)  +2\left(  n-1\right)  }}%
_{\substack{=c_{n-1}x_{\left(  m+1\right)  +\left(  n-1\right)  }+\left(
-1\right)  ^{\left(  n-1\right)  -1}b^{n-1}x_{m+1}\\\text{(by
(\ref{sol.ps2.2.2.c.hyp1}), applied to }m+1\text{ instead of }m\text{)}%
}}+b\underbrace{x_{\left(  m+2\right)  +2\left(  n-2\right)  }}%
_{\substack{=c_{n-2}x_{\left(  m+2\right)  +\left(  n-2\right)  }+\left(
-1\right)  ^{\left(  n-2\right)  -1}b^{n-2}x_{m+2}\\\text{(by
(\ref{sol.ps2.2.2.c.hyp2}), applied to }m+2\text{ instead of }m\text{)}}}\\
&  =a\left(  c_{n-1}\underbrace{x_{\left(  m+1\right)  +\left(  n-1\right)  }%
}_{=x_{m+n}}+\underbrace{\left(  -1\right)  ^{\left(  n-1\right)  -1}%
}_{=\left(  -1\right)  ^{n-2}=\left(  -1\right)  ^{n}}b^{n-1}x_{m+1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +b\left(  c_{n-2}\underbrace{x_{\left(  m+2\right)
+\left(  n-2\right)  }}_{=x_{m+n}}+\underbrace{\left(  -1\right)  ^{\left(
n-2\right)  -1}}_{=\left(  -1\right)  ^{n-3}=\left(  -1\right)  ^{n-1}%
=-\left(  -1\right)  ^{n}}b^{n-2}x_{m+2}\right) \\
&  =a\left(  c_{n-1}x_{m+n}+\left(  -1\right)  ^{n}b^{n-1}x_{m+1}\right)
+b\left(  c_{n-2}x_{m+n}+\left(  -\left(  -1\right)  ^{n}\right)
b^{n-2}x_{m+2}\right) \\
&  =ac_{n-1}x_{m+n}+\left(  -1\right)  ^{n}ab^{n-1}x_{m+1}+bc_{n-2}%
x_{m+n}-\left(  -1\right)  ^{n}\underbrace{bb^{n-2}}_{=b^{n-1}}x_{m+2}\\
&  =ac_{n-1}x_{m+n}+\left(  -1\right)  ^{n}ab^{n-1}x_{m+1}+bc_{n-2}%
x_{m+n}-\left(  -1\right)  ^{n}b^{n-1}x_{m+2}\\
&  =\underbrace{\left(  ac_{n-1}x_{m+n}+bc_{n-2}x_{m+n}\right)  }_{=\left(
ac_{n-1}+bc_{n-2}\right)  x_{m+n}}+\underbrace{\left(  -1\right)  ^{n}%
ab^{n-1}x_{m+1}-\left(  -1\right)  ^{n}b^{n-1}x_{m+2}}_{=\left(  -1\right)
^{n}b^{n-1}\left(  ax_{m+1}-x_{m+2}\right)  }\\
&  =\underbrace{\left(  ac_{n-1}+bc_{n-2}\right)  }_{\substack{=c_{n}%
\\\text{(by (\ref{sol.ps2.2.2.c.1}))}}}x_{m+n}+\left(  -1\right)  ^{n}%
b^{n-1}\underbrace{\left(  ax_{m+1}-x_{m+2}\right)  }_{\substack{=-bx_{m}%
\\\text{(by (\ref{sol.ps2.2.2.c.3}))}}}\\
&  =c_{n}x_{m+n}+\underbrace{\left(  -1\right)  ^{n}b^{n-1}\left(
-bx_{m}\right)  }_{=-\left(  -1\right)  ^{n}b^{n-1}bx_{m}}=c_{n}%
x_{m+n}+\left(  \underbrace{-\left(  -1\right)  ^{n}}_{=\left(  -1\right)
^{n-1}}\underbrace{b^{n-1}b}_{=b^{n}}x_{m}\right) \\
&  =c_{n}x_{m+n}+\left(  -1\right)  ^{n-1}b^{n}x_{m}.
\end{align*}


In other words, (\ref{sol.ps2.2.2.c.goal}) is proven in Case 3.

Thus we have seen that (\ref{sol.ps2.2.2.c.goal}) holds in each of the three
Cases 1, 2 and 3. Since these three cases cover all possibilities, this
finishes the proof of (\ref{sol.ps2.2.2.c.goal}). This finishes our
(inductive) proof of (\ref{sol.ps2.2.2.c.claim}). As we know, this solves
Exercise \ref{exe.ps2.2.2}.
\end{proof}

\begin{remark}
How on earth could one have come up with my definition of the sequence
$\left(  c_{0},c_{1},c_{2},\ldots\right)  $ in the solution above? One way is
to solve parts \textbf{(a)} and \textbf{(b)} of the exercise first (which can
be solved by applying the equation $x_{n}=ax_{n-1}+bx_{n-2}$ several times),
and then guess that the answer to \textbf{(c)} is a pair of the form $\left(
c,d\right)  =\left(  c_{N},\left(  -1\right)  ^{N-1}b^{N}\right)  $ for some
sequence $\left(  c_{0},c_{1},c_{2},\ldots\right)  $. What remains is finding
this sequence. I believe its entry $c_{3}=a\left(  a^{2}+3b\right)  $ is
particularly telltale.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps2.2.3}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.3}.]A set $I$ of integers is said to be
\textit{lacunar} if no two elements of $I$ are consecutive (i.e., there exists
no $i\in\mathbb{Z}$ such that both $i$ and $i+1$ belong to $I$). Then,
Exercise \ref{exe.ps2.2.3} asks us to prove that, for every positive integer
$n$,%
\begin{equation}
\text{the number }f_{n}\text{ is the number of lacunar subsets of }\left\{
1,2,\ldots,n-2\right\}  . \label{sol.ps2.2.3.goal}%
\end{equation}


We shall prove (\ref{sol.ps2.2.3.goal}) by strong induction over $n$. Thus, we
let $N$ be a positive integer, and we assume (as the induction hypothesis)
that (\ref{sol.ps2.2.3.goal}) is proven for every $n<N$. We need to prove
(\ref{sol.ps2.2.3.goal}) for $n=N$. In other words, we need to prove that
\begin{equation}
\text{the number }f_{N}\text{ is the number of lacunar subsets of }\left\{
1,2,\ldots,N-2\right\}  . \label{sol.ps2.2.3.goal2}%
\end{equation}


Recall that $N$ is a positive integer. Hence, we are in one of the following
three cases:

\textit{Case 1:} We have $N=1$.

\textit{Case 2:} We have $N=2$.

\textit{Case 3:} We have $N\geq3$.

Let us first consider Case 1. In this case, we have $N=1$. Thus, $f_{N}%
=f_{1}=1$. On the other hand, the number of lacunar subsets of $\left\{
1,2,\ldots,N-2\right\}  $ is $1$ (since the set $\left\{  1,2,\ldots
,\underbrace{N}_{=1}-2\right\}  =\left\{  1,2,\ldots,1-2\right\}
=\varnothing$ has only one subset, and this subset is lacunar). Thus, $f_{N}$
is the number of lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $.
Hence, (\ref{sol.ps2.2.3.goal2}) is proven in Case 1.

Case 2 can be dealt with similarly (in this case, the set $\left\{
1,2,\ldots,N-2\right\}  $ is still empty, and $f_{N}$ is still $1$), and is
left to the reader.

We now consider Case 3. In this case, we have $N\geq3$. Hence, $N-1$ and $N-2$
are positive integers. Since $N-1$ is a positive integer and $<N$, we know
that (\ref{sol.ps2.2.3.goal2}) is proven for $n=N-1$ (due to our induction
hypothesis). In other words, the number $f_{N-1}$ is the number of lacunar
subsets of $\left\{  1,2,\ldots,\left(  N-1\right)  -2\right\}  $. In other
words, $f_{N-1}$ is the number of lacunar subsets of $\left\{  1,2,\ldots
,N-3\right\}  $.

Also, since $N-2$ is a positive integer and $<N$, we know that
(\ref{sol.ps2.2.3.goal2}) is proven for $n=N-2$ (due to our induction
hypothesis). In other words, the number $f_{N-2}$ is the number of lacunar
subsets of $\left\{  1,2,\ldots,\left(  N-2\right)  -2\right\}  $. In other
words, $f_{N-2}$ is the number of lacunar subsets of $\left\{  1,2,\ldots
,N-4\right\}  $.

Now, how do the lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ look
like? We say that a lacunar subset of $\left\{  1,2,\ldots,N-2\right\}  $ has
\textit{type 1} if it contains $N-2$, and has \textit{type 2} if it does not.
Now, let us count the lacunar subsets having type 1 and those having type 2:

\begin{itemize}
\item If $I$ is a lacunar subset of $\left\{  1,2,\ldots,N-2\right\}  $ which
has type 1, then it contains $N-2$, and thus cannot contain $N-3$ (because it
is lacunar, i.e., contains no two consecutive integers, but $N-3$ and $N-2$
are two consecutive integers); moreover, $I\setminus\left\{  N-2\right\}  $ is
a lacunar subset of $\left\{  1,2,\ldots,N-4\right\}  $%
\ \ \ \ \footnote{Indeed, it is lacunar because $I$ is lacunar; and it is a
subset of $\left\{  1,2,\ldots,N-4\right\}  $ because $I$ cannot contain
$N-3$.}. Thus, to every lacunar subset $I$ of $\left\{  1,2,\ldots
,N-2\right\}  $ which has type 1, we have assigned a lacunar subset of
$\left\{  1,2,\ldots,N-4\right\}  $ (namely, $I\setminus\left\{  N-2\right\}
$). It is easy to see that this assignment is injective (indeed, if $I$ and
$J$ are two lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ which have
type 1, and if $I\setminus\left\{  N-2\right\}  =J\setminus\left\{
N-2\right\}  $, then $I=J$) and surjective (because whenever $K$ is a lacunar
subset of $\left\{  1,2,\ldots,N-4\right\}  $, the set $K\cup\left\{
N-2\right\}  $ is a lacunar subset of $\left\{  1,2,\ldots,N-2\right\}  $
which has type 1, and this set $K\cup\left\{  N-2\right\}  $ is sent back to
$K$ by our assignment); thus, it is bijective. Hence, we have found a
bijection between the lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $
which have type 1 and the lacunar subsets of $\left\{  1,2,\ldots,N-4\right\}
$. Therefore, the number of lacunar subsets of $\left\{  1,2,\ldots
,N-2\right\}  $ which have type 1 equals the number of lacunar subsets of
$\left\{  1,2,\ldots,N-4\right\}  $. But the latter number is $f_{N-2}$.
Hence, the number of lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $
which have type 1 is $f_{N-2}$.

\item The lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ which have
type 2 are precisely the lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}
$ which do not contain $N-2$; in other words, they are precisely the lacunar
subsets of $\left\{  1,2,\ldots,N-3\right\}  $. Hence, the number of lacunar
subsets of $\left\{  1,2,\ldots,N-2\right\}  $ which have type 2 is $f_{N-1}$
(since we know that the number of lacunar subsets of $\left\{  1,2,\ldots
,N-3\right\}  $ is $f_{N-1}$).
\end{itemize}

Now, let us summarize. Each of the lacunar subsets of $\left\{  1,2,\ldots
,N-2\right\}  $ either has type 1 or has type 2 (but not both). Hence, the
number of lacunar subsets of $\left\{  1,2,\ldots,N-2\right\}  $ equals%
\begin{align*}
&  \underbrace{\left(  \text{the number of lacunar subsets of }\left\{
1,2,\ldots,N-2\right\}  \text{ which have type 1}\right)  }_{=f_{N-2}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the number of lacunar
subsets of }\left\{  1,2,\ldots,N-2\right\}  \text{ which have type 2}\right)
}_{=f_{N-1}}\\
&  =f_{N-2}+f_{N-1}=f_{N-1}+f_{N-2}\\
&  =f_{N}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursion of the Fibonacci
numbers}\right)  .
\end{align*}
This proves (\ref{sol.ps2.2.3.goal2}) in Case 3.

Now, (\ref{sol.ps2.2.3.goal2}) is proven in each of the three Cases 1, 2 and
3. Hence, (\ref{sol.ps2.2.3.goal2}) always holds. This completes our
(inductive) proof of (\ref{sol.ps2.2.3.goal}). Exercise \ref{exe.ps2.2.3} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.S}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.S}.]Let us first explain why the right
hand side of (\ref{eq.exe.2.S}) is well-defined. In fact, this is not obvious,
because if $r=0$, then $r^{n-1-2i}$ might not always make sense (because
$n-1-2i$ can be negative). However, it turns out that $\dbinom{n-1-i}{i}=0$
for every $i\in\left\{  0,1,\ldots,n-1\right\}  $ satisfying $n-1-2i<0$%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  0,1,\ldots,n-1\right\}  $
be such that $n-1-2i<0$. Then, $n-1-i\geq0$ (since $i\in\left\{
0,1,\ldots,n-1\right\}  $) but $n-1-i<i$ (since $\left(  n-1-i\right)
-i=n-1-2i<0$), and thus the binomial coefficient $\dbinom{n-1-i}{i}$ is $0$
(because any binomial coefficient $\dbinom{a}{b}$ with $a\geq0$ and $a<b$ must
be $0$), qed.}. Hence, we interpret $\dbinom{n-1-i}{i}r^{n-1-2i}$ as $0$
whenever $n-1-2i<0$ (even if the term $r^{n-1-2i}$ by itself is not
well-defined), following our convention that any expression of the form
$a\cdot b$, where $a$ is $0$, has to be interpreted as $0$. Thus, the right
hand side of (\ref{eq.exe.2.S}) is well-defined.

Now, we shall prove (\ref{eq.exe.2.S}) by strong induction over $n$. Thus, we
let $N\in\mathbb{N}$, and we assume (as the induction hypothesis) that
(\ref{eq.exe.2.S}) is proven for every $n<N$. We need to prove
(\ref{eq.exe.2.S}) for $n=N$. In other words, we need to prove that
\begin{equation}
c_{N}=\sum_{i=0}^{N-1}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}.
\label{sol.ps2.2.S.goal}%
\end{equation}


Recall that $N\in\mathbb{N}$. Hence, we are in one of the following three cases:

\textit{Case 1:} We have $N=0$.

\textit{Case 2:} We have $N=1$.

\textit{Case 3:} We have $N\geq2$.

Let us first consider Case 1. In this case, we have $N=0$. Hence, $c_{N}%
=c_{0}=0$. But also, the sum $\sum_{i=0}^{N-1}\left(  -1\right)  ^{i}%
\dbinom{N-1-i}{i}r^{N-1-2i}$ is an empty sum (since $N=0$) and thus equals
$0$. Therefore, both sides of the equality (\ref{sol.ps2.2.S.goal}) equal $0$.
Hence, the equality (\ref{sol.ps2.2.S.goal}) holds. We thus have proven
(\ref{sol.ps2.2.S.goal}) in Case 1.

Let us now consider Case 2. In this case, we have $N=1$. Thus, $c_{N}=c_{1}%
=1$. On the other hand, from $N=1$, we obtain%
\begin{align*}
\sum_{i=0}^{N-1}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}  &
=\sum_{i=0}^{1-1}\left(  -1\right)  ^{i}\dbinom{1-1-i}{i}r^{1-1-2i}\\
&  =\underbrace{\left(  -1\right)  ^{0}}_{=1}\underbrace{\dbinom{1-1-0}{0}%
}_{=1}\underbrace{r^{1-1-2\cdot0}}_{=r^{0}=1}=1=c_{N}.
\end{align*}
Hence, (\ref{sol.ps2.2.S.goal}) is proven in Case 2.

Let us now consider Case 3. In this case, we have $N\geq2$. Therefore, both
$N-1$ and $N-2$ belong to $\mathbb{N}$.

So we know that $N-1$ is an element of $\mathbb{N}$ satisfying $N-1<N$. Hence,
(\ref{eq.exe.2.S}) is proven for $n=N-1$ (by our induction hypothesis). In
other words,%
\begin{equation}
c_{N-1}=\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}{i}r^{N-2-2i}.
\label{sol.ps2.2.S.hyp1}%
\end{equation}


Also, $N-2$ is an element of $\mathbb{N}$ satisfying $N-2<N$. Hence,
(\ref{eq.exe.2.S}) is proven for $n=N-2$ (by our induction hypothesis). In
other words,%
\begin{equation}
c_{N-2}=\sum_{i=0}^{N-3}\left(  -1\right)  ^{i}\dbinom{N-3-i}{i}r^{N-3-2i}.
\label{sol.ps2.2.S.hyp2}%
\end{equation}


Let us further recall that every positive integer $i$ and every $a\in
\mathbb{Z}$ satisfy%
\begin{equation}
\dbinom{a-1}{i}+\dbinom{a-1}{i-1}=\dbinom{a}{i} \label{sol.ps2.2.S.rec1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps2.2.S.rec1}):} Let $i$ be a positive
integer. Let $a\in\mathbb{Z}$. Then, (\ref{eq.binom.rec.m}) (applied to $m=a$
and $n=i$) shows that $\dbinom{a}{i}=\dbinom{a-1}{i-1}+\dbinom{a-1}{i}%
=\dbinom{a-1}{i}+\dbinom{a-1}{i-1}$. This proves (\ref{sol.ps2.2.S.rec1}).}.

\begin{noncompile}
Here is a proof of (\ref{sol.ps2.2.S.rec1}) that I have written back before I
had written up the section about binomial coefficients:

\textit{Proof of (\ref{sol.ps2.2.S.rec1}):} This is the famous recurrence
relation of Pascal's triangle, saying that every number in Pascal's triangle
equals the sum of the two numbers above. However, Pascal's triangle is usually
only drawn to contain the binomial coefficients $\dbinom{a}{i}$ with $0\leq
i\leq a$ (or else it would not be a triangle), so you might not be aware that
it holds for all $a\in\mathbb{Z}$. Either way, the proof is simple:%
\begin{align*}
&  \underbrace{\dbinom{a-1}{i}}_{\substack{=\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i\right)  }{i!}=\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i+1\right)  }{\left(  i-1\right)  !}\cdot
\dfrac{a-i}{i}\\\text{(since }i!=\left(  i-1\right)  !\cdot i\text{)}%
}}+\underbrace{\dbinom{a-1}{i-1}}_{=\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i+1\right)  }{\left(  i-1\right)  !}}\\
&  =\dfrac{\left(  a-1\right)  \left(  a-2\right)  \cdots\left(  a-i+1\right)
}{\left(  i-1\right)  !}\cdot\dfrac{a-i}{i}+\dfrac{\left(  a-1\right)  \left(
a-2\right)  \cdots\left(  a-i+1\right)  }{\left(  i-1\right)  !}\\
&  =\dfrac{\left(  a-1\right)  \left(  a-2\right)  \cdots\left(  a-i+1\right)
}{\left(  i-1\right)  !}\underbrace{\left(  \dfrac{a-i}{i}+1\right)
}_{=\dfrac{a}{i}}=\dfrac{\left(  a-1\right)  \left(  a-2\right)  \cdots\left(
a-i+1\right)  }{\left(  i-1\right)  !}\cdot\dfrac{a}{i}\\
&  =\dfrac{a\left(  a-1\right)  \left(  a-2\right)  \cdots\left(
a-i+1\right)  }{i!}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
i-1\right)  !\cdot i=i!\right) \\
&  =\dbinom{a}{i}.
\end{align*}

\end{noncompile}

Now, $N\geq2$, so that the recursive definition of the sequence $\left(
c_{0},c_{1},c_{2},\ldots\right)  $ yields%
\begin{align*}
c_{N}  &  =r\underbrace{c_{N-1}}_{\substack{=\sum_{i=0}^{N-2}\left(
-1\right)  ^{i}\dbinom{N-2-i}{i}r^{N-2-2i}\\\text{(by (\ref{sol.ps2.2.S.hyp1}%
))}}}-\underbrace{c_{N-2}}_{\substack{=\sum_{i=0}^{N-3}\left(  -1\right)
^{i}\dbinom{N-3-i}{i}r^{N-3-2i}\\\text{(by (\ref{sol.ps2.2.S.hyp2}))}}}\\
&  =r\left(  \sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}%
{i}r^{N-2-2i}\right)  -\sum_{i=0}^{N-3}\left(  -1\right)  ^{i}\dbinom
{N-3-i}{i}r^{N-3-2i}\\
&  =\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}{i}%
\underbrace{rr^{N-2-2i}}_{=r^{N-1-2i}}-\underbrace{\sum_{i=0}^{N-3}\left(
-1\right)  ^{i}\dbinom{N-3-i}{i}r^{N-3-2i}}_{\substack{=\sum_{i=1}%
^{N-2}\left(  -1\right)  ^{i-1}\dbinom{N-3-\left(  i-1\right)  }%
{i-1}r^{N-3-2\left(  i-1\right)  }\\\text{(here, we substituted }i-1\text{ for
}i\text{ in the sum)}}}\\
&  =\underbrace{\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}%
{i}r^{N-1-2i}}_{=\left(  -1\right)  ^{0}\dbinom{N-2-0}{0}r^{N-1-2\cdot0}%
+\sum_{i=1}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-2-i}{i}r^{N-1-2i}}\\
&  \ \ \ \ \ \ \ \ \ \ -\sum_{i=1}^{N-2}\underbrace{\left(  -1\right)  ^{i-1}%
}_{=-\left(  -1\right)  ^{i}}\underbrace{\dbinom{N-3-\left(  i-1\right)
}{i-1}}_{=\dbinom{N-2-i}{i-1}}\underbrace{r^{N-3-2\left(  i-1\right)  }%
}_{=r^{N-1-2i}}%
\end{align*}%
\begin{align*}
&  =\left(  -1\right)  ^{0}\underbrace{\dbinom{N-2-0}{0}}_{=1=\dbinom
{N-1-0}{0}}r^{N-1-2\cdot0}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{i=1}^{N-2}\left(  -1\right)
^{i}\dbinom{N-2-i}{i}r^{N-1-2i}-\sum_{i=1}^{N-2}\left(  -\left(  -1\right)
^{i}\right)  \dbinom{N-2-i}{i-1}r^{N-1-2i}}_{=\sum_{i=1}^{N-2}\left(
-1\right)  ^{i}\left(  \dbinom{N-2-i}{i}+\dbinom{N-2-i}{i-1}\right)
r^{N-1-2i}}\\
&  =\left(  -1\right)  ^{0}\dbinom{N-1-0}{0}r^{N-1-2\cdot0}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{i=1}^{N-2}\left(  -1\right)  ^{i}%
\underbrace{\left(  \dbinom{N-2-i}{i}+\dbinom{N-2-i}{i-1}\right)
}_{\substack{=\dbinom{\left(  N-1-i\right)  -1}{i}+\dbinom{\left(
N-1-i\right)  -1}{i-1}\\=\dbinom{N-1-i}{i}\\\text{(by (\ref{sol.ps2.2.S.rec1}%
), applied to }a=N-1-i\text{)}}}r^{N-1-2i}\\
&  =\left(  -1\right)  ^{0}\dbinom{N-1-0}{0}r^{N-1-2\cdot0}+\sum_{i=1}%
^{N-2}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}\\
&  =\sum_{i=0}^{N-2}\left(  -1\right)  ^{i}\dbinom{N-1-i}{i}r^{N-1-2i}.
\end{align*}
Hence, (\ref{sol.ps2.2.S.goal}) is proven in Case 3. We thus have proven
(\ref{sol.ps2.2.S.goal}) in all three Cases 1, 2 and 3, so that we conclude
that (\ref{sol.ps2.2.S.goal}) always holds. This completes our proof of
(\ref{eq.exe.2.S}).
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.4}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.4}.]\textbf{(a)} This proof is completely
straightforward, and would be left to the reader in any research paper; we
give a few details only:

Let $i\in\left\{  1,2,\ldots,n-2\right\}  $. We need to prove that $s_{i}\circ
s_{i+1}\circ s_{i}=s_{i+1}\circ s_{i}\circ s_{i+1}$. In order to do so, it is
clearly enough to show that $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)
\left(  h\right)  =\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(
h\right)  $ for every $h\in\left\{  1,2,\ldots,n\right\}  $. So let us fix
$h\in\left\{  1,2,\ldots,n\right\}  $. We must be in one of the following four cases:

\textit{Case 1:} We have $h=i$.

\textit{Case 2:} We have $h=i+1$.

\textit{Case 3:} We have $h=i+2$.

\textit{Case 4:} We have $h\notin\left\{  i,i+1,i+2\right\}  $.

Let us first consider Case 1. In this case, we have $h=i$ and thus
\begin{align*}
\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  \underbrace{h}%
_{=i}\right)   &  =\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(
i\right)  =s_{i}\left(  s_{i+1}\left(  \underbrace{s_{i}\left(  i\right)
}_{=i+1}\right)  \right) \\
&  =s_{i}\left(  \underbrace{s_{i+1}\left(  i+1\right)  }_{=i+2}\right)
=s_{i}\left(  i+2\right)  =i+2.
\end{align*}
A similar computation shows $\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)
\left(  h\right)  =i+2$. Thus, $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)
\left(  h\right)  =i+2=\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(
h\right)  $. Hence, we have proven the equality $\left(  s_{i}\circ
s_{i+1}\circ s_{i}\right)  \left(  h\right)  =\left(  s_{i+1}\circ s_{i}\circ
s_{i+1}\right)  \left(  h\right)  $ in Case 1.

Similarly, we can prove the same equality in Cases 2 and 3.

Now, let us consider Case 4. In this case, we have $h\notin\left\{
i,i+1,i+2\right\}  $. Thus, $h$ is neither $i$ nor $i+1$, so that we have
$s_{i}\left(  h\right)  =h$. Also, $h$ is neither $i+1$ nor $i+2$ (since
$h\notin\left\{  i,i+1,i+2\right\}  $), and thus we have $s_{i+1}\left(
h\right)  =h$. Hence,
\[
\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  h\right)  =s_{i}\left(
s_{i+1}\left(  \underbrace{s_{i}\left(  h\right)  }_{=h}\right)  \right)
=s_{i}\left(  \underbrace{s_{i+1}\left(  h\right)  }_{=h}\right)
=s_{i}\left(  h\right)  =h.
\]
Similarly, $\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)
=h$. Thus, $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  h\right)
=h=\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)  $. Hence,
we have proven $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(
h\right)  =\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)  $
in Case 4.

Altogether, we have proven the equality $\left(  s_{i}\circ s_{i+1}\circ
s_{i}\right)  \left(  h\right)  =\left(  s_{i+1}\circ s_{i}\circ
s_{i+1}\right)  \left(  h\right)  $ in each of the four Cases 1, 2, 3, and 4.
Thus, $\left(  s_{i}\circ s_{i+1}\circ s_{i}\right)  \left(  h\right)
=\left(  s_{i+1}\circ s_{i}\circ s_{i+1}\right)  \left(  h\right)  $ always
holds. Exercise \ref{exe.ps2.2.4} \textbf{(a)} is thus solved.

\textbf{(b)} We follow the hint and delay the solution of this part until
later (see Exercise \ref{exe.ps2.2.5} \textbf{(e)}).

\textbf{(c)} For every $i\in\left\{  1,2,\ldots,n\right\}  $, we let $a_{i}$
be the permutation $s_{i-1}\circ s_{i-2}\circ\cdots\circ s_{1}\in S_{n}%
$\ \ \ \ \footnote{In particular, $a_{1}=s_{1-1}\circ s_{1-2}\circ\cdots\circ
s_{1}$ is the composition of $0$ permutations. What does this mean? Just as a
sum of $0$ terms is defined to be $0$ (because $0$ is the neutral element of
addition), and a product of $0$ terms is $1$ (since $1$ is the neutral element
of multiplication), the composition of $0$ permutations is defined to be the
identity permutation (since the identity permutation is the neutral element of
composition). Hence, $a_{1}$ is the identity permutation, i.e., we have
$a_{1}=\operatorname*{id}$.}. Now, we claim that%
\begin{equation}
w_{0}=a_{1}\circ a_{2}\circ\cdots\circ a_{n}. \label{sol.ps2.2.4.c.claim}%
\end{equation}
This is essentially an explicit way to write $w_{0}$ as a composition of
several permutations of the form $s_{i}$ (because each $a_{i}$ on the right
hand side is the composition $s_{i-1}\circ s_{i-2}\circ\cdots\circ s_{1}$).
Thus, once (\ref{sol.ps2.2.4.c.claim}) is proven, the exercise will be solved.

Before we prove (\ref{sol.ps2.2.4.c.claim}), let us first understand how
$a_{i}$ operates. We claim that%
\begin{equation}
a_{i}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq i;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>i
\end{array}
\right.  \label{sol.ps2.2.4.c.aik}%
\end{equation}
for each $i\in\left\{  1,2,\ldots,n\right\}  $ and $k\in\left\{
1,2,\ldots,n\right\}  $. (In other words, $a_{i}$ is the permutation which
cycles $i\rightarrow i-1\rightarrow\cdots\rightarrow1\rightarrow i$ and leaves
all numbers $>i$ untouched.)

\textit{Proof of (\ref{sol.ps2.2.4.c.aik}):} Let me give an informal proof of
(\ref{sol.ps2.2.4.c.aik}) which, I trust, you can turn into a formal proof if
you so desire.

Let $i\in\left\{  1,2,\ldots,n\right\}  $. We have $a_{i}=s_{i-1}\circ
s_{i-2}\circ\cdots\circ s_{1}$. Therefore, $a_{i}$ is the permutation which
first switches $1$ with $2$, then switches $2$ with $3$, etc., until it
finally switches $i-1$ with $i$. Thus:

\begin{itemize}
\item When we apply $a_{i}$ to $1$, we arrive at $i$ at the end (since the $1$
is carried to $2$ by the first switch, which is then carried to $3$ by the
next switch, and so on, until it finally becomes $i$).

\item When we apply $a_{i}$ to some $k\in\left\{  2,3,\ldots,i\right\}  $, we
arrive at $k-1$ at the end (since the first switch to move $k$ is the $\left(
k-1\right)  $-st switch, which changes it into $k-1$, and from then on all the
following switches leave $k-1$ untouched).

\item When we apply $a_{i}$ to some $k\in\left\{  i+1,i+2,\ldots,n\right\}  $,
we arrive at $k$ at the end (since none of our switches changes $k$).
\end{itemize}

Expressing this in a formula instead of in words, we obtain precisely
(\ref{sol.ps2.2.4.c.aik}).

\begin{verlong}
\textit{Formal proof of (\ref{sol.ps2.2.4.c.aik}):} For the sake of
completeness, let me show how to prove (\ref{sol.ps2.2.4.c.aik}) formally.

We shall prove (\ref{sol.ps2.2.4.c.aik}) by induction on $i$:

\textit{Induction base:} We have $a_{1}=s_{0}\circ s_{-1}\circ\cdots\circ
s_{1}=\left(  \text{a composition of }0\text{ permutations}\right)
=\operatorname*{id}$. Thus, every $k\in\left\{  1,2,\ldots,n\right\}  $
satisfies%
\begin{align*}
\underbrace{a_{1}}_{=\operatorname*{id}}\left(  k\right)   &
=\operatorname*{id}\left(  k\right)  =k=\left\{
\begin{array}
[c]{c}%
k,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>1
\end{array}
\right.  =\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>1
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k=1\text{ when }k=1\right) \\
&  =\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>1
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we added a \textquotedblleft}1<k\leq1\text{\textquotedblright%
\ case, which does not}\\
\text{change the result because this case never happens}%
\end{array}
\right)  .
\end{align*}
In other words, (\ref{sol.ps2.2.4.c.aik}) holds for $i=1$. This completes the
induction base.

\textit{Induction step:} Let $I\in\left\{  1,2,\ldots,n\right\}  $ be such
that $I>1$. Assume that (\ref{sol.ps2.2.4.c.aik}) holds for $i=I-1$. We need
to show that (\ref{sol.ps2.2.4.c.aik}) holds for $i=I$.

We have assumed that (\ref{sol.ps2.2.4.c.aik}) holds for $i=I-1$. In other
words,%
\begin{equation}
a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  \label{sol.ps2.2.4.c.aik.pf.hyp}%
\end{equation}
for each $k\in\left\{  1,2,\ldots,n\right\}  $.

The definition of $a_{I-1}$ yields $a_{I-1}=s_{I-2}\circ s_{I-3}\circ
\cdots\circ s_{1}$. The definition of $a_{I}$ yields $a_{I}=s_{I-1}\circ
s_{I-2}\circ\cdots\circ s_{1}=s_{I-1}\circ\underbrace{\left(  s_{I-2}\circ
s_{I-3}\circ\cdots\circ s_{1}\right)  }_{=a_{I-1}}=s_{I-1}\circ a_{I-1}$.
Hence, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{equation}
a_{I}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  \label{sol.ps2.2.4.c.aik.pf.step}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps2.2.4.c.aik.pf.step}):} Let
$k\in\left\{  1,2,\ldots,n\right\}  $. We need to prove
(\ref{sol.ps2.2.4.c.aik.pf.step}). We are in one of the following four cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $1<k\leq I$ and $k<I$.
\par
\textit{Case 3:} We have $1<k\leq I$ and $k\geq I$.
\par
\textit{Case 4:} We have $k>I$.
\par
Let us first consider Case 1. In this case, we have $k=1$. Thus,
(\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =I-1$ (since $k=1$) and thus $\underbrace{a_{I}}_{=s_{I-1}\circ
a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}\right)  \left(
k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(  k\right)  }%
_{=I-1}\right)  =s_{I-1}\left(  I-1\right)  =I$. Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =I$ (since $k=1$), this yields $a_{I}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $1<k\leq I$ and $k<I$. From
$k<I$, we obtain $k\leq I-1$ (since $k$ and $I$ are integers). Thus,
(\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =k-1$ (since $1<k\leq I-1$) and thus $\underbrace{a_{I}}%
_{=s_{I-1}\circ a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}%
\right)  \left(  k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(
k\right)  }_{=k-1}\right)  =s_{I-1}\left(  k-1\right)  =k-1$ (since $k-1<k\leq
I-1$). Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =k-1$ (since $1<k\leq I$), this yields $a_{I}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 2.
\par
Let us now consider Case 3. In this case, we have $1<k\leq I$ and $k\geq I$.
Combining $k\leq I$ with $k\geq I$, we obtain $k=I>I-1$. Thus,
(\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =k$ (since $k>I-1$) and thus $\underbrace{a_{I}}_{=s_{I-1}\circ
a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}\right)  \left(
k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(  k\right)  }%
_{=k=I}\right)  =s_{I-1}\left(  I\right)  =\underbrace{I}_{=k}-1=k-1$.
Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =k-1$ (since $1<k\leq I$), this yields $a_{I}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 3.
\par
Let us finally consider Case 4. In this case, we have $k>I$. Hence, $k>I>I-1$.
Thus, (\ref{sol.ps2.2.4.c.aik.pf.hyp}) yields $a_{I-1}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
I-1,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I-1
\end{array}
\right.  =k$ (since $k>I-1$) and thus $\underbrace{a_{I}}_{=s_{I-1}\circ
a_{I-1}}\left(  k\right)  =\left(  s_{I-1}\circ a_{I-1}\right)  \left(
k\right)  =s_{I-1}\left(  \underbrace{a_{I-1}\left(  k\right)  }_{=k}\right)
=s_{I-1}\left(  k\right)  =k$ (since $k>I$). Compared with $\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  =k$ (since $k>I$), this yields $a_{I}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
I,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq I;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>I
\end{array}
\right.  $. Thus, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in Case 4.
\par
Hence, (\ref{sol.ps2.2.4.c.aik.pf.step}) is proven in each of the four Cases
1, 2, 3 and 4. Since these four Cases are the only possible cases, this shows
that (\ref{sol.ps2.2.4.c.aik.pf.step}) always holds, qed.}. In other words,
(\ref{sol.ps2.2.4.c.aik}) holds for $i=I$. This completes the induction step.
The induction proof of (\ref{sol.ps2.2.4.c.aik}) is thus finished.
\end{verlong}

Next, for every $m\in\left\{  0,1,\ldots,n\right\}  $, set%
\[
b_{m}=a_{1}\circ a_{2}\circ\cdots\circ a_{m}\in S_{n}.
\]
As a consequence, $b_{0}=a_{0}\circ a_{1}\circ\cdots\circ a_{0}=\left(
\text{a composition of }0\text{ maps}\right)  =\operatorname*{id}$ and
$b_{n}=a_{1}\circ a_{2}\circ\cdots\circ a_{n}$. We claim that%
\begin{equation}
b_{m}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
m+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq m;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>m
\end{array}
\right.  \label{sol.ps2.2.4.c.bik}%
\end{equation}
for every $m\in\left\{  0,1,\ldots,n\right\}  $ and $k\in\left\{
1,2,\ldots,n\right\}  $.

Again, one can prove (\ref{sol.ps2.2.4.c.bik}) either formally by induction on
$m$, or more intuitively by tracking what happens to $k$ under the maps
$a_{m}$, $a_{m-1}$, $\ldots$, $a_{1}$ when these maps are applied one after
the other. This time the informal way is a bit tricky, so let us show the
formal one in all its glory. (You do not ever need to write a proof in this
level of detail unless you are talking to a computer.)

\textit{Proof of (\ref{sol.ps2.2.4.c.bik}):} We shall prove
(\ref{sol.ps2.2.4.c.bik}) by induction on $m$:

\textit{Induction base:} For every $k\in\left\{  1,2,\ldots,n\right\}  $, we
have $\underbrace{b_{0}}_{=\operatorname*{id}}\left(  k\right)
=\operatorname*{id}\left(  k\right)  =k=\left\{
\begin{array}
[c]{c}%
0+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq0;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>0
\end{array}
\right.  $ (since $\left\{
\begin{array}
[c]{c}%
0+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq0;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>0
\end{array}
\right.  =k$ (since $k>0$)). In other words, (\ref{sol.ps2.2.4.c.bik}) holds
for $m=0$. The induction base is thus complete.

\textit{Induction step:} Let $M\in\left\{  0,1,\ldots,n\right\}  $ be such
that $M>0$. Assume that (\ref{sol.ps2.2.4.c.bik}) holds for $m=M-1$. We need
to show that (\ref{sol.ps2.2.4.c.bik}) holds for $m=M$.

We have assumed that (\ref{sol.ps2.2.4.c.bik}) holds for $m=M-1$. In other
words,
\[
b_{M-1}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
\left(  M-1\right)  +1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right.
\]
for every $k\in\left\{  1,2,\ldots,n\right\}  $. Thus, for every $k\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{align}
b_{M-1}\left(  k\right)   &  =\left\{
\begin{array}
[c]{c}%
\left(  M-1\right)  +1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right. \nonumber\\
&  =\left\{
\begin{array}
[c]{c}%
M-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right.  \label{sol.ps2.2.4.c.bik.pf.hyp}%
\end{align}
(since $\left(  M-1\right)  +1-k=M-k$).

The definition of $b_{M-1}$ yields $b_{M-1}=a_{1}\circ a_{2}\circ\cdots\circ
a_{M-1}$. The definition of $b_{M}$ yields $b_{M}=a_{1}\circ a_{2}\circ
\cdots\circ a_{M}=\underbrace{\left(  a_{1}\circ a_{2}\circ\cdots\circ
a_{M-1}\right)  }_{=b_{M-1}}\circ a_{M}=b_{M-1}\circ a_{M}$. Thus, we obtain%
\begin{equation}
b_{M}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  \label{sol.ps2.2.4.c.bik.pf.step}%
\end{equation}
for every $k\in\left\{  1,2,\ldots,n\right\}  $\ \ \ \ \footnote{\textit{Proof
of (\ref{sol.ps2.2.4.c.bik.pf.step}):} Let $k\in\left\{  1,2,\ldots,n\right\}
$. We need to prove (\ref{sol.ps2.2.4.c.bik.pf.step}). We are in one of the
following three cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $1<k\leq M$.
\par
\textit{Case 3:} We have $k>M$.
\par
Let us first consider Case 1. In this case, we have $k=1$. Thus, $k=1\leq M$
(since $M\geq1$ (since $M>0$)). But (\ref{sol.ps2.2.4.c.aik}) (applied to
$i=M$) yields $a_{M}\left(  k\right)  =\left\{
\begin{array}
[c]{c}%
M,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =M$ (since $k=1$), so that
\begin{align*}
\underbrace{b_{M}}_{=b_{M-1}\circ a_{M}}\left(  k\right)   &  =\left(
b_{M-1}\circ a_{M}\right)  \left(  k\right)  =b_{M-1}\left(  \underbrace{a_{M}%
\left(  k\right)  }_{=M}\right)  =b_{M-1}\left(  M\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M-M,\ \ \ \ \ \ \ \ \ \ \text{if }M\leq M-1;\\
M,\ \ \ \ \ \ \ \ \ \ \text{if }M>M-1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.4.c.bik.pf.hyp}%
), applied to }M\text{ instead of }k\right) \\
&  =M\ \ \ \ \ \ \ \ \ \ \left(  \text{since }M>M-1\right) \\
&  =M+1-\underbrace{1}_{=k}=M+1-k=\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =M+1-k\text{ (because }k\leq M\text{)}\right)  .
\end{align*}
Thus, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $1<k\leq M$. Now,
(\ref{sol.ps2.2.4.c.aik}) (applied to $i=M$) yields $a_{M}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
M,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =k-1$ (since $1<k\leq M$), so that%
\begin{align*}
\underbrace{b_{M}}_{=b_{M-1}\circ a_{M}}\left(  k\right)   &  =\left(
b_{M-1}\circ a_{M}\right)  \left(  k\right)  =b_{M-1}\left(  \underbrace{a_{M}%
\left(  k\right)  }_{=k-1}\right)  =b_{M-1}\left(  k-1\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M-\left(  k-1\right)  ,\ \ \ \ \ \ \ \ \ \ \text{if }k-1\leq M-1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }k-1>M-1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{sol.ps2.2.4.c.bik.pf.hyp}), applied to }k-1\\
\text{instead of }k
\end{array}
\right) \\
&  =M-\left(  k-1\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k-1\leq
M-1\text{ (since }k\leq M\text{)}\right) \\
&  =M+1-k=\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =M+1-k\text{ (because }k\leq M\text{)}\right)  .
\end{align*}
Thus, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in Case 2.
\par
Let us finally consider Case 3. In this case, we have $k>M$. Now,
(\ref{sol.ps2.2.4.c.aik}) (applied to $i=M$) yields $a_{M}\left(  k\right)
=\left\{
\begin{array}
[c]{c}%
M,\ \ \ \ \ \ \ \ \ \ \text{if }k=1;\\
k-1,\ \ \ \ \ \ \ \ \ \ \text{if }1<k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =k$ (since $k>M$), so that%
\begin{align*}
\underbrace{b_{M}}_{=b_{M-1}\circ a_{M}}\left(  k\right)   &  =\left(
b_{M-1}\circ a_{M}\right)  \left(  k\right)  =b_{M-1}\left(  \underbrace{a_{M}%
\left(  k\right)  }_{=k}\right)  =b_{M-1}\left(  k\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M-1;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M-1
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.4.c.bik.pf.hyp}%
)}\right) \\
&  =k\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k>M>M-1\right) \\
&  =\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{
\begin{array}
[c]{c}%
M+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq M;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>M
\end{array}
\right.  =k\text{ (because }k>M\text{)}\right)  .
\end{align*}
Thus, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in Case 3.
\par
Hence, (\ref{sol.ps2.2.4.c.bik.pf.step}) is proven in each of the three Cases
1, 2 and 3. Since these three Cases are the only possible cases, this shows
that (\ref{sol.ps2.2.4.c.bik.pf.step}) always holds, qed.}. In other words,
(\ref{sol.ps2.2.4.c.bik}) holds for $m=M$. This completes the induction step.
Thus, the induction proof of (\ref{sol.ps2.2.4.c.bik}) is complete.

Now, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align*}
b_{n}\left(  k\right)   &  =\left\{
\begin{array}
[c]{c}%
n+1-k,\ \ \ \ \ \ \ \ \ \ \text{if }k\leq n;\\
k,\ \ \ \ \ \ \ \ \ \ \text{if }k>n
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.4.c.bik}),
applied to }m=n\right) \\
&  =n+1-k\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\leq n\right) \\
&  =w_{0}\left(  k\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
w_{0}\left(  k\right)  =n+1-k\text{ (by the definition of }w_{0}%
\text{)}\right)  .
\end{align*}
In other words, $b_{n}=w_{0}$, so that $w_{0}=b_{n}=a_{1}\circ a_{2}%
\circ\cdots\circ a_{n}$. This proves (\ref{sol.ps2.2.4.c.claim}). As we know,
this solves Exercise \ref{exe.ps2.2.4} \textbf{(c)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.5}}

Let us first state Exercise \ref{exe.ps2.2.5} \textbf{(d)} as a separate result:

\begin{proposition}
\label{prop.sol.exe.ps2.2.5.d}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ be a
permutation satisfying $\sigma\left(  1\right)  \leq\sigma\left(  2\right)
\leq\cdots\leq\sigma\left(  n\right)  $. Then, $\sigma=\operatorname*{id}$.
\end{proposition}

Proposition \ref{prop.sol.exe.ps2.2.5.d} essentially says that the only way to
list the numbers $1,2,\ldots,n$ in increasing order is $\left(  1,2,\ldots
,n\right)  $. If you think this is intuitively obvious, you are right. Let me
nevertheless give two proofs (the second of which is completely formal):

\begin{proof}
[First proof of Proposition \ref{prop.sol.exe.ps2.2.5.d}.]First of all, every
$k\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies $\sigma\left(  k\right)
<\sigma\left(  k+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $\sigma$ is a permutation, thus
injective. Hence, $\sigma\left(  k\right)  \neq\sigma\left(  k+1\right)  $
(since $k\neq k+1$). Combined with $\sigma\left(  k\right)  \leq\sigma\left(
k+1\right)  $ (since $\sigma\left(  1\right)  \leq\sigma\left(  2\right)
\leq\cdots\leq\sigma\left(  n\right)  $), this yields $\sigma\left(  k\right)
<\sigma\left(  k+1\right)  $, qed.}. In other words, $\sigma\left(  1\right)
<\sigma\left(  2\right)  <\cdots<\sigma\left(  n\right)  $.

Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\sigma\left(  i\right)  $ is
the $i$-th smallest among the numbers \newline$\sigma\left(  1\right)
,\sigma\left(  2\right)  ,\ldots,\sigma\left(  n\right)  $ (because
$\sigma\left(  1\right)  <\sigma\left(  2\right)  <\cdots<\sigma\left(
n\right)  $). But since the numbers $\sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  n\right)  $ are just the numbers
$1,2,\ldots,n$ (possibly in a different order)\footnote{since $\sigma$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $}, it is clear that the $i$-th
smallest among these numbers is $i$. Thus, $\sigma\left(  i\right)  =i$ (since
$\sigma\left(  i\right)  $ is the $i$-th smallest among the numbers
$\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  $). Hence, $\sigma\left(  i\right)  =i=\operatorname*{id}\left(
i\right)  $.

Let us now forget that we fixed $i$. Thus, we have shown that $\sigma\left(
i\right)  =\operatorname*{id}\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,n\right\}  $. In other words, $\sigma=\operatorname*{id}$.
Proposition \ref{prop.sol.exe.ps2.2.5.d} is thus proven.
\end{proof}

\begin{proof}
[Second proof of Proposition \ref{prop.sol.exe.ps2.2.5.d}.]We shall show that
\begin{equation}
\sigma\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,n\right\}  . \label{sol.ps2.2.5.d.1}%
\end{equation}


\textit{Proof of (\ref{sol.ps2.2.5.d.1}):} We shall prove
(\ref{sol.ps2.2.5.d.1}) by strong induction over $i$. Thus, we fix some
$I\in\left\{  1,2,\ldots,n\right\}  $, and we assume that
(\ref{sol.ps2.2.5.d.1}) is proven for every $i<I$. We then have to prove that
(\ref{sol.ps2.2.5.d.1}) holds for $i=I$.

We have assumed that (\ref{sol.ps2.2.5.d.1}) is proven for every $i<I$. In
other words,%
\begin{equation}
\sigma\left(  i\right)  =i\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }i<I. \label{sol.ps2.2.5.d.1.pf.1}%
\end{equation}


We assume (for the sake of contradiction) that $\sigma\left(  I\right)  \neq
I$. But $\sigma$ is a permutation, and thus injective. Hence, from
$\sigma\left(  I\right)  \neq I$, we obtain $\sigma\left(  \sigma\left(
I\right)  \right)  \neq\sigma\left(  I\right)  $. But if $\sigma\left(
I\right)  <I$, then $\sigma\left(  \sigma\left(  I\right)  \right)
=\sigma\left(  I\right)  $ (by (\ref{sol.ps2.2.5.d.1.pf.1}), applied to
$i=\sigma\left(  I\right)  $), which contradicts $\sigma\left(  \sigma\left(
I\right)  \right)  \neq\sigma\left(  I\right)  $. Hence, we cannot have
$\sigma\left(  I\right)  <I$. Thus, we have $\sigma\left(  I\right)  \geq I$.
Combined with $\sigma\left(  I\right)  \neq I$, this yields $\sigma\left(
I\right)  >I$.

Now, let $K=\sigma^{-1}\left(  I\right)  $. Then, $I=\sigma\left(  K\right)
$, so that $\sigma\left(  K\right)  =I\neq\sigma\left(  I\right)  $ and
therefore $K\neq I$. If $K<I$, then $\sigma\left(  K\right)  =K$ (by
(\ref{sol.ps2.2.5.d.1.pf.1}), applied to $i=K$), which contradicts
$\sigma\left(  K\right)  =I\neq K$. Hence, we cannot have $K<I$. We thus have
$I\leq K$.

Now, recall that $\sigma\left(  1\right)  \leq\sigma\left(  2\right)
\leq\cdots\leq\sigma\left(  n\right)  $. In other words, $\sigma\left(
a\right)  \leq\sigma\left(  b\right)  $ for every two elements $a$ and $b$ of
$\left\{  1,2,\ldots,n\right\}  $ satisfying $a\leq b$. Applying this to $a=I$
and $b=K$, we obtain $\sigma\left(  I\right)  \leq\sigma\left(  K\right)  $.
This contradicts $\sigma\left(  I\right)  >I=\sigma\left(  K\right)  $. This
contradiction proves that our assumption (that $\sigma\left(  I\right)  \neq
I$) was wrong. Hence, we must have $\sigma\left(  I\right)  =I$. In other
words, (\ref{sol.ps2.2.5.d.1}) holds for $i=I$. This completes our inductive
proof of (\ref{sol.ps2.2.5.d.1}).

Now, (\ref{sol.ps2.2.5.d.1}) shows that every $i\in\left\{  1,2,\ldots
,n\right\}  $ satisfies $\sigma\left(  i\right)  =i=\operatorname*{id}\left(
i\right)  $. In other words, $\sigma=\operatorname*{id}$. Proposition
\ref{prop.sol.exe.ps2.2.5.d} is solved again.
\end{proof}

For future use, let us record an easy consequence of Proposition
\ref{prop.sol.exe.ps2.2.5.d}:

\begin{corollary}
\label{cor.sol.exe.ps2.2.5.d2}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ be a
permutation satisfying $\ell\left(  \sigma\right)  =0$. Then, $\sigma
=\operatorname*{id}$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.sol.exe.ps2.2.5.d2}.]Let $k\in\left\{
1,2,\ldots,n-1\right\}  $. Assume (for the sake of contradiction) that
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $. Then, $\left(
k,k+1\right)  $ is a pair of integers satisfying $1\leq k<k+1\leq n$ and
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $. In other words,
$\left(  k,k+1\right)  $ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. In other words, $\left(  k,k+1\right)  $ is an inversion of
$\sigma$ (by the definition of an \textquotedblleft
inversion\textquotedblright). Thus, the permutation $\sigma$ has at least one
inversion (namely, $\left(  k,k+1\right)  $).

But the number of inversions of $\sigma$ is $\ell\left(  \sigma\right)  =0$;
in other words, $\sigma$ has no inversions. This contradicts the fact that
$\sigma$ has at least one inversion. This contradiction proves that our
assumption (that $\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $) was
wrong. Hence, we have $\sigma\left(  k\right)  \leq\sigma\left(  k+1\right)  $.

Now, let us forget that we fixed $k$. We thus have shown that $\sigma\left(
k\right)  \leq\sigma\left(  k+1\right)  $ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. Thus, $\sigma\left(  1\right)  \leq\sigma\left(
2\right)  \leq\cdots\leq\sigma\left(  n\right)  $. Therefore, Proposition
\ref{prop.sol.exe.ps2.2.5.d} shows that $\sigma=\operatorname*{id}$. This
proves Corollary \ref{cor.sol.exe.ps2.2.5.d2}.
\end{proof}

Now, we come to the actual solution of Exercise \ref{exe.ps2.2.5}.

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.5}.]Exercise \ref{exe.ps2.2.5}
\textbf{(d)} follows immediately from Proposition \ref{prop.sol.exe.ps2.2.5.d}%
. We shall next prove part \textbf{(f)} of the exercise, then part
\textbf{(a)}, then part \textbf{(e)}, and then the remaining three parts.

Before we come to the actual solution, let us introduce one more notation.

For every $\sigma\in S_{n}$, let $\operatorname*{Inv}\left(  \sigma\right)  $
be the set of all inversions of the permutation $\sigma$. Thus, for every
$\sigma\in S_{n}$, we have%
\begin{align}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \nonumber\\
&  =\left(  \text{the number of elements of }\operatorname*{Inv}\left(
\sigma\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\sigma\right)  \text{ is the set of all inversions of }\sigma\right)
\nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\label{sol.ps2.2.5.f.1}%
\end{align}


\textbf{(d)} Let $\sigma\in S_{n}$ be a permutation satisfying $\sigma\left(
1\right)  \leq\sigma\left(  2\right)  \leq\cdots\leq\sigma\left(  n\right)  $.
Then, Proposition \ref{prop.sol.exe.ps2.2.5.d} shows that $\sigma
=\operatorname*{id}$. This solves Exercise \ref{exe.ps2.2.5} \textbf{(d)}.

\textbf{(f)} Let $\sigma\in S_{n}$. For every $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  $, we have $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  \in\operatorname*{Inv}\left(
\sigma^{-1}\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $\left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $. Then, $\left(
i,j\right)  $ is an inversion of $\sigma$ (since $\operatorname*{Inv}\left(
\sigma\right)  $ is the set of all inversions of $\sigma$). In other words,
$\left(  i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright). Hence,
$\sigma\left(  j\right)  <\sigma\left(  i\right)  $, so that $1\leq
\sigma\left(  j\right)  <\sigma\left(  i\right)  \leq n$; also, $\sigma
^{-1}\left(  \sigma\left(  i\right)  \right)  =i<j=\sigma^{-1}\left(
\sigma\left(  j\right)  \right)  $, so that $\sigma^{-1}\left(  \sigma\left(
j\right)  \right)  >\sigma^{-1}\left(  \sigma\left(  i\right)  \right)  $.
Therefore, $\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)
$ is a pair of integers $\left(  u,v\right)  $ satisfying $1\leq u<v\leq n$
and $\sigma^{-1}\left(  u\right)  >\sigma^{-1}\left(  v\right)  $ (since
$1\leq\sigma\left(  j\right)  <\sigma\left(  i\right)  \leq n$ and
$\sigma^{-1}\left(  \sigma\left(  j\right)  \right)  >\sigma^{-1}\left(
\sigma\left(  i\right)  \right)  $). In other words, $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  $ is an inversion of $\sigma^{-1}$
(because inversions of $\sigma^{-1}$ are defined as pairs of integers $\left(
u,v\right)  $ satisfying $1\leq u<v\leq n$ and $\sigma^{-1}\left(  u\right)
>\sigma^{-1}\left(  v\right)  $). In other words, $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  \in\operatorname*{Inv}\left(
\sigma^{-1}\right)  $ (since $\operatorname*{Inv}\left(  \sigma^{-1}\right)  $
is the set of all inversions of $\sigma^{-1}$), qed.}. Hence, we can define a
map%
\begin{align*}
\Phi:\operatorname*{Inv}\left(  \sigma\right)   &  \rightarrow
\operatorname*{Inv}\left(  \sigma^{-1}\right)  ,\\
\left(  i,j\right)   &  \mapsto\left(  \sigma\left(  j\right)  ,\sigma\left(
i\right)  \right)  .
\end{align*}
This map $\Phi$ is injective\footnote{\textit{Proof.} We simply need to prove
that an element $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  $ can be reconstructed from its image $\left(  \sigma\left(
j\right)  ,\sigma\left(  i\right)  \right)  $. But this is easy: If you know
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  $, then you
know $\sigma\left(  j\right)  $ and $\sigma\left(  i\right)  $, and therefore
also $i$ (since $i=\sigma^{-1}\left(  \sigma\left(  i\right)  \right)  $) and
$j$ (since $j=\sigma^{-1}\left(  \sigma\left(  j\right)  \right)  $), and thus
also $\left(  i,j\right)  $.}. Thus, we have found an injective map from
$\operatorname*{Inv}\left(  \sigma\right)  $ to $\operatorname*{Inv}\left(
\sigma^{-1}\right)  $. Conversely, $\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert \leq\left\vert \operatorname*{Inv}\left(
\sigma^{-1}\right)  \right\vert $. But $\ell\left(  \sigma^{-1}\right)
=\left\vert \operatorname*{Inv}\left(  \sigma^{-1}\right)  \right\vert $ (by
(\ref{sol.ps2.2.5.f.1}), applied to $\sigma^{-1}$ instead of $\sigma$). Now,
(\ref{sol.ps2.2.5.f.1}) yields $\ell\left(  \sigma\right)  =\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert \leq\left\vert
\operatorname*{Inv}\left(  \sigma^{-1}\right)  \right\vert =\ell\left(
\sigma^{-1}\right)  $.

Now, let us forget that we fixed $\sigma$. We thus have proven that
\begin{equation}
\ell\left(  \sigma\right)  \leq\ell\left(  \sigma^{-1}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}. \label{sol.ps2.2.5.f.6}%
\end{equation}


Now, let $\sigma\in S_{n}$ again. We can apply (\ref{sol.ps2.2.5.f.6}) to
$\sigma^{-1}$ instead of $\sigma$, and thus obtain $\ell\left(  \sigma
^{-1}\right)  \leq\ell\left(  \underbrace{\left(  \sigma^{-1}\right)  ^{-1}%
}_{=\sigma}\right)  =\ell\left(  \sigma\right)  $. Combined with
(\ref{sol.ps2.2.5.f.6}), this yields $\ell\left(  \sigma\right)  =\ell\left(
\sigma^{-1}\right)  $. This solves Exercise \ref{exe.ps2.2.5} \textbf{(f)}.

\textbf{(a)} As I warned above, this solution will be a tedious formalization
of the argument sketched in Example \ref{exa.2.5}.

Let us first show a very simple fact: If $u$ and $v$ are two integers such
that $1\leq u<v\leq n$, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $ is such
that $\left(  u,v\right)  \neq\left(  k,k+1\right)  $, then%
\begin{equation}
s_{k}\left(  u\right)  <s_{k}\left(  v\right)  \label{sol.ps2.2.5.a.sk-inc}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps2.2.5.a.sk-inc}):} We can prove
(\ref{sol.ps2.2.5.a.sk-inc}) by analyzing three cases (Case 1 is when $u=k$,
Case 2 is when $u=k+1$, and Case 3 is when $u\notin\left\{  k,k+1\right\}  $),
each of which can be split into three subcases (Subcase 1 is when $v=k$,
Subcase 2 is when $v=k+1$, and Subcase 3 is when $v\notin\left\{
k,k+1\right\}  $). These are (altogether) nine subcases, but four of them
(namely, Subcases 1 and 2 in Case 1, and Subcases 1 and 2 in Case 2) are
impossible (because $u<v$ and $\left(  u,v\right)  \neq\left(  k,k+1\right)
$), and the proof of (\ref{sol.ps2.2.5.a.sk-inc}) is easy in the remaining
five subcases.
\par
Here is a smarter way to prove (\ref{sol.ps2.2.5.a.sk-inc}): Let $u$ and $v$
be two integers such that $1\leq u<v\leq n$. Let $k\in\left\{  1,2,\ldots
,n-1\right\}  $ be such that $\left(  u,v\right)  \neq\left(  k,k+1\right)  $.
We need to prove (\ref{sol.ps2.2.5.a.sk-inc}). Indeed, assume the contrary.
Thus, $s_{k}\left(  u\right)  \geq s_{k}\left(  v\right)  $.
\par
But $u<v$ and thus $u\neq v$. The map $s_{k}$ is a permutation, thus
bijective, and therefore injective. Hence, $s_{k}\left(  u\right)  \neq
s_{k}\left(  v\right)  $ (since $u\neq v$). Combined with $s_{k}\left(
u\right)  \geq s_{k}\left(  v\right)  $, this yields $s_{k}\left(  u\right)
>s_{k}\left(  v\right)  $. Thus, $s_{k}\left(  u\right)  \geq s_{k}\left(
v\right)  +1$ (since $s_{k}\left(  u\right)  $ and $s_{k}\left(  v\right)  $
are integers), so that $s_{k}\left(  v\right)  +1\leq s_{k}\left(  u\right)
$.
\par
We have $u<v$ and thus $u+1\leq v$ (since $u$ and $v$ are integers).
\par
Recall that $s_{k}$ is the permutation in $S_{n}$ which switches $k$ and
$k+1$, while leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $
unchanged. Hence,
\begin{equation}
s_{k}\left(  p\right)  \leq p+1\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  , \label{sol.ps2.2.5.a.sk-inc.pf.1}%
\end{equation}
and this inequality becomes an equality only for $p=k$. For the same reason,
we have%
\begin{equation}
s_{k}\left(  p\right)  \geq p-1\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,n\right\}  , \label{sol.ps2.2.5.a.sk-inc.pf.2}%
\end{equation}
and this inequality becomes an equality only for $p=k+1$.
\par
Applying (\ref{sol.ps2.2.5.a.sk-inc.pf.1}) to $p=u$, we obtain $s_{k}\left(
u\right)  \leq u+1$. Applying (\ref{sol.ps2.2.5.a.sk-inc.pf.2}) to $p=v$, we
obtain $s_{k}\left(  v\right)  \geq v-1$, so that $v-1\leq s_{k}\left(
v\right)  $ and thus $v\leq s_{k}\left(  v\right)  +1\leq s_{k}\left(
u\right)  \leq u+1\leq v$.
\par
Combining $v\leq s_{k}\left(  v\right)  +1$ with $s_{k}\left(  v\right)
+1\leq v$, we obtain $v=s_{k}\left(  v\right)  +1$, so that $s_{k}\left(
v\right)  =v-1$. In other words, $s_{k}\left(  p\right)  =p-1$ holds for
$p=v$. But recall that the inequality (\ref{sol.ps2.2.5.a.sk-inc.pf.2})
becomes an equality only for $p=k+1$. In other words, $s_{k}\left(  p\right)
=p-1$ holds only for $p=k+1$. Applying this to $p=v$, we obtain $v=k+1$ (since
$s_{k}\left(  p\right)  =p-1$ holds for $p=v$).
\par
Combining $s_{k}\left(  u\right)  \leq u+1$ with $u+1\leq v\leq s_{k}\left(
u\right)  $, we obtain $s_{k}\left(  u\right)  =u+1$. In other words,
$s_{k}\left(  p\right)  =p+1$ holds for $p=u$. But recall that the inequality
(\ref{sol.ps2.2.5.a.sk-inc.pf.1}) becomes an equality only for $p=k$. In other
words, $s_{k}\left(  p\right)  =p+1$ holds only for $p=k$. Applying this to
$p=u$, we obtain $u=k$ (since $s_{k}\left(  p\right)  =p+1$ holds for $p=u$).
\par
Now, $\left(  \underbrace{u}_{=k},\underbrace{v}_{=k+1}\right)  =\left(
k,k+1\right)  $ contradicts $\left(  u,v\right)  \neq\left(  k,k+1\right)  $.
This contradiction proves that our assumption was wrong. Hence,
(\ref{sol.ps2.2.5.a.sk-inc}) is proven.}.

We shall now show that%
\begin{align}
&  \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\} \nonumber\\
&  =\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  \label{sol.ps2.2.5.a.equalsets}%
\end{align}
for every $\sigma\in S_{n}$ and $k\in\left\{  1,2,\ldots,n-1\right\}  $.

[Notice that we do not necessarily have $\left(  \sigma^{-1}\left(
k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \in\operatorname*{Inv}%
\left(  \sigma\right)  $; nor do we always have $\left(  \sigma^{-1}\left(
k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)  \in\operatorname*{Inv}%
\left(  s_{k}\circ\sigma\right)  $. In fact, for each given $\sigma$ and $k$,
exactly one of these two statements holds. But we can form the difference
$A\setminus B$ of two sets $A$ and $B$ even if $B$ is not a subset of $A$, so
the statement (\ref{sol.ps2.2.5.a.equalsets}) still makes sense.]

\textit{Proof of (\ref{sol.ps2.2.5.a.equalsets}):} Let $\sigma\in S_{n}$ and
$k\in\left\{  1,2,\ldots,n-1\right\}  $. Let \newline$\left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  $. Thus, $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  $ and \newline$\left(  i,j\right)  \neq\left(  \sigma
^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  $. Therefore,
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  \neq\left(
k,k+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  =\left(
k,k+1\right)  $. Hence, $\sigma\left(  j\right)  =k$ and $\sigma\left(
i\right)  =k+1$. Hence, $\left(  \underbrace{\sigma^{-1}\left(  k+1\right)
}_{\substack{=i\\\text{(since }\sigma\left(  i\right)  =k+1\text{)}%
}},\underbrace{\sigma^{-1}\left(  k\right)  }_{\substack{=j\\\text{(since
}\sigma\left(  j\right)  =k\text{)}}}\right)  =\left(  i,j\right)  \neq\left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  $, which
is absurd. Hence, we have found a contradiction, so that our assumption was
wrong, qed.}.

We have $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $.
In other words, $\left(  i,j\right)  $ is an inversion of $\sigma$. In other
words, $\left(  i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq
n$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. Now,
$\sigma\left(  j\right)  <\sigma\left(  i\right)  $, so that $1\leq
\sigma\left(  j\right)  <\sigma\left(  i\right)  \leq n$. Also, as we know,
$\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  \neq\left(
k,k+1\right)  $. Hence, $s_{k}\left(  \sigma\left(  j\right)  \right)
<s_{k}\left(  \sigma\left(  i\right)  \right)  $ (by
(\ref{sol.ps2.2.5.a.sk-inc}), applied to $u=\sigma\left(  j\right)  $ and
$v=\sigma\left(  i\right)  $). Thus, $\left(  s_{k}\circ\sigma\right)  \left(
j\right)  =s_{k}\left(  \sigma\left(  j\right)  \right)  <s_{k}\left(
\sigma\left(  i\right)  \right)  =\left(  s_{k}\circ\sigma\right)  \left(
i\right)  $, hence $\left(  s_{k}\circ\sigma\right)  \left(  i\right)
>\left(  s_{k}\circ\sigma\right)  \left(  j\right)  $. Hence, $\left(
i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and $\left(
s_{k}\circ\sigma\right)  \left(  i\right)  >\left(  s_{k}\circ\sigma\right)
\left(  j\right)  $. In other words, $\left(  i,j\right)  $ is an inversion of
$s_{k}\circ\sigma$ (by the definition of an inversion). In other words,
$\left(  i,j\right)  \in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $
(since $\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $ is defined as
the set of all inversions of $s_{k}\circ\sigma$). Furthermore, $\left(
i,j\right)  \neq\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(
k+1\right)  \right)  $\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, $\left(  i,j\right)  =\left(  \sigma^{-1}\left(  k\right)  ,\sigma
^{-1}\left(  k+1\right)  \right)  $. Thus, $i=\sigma^{-1}\left(  k\right)  $
and $j=\sigma^{-1}\left(  k+1\right)  $. Hence, $\sigma\left(  i\right)  =k$
(since $i=\sigma^{-1}\left(  k\right)  $) and $\sigma\left(  j\right)  =k+1$
(since $j=\sigma^{-1}\left(  k+1\right)  $). Now, $k+1=\sigma\left(  j\right)
<\sigma\left(  i\right)  =k<k+1$, which is absurd. Thus, we have found a
contradiction, so that our assumption must have been wrong, qed.}. Thus,
$\left(  i,j\right)  \in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)
\setminus\left\{  \left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(
k+1\right)  \right)  \right\}  $ (since $\left(  i,j\right)  \in
\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $ and $\left(  i,j\right)
\neq\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  $).

Now, let us forget that we fixed $\left(  i,j\right)  $. We thus have shown
that every $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\setminus\left\{  \left(  \sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(
k\right)  \right)  \right\}  $ satisfies \newline$\left(  i,j\right)
\in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  $. In other words,%
\begin{align}
&  \operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \right\}
\nonumber\\
&  \subseteq\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus
\left\{  \left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  \right\}  . \label{sol.ps2.2.5.a.equalsets.pf.1}%
\end{align}


Now, let $\tau=s_{k}\circ\sigma$. Then, $s_{k}\circ\underbrace{\tau}%
_{=s_{k}\circ\sigma}=\underbrace{s_{k}\circ s_{k}}_{=s_{k}^{2}%
=\operatorname*{id}}\circ\sigma=\operatorname*{id}\circ\sigma=\sigma$.
Moreover, $\tau^{-1}\left(  k\right)  =\sigma^{-1}\left(  k+1\right)
$\ \ \ \ \footnote{\textit{Proof.} We have $\underbrace{\tau}_{=s_{k}%
\circ\sigma}\left(  \sigma^{-1}\left(  k+1\right)  \right)  =\left(
s_{k}\circ\sigma\right)  \left(  \sigma^{-1}\left(  k+1\right)  \right)
=s_{k}\left(  \underbrace{\sigma\left(  \sigma^{-1}\left(  k+1\right)
\right)  }_{=k+1}\right)  =s_{k}\left(  k+1\right)  =k$ (by the definition of
$s_{k}$). Thus, $\tau^{-1}\left(  k\right)  =\sigma^{-1}\left(  k+1\right)  $,
qed.} and $\tau^{-1}\left(  k+1\right)  =\sigma^{-1}\left(  k\right)
$\ \ \ \ \footnote{for similar reasons}.

But recall that we have proven (\ref{sol.ps2.2.5.a.equalsets.pf.1}). The same
arguments, but carried out for $\tau$ instead of $\sigma$, show that%
\begin{align*}
&  \operatorname*{Inv}\left(  \tau\right)  \setminus\left\{  \left(  \tau
^{-1}\left(  k+1\right)  ,\tau^{-1}\left(  k\right)  \right)  \right\} \\
&  \subseteq\operatorname*{Inv}\left(  s_{k}\circ\tau\right)  \setminus
\left\{  \left(  \tau^{-1}\left(  k\right)  ,\tau^{-1}\left(  k+1\right)
\right)  \right\}  .
\end{align*}
Using the identities $s_{k}\circ\tau=\sigma$, $\tau^{-1}\left(  k\right)
=\sigma^{-1}\left(  k+1\right)  $ and $\tau^{-1}\left(  k+1\right)
=\sigma^{-1}\left(  k\right)  $, we can rewrite this as follows:%
\begin{align*}
&  \operatorname*{Inv}\left(  \tau\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \right\}
\\
&  \subseteq\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  .
\end{align*}
Since $\tau=s_{k}\circ\sigma$, this further rewrites as follows:%
\begin{align*}
&  \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\} \\
&  \subseteq\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  .
\end{align*}
Combining this with (\ref{sol.ps2.2.5.a.equalsets.pf.1}), we obtain%
\begin{align*}
&  \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\} \\
&  =\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  .
\end{align*}
This proves (\ref{sol.ps2.2.5.a.equalsets}).

Now, let $k\in\left\{  1,2,\ldots,n-1\right\}  $.

We shall first show that for every $\sigma\in S_{n}$, we have%
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)
+1\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(  k\right)  <\sigma
^{-1}\left(  k+1\right)  . \label{sol.ps2.2.5.a.part1}%
\end{equation}


\textit{Proof of (\ref{sol.ps2.2.5.a.part1}):} Let $\sigma\in S_{n}$. Assume
that $\sigma^{-1}\left(  k\right)  <\sigma^{-1}\left(  k+1\right)  $. Then,
\newline$\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  $ is a pair of integers satisfying $1\leq\sigma^{-1}\left(  k\right)
<\sigma^{-1}\left(  k+1\right)  \leq n$ and $\left(  s_{k}\circ\sigma\right)
\left(  \sigma^{-1}\left(  k\right)  \right)  >\left(  s_{k}\circ
\sigma\right)  \left(  \sigma^{-1}\left(  k+1\right)  \right)  $%
\ \ \ \ \footnote{because $\left(  s_{k}\circ\sigma\right)  \left(
\sigma^{-1}\left(  k\right)  \right)  =s_{k}\left(  \underbrace{\sigma\left(
\sigma^{-1}\left(  k\right)  \right)  }_{=k}\right)  =s_{k}\left(  k\right)
=k+1$ and similarly $\left(  s_{k}\circ\sigma\right)  \left(  \sigma
^{-1}\left(  k+1\right)  \right)  =k$, so that $\left(  s_{k}\circ
\sigma\right)  \left(  \sigma^{-1}\left(  k\right)  \right)  =k+1>k=\left(
s_{k}\circ\sigma\right)  \left(  \sigma^{-1}\left(  k+1\right)  \right)  $}.
In other words, \newline$\left(  \sigma^{-1}\left(  k\right)  ,\sigma
^{-1}\left(  k+1\right)  \right)  $ is an inversion of $s_{k}\circ\sigma$. In
other words, $\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(
k+1\right)  \right)  \in\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  $.
Hence,%
\begin{equation}
\left\vert \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus
\left\{  \left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)
\right)  \right\}  \right\vert =\left\vert \operatorname*{Inv}\left(
s_{k}\circ\sigma\right)  \right\vert -1 \label{sol.ps2.2.5.a.part1.pf.1}%
\end{equation}


On the other hand, $\left(  \sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(
k\right)  \right)  $ is not an inversion of $\sigma$ (because if it was an
inversion of $\sigma$, then we would have $1\leq\sigma^{-1}\left(  k+1\right)
<\sigma^{-1}\left(  k\right)  \leq n$ and therefore $\sigma^{-1}\left(
k+1\right)  <\sigma^{-1}\left(  k\right)  <\sigma^{-1}\left(  k+1\right)  $,
which would be absurd). In other words, $\left(  \sigma^{-1}\left(
k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)  \notin\operatorname*{Inv}%
\left(  \sigma\right)  $. Thus,%
\[
\operatorname*{Inv}\left(  \sigma\right)  \setminus\left\{  \left(
\sigma^{-1}\left(  k+1\right)  ,\sigma^{-1}\left(  k\right)  \right)
\right\}  =\operatorname*{Inv}\left(  \sigma\right)  ,
\]
so that%
\begin{align}
\operatorname*{Inv}\left(  \sigma\right)   &  =\operatorname*{Inv}\left(
\sigma\right)  \setminus\left\{  \left(  \sigma^{-1}\left(  k+1\right)
,\sigma^{-1}\left(  k\right)  \right)  \right\} \nonumber\\
&  =\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \setminus\left\{
\left(  \sigma^{-1}\left(  k\right)  ,\sigma^{-1}\left(  k+1\right)  \right)
\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.5.a.equalsets}%
)}\right)  . \label{sol.ps2.2.5.a.part1.pf.4}%
\end{align}
Now, (\ref{sol.ps2.2.5.f.1}) yields%
\begin{align}
\ell\left(  \sigma\right)   &  =\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert =\left\vert \operatorname*{Inv}\left(  s_{k}%
\circ\sigma\right)  \setminus\left\{  \left(  \sigma^{-1}\left(  k\right)
,\sigma^{-1}\left(  k+1\right)  \right)  \right\}  \right\vert
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.5.a.part1.pf.4})}\right)
\nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \right\vert
-1\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps2.2.5.a.part1.pf.1}%
)}\right)  . \label{sol.ps2.2.5.a.part1.pf.6}%
\end{align}
But (\ref{sol.ps2.2.5.f.1}) (applied to $s_{k}\circ\sigma$ instead of $\sigma
$) yields $\ell\left(  s_{k}\circ\sigma\right)  =\left\vert
\operatorname*{Inv}\left(  s_{k}\circ\sigma\right)  \right\vert $. Hence,
(\ref{sol.ps2.2.5.a.part1.pf.6}) becomes%
\[
\ell\left(  \sigma\right)  =\underbrace{\left\vert \operatorname*{Inv}\left(
s_{k}\circ\sigma\right)  \right\vert }_{=\ell\left(  s_{k}\circ\sigma\right)
}-1=\ell\left(  s_{k}\circ\sigma\right)  -1,
\]
so that $\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)  +1$.
This proves (\ref{sol.ps2.2.5.a.part1}).

Next, we will show that for every $\sigma\in S_{n}$, we have
\begin{equation}
\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)
-1\ \ \ \ \ \ \ \ \ \ \text{if }\sigma^{-1}\left(  k\right)  >\sigma
^{-1}\left(  k+1\right)  . \label{sol.ps2.2.5.a.part2}%
\end{equation}


\textit{Proof of (\ref{sol.ps2.2.5.a.part2}):} Let $\sigma\in S_{n}$. Assume
that $\sigma^{-1}\left(  k\right)  >\sigma^{-1}\left(  k+1\right)  $. But
$\sigma^{-1}\left(  k\right)  =\left(  s_{k}\circ\sigma\right)  ^{-1}\left(
k+1\right)  $\ \ \ \ \footnote{since $\left(  s_{k}\circ\sigma\right)  \left(
\sigma^{-1}\left(  k\right)  \right)  =s_{k}\left(  \underbrace{\sigma\left(
\sigma^{-1}\left(  k\right)  \right)  }_{=k}\right)  =s_{k}\left(  k\right)
=k+1$} and $\sigma^{-1}\left(  k+1\right)  =\left(  s_{k}\circ\sigma\right)
^{-1}\left(  k\right)  $\ \ \ \ \footnote{for similar reasons}. Thus, $\left(
s_{k}\circ\sigma\right)  ^{-1}\left(  k\right)  =\sigma^{-1}\left(
k+1\right)  <\sigma^{-1}\left(  k\right)  =\left(  s_{k}\circ\sigma\right)
^{-1}\left(  k+1\right)  $. Hence, we can apply (\ref{sol.ps2.2.5.a.part1}) to
$s_{k}\circ\sigma$ instead of $\sigma$. As a result, we obtain%
\[
\ell\left(  s_{k}\circ s_{k}\circ\sigma\right)  =\ell\left(  s_{k}\circ
\sigma\right)  +1.
\]
Since $\underbrace{s_{k}\circ s_{k}}_{=s_{k}^{2}=\operatorname*{id}}%
\circ\sigma=\operatorname*{id}\circ\sigma=\sigma$, this rewrites as
$\ell\left(  \sigma\right)  =\ell\left(  s_{k}\circ\sigma\right)  +1$, so that
$\ell\left(  s_{k}\circ\sigma\right)  =\ell\left(  \sigma\right)  -1$. This
proves (\ref{sol.ps2.2.5.a.part2}).

\begin{vershort}
Now, (\ref{eq.exe.2.5.a.2}) follows immediately by combining
(\ref{sol.ps2.2.5.a.part1}) with (\ref{sol.ps2.2.5.a.part2}).\footnote{The
term \textquotedblleft$\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  $\textquotedblright\ in (\ref{eq.exe.2.5.a.2}) makes sense because
every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies exactly one of the conditions $\sigma\left(  k\right)
<\sigma\left(  k+1\right)  $ and $\sigma\left(  k\right)  >\sigma\left(
k+1\right)  $. (Indeed, $\sigma\left(  k\right)  =\sigma\left(  k+1\right)  $
is impossible, because every permutation $\sigma\in S_{n}$ is injective.)}
\end{vershort}

\begin{verlong}
We will soon prove (\ref{eq.exe.2.5.a.1}) and (\ref{eq.exe.2.5.a.2}). Let us
first show that the right-hand sides of (\ref{eq.exe.2.5.a.1}) and
(\ref{eq.exe.2.5.a.2}) are always well-defined. Indeed, for every $\sigma\in
S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the right-hand side
of (\ref{eq.exe.2.5.a.1}) is well-defined\footnote{\textit{Proof.} Let
$\sigma\in S_{n}$ and $k\in\left\{  1,2,\ldots,n-1\right\}  $. The map
$\sigma$ is a permutation and thus injective. Hence, $\sigma\left(  k\right)
\neq\sigma\left(  k+1\right)  $ (since $k\neq k+1$). Thus, either
$\sigma\left(  k\right)  <\sigma\left(  k+1\right)  $ or $\sigma\left(
k\right)  >\sigma\left(  k+1\right)  $. More precisely, exactly one of the
conditions $\sigma\left(  k\right)  <\sigma\left(  k+1\right)  $ and
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $ is satisfied. Hence, the
right-hand side of (\ref{eq.exe.2.5.a.1}) is well-defined.}, and the
right-hand side of (\ref{eq.exe.2.5.a.2}) is
well-defined\footnote{\textit{Proof.} Let $\sigma\in S_{n}$ and $k\in\left\{
1,2,\ldots,n-1\right\}  $. The map $\sigma^{-1}$ is a permutation and thus
injective. Hence, $\sigma^{-1}\left(  k\right)  \neq\sigma^{-1}\left(
k+1\right)  $ (since $k\neq k+1$). Thus, either $\sigma^{-1}\left(  k\right)
<\sigma^{-1}\left(  k+1\right)  $ or $\sigma^{-1}\left(  k\right)
>\sigma^{-1}\left(  k+1\right)  $. More precisely, exactly one of the
conditions $\sigma^{-1}\left(  k\right)  <\sigma^{-1}\left(  k+1\right)  $ and
$\sigma^{-1}\left(  k\right)  >\sigma^{-1}\left(  k+1\right)  $ is satisfied.
Hence, the right-hand side of (\ref{eq.exe.2.5.a.2}) is well-defined.}.

Now, for every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots
,n-1\right\}  $, the equality (\ref{eq.exe.2.5.a.2}) holds (since it follows
immediately by combining (\ref{sol.ps2.2.5.a.part1}) with
(\ref{sol.ps2.2.5.a.part2})).
\end{verlong}

It remains to prove (\ref{eq.exe.2.5.a.1}). Indeed, let $\sigma\in S_{n}$. Let
us recall that $\left(  \alpha\circ\beta\right)  ^{-1}=\beta^{-1}\circ
\alpha^{-1}$ for any two permutations $\alpha$ and $\beta$ in $S_{n}$.
Applying this to $\alpha=s_{k}$ and $\beta=\sigma^{-1}$, we obtain $\left(
s_{k}\circ\sigma^{-1}\right)  ^{-1}=\underbrace{\left(  \sigma^{-1}\right)
^{-1}}_{=\sigma}\circ\underbrace{s_{k}^{-1}}_{=s_{k}}=\sigma\circ s_{k}$. But
Exercise \ref{exe.ps2.2.5} \textbf{(f)} yields $\ell\left(  \sigma\right)
=\ell\left(  \sigma^{-1}\right)  $. Also, Exercise \ref{exe.ps2.2.5}
\textbf{(f)} (applied to $s_{k}\circ\sigma^{-1}$ instead of $\sigma$) yields
$\ell\left(  s_{k}\circ\sigma^{-1}\right)  =\ell\left(  \underbrace{\left(
s_{k}\circ\sigma^{-1}\right)  ^{-1}}_{=\sigma\circ s_{k}}\right)  =\ell\left(
\sigma\circ s_{k}\right)  $. But applying (\ref{eq.exe.2.5.a.2}) to
$\sigma^{-1}$ instead of $\sigma$, we obtain%
\[
\ell\left(  s_{k}\circ\sigma^{-1}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma^{-1}\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\left(
\sigma^{-1}\right)  ^{-1}\left(  k\right)  <\left(  \sigma^{-1}\right)
^{-1}\left(  k+1\right)  ;\\
\ell\left(  \sigma^{-1}\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\left(
\sigma^{-1}\right)  ^{-1}\left(  k\right)  >\left(  \sigma^{-1}\right)
^{-1}\left(  k+1\right)
\end{array}
\right.  .
\]
Since $\ell\left(  s_{k}\circ\sigma^{-1}\right)  =\ell\left(  \sigma\circ
s_{k}\right)  $, $\ell\left(  \sigma^{-1}\right)  =\ell\left(  \sigma\right)
$ and $\left(  \sigma^{-1}\right)  ^{-1}=\sigma$, this equality rewrites as
follows:%
\[
\ell\left(  \sigma\circ s_{k}\right)  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right.  .
\]
This proves (\ref{eq.exe.2.5.a.1}), and thus completes the solution of
Exercise \ref{exe.ps2.2.5} \textbf{(a)}.

\textbf{(e)} We shall solve Exercise \ref{exe.ps2.2.5} \textbf{(e)} by
induction over $\ell\left(  \sigma\right)  $:

\textit{Induction base:} Exercise \ref{exe.ps2.2.5} \textbf{(e)} holds in the
case when $\ell\left(  \sigma\right)  =0$\ \ \ \ \footnote{\textit{Proof.} Let
$\sigma\in S_{n}$ be such that $\ell\left(  \sigma\right)  =0$. We need to
show that $\sigma$ can be written as a composition of $\ell\left(
\sigma\right)  $ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $).
\par
Recall that the composition of $0$ permutations in $S_{n}$ is
$\operatorname*{id}$ (by definition).
\par
We have $\ell\left(  \sigma\right)  =0$, and thus $\sigma=\operatorname*{id}$
(by Corollary \ref{cor.sol.exe.ps2.2.5.d2}). Therefore, $\sigma$ is a
composition of $0$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $) (because the composition of $0$ permutations in
$S_{n}$ is $\operatorname*{id}$). In other words, $\sigma$ is a composition of
$\ell\left(  \sigma\right)  $ permutations of the form $s_{k}$ (with
$k\in\left\{  1,2,\ldots,n-1\right\}  $) (since $\ell\left(  \sigma\right)
=0$). Thus, Exercise \ref{exe.ps2.2.5} \textbf{(e)} is solved in the case when
$\ell\left(  \sigma\right)  =0$, qed.}. This completes the induction base.

\textit{Induction step:} Let $L$ be a positive integer. Assume that Exercise
\ref{exe.ps2.2.5} \textbf{(e)} is solved in the case when $\ell\left(
\sigma\right)  =L-1$. We need to solve Exercise \ref{exe.ps2.2.5} \textbf{(e)}
in the case when $\ell\left(  \sigma\right)  =L$.

So let $\sigma\in S_{n}$ be such that $\ell\left(  \sigma\right)  =L$. We need
to show that $\sigma$ can be written as a composition of $\ell\left(
\sigma\right)  $ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $).

There exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$\sigma\left(  k\right)  >\sigma\left(  k+1\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, every
$k\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies $\sigma\left(  k\right)
\leq\sigma\left(  k+1\right)  $. In other words, $\sigma\left(  1\right)
\leq\sigma\left(  2\right)  \leq\cdots\leq\sigma\left(  n\right)  $. Exercise
\ref{exe.ps2.2.5} \textbf{(d)} yields $\sigma=\operatorname*{id}$. Hence,
$\ell\left(  \sigma\right)  =\ell\left(  \operatorname*{id}\right)  =0$, so
that $0=\ell\left(  \sigma\right)  =L$. This contradicts the fact that $L$ is
a positive integer. This contradiction shows that our assumption was wrong,
qed.}. Fix such a $k$, and denote it by $j$. Thus, $j$ is an element of
$\left\{  1,2,\ldots,n-1\right\}  $ and satisfies $\sigma\left(  j\right)
>\sigma\left(  j+1\right)  $. From (\ref{eq.exe.2.5.a.1}) (applied to $k=j$),
we obtain%
\begin{align*}
\ell\left(  \sigma\circ s_{j}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
j\right)  <\sigma\left(  j+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
j\right)  >\sigma\left(  j+1\right)
\end{array}
\right. \\
&  =\underbrace{\ell\left(  \sigma\right)  }_{=L}-1\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\sigma\left(  j\right)  >\sigma\left(  j+1\right)  \right) \\
&  =L-1.
\end{align*}
Hence, we can apply Exercise \ref{exe.ps2.2.5} \textbf{(e)} to $\sigma\circ
s_{j}$ instead of $\sigma$ (because we assumed that Exercise \ref{exe.ps2.2.5}
\textbf{(e)} is solved in the case when $\ell\left(  \sigma\right)  =L-1$). As
a result, we conclude that $\sigma\circ s_{j}$ can be written as a composition
of $\ell\left(  \sigma\circ s_{j}\right)  $ permutations of the form $s_{k}$
(with $k\in\left\{  1,2,\ldots,n-1\right\}  $). In other words, $\sigma\circ
s_{j}$ can be written as a composition of $L-1$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $) (since $\ell\left(
\sigma\circ s_{j}\right)  =L-1$). In other words, there exists an $\left(
L-1\right)  $-tuple $\left(  k_{1},k_{2},\ldots,k_{L-1}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{L-1}$ such that $\sigma\circ s_{j}=s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{L-1}}$. Consider this $\left(  k_{1}%
,k_{2},\ldots,k_{L-1}\right)  $.

We have $\sigma\circ\underbrace{s_{j}\circ s_{j}}_{=s_{j}^{2}%
=\operatorname*{id}}=\sigma$ and thus%
\[
\sigma=\underbrace{\sigma\circ s_{j}}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{L-1}}}\circ s_{j}=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{L-1}}\circ s_{j}.
\]
The right hand side of this equality is a composition of $L$ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). Thus,
$\sigma$ can be written as a composition of $L$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). In other words,
$\sigma$ can be written as a composition of $\ell\left(  \sigma\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$) (since $\ell\left(  \sigma\right)  =L$). This solves Exercise
\ref{exe.ps2.2.5} \textbf{(e)} in the case when $\ell\left(  \sigma\right)
=L$. The induction step is thus complete, and Exercise \ref{exe.ps2.2.5}
\textbf{(e)} is solved by induction.

\textbf{(b)} From (\ref{eq.exe.2.5.a.1}), we can easily conclude that%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  \equiv\ell\left(  \sigma\right)
+1\operatorname{mod}2 \label{sol.ps2.2.5.b.stepper}%
\end{equation}
for every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.b.stepper}):} Let $\sigma\in S_{n}$ and
$k\in\left\{  1,2,\ldots,n-1\right\}  $. From (\ref{eq.exe.2.5.a.1}), we
obtain%
\begin{align*}
\ell\left(  \sigma\circ s_{k}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \equiv\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\right)
-1\equiv\ell\left(  \sigma\right)  +1\operatorname{mod}2\text{ in the case
when }\sigma\left(  k\right)  >\sigma\left(  k+1\right)  \right) \\
&  =\ell\left(  \sigma\right)  +1\operatorname{mod}2.
\end{align*}
This proves (\ref{sol.ps2.2.5.b.stepper}).
\end{verlong}

Thus, using induction, it is easy to prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \equiv\ell\left(  \sigma\right)
+p\operatorname{mod}2 \label{sol.ps2.2.5.b.snake}%
\end{equation}
for every $\sigma\in S_{n}$, every $p\in\mathbb{N}$ and every $\left(
k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{p}$.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.b.snake}):} Let $\sigma\in S_{n}$, let
$p\in\mathbb{N}$ and let $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$. We shall prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{q}}\right)  \right)  \equiv\ell\left(  \sigma\right)
+q\operatorname{mod}2 \label{sol.ps2.2.5.b.snake.pf.1}%
\end{equation}
for all $q\in\left\{  0,1,\ldots,p\right\}  $.

Indeed, let us prove (\ref{sol.ps2.2.5.b.snake.pf.1}) by induction over $q$.

\textit{Induction base:} We have $\ell\left(  \sigma\circ\underbrace{\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  }_{=\left(
\text{empty composition}\right)  =\operatorname*{id}}\right)  =\ell\left(
\sigma\circ\operatorname*{id}\right)  =\ell\left(  \sigma\right)  =\ell\left(
\sigma\right)  +0\equiv\ell\left(  \sigma\right)  +0\operatorname{mod}2$. In
other words, (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=0$. This completes
the induction base.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,p\right\}  $ be
positive. Assume that (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q-1$. We
need to show that (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q$.

We have assumed that (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q-1$. In
other words, we have%
\[
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \right)  \equiv\ell\left(  \sigma\right)  +\left(
Q-1\right)  \operatorname{mod}2.
\]
Now,%
\begin{align*}
&  \ell\left(  \sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q}}\right)  }_{=\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q-1}}\right)  \circ s_{k_{Q}}}\right) \\
&  =\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \circ s_{k_{Q}}\right) \\
&  \equiv\underbrace{\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{Q-1}}\right)  \right)  }_{\equiv\ell\left(
\sigma\right)  +\left(  Q-1\right)  \operatorname{mod}2}+1\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{sol.ps2.2.5.b.stepper}), applied to }\sigma\circ\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{Q-1}}\right)  \text{ and }%
k_{Q}\\
\text{instead of }\sigma\text{ and }k
\end{array}
\right) \\
&  \equiv\ell\left(  \sigma\right)  +\left(  Q-1\right)  +1=\ell\left(
\sigma\right)  +Q\operatorname{mod}2.
\end{align*}
In other words, (\ref{sol.ps2.2.5.b.snake.pf.1}) holds for $q=Q$. This
completes the induction step. Thus, (\ref{sol.ps2.2.5.b.snake.pf.1}) is proven
by induction.

Now, applying (\ref{sol.ps2.2.5.b.snake.pf.1}) to $q=p$, we obtain
$\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \equiv\ell\left(  \sigma\right)
+p\operatorname{mod}2$. This proves (\ref{sol.ps2.2.5.b.snake}).
\end{verlong}

Now, let $\sigma$ and $\tau$ be two permutations in $S_{n}$. Exercise
\ref{exe.ps2.2.5} \textbf{(e)} (applied to $\tau$ instead of $\sigma$) yields
that $\tau$ can be written as a composition of $\ell\left(  \tau\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). In other words, there exists an $\ell\left(  \tau\right)  $-tuple $\left(
k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)  }\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{\ell\left(  \tau\right)  }$ such that $\tau
=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(  \tau\right)  }}$.
Consider this $\left(  k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)
}\right)  $. Then,%
\[
\ell\left(  \sigma\circ\underbrace{\tau}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{\ell\left(  \tau\right)  }}}\right)  =\ell\left(
\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(
\tau\right)  }}\right)  \right)  \equiv\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  \operatorname{mod}2
\]
(by (\ref{sol.ps2.2.5.b.snake}), applied to $p=\ell\left(  \tau\right)  $).
This solves Exercise \ref{exe.ps2.2.5} \textbf{(b)}.

\textbf{(c)} The solution of Exercise \ref{exe.ps2.2.5} \textbf{(c)} is mostly
parallel to our above solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}.

From (\ref{eq.exe.2.5.a.1}), we can easily conclude that%
\begin{equation}
\ell\left(  \sigma\circ s_{k}\right)  \leq\ell\left(  \sigma\right)  +1
\label{sol.ps2.2.5.c.stepper}%
\end{equation}
for every $\sigma\in S_{n}$ and every $k\in\left\{  1,2,\ldots,n-1\right\}  $.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.c.stepper}):} Let $\sigma\in S_{n}$ and
$k\in\left\{  1,2,\ldots,n-1\right\}  $. From (\ref{eq.exe.2.5.a.1}), we
obtain%
\begin{align*}
\ell\left(  \sigma\circ s_{k}\right)   &  =\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  -1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \leq\left\{
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  <\sigma\left(  k+1\right)  ;\\
\ell\left(  \sigma\right)  +1,\ \ \ \ \ \ \ \ \ \ \text{if }\sigma\left(
k\right)  >\sigma\left(  k+1\right)
\end{array}
\right. \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\right)
-1\leq\ell\left(  \sigma\right)  +1\text{ in the case when }\sigma\left(
k\right)  >\sigma\left(  k+1\right)  \right) \\
&  =\ell\left(  \sigma\right)  +1.
\end{align*}
This proves (\ref{sol.ps2.2.5.c.stepper}).
\end{verlong}

Thus, using induction, it is easy to prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \leq\ell\left(  \sigma\right)  +p
\label{sol.ps2.2.5.c.snake}%
\end{equation}
for every $\sigma\in S_{n}$, every $p\in\mathbb{N}$ and every $\left(
k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{p}$.

\begin{verlong}
\textit{Proof of (\ref{sol.ps2.2.5.c.snake}):} Let $\sigma\in S_{n}$, let
$p\in\mathbb{N}$ and let $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$. We shall prove that%
\begin{equation}
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{q}}\right)  \right)  \leq\ell\left(  \sigma\right)  +q
\label{sol.ps2.2.5.c.snake.pf.1}%
\end{equation}
for all $q\in\left\{  0,1,\ldots,p\right\}  $.

Indeed, let us prove (\ref{sol.ps2.2.5.c.snake.pf.1}) by induction over $q$.

\textit{Induction base:} We have $\ell\left(  \sigma\circ\underbrace{\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  }_{=\left(
\text{empty composition}\right)  =\operatorname*{id}}\right)  =\ell\left(
\sigma\circ\operatorname*{id}\right)  =\ell\left(  \sigma\right)  =\ell\left(
\sigma\right)  +0\leq\ell\left(  \sigma\right)  +0$. In other words,
(\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=0$. This completes the induction base.

\textit{Induction step:} Let $Q\in\left\{  0,1,\ldots,p\right\}  $ be
positive. Assume that (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q-1$. We
need to show that (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q$.

We have assumed that (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q-1$. In
other words, we have%
\[
\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \right)  \leq\ell\left(  \sigma\right)  +\left(
Q-1\right)  .
\]
Now,%
\begin{align*}
&  \ell\left(  \sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q}}\right)  }_{=\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{Q-1}}\right)  \circ s_{k_{Q}}}\right) \\
&  =\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{Q-1}}\right)  \circ s_{k_{Q}}\right) \\
&  \leq\underbrace{\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{Q-1}}\right)  \right)  }_{\leq\ell\left(
\sigma\right)  +\left(  Q-1\right)  }+1\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{sol.ps2.2.5.c.stepper}), applied to }\sigma\circ\left(
s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{Q-1}}\right)  \text{ and }%
k_{Q}\\
\text{instead of }\sigma\text{ and }k
\end{array}
\right) \\
&  \leq\ell\left(  \sigma\right)  +\left(  Q-1\right)  +1=\ell\left(
\sigma\right)  +Q.
\end{align*}
In other words, (\ref{sol.ps2.2.5.c.snake.pf.1}) holds for $q=Q$. This
completes the induction step. Thus, (\ref{sol.ps2.2.5.c.snake.pf.1}) is proven
by induction.

Now, applying (\ref{sol.ps2.2.5.c.snake.pf.1}) to $q=p$, we obtain
$\ell\left(  \sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  \right)  \leq\ell\left(  \sigma\right)  +p$. This proves
(\ref{sol.ps2.2.5.c.snake}).
\end{verlong}

Now, let $\sigma$ and $\tau$ be two permutations in $S_{n}$. Exercise
\ref{exe.ps2.2.5} \textbf{(e)} (applied to $\tau$ instead of $\sigma$) yields
that $\tau$ can be written as a composition of $\ell\left(  \tau\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). In other words, there exists an $\ell\left(  \tau\right)  $-tuple $\left(
k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)  }\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{\ell\left(  \tau\right)  }$ such that $\tau
=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(  \tau\right)  }}$.
Consider this $\left(  k_{1},k_{2},\ldots,k_{\ell\left(  \tau\right)
}\right)  $. Then,%
\[
\ell\left(  \sigma\circ\underbrace{\tau}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{\ell\left(  \tau\right)  }}}\right)  =\ell\left(
\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{\ell\left(
\tau\right)  }}\right)  \right)  \leq\ell\left(  \sigma\right)  +\ell\left(
\tau\right)
\]
(by (\ref{sol.ps2.2.5.c.snake}), applied to $p=\ell\left(  \tau\right)  $).
This solves Exercise \ref{exe.ps2.2.5} \textbf{(c)}.

\textbf{(g)} Exercise \ref{exe.ps2.2.5} \textbf{(e)} shows that $\sigma$ can
be written as a composition of $\ell\left(  \sigma\right)  $ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). In other
words, $\ell\left(  \sigma\right)  $ is an $N\in\mathbb{N}$ such that $\sigma$
can be written as a composition of $N$ permutations of the form $s_{k}$ (with
$k\in\left\{  1,2,\ldots,n-1\right\}  $). In order to solve Exercise
\ref{exe.ps2.2.5} \textbf{(g)}, it only remains to show that $\ell\left(
\sigma\right)  $ is the \textbf{smallest} such $N$. In other words, it remains
to show that if $N\in\mathbb{N}$ is such that $\sigma$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $), then $N\geq\ell\left(  \sigma\right)  $.

So let $N\in\mathbb{N}$ be such that $\sigma$ can be written as a composition
of $N$ permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots
,n-1\right\}  $). In other words, there exists an $N$-tuple $\left(
k_{1},k_{2},\ldots,k_{N}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{N}$
such that $\sigma=s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{N}}$.
Applying (\ref{sol.ps2.2.5.c.snake}) to $\operatorname*{id}$ and $N$ instead
of $\sigma$ and $p$, we obtain%
\[
\ell\left(  \operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{N}}\right)  \right)  \leq\underbrace{\ell\left(
\operatorname*{id}\right)  }_{=0}+N=N.
\]
Since $\operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{N}}\right)  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{N}}=\sigma$,
this rewrites as $\ell\left(  \sigma\right)  \leq N$. In other words,
$N\geq\ell\left(  \sigma\right)  $. This completes our solution to Exercise
\ref{exe.ps2.2.5} \textbf{(g)}.
\end{proof}

\begin{remark}
The above solution to Exercise \ref{exe.ps2.2.5} owes most of its length to my
attempts at being precise. As Pascal said, \textquotedblleft I have made this
longer than usual because I have not had time to make it
shorter\textquotedblright. The proofs are not, per se, difficult, but this is
combinatorics, and proofs in combinatorics often have to walk a tightrope
between being unreadably long and unreadably terse.

Most parts of Exercise \ref{exe.ps2.2.5} can be proven in more than just one
way. Let me briefly mention an alternative proof for parts \textbf{(b)} and
\textbf{(c)}. Namely, let us use the notation $\operatorname*{Inv}\left(
\sigma\right)  $ for the set of all inversions of a permutation $\sigma$. Let
$\sigma\in S_{n}$ and $\tau\in S_{n}$. Then, it is not hard to convince
oneself that%
\[
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  =A\cup B,
\]
where $A$ and $B$ are the sets defined by%
\begin{align*}
A  &  =\left\{  \left(  i,j\right)  \ \mid\ \left(  i,j\right)  \in
\operatorname*{Inv}\left(  \tau\right)  \text{ and }\left(  \tau\left(
j\right)  ,\tau\left(  i\right)  \right)  \notin\operatorname*{Inv}\left(
\sigma\right)  \right\}  ;\\
B  &  =\left\{  \left(  i,j\right)  \ \mid\ \left(  i,j\right)  \notin%
\operatorname*{Inv}\left(  \tau\right)  \text{ and }\left(  \tau\left(
i\right)  ,\tau\left(  j\right)  \right)  \in\operatorname*{Inv}\left(
\sigma\right)  \right\}
\end{align*}
(where $\left(  i,j\right)  $ are subject to the condition $1\leq i<j\leq n$
both times). (Essentially, this is because an $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ either satisfies
$\tau\left(  i\right)  >\tau\left(  j\right)  $ or satisfies $\tau\left(
i\right)  <\tau\left(  j\right)  $. In the first case, this $\left(
i,j\right)  $ belongs to $A$; in the second case, it belongs to $B$.) It is
furthermore clear that $\left\vert A\right\vert \leq\left\vert
\operatorname*{Inv}\left(  \tau\right)  \right\vert =\ell\left(  \tau\right)
$ (by (\ref{sol.ps2.2.5.f.1})) and $\left\vert B\right\vert \leq\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert =\ell\left(
\sigma\right)  $ (by (\ref{sol.ps2.2.5.f.1})). Hence, (\ref{sol.ps2.2.5.f.1})
yields $\ell\left(  \sigma\circ\tau\right)  =\left\vert
\underbrace{\operatorname*{Inv}\left(  \sigma\circ\tau\right)  }_{=A\cup
B}\right\vert =\left\vert A\cup B\right\vert \leq\underbrace{\left\vert
A\right\vert }_{\leq\ell\left(  \tau\right)  }+\underbrace{\left\vert
B\right\vert }_{\leq\ell\left(  \sigma\right)  }\leq\ell\left(  \tau\right)
+\ell\left(  \sigma\right)  $. This solves Exercise \ref{exe.ps2.2.5}
\textbf{(c)}. To solve part \textbf{(b)}, we need to take a few more steps.
First, it is clear that $A\cap B=\varnothing$, so that $\left\vert A\cup
B\right\vert =\left\vert A\right\vert +\left\vert B\right\vert $. Second, let
us set%
\[
C=\left\{  \left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}%
\ \mid\ i<j\text{, }\tau\left(  i\right)  >\tau\left(  j\right)  \text{ and
}\sigma\left(  \tau\left(  i\right)  \right)  <\sigma\left(  \tau\left(
j\right)  \right)  \right\}  .
\]
Then, it is easy to see that $C\subseteq\operatorname*{Inv}\left(
\tau\right)  $ and $A=\operatorname*{Inv}\left(  \tau\right)  \setminus C$, so
that $\left\vert A\right\vert =\left\vert \operatorname*{Inv}\left(
\tau\right)  \right\vert -\left\vert C\right\vert $. Moreover, if $\mathbf{t}$
denotes the permutation of $\left\{  1,2,\ldots,n\right\}  ^{2}$ which sends
every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ to $\left(
\tau\left(  i\right)  ,\tau\left(  j\right)  \right)  $, and if $\mathbf{f}$
denotes the permutation of $\left\{  1,2,\ldots,n\right\}  ^{2}$ which sends
every $\left(  i,j\right)  $ to $\left(  j,i\right)  $, then $\mathbf{f}%
\left(  C\right)  \subseteq\mathbf{t}^{-1}\left(  \operatorname*{Inv}\left(
\sigma\right)  \right)  $ and $B=\mathbf{t}^{-1}\left(  \operatorname*{Inv}%
\left(  \sigma\right)  \right)  \setminus\mathbf{f}\left(  C\right)  $, so
that $\left\vert B\right\vert =\underbrace{\left\vert \mathbf{t}^{-1}\left(
\operatorname*{Inv}\left(  \sigma\right)  \right)  \right\vert }_{=\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }-\underbrace{\left\vert
\mathbf{f}\left(  C\right)  \right\vert }_{=\left\vert C\right\vert
}=\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert -\left\vert
C\right\vert $. Thus,%
\begin{align*}
\ell\left(  \sigma\circ\tau\right)   &  =\left\vert
\underbrace{\operatorname*{Inv}\left(  \sigma\circ\tau\right)  }_{=A\cup
B}\right\vert =\left\vert A\cup B\right\vert =\underbrace{\left\vert
A\right\vert }_{=\left\vert \operatorname*{Inv}\left(  \tau\right)
\right\vert -\left\vert C\right\vert }+\underbrace{\left\vert B\right\vert
}_{=\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert
-\left\vert C\right\vert }\\
&  =\left(  \underbrace{\left\vert \operatorname*{Inv}\left(  \tau\right)
\right\vert }_{=\ell\left(  \tau\right)  }-\left\vert C\right\vert \right)
+\left(  \underbrace{\left\vert \operatorname*{Inv}\left(  \sigma\right)
\right\vert }_{=\ell\left(  \sigma\right)  }-\left\vert C\right\vert \right)
=\left(  \ell\left(  \tau\right)  -\left\vert C\right\vert \right)  +\left(
\ell\left(  \sigma\right)  -\left\vert C\right\vert \right) \\
&  =\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  -2\left\vert
C\right\vert \equiv\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
\operatorname{mod}2.
\end{align*}
This solves Exercise \ref{exe.ps2.2.5} \textbf{(b)}.
\end{remark}

\subsection{Solution to Exercise \ref{exe.ps2.2.6}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.6}.]We need to show that if
$p\in\mathbb{N}$ and \newline$\left(  k_{1},k_{2},\ldots,k_{p}\right)
\in\left\{  1,2,\ldots,n-1\right\}  ^{p}$ are such that $\sigma=s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{p}}$, then $p\equiv\ell\left(  \sigma\right)
\operatorname{mod}2$.

Let $p\in\mathbb{N}$ and $\left(  k_{1},k_{2},\ldots,k_{p}\right)  \in\left\{
1,2,\ldots,n-1\right\}  ^{p}$ be such that $\sigma=s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{p}}$. We must prove that $p\equiv\ell\left(
\sigma\right)  \operatorname{mod}2$.

Applying (\ref{sol.ps2.2.5.b.snake}) to $\operatorname*{id}$ instead of
$\sigma$, we see that%
\[
\ell\left(  \operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{p}}\right)  \right)  \equiv\underbrace{\ell\left(
\operatorname*{id}\right)  }_{=0}+p=p\operatorname{mod}2.
\]
Since $\operatorname*{id}\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{p}}\right)  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{p}}=\sigma$,
this rewrites as $\ell\left(  \sigma\right)  \equiv p\operatorname{mod}2$. In
other words, $p\equiv\ell\left(  \sigma\right)  \operatorname{mod}2$. This
completes the solution of Exercise \ref{exe.ps2.2.6}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.7}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.7}.]We have $n\geq2$. Thus, the
permutation $s_{1}\in S_{n}$ is well-defined. (This is the permutation which
switches $1$ with $2$ while leaving all other elements of $\left\{
1,2,\ldots,n\right\}  $ unchanged.)

Let $A_{n}$ denote the set of all even permutations in $S_{n}$. Let $C_{n}$
denote the set of all odd permutations in $S_{n}$. The sign of a permutation
in $S_{n}$ is either $1$ or $-1$ (because it is defined to be an integer power
of $-1$), but not both. Hence, every permutation in $S_{n}$ is either even or
odd, but not both. In other words, we have $S_{n}=A_{n}\cup C_{n}$ and
$A_{n}\cap C_{n}=\varnothing$. Therefore, $\left\vert S_{n}\right\vert
=\left\vert A_{n}\right\vert +\left\vert C_{n}\right\vert $.

Now, for every $\sigma\in A_{n}$, we have $\sigma\circ s_{k}\in C_{n}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in A_{n}$. Thus, $\sigma$ is an
even permutation in $S_{n}$ (since $A_{n}$ is the set of all even permutations
in $S_{n}$). Since $\sigma$ is even, we have $\left(  -1\right)  ^{\sigma}=1$,
so that $1=\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }$. Therefore, $\ell\left(  \sigma\right)  \equiv
0\operatorname{mod}2$. Now, (\ref{sol.ps2.2.5.b.stepper}) yields $\ell\left(
\sigma\circ s_{k}\right)  \equiv\underbrace{\ell\left(  \sigma\right)
}_{\equiv0\operatorname{mod}2}+1\equiv1\operatorname{mod}2$, so that $\left(
-1\right)  ^{\ell\left(  \sigma\circ s_{k}\right)  }=-1$. But now, $\left(
-1\right)  ^{\sigma\circ s_{k}}=\left(  -1\right)  ^{\ell\left(  \sigma\circ
s_{k}\right)  }=-1$, so that the permutation $\sigma\circ s_{k}$ is odd. In
other words, $\sigma\circ s_{k}\in C_{n}$ (since $C_{n}$ is the set of all odd
permutations in $S_{n}$), qed.}. Hence, we can define a map $\Phi
:A_{n}\rightarrow C_{n}$ by
\[
\Phi\left(  \sigma\right)  =\sigma\circ s_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in A_{n}.
\]
Similarly, we can define a map $\Psi:C_{n}\rightarrow A_{n}$ by
\[
\Psi\left(  \sigma\right)  =\sigma\circ s_{k}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in C_{n}.
\]
These two maps $\Phi$ and $\Psi$ are mutually inverse (since every $\sigma\in
S_{n}$ satisfies $\sigma\circ\underbrace{s_{k}\circ s_{k}}_{=s_{k}%
^{2}=\operatorname*{id}}=\sigma$). Therefore, the map $\Phi$ is a bijection.
Thus, there exists a bijection form $A_{n}$ to $C_{n}$ (namely, $\Phi$), so
that we obtain $\left\vert A_{n}\right\vert =\left\vert C_{n}\right\vert $.
Hence, $\left\vert S_{n}\right\vert =\underbrace{\left\vert A_{n}\right\vert
}_{=\left\vert C_{n}\right\vert }+\left\vert C_{n}\right\vert =\left\vert
C_{n}\right\vert +\left\vert C_{n}\right\vert =2\left\vert C_{n}\right\vert $
and therefore $\left\vert C_{n}\right\vert =\dfrac{1}{2}\underbrace{\left\vert
S_{n}\right\vert }_{=n!}=\dfrac{1}{2}n!=n!/2$. In other words, the number of
odd permutations in $S_{n}$ is $n!/2$. Similarly, the number of even
permutations in $S_{n}$ is $n!/2$. Exercise \ref{exe.ps2.2.7} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.4'}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.4'}.]The solution of Exercise
\ref{exe.ps2.2.4'} \textbf{(a)} is completely analogous to the solution of
Exercise \ref{exe.ps2.2.4} \textbf{(a)}; it can be obtained from the latter by
replacing $S_{n}$ by $S_{\infty}$, replacing $\left\{  1,2,\ldots,n\right\}  $
by $\left\{  1,2,3,\ldots\right\}  $, and replacing $\left\{  1,2,\ldots
,n-2\right\}  $ by $\left\{  1,2,3,\ldots\right\}  $.

As for Exercise \ref{exe.ps2.2.4'} \textbf{(b)}, we again omit the solution,
because it follows from an exercise that will be solved below (Exercise
\ref{exe.ps2.2.5'} \textbf{(e)}).
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.5'}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.5'}.]A solution of Exercise
\ref{exe.ps2.2.5'} can be obtained by copying our above solution of Exercise
\ref{exe.ps2.2.5} almost verbatim, occasionally doing some replacements (e.g.,
we have to replace $S_{n}$ by $S_{\left(  \infty\right)  }$, to replace $1\leq
i<j\leq n$ by $1\leq i<j$, to replace $\left\{  1,2,\ldots,n-1\right\}  $ by
$\left\{  1,2,3,\ldots\right\}  $, and to replace $\left\{  1,2,\ldots
,n\right\}  $ by $\left\{  1,2,3,\ldots\right\}  $). We leave the
straightforward changes to the reader.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps2.2.6'}}

\begin{proof}
[Solution to Exercise \ref{exe.ps2.2.6'}.]A solution of Exercise
\ref{exe.ps2.2.6'} can be obtained by copying our above solution of Exercise
\ref{exe.ps2.2.6} almost verbatim, occasionally doing some replacements (e.g.,
we have to replace $S_{n}$ by $S_{\left(  \infty\right)  }$, to replace
$\left\{  1,2,\ldots,n-1\right\}  $ by $\left\{  1,2,3,\ldots\right\}  $, and
to replace $\left\{  1,2,\ldots,n\right\}  $ by $\left\{  1,2,3,\ldots
\right\}  $). We leave the straightforward changes to the reader.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.0}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.0}.]In the following, \textquotedblleft
path\textquotedblright\ will always mean \textquotedblleft path on the
(undirected) $n$-th right Bruhat graph\textquotedblright. Hence, we need to
prove that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest length
of a path between $\sigma$ and $\tau$.

We write any path as the tuple consisting of its vertices (from its beginning
to its end).\footnote{This is legitimate, because the (undirected) $n$-th
right Bruhat graph does not have multiple edges.}

Let $L=\ell\left(  \sigma^{-1}\circ\tau\right)  $. We shall first show that
there exists a path of length $L$ between $\sigma$ and $\tau$.

Indeed, Exercise \ref{exe.ps2.2.5} \textbf{(g)} (applied to $\sigma^{-1}%
\circ\tau$ instead of $\sigma$) yields that $\ell\left(  \sigma^{-1}\circ
\tau\right)  $ is the smallest $N\in\mathbb{N}$ such that $\sigma^{-1}%
\circ\tau$ can be written as a composition of $N$ permutations of the form
$s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). Since $L=\ell\left(
\sigma^{-1}\circ\tau\right)  $, we can rewrite this as follows: $L$ is the
smallest $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). Hence, $\sigma^{-1}\circ\tau$ can be written as a
composition of $L$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). In other words, there exists an $L$-tuple $\left(
j_{1},j_{2},\ldots,j_{L}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{L}$
such that $\sigma^{-1}\circ\tau=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ
s_{j_{L}}$. Consider this $\left(  j_{1},j_{2},\ldots,j_{L}\right)  $.

We have $\underbrace{\sigma\circ\sigma^{-1}}_{=\operatorname*{id}}\circ
\tau=\tau$, so that $\tau=\sigma\circ\underbrace{\sigma^{-1}\circ\tau
}_{=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  $. Now, for every
$p\in\left\{  0,1,\ldots,L\right\}  $, define a permutation $\gamma_{p}\in
S_{n}$ by%
\[
\gamma_{p}=\sigma\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ
s_{j_{p}}\right)  .
\]
Thus, $\gamma_{0}=\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{0}}\right)  }_{=\left(  \text{a composition of }0\text{
permutations}\right)  =\operatorname*{id}}=\sigma$ and $\gamma_{L}=\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  =\tau$.

Now, for every $i\in\left\{  1,2,\ldots,L\right\}  $, the vertices $\gamma
_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right Bruhat graph are
adjacent\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,L\right\}  $.
Then, the definition of $\gamma_{i-1}$ yields $\gamma_{i-1}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)  $, whereas the
definition of $\gamma_{i}$ yields%
\[
\gamma_{i}=\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}\circ
\cdots\circ s_{j_{i}}\right)  }_{=\left(  s_{j_{1}}\circ s_{j_{2}}\circ
\cdots\circ s_{j_{i-1}}\right)  \circ s_{j_{i}}}=\underbrace{\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)
}_{=\gamma_{i-1}}\circ s_{j_{i}}=\gamma_{i-1}\circ s_{j_{i}}.
\]
Therefore, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$\gamma_{i}=\gamma_{i-1}\circ s_{k}$ (namely, $k=j_{i}$). In other words, the
vertices $\gamma_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right
Bruhat graph are adjacent (by the definition of the edges of this graph).
Qed.}. Hence, the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$ form a
path. This path connects $\sigma$ to $\tau$ (since it begins at $\gamma
_{0}=\sigma$ and ends at $\gamma_{L}=\tau$), and has length $L$. Thus, there
exists a path between $\sigma$ and $\tau$ which has length $L$ (namely, the
path formed by the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$).

We shall now show that $L$ is the smallest length of a path between $\sigma$
and $\tau$. Indeed, let $\mathbf{d}$ be any path between $\sigma$ and $\tau$.
We shall show that the length of $\mathbf{d}$ is $\geq L$.

The path $\mathbf{d}$ is a path between $\sigma$ and $\tau$. Hence, we can
write the path $\mathbf{d}$ in the form $\mathbf{d}=\left(  \delta_{0}%
,\delta_{1},\ldots,\delta_{M}\right)  $ for some $\delta_{0},\delta_{1}%
,\ldots,\delta_{M}\in S_{n}$ with $\delta_{0}=\sigma$ and $\delta_{M}=\tau$.
Consider these $\delta_{0},\delta_{1},\ldots,\delta_{M}\in S_{n}$.

For every $i\in\left\{  1,2,\ldots,M\right\}  $, there exists a $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k}%
$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,M\right\}  $.
Then, the vertices $\delta_{i}$ and $\delta_{i-1}$ of the (undirected) $n$-th
right Bruhat graph are adjacent (because they are two consecutive vertices on
the path $\left(  \delta_{0},\delta_{1},\ldots,\delta_{M}\right)  =\mathbf{d}%
$). In other words, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $
such that $\delta_{i}=\delta_{i-1}\circ s_{k}$ (by the definition of the edges
of this graph). Qed.}. We denote this $k$ by $k_{i}$. Thus, for every
$i\in\left\{  1,2,\ldots,M\right\}  $, we have defined a $k_{i}\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$.

Now, every $j\in\left\{  0,1,\ldots,M\right\}  $ satisfies%
\begin{equation}
\delta_{j}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{j}}\right)  \label{sol.ps4.short.0.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.short.0.5}):} We shall prove
(\ref{sol.ps4.short.0.5}) by induction over $j$:
\par
\textit{Induction base:} We have $\delta_{0}=\sigma$. Compared with
$\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{0}}\right)  }_{=\left(  \text{a composition of }0\text{ permutations}%
\right)  =\operatorname*{id}}=\sigma$, this yields $\delta_{0}=\sigma
\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  $. In
other words, (\ref{sol.ps4.short.0.5}) holds for $j=0$. This completes the
induction base.
\par
\textit{Induction step:} Let $J\in\left\{  0,1,\ldots,M\right\}  $ be
positive. Assume that (\ref{sol.ps4.short.0.5}) holds for $j=J-1$. We need to
show that (\ref{sol.ps4.short.0.5}) holds for $j=J$.
\par
We have $\delta_{J}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J-1}}\right)  $ (since (\ref{sol.ps4.short.0.5}) holds for
$j=J-1$). Now, recall that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$ for every
$i\in\left\{  1,2,\ldots,M\right\}  $. Applying this to $i=J$, we obtain%
\begin{align*}
\delta_{J}  &  =\underbrace{\delta_{J-1}}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{J-1}}\right)  }\circ s_{k_{J}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }J\in\left\{  1,2,\ldots,M\right\}
\text{ (since }J\in\left\{  0,1,\ldots,M\right\}  \text{ is positive)}\right)
\\
&  =\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{J-1}}\right)  \circ s_{k_{J}}}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}\right)  .
\end{align*}
In other words, (\ref{sol.ps4.short.0.5}) holds for $j=J$. This completes the
induction step. Thus, (\ref{sol.ps4.short.0.5}) is proven by induction.}.
Applying this to $j=M$, we obtain%
\[
\delta_{M}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{M}}\right)  .
\]
Compared with $\delta_{M}=\tau$, this yields%
\[
\tau=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{M}%
}\right)  ,
\]
so that
\[
\sigma^{-1}\circ\underbrace{\tau}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{M}}\right)  }=\underbrace{\sigma^{-1}%
\circ\sigma}_{=\operatorname*{id}}\circ\left(  s_{k_{1}}\circ s_{k_{2}}%
\circ\cdots\circ s_{k_{M}}\right)  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{M}}.
\]
Therefore, the permutation $\sigma^{-1}\circ\tau$ can be written as a
composition of $M$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $) (namely, of the $M$ permutations $s_{k_{1}}$,
$s_{k_{2}}$, $\ldots$, $s_{k_{M}}$).

Now, we recall that $L$ is the smallest $N\in\mathbb{N}$ such that
$\sigma^{-1}\circ\tau$ can be written as a composition of $N$ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $). Hence, if
$N\in\mathbb{N}$ is such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $), then $N\geq L$. We can apply this to $N=M$
(because $\sigma^{-1}\circ\tau$ can be written as a composition of $M$
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$)), and thus obtain $M\geq L$.

But the length of the path $\mathbf{d}$ is $M$ (since $\mathbf{d}=\left(
\delta_{0},\delta_{1},\ldots,\delta_{M}\right)  $). Hence, the length of the
path $\mathbf{d}$ is $\geq L$ (since $M\geq L$).

Let us now forget that we fixed $\mathbf{d}$. We thus have shown that if
$\mathbf{d}$ is any path between $\sigma$ and $\tau$, then the length of the
path $\mathbf{d}$ is $\geq L$. In other words, every path between $\sigma$ and
$\tau$ has length $\geq L$.

Altogether, we have proven the following two statements:

\begin{itemize}
\item There exists a path of length $L$ between $\sigma$ and $\tau$.

\item Every path between $\sigma$ and $\tau$ has length $\geq L$.
\end{itemize}

Therefore, $L$ is the smallest length of a path between $\sigma$ and $\tau$.
In other words, $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest
length of a path between $\sigma$ and $\tau$ (since $L=\ell\left(  \sigma
^{-1}\circ\tau\right)  $). Exercise \ref{exe.ps4.0} is solved.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.0}.]Let us first forget about $\sigma$ and
$\tau$, and make a few general remarks.

Let $\sigma\in S_{n}$. Then, Exercise \ref{exe.ps2.2.5} \textbf{(g)} shows
that
\begin{equation}
\left(
\begin{array}
[c]{c}%
\ell\left(  \sigma\right)  \text{ is the smallest }N\in\mathbb{N}\text{ such
that }\sigma\text{ can be written as a composition}\\
\text{of }N\text{ permutations of the form }s_{k}\text{ (with }k\in\left\{
1,2,\ldots,n-1\right\}  \text{)}%
\end{array}
\right)  . \label{sol.ps4.0.lem}%
\end{equation}


Let us now forget that we fixed $\sigma$. We thus have proven
(\ref{sol.ps4.0.lem}) for every $\sigma\in S_{n}$.

Now, let $\sigma\in S_{n}$ and $\tau\in S_{n}$. In the following,
\textquotedblleft path\textquotedblright\ will always mean \textquotedblleft
path on the (undirected) $n$-th right Bruhat graph\textquotedblright. Hence,
we need to prove that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the
smallest length of a path between $\sigma$ and $\tau$.

We write any path as the tuple consisting of its vertices (from its beginning
to its end).\footnote{This is legitimate, because the (undirected) $n$-th
right Bruhat graph does not have multiple edges.}

Let $L=\ell\left(  \sigma^{-1}\circ\tau\right)  $. We shall first show that
there exists a path of length $L$ between $\sigma$ and $\tau$.

Indeed, (\ref{sol.ps4.0.lem}) (applied to $\sigma^{-1}\circ\tau$ instead of
$\sigma$) yields that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the
smallest $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). Hence, $\ell\left(  \sigma^{-1}\circ\tau\right)  $
is an $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). In other words, $\sigma^{-1}\circ\tau$ can be
written as a composition of $\ell\left(  \sigma^{-1}\circ\tau\right)  $
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$). In other words, $\sigma^{-1}\circ\tau$ can be written as a composition of
$L$ permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots
,n-1\right\}  $) (since $L=\ell\left(  \sigma^{-1}\circ\tau\right)  $). In
other words, there exists an $L$-tuple $\left(  j_{1},j_{2},\ldots
,j_{L}\right)  \in\left\{  1,2,\ldots,n-1\right\}  ^{L}$ such that
$\sigma^{-1}\circ\tau=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}$.
Consider this $\left(  j_{1},j_{2},\ldots,j_{L}\right)  $.

We have $\underbrace{\sigma\circ\sigma^{-1}}_{=\operatorname*{id}}\circ
\tau=\tau$, so that $\tau=\sigma\circ\underbrace{\sigma^{-1}\circ\tau
}_{=s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  $. Now, for every
$p\in\left\{  0,1,\ldots,L\right\}  $, define a permutation $\gamma_{p}\in
S_{n}$ by%
\[
\gamma_{p}=\sigma\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ
s_{j_{p}}\right)  .
\]
Thus, $\gamma_{0}=\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{0}}\right)  }_{=\left(  \text{a composition of }0\text{
permutations}\right)  =\operatorname*{id}}=\sigma$ and $\gamma_{L}=\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{L}}\right)  =\tau$.

Now, for every $i\in\left\{  1,2,\ldots,L\right\}  $, the vertices $\gamma
_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right Bruhat graph are
adjacent\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,L\right\}  $.
Then, the definition of $\gamma_{i-1}$ yields $\gamma_{i-1}=\sigma\circ\left(
s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)  $, whereas the
definition of $\gamma_{i}$ yields%
\begin{align*}
\gamma_{i}  &  =\sigma\circ\underbrace{\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{i}}\right)  }_{=\left(  s_{j_{1}}\circ s_{j_{2}}%
\circ\cdots\circ s_{j_{i-1}}\right)  \circ s_{j_{i}}}=\underbrace{\sigma
\circ\left(  s_{j_{1}}\circ s_{j_{2}}\circ\cdots\circ s_{j_{i-1}}\right)
}_{=\gamma_{i-1}}\circ s_{j_{i}}\\
&  =\gamma_{i-1}\circ s_{j_{i}}.
\end{align*}
Therefore, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$\gamma_{i}=\gamma_{i-1}\circ s_{k}$ (namely, $k=j_{i}$). In other words, the
vertices $\gamma_{i}$ and $\gamma_{i-1}$ of the (undirected) $n$-th right
Bruhat graph are adjacent (by the definition of the edges of this graph).
Qed.}. Hence, the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$ form a
path. This path connects $\sigma$ to $\tau$ (since it begins at $\gamma
_{0}=\sigma$ and ends at $\gamma_{L}=\tau$), and has length $L$. Thus, there
exists a path between $\sigma$ and $\tau$ which has length $L$ (namely, the
path formed by the vertices $\gamma_{0},\gamma_{1},\ldots,\gamma_{L}$).

We shall now show that $L$ is the smallest length of a path between $\sigma$
and $\tau$. Indeed, let $\mathbf{d}$ be any path between $\sigma$ and $\tau$.
We shall show that the length of $\mathbf{d}$ is $\geq L$.

The path $\mathbf{d}$ is a path between $\sigma$ and $\tau$. Hence, we can
write the path $\mathbf{d}$ in the form $\mathbf{d}=\left(  \delta_{0}%
,\delta_{1},\ldots,\delta_{M}\right)  $ for some $\delta_{0},\delta_{1}%
,\ldots,\delta_{M}\in S_{n}$ with $\delta_{0}=\sigma$ and $\delta_{M}=\tau$.
Consider these $\delta_{0},\delta_{1},\ldots,\delta_{M}\in S_{n}$.

For every $i\in\left\{  1,2,\ldots,M\right\}  $, there exists a $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k}%
$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,M\right\}  $.
Then, the vertices $\delta_{i}$ and $\delta_{i-1}$ of the (undirected) $n$-th
right Bruhat graph are adjacent (because they are two consecutive vertices on
the path $\left(  \delta_{0},\delta_{1},\ldots,\delta_{M}\right)  =\mathbf{d}%
$). In other words, there exists a $k\in\left\{  1,2,\ldots,n-1\right\}  $
such that $\delta_{i}=\delta_{i-1}\circ s_{k}$ (by the definition of the edges
of this graph). Qed.}. We denote this $k$ by $k_{i}$. Thus, for every
$i\in\left\{  1,2,\ldots,M\right\}  $, we have defined a $k_{i}\in\left\{
1,2,\ldots,n-1\right\}  $ such that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$.

Now, every $j\in\left\{  0,1,\ldots,M\right\}  $ satisfies%
\begin{equation}
\delta_{j}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{j}}\right)  \label{sol.ps4.0.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.0.5}):} We shall prove
(\ref{sol.ps4.0.5}) by induction over $j$:
\par
\textit{Induction base:} We have $\delta_{0}=\sigma$. Compared with
$\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{0}}\right)  }_{=\left(  \text{a composition of }0\text{ permutations}%
\right)  =\operatorname*{id}}=\sigma$, this yields $\delta_{0}=\sigma
\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{0}}\right)  $. In
other words, (\ref{sol.ps4.0.5}) holds for $j=0$. This completes the induction
base.
\par
\textit{Induction step:} Let $J\in\left\{  0,1,\ldots,M\right\}  $ be
positive. Assume that (\ref{sol.ps4.0.5}) holds for $j=J-1$. We need to show
that (\ref{sol.ps4.0.5}) holds for $j=J$.
\par
We have $\delta_{J}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J-1}}\right)  $ (since (\ref{sol.ps4.0.5}) holds for
$j=J-1$). Now, recall that $\delta_{i}=\delta_{i-1}\circ s_{k_{i}}$ for every
$i\in\left\{  1,2,\ldots,M\right\}  $. Applying this to $i=J$, we obtain%
\begin{align*}
\delta_{J}  &  =\underbrace{\delta_{J-1}}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{J-1}}\right)  }\circ s_{k_{J}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }J\in\left\{  1,2,\ldots,M\right\}
\text{ (since }J\in\left\{  0,1,\ldots,M\right\}  \text{ is positive)}\right)
\\
&  =\sigma\circ\underbrace{\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{J-1}}\right)  \circ s_{k_{J}}}_{=s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ
\cdots\circ s_{k_{J}}\right)  .
\end{align*}
In other words, (\ref{sol.ps4.0.5}) holds for $j=J$. This completes the
induction step. Thus, (\ref{sol.ps4.0.5}) is proven by induction.}. Applying
this to $j=M$, we obtain%
\[
\delta_{M}=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ
s_{k_{M}}\right)  .
\]
Compared with $\delta_{M}=\tau$, this yields%
\[
\tau=\sigma\circ\left(  s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{M}%
}\right)  ,
\]
so that
\begin{align*}
\sigma^{-1}\circ\underbrace{\tau}_{=\sigma\circ\left(  s_{k_{1}}\circ
s_{k_{2}}\circ\cdots\circ s_{k_{M}}\right)  }  &  =\underbrace{\sigma
^{-1}\circ\sigma}_{=\operatorname*{id}}\circ\left(  s_{k_{1}}\circ s_{k_{2}%
}\circ\cdots\circ s_{k_{M}}\right) \\
&  =s_{k_{1}}\circ s_{k_{2}}\circ\cdots\circ s_{k_{M}}.
\end{align*}
Therefore, the permutation $\sigma^{-1}\circ\tau$ can be written as a
composition of $M$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $) (namely, of the $M$ permutations $s_{k_{1}}$,
$s_{k_{2}}$, $\ldots$, $s_{k_{M}}$).

Now, we recall that $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the
smallest $N\in\mathbb{N}$ such that $\sigma^{-1}\circ\tau$ can be written as a
composition of $N$ permutations of the form $s_{k}$ (with $k\in\left\{
1,2,\ldots,n-1\right\}  $). Hence, if $N\in\mathbb{N}$ is such that
$\sigma^{-1}\circ\tau$ can be written as a composition of $N$ permutations of
the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}  $), then
$N\geq\ell\left(  \sigma^{-1}\circ\tau\right)  $. We can apply this to $N=M$
(because $\sigma^{-1}\circ\tau$ can be written as a composition of $M$
permutations of the form $s_{k}$ (with $k\in\left\{  1,2,\ldots,n-1\right\}
$)), and thus obtain $M\geq\ell\left(  \sigma^{-1}\circ\tau\right)  =L$.

But the length of the path $\mathbf{d}$ is $M$ (since $\mathbf{d}=\left(
\delta_{0},\delta_{1},\ldots,\delta_{M}\right)  $). Hence, the length of the
path $\mathbf{d}$ is $\geq L$ (since $M\geq L$).

Let us now forget that we fixed $\mathbf{d}$. We thus have shown that if
$\mathbf{d}$ is any path between $\sigma$ and $\tau$, then the length of the
path $\mathbf{d}$ is $\geq L$. In other words, every path between $\sigma$ and
$\tau$ has length $\geq L$.

Altogether, we have proven the following two statements:

\begin{itemize}
\item There exists a path of length $L$ between $\sigma$ and $\tau$.

\item Every path between $\sigma$ and $\tau$ has length $\geq L$.
\end{itemize}

Therefore, $L$ is the smallest length of a path between $\sigma$ and $\tau$.
In other words, $\ell\left(  \sigma^{-1}\circ\tau\right)  $ is the smallest
length of a path between $\sigma$ and $\tau$ (since $L=\ell\left(  \sigma
^{-1}\circ\tau\right)  $). Exercise \ref{exe.ps4.0} is solved.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.ps4.1ab}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.1ab}.]\textbf{(a)} We shall prove that%
\begin{equation}
\ell\left(  t_{i,j}\right)  =2\left\vert j-i\right\vert -1.
\label{sol.ps4.1ab.a.claim}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.1ab.a.claim}):} We know that $t_{i,j}$ is the
permutation in $S_{n}$ which switches $i$ with $j$ while leaving all other
elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged; on the other hand,
$t_{j,i}$ is the permutation in $S_{n}$ which switches $j$ with $i$ while
leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged.
Comparing these two descriptions of $t_{i,j}$ and $t_{j,i}$, we immediately
see that they are identical (since switching $i$ with $j$ is the same thing as
switching $j$ with $i$). Thus, $t_{i,j}=t_{j,i}$. Also, clearly, $\left\vert
j-i\right\vert =\left\vert i-j\right\vert $. Hence, the claim
(\ref{sol.ps4.1ab.a.claim}) does not change if we switch $i$ with $j$. Thus,
we can WLOG assume that $i\leq j$ (because otherwise, we can just switch $i$
with $j$). Assume this. Now, $i\leq j$, so that $i<j$ (since $i$ and $j$ are
distinct). Hence, $j>i$, so that $j-i>0$, so that $\left\vert j-i\right\vert
=j-i$.

\begin{vershort}
Set%
\begin{align*}
A  &  =\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  \ \ \ \ \ \ \ \ \ \ \text{and}\\
B  &  =\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right\}  .
\end{align*}
These two sets $A$ and $B$ satisfy $\left\vert A\right\vert =j-i$ and
$\left\vert B\right\vert =j-i-1$. Also, it is easy to see that these sets $A$
and $B$ are disjoint\footnote{\textit{Proof.} Every element of $B$ is a pair
$\left(  k,j\right)  $ whose first entry is $>i$, whereas every element of $A$
is a pair $\left(  i,k\right)  $ whose first entry equals $i$. Thus, if the
sets $A$ and $B$ had an element $e$ in common, then $e$ would be a pair whose
first entry is $>i$ (since $e\in B$) and equals $i$ (since $e\in A$) at the
same time, which of course is impossible. Hence, $A$ and $B$ are disjoint.}.
Thus,%
\[
\left\vert A\cup B\right\vert =\underbrace{\left\vert A\right\vert }%
_{=j-i}+\underbrace{\left\vert B\right\vert }_{=j-i-1}=\left(  j-i\right)
+\left(  j-i-1\right)  =2\underbrace{\left(  j-i\right)  }_{=\left\vert
j-i\right\vert }-1=2\left\vert j-i\right\vert -1.
\]


Now, let $\operatorname*{Inv}\left(  t_{i,j}\right)  $ denote the set of all
inversions of $t_{i,j}$. Then, $\ell\left(  t_{i,j}\right)  =\left\vert
\operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert $ (because $\ell\left(
t_{i,j}\right)  $ was defined as the number of inversions of $t_{i,j}$, which
number is obviously $\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)
\right\vert $).

We shall now show that $\operatorname*{Inv}\left(  t_{i,j}\right)  =A\cup B$.
Indeed, it is clearly enough to prove $A\cup B\subseteq\operatorname*{Inv}%
\left(  t_{i,j}\right)  $ and $\operatorname*{Inv}\left(  t_{i,j}\right)
\subseteq A\cup B$. Proving that $A\cup B\subseteq\operatorname*{Inv}\left(
t_{i,j}\right)  $ means proving that every element of $A\cup B$ is an
inversion of $t_{i,j}$; this is straightforward\footnote{\textit{Proof.} We
want to show that $A\cup B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)
$. In other words, we want to prove that $e\in\operatorname*{Inv}\left(
t_{i,j}\right)  $ for every $e\in A\cup B$.
\par
So let $e\in A\cup B$. Thus, either $e\in A$ or $e\in B$.
\par
Let us first consider the case when $e\in A$. Thus, $e\in A=\left\{  \left(
i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\}  $. In
other words, $e$ has the form $e=\left(  i,k\right)  $ for some $k\in\left\{
i+1,i+2,\ldots,j\right\}  $. Consider this $k$. The permutation $t_{i,j}$
switches $i$ with $j$ while leaving all other numbers fixed. Thus, the
permutation $t_{i,j}$ leaves the numbers $i+1,i+2,\ldots,j-1$ fixed, while
sending the number $j$ to $i$. Consequently, $t_{i,j}$ sends the numbers
$i+1,i+2,\ldots,j-1,j$ to $i+1,i+2,\ldots,j-1,i$, respectively. Notice that
all of the latter numbers $i+1,i+2,\ldots,j-1,i$ are smaller than $j$. Thus,
$t_{i,j}\left(  p\right)  <j$ for every $p\in\left\{  i+1,i+2,\ldots
,j\right\}  $. Applying this to $p=k$, we conclude that $t_{i,j}\left(
k\right)  <j$.
\par
We have $i<k$ (since $k\in\left\{  i+1,i+2,\ldots,j\right\}  $) but
$t_{i,j}\left(  i\right)  =j>t_{i,j}\left(  k\right)  $ (since we have just
showed that $t_{i,j}\left(  k\right)  <j$). Thus, $\left(  i,k\right)  $ is an
inversion of $t_{i,j}$. In other words, $\left(  i,k\right)  \in
\operatorname*{Inv}\left(  t_{i,j}\right)  $. Thus, $e=\left(  i,k\right)
\in\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
Thus, we have proven that $e\in\operatorname*{Inv}\left(  t_{i,j}\right)  $ in
the case when $e\in A$. A similar argument (but now using $t_{i,j}\left(
k\right)  >i$ instead of $t_{i,j}\left(  k\right)  <j$) shows that
$e\in\operatorname*{Inv}\left(  t_{i,j}\right)  $ in the case when $e\in B$.
Since either of these two cases must hold (because we have either $e\in A$ or
$e\in B$), we thus conclude that $e\in\operatorname*{Inv}\left(
t_{i,j}\right)  $. This concludes the proof.}. Proving that
$\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$ means proving
that every inversion of $t_{i,j}$ belongs to $A\cup B$; this is equally
straightforward (although more tiresome)\footnote{\textit{Proof.} We want to
show that $\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$. In
other words, we want to prove that $c\in A\cup B$ for every $c\in
\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
So let $c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $. Thus, $c$ is an
inversion of $t_{i,j}$. In other words, $c$ is a pair $\left(  u,v\right)  $
of integers satisfying $1\leq u<v\leq n$ and $t_{i,j}\left(  u\right)
>t_{i,j}\left(  v\right)  $. Consider this $\left(  u,v\right)  $. Thus,
$c=\left(  u,v\right)  $. Our goal is to show that $c\in A\cup B$.
\par
The permutation $t_{i,j}$ switches $i$ with $j$ while leaving all other
numbers fixed. It thus makes sense to analyze several cases separately,
depending on which of the numbers $u$ and $v$ belongs to $\left\{
i,j\right\}  $. Four cases are possible:
\par
\textit{Case 1:} We have $u\in\left\{  i,j\right\}  $ and $v\in\left\{
i,j\right\}  $.
\par
\textit{Case 2:} We have $u\in\left\{  i,j\right\}  $ and $v\notin\left\{
i,j\right\}  $.
\par
\textit{Case 3:} We have $u\notin\left\{  i,j\right\}  $ and $v\in\left\{
i,j\right\}  $.
\par
\textit{Case 4:} We have $u\notin\left\{  i,j\right\}  $ and $v\notin\left\{
i,j\right\}  $.
\par
Let us first consider Case 1. In this case, we have $u\in\left\{  i,j\right\}
$ and $v\in\left\{  i,j\right\}  $. Thus, $u$ and $v$ are two elements of
$\left\{  i,j\right\}  $. Since $u<v$, this leaves only one possibility for
the pair $\left(  u,v\right)  $: namely, $\left(  u,v\right)  =\left(
i,j\right)  $. Thus,
\begin{align*}
c  &  =\left(  u,v\right)  =\left(  i,j\right)  \in\left\{  \left(
i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j\in\left\{  i+1,i+2,\ldots
,j\right\}  \right) \\
&  =A\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $u\in\left\{  i,j\right\}
$ and $v\notin\left\{  i,j\right\}  $. Since $v\notin\left\{  i,j\right\}  $,
we have $t_{i,j}\left(  v\right)  =v$ (since $t_{i,j}$ leaves all numbers
other than $i$ and $j$ unchanged). Thus, $t_{i,j}\left(  u\right)
>t_{i,j}\left(  v\right)  =v>u$ (since $u<v$). If we had $u=j$, then this
would rewrite as $t_{i,j}\left(  j\right)  >j$, which would contradict
$t_{i,j}\left(  j\right)  =i<j$. Thus, we cannot have $u=j$. Hence, we must
have $u=i$ (since $u\in\left\{  i,j\right\}  $ forces $u$ to be either $i$ or
$j$). But we have shown that $t_{i,j}\left(  u\right)  >v$, so that
$v<t_{i,j}\left(  \underbrace{u}_{=i}\right)  =t_{i,j}\left(  i\right)  =j$
(since $t_{i,j}$ switches $i$ with $j$). Combined with $v>u=i$, this yields
$i<v<j$, so that $v\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, so that%
\begin{align*}
c  &  =\left(  \underbrace{u}_{=i},v\right)  =\left(  i,v\right)  \in\left\{
\left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right) \\
&  \subseteq\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  =A\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 2.
\par
Let us next consider Case 3. In this case, we have $u\notin\left\{
i,j\right\}  $ and $v\in\left\{  i,j\right\}  $. Since $u\notin\left\{
i,j\right\}  $, we have $t_{i,j}\left(  u\right)  =u$ (since $t_{i,j}$ leaves
all numbers other than $i$ and $j$ unchanged). Thus, from $t_{i,j}\left(
u\right)  >t_{i,j}\left(  v\right)  $, we obtain $t_{i,j}\left(  v\right)
<t_{i,j}\left(  u\right)  =u<v$. If we had $v=i$, then this would rewrite as
$t_{i,j}\left(  i\right)  <i$, which would contradict $t_{i,j}\left(
i\right)  =j>i$. Thus, we cannot have $v=i$. Hence, we must have $v=j$ (since
$v\in\left\{  i,j\right\}  $ forces $v$ to be either $i$ or $j$). But we have
shown that $t_{i,j}\left(  v\right)  <u$, so that $u>t_{i,j}\left(
\underbrace{v}_{=j}\right)  =t_{i,j}\left(  j\right)  =i$ (since $t_{i,j}$
switches $i$ with $j$). Combined with $u<v=j$, this yields $i<u<j$, so that
$u\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, so that%
\begin{align*}
c  &  =\left(  u,\underbrace{v}_{=j}\right)  =\left(  u,j\right)  \in\left\{
\left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }u\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right) \\
&  =B\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 3.
\par
(Notice that Case 3 was very similar to Case 2 -- almost like a mirror version
of that case, if not for a slight asymmetry in our definition of the sets $A$
and $B$.)
\par
Let us finally consider Case 4. In this case, we have $u\notin\left\{
i,j\right\}  $ and $v\notin\left\{  i,j\right\}  $. Thus, $t_{i,j}\left(
u\right)  =u$ (as in Case 3) and $t_{i,j}\left(  v\right)  =v$ (as in Case 2),
so that $t_{i,j}\left(  u\right)  =u<v=t_{i,j}\left(  v\right)  $. This
contradicts $t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)  $. This
contradiction shows that Case 4 cannot happen; thus, we can ignore this case
completely. (Or we can argue that because \textquotedblleft ex falso
quodlibet\textquotedblright, we have $c\in A\cup B$ in Case 4.
\par
[The principle of
\href{https://en.wikipedia.org/wiki/Principle_of_explosion}{``ex falso
quodlibet''} says that from a false assertion, any arbitrary assertion
follows. (For example, if $1 = 0$, then anything is true.) This is one of the
basic principles in logic, and we could use it here to prove $c \in A \cup B$
in Case 4: Namely, since we have derived a contradiction (i.e., proven a false
assertion) in Case 4, we see that any arbitrary assertion holds in Case 4; in
particular, $c \in A \cup B$ holds in Case 4.])
\par
We have now checked that $c\in A\cup B$ in each of the four cases 1, 2, 3 and
4 (or in each of the three cases 1, 2 and 3, and Case 4 never happens). Thus,
$c\in A\cup B$ always holds. This completes our proof.}. Hence, both $A\cup
B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $ and
$\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$ are proven, and
we conclude that $\operatorname*{Inv}\left(  t_{i,j}\right)  =A\cup B$. Thus,
$\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert =\left\vert
A\cup B\right\vert =2\left\vert j-i\right\vert -1$. Thus, $\ell\left(
t_{i,j}\right)  =\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)
\right\vert =2\left\vert j-i\right\vert -1$. This solves Exercise
\ref{exe.ps4.1ab} \textbf{(a)}.
\end{vershort}

\begin{verlong}
The permutation $t_{i,j}$ switches $i$ with $j$ while leaving all other
elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged. In other words, we
have $t_{i,j}\left(  i\right)  =j$, $t_{i,j}\left(  j\right)  =i$ and%
\begin{equation}
t_{i,j}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{  i,j\right\}  .
\label{sol.ps4.1ab.a.tij}%
\end{equation}


Let%
\begin{align*}
A  &  =\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  \ \ \ \ \ \ \ \ \ \ \text{and}\\
B  &  =\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right\}  .
\end{align*}
These two sets $A$ and $B$ satisfy $\left\vert A\right\vert =j-i$%
\ \ \ \ \footnote{\textit{Proof.} The elements $\left(  i,k\right)  $ for all
$k\in\left\{  i+1,i+2,\ldots,j\right\}  $ are pairwise distinct (because $k$
can be reconstructed from $\left(  i,k\right)  $). Therefore, the number of
these elements (counted without multiplicities) is $j-i$ (since the number of
all $k\in\left\{  i+1,i+2,\ldots,j\right\}  $ is $j-i$). In other words,
$\left\vert \left\{  \left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots
,j\right\}  \right\}  \right\vert =j-i$. Now,%
\[
\left\vert \underbrace{A}_{=\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j\right\}  \right\}  }\right\vert =\left\vert \left\{  \left(
i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\}
\right\vert =j-i,
\]
qed.} and $\left\vert B\right\vert =j-i-1$\ \ \ \ \footnote{\textit{Proof.}
The elements $\left(  k,j\right)  $ for all $k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  $ are pairwise distinct (because $k$ can be reconstructed from
$\left(  k,j\right)  $). Therefore, the number of these elements (counted
without multiplicities) is $j-i-1$ (since the number of all $k\in\left\{
i+1,i+2,\ldots,j-1\right\}  $ is $\left(  j-1\right)  -i=j-i-1$). In other
words, $\left\vert \left\{  \left(  k,j\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j-1\right\}  \right\}  \right\vert =j-i-1$. Now,%
\[
\left\vert \underbrace{B}_{=\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j-1\right\}  \right\}  }\right\vert =\left\vert \left\{
\left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\right\vert =j-i-1,
\]
qed.}. Also, these sets $A$ and $B$ are disjoint\footnote{\textit{Proof.}
Assume the contrary. Then, $A\cap B\neq\varnothing$. Hence, there exists some
element $c$ of the set $A\cap B$. Consider such a $c$. We have%
\[
c\in A\cap B\subseteq A=\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j\right\}  \right\}  .
\]
In other words, we can write $c$ in the form $c=\left(  i,k\right)  $ for some
$k\in\left\{  i+1,i+2,\ldots,j\right\}  $. Let us denote this $k$ by $\ell$.
Then, $\ell$ is an element of $\left\{  i+1,i+2,\ldots,j\right\}  $ and
satisfies $c=\left(  i,\ell\right)  $.
\par
Also, $c\in A\cap B\subseteq B=\left\{  \left(  k,j\right)  \ \mid
\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}  $. Hence, we can write
$c$ in the form $c=\left(  k,j\right)  $ for some $k\in\left\{  i+1,i+2,\ldots
,j-1\right\}  $. Consider this $k$. Now, $c=\left(  k,j\right)  $, so that
$\left(  k,j\right)  =c=\left(  i,\ell\right)  $. Hence, $k=i$ and $j=\ell$.
Thus, $i=k\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, which contradicts
$i\notin\left\{  i+1,i+2,\ldots,j-1\right\}  $. This contradiction shows that
our assumption was wrong, qed.}. Hence,
\[
\left\vert A\cup B\right\vert =\underbrace{\left\vert A\right\vert }%
_{=j-i}+\underbrace{\left\vert B\right\vert }_{=j-i-1}=\left(  j-i\right)
+\left(  j-i-1\right)  =2\underbrace{\left(  j-i\right)  }_{=\left\vert
j-i\right\vert }-1=2\left\vert j-i\right\vert -1.
\]


Now, let $\operatorname*{Inv}\left(  t_{i,j}\right)  $ denote the set of all
inversions of $t_{i,j}$. Then, $\ell\left(  t_{i,j}\right)  =\left\vert
\operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert $%
\ \ \ \ \footnote{\textit{Proof.} Recall that $\ell\left(  t_{i,j}\right)  $
is defined as the number of inversions of $t_{i,j}$. Thus,%
\begin{align*}
\ell\left(  t_{i,j}\right)   &  =\left(  \text{the number of inversions of
}t_{i,j}\right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
t_{i,j}\right)  }_{=\operatorname*{Inv}\left(  t_{i,j}\right)  }\right\vert
=\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert ,
\end{align*}
qed.}.

But $A\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in A$. We shall show that
$c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
We have $c\in A=\left\{  \left(  i,k\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  i,k\right)  $ for some $k\in\left\{  i+1,i+2,\ldots,j\right\}  $.
Consider this $k$. From $k\in\left\{  i+1,i+2,\ldots,j\right\}  $, we obtain
$i<k\leq j$, so that $k\leq j\leq n$ and thus $1\leq i<k\leq n$.
\par
We shall now show that $j>t_{i,j}\left(  k\right)  $. In fact, we must be in
one of the following two cases:
\par
\textit{Case 1:} We have $k=j$.
\par
\textit{Case 2:} We have $k\neq j$.
\par
Let us first consider Case 1. In this case, we have $k=j$. Thus,
$t_{i,j}\left(  \underbrace{k}_{=j}\right)  =t_{i,j}\left(  j\right)  =i$.
Thus, $j>i=t_{i,j}\left(  k\right)  $. Hence, $j>t_{i,j}\left(  k\right)  $ is
proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $k\neq j$. Combined with
$k\neq i$ (since $i<k$), this yields $k\notin\left\{  i,j\right\}  $. Hence,
$t_{i,j}\left(  k\right)  =k$ (by (\ref{sol.ps4.1ab.a.tij})). Also, combining
$k\leq j$ with $k\neq j$, we obtain $k<j$ and thus $j>k=t_{i,j}\left(
k\right)  $. Hence, $j>t_{i,j}\left(  k\right)  $ is proven in Case 2.
\par
Now, we have proven $j>t_{i,j}\left(  k\right)  $ in each of the two Cases 1
and 2. Thus, $j>t_{i,j}\left(  k\right)  $ always holds.
\par
Now, $t_{i,j}\left(  i\right)  =j>t_{i,j}\left(  k\right)  $. Hence, $\left(
i,k\right)  $ is a pair of integers satisfying $1\leq i<k\leq n$ and
$t_{i,j}\left(  i\right)  >t_{i,j}\left(  k\right)  $. In other words,
$\left(  i,k\right)  $ is an inversion of $t_{i,j}$ (by the definition of
\textquotedblleft inversion of $t_{i,j}$\textquotedblright). In other words,
$\left(  i,k\right)  \in\operatorname*{Inv}\left(  t_{i,j}\right)  $ (since
$\operatorname*{Inv}\left(  t_{i,j}\right)  $ is the set of all inversions of
$t_{i,j}$). Thus, $c=\left(  i,k\right)  \in\operatorname*{Inv}\left(
t_{i,j}\right)  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  t_{i,j}\right)  $ for every $c\in A$. In other
words, $A\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $, qed.} and
$B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in B$. We shall show that
$c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $.
\par
We have $c\in B=\left\{  \left(  k,j\right)  \ \mid\ k\in\left\{
i+1,i+2,\ldots,j-1\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  k,j\right)  $ for some $k\in\left\{  i+1,i+2,\ldots,j-1\right\}  $.
Consider this $k$. From $k\in\left\{  i+1,i+2,\ldots,j-1\right\}  $, we obtain
$i<k<j$, so that $1\leq i<k$ and thus $1\leq k<j\leq n$. Also, combining
$k\neq i$ (since $i<k$) and $k\neq j$ (since $k<j$), we obtain $k\notin%
\left\{  i,j\right\}  $. Hence, $t_{i,j}\left(  k\right)  =k$ (by
(\ref{sol.ps4.1ab.a.tij})). But $t_{i,j}\left(  j\right)  =i<k=t_{i,j}\left(
k\right)  $, so that $t_{i,j}\left(  k\right)  >t_{i,j}\left(  j\right)  $.
\par
Hence, $\left(  k,j\right)  $ is a pair of integers satisfying $1\leq k<j\leq
n$ and $t_{i,j}\left(  k\right)  >t_{i,j}\left(  j\right)  $. In other words,
$\left(  k,j\right)  $ is an inversion of $t_{i,j}$ (by the definition of
\textquotedblleft inversion of $t_{i,j}$\textquotedblright). In other words,
$\left(  k,j\right)  \in\operatorname*{Inv}\left(  t_{i,j}\right)  $ (since
$\operatorname*{Inv}\left(  t_{i,j}\right)  $ is the set of all inversions of
$t_{i,j}$). Thus, $c=\left(  k,j\right)  \in\operatorname*{Inv}\left(
t_{i,j}\right)  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  t_{i,j}\right)  $ for every $c\in B$. In other
words, $B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $, qed.}. Hence,%
\[
\underbrace{A}_{\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  }%
\cup\underbrace{B}_{\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)
}\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  \cup\operatorname*{Inv}%
\left(  t_{i,j}\right)  =\operatorname*{Inv}\left(  t_{i,j}\right)  .
\]


On the other hand, $\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup
B$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
t_{i,j}\right)  $. We shall show that $c\in A\cup B$.
\par
We have $c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $. In other words, $c$
is an inversion of $t_{i,j}$ (since $\operatorname*{Inv}\left(  t_{i,j}%
\right)  $ is the set of all inversions of $t_{i,j}$). In other words, $c$ is
a pair $\left(  u,v\right)  $ of integers satisfying $1\leq u<v\leq n$ and
$t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)  $. Consider this $\left(
u,v\right)  $. Thus, $c=\left(  u,v\right)  $.
\par
We must be in one of the following two cases:
\par
\textit{Case 1:} We have $t_{i,j}\left(  u\right)  >u$.
\par
\textit{Case 2:} We don't have $t_{i,j}\left(  u\right)  >u$.
\par
Let us first consider Case 1. In this case, we have $t_{i,j}\left(  u\right)
>u$. If we had $u\notin\left\{  i,j\right\}  $, then we would have
$t_{i,j}\left(  u\right)  =u$ (by (\ref{sol.ps4.1ab.a.tij}), applied to
$k=u$), which would contradict $t_{i,j}\left(  u\right)  >u$. Hence, we cannot
have $u\notin\left\{  i,j\right\}  $. Thus, we have $u\in\left\{  i,j\right\}
$. If we had $u=j$, then we would have $t_{i,j}\left(  \underbrace{u}%
_{=j}\right)  =t_{i,j}\left(  j\right)  =i<j$, which would contradict
$t_{i,j}\left(  u\right)  >u=j$. Hence, we cannot have $u=j$. We thus have
$u\neq j$. Since $u\in\left\{  i,j\right\}  $ but $u\neq j$, we must have
$u=i$. Hence, $t_{i,j}\left(  \underbrace{u}_{=i}\right)  =t_{i,j}\left(
i\right)  =j$, so that $j=t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)
$.
\par
Now, we assume (for the sake of contradiction) that $v>j$. Combining $v\neq i$
(since $v>j>i$) and $v\neq j$ (since $v>j$), we obtain $v\notin\left\{
i,j\right\}  $, so that $t_{i,j}\left(  v\right)  =v$ (by
(\ref{sol.ps4.1ab.a.tij}), applied to $k=v$). Hence, $j>t_{i,j}\left(
v\right)  =v>j$, which is absurd. Thus, we have obtained a contradiction.
Therefore, our assumption (that $v>j$) was wrong. We thus must have $v\leq j$.
Combined with $i=u<v$, this yields $i<v\leq j$, so that $v\in\left\{
i+1,i+2,\ldots,j\right\}  $. Now,%
\begin{align*}
c  &  =\left(  \underbrace{u}_{=i},v\right)  =\left(  i,v\right)  \in\left\{
\left(  i,k\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j\right\}  \right\} \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\in\left\{  i+1,i+2,\ldots
,j\right\}  \right) \\
&  =A\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 1.
\par
Let us next consider Case 2. In this case, we don't have $t_{i,j}\left(
u\right)  >u$. Hence, we have $t_{i,j}\left(  u\right)  \leq u$, so that
$u\geq t_{i,j}\left(  u\right)  >t_{i,j}\left(  v\right)  $ and thus
$t_{i,j}\left(  v\right)  <u<v$. If we had $v\notin\left\{  i,j\right\}  $,
then we would have $t_{i,j}\left(  v\right)  =v$ (by (\ref{sol.ps4.1ab.a.tij}%
), applied to $k=v$), which would contradict $t_{i,j}\left(  v\right)  <v$.
Hence, we cannot have $v\notin\left\{  i,j\right\}  $. Thus, we have
$v\in\left\{  i,j\right\}  $. If we had $v=i$, then we would have
$t_{i,j}\left(  \underbrace{v}_{=i}\right)  =t_{i,j}\left(  i\right)  =j>i$,
which would contradict $t_{i,j}\left(  v\right)  <v=i$. Hence, we cannot have
$v=i$. We thus have $v\neq i$. Since $v\in\left\{  i,j\right\}  $ but $v\neq
i$, we must have $v=j$. Hence, $t_{i,j}\left(  \underbrace{v}_{=j}\right)
=t_{i,j}\left(  j\right)  =i$, so that $t_{i,j}\left(  u\right)
>t_{i,j}\left(  v\right)  =i$. Hence, $i<t_{i,j}\left(  u\right)  $.
\par
If we had $u=i$, then we would have $t_{i,j}\left(  \underbrace{u}%
_{=i}\right)  =t_{i,j}\left(  i\right)  =j>i=u$, which would contradict
$t_{i,j}\left(  u\right)  \leq u$. Thus, we cannot have $u=i$. Hence, $u\neq
i$.
\par
Now, we assume (for the sake of contradiction) that $u\leq i$. Combining
$u\neq i$ and $u\neq j$ (since $u\leq i<j$), we obtain $u\notin\left\{
i,j\right\}  $, so that $t_{i,j}\left(  u\right)  =u$ (by
(\ref{sol.ps4.1ab.a.tij}), applied to $k=u$). Hence, $i<t_{i,j}\left(
u\right)  =u\leq i$, which is absurd. Thus, we have obtained a contradiction.
Therefore, our assumption (that $u\leq i$) was wrong. We thus must have $u>i$.
In other words, $i<u$. Combined with $u<v\leq j$, this yields $i<u<j$, so that
$u\in\left\{  i+1,i+2,\ldots,j-1\right\}  $. Now,%
\begin{align*}
c  &  =\left(  u,\underbrace{v}_{=j}\right)  =\left(  u,j\right)  \in\left\{
\left(  k,j\right)  \ \mid\ k\in\left\{  i+1,i+2,\ldots,j-1\right\}  \right\}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\in\left\{  i+1,i+2,\ldots
,j-1\right\}  \right) \\
&  =B\subseteq A\cup B.
\end{align*}
Thus, $c\in A\cup B$ is proven in Case 2.
\par
We therefore have shown that $c\in A\cup B$ in each of the two Cases 1 and 2.
Hence, $c\in A\cup B$ always holds.
\par
Now, let us forget that we fixed $c$. We thus have proven that $c\in A\cup B$
for every $c\in\operatorname*{Inv}\left(  t_{i,j}\right)  $. In other words,
$\operatorname*{Inv}\left(  t_{i,j}\right)  \subseteq A\cup B$, qed.}.
Combined with $A\cup B\subseteq\operatorname*{Inv}\left(  t_{i,j}\right)  $,
this yields $A\cup B=\operatorname*{Inv}\left(  t_{i,j}\right)  $. Thus,
$\left\vert A\cup B\right\vert =\left\vert \operatorname*{Inv}\left(
t_{i,j}\right)  \right\vert $. Compared with $\ell\left(  t_{i,j}\right)
=\left\vert \operatorname*{Inv}\left(  t_{i,j}\right)  \right\vert $, this
yields $\ell\left(  t_{i,j}\right)  =\left\vert A\cup B\right\vert
=2\left\vert j-i\right\vert -1$. This proves (\ref{sol.ps4.1ab.a.claim}).
Exercise \ref{exe.ps4.1ab} \textbf{(a)} is thus solved.
\end{verlong}

\textbf{(b)} \textit{First solution to Exercise \ref{exe.ps4.1ab}
\textbf{(b)}:} From (\ref{sol.ps4.1ab.a.claim}), we have $\ell\left(
t_{i,j}\right)  =2\left\vert j-i\right\vert -1$.

But the integer $2\left\vert j-i\right\vert -1$ is odd. Thus, $\left(
-1\right)  ^{2\left\vert j-i\right\vert -1}=-1$. But the definition of
$\left(  -1\right)  ^{t_{i,j}}$ yields%
\begin{align*}
\left(  -1\right)  ^{t_{i,j}}  &  =\left(  -1\right)  ^{\ell\left(
t_{i,j}\right)  }=\left(  -1\right)  ^{2\left\vert j-i\right\vert
-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  t_{i,j}\right)
=2\left\vert j-i\right\vert -1\right) \\
&  =-1.
\end{align*}
This solves Exercise \ref{exe.ps4.1ab} \textbf{(b)}.

\textit{Second solution to Exercise \ref{exe.ps4.1ab} \textbf{(b)}:} Here is
an alternative solution of Exercise \ref{exe.ps4.1ab} \textbf{(b)} which makes
no use of part \textbf{(a)}.

The set $\left\{  1,2,\ldots,n\right\}  $ has at least two distinct elements
(namely, $i$ and $j$). Hence, $n\geq2$.

There exists a permutation $\sigma\in S_{n}$ such that $\left(  i,j\right)
=\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  \right)
$\ \ \ \ \footnote{\textit{Proof.} Let $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Notice that $n\geq2$ and thus $2\in\left\{
0,1,\ldots,n\right\}  $.
\par
The integers $i$ and $j$ are distinct. Hence, $\left(  i,j\right)  $ is a list
of some elements of $\left[  n\right]  $ such that $i$ and $j$ are distinct.
Therefore, Proposition \ref{prop.perms.lists} \textbf{(c)} (applied to $k=2$
and $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =\left(  i,j\right)  $) yields
that there exists a permutation $\sigma\in S_{n}$ such that $\left(
i,j\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  \right)
$. Qed.}. Consider such a $\sigma$.

We have $\left(  i,j\right)  =\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  \right)  $, thus $\left(  \sigma\left(  1\right)  ,\sigma\left(
2\right)  \right)  =\left(  i,j\right)  $. In other words, $\sigma\left(
1\right)  =i$ and $\sigma\left(  2\right)  =j$.

We have $n\geq2$. Hence, the permutation $s_{1}$ in $S_{n}$ is well-defined.
According to its definition, this permutation $s_{1}$ switches $1$ with $2$
but leaves all other numbers unchanged. In other words, we have $s_{1}\left(
1\right)  =2$, $s_{1}\left(  2\right)  =1$, and%
\begin{equation}
s_{1}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{  1,2\right\}  .
\label{sol.ps4.1ab.b.s1}%
\end{equation}


On the other hand, the permutation $t_{i,j}$ switches $i$ with $j$ while
leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $ unchanged. In
other words, we have $t_{i,j}\left(  i\right)  =j$, $t_{i,j}\left(  j\right)
=i$ and%
\begin{equation}
t_{i,j}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{  i,j\right\}  .
\label{sol.ps4.1ab.b.tij}%
\end{equation}


\begin{vershort}
Now, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots
,n\right\}  $. We need to show that $t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  $.
\par
We are in one of the following three cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $k=2$.
\par
\textit{Case 3:} We have $k\notin\left\{  1,2\right\}  $.
\par
Let us first consider Case 1. In this case, we have $k=1$. Hence,
$t_{i,j}\left(  \sigma\left(  \underbrace{k}_{=1}\right)  \right)
=t_{i,j}\left(  \underbrace{\sigma\left(  1\right)  }_{=i}\right)
=t_{i,j}\left(  i\right)  =j$. Compared with $\sigma\left(  s_{1}\left(
\underbrace{k}_{=1}\right)  \right)  =\sigma\left(  \underbrace{s_{1}\left(
1\right)  }_{=2}\right)  =\sigma\left(  2\right)  =j$, this yields
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(
k\right)  \right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 1.
\par
The proof of $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ in Case 2 is similar and left to the reader.
\par
Let us first consider Case 3. In this case, we have $k\notin\left\{
1,2\right\}  $. Hence, $s_{1}\left(  k\right)  =k$ (by (\ref{sol.ps4.1ab.b.s1}%
)). On the other hand, the map $\sigma$ is a permutation (since $\sigma\in
S_{n}$), thus injective. Now, from $k\neq1$ (since $k\notin\left\{
1,2\right\}  $), we obtain $\sigma\left(  k\right)  \neq\sigma\left(
1\right)  $ (since the map $\sigma$ is injective), so that $\sigma\left(
k\right)  \neq\sigma\left(  1\right)  =i$. Similarly, from $k\neq2$, we can
obtain $\sigma\left(  k\right)  \neq j$. Combining $\sigma\left(  k\right)
\neq i$ with $\sigma\left(  k\right)  \neq j$, we obtain $\sigma\left(
k\right)  \notin\left\{  i,j\right\}  $, and therefore $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  k\right)  $ (by
(\ref{sol.ps4.1ab.b.tij}), applied to $\sigma\left(  k\right)  $ instead of
$k$). Compared with $\sigma\left(  \underbrace{s_{1}\left(  k\right)  }%
_{=k}\right)  =\sigma\left(  k\right)  $, this yields $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 3.
\par
Thus, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ is proven in each of the three Cases 1, 2
and 3. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ always holds, qed.}. Hence, every
$k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $\left(  t_{i,j}\circ
\sigma\right)  \left(  k\right)  =t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  =\left(  \sigma\circ
s_{1}\right)  \left(  k\right)  $. In other words, $t_{i,j}\circ\sigma
=\sigma\circ s_{1}$.
\end{vershort}

\begin{verlong}
Now, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots
,n\right\}  $. We need to show that $t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  $.
\par
We are in one of the following three cases:
\par
\textit{Case 1:} We have $k=1$.
\par
\textit{Case 2:} We have $k=2$.
\par
\textit{Case 3:} We have $k\notin\left\{  1,2\right\}  $.
\par
Let us first consider Case 1. In this case, we have $k=1$. Hence,
$t_{i,j}\left(  \sigma\left(  \underbrace{k}_{=1}\right)  \right)
=t_{i,j}\left(  \underbrace{\sigma\left(  1\right)  }_{=i}\right)
=t_{i,j}\left(  i\right)  =j$. Compared with $\sigma\left(  s_{1}\left(
\underbrace{k}_{=1}\right)  \right)  =\sigma\left(  \underbrace{s_{1}\left(
1\right)  }_{=2}\right)  =\sigma\left(  2\right)  =j$, this yields
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(
k\right)  \right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 1.
\par
Let us next consider Case 2. In this case, we have $k=2$. Hence,
$t_{i,j}\left(  \sigma\left(  \underbrace{k}_{=2}\right)  \right)
=t_{i,j}\left(  \underbrace{\sigma\left(  2\right)  }_{=j}\right)
=t_{i,j}\left(  j\right)  =i$. Compared with $\sigma\left(  s_{1}\left(
\underbrace{k}_{=2}\right)  \right)  =\sigma\left(  \underbrace{s_{1}\left(
2\right)  }_{=1}\right)  =\sigma\left(  1\right)  =i$, this yields
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(
k\right)  \right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 2.
\par
Let us first consider Case 3. In this case, we have $k\notin\left\{
1,2\right\}  $. Hence, $s_{1}\left(  k\right)  =k$ (by (\ref{sol.ps4.1ab.b.s1}%
)). On the other hand, the map $\sigma$ is a permutation (since $\sigma\in
S_{n}$), thus injective. Now, from $k\neq1$ (since $k\notin\left\{
1,2\right\}  $), we obtain $\sigma\left(  k\right)  \neq\sigma\left(
1\right)  $ (since the map $\sigma$ is injective), so that $\sigma\left(
k\right)  \neq\sigma\left(  1\right)  =i$. Also, from $k\neq2$ (since
$k\notin\left\{  1,2\right\}  $), we obtain $\sigma\left(  k\right)
\neq\sigma\left(  2\right)  $ (since the map $\sigma$ is injective), so that
$\sigma\left(  k\right)  \neq\sigma\left(  2\right)  =j$. Combining
$\sigma\left(  k\right)  \neq i$ with $\sigma\left(  k\right)  \neq j$, we
obtain $\sigma\left(  k\right)  \notin\left\{  i,j\right\}  $, and therefore
$t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(  k\right)  $
(by (\ref{sol.ps4.1ab.b.tij}), applied to $\sigma\left(  k\right)  $ instead
of $k$). Compared with $\sigma\left(  \underbrace{s_{1}\left(  k\right)
}_{=k}\right)  =\sigma\left(  k\right)  $, this yields $t_{i,j}\left(
\sigma\left(  k\right)  \right)  =\sigma\left(  s_{1}\left(  k\right)
\right)  $. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)
=\sigma\left(  s_{1}\left(  k\right)  \right)  $ is proven in Case 3.
\par
Thus, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ is proven in each of the three Cases 1, 2
and 3. Hence, $t_{i,j}\left(  \sigma\left(  k\right)  \right)  =\sigma\left(
s_{1}\left(  k\right)  \right)  $ always holds, qed.}. Hence, every
$k\in\left\{  1,2,\ldots,n\right\}  $ satisfies $\left(  t_{i,j}\circ
\sigma\right)  \left(  k\right)  =t_{i,j}\left(  \sigma\left(  k\right)
\right)  =\sigma\left(  s_{1}\left(  k\right)  \right)  =\left(  \sigma\circ
s_{1}\right)  \left(  k\right)  $. In other words, $t_{i,j}\circ\sigma
=\sigma\circ s_{1}$.
\end{verlong}

Recall that $\left(  -1\right)  ^{s_{k}}=-1$ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. Applied to $k=1$, this yields $\left(  -1\right)
^{s_{1}}=-1$.

On the other hand, (\ref{eq.sign.prod}) (applied to $\tau=s_{1}$) yields
$\left(  -1\right)  ^{\sigma\circ s_{1}}=\left(  -1\right)  ^{\sigma}%
\cdot\underbrace{\left(  -1\right)  ^{s_{1}}}_{=-1}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  $.

But (\ref{eq.sign.prod}) (applied to $t_{i,j}$ and $\sigma$ instead of
$\sigma$ and $\tau$) yields $\left(  -1\right)  ^{t_{i,j}\circ\sigma}=\left(
-1\right)  ^{t_{i,j}}\cdot\left(  -1\right)  ^{\sigma}$, so that%
\begin{align*}
\left(  -1\right)  ^{t_{i,j}}\cdot\left(  -1\right)  ^{\sigma}  &  =\left(
-1\right)  ^{t_{i,j}\circ\sigma}=\left(  -1\right)  ^{\sigma\circ s_{1}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }t_{i,j}\circ\sigma=\sigma\circ
s_{1}\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  .
\end{align*}
We can cancel $\left(  -1\right)  ^{\sigma}$ from this equality (since
$\left(  -1\right)  ^{\sigma}\in\left\{  1,-1\right\}  $ is a nonzero
integer), and thus obtain $\left(  -1\right)  ^{t_{i,j}}=-1$. This solves
Exercise \ref{exe.ps4.1ab} \textbf{(b)} again.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.1c}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.1c}.]The inversions of $w_{0}$ are the
pairs $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$w_{0}\left(  i\right)  >w_{0}\left(  j\right)  $ (because this is how we
defined inversions). Since \textbf{every} pair of integers $\left(
i,j\right)  $ satisfying $1\leq i<j\leq n$ automatically satisfies
$w_{0}\left(  i\right)  >w_{0}\left(  j\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  $ be a pair of
integers satisfying $1\leq i<j\leq n$. The definition of $w_{0}$ yields
$w_{0}\left(  i\right)  =n+1-i$ and $w_{0}\left(  j\right)  =n+1-j$. Hence,
$w_{0}\left(  i\right)  =n+1-\underbrace{i}_{<j}>n+1-j=w_{0}\left(  j\right)
$, qed.}, we can simplify this statement as follows: The inversions of $w_{0}$
are the pairs $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$.
But the number of such pairs is $n\left(  n-1\right)  /2$%
\ \ \ \ \footnote{There are two ways to prove this:
\par
\begin{itemize}
\item Either we can argue that these pairs are in a one-to-one correspondence
with the $2$-element subsets of $\left\{  1,2,\ldots,n\right\}  $. (Namely,
any pair $\left(  i,j\right)  $ corresponds to the subset $\left\{
i,j\right\}  $, and conversely, any subset $S$ corresponds to the pair
$\left(  \min S,\max S\right)  $.) Therefore, the number of such pairs equals
the number of all $2$-element subsets of $\left\{  1,2,\ldots,n\right\}  $;
but the latter number is known to be $\dbinom{n}{2}=n\left(  n-1\right)  /2$.
\par
\item Alternatively, we can compute this number as follows: For every pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$, we have
$j\in\left\{  2,3,\ldots,n\right\}  $ (since $1<j\leq n$). Hence,
\begin{align*}
&  \left(  \text{the number of pairs }\left(  i,j\right)  \text{ of integers
satisfying }1\leq i<j\leq n\right) \\
&  =\sum_{k=2}^{n}\underbrace{\left(  \text{the number of pairs } \left(  i,
j\right)  \text{ satisfying }1\leq i<j\leq n\text{ and } j=k\right)
}_{=\left(  \text{the number of integers }i\text{ satisfying }1\leq
i<k\right)  = k-1 }\\
&  =\sum_{k=2}^{n}\left(  k-1\right)  =\sum_{j=1}^{n-1}%
j\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we substituted }j\text{ for
}k-1\text{ in the sum}\right) \\
&  = 1+2+\cdots+\left(  n-1\right)  =\left(  n-1\right)  n/2\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by a famous formula commonly ascribed to
Gauss}\right) \\
&  =n\left(  n-1\right)  /2.
\end{align*}
\end{itemize}
}. Thus, the number of inversions of $w_{0}$ is $n\left(  n-1\right)  /2$. In
other words, $\ell\left(  w_{0}\right)  =n\left(  n-1\right)  /2$ (since
$\ell\left(  w_{0}\right)  $ is defined as the number of inversions of $w_{0}%
$). Therefore, the definition of $\left(  -1\right)  ^{w_{0}}$ yields $\left(
-1\right)  ^{w_{0}}=\left(  -1\right)  ^{\ell\left(  w_{0}\right)  }=\left(
-1\right)  ^{n\left(  n-1\right)  /2}$ (since $\ell\left(  w_{0}\right)
=n\left(  n-1\right)  /2$).

At this point, we could declare Exercise \ref{exe.ps4.1c} to be solved, since
we have found formulas for both $\ell\left(  w_{0}\right)  $ and $\left(
-1\right)  ^{w_{0}}$. Nevertheless, let us give a different expression for
$\left(  -1\right)  ^{w_{0}}$, which can be evaluated faster. Namely, we claim
that%
\begin{equation}
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  . \label{sol.ps4.1c.short.extra}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.1c.short.extra}):} We must be in one of the
following four cases:

\textit{Case 1:} We have $n\equiv0\operatorname{mod}4$.

\textit{Case 2:} We have $n\equiv1\operatorname{mod}4$.

\textit{Case 3:} We have $n\equiv2\operatorname{mod}4$.

\textit{Case 4:} We have $n\equiv3\operatorname{mod}4$.

The proofs of (\ref{sol.ps4.1c.short.extra}) in these four cases are more or
less analogous. Let us only show the proof in Case 4. In this case, we have
$n\equiv3\operatorname{mod}4$. Thus, $n=4m+3$ for some $m\in\mathbb{Z}$.
Consider this $m$. We have%
\begin{align*}
\underbrace{n}_{=4m+3}\underbrace{\left(  n-1\right)  }%
_{\substack{=4m+2\\\text{(since }n=4m+3\text{)}}}/2  &  =\left(  4m+3\right)
\underbrace{\left(  4m+2\right)  /2}_{=2m+1}=\left(  4m+3\right)  \left(
2m+1\right) \\
&  =8m^{2}+10m+3=2\left(  4m^{2}+5m+1\right)  +1.
\end{align*}
Thus, the integer $n\left(  n-1\right)  /2$ is odd, so that $\left(
-1\right)  ^{n\left(  n-1\right)  /2}=-1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=-1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =-1$ (since $n\equiv3\operatorname{mod}4$), this yields $\left(
-1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  $. Thus, (\ref{sol.ps4.1c.short.extra}) is proven in Case 4. The
other three cases are analogous, and so we conclude that
(\ref{sol.ps4.1c.short.extra}) holds.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.1c}.]Let $G=\left\{  \left(  i,j\right)
\in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  $. Then, $\left\vert
G\right\vert =n\left(  n-1\right)  /2$\ \ \ \ \footnote{\textit{Proof.} This
is fairly obvious, but let us nevertheless give a proof for the sake of
completeness.
\par
It is well-known that $\sum_{i=1}^{m}i=m\left(  m+1\right)  /2$ for every
$m\in\mathbb{N}$. Applying this to $m=n-1$, we obtain $\sum_{i=1}%
^{n-1}i=\left(  n-1\right)  \underbrace{\left(  \left(  n-1\right)  +1\right)
}_{=n}/2=\left(  n-1\right)  n/2=n\left(  n-1\right)  /2$.
\par
We have%
\[
\sum_{\substack{\left(  i,j\right)  \in\mathbb{Z}^{2};\\1\leq i<j\leq
n}}1=\left\vert \underbrace{\left\{  \left(  i,j\right)  \in\mathbb{Z}%
^{2}\ \mid\ 1\leq i<j\leq n\right\}  }_{=G}\right\vert \cdot1=\left\vert
G\right\vert \cdot1=\left\vert G\right\vert ,
\]
so that%
\begin{align*}
\left\vert G\right\vert  &  =\underbrace{\sum_{\substack{\left(  i,j\right)
\in\mathbb{Z}^{2};\\1\leq i<j\leq n}}}_{\substack{=\sum_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\1\leq i\leq n;\ i<j\leq n}}\\\text{(since the
condition }1\leq i<j\leq n\\\text{is equivalent to }\left(  1\leq i\leq
n\text{ and }i<j\leq n\right)  \text{)}}}1=\underbrace{\sum_{\substack{\left(
i,j\right)  \in\mathbb{Z}^{2};\\1\leq i\leq n;\ i<j\leq n}}}_{=\sum
_{\substack{i\in\mathbb{Z};\\1\leq i\leq n}}\sum_{\substack{j\in
\mathbb{Z};\\i<j\leq n}}}1=\sum_{\substack{i\in\mathbb{Z};\\1\leq i\leq
n}}\underbrace{\sum_{\substack{j\in\mathbb{Z};\\i<j\leq n}}1}%
_{\substack{=\left\vert \left\{  j\in\mathbb{Z}\ \mid\ i<j\leq n\right\}
\right\vert \cdot1\\=\left\vert \left\{  j\in\mathbb{Z}\ \mid\ i<j\leq
n\right\}  \right\vert }}\\
&  =\sum_{\substack{i\in\mathbb{Z};\\1\leq i\leq n}}\left\vert
\underbrace{\left\{  j\in\mathbb{Z}\ \mid\ i<j\leq n\right\}  }_{=\left\{
i+1,i+2,\ldots,n\right\}  }\right\vert =\underbrace{\sum_{\substack{i\in
\mathbb{Z};\\1\leq i\leq n}}}_{=\sum_{i=1}^{n}}\underbrace{\left\vert \left\{
i+1,i+2,\ldots,n\right\}  \right\vert }_{\substack{=n-i\\\text{(since }i\leq
n\text{)}}}\\
&  =\sum_{i=1}^{n}\left(  n-i\right)  =\sum_{i=0}^{n-1}%
i\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}n-i\text{ in the sum}\right) \\
&  =0+\sum_{i=1}^{n-1}i=\sum_{i=1}^{n-1}i=n\left(  n-1\right)  /2,
\end{align*}
qed.}.

Now, let $\operatorname*{Inv}\left(  w_{0}\right)  $ denote the set of all
inversions of $w_{0}$. Then, $\ell\left(  w_{0}\right)  =\left\vert
\operatorname*{Inv}\left(  w_{0}\right)  \right\vert $%
\ \ \ \ \footnote{\textit{Proof.} Recall that $\ell\left(  w_{0}\right)  $ is
defined as the number of inversions of $w_{0}$. Thus,%
\begin{align*}
\ell\left(  w_{0}\right)   &  =\left(  \text{the number of inversions of
}w_{0}\right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
w_{0}\right)  }_{=\operatorname*{Inv}\left(  w_{0}\right)  }\right\vert
=\left\vert \operatorname*{Inv}\left(  w_{0}\right)  \right\vert ,
\end{align*}
qed.}.

On the other hand, $\operatorname*{Inv}\left(  w_{0}\right)  \subseteq
G$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
w_{0}\right)  $. We shall show that $c\in G$.
\par
We have $c\in\operatorname*{Inv}\left(  w_{0}\right)  $. In other words, $c$
is an inversion of $w_{0}$ (since $\operatorname*{Inv}\left(  w_{0}\right)  $
is the set of all inversions of $w_{0}$). In other words, $c$ is a pair
$\left(  u,v\right)  $ of integers satisfying $1\leq u<v\leq n$ and
$w_{0}\left(  u\right)  >w_{0}\left(  v\right)  $. Consider this $\left(
u,v\right)  $. We have $c=\left(  u,v\right)  $ and $1\leq u<v\leq n$. Thus,
$c$ has the form $c=\left(  i,j\right)  $ for a pair $\left(  i,j\right)
\in\mathbb{Z}^{2}$ satisfying $1\leq i<j\leq n$ (namely, $\left(  i,j\right)
=\left(  u,v\right)  $). In other words, $c\in\left\{  \left(  i,j\right)
\in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq n\right\}  =G$.
\par
Let us now forget that we fixed $c$. We thus have shown that $c\in G$ for
every $c\in\operatorname*{Inv}\left(  w_{0}\right)  $. In other words,
$\operatorname*{Inv}\left(  w_{0}\right)  \subseteq G$, qed.} and
$G\subseteq\operatorname*{Inv}\left(  w_{0}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in G$. We shall prove that
$c\in\operatorname*{Inv}\left(  w_{0}\right)  $.
\par
We have $c\in G=\left\{  \left(  i,j\right)  \in\mathbb{Z}^{2}\ \mid\ 1\leq
i<j\leq n\right\}  $. In other words, $c$ can be written in the form
$c=\left(  i,j\right)  $ for some $\left(  i,j\right)  \in\mathbb{Z}^{2}$
satisfying $1\leq i<j\leq n$. Consider this $\left(  i,j\right)  $.
\par
The definition of $w_{0}$ yields $w_{0}\left(  j\right)  =n+1-j$ and
$w_{0}\left(  i\right)  =n+1-\underbrace{i}_{<j}>n+1-j=w_{0}\left(  j\right)
$. Hence, $\left(  i,j\right)  $ is a pair of integers satisfying $1\leq
i<j\leq n$ and $w_{0}\left(  i\right)  >w_{0}\left(  j\right)  $. In other
words, $\left(  i,j\right)  $ is an inversion of $w_{0}$ (by the definition of
an \textquotedblleft inversion of $w_{0}$\textquotedblright). In other words,
$\left(  i,j\right)  \in\operatorname*{Inv}\left(  w_{0}\right)  $ (since
$\operatorname*{Inv}\left(  w_{0}\right)  $ is the set of all inversions of
$w_{0}$). Thus, $c=\left(  i,j\right)  \in\operatorname*{Inv}\left(
w_{0}\right)  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  w_{0}\right)  $ for every $c\in G$. In other words,
$G\subseteq\operatorname*{Inv}\left(  w_{0}\right)  $, qed.}. Combining these
two relations, we obtain $\operatorname*{Inv}\left(  w_{0}\right)  =G$. Thus,
$\left\vert \underbrace{\operatorname*{Inv}\left(  w_{0}\right)  }%
_{=G}\right\vert =\left\vert G\right\vert =n\left(  n-1\right)  /2$, so that
$\ell\left(  w_{0}\right)  =\left\vert \operatorname*{Inv}\left(
w_{0}\right)  \right\vert =n\left(  n-1\right)  /2$.

Now, the definition of $\left(  -1\right)  ^{w_{0}}$ yields $\left(
-1\right)  ^{w_{0}}=\left(  -1\right)  ^{\ell\left(  w_{0}\right)  }=\left(
-1\right)  ^{n\left(  n-1\right)  /2}$ (since $\ell\left(  w_{0}\right)
=n\left(  n-1\right)  /2$).

At this point, we could declare Exercise \ref{exe.ps4.1c} to be solved, since
we have found formulas for both $\ell\left(  w_{0}\right)  $ and $\left(
-1\right)  ^{w_{0}}$. Nevertheless, let us give a different expression for
$\left(  -1\right)  ^{w_{0}}$, which can be evaluated faster. Namely, we claim
that%
\begin{equation}
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  . \label{sol.ps4.1c.extra}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.1c.extra}):} We must be in one of the following
four cases:

\textit{Case 1:} We have $n\equiv0\operatorname{mod}4$.

\textit{Case 2:} We have $n\equiv1\operatorname{mod}4$.

\textit{Case 3:} We have $n\equiv2\operatorname{mod}4$.

\textit{Case 4:} We have $n\equiv3\operatorname{mod}4$.

Let us first consider Case 1. In this case, we have $n\equiv
0\operatorname{mod}4$. Thus, $n=4m$ for some $m\in\mathbb{Z}$. Consider this
$m$. We have $\underbrace{n}_{=4m}\left(  n-1\right)  /2=4m\left(  n-1\right)
/2=2m\left(  n-1\right)  $. Thus, the integer $n\left(  n-1\right)  /2$ is
even, so that $\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =1$ (since $n\equiv0\operatorname{mod}4$ or $n\equiv
1\operatorname{mod}4$ (since $n\equiv0\operatorname{mod}4$)), this yields
\[
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  .
\]
Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 1.

Let us next consider Case 2. In this case, we have $n\equiv1\operatorname{mod}%
4$. Thus, $n=4m+1$ for some $m\in\mathbb{Z}$. Consider this $m$. We have
$n\underbrace{\left(  n-1\right)  }_{\substack{=4m\\\text{(since
}n=4m+1\text{)}}}/2=n\cdot4m/2=2nm$. Thus, the integer $n\left(  n-1\right)
/2$ is even, so that $\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =1$ (since $n\equiv0\operatorname{mod}4$ or $n\equiv
1\operatorname{mod}4$ (since $n\equiv1\operatorname{mod}4$)), this yields
\[
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  .
\]
Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 2.

Let us next consider Case 3. In this case, we have $n\equiv2\operatorname{mod}%
4$. Thus, $n=4m+2$ for some $m\in\mathbb{Z}$. Consider this $m$. We have%
\begin{align*}
\underbrace{n}_{\substack{=4m+2\\=2\left(  2m+1\right)  }}\underbrace{\left(
n-1\right)  }_{\substack{=4m+1\\\text{(since }n=4m+2\text{)}}}/2  &  =2\left(
2m+1\right)  \left(  4m+1\right)  /2=\left(  2m+1\right)  \left(  4m+1\right)
\\
&  =8m^{2}+6m+1=2\left(  4m^{2}+3m\right)  +1.
\end{align*}
Thus, the integer $n\left(  n-1\right)  /2$ is odd, so that $\left(
-1\right)  ^{n\left(  n-1\right)  /2}=-1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=-1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =-1$ (since $n\equiv2\operatorname{mod}4$ or $n\equiv
3\operatorname{mod}4$ (since $n\equiv2\operatorname{mod}4$)), this yields
\[
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  .
\]
Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 3.

Let us finally consider Case 4. In this case, we have $n\equiv
3\operatorname{mod}4$. Thus, $n=4m+3$ for some $m\in\mathbb{Z}$. Consider this
$m$. We have%
\begin{align*}
\underbrace{n}_{=4m+3}\underbrace{\left(  n-1\right)  }%
_{\substack{=4m+2\\\text{(since }n=4m+3\text{)}}}/2  &  =\left(  4m+3\right)
\underbrace{\left(  4m+2\right)  /2}_{=2m+1}=\left(  4m+3\right)  \left(
2m+1\right) \\
&  =8m^{2}+10m+3=2\left(  4m^{2}+5m+1\right)  +1.
\end{align*}
Thus, the integer $n\left(  n-1\right)  /2$ is odd, so that $\left(
-1\right)  ^{n\left(  n-1\right)  /2}=-1$. Hence,%
\[
\left(  -1\right)  ^{w_{0}}=\left(  -1\right)  ^{n\left(  n-1\right)  /2}=-1.
\]
Compared with $\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  =-1$ (since $n\equiv2\operatorname{mod}4$ or $n\equiv
3\operatorname{mod}4$ (since $n\equiv3\operatorname{mod}4$)), this yields
\[
\left(  -1\right)  ^{w_{0}}=\left\{
\begin{array}
[c]{c}%
1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv0\operatorname{mod}4\text{ or }%
n\equiv1\operatorname{mod}4;\\
-1,\ \ \ \ \ \ \ \ \ \ \text{if }n\equiv2\operatorname{mod}4\text{ or }%
n\equiv3\operatorname{mod}4
\end{array}
\right.  .
\]
Thus, (\ref{sol.ps4.1c.extra}) is proven in Case 4.

We now have proven (\ref{sol.ps4.1c.extra}) in each of the four Cases 1, 2, 3
and 4. Thus, (\ref{sol.ps4.1c.extra}) always holds. This finishes our solution
of Exercise \ref{exe.ps4.1c}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.ps4.2}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.2}.]\textbf{(a)} Let $\sigma$ be a
permutation of $X$. We need to prove that $\left(  -1\right)  _{\phi}^{\sigma
}$ depends only on the permutation $\sigma$ of $X$, but not on the bijection
$\phi$. In other words, we need to prove that any two different choices of
$\phi$ will lead to the same $\left(  -1\right)  _{\phi}^{\sigma}$. In other
words, we need to prove that if $\phi_{1}$ and $\phi_{2}$ are two bijections
$\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$
(possibly distinct), then $\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(
-1\right)  _{\phi_{2}}^{\sigma}$.

So let $\phi_{1}$ and $\phi_{2}$ be two bijections $\phi:X\rightarrow\left\{
1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$ (possibly distinct). We must
show that $\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(  -1\right)
_{\phi_{2}}^{\sigma}$.

The map $\sigma$ is a permutation of $X$, thus a bijection $X\rightarrow X$.

\begin{vershort}
We know that $\phi_{1}$ is a bijection $X\rightarrow\left\{  1,2,\ldots
,n\right\}  $ for some $n\in\mathbb{N}$. In other words, there exist some
$n\in\mathbb{N}$ such that $\phi_{1}$ is a bijection $X\rightarrow\left\{
1,2,\ldots,n\right\}  $. Denote this $n$ by $n_{1}$. Thus, $\phi_{1}$ is a
bijection $X\rightarrow\left\{  1,2,\ldots,n_{1}\right\}  $. The definition of
$\left(  -1\right)  _{\phi_{1}}^{\sigma}$ yields $\left(  -1\right)
_{\phi_{1}}^{\sigma}=\left(  -1\right)  ^{\phi_{1}\circ\sigma\circ\phi
_{1}^{-1}}$.
\end{vershort}

\begin{verlong}
We know that $\phi_{1}$ is a bijection $\phi:X\rightarrow\left\{
1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$. In other words, $\phi_{1}$
is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. In other words, there exist some $n\in\mathbb{N}$ such that
$\phi_{1}$ is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $.
Denote this $n$ by $n_{1}$. Thus, $\phi_{1}$ is a bijection $X\rightarrow
\left\{  1,2,\ldots,n_{1}\right\}  $. The definition of $\left(  -1\right)
_{\phi_{1}}^{\sigma}$ yields $\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(
-1\right)  ^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}$.
\end{verlong}

The map $\phi_{1}$ is a bijection. Thus, its inverse $\phi_{1}^{-1}$ is
well-defined and also a bijection.

The map $\phi_{1}\circ\sigma\circ\phi_{1}^{-1}:\left\{  1,2,\ldots
,n_{1}\right\}  \rightarrow\left\{  1,2,\ldots,n_{1}\right\}  $ is a bijection
(since it is a composition of the three bijections $\phi_{1}$, $\sigma$ and
$\phi_{1}^{-1}$). In other words, the map $\phi_{1}\circ\sigma\circ\phi
_{1}^{-1}$ is a permutation of $\left\{  1,2,\ldots,n_{1}\right\}  $. Thus,
$\phi_{1}\circ\sigma\circ\phi_{1}^{-1}\in S_{n_{1}}$.

\begin{vershort}
We thus have shown that $\phi_{1}^{-1}$ is well-defined and a bijection, and
found an $n_{1}\in\mathbb{N}$ such that $\phi_{1}\circ\sigma\circ\phi_{1}%
^{-1}\in S_{n_{1}}$. Similarly, we can show that $\phi_{2}^{-1}$ is
well-defined and a bijection, and find an $n_{2}\in\mathbb{N}$ such that
$\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\in S_{n_{2}}$. Consider this $n_{2}$.
(We shall soon see that $n_{1}=n_{2}$.) We have $\left(  -1\right)  _{\phi
_{2}}^{\sigma}=\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}}$
(by the definition of $\left(  -1\right)  _{\phi_{2}}^{\sigma}$).
\end{vershort}

\begin{verlong}
We know that $\phi_{2}$ is a bijection $\phi:X\rightarrow\left\{
1,2,\ldots,n\right\}  $ for some $n\in\mathbb{N}$. In other words, $\phi_{2}$
is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. In other words, there exist some $n\in\mathbb{N}$ such that
$\phi_{2}$ is a bijection $X\rightarrow\left\{  1,2,\ldots,n\right\}  $.
Denote this $n$ by $n_{2}$. Thus, $\phi_{2}$ is a bijection $X\rightarrow
\left\{  1,2,\ldots,n_{2}\right\}  $. The definition of $\left(  -1\right)
_{\phi_{2}}^{\sigma}$ yields $\left(  -1\right)  _{\phi_{2}}^{\sigma}=\left(
-1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}}$. (We will soon see that
$n_{1}=n_{2}$.)

The map $\phi_{2}$ is a bijection. Thus, its inverse $\phi_{2}^{-1}$ is
well-defined and also a bijection.

The map $\phi_{2}\circ\sigma\circ\phi_{2}^{-1}:\left\{  1,2,\ldots
,n_{2}\right\}  \rightarrow\left\{  1,2,\ldots,n_{2}\right\}  $ is a bijection
(since it is a composition of the three bijections $\phi_{2}$, $\sigma$ and
$\phi_{2}^{-1}$). In other words, the map $\phi_{2}\circ\sigma\circ\phi
_{2}^{-1}$ is a permutation of $\left\{  1,2,\ldots,n_{2}\right\}  $. Thus,
$\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\in S_{n_{2}}$.
\end{verlong}

There exists a bijection from $X$ to $\left\{  1,2,\ldots,n_{1}\right\}  $
(namely, $\phi_{1}$). Hence, $\left\vert X\right\vert =\left\vert \left\{
1,2,\ldots,n_{1}\right\}  \right\vert =n_{1}$. Similarly, $\left\vert
X\right\vert =n_{2}$. Thus, $n_{1}=\left\vert X\right\vert =n_{2}$. Thus, we
can define an $n\in\mathbb{N}$ by $n=n_{1}=n_{2}$. Consider this $n$.

The map $\phi_{2}\circ\phi_{1}^{-1}:\left\{  1,2,\ldots,n_{1}\right\}
\rightarrow\left\{  1,2,\ldots,n_{2}\right\}  $ is a bijection (since it is a
composition of two bijections). Since $n_{1}=n$ and $n_{2}=n$, this rewrites
as follows: The map $\phi_{2}\circ\phi_{1}^{-1}:\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}  $ is a bijection. In other words,
the map $\phi_{2}\circ\phi_{1}^{-1}$ is a permutation of $\left\{
1,2,\ldots,n\right\}  $. Thus, $\phi_{2}\circ\phi_{1}^{-1}\in S_{n}$.

We have $\phi_{1}\circ\sigma\circ\phi_{1}^{-1}\in S_{n_{1}}=S_{n}$ (since
$n_{1}=n$) and $\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\in S_{n_{2}}=S_{n}$
(since $n_{2}=n$).

Now, (\ref{eq.sign.prod}) (applied to $\phi_{2}\circ\phi_{1}^{-1}$ and
$\phi_{1}\circ\sigma\circ\phi_{1}^{-1}$ instead of $\sigma$ and $\tau$) yields%
\[
\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}\circ\phi_{1}\circ\sigma
\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}%
\cdot\left(  -1\right)  ^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}.
\]
Since $\phi_{2}\circ\underbrace{\phi_{1}^{-1}\circ\phi_{1}}%
_{=\operatorname*{id}}\circ\sigma\circ\phi_{1}^{-1}=\phi_{2}\circ\sigma
\circ\phi_{1}^{-1}$, this rewrites as
\begin{equation}
\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)
^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{1}\circ
\sigma\circ\phi_{1}^{-1}}. \label{sol.ps4.2.1}%
\end{equation}


On the other hand, (\ref{eq.sign.prod}) (applied to $\phi_{2}\circ\sigma
\circ\phi_{2}^{-1}$ and $\phi_{2}\circ\phi_{1}^{-1}$ instead of $\sigma$ and
$\tau$) yields%
\[
\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}^{-1}\circ\phi_{2}%
\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{2}%
^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}=\left(  -1\right)
^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ
\sigma\circ\phi_{2}^{-1}}.
\]
Since $\phi_{2}\circ\sigma\circ\underbrace{\phi_{2}^{-1}\circ\phi_{2}%
}_{=\operatorname*{id}}\circ\phi_{1}^{-1}=\phi_{2}\circ\sigma\circ\phi
_{1}^{-1}$, this rewrites as
\[
\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)
^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ
\sigma\circ\phi_{2}^{-1}}.
\]
Comparing this with (\ref{sol.ps4.2.1}), we obtain
\[
\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}\cdot\left(  -1\right)
^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}%
\circ\phi_{1}^{-1}}\cdot\left(  -1\right)  ^{\phi_{2}\circ\sigma\circ\phi
_{2}^{-1}}.
\]
We can cancel $\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}$ from this
equality (since $\left(  -1\right)  ^{\phi_{2}\circ\phi_{1}^{-1}}\in\left\{
1,-1\right\}  $ is a nonzero integer), and thus obtain $\left(  -1\right)
^{\phi_{1}\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}%
\circ\sigma\circ\phi_{2}^{-1}}$. Hence,%
\[
\left(  -1\right)  _{\phi_{1}}^{\sigma}=\left(  -1\right)  ^{\phi_{1}%
\circ\sigma\circ\phi_{1}^{-1}}=\left(  -1\right)  ^{\phi_{2}\circ\sigma
\circ\phi_{2}^{-1}}=\left(  -1\right)  _{\phi_{2}}^{\sigma}.
\]
As we know, this completes the solution of Exercise \ref{exe.ps4.2}
\textbf{(a)}.

\textbf{(b)} Fix a bijection $\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}
$ for some $n\in\mathbb{N}$. (Such a bijection always exists.) We shall denote
the identity permutation of $X$ by $\operatorname*{id}\nolimits_{X}$, so as to
distinguish it from the identity permutation of $\left\{  1,2,\ldots
,n\right\}  $ (which we keep denoting by $\operatorname*{id}$ without a
subscript). The definition of $\left(  -1\right)  ^{\operatorname*{id}%
\nolimits_{X}}$ now yields
\begin{align*}
\left(  -1\right)  ^{\operatorname*{id}\nolimits_{X}}  &  =\left(  -1\right)
_{\phi}^{\operatorname*{id}\nolimits_{X}}=\left(  -1\right)  ^{\phi
\circ\operatorname*{id}\nolimits_{X}\circ\phi^{-1}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\left(  -1\right)  _{\phi}^{\operatorname*{id}%
\nolimits_{X}}\right) \\
&  =\left(  -1\right)  ^{\operatorname*{id}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\phi\circ\operatorname*{id}\nolimits_{X}\circ\phi^{-1}=\phi
\circ\phi^{-1}=\operatorname*{id}\right) \\
&  =1.
\end{align*}
In other words, $\left(  -1\right)  ^{\operatorname*{id}}=1$ for the identity
permutation $\operatorname*{id}:X\rightarrow X$ of $X$. Exercise
\ref{exe.ps4.2} \textbf{(b)} is thus solved.

\textbf{(c)} Let $\sigma$ and $\tau$ be two permutations of $X$. Fix a
bijection $\phi:X\rightarrow\left\{  1,2,\ldots,n\right\}  $ for some
$n\in\mathbb{N}$. (Such a bijection always exists.) The definition of $\left(
-1\right)  ^{\sigma}$ yields%
\begin{equation}
\left(  -1\right)  ^{\sigma}=\left(  -1\right)  _{\phi}^{\sigma}=\left(
-1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\left(  -1\right)  _{\phi}^{\sigma}\right)  .
\label{sol.ps4.2.c.1}%
\end{equation}
The definition of $\left(  -1\right)  ^{\tau}$ yields%
\begin{equation}
\left(  -1\right)  ^{\tau}=\left(  -1\right)  _{\phi}^{\tau}=\left(
-1\right)  ^{\phi\circ\tau\circ\phi^{-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\left(  -1\right)  _{\phi}^{\tau}\right)  .
\label{sol.ps4.2.c.2}%
\end{equation}
The maps $\phi\circ\sigma\circ\phi^{-1}$ and $\phi\circ\tau\circ\phi^{-1}$ are
permutations in $S_{n}$. Therefore, (\ref{eq.sign.prod}) (applied to
$\phi\circ\sigma\circ\phi^{-1}$ and $\phi\circ\tau\circ\phi^{-1}$ instead of
$\sigma$ and $\tau$) yields%
\[
\left(  -1\right)  ^{\phi\circ\sigma\circ\phi^{-1}\circ\phi\circ\tau\circ
\phi^{-1}}=\left(  -1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}\cdot\left(
-1\right)  ^{\phi\circ\tau\circ\phi^{-1}}.
\]
Since $\phi\circ\sigma\circ\underbrace{\phi^{-1}\circ\phi}%
_{=\operatorname*{id}}\circ\tau\circ\phi^{-1}=\phi\circ\sigma\circ\tau
\circ\phi^{-1}$, this rewrites as
\[
\left(  -1\right)  ^{\phi\circ\sigma\circ\tau\circ\phi^{-1}}=\left(
-1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}\cdot\left(  -1\right)  ^{\phi
\circ\tau\circ\phi^{-1}}.
\]
But the definition of $\left(  -1\right)  ^{\sigma\circ\tau}$ yields%
\begin{align*}
\left(  -1\right)  ^{\sigma\circ\tau}  &  =\left(  -1\right)  _{\phi}%
^{\sigma\circ\tau}=\left(  -1\right)  ^{\phi\circ\sigma\circ\tau\circ\phi
^{-1}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(
-1\right)  _{\phi}^{\sigma\circ\tau}\right) \\
&  =\underbrace{\left(  -1\right)  ^{\phi\circ\sigma\circ\phi^{-1}}%
}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by (\ref{sol.ps4.2.c.1}))}%
}}\cdot\underbrace{\left(  -1\right)  ^{\phi\circ\tau\circ\phi^{-1}}%
}_{\substack{=\left(  -1\right)  ^{\tau}\\\text{(by (\ref{sol.ps4.2.c.2}))}%
}}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}.
\end{align*}
This solves Exercise \ref{exe.ps4.2} \textbf{(c)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.perm.sign.pseudoexplicit}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.perm.sign.pseudoexplicit}.]\textbf{(b)} We
start with some trivia on sets and subsets.

If $A$ is any set, then $\mathcal{P}_{2}\left(  A\right)  $ shall denote the
set of all $2$-element subsets of $A$. In other words, $\mathcal{P}_{2}\left(
A\right)  $ is defined to be $\left\{  S\subseteq A\ \mid\ \left\vert
S\right\vert =2\right\}  $. For instance,
\begin{align*}
\mathcal{P}_{2}\left(  \left\{  3,6,7\right\}  \right)   &  =\left\{  \left\{
3,6\right\}  ,\left\{  3,7\right\}  ,\left\{  6,7\right\}  \right\}  ;\\
\mathcal{P}_{2}\left(  \left\{  1,2,3,4\right\}  \right)   &  =\left\{
\left\{  1,2\right\}  ,\left\{  1,3\right\}  ,\left\{  1,4\right\}  ,\left\{
2,3\right\}  ,\left\{  2,4\right\}  ,\left\{  3,4\right\}  \right\}  ;\\
\mathcal{P}_{2}\left(  \left\{  3\right\}  \right)   &  =\varnothing;\\
\mathcal{P}_{2}\left(  \varnothing\right)   &  =\varnothing.
\end{align*}
\footnote{Several authors write $\dbinom{A}{2}$ for $\mathcal{P}_{2}\left(
A\right)  $. This notation looks like a binomial coefficient, but with $A$ at
the top. Of course, this notation is chosen for its suggestiveness: When $A$
is finite, the set $\dbinom{A}{2}$ satisfies $\left\vert \dbinom{A}%
{2}\right\vert =\dbinom{\left\vert A\right\vert }{2}$.}

If $A$ and $B$ are two sets, and if $f:A\rightarrow B$ is an injective map,
then we can define a map $f_{\ast}:\mathcal{P}_{2}\left(  A\right)
\rightarrow\mathcal{P}_{2}\left(  B\right)  $ by%
\[
\left(  f_{\ast}\left(  S\right)  =f\left(  S\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }S\in\mathcal{P}_{2}\left(  A\right)
\right)  .
\]
\footnote{At this point, we need to check that this map $f_{\ast}$ is
well-defined. Before I do this, let me rewrite the definition of $f_{\ast}$ in
a more intuitive way: An element of $\mathcal{P}_{2}\left(  A\right)  $ is a
$2$-element subset $\left\{  a,a^{\prime}\right\}  $ of $A$. The map $f_{\ast
}$ takes this subset to $\left\{  f\left(  a\right)  ,f\left(  a^{\prime
}\right)  \right\}  $ (in other words, it applies $f$ to each of its
elements).
\par
So why is the map $f_{\ast}$ well-defined? It is supposed to send every
$S\in\mathcal{P}_{2}\left(  A\right)  $ to $f\left(  S\right)  $. Thus, in
order to prove that it is well-defined, we need to show that $f\left(
S\right)  \in\mathcal{P}_{2}\left(  B\right)  $ for every $S\in\mathcal{P}%
_{2}\left(  A\right)  $.
\par
Let $S\in\mathcal{P}_{2}\left(  A\right)  $. Thus, the set $S$ is a
$2$-element subset of $A$. The map $f$ sends its two elements to two
\textbf{distinct} elements of $B$ (they are distinct because $f$ is
injective). In other words, the set $f\left(  S\right)  $ has $2$ elements.
Thus, $f\left(  S\right)  $ is a $2$-element subset of $B$; in other words,
$f\left(  S\right)  \in\mathcal{P}_{2}\left(  B\right)  $. This proves that
the map $f_{\ast}$ is well-defined.
\par
Notice that we have used the injectivity of $f$ in this argument.} This
construction has the following three basic properties:

\begin{enumerate}
\item If $A$ is a set, then $\left(  \operatorname*{id}\nolimits_{A}\right)
_{\ast}=\operatorname*{id}\nolimits_{\mathcal{P}_{2}\left(  A\right)  }$.

\item If $A$, $B$ and $C$ are three sets, and if $f:A\rightarrow B$ and
$g:B\rightarrow C$ are two injective maps, then $\left(  g\circ f\right)
_{\ast}=g_{\ast}\circ f_{\ast}$. (Of course, $g\circ f$ is injective here, so
$\left(  g\circ f\right)  _{\ast}$ makes sense.)

\item If $A$ and $B$ are two sets, and if $f:A\rightarrow B$ is an invertible
map, then $f_{\ast}$ is invertible as well and satisfies $\left(
f^{-1}\right)  _{\ast}=\left(  f_{\ast}\right)  ^{-1}$.
\end{enumerate}

(The first of these three properties is obvious; the second follows by
observing that $\left(  g\circ f\right)  \left(  S\right)  =g\left(  f\left(
S\right)  \right)  $ for every $S\in\mathcal{P}_{2}\left(  A\right)  $; the
third can be proven using the second or directly.)

Let $\left[  n\right]  $ be the set $\left\{  1,2,\ldots,n\right\}  $. Recall
that $S_{n}$ is the set of all permutations of the set $\left\{
1,2,\ldots,n\right\}  $. In other words, $S_{n}$ is the set of all
permutations of the set $\left[  n\right]  $ (since $\left\{  1,2,\ldots
,n\right\}  =\left[  n\right]  $).

We have $\sigma\in S_{n}$. In other words, $\sigma$ is a permutation of the
set $\left[  n\right]  $ (since $S_{n}$ is the set of all such permutations).
Thus, $\sigma$ is a bijective map $\left[  n\right]  \rightarrow\left[
n\right]  $. In particular, the map $\sigma_{\ast}:\mathcal{P}_{2}\left(
\left[  n\right]  \right)  \rightarrow\mathcal{P}_{2}\left(  \left[  n\right]
\right)  $ is well-defined.

Recall that if $A$ and $B$ are two sets, and if $f:A\rightarrow B$ is an
invertible map, then $f_{\ast}$ is invertible as well. Applying this to
$A=\left[  n\right]  $, $B=\left[  n\right]  $ and $f=\sigma$, we conclude
that $\sigma_{\ast}$ is invertible (since $\sigma$ is invertible).

Let $G$ be the subset%
\[
\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}
\]
of $\left[  n\right]  ^{2}$.

For example, if $n=4$, then%
\[
G=\left\{  \left(  1,2\right)  ,\left(  1,3\right)  ,\left(  1,4\right)
,\left(  2,3\right)  ,\left(  2,4\right)  ,\left(  3,4\right)  \right\}  .
\]
Comparing this with%
\[
\mathcal{P}_{2}\left(  \left[  4\right]  \right)  =\left\{  \left\{
1,2\right\}  ,\left\{  1,3\right\}  ,\left\{  1,4\right\}  ,\left\{
2,3\right\}  ,\left\{  2,4\right\}  ,\left\{  3,4\right\}  \right\}  ,
\]
we observe that the set $\mathcal{P}_{2}\left(  \left[  n\right]  \right)  $
is obtained from $G$ by \textquotedblleft replacing all parentheses by
brackets\textquotedblright\ (i.e., replacing each $\left(  i,j\right)  \in G$
by $\left\{  i,j\right\}  $). This holds for all $n$, not just for $n=4$. Let
us make this observation somewhat more bulletproof: We can define a map
$\rho:G\rightarrow\mathcal{P}_{2}\left(  \left[  n\right]  \right)  $ by
setting%
\[
\left(  \rho\left(  \left(  i,j\right)  \right)  =\left\{  i,j\right\}
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)  \in G\right)  .
\]
This map $\rho$ is injective (indeed, we can reconstruct every $\left(
i,j\right)  \in G$ from its image $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $, because $\left(  i,j\right)  \in G$ entails $i<j$)
and surjective (since every two-element subset $S$ of $\left[  n\right]  $ has
the form $\left\{  i,j\right\}  $ for some $\left(  i,j\right)  \in\left[
n\right]  ^{2}$ satisfying $i<j$). Hence, the map $\rho$ is bijective.

For every $S\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  $, we define
an element $a_{S}$ of $\mathbb{K}$ by%
\[
a_{S}=a_{\left(  \min S,\max S\right)  }.
\]
(In other words, for every $S\in\mathcal{P}_{2}\left(  \left[  n\right]
\right)  $, we define an element $a_{S}$ of $\mathbb{K}$ by $a_{S}=a_{\left(
i,j\right)  }$, where $i$ and $j$ are the two elements of $S$ in increasing order.)

Let $\operatorname*{Inv}\left(  \sigma\right)  $ be the set of inversions of
$\sigma$. Then, $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }$%
\ \ \ \ \footnote{\textit{Proof.} The definition of $\ell\left(
\sigma\right)  $ shows that%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  =\left(  \text{the number of elements of }\operatorname*{Inv}%
\left(  \sigma\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\sigma\right)  \text{ is the set of all inversions of }\sigma\right) \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\end{align*}
\par
But the definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }=\left(
-1\right)  ^{\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert
}$ (since $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert $), qed.}. Furthermore, $\operatorname*{Inv}\left(
\sigma\right)  \subseteq G$\ \ \ \ \footnote{\textit{Proof.} Let
$c\in\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $c$ is an inversion of
$\sigma$ (since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of
inversions of $\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $
of integers satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. Consider this $\left(  i,j\right)  $. We have
$i\in\left[  n\right]  $ (since $1\leq i\leq n$) and $j\in\left[  n\right]  $
(since $1\leq j\leq n$). Thus, $\left(  i,j\right)  \in\left[  n\right]  ^{2}%
$. Thus, $\left(  i,j\right)  $ is an element of $\left[  n\right]  ^{2}$ and
satisfies $i<j$. In other words, $\left(  i,j\right)  \in\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}  =G$. Hence,
$c=\left(  i,j\right)  \in G$.
\par
Now, let us forget that we fixed $c$. We thus have proven that every
$c\in\operatorname*{Inv}\left(  \sigma\right)  $ satisfies $c\in G$. In other
words, $\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$, qed.}.

Now, we notice the following facts:

\begin{itemize}
\item For every $\left(  i,j\right)  \in G$, we have%
\begin{equation}
a_{\left(  i,j\right)  }=a_{\rho\left(  \left(  i,j\right)  \right)  }.
\label{sol.perm.sign.pseudoexplicit.short.b.a1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.short.b.a1}):}
Let $\left(  i,j\right)  \in G$. Thus, $\left(  i,j\right)  \in G=\left\{
\left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}  $. In other
words, $\left(  i,j\right)  \in\left[  n\right]  ^{2}$ and $i<j$.
\par
The definition of $\rho$ shows that $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $. Since $i<j$, this shows that the elements of the set
$\rho\left(  \left(  i,j\right)  \right)  $ listed in increasing order are $i$
and $j$. Hence, $\min\left(  \rho\left(  \left(  i,j\right)  \right)  \right)
=i$ and $\max\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  =j$.
\par
Now, the definition of $a_{\rho\left(  \left(  i,j\right)  \right)  }$ shows
that $a_{\rho\left(  \left(  i,j\right)  \right)  }=a_{\left(  \min\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  ,\max\left(  \rho\left(
\left(  i,j\right)  \right)  \right)  \right)  }=a_{\left(  i,j\right)  }$
(since $\min\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  =i$ and
$\max\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  =j$). This
proves (\ref{sol.perm.sign.pseudoexplicit.short.b.a1}).}

\item For every $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  $, we have%
\begin{equation}
a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=-a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }.
\label{sol.perm.sign.pseudoexplicit.short.b.a2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.short.b.a2}):}
Let $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $. Thus,
$\left(  i,j\right)  $ is an inversion of $\sigma$. In other words, $\left(
i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $.
\par
The definition of $\rho$ shows that $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $. Hence, $\sigma\left(  \underbrace{\rho\left(
\left(  i,j\right)  \right)  }_{=\left\{  i,j\right\}  }\right)
=\sigma\left(  \left\{  i,j\right\}  \right)  =\left\{  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right\}  $. Since $\sigma\left(  i\right)
>\sigma\left(  j\right)  $, this shows that the elements of the set
$\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  $ listed in
increasing order are $\sigma\left(  j\right)  $ and $\sigma\left(  i\right)
$. Hence, $\min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  =\sigma\left(  j\right)  $ and $\max\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
i\right)  $.
\par
Now, the definition of $a_{\sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  }$ shows that%
\begin{align*}
a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }  &
=a_{\left(  \min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  ,\max\left(  \sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  \right)  \right)  }=a_{\left(  \sigma\left(  j\right)
,\sigma\left(  i\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\min\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
j\right)  \text{ and }\max\left(  \sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  \right)  =\sigma\left(  i\right)  \right) \\
&  =-a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.exe.perm.sign.pseudoexplicit.b.skew}), applied to }\sigma\left(
i\right)  \text{ and }\sigma\left(  j\right)  \text{ instead of }i\text{ and
}j\right)  .
\end{align*}
Hence, $a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=-a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }$. This
proves (\ref{sol.perm.sign.pseudoexplicit.short.b.a2}).}

\item For every $\left(  i,j\right)  \in G$ satisfying $\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  $, we have%
\begin{equation}
a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }.
\label{sol.perm.sign.pseudoexplicit.short.b.a3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.short.b.a3}):}
Let $\left(  i,j\right)  \in G$ be such that $\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $\left(  i,j\right)
$ is an element of $\left[  n\right]  ^{2}$ satisfying $i<j$ (since $\left(
i,j\right)  \in G=\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\right\}  $).
\par
If we had $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, then $\left(
i,j\right)  $ would be an inversion of $\sigma$ (since $\left(  i,j\right)  $
is a pair of integers satisfying $1\leq i<j\leq n$), and thus would belong to
$\operatorname*{Inv}\left(  \sigma\right)  $; this would contradict $\left(
i,j\right)  \notin\operatorname*{Inv}\left(  \sigma\right)  $. Hence, we
cannot have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. Thus, we have
$\sigma\left(  i\right)  \leq\sigma\left(  j\right)  $. Since $\sigma\left(
i\right)  \neq\sigma\left(  j\right)  $ (because $i\neq j$ and because
$\sigma$ is injective), this shows that $\sigma\left(  i\right)
<\sigma\left(  j\right)  $.
\par
The definition of $\rho$ shows that $\rho\left(  \left(  i,j\right)  \right)
=\left\{  i,j\right\}  $. Hence, $\sigma\left(  \underbrace{\rho\left(
\left(  i,j\right)  \right)  }_{=\left\{  i,j\right\}  }\right)
=\sigma\left(  \left\{  i,j\right\}  \right)  =\left\{  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right\}  $. Since $\sigma\left(  i\right)
<\sigma\left(  j\right)  $, this shows that the elements of the set
$\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  $ listed in
increasing order are $\sigma\left(  i\right)  $ and $\sigma\left(  j\right)
$. Hence, $\min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  =\sigma\left(  i\right)  $ and $\max\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
j\right)  $.
\par
Now, the definition of $a_{\sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  }$ shows that%
\begin{align*}
a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }  &
=a_{\left(  \min\left(  \sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  \right)  ,\max\left(  \sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  \right)  \right)  }=a_{\left(  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\min\left(  \sigma\left(
\rho\left(  \left(  i,j\right)  \right)  \right)  \right)  =\sigma\left(
i\right)  \text{ and }\max\left(  \sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  \right)  =\sigma\left(  j\right)  \right)  .
\end{align*}
This proves (\ref{sol.perm.sign.pseudoexplicit.short.b.a3}).}.
\end{itemize}

Now, recall that $G=\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\right\}  $. Hence, the product sign $\prod_{1\leq i<j\leq n}$
in $\prod_{1\leq i<j\leq n}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }$ can be replaced by the product sign $\prod_{\left(
i,j\right)  \in G}$. Thus, we have%
\begin{align}
&  \underbrace{\prod_{1\leq i<j\leq n}}_{=\prod_{\left(  i,j\right)  \in G}%
}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}\nonumber\\
&  =\prod_{\left(  i,j\right)  \in G}a_{\left(  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right)  }=\left(  \prod_{\substack{\left(
i,j\right)  \in G;\\\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  }}\underbrace{a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }}_{\substack{=-a_{\sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  }\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.short.b.a2}))}}}\right)  \cdot\left(
\prod_{\substack{\left(  i,j\right)  \in G;\\\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  }}\underbrace{a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }}%
_{\substack{=a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  }\\\text{(by (\ref{sol.perm.sign.pseudoexplicit.short.b.a3}))}%
}}\right) \nonumber\\
&  =\underbrace{\left(  \prod_{\substack{\left(  i,j\right)  \in G;\\\left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  }}\left(
-a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }\right)
\right)  }_{=\left(  -1\right)  ^{\left\vert \left\{  \left(  i,j\right)  \in
G\ \mid\ \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right\}  \right\vert }\left(  \prod_{\substack{\left(  i,j\right)  \in
G;\\\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
}}a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }\right)
}\cdot\left(  \prod_{\substack{\left(  i,j\right)  \in G;\\\left(  i,j\right)
\notin\operatorname*{Inv}\left(  \sigma\right)  }}a_{\sigma\left(  \rho\left(
\left(  i,j\right)  \right)  \right)  }\right) \nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\left\vert \left\{  \left(  i,j\right)
\in G\ \mid\ \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right\}  \right\vert }}_{\substack{=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }\\\text{(since
}\operatorname*{Inv}\left(  \sigma\right)  \subseteq G\text{, and
thus}\\\left\{  \left(  i,j\right)  \in G\ \mid\ \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right\}  =\operatorname*{Inv}%
\left(  \sigma\right)  \text{)}}}\underbrace{\left(  \prod_{\substack{\left(
i,j\right)  \in G;\\\left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  }}a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)
\right)  }\right)  \cdot\left(  \prod_{\substack{\left(  i,j\right)  \in
G;\\\left(  i,j\right)  \notin\operatorname*{Inv}\left(  \sigma\right)
}}a_{\sigma\left(  \rho\left(  \left(  i,j\right)  \right)  \right)  }\right)
}_{=\prod_{\left(  i,j\right)  \in G}a_{\sigma\left(  \rho\left(  \left(
i,j\right)  \right)  \right)  }}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert }}_{=\left(  -1\right)  ^{\sigma}}\underbrace{\prod
_{\left(  i,j\right)  \in G}a_{\sigma\left(  \rho\left(  \left(  i,j\right)
\right)  \right)  }}_{\substack{=\prod_{T\in\mathcal{P}_{2}\left(  \left[
n\right]  \right)  }a_{\sigma\left(  T\right)  }\\\text{(here, we have
substituted }T\\\text{for }\rho\left(  \left(  i,j\right)  \right)  \text{ in
the product,}\\\text{since the map }\rho:G\rightarrow\mathcal{P}_{2}\left(
\left[  n\right]  \right)  \\\text{is bijective)}}}=\left(  -1\right)
^{\sigma}\cdot\prod_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)
}a_{\sigma\left(  T\right)  }. \label{sol.perm.sign.pseudoexplicit.short.b.u}%
\end{align}
On the other hand,
\begin{align}
&  \underbrace{\prod_{1\leq i<j\leq n}}_{=\prod_{\left(  i,j\right)  \in G}%
}\underbrace{a_{\left(  i,j\right)  }}_{\substack{=a_{\rho\left(  \left(
i,j\right)  \right)  }\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.short.b.a1}))}}}\nonumber\\
&  =\prod_{\left(  i,j\right)  \in G}a_{\rho\left(  \left(  i,j\right)
\right)  }=\prod_{S\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  }%
a_{S}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }S\text{ for }\rho\left(  \left(  i,j\right)
\right)  \text{ in the product,}\\
\text{since the map }\rho:G\rightarrow\mathcal{P}_{2}\left(  \left[  n\right]
\right)  \text{ is bijective}%
\end{array}
\right) \nonumber\\
&  =\prod_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)
}\underbrace{a_{\sigma_{\ast}\left(  T\right)  }}_{\substack{=a_{\sigma\left(
T\right)  }\\\text{(since }\sigma_{\ast}\left(  T\right)  =\sigma\left(
T\right)  \\\text{(by the definition of }\sigma_{\ast}\text{))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma_{\ast}\left(  T\right)  \text{ for
}S\text{ in the product,}\\
\text{since the map }\sigma_{\ast}:\mathcal{P}_{2}\left(  \left[  n\right]
\right)  \rightarrow\mathcal{P}_{2}\left(  \left[  n\right]  \right)  \text{
is bijective}%
\end{array}
\right) \nonumber\\
&  =\prod_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  }%
a_{\sigma\left(  T\right)  }. \label{sol.perm.sign.pseudoexplicit.short.b.v}%
\end{align}
Hence, (\ref{sol.perm.sign.pseudoexplicit.short.b.u}) becomes%
\[
\prod_{1\leq i<j\leq n}a_{\left(  \sigma\left(  i\right)  ,\sigma\left(
j\right)  \right)  }=\left(  -1\right)  ^{\sigma}\cdot\underbrace{\prod
_{T\in\mathcal{P}_{2}\left(  \left[  n\right]  \right)  }a_{\sigma\left(
T\right)  }}_{\substack{=\prod_{1\leq i<j\leq n}a_{\left(  i,j\right)
}\\\text{(by (\ref{sol.perm.sign.pseudoexplicit.short.b.v}))}}}=\left(
-1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}a_{\left(  i,j\right)  }.
\]
This solves Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)}.

\textbf{(a)} We have $x_{j}-x_{i}=-\left(  x_{i}-x_{j}\right)  $ for every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence, we can
apply Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)} to $a_{\left(
i,j\right)  }=x_{i}-x_{j}$. As a result, we obtain $\prod_{1\leq i<j\leq
n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(  j\right)  }\right)
=\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  $. This solves Exercise \ref{exe.perm.sign.pseudoexplicit}
\textbf{(a)}.

\textbf{(c)} Applying Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)}
to $x_{i}=i$, we obtain $\prod_{1\leq i<j\leq n}\left(  \sigma\left(
i\right)  -\sigma\left(  j\right)  \right)  =\left(  -1\right)  ^{\sigma}%
\cdot\prod_{1\leq i<j\leq n}\left(  i-j\right)  $. We can divide both sides of
this equality by $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ (because
$\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is a product of nonzero
integers, and thus nonzero). As a result, we obtain $\dfrac{\prod_{1\leq
i<j\leq n}\left(  \sigma\left(  i\right)  -\sigma\left(  j\right)  \right)
}{\prod_{1\leq i<j\leq n}\left(  i-j\right)  }=\left(  -1\right)  ^{\sigma}$.
Thus,%
\[
\left(  -1\right)  ^{\sigma}=\dfrac{\prod_{1\leq i<j\leq n}\left(
\sigma\left(  i\right)  -\sigma\left(  j\right)  \right)  }{\prod_{1\leq
i<j\leq n}\left(  i-j\right)  }=\prod_{1\leq i<j\leq n}\dfrac{\sigma\left(
i\right)  -\sigma\left(  j\right)  }{i-j}.
\]
Thus, (\ref{eq.sign.pseudoexplicit}) is proven. This solves Exercise
\ref{exe.perm.sign.pseudoexplicit} \textbf{(c)}.

\textbf{(d)} See below.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.perm.sign.pseudoexplicit}.]\textbf{(b)} Let
$\left[  n\right]  $ be the set $\left\{  1,2,\ldots,n\right\}  $. Recall that
$S_{n}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $. In other words, $S_{n}$ is the set of all permutations of the
set $\left[  n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $).

Let $G$ be the subset%
\[
\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\right\}
\]
of $\left[  n\right]  ^{2}$. Thus,%
\[
G=\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\right\}
=\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}
\]
(here, we renamed the index $\left(  i,j\right)  $ as $\left(  u,v\right)  $).
Clearly, the set $G$ is finite (since it is a subset of the finite set
$\left[  n\right]  ^{2}$).

Let $\operatorname*{Inv}\left(  \sigma\right)  $ be the set of inversions of
$\sigma$. Then, $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }$%
\ \ \ \ \footnote{\textit{Proof.} We have%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \\
&  =\left(  \text{the number of elements of }\operatorname*{Inv}\left(
\sigma\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\sigma\right)  \text{ is the set of all inversions of }\sigma\right) \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\end{align*}
\par
But the definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }=\left(
-1\right)  ^{\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert
}$ (since $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}\left(
\sigma\right)  \right\vert $), qed.}. We notice furthermore that
$\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$%
\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
\sigma\right)  $. Thus, $c$ is an inversion of $\sigma$ (since
$\operatorname*{Inv}\left(  \sigma\right)  $ is the set of inversions of
$\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. Consider this $\left(  i,j\right)  $. We have $i\in\left[
n\right]  $ (since $1\leq i\leq n$) and $j\in\left[  n\right]  $ (since $1\leq
j\leq n$). Thus, $\left(  i,j\right)  \in\left[  n\right]  ^{2}$ (since
$i\in\left[  n\right]  $ and $j\in\left[  n\right]  $). Hence, $\left(
i,j\right)  $ is an element of $\left[  n\right]  ^{2}$ and satisfies $i<j$.
In other words, $\left(  i,j\right)  $ is an element $\left(  u,v\right)  $ of
$\left[  n\right]  ^{2}$ and satisfies $u<v$. In other words, $\left(
i,j\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\right\}  =G$. Hence, $c=\left(  i,j\right)  \in G$.
\par
Now, let us forget that we fixed $c$. We thus have proven that every
$c\in\operatorname*{Inv}\left(  \sigma\right)  $ satisfies $c\in G$. In other
words, $\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$, qed.}.

We shall use the following notation (known as
\href{https://en.wikipedia.org/wiki/Iverson_bracket}{the \textit{Iverson
bracket}}): If $\mathcal{A}$ is any logical statement, then $\left[
\mathcal{A}\right]  $ will mean the integer $%
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}%
\end{cases}
$. For example, $\left[  1+1=2\right]  =1$ (since $1+1=2$ is true), whereas
$\left[  1+1=1\right]  =0$ (since $1+1=1$ is false). Clearly, if $\mathcal{A}$
and $\mathcal{B}$ are two equivalent logical statements, then $\left[
\mathcal{A}\right]  =\left[  \mathcal{B}\right]  $.

A useful property of the Iverson bracket is that it turns cardinalities of
sets into sums: Namely, if $S$ is a finite set, and if $T$ is a subset of $S$,
then%
\begin{equation}
\left\vert T\right\vert =\sum_{s\in S}\left[  s\in T\right]
\label{sol.perm.sign.pseudoexplicit.b.iverson.sum}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.iverson.sum}%
):} Let $S$ be a finite set. Let $T$ be a subset of $S$. Then,%
\begin{align*}
\sum_{s\in S}\left[  s\in T\right]   &  =\underbrace{\sum_{\substack{s\in
S;\\s\in T}}}_{\substack{=\sum_{s\in T}\\\text{(since }T\text{ is a subset of
}S\text{)}}}\underbrace{\left[  s\in T\right]  }_{\substack{=1\\\text{(since
}s\in T\text{ is true)}}}+\sum_{\substack{s\in S;\\s\notin T}%
}\underbrace{\left[  s\in T\right]  }_{\substack{=0\\\text{(since }s\in
T\text{ is false}\\\text{(since }s\notin T\text{))}}}\\
&  =\underbrace{\sum_{s\in T}1}_{=\left\vert T\right\vert \cdot1}%
+\underbrace{\sum_{\substack{s\in S;\\s\notin T}}0}_{=0}=\left\vert
T\right\vert \cdot1+0=\left\vert T\right\vert ,
\end{align*}
qed.}. Consequently, if $S$ is a finite set, and if $T$ is a subset of $S$,
then%
\begin{equation}
\left(  -1\right)  ^{\left\vert T\right\vert }=\prod_{s\in S}\left(
-1\right)  ^{\left[  s\in T\right]  }
\label{sol.perm.sign.pseudoexplicit.b.iverson.prod}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.iverson.prod}%
):} Let $S$ be a finite set. Let $T$ be a subset of $S$. Then,
(\ref{sol.perm.sign.pseudoexplicit.b.iverson.sum}) yields $\left\vert
T\right\vert =\sum_{s\in S}\left[  s\in T\right]  $. Hence,
\[
\left(  -1\right)  ^{\left\vert T\right\vert }=\left(  -1\right)  ^{\sum_{s\in
S}\left[  s\in T\right]  }=\prod_{s\in S}\left(  -1\right)  ^{\left[  s\in
T\right]  }%
\]
(because $a^{\sum_{s\in S}b_{s}}=\prod_{s\in S}a^{b_{s}}$ whenever $a$ is an
integer and $b_{s}$ is an integer for every $s\in S$). This proves
(\ref{sol.perm.sign.pseudoexplicit.b.iverson.prod}).}. We can apply this to
$S=G$ and $T=\operatorname*{Inv}\left(  \sigma\right)  $ (since
$\operatorname*{Inv}\left(  \sigma\right)  \subseteq G$). As a result, we
obtain%
\[
\left(  -1\right)  ^{\left\vert \operatorname*{Inv}\left(  \sigma\right)
\right\vert }=\prod_{s\in G}\left(  -1\right)  ^{\left[  s\in
\operatorname*{Inv}\left(  \sigma\right)  \right]  }=\prod_{\left(
i,j\right)  \in G}\left(  -1\right)  ^{\left[  \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right]  }%
\]
(here, we renamed the index $s$ as $\left(  i,j\right)  $, because all
elements of $G$ are pairs). Thus,%
\begin{equation}
\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\left\vert
\operatorname*{Inv}\left(  \sigma\right)  \right\vert }=\prod_{\left(
i,j\right)  \in G}\left(  -1\right)  ^{\left[  \left(  i,j\right)
\in\operatorname*{Inv}\left(  \sigma\right)  \right]  }.
\label{sol.perm.sign.pseudoexplicit.b.2}%
\end{equation}


On the other hand, for every $\tau\in S_{n}$, we define a map $\tau^{\left[
2\right]  }:G\rightarrow G$ by setting%
\begin{equation}
\left(  \tau^{\left[  2\right]  }\left(  i,j\right)  =\left(  \min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  ,\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)  \in G\right)  .
\label{sol.perm.sign.pseudoexplicit.b.tau}%
\end{equation}
This map $\tau^{\left[  2\right]  }$ is well-defined\footnote{\textit{Proof.}
In order to prove this, we need to show that every element of $G$ can be
written in the form $\left(  i,j\right)  $, and that every $\left(
i,j\right)  \in G$ satisfies $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  \in G$. The first of these two
claims (i.e., that every element of $G$ can be written in the form $\left(
i,j\right)  $) is obvious. It thus remains to prove the second of these two
claims. In other words, it remains to prove that every $\left(  i,j\right)
\in G$ satisfies $\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  \right)  \in G$.
\par
So let $\left(  i,j\right)  \in G$. We must prove that $\left(  \min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  ,\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)  \in G$.
\par
We have $\left(  i,j\right)  \in G=\left\{  \left(  u,v\right)  \in\left[
n\right]  ^{2}\ \mid\ u<v\right\}  $. In other words, $\left(  i,j\right)  $
is an element $\left(  u,v\right)  $ of $\left[  n\right]  ^{2}$ satisfying
$u<v$. In other words, $\left(  i,j\right)  $ is an element of $\left[
n\right]  ^{2}$ satisfying $i<j$.
\par
Now, $\tau\in S_{n}$. Hence, $\tau$ is a permutation of the set $\left[
n\right]  $ (since $S_{n}$ is the set of all permutations of the set $\left[
n\right]  $). In other words, $\tau$ is a bijective map $\left[  n\right]
\rightarrow\left[  n\right]  $. Hence, $\tau$ is both surjective and
injective. We have $i\neq j$ (since $i<j$) and thus $\tau\left(  i\right)
\neq\tau\left(  j\right)  $ (since $\tau$ is injective). Clearly, both
$\tau\left(  i\right)  $ and $\tau\left(  j\right)  $ are elements of $\left[
n\right]  $. Thus, $\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  \subseteq\left[  n\right]  $, so that $\min\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  \in\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \subseteq\left[  n\right]  $ and
$\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
\in\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
\subseteq\left[  n\right]  $. Hence,
\[
\left(  \underbrace{\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  }_{\in\left[  n\right]  },\underbrace{\max\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  }_{\in\left[  n\right]  }\right)
\in\left[  n\right]  \times\left[  n\right]  =\left[  n\right]  ^{2}.
\]
\par
Now, let us show that $\min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  <\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  $. Indeed, we assume the contrary (for the sake of contradiction).
Thus, $\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
\geq\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Now, $\tau\left(  i\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus greater or equal to the
minimum of this set. In other words, we have $\tau\left(  i\right)  \geq
\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Also, $\tau\left(  i\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus less or equal to the
maximum of this set. In other words, we have $\tau\left(  i\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Combining $\tau\left(  i\right)  \geq\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \geq\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  $ with $\tau\left(  i\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $, we
obtain $\tau\left(  i\right)  =\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  $.
\par
Also, $\tau\left(  j\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus greater or equal to the
minimum of this set. In other words, we have $\tau\left(  j\right)  \geq
\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Also, $\tau\left(  j\right)  $ is an element of the set $\left\{  \tau\left(
i\right)  ,\tau\left(  j\right)  \right\}  $, and thus less or equal to the
maximum of this set. In other words, we have $\tau\left(  j\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $.
\par
Combining $\tau\left(  j\right)  \geq\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \geq\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  $ with $\tau\left(  j\right)  \leq
\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $, we
obtain $\tau\left(  j\right)  =\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  $. Compared with $\tau\left(  i\right)  =\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $, this yields
$\tau\left(  i\right)  =\tau\left(  j\right)  $, which contradicts
$\tau\left(  i\right)  \neq\tau\left(  j\right)  $. This contradiction proves
that our assumption was wrong.
\par
Hence, $\min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
<\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  $ is
proven. Now, we know that $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  $ is an element of $\left[
n\right]  ^{2}$ satisfying $\min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  <\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  $. In other words, $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  $ is an element $\left(  u,v\right)
\in\left[  n\right]  ^{2}$ satisfying $u<v$. In other words,%
\[
\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)
\in\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}
=G.
\]
Thus, we have proven that $\left(  \min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  \right)  \in G$. Qed.}. Moreover, we have%
\begin{equation}
\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[  2\right]
}=\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \text{for every }\tau\in S_{n}
\label{sol.perm.sign.pseudoexplicit.b.tautau}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.tautau}):} Let
$\tau\in S_{n}$. Let $c\in G$. We are going to prove that $\left(  \left(
\tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[  2\right]  }\right)
\left(  c\right)  =c$.
\par
We have $c\in G=\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}%
\ \mid\ i<j\right\}  $. In other words, $c$ can be written in the form
$c=\left(  i,j\right)  $ for some $\left(  i,j\right)  \in\left[  n\right]
^{2}$ satisfying $i<j$. Consider this $\left(  i,j\right)  $.
\par
The definition of $\tau^{\left[  2\right]  }$ yields $\tau^{\left[  2\right]
}\left(  i,j\right)  =\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  ,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)
\right\}  \right)  $. Thus, $\tau^{\left[  2\right]  }\left(  \underbrace{c}%
_{=\left(  i,j\right)  }\right)  =\tau^{\left[  2\right]  }\left(  i,j\right)
=\left(  \min\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}
,\max\left\{  \tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  \right)
$.
\par
Now, $\tau\in S_{n}$. Hence, $\tau$ is a permutation of the set $\left[
n\right]  $ (since $S_{n}$ is the set of all permutations of the set $\left[
n\right]  $). In other words, $\tau$ is a bijective map $\left[  n\right]
\rightarrow\left[  n\right]  $. Hence, $\tau$ is both surjective and
injective. We have $i\neq j$ (since $i<j$) and thus $\tau\left(  i\right)
\neq\tau\left(  j\right)  $ (since $\tau$ is injective). Hence, we have either
$\tau\left(  i\right)  <\tau\left(  j\right)  $ or $\tau\left(  i\right)
>\tau\left(  j\right)  $. In other words, we are in one of the following two
cases:
\par
\textit{Case 1:} We have $\tau\left(  i\right)  <\tau\left(  j\right)  $.
\par
\textit{Case 2:} We have $\tau\left(  i\right)  >\tau\left(  j\right)  $.
\par
Let us first consider Case 1. In this case, we have $\tau\left(  i\right)
<\tau\left(  j\right)  $. Thus, $\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  =\tau\left(  i\right)  $ and $\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  =\tau\left(  j\right)
$. Hence,%
\[
\tau^{\left[  2\right]  }\left(  c\right)  =\left(  \underbrace{\min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  }_{=\tau\left(
i\right)  },\underbrace{\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  }_{=\tau\left(  j\right)  }\right)  =\left(  \tau\left(
i\right)  ,\tau\left(  j\right)  \right)  .
\]
Now,%
\begin{align*}
\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[
2\right]  }\right)  \left(  c\right)   &  =\left(  \tau^{-1}\right)  ^{\left[
2\right]  }\left(  \underbrace{\tau^{\left[  2\right]  }\left(  c\right)
}_{=\left(  \tau\left(  i\right)  ,\tau\left(  j\right)  \right)  }\right)
=\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\left(  \tau\left(  i\right)
,\tau\left(  j\right)  \right) \\
&  =\left(  \min\left\{  \underbrace{\tau^{-1}\left(  \tau\left(  i\right)
\right)  }_{=i},\underbrace{\tau^{-1}\left(  \tau\left(  j\right)  \right)
}_{=j}\right\}  ,\max\left\{  \underbrace{\tau^{-1}\left(  \tau\left(
i\right)  \right)  }_{=i},\underbrace{\tau^{-1}\left(  \tau\left(  j\right)
\right)  }_{=j}\right\}  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  \tau
^{-1}\right)  ^{\left[  2\right]  }\right) \\
&  =\left(  \underbrace{\min\left\{  i,j\right\}  }%
_{\substack{=i\\\text{(since }i<j\text{)}}},\underbrace{\max\left\{
i,j\right\}  }_{\substack{=j\\\text{(since }i<j\text{)}}}\right)  =\left(
i,j\right)  =c.
\end{align*}
Hence, $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $\tau\left(  i\right)
>\tau\left(  j\right)  $. Thus, $\min\left\{  \tau\left(  i\right)
,\tau\left(  j\right)  \right\}  =\tau\left(  j\right)  $ and $\max\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  =\tau\left(  i\right)
$. Hence,%
\[
\tau^{\left[  2\right]  }\left(  c\right)  =\left(  \underbrace{\min\left\{
\tau\left(  i\right)  ,\tau\left(  j\right)  \right\}  }_{=\tau\left(
j\right)  },\underbrace{\max\left\{  \tau\left(  i\right)  ,\tau\left(
j\right)  \right\}  }_{=\tau\left(  i\right)  }\right)  =\left(  \tau\left(
j\right)  ,\tau\left(  i\right)  \right)  .
\]
Now,%
\begin{align*}
\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[
2\right]  }\right)  \left(  c\right)   &  =\left(  \tau^{-1}\right)  ^{\left[
2\right]  }\left(  \underbrace{\tau^{\left[  2\right]  }\left(  c\right)
}_{=\left(  \tau\left(  j\right)  ,\tau\left(  i\right)  \right)  }\right)
=\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\left(  \tau\left(  j\right)
,\tau\left(  i\right)  \right) \\
&  =\left(  \min\left\{  \underbrace{\tau^{-1}\left(  \tau\left(  j\right)
\right)  }_{=j},\underbrace{\tau^{-1}\left(  \tau\left(  i\right)  \right)
}_{=i}\right\}  ,\max\left\{  \underbrace{\tau^{-1}\left(  \tau\left(
j\right)  \right)  }_{=j},\underbrace{\tau^{-1}\left(  \tau\left(  i\right)
\right)  }_{=i}\right\}  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  \tau
^{-1}\right)  ^{\left[  2\right]  }\right) \\
&  =\left(  \underbrace{\min\left\{  j,i\right\}  }%
_{\substack{=i\\\text{(since }i<j\text{)}}},\underbrace{\max\left\{
j,i\right\}  }_{\substack{=j\\\text{(since }i<j\text{)}}}\right)  =\left(
i,j\right)  =c.
\end{align*}
Hence, $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ is proven in Case 2.
\par
We have now proven $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]
}\circ\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ in each of the
two Cases 1 and 2. Thus, $\left(  \left(  \tau^{-1}\right)  ^{\left[
2\right]  }\circ\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c$ always
holds.
\par
So we have $\left(  \left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }\right)  \left(  c\right)  =c=\operatorname*{id}%
\left(  c\right)  $.
\par
Let us now forget that we fixed $c$. We thus have proven that $\left(  \left(
\tau^{-1}\right)  ^{\left[  2\right]  }\circ\tau^{\left[  2\right]  }\right)
\left(  c\right)  =\operatorname*{id}\left(  c\right)  $ for every $c\in G$.
In other words, $\left(  \tau^{-1}\right)  ^{\left[  2\right]  }\circ
\tau^{\left[  2\right]  }=\operatorname*{id}$. This proves
(\ref{sol.perm.sign.pseudoexplicit.b.tautau}).}. Thus, for every $\tau\in
S_{n}$, the map $\tau^{\left[  2\right]  }$ is a
bijection\footnote{\textit{Proof.} Let $\tau\in S_{n}$. Applying
(\ref{sol.perm.sign.pseudoexplicit.b.tautau}) to $\tau^{-1}$ instead of $\tau
$, we obtain $\left(  \left(  \tau^{-1}\right)  ^{-1}\right)  ^{\left[
2\right]  }\circ\left(  \tau^{-1}\right)  ^{\left[  2\right]  }%
=\operatorname*{id}$. Since $\left(  \tau^{-1}\right)  ^{-1}=\tau$, this
rewrites as $\tau^{\left[  2\right]  }\circ\left(  \tau^{-1}\right)  ^{\left[
2\right]  }=\operatorname*{id}$. Combined with $\left(  \tau^{-1}\right)
^{\left[  2\right]  }\circ\tau^{\left[  2\right]  }=\operatorname*{id}$ (which
follows from (\ref{sol.perm.sign.pseudoexplicit.b.tautau})), this yields that
the maps $\tau^{\left[  2\right]  }$ and $\left(  \tau^{-1}\right)  ^{\left[
2\right]  }$ are mutually inverse. Hence, the map $\tau^{\left[  2\right]  }$
is invertible, i.e., is a bijection. Qed.}. Applying this to $\tau=\sigma$, we
see that the map $\sigma^{\left[  2\right]  }$ is a bijection.

Now, every $\left(  i,j\right)  \in G$ satisfies%
\begin{equation}
\left(  -1\right)  ^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  \right]  }a_{\sigma^{\left[  2\right]  }\left(  i,j\right)
}=a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }
\label{sol.perm.sign.pseudoexplicit.b.a-inv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}):} Let
$\left(  i,j\right)  \in G$. Thus, $\left(  i,j\right)  \in G=\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\right\}  $. In other words,
$\left(  i,j\right)  $ is an element $\left(  u,v\right)  $ of $\left[
n\right]  ^{2}$ satisfying $u<v$. In other words, $\left(  i,j\right)  $ is an
element of $\left[  n\right]  ^{2}$ satisfying $i<j$. From $\left(
i,j\right)  \in\left[  n\right]  ^{2}$, we obtain $i\in\left[  n\right]  $ and
$j\in\left[  n\right]  $. Thus, $1\leq i$ (since $i\in\left[  n\right]  $) and
$j\leq n$ (since $j\in\left[  n\right]  $), so that $1\leq i<j\leq n$.
\par
We must be in one of the following two cases:
\par
\textit{Case 1:} We have $\sigma\left(  i\right)  \leq\sigma\left(  j\right)
$.
\par
\textit{Case 2:} We have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $.
\par
Let us first consider Case 1. In this case, we have $\sigma\left(  i\right)
\leq\sigma\left(  j\right)  $. Thus, $\min\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  =\sigma\left(  i\right)  $ and
$\max\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}
=\sigma\left(  j\right)  $. Now, the definition of $\sigma^{\left[  2\right]
}$ yields%
\[
\sigma^{\left[  2\right]  }\left(  i,j\right)  =\left(  \underbrace{\min
\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}  }%
_{=\sigma\left(  i\right)  },\underbrace{\max\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  }_{=\sigma\left(  j\right)  }\right)
=\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  .
\]
Hence, $a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }=a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }$.
\par
On the other hand, let us show that $\left(  i,j\right)  \notin%
\operatorname*{Inv}\left(  \sigma\right)  $. Indeed, we assume the contrary
(for the sake of contradiction). Thus, $\left(  i,j\right)  \in
\operatorname*{Inv}\left(  \sigma\right)  $. In other words, $\left(
i,j\right)  $ is an inversion of $\sigma$ (since $\operatorname*{Inv}\left(
\sigma\right)  $ is the set of inversions of $\sigma$). In other words,
$\left(  i,j\right)  $ is a pair of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $ (because of how an
\textquotedblleft inversion\textquotedblright\ is defined). But $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ contradicts $\sigma\left(  i\right)
\leq\sigma\left(  j\right)  $. Thus, we have obtained a contradiction.
Therefore, our assumption must have been false. This proves that $\left(
i,j\right)  \notin\operatorname*{Inv}\left(  \sigma\right)  $.
\par
Hence, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $ is
false. Thus, $\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(
\sigma\right)  \right]  =0$, so that $\left(  -1\right)  ^{\left[  \left(
i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]  }=\left(
-1\right)  ^{0}=1$ and therefore $\underbrace{\left(  -1\right)  ^{\left[
\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]  }%
}_{=1}\underbrace{a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }%
}_{=a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }%
}=1a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)
}=a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }$.
Thus, (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. Thus, $\min\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  =\sigma\left(  j\right)  $ and
$\max\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}
=\sigma\left(  i\right)  $. Now, the definition of $\sigma^{\left[  2\right]
}$ yields%
\[
\sigma^{\left[  2\right]  }\left(  i,j\right)  =\left(  \underbrace{\min
\left\{  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right\}  }%
_{=\sigma\left(  j\right)  },\underbrace{\max\left\{  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right\}  }_{=\sigma\left(  i\right)  }\right)
=\left(  \sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  .
\]
Hence, $a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }=a_{\left(
\sigma\left(  j\right)  ,\sigma\left(  i\right)  \right)  }=-a_{\left(
\sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }$ (by
(\ref{eq.exe.perm.sign.pseudoexplicit.b.skew}), applied to $\sigma\left(
i\right)  $ and $\sigma\left(  j\right)  $ instead of $i$ and $j$).
\par
On the other hand, $\left(  i,j\right)  $ is a pair of integers satisfying
$1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. In
other words, $\left(  i,j\right)  $ is an inversion of $\sigma$ (because of
how an \textquotedblleft inversion\textquotedblright\ is defined). In other
words, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $
(since $\operatorname*{Inv}\left(  \sigma\right)  $ is the set of inversions
of $\sigma$). Hence, $\left[  \left(  i,j\right)  \in\operatorname*{Inv}%
\left(  \sigma\right)  \right]  =1$, so that $\left(  -1\right)  ^{\left[
\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]
}=\left(  -1\right)  ^{1}=-1$ and therefore $\underbrace{\left(  -1\right)
^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right]  }}_{=-1}\underbrace{a_{\sigma^{\left[  2\right]  }\left(  i,j\right)
}}_{=-a_{\left(  \sigma\left(  i\right)  ,\sigma\left(  j\right)  \right)  }%
}=\left(  -1\right)  \left(  -a_{\left(  \sigma\left(  i\right)
,\sigma\left(  j\right)  \right)  }\right)  =a_{\left(  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right)  }$. Thus,
(\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) is proven in Case 2.
\par
We have now proven (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) in each of the
two Cases 1 and 2. Thus, (\ref{sol.perm.sign.pseudoexplicit.b.a-inv}) always
holds, qed.}.

Now,%
\begin{align*}
&  \underbrace{\prod_{1\leq i<j\leq n}}_{\substack{=\prod_{\substack{\left(
i,j\right)  \in\left[  n\right]  ^{2};\\i<j}}=\prod_{\left(  i,j\right)  \in
G}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\left[  n\right]
^{2}\ \mid\ i<j\right\}  \text{)}}}\underbrace{a_{\left(  \sigma\left(
i\right)  ,\sigma\left(  j\right)  \right)  }}_{\substack{=\left(  -1\right)
^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right]  }a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.b.a-inv}))}}}\\
&  =\prod_{\left(  i,j\right)  \in G}\left(  \left(  -1\right)  ^{\left[
\left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)  \right]
}a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }\right)
=\underbrace{\left(  \prod_{\left(  i,j\right)  \in G}\left(  -1\right)
^{\left[  \left(  i,j\right)  \in\operatorname*{Inv}\left(  \sigma\right)
\right]  }\right)  }_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{sol.perm.sign.pseudoexplicit.b.2}))}}}\left(  \prod_{\left(  i,j\right)
\in G}a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }\right) \\
&  =\left(  -1\right)  ^{\sigma}\prod_{\left(  i,j\right)  \in G}%
a_{\sigma^{\left[  2\right]  }\left(  i,j\right)  }=\left(  -1\right)
^{\sigma}\prod_{c\in G}a_{\sigma^{\left[  2\right]  }\left(  c\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
i,j\right)  \text{ as }c\text{ in the product}\right) \\
&  =\left(  -1\right)  ^{\sigma}\prod_{c\in G}a_{c}\ \ \ \ \ \ \ \ \ \ \left(
%
\begin{array}
[c]{c}%
\text{here, we have substituted }c\text{ for }\sigma^{\left[  2\right]
}\left(  c\right)  \text{ in the product,}\\
\text{since the map }\sigma^{\left[  2\right]  }:G\rightarrow G\text{ is a
bijection}%
\end{array}
\right) \\
&  =\left(  -1\right)  ^{\sigma}\underbrace{\prod_{\left(  i,j\right)  \in G}%
}_{\substack{=\prod_{\substack{\left(  i,j\right)  \in\left[  n\right]
^{2};\\i<j}}\\\text{(since }G=\left\{  \left(  i,j\right)  \in\left[
n\right]  ^{2}\ \mid\ i<j\right\}  \text{)}}}a_{\left(  i,j\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the index }c\text{ as }\left(  i,j\right)  \text{
in the product,}\\
\text{since every element of }G\text{ has the form }\left(  i,j\right)
\end{array}
\right) \\
&  =\left(  -1\right)  ^{\sigma}\underbrace{\prod_{\substack{\left(
i,j\right)  \in\left[  n\right]  ^{2};\\i<j}}}_{=\prod_{1\leq i<j\leq n}%
}a_{\left(  i,j\right)  }=\left(  -1\right)  ^{\sigma}\prod_{1\leq i<j\leq
n}a_{\left(  i,j\right)  }.
\end{align*}
This solves Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)}.

\textbf{(a)} We have $x_{j}-x_{i}=-\left(  x_{i}-x_{j}\right)  $ for every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence, we can
apply Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(b)} to $a_{\left(
i,j\right)  }=x_{i}-x_{j}$. As a result, we obtain $\prod_{1\leq i<j\leq
n}\left(  x_{\sigma\left(  i\right)  }-x_{\sigma\left(  j\right)  }\right)
=\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  $. This solves Exercise \ref{exe.perm.sign.pseudoexplicit}
\textbf{(a)}.

\textbf{(c)} Applying Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)}
to $x_{i}=i$, we obtain $\prod_{1\leq i<j\leq n}\left(  \sigma\left(
i\right)  -\sigma\left(  j\right)  \right)  =\left(  -1\right)  ^{\sigma}%
\cdot\prod_{1\leq i<j\leq n}\left(  i-j\right)  $. We can divide both sides of
this equality by $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ (because
$\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is a product of nonzero
integers, and thus nonzero). As a result, we obtain $\dfrac{\prod_{1\leq
i<j\leq n}\left(  \sigma\left(  i\right)  -\sigma\left(  j\right)  \right)
}{\prod_{1\leq i<j\leq n}\left(  i-j\right)  }=\left(  -1\right)  ^{\sigma}$.
Thus,%
\[
\left(  -1\right)  ^{\sigma}=\dfrac{\prod_{1\leq i<j\leq n}\left(
\sigma\left(  i\right)  -\sigma\left(  j\right)  \right)  }{\prod_{1\leq
i<j\leq n}\left(  i-j\right)  }=\prod_{1\leq i<j\leq n}\dfrac{\sigma\left(
i\right)  -\sigma\left(  j\right)  }{i-j}.
\]
Thus, (\ref{eq.sign.pseudoexplicit}) is proven. This solves Exercise
\ref{exe.perm.sign.pseudoexplicit} \textbf{(c)}.

\textbf{(d)} See below.
\end{proof}
\end{verlong}

Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(d)} asks us to give an
alternative solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}. Let us do
this now:

\begin{proof}
[Alternative solution to Exercise \ref{exe.ps2.2.5} \textbf{(b)}.]Let
$n\in\mathbb{N}$. Let $\sigma$ and $\tau$ be two permutations in $S_{n}$. We
need to show that $\ell\left(  \sigma\circ\tau\right)  \equiv\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  \operatorname{mod}2$.

We are going to prove that $\left(  -1\right)  ^{\sigma\circ\tau}=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ first.

Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} (applied to $x_{i}%
=i$) yields%
\[
\prod_{1\leq i<j\leq n}\left(  \sigma\left(  i\right)  -\sigma\left(
j\right)  \right)  =\left(  -1\right)  ^{\sigma}\cdot\prod_{1\leq i<j\leq
n}\left(  i-j\right)  .
\]
Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} (applied to
$\sigma\left(  i\right)  $ and $\tau$ instead of $x_{i}$ and $\sigma$) yields%
\begin{align}
\prod_{1\leq i<j\leq n}\left(  \sigma\left(  \tau\left(  i\right)  \right)
-\sigma\left(  \tau\left(  j\right)  \right)  \right)   &  =\left(  -1\right)
^{\tau}\cdot\underbrace{\prod_{1\leq i<j\leq n}\left(  \sigma\left(  i\right)
-\sigma\left(  j\right)  \right)  }_{=\left(  -1\right)  ^{\sigma}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right)  }\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\tau}\cdot\left(  -1\right)  ^{\sigma}%
}_{=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right) \nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right)  .
\label{sol.perm.sign.pseudoexplicit.d.1}%
\end{align}
Exercise \ref{exe.perm.sign.pseudoexplicit} \textbf{(a)} (applied to
$\sigma\circ\tau$ instead of $\sigma$) yields%
\[
\prod_{1\leq i<j\leq n}\left(  \left(  \sigma\circ\tau\right)  \left(
i\right)  -\left(  \sigma\circ\tau\right)  \left(  j\right)  \right)  =\left(
-1\right)  ^{\sigma\circ\tau}\cdot\prod_{1\leq i<j\leq n}\left(  i-j\right)
.
\]
Thus,%
\begin{align}
\left(  -1\right)  ^{\sigma\circ\tau}\cdot\prod_{1\leq i<j\leq n}\left(
i-j\right)   &  =\prod_{1\leq i<j\leq n}\left(  \underbrace{\left(
\sigma\circ\tau\right)  \left(  i\right)  }_{=\sigma\left(  \tau\left(
i\right)  \right)  }-\underbrace{\left(  \sigma\circ\tau\right)  \left(
j\right)  }_{=\sigma\left(  \tau\left(  j\right)  \right)  }\right)
\nonumber\\
&  =\prod_{1\leq i<j\leq n}\left(  \sigma\left(  \tau\left(  i\right)
\right)  -\sigma\left(  \tau\left(  j\right)  \right)  \right) \nonumber\\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}\cdot
\prod_{1\leq i<j\leq n}\left(  i-j\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.perm.sign.pseudoexplicit.d.1})}\right)  .
\label{sol.perm.sign.pseudoexplicit.d.2}%
\end{align}


\begin{vershort}
But the integer $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is nonzero
(since it is a product of the nonzero integers $i-j$). Hence, we can divide
both sides of the equality (\ref{sol.perm.sign.pseudoexplicit.d.2}) by
$\prod_{1\leq i<j\leq n}\left(  i-j\right)  $. We thus obtain $\left(
-1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(
-1\right)  ^{\tau}$.
\end{vershort}

\begin{verlong}
But the integer $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is
nonzero\footnote{\textit{Proof.} If $i$ and $j$ are two integers satisfying
$1\leq i<j\leq n$, then the integer $i-j$ is negative (since $i<j$) and thus
nonzero. Hence, the product $\prod_{1\leq i<j\leq n}\left(  i-j\right)  $ is a
product of nonzero integers, and therefore itself nonzero (since a product of
nonzero integers is always nonzero). Qed.}. Hence, we can divide both sides of
the equality (\ref{sol.perm.sign.pseudoexplicit.d.2}) by $\prod_{1\leq i<j\leq
n}\left(  i-j\right)  $. We thus obtain $\left(  -1\right)  ^{\sigma\circ\tau
}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau}$.
\end{verlong}

Now, the definition of $\left(  -1\right)  ^{\sigma}$ yields $\left(
-1\right)  ^{\sigma}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  }$. Also,
the definition of $\left(  -1\right)  ^{\tau}$ yields $\left(  -1\right)
^{\tau}=\left(  -1\right)  ^{\ell\left(  \tau\right)  }$. Hence,
\[
\underbrace{\left(  -1\right)  ^{\sigma}}_{=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }}\cdot\underbrace{\left(  -1\right)  ^{\tau}}_{=\left(
-1\right)  ^{\ell\left(  \tau\right)  }}=\left(  -1\right)  ^{\ell\left(
\sigma\right)  }\cdot\left(  -1\right)  ^{\ell\left(  \tau\right)  }=\left(
-1\right)  ^{\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  }.
\]


Finally, the definition of $\left(  -1\right)  ^{\sigma\circ\tau}$ yields
$\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)  ^{\ell\left(
\sigma\circ\tau\right)  }$. Thus,%
\[
\left(  -1\right)  ^{\ell\left(  \sigma\circ\tau\right)  }=\left(  -1\right)
^{\sigma\circ\tau}=\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\tau
}=\left(  -1\right)  ^{\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
}.
\]
But it is obvious that if two integers $u$ and $v$ satisfy $\left(  -1\right)
^{u}=\left(  -1\right)  ^{v}$, then $u\equiv v\operatorname{mod}2$. Applying
this to $u=\ell\left(  \sigma\circ\tau\right)  $ and $v=\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $, we obtain $\ell\left(  \sigma
\circ\tau\right)  \equiv\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
\operatorname{mod}2$. Thus, Exercise \ref{exe.ps2.2.5} \textbf{(b)} is solved again.
\end{proof}

\subsection{Solution to Exercise \ref{exe.Ialbe}}

Exercise \ref{exe.Ialbe} is an example of a combinatorial fact that one can
easily convince oneself of (with some handwaving), but that is quite hard to
prove in a formal, bulletproof way. Thus, the solution given below is going to
be long, but we hope that the reader can avoid major parts of it by figuring
them out independently.

Before we start solving Exercise \ref{exe.Ialbe}, let us define some notations.

\begin{definition}
\label{def.sol.exe.Ialbe.12n}For every $n\in\mathbb{N}$, we let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.
\end{definition}

\begin{definition}
For every $n\in\mathbb{N}$ and every $n$-tuple $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ of integers, we define the following notations:

\begin{itemize}
\item An \textit{inversion} of $\mathbf{a}$ will mean a pair $\left(
i,j\right)  \in\left[  n\right]  ^{2}$ satisfying $i<j$ and $a_{i}>a_{j}$.

\item We denote by $\operatorname*{Inv}\left(  \mathbf{a}\right)  $ the set of
all inversions of $\mathbf{a}$. Thus, $\operatorname*{Inv}\left(
\mathbf{a}\right)  \subseteq\left[  n\right]  ^{2}$. More precisely,%
\begin{align}
\operatorname*{Inv}\left(  \mathbf{a}\right)   &  =\left(  \text{the set of
all inversions of }\mathbf{a}\right) \nonumber\\
&  =\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\text{
and }a_{i}>a_{j}\right\} \label{sol.exe.Ialbe.Inv.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the inversions of }\mathbf{a}\text{ are the pairs }\left(
i,j\right)  \in\left[  n\right]  ^{2}\\
\text{satisfying }i<j\text{ and }a_{i}>a_{j}\text{ (by the definition}\\
\text{of an \textquotedblleft inversion\textquotedblright)}%
\end{array}
\right) \nonumber\\
&  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{
and }a_{u}>a_{v}\right\} \label{sol.exe.Ialbe.Inv.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
i,j\right)  \text{ as }\left(  u,v\right)  \right)  .\nonumber
\end{align}


\item We denote by $\ell\left(  \mathbf{a}\right)  $ the number $\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert $. (This is
well-defined because $\operatorname*{Inv}\left(  \mathbf{a}\right)  $ is
finite (since $\operatorname*{Inv}\left(  \mathbf{a}\right)  \subseteq\left[
n\right]  ^{2}$).) Thus,%
\begin{align}
\ell\left(  \mathbf{a}\right)   &  =\left\vert \underbrace{\operatorname*{Inv}%
\left(  \mathbf{a}\right)  }_{=\left(  \text{the set of all inversions of
}\mathbf{a}\right)  }\right\vert =\left\vert \left(  \text{the set of all
inversions of }\mathbf{a}\right)  \right\vert \nonumber\\
&  =\left(  \text{the number of all inversions of }\mathbf{a}\right)  .
\label{sol.exe.Ialbe.l(a)}%
\end{align}

\end{itemize}
\end{definition}

Let us now notice something almost trivial:

\begin{lemma}
\label{lem.sol.exe.Ialbe.Inv=Inv}Let $P$ be a finite set of integers. Let
$m=\left\vert P\right\vert $. Let $\sigma\in S_{m}$. Let $\left(  p_{1}%
,p_{2},\ldots,p_{m}\right)  $ be the list of all elements of $P$ in increasing
order (with no repetitions). Let $\operatorname*{Inv}\left(  \sigma\right)  $
denote the set of all inversions of $\sigma$.

\textbf{(a)} We have $\operatorname*{Inv}\left(  \sigma\right)
=\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  $.

\textbf{(b)} We have $\ell\left(  \sigma\right)  =\ell\left(  p_{\sigma\left(
1\right)  },p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)
}\right)  $.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv}.]The inversions of $\sigma$
are the pairs $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq m$
and $\sigma\left(  i\right)  >\sigma\left(  j\right)  $. Thus,
$\operatorname*{Inv}\left(  \sigma\right)  $ (which is the set of all
inversions of $\sigma$) is the set of all such pairs $\left(  i,j\right)  $.
In other words,
\begin{align}
\operatorname*{Inv}\left(  \sigma\right)   &  =\left\{  \left(  i,j\right)
\in\mathbb{Z}^{2}\ \mid\ 1\leq i<j\leq m\text{ and }\sigma\left(  i\right)
>\sigma\left(  j\right)  \right\} \nonumber\\
&  =\left\{  \left(  i,j\right)  \in\left[  m\right]  ^{2}\ \mid\ i<j\text{
and }\sigma\left(  i\right)  >\sigma\left(  j\right)  \right\} \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because the pairs }\left(  i,j\right)  \in\mathbb{Z}^{2}\text{
satisfying }1\leq i<j\leq m\\
\text{are precisely the pairs }\left(  i,j\right)  \in\left[  m\right]
^{2}\text{ satisfying }i<j
\end{array}
\right) \nonumber\\
&  =\left\{  \left(  u,v\right)  \in\left[  m\right]  ^{2}\ \mid\ u<v\text{
and }\sigma\left(  u\right)  >\sigma\left(  v\right)  \right\}
\label{pf.lem.sol.exe.Ialbe.Inv=Inv.short.lhs=}%
\end{align}
(here, we have renamed the index $\left(  i,j\right)  $ as $\left(
u,v\right)  $).

But $\left(  p_{1},p_{2},\ldots,p_{m}\right)  $ is the list of all elements of
$P$ in increasing order (with no repetitions). Thus, $\left(  p_{1}%
,p_{2},\ldots,p_{m}\right)  $ is a strictly increasing list. In other words,
$p_{1}<p_{2}<\cdots<p_{m}$. Hence, if $i$ and $j$ are two elements of $\left[
m\right]  $, then we have the following logical equivalence:%
\begin{equation}
\left(  p_{i}>p_{j}\right)  \Longleftrightarrow\left(  i>j\right)  .
\label{pf.lem.sol.exe.Ialbe.Inv=Inv.short.1}%
\end{equation}


But (\ref{sol.exe.Ialbe.Inv.2}) (applied to $m$, $\left(  p_{\sigma\left(
1\right)  },p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)
}\right)  $ and $p_{\sigma\left(  i\right)  }$ instead of $n$, $\mathbf{a}$
and $a_{i}$) yields%
\begin{align*}
&  \operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right) \\
&  =\left\{  \left(  u,v\right)  \in\left[  m\right]  ^{2}\ \mid\ u<v\text{
and }\underbrace{p_{\sigma\left(  u\right)  }>p_{\sigma\left(  v\right)  }%
}_{\substack{\text{this is equivalent to }\sigma\left(  u\right)
>\sigma\left(  v\right)  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.short.1}), applied to }i=\sigma\left(
u\right)  \text{ and }j=\sigma\left(  v\right)  \text{)}}}\right\} \\
&  =\left\{  \left(  u,v\right)  \in\left[  m\right]  ^{2}\ \mid\ u<v\text{
and }\sigma\left(  u\right)  >\sigma\left(  v\right)  \right\}
=\operatorname*{Inv}\left(  \sigma\right)
\end{align*}
(by (\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.short.lhs=})). This proves Lemma
\ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(a)}.

\textbf{(b)} We know that $\ell\left(  \sigma\right)  $ is the number of all
inversions of $\sigma$ (by the definition of $\ell\left(  \sigma\right)  $).
In other words, $\ell\left(  \sigma\right)  $ is the size of the set of all
inversions of $\sigma$. In other words, $\ell\left(  \sigma\right)  $ is the
size of $\operatorname*{Inv}\left(  \sigma\right)  $ (since the set of all
inversions of $\sigma$ is $\operatorname*{Inv}\left(  \sigma\right)  $).
Hence,%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left\vert \underbrace{\operatorname*{Inv}%
\left(  \sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  p_{\sigma
\left(  1\right)  },p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(
m\right)  }\right)  \\\text{(by Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv}
\textbf{(a)})}}}\right\vert =\left\vert \operatorname*{Inv}\left(
p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)  },\ldots
,p_{\sigma\left(  m\right)  }\right)  \right\vert \\
&  =\ell\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)
},\ldots,p_{\sigma\left(  m\right)  }\right)
\end{align*}
(since $\ell\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)
},\ldots,p_{\sigma\left(  m\right)  }\right)  $ is defined as $\left\vert
\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  \right\vert $). This
proves Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv}.]We know that $\left(
p_{1},p_{2},\ldots,p_{m}\right)  $ is the list of all elements of $P$ in
increasing order (with no repetitions). Thus, $\left(  p_{1},p_{2}%
,\ldots,p_{m}\right)  $ is a strictly increasing list. In other words, we have
$p_{1}<p_{2}<\cdots<p_{m}$. In other words, if $u$ and $v$ are two elements of
$\left[  m\right]  $ such that $u<v$, then%
\begin{equation}
p_{u}<p_{v}. \label{pf.lem.sol.exe.Ialbe.Inv=Inv.pupv}%
\end{equation}


\textbf{(a)} We have%
\begin{equation}
\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  =\left\{  \left(
u,v\right)  \in\left[  m\right]  ^{2}\ \mid\ u<v\text{ and }p_{\sigma\left(
u\right)  }>p_{\sigma\left(  v\right)  }\right\}
\label{pf.lem.sol.exe.Ialbe.Inv=Inv.a.Inv}%
\end{equation}
(by (\ref{sol.exe.Ialbe.Inv.2}), applied to $m$, $\left(  p_{\sigma\left(
1\right)  },p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)
}\right)  $ and $p_{\sigma\left(  i\right)  }$ instead of $n$, $\mathbf{a}$
and $a_{i}$).

Let $c\in\operatorname*{Inv}\left(  \sigma\right)  $. Thus, $c$ is an element
of $\operatorname*{Inv}\left(  \sigma\right)  $. In other words, $c$ is an
inversion of $\sigma$ (since $\operatorname*{Inv}\left(  \sigma\right)  $ is
the set of all inversions of $\sigma$). In other words, $c$ is a pair $\left(
i,j\right)  $ of integers satisfying $1\leq i<j\leq m$ and $\sigma\left(
i\right)  >\sigma\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\sigma$\textquotedblright).

From $i<j\leq m$, we obtain $i\leq m$. From $1\leq i\leq m$, we obtain
$i\in\left[  m\right]  $.

From $1\leq i<j$, we obtain $1\leq j$. From $1\leq j\leq m$, we obtain
$j\in\left[  m\right]  $.

From $i\in\left[  m\right]  $ and $j\in\left[  m\right]  $, we obtain $\left(
i,j\right)  \in\left[  m\right]  \times\left[  m\right]  =\left[  m\right]
^{2}$. From $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, we obtain
$\sigma\left(  j\right)  <\sigma\left(  i\right)  $. Hence, $p_{\sigma\left(
j\right)  }<p_{\sigma\left(  i\right)  }$ (by
(\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.pupv}), applied to $u=\sigma\left(
j\right)  $ and $v=\sigma\left(  i\right)  $). In other words, $p_{\sigma
\left(  i\right)  }>p_{\sigma\left(  j\right)  }$. Hence, $\left(  i,j\right)
$ is an element of $\left[  m\right]  ^{2}$ and satisfies $i<j$ and
$p_{\sigma\left(  i\right)  }>p_{\sigma\left(  j\right)  }$. In other words,
$\left(  i,j\right)  $ is an element $\left(  u,v\right)  \in\left[  m\right]
^{2}$ satisfying $u<v$ and $p_{\sigma\left(  u\right)  }>p_{\sigma\left(
v\right)  }$. Thus,%
\begin{align*}
\left(  i,j\right)   &  \in\left\{  \left(  u,v\right)  \in\left[  m\right]
^{2}\ \mid\ u<v\text{ and }p_{\sigma\left(  u\right)  }>p_{\sigma\left(
v\right)  }\right\} \\
&  =\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.a.Inv}%
)}\right)  .
\end{align*}
Hence, $c=\left(  i,j\right)  \in\operatorname*{Inv}\left(  p_{\sigma\left(
1\right)  },p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)
}\right)  $.

Now, forget that we fixed $c$. We thus have shown that every $c\in
\operatorname*{Inv}\left(  \sigma\right)  $ satisfies $c\in\operatorname*{Inv}%
\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)  }%
,\ldots,p_{\sigma\left(  m\right)  }\right)  $. In other words,
\begin{equation}
\operatorname*{Inv}\left(  \sigma\right)  \subseteq\operatorname*{Inv}\left(
p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)  },\ldots
,p_{\sigma\left(  m\right)  }\right)  .
\label{pf.lem.sol.exe.Ialbe.Inv=Inv.a.dir1}%
\end{equation}


On the other hand, let $d\in\operatorname*{Inv}\left(  p_{\sigma\left(
1\right)  },p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)
}\right)  $. Thus,%
\begin{align*}
d  &  \in\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  }%
,p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right) \\
&  =\left\{  \left(  u,v\right)  \in\left[  m\right]  ^{2}\ \mid\ u<v\text{
and }p_{\sigma\left(  u\right)  }>p_{\sigma\left(  v\right)  }\right\}
\end{align*}
(by (\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.a.Inv})). In other words, $d=\left(
u,v\right)  $ for some $\left(  u,v\right)  \in\left[  m\right]  ^{2}$
satisfying $u<v$ and $p_{\sigma\left(  u\right)  }>p_{\sigma\left(  v\right)
}$. Consider this $\left(  u,v\right)  $.

From $\left(  u,v\right)  \in\left[  m\right]  ^{2}$, we obtain $u\in\left[
m\right]  $ and $v\in\left[  m\right]  $. From $u\in\left[  m\right]  $, we
obtain $1\leq u$. From $v\in\left[  m\right]  $, we obtain $v\leq m$. Thus,
$1\leq u<v\leq m$.

From $u<v$, we obtain $u\neq v$. If we had $\sigma\left(  u\right)
=\sigma\left(  v\right)  $, then we would have $\sigma^{-1}\left(
\underbrace{\sigma\left(  u\right)  }_{=\sigma\left(  v\right)  }\right)
=\sigma^{-1}\left(  \sigma\left(  v\right)  \right)  =v$, which would
contradict $\sigma^{-1}\left(  \sigma\left(  u\right)  \right)  =u\neq v$.
Thus, we cannot have $\sigma\left(  u\right)  =\sigma\left(  v\right)  $. In
other words, we have $\sigma\left(  u\right)  \neq\sigma\left(  v\right)  $.

Now, let us assume (for the sake of contradiction) that $\sigma\left(
u\right)  <\sigma\left(  v\right)  $. Then, $p_{\sigma\left(  u\right)
}<p_{\sigma\left(  v\right)  }$ (by (\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.pupv}),
applied to $\sigma\left(  u\right)  $ and $\sigma\left(  v\right)  $ instead
of $u$ and $v$). This contradicts $p_{\sigma\left(  u\right)  }>p_{\sigma
\left(  v\right)  }$. This contradiction proves that our assumption (that
$\sigma\left(  u\right)  <\sigma\left(  v\right)  $) was false. Hence, we
cannot have $\sigma\left(  u\right)  <\sigma\left(  v\right)  $. We thus have
$\sigma\left(  u\right)  \geq\sigma\left(  v\right)  $. Combining this with
$\sigma\left(  u\right)  \neq\sigma\left(  v\right)  $, we obtain
$\sigma\left(  u\right)  >\sigma\left(  v\right)  $.

So we know that $\left(  u,v\right)  $ is a pair of integers satisfying $1\leq
u<v\leq m$ and $\sigma\left(  u\right)  >\sigma\left(  v\right)  $. In other
words, $\left(  u,v\right)  $ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq m$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. In other words, $\left(  u,v\right)  $ is an inversion of
$\sigma$ (by the definition of an \textquotedblleft inversion of $\sigma
$\textquotedblright). In other words, $\left(  u,v\right)  $ is an element of
$\operatorname*{Inv}\left(  \sigma\right)  $ (since $\operatorname*{Inv}%
\left(  \sigma\right)  $ is the set of all inversions of $\sigma$). In other
words, $\left(  u,v\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $.
Thus, $d=\left(  u,v\right)  \in\operatorname*{Inv}\left(  \sigma\right)  $.

Now, forget that we fixed $d$. We thus have shown that every \newline%
$d\in\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  $ satisfies
$d\in\operatorname*{Inv}\left(  \sigma\right)  $. In other words,%
\[
\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  \subseteq
\operatorname*{Inv}\left(  \sigma\right)  .
\]
Combining this with (\ref{pf.lem.sol.exe.Ialbe.Inv=Inv.a.dir1}), we obtain
$\operatorname*{Inv}\left(  \sigma\right)  =\operatorname*{Inv}\left(
p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)  },\ldots
,p_{\sigma\left(  m\right)  }\right)  $. This proves Lemma
\ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(a)}.

\textbf{(b)} Recall that $\ell\left(  \sigma\right)  $ is the number of
inversions of $\sigma$ (indeed, this is how $\ell\left(  \sigma\right)  $ was
defined). Thus,%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \\
&  =\left\vert \underbrace{\operatorname*{Inv}\left(  \sigma\right)
}_{\substack{=\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)
},p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)
\\\text{(by Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(a)})}}}\right\vert
=\left\vert \operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  }%
,p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)
\right\vert .
\end{align*}
Comparing this with%
\begin{align*}
\ell\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)  }%
,\ldots,p_{\sigma\left(  m\right)  }\right)   &  =\left\vert
\operatorname*{Inv}\left(  p_{\sigma\left(  1\right)  },p_{\sigma\left(
2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  \right\vert \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\ell\left(
p_{\sigma\left(  1\right)  },p_{\sigma\left(  2\right)  },\ldots
,p_{\sigma\left(  m\right)  }\right)  \right)  ,
\end{align*}
we obtain $\ell\left(  \sigma\right)  =\ell\left(  p_{\sigma\left(  1\right)
},p_{\sigma\left(  2\right)  },\ldots,p_{\sigma\left(  m\right)  }\right)  $.
This proves Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(b)}.
\end{proof}
\end{verlong}

In the following, we shall use the following notation (known as
\href{https://en.wikipedia.org/wiki/Iverson_bracket}{the \textit{Iverson
bracket}}): If $\mathcal{A}$ is any logical statement, then $\left[
\mathcal{A}\right]  $ will mean the integer $%
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}%
\end{cases}
$. For example, $\left[  1+1=2\right]  =1$ (since $1+1=2$ is true), whereas
$\left[  1+1=1\right]  =0$ (since $1+1=1$ is false). Clearly, if $\mathcal{A}$
and $\mathcal{B}$ are two equivalent logical statements, then $\left[
\mathcal{A}\right]  =\left[  \mathcal{B}\right]  $.

Now, let us show some more lemmas:

\begin{lemma}
\label{lem.sol.exe.Ialbe.iverson}Let $n\in\mathbb{N}$. Let $\mathbf{a}=\left(
a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of integers. Then,%
\[
\ell\left(  \mathbf{a}\right)  =\sum_{i=1}^{n}\sum_{j=1}^{n}\left[
i<j\right]  \left[  a_{i}>a_{j}\right]  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.iverson}.]If $\mathcal{A}$ and
$\mathcal{B}$ are two logical statements, then%
\begin{equation}
\left[  \mathcal{A}\right]  \left[  \mathcal{B}\right]  =\left[
\mathcal{A}\text{ and }\mathcal{B}\right]
\label{pf.lem.sol.exe.Ialbe.iverson.short.AandB}%
\end{equation}
(because both sides of this equality are $1$ if $\mathcal{A}$ and
$\mathcal{B}$ are both satisfied, and both sides are $0$ if not).

We have%
\begin{align*}
&  \underbrace{\sum_{i=1}^{n}\sum_{j=1}^{n}}_{=\sum_{i\in\left[  n\right]
}\sum_{j\in\left[  n\right]  }=\sum_{\left(  i,j\right)  \in\left[  n\right]
^{2}}}\underbrace{\left[  i<j\right]  \left[  a_{i}>a_{j}\right]
}_{\substack{=\left[  i<j\text{ and }a_{i}>a_{j}\right]  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.iverson.short.AandB}))}}}\\
&  =\sum_{\left(  i,j\right)  \in\left[  n\right]  ^{2}}\left[  i<j\text{ and
}a_{i}>a_{j}\right] \\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{
and }a_{i}>a_{j}}}\underbrace{\left[  i<j\text{ and }a_{i}>a_{j}\right]
}_{\substack{=1\\\text{(since }i<j\text{ and }a_{i}>a_{j}\text{)}}%
}+\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\\text{not
}\left(  i<j\text{ and }a_{i}>a_{j}\right)  }}\underbrace{\left[  i<j\text{
and }a_{i}>a_{j}\right]  }_{\substack{=0\\\text{(since not }\left(  i<j\text{
and }a_{i}>a_{j}\right)  \text{)}}}\\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{
and }a_{i}>a_{j}}}1+\underbrace{\sum_{\substack{\left(  i,j\right)  \in\left[
n\right]  ^{2};\\\text{not }\left(  i<j\text{ and }a_{i}>a_{j}\right)  }%
}0}_{=0}=\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]
^{2};\\i<j\text{ and }a_{i}>a_{j}}}1\\
&  =\left\vert \underbrace{\left\{  \left(  i,j\right)  \in\left[  n\right]
^{2}\ \mid\ i<j\text{ and }a_{i}>a_{j}\right\}  }%
_{\substack{=\operatorname*{Inv}\left(  \mathbf{a}\right)  \\\text{(by
(\ref{sol.exe.Ialbe.Inv.1}))}}}\right\vert \cdot1=\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert \cdot1\\
&  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert
=\ell\left(  \mathbf{a}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }%
\ell\left(  \mathbf{a}\right)  \text{ was defined to be }\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert \right)  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.iverson}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.iverson}.]We have%
\begin{align*}
\sum_{c\in\operatorname*{Inv}\left(  \mathbf{a}\right)  }1  &  =\sum
_{c\in\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{
and }a_{u}>a_{v}\right\}  }1\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\operatorname*{Inv}\left(
\mathbf{a}\right)  =\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  \text{ (by
(\ref{sol.exe.Ialbe.Inv.2}))}\right) \\
&  =\underbrace{\sum_{\left(  i,j\right)  \in\left\{  \left(  u,v\right)
\in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  }}%
_{=\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{
and }a_{i}>a_{j}}}}1\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\left(
i,j\right)  \text{ for }c\text{ in the sum}\right) \\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{
and }a_{i}>a_{j}}}1.
\end{align*}
Comparing this with%
\begin{align*}
&  \sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2}%
;\\i<j}}\left[  a_{i}>a_{j}\right] \\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{
and }a_{i}>a_{j}}}\underbrace{\left[  a_{i}>a_{j}\right]  }%
_{\substack{=1\\\text{(since }a_{i}>a_{j}\text{)}}}+\sum_{\substack{\left(
i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{ and not }a_{i}>a_{j}%
}}\underbrace{\left[  a_{i}>a_{j}\right]  }_{\substack{=0\\\text{(since we
don't have }a_{i}>a_{j}\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every }\left(  i,j\right)  \in\left[  n\right]  ^{2}\text{
satisfies either }a_{i}>a_{j}\text{ or}\\
\left(  \text{not }a_{i}>a_{j}\right)  \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{
and }a_{i}>a_{j}}}1+\underbrace{\sum_{\substack{\left(  i,j\right)  \in\left[
n\right]  ^{2};\\i<j\text{ and not }a_{i}>a_{j}}}0}_{=0}=\sum
_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j\text{ and
}a_{i}>a_{j}}}1,
\end{align*}
we obtain%
\[
\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2};\\i<j}}\left[
a_{i}>a_{j}\right]  =\sum_{c\in\operatorname*{Inv}\left(  \mathbf{a}\right)
}1=\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert
\cdot1=\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert .
\]
Comparing this with%
\[
\ell\left(  \mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(
\mathbf{a}\right)  \right\vert \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\ell\left(  \mathbf{a}\right)  \right)  ,
\]
we obtain%
\[
\ell\left(  \mathbf{a}\right)  =\sum_{\substack{\left(  i,j\right)  \in\left[
n\right]  ^{2};\\i<j}}\left[  a_{i}>a_{j}\right]  .
\]
Comparing this with%
\begin{align*}
&  \sum_{\left(  i,j\right)  \in\left[  n\right]  ^{2}}\left[  i<j\right]
\left[  a_{i}>a_{j}\right] \\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2}%
;\\i<j}}\underbrace{\left[  i<j\right]  }_{\substack{=1\\\text{(since
}i<j\text{)}}}\left[  a_{i}>a_{j}\right]  +\sum_{\substack{\left(  i,j\right)
\in\left[  n\right]  ^{2};\\\text{not }i<j}}\underbrace{\left[  i<j\right]
}_{\substack{=0\\\text{(since we don't have }i<j\text{)}}}\left[  a_{i}%
>a_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every }\left(  i,j\right)  \in\left[  n\right]  ^{2}\text{
satisfies either }i<j\text{ or}\\
\left(  \text{not }i<j\right)  \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]  ^{2}%
;\\i<j}}\left[  a_{i}>a_{j}\right]  +\underbrace{\sum_{\substack{\left(
i,j\right)  \in\left[  n\right]  ^{2};\\\text{not }i<j}}0\left[  a_{i}%
>a_{j}\right]  }_{=0}=\sum_{\substack{\left(  i,j\right)  \in\left[  n\right]
^{2};\\i<j}}\left[  a_{i}>a_{j}\right]  ,
\end{align*}
we obtain%
\begin{align*}
\ell\left(  \mathbf{a}\right)   &  =\underbrace{\sum_{\left(  i,j\right)
\in\left[  n\right]  ^{2}}}_{=\sum_{i\in\left[  n\right]  }\sum_{j\in\left[
n\right]  }}\left[  i<j\right]  \left[  a_{i}>a_{j}\right] \\
&  =\underbrace{\sum_{i\in\left[  n\right]  }}_{\substack{=\sum_{i\in\left\{
1,2,\ldots,n\right\}  }\\\text{(since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \text{)}}}\underbrace{\sum_{j\in\left[  n\right]  }%
}_{\substack{=\sum_{j\in\left\{  1,2,\ldots,n\right\}  }\\\text{(since
}\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  \text{)}}}\left[
i<j\right]  \left[  a_{i}>a_{j}\right] \\
&  =\underbrace{\sum_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{i=1}^{n}%
}\underbrace{\sum_{j\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{j=1}^{n}%
}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  =\sum_{i=1}^{n}\sum_{j=1}%
^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.iverson}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exe.Ialbe.ab}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers. Let $\mathbf{b}=\left(  b_{1},b_{2},\ldots,b_{m}\right)  $ be an
$m$-tuple of integers. Let $\mathbf{c}$ be the $\left(  n+m\right)  $-tuple
$\left(  a_{1},a_{2},\ldots,a_{n},b_{1},b_{2},\ldots,b_{m}\right)  $ of
integers. Then,%
\[
\ell\left(  \mathbf{c}\right)  =\ell\left(  \mathbf{a}\right)  +\ell\left(
\mathbf{b}\right)  +\sum_{\left(  i,j\right)  \in\left[  n\right]
\times\left[  m\right]  }\left[  a_{i}>b_{j}\right]  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.ab}.]Lemma
\ref{lem.sol.exe.Ialbe.iverson} yields%
\begin{equation}
\ell\left(  \mathbf{a}\right)  =\sum_{i=1}^{n}\sum_{j=1}^{n}\left[
i<j\right]  \left[  a_{i}>a_{j}\right]  .
\label{pf.lem.sol.exe.Ialbe.ab.short.la=}%
\end{equation}
Also, Lemma \ref{lem.sol.exe.Ialbe.iverson} (applied to $m$, $\mathbf{b}$ and
$b_{i}$ instead of $n$, $\mathbf{a}$ and $a_{i}$) yields%
\begin{equation}
\ell\left(  \mathbf{b}\right)  =\sum_{i=1}^{m}\sum_{j=1}^{m}\left[
i<j\right]  \left[  b_{i}>b_{j}\right]  .
\label{pf.lem.sol.exe.Ialbe.ab.short.lb=}%
\end{equation}


Write the $\left(  n+m\right)  $-tuple $\mathbf{c}$ in the form $\mathbf{c}%
=\left(  c_{1},c_{2},\ldots,c_{n+m}\right)  $. Thus,%
\[
\left(  c_{1},c_{2},\ldots,c_{n+m}\right)  =\mathbf{c}=\left(  a_{1}%
,a_{2},\ldots,a_{n},b_{1},b_{2},\ldots,b_{m}\right)
\]
(by the definition of $\mathbf{c}$). In other words,%
\begin{equation}
\left(  c_{i}=a_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,n\right\}  \right)  \label{pf.lem.sol.exe.Ialbe.ab.short.ci.a}%
\end{equation}
and%
\begin{equation}
\left(  c_{i}=b_{i-n}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
n+1,n+2,\ldots,n+m\right\}  \right)  .
\label{pf.lem.sol.exe.Ialbe.ab.short.ci.b}%
\end{equation}


But Lemma \ref{lem.sol.exe.Ialbe.iverson} (applied to $n+m$, $\mathbf{c}$ and
$c_{i}$ instead of $n$, $\mathbf{a}$ and $a_{i}$) yields%
\begin{align}
\ell\left(  \mathbf{c}\right)   &  =\sum_{i=1}^{n+m}\sum_{j=1}^{n+m}\left[
i<j\right]  \left[  c_{i}>c_{j}\right] \nonumber\\
&  =\sum_{i=1}^{n}\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}%
>c_{j}\right]  +\sum_{i=n+1}^{n+m}\sum_{j=1}^{n+m}\left[  i<j\right]  \left[
c_{i}>c_{j}\right]  \label{pf.lem.sol.exe.Ialbe.ab.short.1}%
\end{align}
(since $0\leq n\leq n+m$).

But every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]  =\sum
_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  +\sum_{j=1}%
^{m}\left[  a_{i}>b_{j}\right]  \label{pf.lem.sol.exe.Ialbe.ab.short.3a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.ab.short.3a}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Thus, $c_{i}=a_{i}$ (by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.ci.a})).
\par
For every $j\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
c_{j}=a_{j} \label{pf.lem.sol.exe.Ialbe.ab.short.3a.pf.1}%
\end{equation}
(by (\ref{pf.lem.sol.exe.Ialbe.ab.short.ci.a}), applied to $j$ instead of
$i$).
\par
For every $j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $, we have%
\begin{equation}
c_{j}=b_{j-n} \label{pf.lem.sol.exe.Ialbe.ab.short.3a.pf.2}%
\end{equation}
(by (\ref{pf.lem.sol.exe.Ialbe.ab.short.ci.b}), applied to $j$ instead of
$i$).
\par
We have $i\leq n$ (since $i\in\left\{  1,2,\ldots,n\right\}  $). For every
$j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $, we have $j\geq n+1>n\geq i$
(since $i\leq n$), thus $i<j$, therefore%
\begin{equation}
\left[  i<j\right]  =1. \label{pf.lem.sol.exe.Ialbe.ab.short.3a.pf.3}%
\end{equation}
\par
Recall that $0\leq n\leq n+m$. Hence,%
\begin{align*}
\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]   &
=\sum_{j=1}^{n}\left[  i<j\right]  \left[  \underbrace{c_{i}}_{=a_{i}%
}>\underbrace{c_{j}}_{\substack{=a_{j}\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.3a.pf.1}))}}}\right]  +\sum_{j=n+1}%
^{n+m}\underbrace{\left[  i<j\right]  }_{\substack{=1\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.3a.pf.3}))}}}\left[  \underbrace{c_{i}%
}_{=a_{i}}>\underbrace{c_{j}}_{\substack{=b_{j-n}\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.3a.pf.2}))}}}\right] \\
&  =\sum_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]
+\sum_{j=n+1}^{n+m}\left[  a_{i}>b_{j-n}\right] \\
&  =\sum_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  +\sum
_{j=1}^{m}\left[  a_{i}>b_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j\text{ for
}j-n\text{ in the second sum}\right)  .
\end{align*}
This proves (\ref{pf.lem.sol.exe.Ialbe.ab.short.3a}).}.

Also, every $i\in\left\{  n+1,n+2,\ldots,n+m\right\}  $ satisfies%
\begin{equation}
\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]  =\sum
_{j=1}^{m}\left[  i-n<j\right]  \left[  b_{i-n}>b_{j}\right]
\label{pf.lem.sol.exe.Ialbe.ab.short.3b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.ab.short.3b}):} Let
$i\in\left\{  n+1,n+2,\ldots,n+m\right\}  $. Thus, $c_{i}=b_{i-n}$ (by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.ci.b})).
\par
For every $j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $, we have%
\begin{equation}
c_{j}=b_{j-n} \label{pf.lem.sol.exe.Ialbe.ab.short.3b.pf.2}%
\end{equation}
(by (\ref{pf.lem.sol.exe.Ialbe.ab.short.ci.b}), applied to $j$ instead of
$i$).
\par
We have $i\geq n+1$ (since $i\in\left\{  n+1,n+2,\ldots,n+m\right\}  $) and
thus $i\geq n+1>n$, so that $n<i$. For every $j\in\left\{  1,2,\ldots
,n\right\}  $, we have $j\leq n\leq i$ (since $i\geq n$) and thus $i\geq j$.
Hence, for every $j\in\left\{  1,2,\ldots,n\right\}  $, we don't have $i<j$.
Thus, for every $j\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left[  i<j\right]  =0. \label{pf.lem.sol.exe.Ialbe.ab.short.3b.pf.3}%
\end{equation}
\par
Recall that $0\leq n\leq n+m$. Hence,%
\begin{align*}
\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]   &
=\sum_{j=1}^{n}\underbrace{\left[  i<j\right]  }_{\substack{=0\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.3b.pf.3}))}}}\left[  c_{i}>c_{j}\right]
+\sum_{j=n+1}^{n+m}\underbrace{\left[  i<j\right]  }_{\substack{=\left[
i-n<j-n\right]  \\\text{(since }i<j\text{ is equivalent to}\\i-n<j-n\text{)}%
}}\left[  \underbrace{c_{i}}_{=b_{i-n}}>\underbrace{c_{j}}_{\substack{=b_{j-n}%
\\\text{(by (\ref{pf.lem.sol.exe.Ialbe.ab.short.3b.pf.2}))}}}\right] \\
&  =\underbrace{\sum_{j=1}^{n}0\left[  c_{i}>c_{j}\right]  }_{=0}+\sum
_{j=n+1}^{n+m}\left[  i-n<j-n\right]  \left[  b_{i-n}>b_{j-n}\right] \\
&  =\sum_{j=n+1}^{n+m}\left[  i-n<j-n\right]  \left[  b_{i-n}>b_{j-n}\right]
\\
&  =\sum_{j=1}^{m}\left[  i-n<j\right]  \left[  b_{i-n}>b_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j\text{ for
}j-n\text{ in the sum}\right)  .
\end{align*}
This proves (\ref{pf.lem.sol.exe.Ialbe.ab.short.3b}).}.

Now, (\ref{pf.lem.sol.exe.Ialbe.ab.short.1}) becomes%
\begin{align*}
&  \ell\left(  \mathbf{c}\right) \\
&  =\sum_{i=1}^{n}\underbrace{\sum_{j=1}^{n+m}\left[  i<j\right]  \left[
c_{i}>c_{j}\right]  }_{\substack{=\sum_{j=1}^{n}\left[  i<j\right]  \left[
a_{i}>a_{j}\right]  +\sum_{j=1}^{m}\left[  a_{i}>b_{j}\right]  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.3a}))}}}+\sum_{i=n+1}^{n+m}%
\underbrace{\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]
}_{\substack{=\sum_{j=1}^{m}\left[  i-n<j\right]  \left[  b_{i-n}%
>b_{j}\right]  \\\text{(by (\ref{pf.lem.sol.exe.Ialbe.ab.short.3b}))}}}\\
&  =\underbrace{\sum_{i=1}^{n}\left(  \sum_{j=1}^{n}\left[  i<j\right]
\left[  a_{i}>a_{j}\right]  +\sum_{j=1}^{m}\left[  a_{i}>b_{j}\right]
\right)  }_{=\sum_{i=1}^{n}\sum_{j=1}^{n}\left[  i<j\right]  \left[
a_{i}>a_{j}\right]  +\sum_{i=1}^{n}\sum_{j=1}^{m}\left[  a_{i}>b_{j}\right]
}+\underbrace{\sum_{i=n+1}^{n+m}\sum_{j=1}^{m}\left[  i-n<j\right]  \left[
b_{i-n}>b_{j}\right]  }_{\substack{=\sum_{i=1}^{m}\sum_{j=1}^{m}\left[
i<j\right]  \left[  b_{i}>b_{j}\right]  \\\text{(here, we have substituted
}i\text{ for }i-n\\\text{in the outer sum)}}}\\
&  =\underbrace{\sum_{i=1}^{n}\sum_{j=1}^{n}\left[  i<j\right]  \left[
a_{i}>a_{j}\right]  }_{\substack{=\ell\left(  \mathbf{a}\right)  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.la=}))}}}+\underbrace{\sum_{i=1}^{n}%
}_{\substack{=\sum_{i\in\left[  n\right]  }}}\underbrace{\sum_{j=1}^{m}%
}_{\substack{=\sum_{j\in\left[  m\right]  }}}\left[  a_{i}>b_{j}\right]
+\underbrace{\sum_{i=1}^{m}\sum_{j=1}^{m}\left[  i<j\right]  \left[
b_{i}>b_{j}\right]  }_{\substack{=\ell\left(  \mathbf{b}\right)  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.short.lb=}))}}}\\
&  =\ell\left(  \mathbf{a}\right)  +\underbrace{\sum_{i\in\left[  n\right]
}\sum_{j\in\left[  m\right]  }}_{=\sum_{\left(  i,j\right)  \in\left[
n\right]  \times\left[  m\right]  }}\left[  a_{i}>b_{j}\right]  +\ell\left(
\mathbf{b}\right) \\
&  =\ell\left(  \mathbf{a}\right)  +\sum_{\left(  i,j\right)  \in\left[
n\right]  \times\left[  m\right]  }\left[  a_{i}>b_{j}\right]  +\ell\left(
\mathbf{b}\right)  =\ell\left(  \mathbf{a}\right)  +\ell\left(  \mathbf{b}%
\right)  +\sum_{\left(  i,j\right)  \in\left[  n\right]  \times\left[
m\right]  }\left[  a_{i}>b_{j}\right]  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.ab}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.ab}.]We have $n+\underbrace{m}_{\geq
0}\geq n$, so that $n\leq n+m$. Also, $n\geq0$, so that $0\leq n$.

Lemma \ref{lem.sol.exe.Ialbe.iverson} yields%
\begin{equation}
\ell\left(  \mathbf{a}\right)  =\sum_{i=1}^{n}\sum_{j=1}^{n}\left[
i<j\right]  \left[  a_{i}>a_{j}\right]  . \label{pf.lem.sol.exe.Ialbe.ab.la=}%
\end{equation}
Also, Lemma \ref{lem.sol.exe.Ialbe.iverson} (applied to $m$, $\mathbf{b}$ and
$b_{i}$ instead of $n$, $\mathbf{a}$ and $a_{i}$) yields%
\begin{equation}
\ell\left(  \mathbf{b}\right)  =\sum_{i=1}^{m}\sum_{j=1}^{m}\left[
i<j\right]  \left[  b_{i}>b_{j}\right]  . \label{pf.lem.sol.exe.Ialbe.ab.lb=}%
\end{equation}


Write the $\left(  n+m\right)  $-tuple $\mathbf{c}$ in the form $\mathbf{c}%
=\left(  c_{1},c_{2},\ldots,c_{n+m}\right)  $. Thus,%
\[
\left(  c_{1},c_{2},\ldots,c_{n+m}\right)  =\mathbf{c}=\left(  a_{1}%
,a_{2},\ldots,a_{n},b_{1},b_{2},\ldots,b_{m}\right)
\]
(by the definition of $\mathbf{c}$). In other words,%
\begin{equation}
c_{i}=%
\begin{cases}
a_{i}, & \text{if }i\leq n;\\
b_{i-n}, & \text{if }i>n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n+m\right\}  .
\label{pf.lem.sol.exe.Ialbe.ab.ci.1}%
\end{equation}
Now, we have%
\begin{equation}
c_{i}=a_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots
,n\right\}  \label{pf.lem.sol.exe.Ialbe.ab.ci.a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.ab.ci.a}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, $1\leq i\leq n$. Now,
$n+\underbrace{m}_{\geq0}\geq n$, so that $n\leq n+m$ and thus $i\leq n\leq
n+m$. Combining this with $1\leq i$, we obtain $1\leq i\leq n+m$, so that
$i\in\left\{  1,2,\ldots,n+m\right\}  $. Thus,
(\ref{pf.lem.sol.exe.Ialbe.ab.ci.1}) yields $c_{i}=%
\begin{cases}
a_{i}, & \text{if }i\leq n;\\
b_{i-n}, & \text{if }i>n
\end{cases}
=a_{i}$ (since $i\leq n$). This proves (\ref{pf.lem.sol.exe.Ialbe.ab.ci.a}).}.
Also,%
\begin{equation}
c_{i}=b_{i-n}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  n+1,n+2,\ldots
,n+m\right\}  \label{pf.lem.sol.exe.Ialbe.ab.ci.b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.ab.ci.a}):} Let
$i\in\left\{  n+1,n+2,\ldots,n+m\right\}  $. Then, $n+1\leq i\leq n+m$. Now,
$\underbrace{n}_{\geq0}+1\geq1$, so that $1\leq n+1\leq i$. Combining this
with $i\leq n+m$, we obtain $1\leq i\leq n+m$, so that $i\in\left\{
1,2,\ldots,n+m\right\}  $. Thus, (\ref{pf.lem.sol.exe.Ialbe.ab.ci.1}) yields
$c_{i}=%
\begin{cases}
a_{i}, & \text{if }i\leq n;\\
b_{i-n}, & \text{if }i>n
\end{cases}
=b_{i-n}$ (since $i>n$). This proves (\ref{pf.lem.sol.exe.Ialbe.ab.ci.b}).}.

But recall that $\mathbf{c}=\left(  c_{1},c_{2},\ldots,c_{n+m}\right)  $ is an
$\left(  n+m\right)  $-tuple of integers. Thus, Lemma
\ref{lem.sol.exe.Ialbe.iverson} (applied to $n+m$, $\mathbf{c}$ and $c_{i}$
instead of $n$, $\mathbf{a}$ and $a_{i}$) yields%
\begin{align}
\ell\left(  \mathbf{c}\right)   &  =\sum_{i=1}^{n+m}\sum_{j=1}^{n+m}\left[
i<j\right]  \left[  c_{i}>c_{j}\right] \nonumber\\
&  =\sum_{i=1}^{n}\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}%
>c_{j}\right]  +\sum_{i=n+1}^{n+m}\sum_{j=1}^{n+m}\left[  i<j\right]  \left[
c_{i}>c_{j}\right]  \label{pf.lem.sol.exe.Ialbe.ab.1}%
\end{align}
(since $0\leq n\leq n+m$). But every $i\in\left\{  1,2,\ldots,n\right\}  $
satisfies%
\begin{equation}
\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]  =\sum
_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  +\sum_{j=1}%
^{m}\left[  a_{i}>b_{j}\right]  \label{pf.lem.sol.exe.Ialbe.ab.3a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.ab.3a}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Thus, $c_{i}=a_{i}$ (by
(\ref{pf.lem.sol.exe.Ialbe.ab.ci.a})).
\par
For every $j\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
c_{j}=a_{j} \label{pf.lem.sol.exe.Ialbe.ab.3a.pf.1}%
\end{equation}
(by (\ref{pf.lem.sol.exe.Ialbe.ab.ci.a}), applied to $j$ instead of $i$).
\par
For every $j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $, we have%
\begin{equation}
c_{j}=b_{j-n} \label{pf.lem.sol.exe.Ialbe.ab.3a.pf.2}%
\end{equation}
(by (\ref{pf.lem.sol.exe.Ialbe.ab.ci.b}), applied to $j$ instead of $i$).
\par
We have $i\leq n$ (since $i\in\left\{  1,2,\ldots,n\right\}  $). For every
$j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $, we have $j\geq n+1>n\geq i$
(since $i\leq n$). Hence, for every $j\in\left\{  n+1,n+2,\ldots,n+m\right\}
$, we have $i<j$. Thus, for every $j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $,
we have%
\begin{equation}
\left[  i<j\right]  =1. \label{pf.lem.sol.exe.Ialbe.ab.3a.pf.3}%
\end{equation}
\par
Recall that $0\leq n\leq n+m$. Hence,%
\begin{align*}
&  \sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right] \\
&  =\sum_{j=1}^{n}\left[  i<j\right]  \left[  \underbrace{c_{i}}_{=a_{i}%
}>\underbrace{c_{j}}_{\substack{=a_{j}\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.3a.pf.1}))}}}\right]  +\sum_{j=n+1}%
^{n+m}\underbrace{\left[  i<j\right]  }_{\substack{=1\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.3a.pf.3}))}}}\left[  \underbrace{c_{i}}_{=a_{i}%
}>\underbrace{c_{j}}_{\substack{=b_{j-n}\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.3a.pf.2}))}}}\right] \\
&  =\sum_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]
+\sum_{j=n+1}^{n+m}\left[  a_{i}>b_{j-n}\right] \\
&  =\sum_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  +\sum
_{j=1}^{m}\left[  a_{i}>b_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j\text{ for
}j-n\text{ in the second sum}\right)  .
\end{align*}
This proves (\ref{pf.lem.sol.exe.Ialbe.ab.3a}).}. Also, every $i\in\left\{
n+1,n+2,\ldots,n+m\right\}  $ satisfies%
\begin{equation}
\sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]  =\sum
_{j=1}^{m}\left[  i-n<j\right]  \left[  b_{i-n}>b_{j}\right]
\label{pf.lem.sol.exe.Ialbe.ab.3b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.ab.3b}):} Let
$i\in\left\{  n+1,n+2,\ldots,n+m\right\}  $. Thus, $c_{i}=b_{i-n}$ (by
(\ref{pf.lem.sol.exe.Ialbe.ab.ci.b})).
\par
For every $j\in\left\{  n+1,n+2,\ldots,n+m\right\}  $, we have%
\begin{equation}
c_{j}=b_{j-n} \label{pf.lem.sol.exe.Ialbe.ab.3b.pf.2}%
\end{equation}
(by (\ref{pf.lem.sol.exe.Ialbe.ab.ci.b}), applied to $j$ instead of $i$).
\par
We have $i\geq n+1$ (since $i\in\left\{  n+1,n+2,\ldots,n+m\right\}  $) and
thus $i\geq n+1>n$, so that $n<i$. For every $j\in\left\{  1,2,\ldots
,n\right\}  $, we have $j\leq n\leq i$ (since $i\geq n$) and thus $i\geq j$.
Hence, for every $j\in\left\{  1,2,\ldots,n\right\}  $, we don't have $i<j$.
Thus, for every $j\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left[  i<j\right]  =0. \label{pf.lem.sol.exe.Ialbe.ab.3b.pf.3}%
\end{equation}
\par
Recall that $0\leq n\leq n+m$. Hence,%
\begin{align*}
&  \sum_{j=1}^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right] \\
&  =\sum_{j=1}^{n}\underbrace{\left[  i<j\right]  }_{\substack{=0\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.3b.pf.3}))}}}\left[  c_{i}>c_{j}\right]
+\sum_{j=n+1}^{n+m}\underbrace{\left[  i<j\right]  }_{\substack{=\left[
i-n<j-n\right]  \\\text{(since }i<j\text{ is equivalent to}\\i-n<j-n\text{)}%
}}\left[  \underbrace{c_{i}}_{=b_{i-n}}>\underbrace{c_{j}}_{\substack{=b_{j-n}%
\\\text{(by (\ref{pf.lem.sol.exe.Ialbe.ab.3b.pf.2}))}}}\right] \\
&  =\underbrace{\sum_{j=1}^{n}0\left[  c_{i}>c_{j}\right]  }_{=0}+\sum
_{j=n+1}^{n+m}\left[  i-n<j-n\right]  \left[  b_{i-n}>b_{j-n}\right] \\
&  =\sum_{j=n+1}^{n+m}\left[  i-n<j-n\right]  \left[  b_{i-n}>b_{j-n}\right]
\\
&  =\sum_{j=1}^{m}\left[  i-n<j\right]  \left[  b_{i-n}>b_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j\text{ for
}j-n\text{ in the sum}\right)  .
\end{align*}
This proves (\ref{pf.lem.sol.exe.Ialbe.ab.3b}).}.

Now, (\ref{pf.lem.sol.exe.Ialbe.ab.1}) becomes%
\begin{align*}
\ell\left(  \mathbf{c}\right)   &  =\sum_{i=1}^{n}\underbrace{\sum_{j=1}%
^{n+m}\left[  i<j\right]  \left[  c_{i}>c_{j}\right]  }_{\substack{=\sum
_{j=1}^{n}\left[  i<j\right]  \left[  a_{i}>a_{j}\right]  +\sum_{j=1}%
^{m}\left[  a_{i}>b_{j}\right]  \\\text{(by (\ref{pf.lem.sol.exe.Ialbe.ab.3a}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{i=n+1}^{n+m}\underbrace{\sum_{j=1}^{n+m}\left[
i<j\right]  \left[  c_{i}>c_{j}\right]  }_{\substack{=\sum_{j=1}^{m}\left[
i-n<j\right]  \left[  b_{i-n}>b_{j}\right]  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.3b}))}}}\\
&  =\underbrace{\sum_{i=1}^{n}\left(  \sum_{j=1}^{n}\left[  i<j\right]
\left[  a_{i}>a_{j}\right]  +\sum_{j=1}^{m}\left[  a_{i}>b_{j}\right]
\right)  }_{=\sum_{i=1}^{n}\sum_{j=1}^{n}\left[  i<j\right]  \left[
a_{i}>a_{j}\right]  +\sum_{i=1}^{n}\sum_{j=1}^{m}\left[  a_{i}>b_{j}\right]
}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{i=n+1}^{n+m}\sum_{j=1}^{m}\left[
i-n<j\right]  \left[  b_{i-n}>b_{j}\right]  }_{\substack{=\sum_{i=1}^{m}%
\sum_{j=1}^{m}\left[  i<j\right]  \left[  b_{i}>b_{j}\right]  \\\text{(here,
we have substituted }i\text{ for }i-n\\\text{in the outer sum)}}}\\
&  =\underbrace{\sum_{i=1}^{n}\sum_{j=1}^{n}\left[  i<j\right]  \left[
a_{i}>a_{j}\right]  }_{\substack{=\ell\left(  \mathbf{a}\right)  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.ab.la=}))}}}+\underbrace{\sum_{i=1}^{n}%
}_{\substack{=\sum_{i\in\left\{  1,2,\ldots,n\right\}  }=\sum_{i\in\left[
n\right]  }\\\text{(since }\left\{  1,2,\ldots,n\right\}  =\left[  n\right]
\\\text{(because }\left[  n\right]  =\left\{  1,2,\ldots,n\right\}
\\\text{(by the definition of }\left[  n\right]  \text{)))}}}\underbrace{\sum
_{j=1}^{m}}_{\substack{=\sum_{j\in\left\{  1,2,\ldots,m\right\}  }=\sum
_{j\in\left[  m\right]  }\\\text{(since }\left\{  1,2,\ldots,m\right\}
=\left[  m\right]  \\\text{(because }\left[  m\right]  =\left\{
1,2,\ldots,m\right\}  \\\text{(by the definition of }\left[  m\right]
\text{)))}}}\left[  a_{i}>b_{j}\right] \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{i=1}^{m}\sum_{j=1}^{m}\left[
i<j\right]  \left[  b_{i}>b_{j}\right]  }_{\substack{=\ell\left(
\mathbf{b}\right)  \\\text{(by (\ref{pf.lem.sol.exe.Ialbe.ab.lb=}))}}}\\
&  =\ell\left(  \mathbf{a}\right)  +\underbrace{\sum_{i\in\left[  n\right]
}\sum_{j\in\left[  m\right]  }}_{=\sum_{\left(  i,j\right)  \in\left[
n\right]  \times\left[  m\right]  }}\left[  a_{i}>b_{j}\right]  +\ell\left(
\mathbf{b}\right) \\
&  =\ell\left(  \mathbf{a}\right)  +\sum_{\left(  i,j\right)  \in\left[
n\right]  \times\left[  m\right]  }\left[  a_{i}>b_{j}\right]  +\ell\left(
\mathbf{b}\right)  =\ell\left(  \mathbf{a}\right)  +\ell\left(  \mathbf{b}%
\right)  +\sum_{\left(  i,j\right)  \in\left[  n\right]  \times\left[
m\right]  }\left[  a_{i}>b_{j}\right]  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.ab}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exe.Ialbe.inclist}Let $S$ be a finite set of integers. Let
$\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ be a list of all elements of $S$
(with no repetitions).

\textbf{(a)} Then, the map $\left[  s\right]  \rightarrow S,\ h\mapsto c_{h}$
is well-defined and a bijection.

\textbf{(b)} Let $\pi\in S_{s}$. Then, $\left(  c_{\pi\left(  1\right)
},c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ is a
list of all elements of $S$ (with no repetitions).
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.inclist}.]Lemma
\ref{lem.sol.exe.Ialbe.inclist} should really be intuitively obvious; let me
merely sketch how to formalize the intuition.

We have the following basic fact:

\begin{statement}
\textit{Statement 1:} Let $X$ and $Y$ be two sets. Let $\phi:X\rightarrow Y$
be a map. Then, $\phi$ is a bijection if and only if each $i\in Y$ has exactly
one preimage under $\phi$.
\end{statement}

Statement 1 is well-known and easy to prove; it is a rather useful (necessary
and sufficient) criterion for proving the bijectivity of maps (particularly
since it does not require proving injectivity and surjectivity separately).

We assumed that $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ is a list of all
elements of $S$ (with no repetitions). This means that the following two
statements are valid:

\begin{statement}
\textit{Statement 2:} The list $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ is
a list of elements of $S$. (In other words, we have $c_{h}\in S$ for every
$h\in\left[  s\right]  $.)
\end{statement}

\begin{statement}
\textit{Statement 3:} Each element of $S$ appears exactly once in the list
$\left(  c_{1},c_{2},\ldots,c_{s}\right)  $. In other words, for each $i\in
S$, there exists exactly one $h\in\left[  s\right]  $ satisfying $i=c_{h}$.
\end{statement}

Statement 2 shows that $c_{h}\in S$ for every $h\in\left[  s\right]  $. Thus,
the map $\left[  s\right]  \rightarrow S,\ h\mapsto c_{h}$ is well-defined.
Denote this map by $\alpha$. Thus, $\alpha\left(  h\right)  =c_{h}$ for every
$h\in\left[  s\right]  $.

\textbf{(a)} Statement 3 shows that, for each $i\in S$, there exists exactly
one $h\in\left[  s\right]  $ satisfying $i=c_{h}$. Since $\alpha\left(
h\right)  =c_{h}$ for every $h\in\left[  s\right]  $, we can rewrite this as
follows: For each $i\in S$, there exists exactly one $h\in\left[  s\right]  $
satisfying $i=\alpha\left(  h\right)  $. In other words, each $i\in S$ has
exactly one preimage under $\alpha$. According to Statement 1 (applied to
$X=\left[  s\right]  $, $Y=S$ and $\phi=\alpha$), this holds if and only if
$\alpha$ is a bijection. Thus, $\alpha$ is a bijection. In other words, the
map $\left[  s\right]  \rightarrow S,\ h\mapsto c_{h}$ is a bijection (because
$\alpha$ is precisely this map). Thus, Lemma \ref{lem.sol.exe.Ialbe.inclist}
\textbf{(a)} is proven.

\textbf{(b)} We must prove that $\left(  c_{\pi\left(  1\right)  }%
,c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ is a
list of all elements of $S$ (with no repetitions). This means proving the
following two statements:

\begin{statement}
\textit{Statement 4:} The list $\left(  c_{\pi\left(  1\right)  }%
,c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ is a
list of elements of $S$. (In other words, we have $c_{\pi\left(  h\right)
}\in S$ for every $h\in\left[  s\right]  $.)
\end{statement}

\begin{statement}
\textit{Statement 5:} Each element of $S$ appears exactly once in the list
$\left(  c_{\pi\left(  1\right)  },c_{\pi\left(  2\right)  },\ldots
,c_{\pi\left(  s\right)  }\right)  $. In other words, for each $i\in S$, there
exists exactly one $h\in\left[  s\right]  $ satisfying $i=c_{\pi\left(
h\right)  }$.
\end{statement}

\textit{Proof of Statement 4:} Statement 2 shows that $c_{h}\in S$ for every
$h\in\left[  s\right]  $. Applying this to $\pi\left(  h\right)  $ instead of
$h$, we conclude that $c_{\pi\left(  h\right)  }\in S$ for every $h\in\left[
s\right]  $. This proves Statement 4.

\textit{Proof of Statement 5:} The map $\alpha\circ\pi$ is a bijection (since
it is the composition of the two bijections $\alpha$ and $\pi$). According to
Statement 1 (applied to $X=\left[  s\right]  $, $Y=S$ and $\phi=\alpha\circ
\pi$), this means that each $i\in S$ has exactly one preimage under
$\alpha\circ\pi$. In other words, for each $i\in S$, there exists exactly one
$h\in\left[  s\right]  $ satisfying $i=\left(  \alpha\circ\pi\right)  \left(
h\right)  $. Since every $h\in\left[  s\right]  $ satisfies%
\[
\left(  \alpha\circ\pi\right)  \left(  h\right)  =\alpha\left(  \pi\left(
h\right)  \right)  =c_{\pi\left(  h\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\alpha\right)  ,
\]
we can rewrite this as follows: For each $i\in S$, there exists exactly one
$h\in\left[  s\right]  $ satisfying $i=c_{\pi\left(  h\right)  }$. This proves
Statement 5.

Now, Statements 4 and 5 are proven; thus, $\left(  c_{\pi\left(  1\right)
},c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ is a
list of all elements of $S$ (with no repetitions). This proves Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.inclist}.]We have $\left[  s\right]
=\left\{  1,2,\ldots,s\right\}  $ (by the definition of $\left[  s\right]  $).

The list $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ be a list of all elements
of $S$ (with no repetitions). In other words, the list $\left(  c_{1}%
,c_{2},\ldots,c_{s}\right)  $ is a list of elements of $S$, and contains each
element of $S$ exactly once.

The list $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ contains each element of
$S$ exactly once. In other words, for every $i\in S$, the list $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $ contains the element $i$ exactly once. In
other words,
\begin{equation}
\text{for every }i\in S\text{, there exists a unique }p\in\left\{
1,2,\ldots,s\right\}  \text{ such that }i=c_{p}.
\label{pf.lem.sol.exe.Ialbe.inclist.exonce}%
\end{equation}


For every $h\in\left[  s\right]  $, the element $c_{h}$ of $S$ is
well-defined\footnote{\textit{Proof.} Let $h\in\left[  s\right]  $. Then,
$h\in\left[  s\right]  =\left\{  1,2,\ldots,s\right\}  $. Thus, the element
$c_{h}$ of $S$ is well-defined (since $\left(  c_{1},c_{2},\ldots
,c_{s}\right)  $ is a list of elements of $S$). Qed.}. Thus, the map $\left[
s\right]  \rightarrow S,\ h\mapsto c_{h}$ is well-defined. Let us denote this
map by $\phi$. Thus, $\phi$ is the map $\left[  s\right]  \rightarrow
S,\ h\mapsto c_{h}$.

The map $\phi$ is injective\footnote{\textit{Proof.} Let $g$ and $h$ be two
elements of $\left[  s\right]  $ such that $\phi\left(  g\right)  =\phi\left(
h\right)  $. We shall show that $g=h$.
\par
We have $g\in\left[  s\right]  =\left\{  1,2,\ldots,s\right\}  $ and
$h\in\left[  s\right]  =\left\{  1,2,\ldots,s\right\}  $.
\par
The definition of $\phi$ yields $\phi\left(  g\right)  =c_{g}$. Hence,
$c_{g}=\phi\left(  g\right)  =\phi\left(  h\right)  =c_{h}$ (by the definition
of $\phi$). Hence, $h$ is an element of $\left\{  1,2,\ldots,s\right\}  $
satisfying $c_{g}=c_{h}$. In other words, $h$ is an element $p\in\left\{
1,2,\ldots,s\right\}  $ satisfying $c_{g}=c_{p}$.
\par
Also, $g$ is an element of $\left\{  1,2,\ldots,s\right\}  $ satisfying
$c_{g}=c_{g}$. In other words, $g$ is an element $p\in\left\{  1,2,\ldots
,s\right\}  $ satisfying $c_{g}=c_{p}$.
\par
But $c_{g}\in S$ (since $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ is a list
of elements of $S$). Hence, (\ref{pf.lem.sol.exe.Ialbe.inclist.exonce})
(applied to $i=c_{g}$) shows that there exists a unique $p\in\left\{
1,2,\ldots,s\right\}  $ such that $c_{g}=c_{p}$. In particular, there exists
\textbf{at most one} such $p$. In other words, if $u$ and $v$ are two elements
$p\in\left\{  1,2,\ldots,s\right\}  $ satisfying $c_{g}=c_{p}$, then $u=v$.
Applying this to $u=g$ and $v=h$, we obtain $g=h$ (since $g$ and $h$ are two
elements $p\in\left\{  1,2,\ldots,s\right\}  $ satisfying $c_{g}=c_{p}$).
\par
Now, forget that we fixed $g$ and $h$. We thus have proven that if $g$ and $h$
are two elements of $\left[  s\right]  $ such that $\phi\left(  g\right)
=\phi\left(  h\right)  $, then $g=h$. In other words, the map $\phi$ is
injective. Qed.} and surjective\footnote{\textit{Proof.} Let $i\in S$. Then,
there exists a unique $p\in\left\{  1,2,\ldots,s\right\}  $ such that
$i=c_{p}$ (by (\ref{pf.lem.sol.exe.Ialbe.inclist.exonce})). Consider this $p$.
We have $p\in\left\{  1,2,\ldots,s\right\}  =\left[  s\right]  $ (since
$\left[  s\right]  =\left\{  1,2,\ldots,s\right\}  $). Thus, $\phi\left(
p\right)  $ is well-defined. Now, the definition of $\phi$ yields $\phi\left(
p\right)  =c_{p}=i$. Thus, $i=\phi\left(  \underbrace{p}_{\in\left[  s\right]
}\right)  \in\phi\left(  \left[  s\right]  \right)  $.
\par
Now, forget that we fixed $i$. We thus have shown that every $i\in S$
satisfies $i\in\phi\left(  \left[  s\right]  \right)  $. In other words,
$S\subseteq\phi\left(  \left[  s\right]  \right)  $. In other words, the map
$\phi$ is surjective. Qed.}. Hence, this map $\phi$ is bijective. In other
words, $\phi$ is a bijection. In other words, the map $\left[  s\right]
\rightarrow S,\ h\mapsto c_{h}$ is a bijection (since $\phi$ is the map
$\left[  s\right]  \rightarrow S,\ h\mapsto c_{h}$). This completes the proof
of Lemma \ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)}.

\textbf{(b)} We know that $S_{s}$ is the set of all permutations of $\left\{
1,2,\ldots,s\right\}  $ (by the definition of $S_{s}$). In other words,
$S_{s}$ is the set of all permutations of $\left[  s\right]  $ (since
$\left\{  1,2,\ldots,s\right\}  =\left[  s\right]  $). Now, $\pi$ is an
element of $S_{s}$. In other words, $\pi$ is a permutation of $\left[
s\right]  $ (since $S_{s}$ is the set of all permutations of $\left[
s\right]  $). In other words, $\pi$ is a bijection $\left[  s\right]
\rightarrow\left[  s\right]  $. So the map $\pi:\left[  s\right]
\rightarrow\left[  s\right]  $ is bijective. Thus, this map $\pi$ is
surjective and injective.

The map $\pi$ is a map $\left[  s\right]  \rightarrow\left[  s\right]  $. In
other words, $\pi$ is a map $\left\{  1,2,\ldots,s\right\}  \rightarrow
\left\{  1,2,\ldots,s\right\}  $ (since $\left[  s\right]  =\left\{
1,2,\ldots,s\right\}  $). Hence, for every $i\in\left\{  1,2,\ldots,s\right\}
$, we have $\pi\left(  i\right)  \in\left\{  1,2,\ldots,s\right\}  $. Thus,
for every $i\in\left\{  1,2,\ldots,s\right\}  $, the element $c_{\pi\left(
i\right)  }$ is a well-defined element of $S$. Thus, $\left(  c_{\pi\left(
1\right)  },c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)
$ is a list of elements of $S$.

\begin{noncompile}
The map $\phi\circ\pi:\left[  s\right]  \rightarrow S$ is bijective (since it
is the composition of the two bijective maps $\phi$ and $\pi$), and thus
injective and surjective.
\end{noncompile}

Let $i\in S$. We shall show that there exists a unique $p\in\left[  s\right]
$ such that $i=c_{\pi\left(  p\right)  }$.

If $g$ and $h$ are two elements $p\in\left[  s\right]  $ such that
$i=c_{\pi\left(  p\right)  }$, then $g=h$\ \ \ \ \footnote{\textit{Proof.} Let
$g$ and $h$ be two elements $p\in\left[  s\right]  $ such that $i=c_{\pi
\left(  p\right)  }$. We must prove that $g=h$.
\par
We know that $g$ is an element $p\in\left[  s\right]  $ such that
$i=c_{\pi\left(  p\right)  }$. In other words, $g$ is an element of $\left[
s\right]  $ and satisfies $i=c_{\pi\left(  g\right)  }$. We have $\pi\left(
g\right)  \in\left[  s\right]  $ (since $\pi$ is a map $\left[  s\right]
\rightarrow\left[  s\right]  $) and thus $\phi\left(  \pi\left(  g\right)
\right)  =c_{\pi\left(  g\right)  }$ (by the definition of $\phi$). Comparing
this with $i=c_{\pi\left(  g\right)  }$, we obtain $\phi\left(  \pi\left(
g\right)  \right)  =i$.
\par
We know that $h$ is an element $p\in\left[  s\right]  $ such that
$i=c_{\pi\left(  p\right)  }$. In other words, $h$ is an element of $\left[
s\right]  $ and satisfies $i=c_{\pi\left(  h\right)  }$. We have $\pi\left(
h\right)  \in\left[  s\right]  $ (since $\pi$ is a map $\left[  s\right]
\rightarrow\left[  s\right]  $) and thus $\phi\left(  \pi\left(  h\right)
\right)  =c_{\pi\left(  h\right)  }$ (by the definition of $\phi$). Comparing
this with $i=c_{\pi\left(  h\right)  }$, we obtain $\phi\left(  \pi\left(
h\right)  \right)  =i$.
\par
Now, $\phi\left(  \pi\left(  g\right)  \right)  =i=\phi\left(  \pi\left(
h\right)  \right)  $. This yields $\pi\left(  g\right)  =\pi\left(  h\right)
$ (since the map $\phi$ is injective). Therefore, $g=h$ (since the map $\pi$
is injective). Qed.}. In other words,
\begin{equation}
\text{there exists \textbf{at most one} }p\in\left[  s\right]  \text{ such
that }i=c_{\pi\left(  p\right)  }. \label{pf.lem.sol.exe.Ialbe.inclist.b.1}%
\end{equation}


On the other hand, $i\in S=\phi\left(  \left[  s\right]  \right)  $ (since the
map $\phi$ is surjective). In other words, there exists some $h\in\left[
s\right]  $ such that $i=\phi\left(  h\right)  $. Consider this $h$. We have
$h\in\left[  s\right]  =\pi\left(  \left[  s\right]  \right)  $ (since the map
$\pi$ is surjective). In other words, there exists some $g\in\left[  s\right]
$ such that $h=\pi\left(  g\right)  $. Consider this $g$.

Now, the definition of $\phi$ yields $\phi\left(  h\right)  =c_{h}$, so that
$c_{h}=\phi\left(  h\right)  =i$. Thus, $i=c_{h}=c_{\pi\left(  g\right)  }$
(since $h=\pi\left(  g\right)  $). Thus, $g$ is an element of $\left[
s\right]  $ and satisfies $i=c_{\pi\left(  g\right)  }$. In other words, $g$
is a $p\in\left[  s\right]  $ such that $i=c_{\pi\left(  p\right)  }$. Hence,
there exists \textbf{at least one} $p\in\left[  s\right]  $ such that
$i=c_{\pi\left(  p\right)  }$ (namely, $p=g$). Combining this with
(\ref{pf.lem.sol.exe.Ialbe.inclist.b.1}), we conclude that there exists a
unique $p\in\left[  s\right]  $ such that $i=c_{\pi\left(  p\right)  }$. In
other words, there exists a unique $p\in\left\{  1,2,\ldots,s\right\}  $ such
that $i=c_{\pi\left(  p\right)  }$ (since $\left[  s\right]  =\left\{
1,2,\ldots,s\right\}  $).

Now, forget that we fixed $i$. We thus have shown that%
\begin{equation}
\text{for every }i\in S\text{, there exists a unique }p\in\left\{
1,2,\ldots,s\right\}  \text{ such that }i=c_{\pi\left(  p\right)  }.
\label{pf.lem.sol.exe.Ialbe.inclist.b.2}%
\end{equation}
In other words, the list $\left(  c_{\pi\left(  1\right)  },c_{\pi\left(
2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ contains each element
of $S$ exactly once. Since $\left(  c_{\pi\left(  1\right)  },c_{\pi\left(
2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ is a list of elements
of $S$, this yields that\newline$\left(  c_{\pi\left(  1\right)  }%
,c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(  s\right)  }\right)  $ is a
list of all elements of $S$ (with no repetitions). This proves Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(b)}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exe.Ialbe.II}Let $I$ be a finite set of integers. Let
$k=\left\vert I\right\vert $. Then,%
\[
\sum_{x\in I}\sum_{y\in I}\left[  x>y\right]  =0+1+\cdots+\left(  k-1\right)
.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.II}.]Let $\left(  i_{1},i_{2}%
,\ldots,i_{k}\right)  $ be the list of all elements of $I$ in increasing order
(with no repetitions). (Such a list exists, since $\left\vert I\right\vert
=k$.) Then, Lemma \ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to
$I$, $k$ and $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ instead of $S$, $s$
and $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[
k\right]  \rightarrow I,\ h\mapsto i_{h}$ is well-defined and a bijection.

We have $i_{1}<i_{2}<\cdots<i_{k}$ (because of how $\left(  i_{1},i_{2}%
,\ldots,i_{k}\right)  $ was defined). Hence, if $u$ and $v$ are two elements
of $\left[  k\right]  $, then $i_{u}>i_{v}$ holds if and only if $u>v$. In
other words, if $u$ and $v$ are two elements of $\left[  k\right]  $, then%
\begin{equation}
\left[  i_{u}>i_{v}\right]  =\left[  u>v\right]  .
\label{pf.lem.sol.exe.Ialbe.II.short.3}%
\end{equation}


Now, every $x\in I$ satisfies $\sum_{y\in I}\left[  x>y\right]  =\sum
_{v\in\left[  k\right]  }\left[  x>i_{v}\right]  $ (here, we have substituted
$i_{v}$ for $y$ in the sum, since the map $\left[  k\right]  \rightarrow
I,\ h\mapsto i_{h}$ is a bijection). Thus,%
\begin{align}
\sum_{x\in I}\underbrace{\sum_{y\in I}\left[  x>y\right]  }_{=\sum
_{v\in\left[  k\right]  }\left[  x>i_{v}\right]  }  &  =\sum_{x\in I}%
\sum_{v\in\left[  k\right]  }\left[  x>i_{v}\right] \nonumber\\
&  =\sum_{u\in\left[  k\right]  }\sum_{v\in\left[  k\right]  }%
\underbrace{\left[  i_{u}>i_{v}\right]  }_{\substack{=\left[  u>v\right]
\\\text{(by (\ref{pf.lem.sol.exe.Ialbe.II.short.3}))}}%
}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }i_{u}\\
\text{for }x\text{ in the sum, since the}\\
\text{map }\left[  k\right]  \rightarrow I,\ h\mapsto i_{h}\text{ is a
bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{u\in\left[  k\right]  }\sum_{v\in\left[  k\right]  }\left[
u>v\right]  . \label{pf.lem.sol.exe.Ialbe.II.short.4}%
\end{align}
But every $u\in\left[  k\right]  $ satisfies%
\begin{equation}
\sum_{v\in\left[  k\right]  }\left[  u>v\right]  =u-1
\label{pf.lem.sol.exe.Ialbe.II.short.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.II.short.5}):} Let
$u\in\left[  k\right]  $. Then,%
\begin{align*}
\underbrace{\sum_{v\in\left[  k\right]  }}_{=\sum_{v=1}^{k}}\left[
u>v\right]   &  =\sum_{v=1}^{k}\left[  u>v\right]  =\sum_{v=1}^{u-1}%
\underbrace{\left[  u>v\right]  }_{\substack{=1\\\text{(since }%
u>v\\\text{(since }v\leq u-1<u\text{))}}}+\sum_{v=u}^{k}\underbrace{\left[
u>v\right]  }_{\substack{=0\\\text{(since we don't have }u>v\\\text{(since
}v\geq u\text{))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq u\leq
k\right) \\
&  =\sum_{v=1}^{u-1}1+\underbrace{\sum_{v=u}^{k}0}_{=0}=\sum_{v=1}%
^{u-1}1=\left(  u-1\right)  1=u-1.
\end{align*}
This proves (\ref{pf.lem.sol.exe.Ialbe.II.short.5}).}.

Now, (\ref{pf.lem.sol.exe.Ialbe.II.short.4}) becomes%
\begin{align*}
\sum_{x\in I}\sum_{y\in I}\left[  x>y\right]   &  =\underbrace{\sum
_{u\in\left[  k\right]  }}_{=\sum_{u=1}^{k}}\underbrace{\sum_{v\in\left[
k\right]  }\left[  u>v\right]  }_{\substack{=u-1\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.II.short.5}))}}}=\sum_{u=1}^{k}\left(  u-1\right)
\\
&  =\sum_{u=0}^{k-1}u\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have
substituted }u\text{ for }u-1\text{ in the sum}\right) \\
&  =0+1+\cdots+\left(  k-1\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.II}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.II}.]We have $\left[  k\right]
=\left\{  1,2,\ldots,k\right\}  $ (by the definition of $\left[  k\right]  $).

Let $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ be the list of all elements of
$I$ in increasing order (with no repetitions). (Such a list exists, since
$\left\vert I\right\vert =k$.) Then, Lemma \ref{lem.sol.exe.Ialbe.inclist}
\textbf{(a)} (applied to $I$, $k$ and $\left(  i_{1},i_{2},\ldots
,i_{k}\right)  $ instead of $S$, $s$ and $\left(  c_{1},c_{2},\ldots
,c_{s}\right)  $) shows that the map $\left[  k\right]  \rightarrow
I,\ h\mapsto i_{h}$ is well-defined and a bijection.

We know that $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ is the list of all
elements of $I$ in increasing order (with no repetitions). Thus, $\left(
i_{1},i_{2},\ldots,i_{k}\right)  $ is a strictly increasing list. In other
words, we have $i_{1}<i_{2}<\cdots<i_{k}$. In other words, if $u$ and $v$ are
two elements of $\left[  k\right]  $ such that $u<v$, then%
\begin{equation}
i_{u}<i_{v}. \label{pf.lem.sol.exe.Ialbe.II.1}%
\end{equation}
Moreover, if $u$ and $v$ are two elements of $\left[  k\right]  $ such that
$u\leq v$, then%
\begin{equation}
i_{u}\leq i_{v} \label{pf.lem.sol.exe.Ialbe.II.2}%
\end{equation}
(since $i_{1}<i_{2}<\cdots<i_{k}$).

Now, every two elements $u$ and $v$ of $\left[  k\right]  $ satisfy
\begin{equation}
\left[  i_{u}>i_{v}\right]  =\left[  u>v\right]
\label{pf.lem.sol.exe.Ialbe.II.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.II.3}):} Let $u$ and $v$
be two elements of $\left[  k\right]  $. We are in one of the following two
cases:
\par
\textit{Case 1:} We have $u>v$.
\par
\textit{Case 2:} We have $u\leq v$.
\par
Let us first consider Case 1. In this case, we have $u>v$. Hence, $v<u$. Thus,
(\ref{pf.lem.sol.exe.Ialbe.II.1}) (applied to $v$ and $u$ instead of $u$ and
$v$) yields $i_{v}<i_{u}$. In other words, $i_{u}>i_{v}$. Hence, $\left[
i_{u}>i_{v}\right]  =1$. But $u>v$, and thus $\left[  u>v\right]  =1$.
Comparing this with $\left[  i_{u}>i_{v}\right]  =1$, we obtain $\left[
i_{u}>i_{v}\right]  =\left[  u>v\right]  $. Thus,
(\ref{pf.lem.sol.exe.Ialbe.II.3}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $u\leq v$. Hence, $i_{u}\leq
i_{v}$ (by (\ref{pf.lem.sol.exe.Ialbe.II.2})). In other words, we don't have
$i_{u}>i_{v}$. Hence, $\left[  i_{u}>i_{v}\right]  =0$. But $u\leq v$. In
other words, we don't have $u>v$. Hence, $\left[  u>v\right]  =0$. Comparing
this with $\left[  i_{u}>i_{v}\right]  =0$, we obtain $\left[  i_{u}%
>i_{v}\right]  =\left[  u>v\right]  $. Thus, (\ref{pf.lem.sol.exe.Ialbe.II.3})
is proven in Case 2.
\par
We thus have proven (\ref{pf.lem.sol.exe.Ialbe.II.3}) in each of the two Cases
1 and 2. Since these two Cases cover all possibilities, this shows that
(\ref{pf.lem.sol.exe.Ialbe.II.3}) always holds. Qed.}.

Now, every $x\in I$ satisfies%
\begin{align*}
\sum_{y\in I}\left[  x>y\right]   &  =\sum_{h\in\left[  k\right]  }\left[
x>i_{h}\right]  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }i_{h}\text{ for }y\text{ in the sum,}\\
\text{since the map }\left[  k\right]  \rightarrow I,\ h\mapsto i_{h}\text{ is
a bijection}%
\end{array}
\right) \\
&  =\sum_{v\in\left[  k\right]  }\left[  x>i_{v}\right]
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}h\text{ as }v\right)  .
\end{align*}
Thus,%
\begin{align}
&  \sum_{x\in I}\underbrace{\sum_{y\in I}\left[  x>y\right]  }_{=\sum
_{v\in\left[  k\right]  }\left[  x>i_{v}\right]  }\nonumber\\
&  =\sum_{x\in I}\sum_{v\in\left[  k\right]  }\left[  x>i_{v}\right]
\nonumber\\
&  =\sum_{h\in\left[  k\right]  }\sum_{v\in\left[  k\right]  }\left[
i_{h}>i_{v}\right]  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }i_{h}\text{ for }x\text{ in the outer sum,}\\
\text{since the map }\left[  k\right]  \rightarrow I,\ h\mapsto i_{h}\text{ is
a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{u\in\left[  k\right]  }\sum_{v\in\left[  k\right]  }%
\underbrace{\left[  i_{u}>i_{v}\right]  }_{\substack{=\left[  u>v\right]
\\\text{(by (\ref{pf.lem.sol.exe.Ialbe.II.3}))}}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the summation}\\
\text{index }h\text{ as }u\text{ in the outer sum}%
\end{array}
\right) \nonumber\\
&  =\sum_{u\in\left[  k\right]  }\sum_{v\in\left[  k\right]  }\left[
u>v\right]  . \label{pf.lem.sol.exe.Ialbe.II.4}%
\end{align}
But every $u\in\left[  k\right]  $ satisfies%
\begin{equation}
\sum_{v\in\left[  k\right]  }\left[  u>v\right]  =u-1
\label{pf.lem.sol.exe.Ialbe.II.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.II.5}):} Let
$u\in\left[  k\right]  $. Then, $u\in\left[  k\right]  =\left\{
1,2,\ldots,k\right\}  $ (by the definition of $\left[  k\right]  $). Hence,
$1\leq u\leq k$. Now,%
\begin{align*}
\underbrace{\sum_{v\in\left[  k\right]  }}_{\substack{=\sum_{v\in\left\{
1,2,\ldots,k\right\}  }\\\text{(since }\left[  k\right]  =\left\{
1,2,\ldots,k\right\}  \text{)}}}\left[  u>v\right]   &  =\underbrace{\sum
_{v\in\left\{  1,2,\ldots,k\right\}  }}_{=\sum_{v=1}^{k}}\left[  u>v\right]
=\sum_{v=1}^{k}\left[  u>v\right] \\
&  =\sum_{v=1}^{u-1}\underbrace{\left[  u>v\right]  }%
_{\substack{=1\\\text{(since }u>v\\\text{(since }v\leq u-1<u\text{))}}%
}+\sum_{v=u}^{k}\underbrace{\left[  u>v\right]  }_{\substack{=0\\\text{(since
we don't have }u>v\\\text{(since }v\geq u\text{))}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq u\leq k\right) \\
&  =\sum_{v=1}^{u-1}1+\underbrace{\sum_{v=u}^{k}0}_{=0}=\sum_{v=1}%
^{u-1}1=\left(  u-1\right)  1=u-1.
\end{align*}
This proves (\ref{pf.lem.sol.exe.Ialbe.II.5}).}.

Now, (\ref{pf.lem.sol.exe.Ialbe.II.4}) becomes%
\begin{align*}
\sum_{x\in I}\sum_{y\in I}\left[  x>y\right]   &  =\underbrace{\sum
_{u\in\left[  k\right]  }}_{\substack{=\sum_{u\in\left\{  1,2,\ldots
,k\right\}  }\\\text{(since }\left[  k\right]  =\left\{  1,2,\ldots,k\right\}
\text{)}}}\underbrace{\sum_{v\in\left[  k\right]  }\left[  u>v\right]
}_{\substack{=u-1\\\text{(by (\ref{pf.lem.sol.exe.Ialbe.II.5}))}%
}}=\underbrace{\sum_{u\in\left\{  1,2,\ldots,k\right\}  }}_{=\sum_{u=1}^{k}%
}\left(  u-1\right) \\
&  =\sum_{u=1}^{k}\left(  u-1\right) \\
&  =\sum_{u=0}^{k-1}u\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have
substituted }u\text{ for }u-1\text{ in the sum}\right) \\
&  =0+1+\cdots+\left(  k-1\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.II}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exe.Ialbe.inclist2}Let $S$ be a finite set of integers. Let
$\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ be a list of all elements of $S$
(with no repetitions).

Let $p_{1},p_{2},\ldots,p_{s}$ be $s$ pairwise distinct elements of $S$. Then,
there exists a $\pi\in S_{s}$ such that $\left(  p_{1},p_{2},\ldots
,p_{s}\right)  =\left(  c_{\pi\left(  1\right)  },c_{\pi\left(  2\right)
},\ldots,c_{\pi\left(  s\right)  }\right)  $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.inclist2}.]We know that $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $ is a list of all elements of $S$. Hence,
$\left\{  c_{1},c_{2},\ldots,c_{s}\right\}  =S$.

The elements $p_{1},p_{2},\ldots,p_{s}$ are pairwise distinct. In other words,
if $i$ and $j$ are two distinct elements of $\left\{  1,2,\ldots,s\right\}  $,
then%
\begin{equation}
p_{i}\neq p_{j}. \label{pf.lem.sol.exe.Ialbe.inclist2.1}%
\end{equation}


For every $i\in\left\{  1,2,\ldots,s\right\}  $, there exists an $h\in\left\{
1,2,\ldots,s\right\}  $ such that $p_{i}=c_{h}$%
\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,s\right\}  $.
Then, $p_{i}\in S$ (since $p_{1},p_{2},\ldots,p_{s}$ are $s$ elements of $S$).
Thus, $p_{i}\in S=\left\{  c_{1},c_{2},\ldots,c_{s}\right\}  $. In other
words, there exists an $h\in\left\{  1,2,\ldots,s\right\}  $ such that
$p_{i}=c_{h}$. Qed.}. Fix such an $h$, and denote it by $h_{i}$. Thus, for
every $i\in\left\{  1,2,\ldots,s\right\}  $, we have defined an $h_{i}%
\in\left\{  1,2,\ldots,s\right\}  $ such that%
\begin{equation}
p_{i}=c_{h_{i}}. \label{pf.lem.sol.exe.Ialbe.inclist2.2}%
\end{equation}


Define a map $\varphi:\left\{  1,2,\ldots,s\right\}  \rightarrow\left\{
1,2,\ldots,s\right\}  $ by%
\[
\left(  \varphi\left(  i\right)  =h_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,\ldots,s\right\}  \right)  .
\]
Then, the map $\varphi$ is injective\footnote{\textit{Proof.} Let $i$ and $j$
be two elements of $\left\{  1,2,\ldots,s\right\}  $ such that $\varphi\left(
i\right)  =\varphi\left(  j\right)  $. We shall prove that $i=j$.
\par
The definition of $\varphi$ yields $\varphi\left(  i\right)  =h_{i}$ and
$\varphi\left(  j\right)  =h_{j}$. But (\ref{pf.lem.sol.exe.Ialbe.inclist2.2})
yields $p_{i}=c_{h_{i}}$. Also, (\ref{pf.lem.sol.exe.Ialbe.inclist2.2})
(applied to $j$ instead of $i$) yields $p_{j}=c_{h_{j}}$. Now, $p_{i}%
=c_{h_{i}}=c_{h_{j}}$ (since $h_{i}=\varphi\left(  i\right)  =\varphi\left(
j\right)  =h_{j}$). Comparing this with $p_{j}=c_{h_{j}}$, we obtain
$p_{i}=p_{j}$.
\par
If the elements $i$ and $j$ were distinct, then we would have $p_{i}\neq
p_{j}$ (by (\ref{pf.lem.sol.exe.Ialbe.inclist2.1})); but this would contradict
$p_{i}=p_{j}$. Hence, the elements $i$ and $j$ cannot be distinct. In other
words, we have $i=j$.
\par
Now, forget that we fixed $i$ and $j$. We thus have proven that if $i$ and $j$
are two elements of $\left\{  1,2,\ldots,s\right\}  $ such that $\varphi
\left(  i\right)  =\varphi\left(  j\right)  $, then $i=j$. In other words, the
map $\varphi$ is injective. Qed.} and therefore
surjective\footnote{\textit{Proof.} It is well-known that any injective map
from a finite set to itself is surjective. In other words, if $U$ is a finite
set, and if $f:U\rightarrow U$ is an injective map, then $f$ is surjective.
Applying this to $U=\left\{  1,2,\ldots,s\right\}  $ and $f=\varphi$, we
conclude that $\varphi$ is surjective (since $\left\{  1,2,\ldots,s\right\}  $
is a finite set, and since $\varphi$ is injective). Qed.}. Hence, the map
$\varphi$ is bijective (since $\varphi$ is injective and surjective). Thus,
$\varphi$ is a bijective map $\left\{  1,2,\ldots,s\right\}  \rightarrow
\left\{  1,2,\ldots,s\right\}  $. In other words, $\varphi$ is a permutation
of the set $\left\{  1,2,\ldots,s\right\}  $. In other words, $\varphi\in
S_{s}$ (since $S_{s}$ is the set of all permutations of the set $\left\{
1,2,\ldots,s\right\}  $). Furthermore, $\left(  p_{1},p_{2},\ldots
,p_{s}\right)  =\left(  c_{\varphi\left(  1\right)  },c_{\varphi\left(
2\right)  },\ldots,c_{\varphi\left(  s\right)  }\right)  $%
\ \ \ \ \footnote{\textit{Proof.} For every $i\in\left\{  1,2,\ldots
,s\right\}  $, we have $\varphi\left(  i\right)  =h_{i}$ (by the definition of
$\varphi$) and thus $c_{\varphi\left(  i\right)  }=c_{h_{i}}=p_{i}$ (by
(\ref{pf.lem.sol.exe.Ialbe.inclist2.2})). In other words, for every
$i\in\left\{  1,2,\ldots,s\right\}  $, we have $p_{i}=c_{\varphi\left(
i\right)  }$. In other words, we have $\left(  p_{1},p_{2},\ldots
,p_{s}\right)  =\left(  c_{\varphi\left(  1\right)  },c_{\varphi\left(
2\right)  },\ldots,c_{\varphi\left(  s\right)  }\right)  $. Qed.}. Hence,
$\varphi$ is an element of $S_{s}$ and satisfies $\left(  p_{1},p_{2}%
,\ldots,p_{s}\right)  =\left(  c_{\varphi\left(  1\right)  },c_{\varphi\left(
2\right)  },\ldots,c_{\varphi\left(  s\right)  }\right)  $. Thus, there exists
a $\pi\in S_{s}$ such that $\left(  p_{1},p_{2},\ldots,p_{s}\right)  =\left(
c_{\pi\left(  1\right)  },c_{\pi\left(  2\right)  },\ldots,c_{\pi\left(
s\right)  }\right)  $ (namely, $\pi=\varphi$). This proves Lemma
\ref{lem.sol.exe.Ialbe.inclist2}.
\end{proof}

\begin{lemma}
\label{lem.sol.exe.Ialbe.InotI}Let $n\in\mathbb{N}$. Let $I$ be a subset of
$\left[  n\right]  $. Let $k=\left\vert I\right\vert $.

Let $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ be a list of all elements of
$I$ (with no repetitions). Let $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $
be a list of all elements of $\left[  n\right]  \setminus I$ (with no repetitions).

\textbf{(a)} There exists a unique $\sigma\in S_{n}$ satisfying%
\[
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2}%
,\ldots,b_{n-k}\right)  .
\]


\textbf{(b)} Let $\sum I$ denote the sum of all elements of $I$. (Thus, $\sum
I=\sum_{i\in I}i$.) Then,%
\[
\sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[  n-k\right]
}\left[  a_{i}>b_{j}\right]  =\sum I-\left(  1+2+\cdots+k\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.InotI}.]\textbf{(a)} The $k$ elements
$a_{1},a_{2},\ldots,a_{k}$ belong to $I$ (since $\left(  a_{1},a_{2}%
,\ldots,a_{k}\right)  $ is a list of all elements of $I$), and thus belong to
$\left[  n\right]  $ as well (since $I\subseteq\left[  n\right]  $). Also, the
$k$ elements $a_{1},a_{2},\ldots,a_{k}$ are pairwise distinct (since $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ is a list with no repetitions).

The $n-k$ elements $b_{1},b_{2},\ldots,b_{n-k}$ belong to $\left[  n\right]
\setminus I$ (since $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list of
all elements of $\left[  n\right]  \setminus I$), and thus belong to $\left[
n\right]  $ as well (since $\left[  n\right]  \setminus I\subseteq\left[
n\right]  $). Also, the $n-k$ elements $b_{1},b_{2},\ldots,b_{n-k}$ are
pairwise distinct (since $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a
list with no repetitions).

The $n$ elements $a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}$ are
pairwise distinct\footnote{\textit{Proof.} Assume the contrary. Thus, two of
the $n$ elements $a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}$ are
equal. These two elements are either two of the elements $a_{1},a_{2}%
,\ldots,a_{k}$, or two of the elements $b_{1},b_{2},\ldots,b_{n-k}$, or one of
the former and one of the latter (in some order). Hence, we are in one of the
following three cases:
\par
\textit{Case 1:} Two of the $k$ elements $a_{1},a_{2},\ldots,a_{k}$ are equal.
\par
\textit{Case 2:} Two of the $n-k$ elements $b_{1},b_{2},\ldots,b_{n-k}$ are
equal.
\par
\textit{Case 3:} One of the $k$ elements $a_{1},a_{2},\ldots,a_{k}$ is equal
to one of the elements $b_{1},b_{2},\ldots,b_{n-k}$.
\par
But Case 1 is impossible (since the $k$ elements $a_{1},a_{2},\ldots,a_{k}$
are pairwise distinct), and Case 2 is also impossible (since the $n-k$
elements $b_{1},b_{2},\ldots,b_{n-k}$ are pairwise distinct). Hence, we must
be in Case 3. In other words, one of the $k$ elements $a_{1},a_{2}%
,\ldots,a_{k}$ is equal to one of the elements $b_{1},b_{2},\ldots,b_{n-k}$.
In other words, we must have $a_{i}=b_{j}$ for some $i\in\left\{
1,2,\ldots,k\right\}  $ and some $j\in\left\{  1,2,\ldots,n-k\right\}  $.
Consider these $i$ and $j$. We have $a_{i}\in I$ (since the $k$ elements
$a_{1},a_{2},\ldots,a_{k}$ belong to $I$). But $b_{j}\in\left[  n\right]
\setminus I$ (since the $n-k$ elements $b_{1},b_{2},\ldots,b_{n-k}$ belong to
$\left[  n\right]  \setminus I$) and thus $b_{j}\notin I$. Hence, $a_{i}%
=b_{j}\notin I$; but this contradicts $a_{i}\in I$. This contradiction proves
that our assumption was wrong; qed.} and belong to $\left[  n\right]
$\ \ \ \ \footnote{This is because the $k$ elements $a_{1},a_{2},\ldots,a_{k}$
belong to $\left[  n\right]  $, and because the $n-k$ elements $b_{1}%
,b_{2},\ldots,b_{n-k}$ belong to $\left[  n\right]  $.}. Furthermore, $\left(
1,2,\ldots,n\right)  $ is a list of all elements of $\left[  n\right]  $ (with
no repetition). Hence, Lemma \ref{lem.sol.exe.Ialbe.inclist2} (applied to
$S=\left[  n\right]  $, $s=n$, $\left(  c_{1},c_{2},\ldots,c_{s}\right)
=\left(  1,2,\ldots,n\right)  $ and $\left(  p_{1},p_{2},\ldots,p_{s}\right)
=\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  $) yields
that there exists a $\pi\in S_{n}$ such that
\[
\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  =\left(
\pi\left(  1\right)  ,\pi\left(  2\right)  ,\ldots,\pi\left(  n\right)
\right)  .
\]
Consider this $\pi$.

We have $\pi\in S_{n}$ and $\left(  \pi\left(  1\right)  ,\pi\left(  2\right)
,\ldots,\pi\left(  n\right)  \right)  =\left(  a_{1},a_{2},\ldots,a_{k}%
,b_{1},b_{2},\ldots,b_{n-k}\right)  $.

But our goal is to show that there exists a unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2}%
,\ldots,b_{n-k}\right)  . \label{pf.lem.sol.exe.Ialbe.InotI.short.a.1}%
\end{equation}
We already know that there exists \textbf{at least one} such $\sigma$ (namely,
$\sigma=\pi$). Hence, it remains to show that there exists \textbf{at most
one} such $\sigma$. In other words, it remains to show that if $\sigma_{1}$
and $\sigma_{2}$ are two elements $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.short.a.1}), then $\sigma_{1}=\sigma_{2}$.

So let $\sigma_{1}$ and $\sigma_{2}$ be two elements $\sigma\in S_{n}$
satisfying (\ref{pf.lem.sol.exe.Ialbe.InotI.short.a.1}). We must prove that
$\sigma_{1}=\sigma_{2}$.

We know that $\sigma_{1}$ is an element $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.short.a.1}). In other words, $\sigma_{1}$ is
an element of $S_{n}$ and satisfies%
\begin{equation}
\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  =\left(  a_{1},a_{2},\ldots
,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  .
\label{pf.lem.sol.exe.Ialbe.InotI.short.a.3a}%
\end{equation}
Similarly, $\sigma_{2}$ is an element of $S_{n}$ and satisfies%
\begin{equation}
\left(  \sigma_{2}\left(  1\right)  ,\sigma_{2}\left(  2\right)
,\ldots,\sigma_{2}\left(  n\right)  \right)  =\left(  a_{1},a_{2},\ldots
,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  .
\label{pf.lem.sol.exe.Ialbe.InotI.short.a.3b}%
\end{equation}
Comparing (\ref{pf.lem.sol.exe.Ialbe.InotI.short.a.3a}) with
(\ref{pf.lem.sol.exe.Ialbe.InotI.short.a.3b}), we obtain \newline$\left(
\sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)  ,\ldots,\sigma
_{1}\left(  n\right)  \right)  =\left(  \sigma_{2}\left(  1\right)
,\sigma_{2}\left(  2\right)  ,\ldots,\sigma_{2}\left(  n\right)  \right)  $.
In other words, $\sigma_{1}\left(  i\right)  =\sigma_{2}\left(  i\right)  $
for every $i\in\left[  n\right]  $. Since both $\sigma_{1}$ and $\sigma_{2}$
are maps $\left[  n\right]  \rightarrow\left[  n\right]  $, this shows that
$\sigma_{1}=\sigma_{2}$. This is precisely what we wanted to show.

Thus, we have proven that there exists \textbf{at most one} such $\sigma$
satisfying (\ref{pf.lem.sol.exe.Ialbe.InotI.short.a.1}). As explained above,
this completes the proof of Lemma \ref{lem.sol.exe.Ialbe.InotI} \textbf{(a)}.

\textbf{(b)} Clearly,
\begin{equation}
\sum I=\sum_{i\in I}i=\sum_{x\in I}x
\label{pf.lem.sol.exe.Ialbe.InotI.short.b.triv}%
\end{equation}
(here, we have renamed the summation index $i$ as $x$).

We know that $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is a list of all
elements of $I$ (with no repetitions). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $I$, $k$ and $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ instead of $S$, $s$ and $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[  k\right]
\rightarrow I,\ h\mapsto a_{h}$ is well-defined and a bijection.

We know that $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list of all
elements of $\left[  n\right]  \setminus I$ (with no repetitions). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $\left[  n\right]
\setminus I$, $n-k$ and $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ instead
of $S$, $s$ and $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the
map $\left[  n-k\right]  \rightarrow\left[  n\right]  \setminus I,\ h\mapsto
b_{h}$ is well-defined and a bijection.

Now,%
\begin{align}
\underbrace{\sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[
n-k\right]  }}_{=\sum_{i\in\left[  k\right]  }\sum_{j\in\left[  n-k\right]  }%
}\left[  a_{i}>b_{j}\right]   &  =\sum_{i\in\left[  k\right]  }%
\underbrace{\sum_{j\in\left[  n-k\right]  }\left[  a_{i}>b_{j}\right]
}_{\substack{=\sum_{y\in\left[  n\right]  \setminus I}\left[  a_{i}>y\right]
\\\text{(here, we have substituted }y\text{ for }b_{j}\\\text{in the sum,
since the}\\\text{map }\left[  n-k\right]  \rightarrow\left[  n\right]
\setminus I,\ h\mapsto b_{h}\text{ is a bijection)}}}=\sum_{i\in\left[
k\right]  }\sum_{y\in\left[  n\right]  \setminus I}\left[  a_{i}>y\right]
\nonumber\\
&  =\sum_{x\in I}\sum_{y\in\left[  n\right]  \setminus I}\left[  x>y\right]
\label{pf.lem.sol.exe.Ialbe.InotI.short.b.1}%
\end{align}
(here, we have substituted $x$ for $a_{i}$ in the outer sum, since the map
$\left[  k\right]  \rightarrow I,\ h\mapsto a_{h}$ is a bijection). But every
$x\in I$ satisfies%
\begin{equation}
\sum_{y\in\left[  n\right]  \setminus I}\left[  x>y\right]  =x-1-\sum_{y\in
I}\left[  x>y\right]  \label{pf.lem.sol.exe.Ialbe.InotI.short.b.sum1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.InotI.short.b.sum1}):}
Let $x\in I$. Then, $x\in I\subseteq\left[  n\right]  $, so that $1\leq x\leq
n$.
\par
Every $y\in\left[  n\right]  $ satisfies either $y\in I$ or $y\notin I$ (but
not both). Hence,%
\[
\sum_{y\in\left[  n\right]  }\left[  x>y\right]  =\underbrace{\sum
_{\substack{y\in\left[  n\right]  ;\\y\in I}}}_{\substack{=\sum_{y\in
I}\\\text{(since }I\subseteq\left[  n\right]  \text{)}}}\left[  x>y\right]
+\underbrace{\sum_{\substack{y\in\left[  n\right]  ;\\y\notin I}}}%
_{=\sum_{y\in\left[  n\right]  \setminus I}}\left[  x>y\right]  =\sum_{y\in
I}\left[  x>y\right]  +\sum_{y\in\left[  n\right]  \setminus I}\left[
x>y\right]  .
\]
Comparing this with%
\begin{align*}
\underbrace{\sum_{y\in\left[  n\right]  }}_{=\sum_{y=1}^{n}}\left[
x>y\right]   &  =\sum_{y=1}^{n}\left[  x>y\right] \\
&  =\sum_{y=1}^{x-1}\underbrace{\left[  x>y\right]  }%
_{\substack{=1\\\text{(since }x>y\\\text{(since }y\leq x-1<x\text{))}}%
}+\sum_{y=x}^{n}\underbrace{\left[  x>y\right]  }_{\substack{=0\\\text{(since
we don't have }x>y\\\text{(since }y\geq x\text{))}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq x\leq n\right) \\
&  =\sum_{y=1}^{x-1}1+\underbrace{\sum_{y=x}^{n}0}_{=0}=\sum_{y=1}%
^{x-1}1=\left(  x-1\right)  1=x-1,
\end{align*}
we obtain%
\[
\sum_{y\in I}\left[  x>y\right]  +\sum_{y\in\left[  n\right]  \setminus
I}\left[  x>y\right]  =x-1.
\]
Subtracting $\sum_{y\in I}\left[  x>y\right]  $ from both sides of this
equality, we obtain $\sum_{y\in\left[  n\right]  \setminus I}\left[
x>y\right]  =x-1-\sum_{y\in I}\left[  x>y\right]  $. This proves
(\ref{pf.lem.sol.exe.Ialbe.InotI.short.b.sum1}).}.

Now, (\ref{pf.lem.sol.exe.Ialbe.InotI.short.b.1}) becomes%
\begin{align*}
&  \sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[  n-k\right]
}\left[  a_{i}>b_{j}\right] \\
&  =\sum_{x\in I}\underbrace{\sum_{y\in\left[  n\right]  \setminus I}\left[
x>y\right]  }_{\substack{=x-1-\sum_{y\in I}\left[  x>y\right]  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.InotI.short.b.sum1}))}}}=\sum_{x\in I}\left(
x-1-\sum_{y\in I}\left[  x>y\right]  \right) \\
&  =\underbrace{\sum_{x\in I}x}_{\substack{=\sum I\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.InotI.short.b.triv}))}}}-\underbrace{\sum_{x\in
I}1}_{=\left\vert I\right\vert \cdot1=\left\vert I\right\vert =k}%
-\underbrace{\sum_{x\in I}\sum_{y\in I}\left[  x>y\right]  }%
_{\substack{=0+1+\cdots+\left(  k-1\right)  \\\text{(by Lemma
\ref{lem.sol.exe.Ialbe.II})}}}=\sum I-k-\left(  0+1+\cdots+\left(  k-1\right)
\right) \\
&  =\sum I-\underbrace{\left(  \left(  0+1+\cdots+\left(  k-1\right)  \right)
+k\right)  }_{=0+1+\cdots+k=1+2+\cdots+k}=\sum I-\left(  1+2+\cdots+k\right)
.
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.InotI} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exe.Ialbe.InotI}.]\textbf{(a)} The list $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ is a list of all elements of $I$, and thus
belongs to $I^{k}$. Hence, $\left(  a_{1},a_{2},\ldots,a_{k}\right)  \in
I^{k}\subseteq\left[  n\right]  ^{k}$ (since $I\subseteq\left[  n\right]  $).

The list $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list of all
elements of $\left[  n\right]  \setminus I$, and thus belongs to $\left(
\left[  n\right]  \setminus I\right)  ^{n-k}$. Hence, $\left(  b_{1}%
,b_{2},\ldots,b_{n-k}\right)  \in\left(  \left[  n\right]  \setminus I\right)
^{n-k}\subseteq\left[  n\right]  ^{n-k}$ (since $\left[  n\right]  \setminus
I\subseteq\left[  n\right]  $).

From $\left(  a_{1},a_{2},\ldots,a_{k}\right)  \in\left[  n\right]  ^{k}$ and
$\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  \in\left[  n\right]  ^{n-k}$, we
obtain
\[
\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  \in\left[
n\right]  ^{k+\left(  n-k\right)  }=\left[  n\right]  ^{n}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k+\left(  n-k\right)  =n\right)  .
\]
In other words, $\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots
,b_{n-k}\right)  $ is an $n$-tuple of elements of $\left[  n\right]  $. Denote
this $n$-tuple by $\left(  c_{1},c_{2},\ldots,c_{n}\right)  $. Thus,%
\[
\left(  c_{1},c_{2},\ldots,c_{n}\right)  =\left(  a_{1},a_{2},\ldots
,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  .
\]
In other words,%
\[
c_{h}=%
\begin{cases}
a_{h}, & \text{if }h\leq k;\\
b_{h-k}, & \text{if }h>k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\left\{  1,2,\ldots,n\right\}  .
\]
In other words,%
\begin{equation}
c_{h}=%
\begin{cases}
a_{h}, & \text{if }h\leq k;\\
b_{h-k}, & \text{if }h>k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\left[  n\right]
\label{pf.lem.sol.exe.Ialbe.InotI.a.ch=}%
\end{equation}
(since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $). Notice that%
\begin{equation}
c_{h}\notin I\ \ \ \ \ \ \ \ \ \ \text{for every }h\in\left[  n\right]  \text{
satisfying }h>k \label{pf.lem.sol.exe.Ialbe.InotI.a.notinI}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.InotI.a.notinI}):} Let
$h\in\left[  n\right]  $ be such that $h>k$. Then,%
\begin{align*}
c_{h}  &  =%
\begin{cases}
a_{h}, & \text{if }h\leq k;\\
b_{h-k}, & \text{if }h>k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.sol.exe.Ialbe.InotI.a.ch=}%
)}\right) \\
&  =b_{h-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }h>k\right) \\
&  \in\left[  n\right]  \setminus I\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  \in\left(  \left[  n\right]
\setminus I\right)  ^{n-k}\right)  .
\end{align*}
In other words, $c_{h}\in\left[  n\right]  $ and $c_{h}\notin I$. This proves
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.notinI}).}.

It is well-known that if $U$ is a set, and if $V$ is a subset of $U$, then
$V\cup\left(  U\setminus V\right)  =U$. Applying this to $U=\left[  n\right]
$ and $V=I$, we obtain $I\cup\left(  \left[  n\right]  \setminus I\right)
=\left[  n\right]  $ (since $I$ is a subset of $\left[  n\right]  $).

Now, define a map $f:\left[  n\right]  \rightarrow\left[  n\right]  $ by%
\[
\left(  f\left(  h\right)  =c_{h}\ \ \ \ \ \ \ \ \ \ \text{for every }%
h\in\left[  n\right]  \right)  .
\]
(This is well-defined, because $c_{h}$ is a well-defined element of $\left[
n\right]  $ for every $h\in\left[  n\right]  $\ \ \ \ \footnote{Indeed, this
follows from (\ref{pf.lem.sol.exe.Ialbe.InotI.a.ch=}).}.) We have%
\begin{align*}
\left(  f\left(  1\right)  ,f\left(  2\right)  ,\ldots,f\left(  n\right)
\right)   &  =\left(  c_{1},c_{2},\ldots,c_{n}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }f\left(  h\right)  =c_{h}\text{ for
every }h\in\left[  n\right]  \right) \\
&  =\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)
\end{align*}
and thus%
\begin{align*}
\left\{  f\left(  1\right)  ,f\left(  2\right)  ,\ldots,f\left(  n\right)
\right\}   &  =\left\{  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots
,b_{n-k}\right\} \\
&  =\underbrace{\left\{  a_{1},a_{2},\ldots,a_{k}\right\}  }%
_{\substack{=I\\\text{(since }\left(  a_{1},a_{2},\ldots,a_{k}\right)  \text{
is a list}\\\text{of all elements of }I\text{)}}}\cup\underbrace{\left\{
b_{1},b_{2},\ldots,b_{n-k}\right\}  }_{\substack{=\left[  n\right]  \setminus
I\\\text{(since }\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  \text{ is a
list}\\\text{of all elements of }\left[  n\right]  \setminus I\text{)}}}\\
&  =I\cup\left(  \left[  n\right]  \setminus I\right)  =\left[  n\right]  .
\end{align*}
Now,%
\[
f\left(  \underbrace{\left[  n\right]  }_{=\left\{  1,2,\ldots,n\right\}
}\right)  =f\left(  \left\{  1,2,\ldots,n\right\}  \right)  =\left\{  f\left(
1\right)  ,f\left(  2\right)  ,\ldots,f\left(  n\right)  \right\}  =\left[
n\right]  ,
\]
so that $\left[  n\right]  \subseteq\left[  n\right]  =f\left(  \left[
n\right]  \right)  $. In other words, the map $f:\left[  n\right]
\rightarrow\left[  n\right]  $ is surjective. From this, it is easy to
conclude that this map $f$ is injective\footnote{\textit{First proof.} It is
well-known that any surjective map from a finite set to itself is injective.
In other words, if $S$ is a finite set, and if $g:S\rightarrow S$ is a
surjective map, then $g$ is injective. Applying this to $S=\left[  n\right]  $
and $g=f$, we conclude that $f$ is injective (since $\left[  n\right]  $ is a
finite set, and since $f$ is surjective). Qed.
\par
\textit{Second proof.} Let us give a different proof of the injectivity of
$f$, which does not use the finiteness of $\left[  n\right]  $.
\par
Let $g$ and $h$ be two elements of $\left[  n\right]  $ such that $f\left(
g\right)  =f\left(  h\right)  $. We shall prove that $g=h$.
\par
We can WLOG assume that $g\leq h$ (since otherwise, we can simply switch $g$
with $h$). Assume this.
\par
The definition of $f$ yields $f\left(  g\right)  =c_{g}$ and $f\left(
h\right)  =c_{h}$. Thus, $c_{g}=f\left(  g\right)  =f\left(  h\right)  =c_{h}%
$.
\par
For the sake of contradiction, let us assume that $g\neq h$. Combining this
with $g\leq h$, we obtain $g<h$. We are in one of the following two cases:
\par
\textit{Case 1:} We have $g\leq k$.
\par
\textit{Case 2:} We have $g>k$.
\par
Let us first consider Case 1. In this case, we have $g\leq k$. Thus,
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.ch=}) (applied to $g$ instead of $h$)
yields $c_{g}=%
\begin{cases}
a_{g}, & \text{if }g\leq k;\\
b_{g-k}, & \text{if }g>k
\end{cases}
=a_{g}$ (since $g\leq k$). Thus, $c_{g}=a_{g}\in I$ (since $\left(
a_{1},a_{2},\ldots,a_{k}\right)  \in I^{k}$). If we had $h>k$, then we would
have%
\[
c_{g}=c_{h}\notin I\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.notinI})}\right)  ,
\]
which would contradict $c_{g}\in I$. Thus, we cannot have $h>k$. Hence, we
must have $h\leq k$. Thus, (\ref{pf.lem.sol.exe.Ialbe.InotI.a.ch=}) yields
$c_{h}=%
\begin{cases}
a_{h}, & \text{if }h\leq k;\\
b_{h-k}, & \text{if }h>k
\end{cases}
=a_{h}$ (since $h\leq k$). Now, $c_{g}=a_{g}$, so that $a_{g}=c_{g}%
=c_{h}=a_{h}$.
\par
From $g\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $, we obtain
$g\geq1$. Combining this with $g\leq k$, we obtain $g\in\left\{
1,2,\ldots,k\right\}  $.
\par
From $h\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $, we obtain
$h\geq1$. Combining this with $h\leq k$, we obtain $h\in\left\{
1,2,\ldots,k\right\}  $.
\par
Now, $g$ and $h$ are two elements of $\left\{  1,2,\ldots,k\right\}  $ (since
$g\in\left\{  1,2,\ldots,k\right\}  $ and $h\in\left\{  1,2,\ldots,k\right\}
$). These two elements are distinct (since $g<h$).
\par
But $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is a list of all elements of
$I$ (with no repetitions). Thus, the list $\left(  a_{1},a_{2},\ldots
,a_{k}\right)  $ contains no repetitions. In other words, the elements
$a_{1},a_{2},\ldots,a_{k}$ are pairwise distinct. In other words, if $u$ and
$v$ are two distinct elements of $\left\{  1,2,\ldots,k\right\}  $, then
$a_{u}\neq a_{v}$. Applying this to $u=g$ and $v=h$, we obtain $a_{g}\neq
a_{h}$. This contradicts $a_{g}=a_{h}$. Hence, we have obtained a
contradiction in Case 1.
\par
Let us now consider Case 2. In this case, we have $g>k$. Hence, $g\geq k+1$
(since $g$ and $k$ are integers). But (\ref{pf.lem.sol.exe.Ialbe.InotI.a.ch=})
(applied to $g$ instead of $h$) yields $c_{g}=%
\begin{cases}
a_{g}, & \text{if }g\leq k;\\
b_{g-k}, & \text{if }g>k
\end{cases}
=b_{g-k}$ (since $g>k$). On the other hand, $g\leq h$, so that $h\geq g\geq
k+1>k$. Thus, (\ref{pf.lem.sol.exe.Ialbe.InotI.a.ch=}) yields $c_{h}=%
\begin{cases}
a_{h}, & \text{if }h\leq k;\\
b_{h-k}, & \text{if }h>k
\end{cases}
=b_{h-k}$ (since $h>k$). Now, $c_{g}=b_{g-k}$, so that $b_{g-k}=c_{g}%
=c_{h}=b_{h-k}$.
\par
From $g\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $, we obtain
$g\leq n$. Combining this with $g\geq k+1$, we obtain $g\in\left\{
k+1,k+2,\ldots,n\right\}  $. Hence, $g-k\in\left\{  1,2,\ldots,n-k\right\}  $.
\par
From $h\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $, we obtain
$h\leq n$. Combining this with $h\geq k+1$, we obtain $h\in\left\{
k+1,k+2,\ldots,n\right\}  $. Hence, $h-k\in\left\{  1,2,\ldots,n-k\right\}  $.
\par
Now, $g-k$ and $h-k$ are two elements of $\left\{  1,2,\ldots,n-k\right\}  $
(since $g-k\in\left\{  1,2,\ldots,n-k\right\}  $ and $h-k\in\left\{
1,2,\ldots,n-k\right\}  $). These two elements are distinct (since
$\underbrace{g}_{<h}-k<h-k$).
\par
But $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list of all elements of
$\left[  n\right]  \setminus I$ (with no repetitions). Thus, the list $\left(
b_{1},b_{2},\ldots,b_{n-k}\right)  $ contains no repetitions. In other words,
the elements $b_{1},b_{2},\ldots,b_{n-k}$ are pairwise distinct. In other
words, if $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots
,n-k\right\}  $, then $b_{u}\neq b_{v}$. Applying this to $u=g-k$ and $v=h-k$,
we obtain $b_{g-k}\neq b_{h-k}$. This contradicts $b_{g-k}=b_{h-k}$. Hence, we
have obtained a contradiction in Case 2.
\par
Now, we have obtained a contradiction in each of the two Cases 1 and 2. Thus,
we always have a contradiction (since the two Cases 1 and 2 cover all
possibilities). This contradiction proves that our assumption (that $g\neq h$)
was wrong. Hence, we cannot have $g\neq h$. We thus have $g=h$.
\par
Now, forget that we fixed $g$ and $h$. We thus have proven that if $g$ and $h$
are two elements of $\left[  n\right]  $ such that $f\left(  g\right)
=f\left(  h\right)  $, then $g=h$. In other words, the map $f$ is injective.
Qed.}. Hence, the map $f$ is bijective (since $f$ is injective and
surjective). Hence, $f$ is a bijective map $\left[  n\right]  \rightarrow
\left[  n\right]  $. In other words, $f$ is a permutation of $\left[
n\right]  $.

But $S_{n}$ is the set of all permutations of $\left\{  1,2,\ldots,n\right\}
$ (by the definition of $S_{n}$). In other words, $S_{n}$ is the set of all
permutations of $\left[  n\right]  $ (since $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $). Now, $f$ is a permutation of $\left[  n\right]  $.
In other words, $f$ is an element of $S_{n}$ (since $S_{n}$ is the set of all
permutations of $\left[  n\right]  $). In other words, $f\in S_{n}$. Hence,
$f$ is a $\sigma\in S_{n}$ satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2}%
,\ldots,b_{n-k}\right)  \label{pf.lem.sol.exe.Ialbe.InotI.a.silis}%
\end{equation}
(since $f$ is an element of $S_{n}$ and satisfies \newline$\left(  f\left(
1\right)  ,f\left(  2\right)  ,\ldots,f\left(  n\right)  \right)  =\left(
a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  $). Thus, there
exists \textbf{at least one} $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}).

On the other hand, if $\sigma_{1}$ and $\sigma_{2}$ are two elements
$\sigma\in S_{n}$ satisfying (\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}), then
$\sigma_{1}=\sigma_{2}$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma_{1}$ and
$\sigma_{2}$ be two elements $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}). We shall show that $\sigma
_{1}=\sigma_{2}$.
\par
We know that $\sigma_{1}$ is an element $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}). In other words, $\sigma_{1}$ is an
element of $S_{n}$ and satisfies%
\[
\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  =\left(  a_{1},a_{2},\ldots
,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  .
\]
\par
We know that $\sigma_{2}$ is an element $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}). In other words, $\sigma_{2}$ is an
element of $S_{n}$ and satisfies%
\[
\left(  \sigma_{2}\left(  1\right)  ,\sigma_{2}\left(  2\right)
,\ldots,\sigma_{2}\left(  n\right)  \right)  =\left(  a_{1},a_{2},\ldots
,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  .
\]
\par
Now,
\[
\left(  \sigma_{1}\left(  1\right)  ,\sigma_{1}\left(  2\right)
,\ldots,\sigma_{1}\left(  n\right)  \right)  =\left(  a_{1},a_{2},\ldots
,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  =\left(  \sigma_{2}\left(  1\right)
,\sigma_{2}\left(  2\right)  ,\ldots,\sigma_{2}\left(  n\right)  \right)  .
\]
In other words, $\sigma_{1}\left(  i\right)  =\sigma_{2}\left(  i\right)  $
for every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words, $\sigma
_{1}\left(  i\right)  =\sigma_{2}\left(  i\right)  $ for every $i\in\left[
n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $).
\par
Now, $\sigma_{1}$ is an element of $S_{n}$. In other words, $\sigma_{1}$ is a
permutation of $\left[  n\right]  $ (since $S_{n}$ is the set of all
permutations of $\left[  n\right]  $). Hence, $\sigma_{1}$ is a map $\left[
n\right]  \rightarrow\left[  n\right]  $. The same argument (applied to
$\sigma_{2}$ instead of $\sigma_{1}$) shows that $\sigma_{2}$ is a map
$\left[  n\right]  \rightarrow\left[  n\right]  $. Hence, $\sigma_{1}$ and
$\sigma_{2}$ are maps $\left[  n\right]  \rightarrow\left[  n\right]  $. Thus,
$\sigma_{1}=\sigma_{2}$ (because $\sigma_{1}\left(  i\right)  =\sigma
_{2}\left(  i\right)  $ for every $i\in\left[  n\right]  $). Qed.}. In other
words, there exists \textbf{at most one} $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}).

We have now proven that there exists \textbf{at least one} $\sigma\in S_{n}$
satisfying (\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}), and that there exists
\textbf{at most one} such $\sigma$. Hence, there exists a \textbf{unique}
$\sigma\in S_{n}$ satisfying (\ref{pf.lem.sol.exe.Ialbe.InotI.a.silis}). This
proves Lemma \ref{lem.sol.exe.Ialbe.InotI} \textbf{(a)}.

\textbf{(b)} Clearly,
\begin{equation}
\sum I=\sum_{i\in I}i=\sum_{x\in I}x \label{pf.lem.sol.exe.Ialbe.InotI.b.triv}%
\end{equation}
(here, we have renamed the summation index $i$ as $x$).

We know that $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is a list of all
elements of $I$ (with no repetitions). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $I$, $k$ and $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ instead of $S$, $s$ and $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[  k\right]
\rightarrow I,\ h\mapsto a_{h}$ is well-defined and a bijection.

We know that $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list of all
elements of $\left[  n\right]  \setminus I$ (with no repetitions). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $\left[  n\right]
\setminus I$, $n-k$ and $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ instead
of $S$, $s$ and $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the
map $\left[  n-k\right]  \rightarrow\left[  n\right]  \setminus I,\ h\mapsto
b_{h}$ is well-defined and a bijection.

Now,%
\begin{align}
&  \underbrace{\sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[
n-k\right]  }}_{=\sum_{i\in\left[  k\right]  }\sum_{j\in\left[  n-k\right]  }%
}\left[  a_{i}>b_{j}\right] \nonumber\\
&  =\sum_{i\in\left[  k\right]  }\underbrace{\sum_{j\in\left[  n-k\right]
}\left[  a_{i}>b_{j}\right]  }_{\substack{=\sum_{h\in\left[  n-k\right]
}\left[  a_{i}>b_{h}\right]  \\\text{(here, we have renamed the}%
\\\text{summation index }j\text{ as }h\text{)}}}=\sum_{i\in\left[  k\right]
}\underbrace{\sum_{h\in\left[  n-k\right]  }\left[  a_{i}>b_{h}\right]
}_{\substack{=\sum_{y\in\left[  n\right]  \setminus I}\left[  a_{i}>y\right]
\\\text{(here, we have substituted }y\text{ for }b_{h}\text{ in the
sum,}\\\text{since the map }\left[  n-k\right]  \rightarrow\left[  n\right]
\setminus I,\ h\mapsto b_{h}\text{ is a bijection)}}}\nonumber\\
&  =\sum_{i\in\left[  k\right]  }\sum_{y\in\left[  n\right]  \setminus
I}\left[  a_{i}>y\right] \nonumber\\
&  =\sum_{h\in\left[  k\right]  }\sum_{y\in\left[  n\right]  \setminus
I}\left[  a_{h}>y\right]  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the}\\
\text{summation index }i\text{ as }h\text{ in the outer sum}%
\end{array}
\right) \nonumber\\
&  =\sum_{x\in I}\sum_{y\in\left[  n\right]  \setminus I}\left[  x>y\right]
\label{pf.lem.sol.exe.Ialbe.InotI.b.1}%
\end{align}
(here, we have substituted $x$ for $a_{h}$ in the outer sum, since the map
$\left[  k\right]  \rightarrow I,\ h\mapsto a_{h}$ is a bijection). But every
$x\in I$ satisfies%
\begin{equation}
\sum_{y\in\left[  n\right]  \setminus I}\left[  x>y\right]  =x-1-\sum_{y\in
I}\left[  x>y\right]  \label{pf.lem.sol.exe.Ialbe.InotI.b.sum1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exe.Ialbe.InotI.b.sum1}):} Let
$x\in I$. Then, $x\in I\subseteq\left[  n\right]  =\left\{  1,2,\ldots
,n\right\}  $, so that $1\leq x\leq n$.
\par
Every $y\in\left[  n\right]  $ satisfies either $y\in I$ or $y\notin I$ (but
not both). Hence,%
\begin{align*}
\sum_{y\in\left[  n\right]  }\left[  x>y\right]   &  =\underbrace{\sum
_{\substack{y\in\left[  n\right]  ;\\y\in I}}}_{\substack{=\sum_{y\in
I}\\\text{(since }I\subseteq\left[  n\right]  \text{)}}}\left[  x>y\right]
+\underbrace{\sum_{\substack{y\in\left[  n\right]  ;\\y\notin I}}}%
_{=\sum_{y\in\left[  n\right]  \setminus I}}\left[  x>y\right] \\
&  =\sum_{y\in I}\left[  x>y\right]  +\sum_{y\in\left[  n\right]  \setminus
I}\left[  x>y\right]  .
\end{align*}
Comparing this with%
\begin{align*}
\underbrace{\sum_{y\in\left[  n\right]  }}_{\substack{=\sum_{y\in\left\{
1,2,\ldots,n\right\}  }\\\text{(since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \text{)}}}\left[  x>y\right]   &  =\underbrace{\sum
_{y\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{y=1}^{n}}\left[  x>y\right]
=\sum_{y=1}^{n}\left[  x>y\right] \\
&  =\sum_{y=1}^{x-1}\underbrace{\left[  x>y\right]  }%
_{\substack{=1\\\text{(since }x>y\\\text{(since }y\leq x-1<x\text{))}}%
}+\sum_{y=x}^{n}\underbrace{\left[  x>y\right]  }_{\substack{=0\\\text{(since
we don't have }x>y\\\text{(since }y\geq x\text{))}}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq x\leq n\right) \\
&  =\sum_{y=1}^{x-1}1+\underbrace{\sum_{y=x}^{n}0}_{=0}=\sum_{y=1}%
^{x-1}1=\left(  x-1\right)  1=x-1,
\end{align*}
we obtain%
\[
\sum_{y\in I}\left[  x>y\right]  +\sum_{y\in\left[  n\right]  \setminus
I}\left[  x>y\right]  =x-1.
\]
Subtracting $\sum_{y\in I}\left[  x>y\right]  $ from both sides of this
equality, we obtain $\sum_{y\in\left[  n\right]  \setminus I}\left[
x>y\right]  =x-1-\sum_{y\in I}\left[  x>y\right]  $. This proves
(\ref{pf.lem.sol.exe.Ialbe.InotI.b.sum1}).}. Thus,
(\ref{pf.lem.sol.exe.Ialbe.InotI.b.1}) becomes%
\begin{align*}
&  \sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[  n-k\right]
}\left[  a_{i}>b_{j}\right] \\
&  =\sum_{x\in I}\underbrace{\sum_{y\in\left[  n\right]  \setminus I}\left[
x>y\right]  }_{\substack{=x-1-\sum_{y\in I}\left[  x>y\right]  \\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.InotI.b.sum1}))}}}\\
&  =\sum_{x\in I}\left(  x-1-\sum_{y\in I}\left[  x>y\right]  \right)
=\underbrace{\sum_{x\in I}x}_{\substack{=\sum I\\\text{(by
(\ref{pf.lem.sol.exe.Ialbe.InotI.b.triv}))}}}-\underbrace{\sum_{x\in I}%
1}_{=\left\vert I\right\vert \cdot1=\left\vert I\right\vert =k}%
-\underbrace{\sum_{x\in I}\sum_{y\in I}\left[  x>y\right]  }%
_{\substack{=0+1+\cdots+\left(  k-1\right)  \\\text{(by Lemma
\ref{lem.sol.exe.Ialbe.II})}}}\\
&  =\sum I-k-\left(  0+1+\cdots+\left(  k-1\right)  \right)  =\sum
I-\underbrace{\left(  \left(  0+1+\cdots+\left(  k-1\right)  \right)
+k\right)  }_{=0+1+\cdots+k=1+2+\cdots+k}\\
&  =\sum I-\left(  1+2+\cdots+k\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.exe.Ialbe.InotI} \textbf{(b)}.
\end{proof}
\end{verlong}

Let us now come to the actual solution of Exercise \ref{exe.Ialbe}:

\begin{proof}
[Solution to Exercise \ref{exe.Ialbe}.]We have $I\subseteq\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  $ (since $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $). Hence, $\left\vert \left[  n\right]  \setminus
I\right\vert =\left\vert \underbrace{\left[  n\right]  }_{=\left\{
1,2,\ldots,n\right\}  }\right\vert -\underbrace{\left\vert I\right\vert }%
_{=k}=\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert }%
_{=n}-k=n-k$. Thus, $n-k=\left\vert \left[  n\right]  \setminus I\right\vert
\in\mathbb{N}$ (since the cardinality of any finite set is $\in\mathbb{N}$).
Hence, $n-k\geq0$, so that $k\leq n$. Also, $k=\left\vert I\right\vert
\in\mathbb{N}$.

We know that $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is a list of all
elements of $I$ (with no repetitions). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(b)} (applied to $I$, $k$, $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ and $\alpha$ instead of $S$, $s$, $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $ and $\pi$) shows that $\left(
a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots
,a_{\alpha\left(  k\right)  }\right)  $ is a list of all elements of $I$ (with
no repetitions).

We know that $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list of all
elements of $\left\{  1,2,\ldots,n\right\}  \setminus I$ (with no
repetitions). In other words, $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is
a list of all elements of $\left[  n\right]  \setminus I$ (with no
repetitions) (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).
Thus, Lemma \ref{lem.sol.exe.Ialbe.inclist} \textbf{(b)} (applied to $\left[
n\right]  \setminus I$, $n-k$, $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $
and $\beta$ instead of $S$, $s$, $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $
and $\pi$) shows that $\left(  b_{\beta\left(  1\right)  },b_{\beta\left(
2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  $ is a list of all
elements of $\left[  n\right]  \setminus I$ (with no repetitions).

\textbf{(a)} Lemma \ref{lem.sol.exe.Ialbe.InotI} \textbf{(a)} (applied to
$\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right)  $ and $\left(  b_{\beta\left(
1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)
}\right)  $ instead of $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ and
$\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $) shows that there exists a
unique $\sigma\in S_{n}$ satisfying%
\[
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\]
This solves Exercise \ref{exe.Ialbe} \textbf{(a)}.

\textbf{(b)} Let $\mathbf{a}$ be the $k$-tuple $\left(  a_{\alpha\left(
1\right)  },a_{\alpha\left(  2\right)  },\ldots,a_{\alpha\left(  k\right)
}\right)  $ of integers. Recall that $\left(  a_{1},a_{2},\ldots,a_{k}\right)
$ is the list of all elements of $I$ in increasing order (with no
repetitions). Moreover, $k=\left\vert I\right\vert $. Hence, Lemma
\ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(b)} (applied to $I$, $k$, $\alpha$
and $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ instead of $P$, $m$, $\sigma$
and $\left(  p_{1},p_{2},\ldots,p_{m}\right)  $) yields%
\begin{equation}
\ell\left(  \alpha\right)  =\ell\left(  \underbrace{\left(  a_{\alpha\left(
1\right)  },a_{\alpha\left(  2\right)  },\ldots,a_{\alpha\left(  k\right)
}\right)  }_{=\mathbf{a}}\right)  =\ell\left(  \mathbf{a}\right)  .
\label{sol.Ialbe.b.la}%
\end{equation}


Let $\mathbf{b}$ be the $\left(  n-k\right)  $-tuple $\left(  b_{\beta\left(
1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)
}\right)  $ of integers. Recall that \newline$\left(  b_{1},b_{2}%
,\ldots,b_{n-k}\right)  $ is the list of all elements of $\left\{
1,2,\ldots,n\right\}  \setminus I$ in increasing order (with no repetitions).
In other words, $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is the list of
all elements of $\left[  n\right]  \setminus I$ in increasing order (with no
repetitions) (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).
Moreover, $n-k=\left\vert \left[  n\right]  \setminus I\right\vert $ (since
$\left\vert \left[  n\right]  \setminus I\right\vert =n-k$). Hence, Lemma
\ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(b)} (applied to $\left[  n\right]
\setminus I$, $n-k$, $\beta$ and $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $
instead of $P$, $m$, $\sigma$ and $\left(  p_{1},p_{2},\ldots,p_{m}\right)  $)
yields%
\begin{equation}
\ell\left(  \beta\right)  =\ell\left(  \underbrace{\left(  b_{\beta\left(
1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)
}\right)  }_{=\mathbf{b}}\right)  =\ell\left(  \mathbf{b}\right)  .
\label{sol.Ialbe.b.lb}%
\end{equation}


Let $\mathbf{c}$ be the $\left(  k+\left(  n-k\right)  \right)  $-tuple
$\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  $ of integers.

The permutation $\sigma_{I,\alpha,\beta}$ is the unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.b.1}%
\end{equation}
Thus, $\sigma_{I,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{sol.Ialbe.b.1}). In other words, we have%
\begin{align}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\nonumber\\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)
=\mathbf{c} \label{sol.Ialbe.b.2}%
\end{align}
(since $\mathbf{c}=\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  $).

Now, $\left\{  1,2,\ldots,n\right\}  $ is a finite set of integers and
satisfies $n=\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert $.
Moreover, $\left(  1,2,\ldots,n\right)  $ is the list of all elements of
$\left\{  1,2,\ldots,n\right\}  $ in increasing order (with no repetitions).
Hence, Lemma \ref{lem.sol.exe.Ialbe.Inv=Inv} \textbf{(b)} (applied to
$\left\{  1,2,\ldots,n\right\}  $, $n$, $\sigma_{I,\alpha,\beta}$ and $\left(
1,2,\ldots,n\right)  $) yields%
\begin{align}
\ell\left(  \sigma_{I,\alpha,\beta}\right)   &  =\ell\left(
\underbrace{\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma
_{I,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(
n\right)  \right)  }_{\substack{=\mathbf{c}\\\text{(by (\ref{sol.Ialbe.b.2}%
))}}}\right) \nonumber\\
&  =\ell\left(  \mathbf{c}\right)  =\ell\left(  \mathbf{a}\right)
+\ell\left(  \mathbf{b}\right)  +\sum_{\left(  i,j\right)  \in\left[
k\right]  \times\left[  n-k\right]  }\left[  a_{\alpha\left(  i\right)
}>b_{\beta\left(  j\right)  }\right]  \label{sol.Ialbe.b.9}%
\end{align}
(by Lemma \ref{lem.sol.exe.Ialbe.ab} (applied to $k$, $n-k$, $\left(
a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots
,a_{\alpha\left(  k\right)  }\right)  $ and $\left(  b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  $
instead of $n$, $m$, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ and $\left(
b_{1},b_{2},\ldots,b_{m}\right)  $)).

Now, recall that $\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  }\right)  $ is a list of all
elements of $I$ (with no repetitions). Also, recall that $\left(
b_{\beta\left(  1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(
n-k\right)  }\right)  $ is a list of all elements of $\left[  n\right]
\setminus I$ (with no repetitions). Lemma \ref{lem.sol.exe.Ialbe.InotI}
\textbf{(b)} (applied to $\left(  a_{\alpha\left(  1\right)  },a_{\alpha
\left(  2\right)  },\ldots,a_{\alpha\left(  k\right)  }\right)  $ and $\left(
b_{\beta\left(  1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(
n-k\right)  }\right)  $ instead of $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $
and $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $) thus shows that%
\[
\sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[  n-k\right]
}\left[  a_{\alpha\left(  i\right)  }>b_{\beta\left(  j\right)  }\right]
=\sum I-\left(  1+2+\cdots+k\right)  .
\]
Hence, (\ref{sol.Ialbe.b.9}) becomes%
\begin{align*}
\ell\left(  \sigma_{I,\alpha,\beta}\right)   &  =\underbrace{\ell\left(
\mathbf{a}\right)  }_{\substack{=\ell\left(  \alpha\right)  \\\text{(by
(\ref{sol.Ialbe.b.la}))}}}+\underbrace{\ell\left(  \mathbf{b}\right)
}_{\substack{=\ell\left(  \beta\right)  \\\text{(by (\ref{sol.Ialbe.b.lb}))}%
}}+\underbrace{\sum_{\left(  i,j\right)  \in\left[  k\right]  \times\left[
n-k\right]  }\left[  a_{\alpha\left(  i\right)  }>b_{\beta\left(  j\right)
}\right]  }_{=\sum I-\left(  1+2+\cdots+k\right)  }\\
&  =\ell\left(  \alpha\right)  +\ell\left(  \beta\right)  +\sum I-\left(
1+2+\cdots+k\right)  .
\end{align*}


It thus remains to prove that $\left(  -1\right)  ^{\sigma_{I,\alpha,\beta}%
}=\left(  -1\right)  ^{\alpha}\cdot\left(  -1\right)  ^{\beta}\cdot\left(
-1\right)  ^{\sum I-\left(  1+2+\cdots+k\right)  }$.

But the definition of $\left(  -1\right)  ^{\sigma_{I,\alpha,\beta}}$ yields
\begin{align*}
\left(  -1\right)  ^{\sigma_{I,\alpha,\beta}}  &  =\left(  -1\right)
^{\ell\left(  \sigma_{I,\alpha,\beta}\right)  }=\left(  -1\right)
^{\ell\left(  \alpha\right)  +\ell\left(  \beta\right)  +\sum I-\left(
1+2+\cdots+k\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma_{I,\alpha
,\beta}\right)  =\ell\left(  \alpha\right)  +\ell\left(  \beta\right)  +\sum
I-\left(  1+2+\cdots+k\right)  \right) \\
&  =\left(  -1\right)  ^{\ell\left(  \alpha\right)  }\cdot\left(  -1\right)
^{\ell\left(  \beta\right)  }\cdot\left(  -1\right)  ^{\sum I-\left(
1+2+\cdots+k\right)  }.
\end{align*}
Comparing this with%
\begin{align*}
&  \underbrace{\left(  -1\right)  ^{\alpha}}_{\substack{=\left(  -1\right)
^{\ell\left(  \alpha\right)  }\\\text{(by the definition of }\left(
-1\right)  ^{\alpha}\text{)}}}\cdot\underbrace{\left(  -1\right)  ^{\beta}%
}_{\substack{=\left(  -1\right)  ^{\ell\left(  \beta\right)  }\\\text{(by the
definition of }\left(  -1\right)  ^{\beta}\text{)}}}\cdot\left(  -1\right)
^{\sum I-\left(  1+2+\cdots+k\right)  }\\
&  =\left(  -1\right)  ^{\ell\left(  \alpha\right)  }\cdot\left(  -1\right)
^{\ell\left(  \beta\right)  }\cdot\left(  -1\right)  ^{\sum I-\left(
1+2+\cdots+k\right)  },
\end{align*}
this yields $\left(  -1\right)  ^{\sigma_{I,\alpha,\beta}}=\left(  -1\right)
^{\alpha}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum
I-\left(  1+2+\cdots+k\right)  }$. This completes the solution of Exercise
\ref{exe.Ialbe} \textbf{(b)}.

\begin{vershort}
\textbf{(c)} For every $\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}$,
the permutation $\sigma_{I,\alpha,\beta}\in S_{n}$ satisfies%
\begin{align}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\nonumber\\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)
\label{sol.Ialbe.c.short.siIAB}%
\end{align}
\footnote{\textit{Proof of (\ref{sol.Ialbe.c.short.siIAB}):} Let $\left(
\alpha,\beta\right)  \in S_{k}\times S_{n-k}$. The permutation $\sigma
_{I,\alpha,\beta}$ is the unique $\sigma\in S_{n}$ satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.short.siIAB.pf.1}%
\end{equation}
Thus, $\sigma_{I,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{sol.Ialbe.c.short.siIAB.pf.1}). In other words, we have%
\[
\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
=\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\]
Qed.}.

For every $\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}$, the element
$\sigma_{I,\alpha,\beta}$ is a well-defined element of $\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =I\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  \alpha,\beta\right)  \in
S_{k}\times S_{n-k}$. We must show that the element $\sigma_{I,\alpha,\beta}$
is a well-defined element of $\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =I\right\}  $.
\par
Now,
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma
_{I,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(
n\right)  \right)  }_{\substack{=\left(  a_{\alpha\left(  1\right)
},a_{\alpha\left(  2\right)  },\ldots,a_{\alpha\left(  k\right)  }%
,b_{\beta\left(  1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta
\left(  n-k\right)  }\right)  \\\text{(by (\ref{sol.Ialbe.c.short.siIAB}))}%
}}\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots
,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  },b_{\beta\left(
2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  \right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right)  .
\end{align*}
Hence,%
\[
\left\{  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right\}
=\left\{  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right\}  =I
\]
(since $\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)
},\ldots,a_{\alpha\left(  k\right)  }\right)  $ is a list of all elements of
$I$). Now,%
\[
\sigma_{I,\alpha,\beta}\left(  \left\{  1,2,\ldots,k\right\}  \right)
=\left\{  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right\}
=I.
\]
\par
So we know that $\sigma_{I,\alpha,\beta}$ is an element of $S_{n}$ and
satisfies $\sigma_{I,\alpha,\beta}\left(  \left\{  1,2,\ldots,k\right\}
\right)  =I$. In other words,%
\[
\sigma_{I,\alpha,\beta}\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =I\right\}  .
\]
We thus have proven that $\sigma_{I,\alpha,\beta}$ is a well-defined element
of $\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}
\right)  =I\right\}  $. Qed.}. Hence, the map%
\begin{align*}
S_{k}\times S_{n-k}  &  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =I\right\}  ,\\
\left(  \alpha,\beta\right)   &  \mapsto\sigma_{I,\alpha,\beta}%
\end{align*}
is well-defined. Denote this map by $\mu$.
\end{vershort}

\begin{verlong}
\textbf{(c)} For every $\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}$,
the element $\sigma_{I,\alpha,\beta}$ is a well-defined element of $\left\{
\tau\in S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)
=I\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let $\left(  \alpha
,\beta\right)  \in S_{k}\times S_{n-k}$. We must show that the element
$\sigma_{I,\alpha,\beta}$ is a well-defined element of $\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =I\right\}
$.
\par
We have $\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}$. In other
words, $\alpha\in S_{k}$ and $\beta\in S_{n-k}$. Hence, $\sigma_{I,\alpha
,\beta}$ is a well-defined element of $S_{n}$.
\par
The permutation $\sigma_{I,\alpha,\beta}$ is the unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.wd.fn1.1}%
\end{equation}
Thus, $\sigma_{I,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{sol.Ialbe.c.wd.fn1.1}). In other words, we have%
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\end{align*}
Now,
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma
_{I,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(
n\right)  \right)  }_{=\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)
}\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots
,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  },b_{\beta\left(
2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  \right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right)  .
\end{align*}
Hence,%
\[
\left\{  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right\}
=\left\{  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right\}  =I
\]
(since $\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)
},\ldots,a_{\alpha\left(  k\right)  }\right)  $ is a list of all elements of
$I$ (with no repetitions)). Now,%
\[
\sigma_{I,\alpha,\beta}\left(  \left\{  1,2,\ldots,k\right\}  \right)
=\left\{  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right\}
=I.
\]
\par
So we know that $\sigma_{I,\alpha,\beta}$ is an element of $S_{n}$ and
satisfies $\sigma_{I,\alpha,\beta}\left(  \left\{  1,2,\ldots,k\right\}
\right)  =I$. In other words, $\sigma_{I,\alpha,\beta}$ is an element $\tau\in
S_{n}$ satisfying $\tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =I$. In
other words,%
\[
\sigma_{I,\alpha,\beta}\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =I\right\}  .
\]
We thus have proven that $\sigma_{I,\alpha,\beta}$ is a well-defined element
of $\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}
\right)  =I\right\}  $. Qed.}. Hence, the map%
\begin{align*}
S_{k}\times S_{n-k}  &  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =I\right\}  ,\\
\left(  \alpha,\beta\right)   &  \mapsto\sigma_{I,\alpha,\beta}%
\end{align*}
is well-defined. Denote this map by $\mu$.
\end{verlong}

\begin{vershort}
Now, it is easy to see that the map $\mu$ is
injective\footnote{\textit{Proof.} Let $\left(  \alpha,\beta\right)  $ and
$\left(  \alpha^{\prime},\beta^{\prime}\right)  $ be two elements of
$S_{k}\times S_{n-k}$ satisfying $\mu\left(  \alpha,\beta\right)  =\mu\left(
\alpha^{\prime},\beta^{\prime}\right)  $. We will show that $\left(
\alpha,\beta\right)  =\left(  \alpha^{\prime},\beta^{\prime}\right)  $.
\par
The definition of $\mu\left(  \alpha,\beta\right)  $ yields $\mu\left(
\alpha,\beta\right)  =\sigma_{I,\alpha,\beta}$. The definition of $\mu\left(
\alpha^{\prime},\beta^{\prime}\right)  $ yields $\mu\left(  \alpha^{\prime
},\beta^{\prime}\right)  =\sigma_{I,\alpha^{\prime},\beta^{\prime}}$. Hence,
$\sigma_{I,\alpha^{\prime},\beta^{\prime}}=\mu\left(  \alpha^{\prime}%
,\beta^{\prime}\right)  =\mu\left(  \alpha,\beta\right)  =\sigma
_{I,\alpha,\beta}$.
\par
Now, (\ref{sol.Ialbe.c.short.siIAB}) (applied to $\left(  \alpha^{\prime
},\beta^{\prime}\right)  $ instead of $\left(  \alpha,\beta\right)  $) yields%
\begin{align*}
&  \left(  \sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(  1\right)
,\sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(  2\right)  ,\ldots
,\sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(  n\right)  \right) \\
&  =\left(  a_{\alpha^{\prime}\left(  1\right)  },a_{\alpha^{\prime}\left(
2\right)  },\ldots,a_{\alpha^{\prime}\left(  k\right)  },b_{\beta^{\prime
}\left(  1\right)  },b_{\beta^{\prime}\left(  2\right)  },\ldots
,b_{\beta^{\prime}\left(  n-k\right)  }\right)  .
\end{align*}
Comparing this with%
\begin{align*}
&  \left(  \sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(  1\right)
,\sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(  2\right)  ,\ldots
,\sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(  n\right)  \right) \\
&  =\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\sigma_{I,\alpha^{\prime}%
,\beta^{\prime}}=\sigma_{I,\alpha,\beta}\right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  ,
\end{align*}
we obtain%
\begin{align*}
&  \left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right) \\
&  =\left(  a_{\alpha^{\prime}\left(  1\right)  },a_{\alpha^{\prime}\left(
2\right)  },\ldots,a_{\alpha^{\prime}\left(  k\right)  },b_{\beta^{\prime
}\left(  1\right)  },b_{\beta^{\prime}\left(  2\right)  },\ldots
,b_{\beta^{\prime}\left(  n-k\right)  }\right)  .
\end{align*}
In other words,%
\begin{equation}
\left(  a_{\alpha\left(  i\right)  }=a_{\alpha^{\prime}\left(  i\right)
}\text{ for every }i\in\left\{  1,2,\ldots,k\right\}  \right)
\label{sol.Ialbe.c.short.inj.pf.1}%
\end{equation}
and%
\begin{equation}
\left(  b_{\beta\left(  j\right)  }=b_{\beta^{\prime}\left(  j\right)  }\text{
for every }j\in\left\{  1,2,\ldots,n-k\right\}  \right)  .
\label{sol.Ialbe.c.short.inj.pf.2}%
\end{equation}
\par
Now, we shall prove that $\alpha=\alpha^{\prime}$.
\par
Indeed, fix $i\in\left\{  1,2,\ldots,k\right\}  $. The list $\left(
a_{1},a_{2},\ldots,a_{k}\right)  $ has no repetitions. In other words, the
elements $a_{1},a_{2},\ldots,a_{k}$ are pairwise distinct. Thus, if $u$ and
$v$ are two elements of $\left\{  1,2,\ldots,k\right\}  $ such that
$a_{u}=a_{v}$, then $u=v$. Applying this to $u=\alpha\left(  i\right)  $ and
$v=\alpha^{\prime}\left(  i\right)  $, we obtain $\alpha\left(  i\right)
=\alpha^{\prime}\left(  i\right)  $ (since (\ref{sol.Ialbe.c.short.inj.pf.1})
yields $a_{\alpha\left(  i\right)  }=a_{\alpha^{\prime}\left(  i\right)  }$).
\par
Now, forget that we fixed $i$. We thus have proven that $\alpha\left(
i\right)  =\alpha^{\prime}\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,k\right\}  $. Thus, $\alpha=\alpha^{\prime}$ (since $\alpha$ and
$\alpha^{\prime}$ are permutations of $\left\{  1,2,\ldots,k\right\}  $).
\par
We have thus proven $\alpha=\alpha^{\prime}$ using the equalities
(\ref{sol.Ialbe.c.short.inj.pf.1}). Similarly, we can prove $\beta
=\beta^{\prime}$ using the equalities (\ref{sol.Ialbe.c.short.inj.pf.2}).
\par
Now, $\left(  \underbrace{\alpha}_{=\alpha^{\prime}},\underbrace{\beta
}_{=\beta^{\prime}}\right)  =\left(  \alpha^{\prime},\beta^{\prime}\right)  $.
\par
Now, forget that we fixed $\left(  \alpha,\beta\right)  $ and $\left(
\alpha^{\prime},\beta^{\prime}\right)  $. We thus have proven that if $\left(
\alpha,\beta\right)  $ and $\left(  \alpha^{\prime},\beta^{\prime}\right)  $
are two elements of $S_{k}\times S_{n-k}$ satisfying $\mu\left(  \alpha
,\beta\right)  =\mu\left(  \alpha^{\prime},\beta^{\prime}\right)  $, then
$\left(  \alpha,\beta\right)  =\left(  \alpha^{\prime},\beta^{\prime}\right)
$. In other words, the map $\mu$ is injective. Qed.}.
\end{vershort}

\begin{verlong}
For every $\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}$ and every
$i\in\left\{  1,2,\ldots,k\right\}  $, we have%
\begin{equation}
a_{\alpha\left(  i\right)  }=\sigma_{I,\alpha,\beta}\left(  i\right)
\label{sol.Ialbe.c.aai=}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.Ialbe.c.aai=}):} Let $\left(
\alpha,\beta\right)  \in S_{k}\times S_{n-k}$.
\par
The permutation $\sigma_{I,\alpha,\beta}$ is the unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.aai=.pf.1}%
\end{equation}
Thus, $\sigma_{I,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{sol.Ialbe.c.aai=.pf.1}). In other words, we have%
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\end{align*}
Now,%
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma
_{I,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(
n\right)  \right)  }_{=\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)
}\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots
,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  },b_{\beta\left(
2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  \right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right)  .
\end{align*}
In other words, $\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  }\right)  =\left(
\sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta}\left(
2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  k\right)  \right)  $. In
other words, $a_{\alpha\left(  i\right)  }=\sigma_{I,\alpha,\beta}\left(
i\right)  $ for every $i\in\left\{  1,2,\ldots,k\right\}  $. This proves
(\ref{sol.Ialbe.c.aai=}).}. For every $\left(  \alpha,\beta\right)  \in
S_{k}\times S_{n-k}$ and every $j\in\left\{  1,2,\ldots,n-k\right\}  $, we
have%
\begin{equation}
b_{\beta\left(  j\right)  }=\sigma_{I,\alpha,\beta}\left(  k+j\right)
\label{sol.Ialbe.c.bbj=}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.Ialbe.c.bbj=}):} Let $\left(
\alpha,\beta\right)  \in S_{k}\times S_{n-k}$.
\par
The permutation $\sigma_{I,\alpha,\beta}$ is the unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.bbj=.pf.1}%
\end{equation}
Thus, $\sigma_{I,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{sol.Ialbe.c.bbj=.pf.1}). In other words, we have%
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\end{align*}
Now,%
\begin{align*}
&  \left(  \sigma_{I,\alpha,\beta}\left(  k+1\right)  ,\sigma_{I,\alpha,\beta
}\left(  k+2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\\
&  =\left(  \text{the list of the last }n-k\text{ entries of the list
}\underbrace{\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma
_{I,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(
n\right)  \right)  }_{=\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)
}\right) \\
&  =\left(  \text{the list of the last }n-k\text{ entries of the list }\left(
a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots
,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  },b_{\beta\left(
2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  \right) \\
&  =\left(  b_{\beta\left(  1\right)  },b_{\beta\left(  2\right)  }%
,\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\end{align*}
In other words, $\left(  b_{\beta\left(  1\right)  },b_{\beta\left(  2\right)
},\ldots,b_{\beta\left(  n-k\right)  }\right)  =\left(  \sigma_{I,\alpha
,\beta}\left(  k+1\right)  ,\sigma_{I,\alpha,\beta}\left(  k+2\right)
,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)  $. In other words,
$b_{\beta\left(  j\right)  }=\sigma_{I,\alpha,\beta}\left(  k+j\right)  $ for
every $i\in\left\{  1,2,\ldots,k\right\}  $. This proves
(\ref{sol.Ialbe.c.bbj=}).}.

The map $\mu$ is injective\footnote{\textit{Proof.} Let $\eta$ and $\xi$ be
two elements of $S_{k}\times S_{n-k}$ such that $\mu\left(  \eta\right)
=\mu\left(  \xi\right)  $. We shall prove that $\eta=\xi$.
\par
We have $\eta\in S_{k}\times S_{n-k}$. Hence, $\eta=\left(  \alpha
,\beta\right)  $ for some $\alpha\in S_{k}$ and $\beta\in S_{n-k}$. Consider
these $\alpha$ and $\beta$.
\par
We have $\xi\in S_{k}\times S_{n-k}$. Hence, $\xi=\left(  \alpha^{\prime
},\beta^{\prime}\right)  $ for some $\alpha^{\prime}\in S_{k}$ and
$\beta^{\prime}\in S_{n-k}$. Consider these $\alpha^{\prime}$ and
$\beta^{\prime}$.
\par
We have $\eta=\left(  \alpha,\beta\right)  $ and thus $\mu\left(  \eta\right)
=\mu\left(  \alpha,\beta\right)  =\sigma_{I,\alpha,\beta}$ (by the definition
of the map $\mu$).
\par
We have $\xi=\left(  \alpha^{\prime},\beta^{\prime}\right)  $ and thus
$\mu\left(  \xi\right)  =\mu\left(  \alpha^{\prime},\beta^{\prime}\right)
=\sigma_{I,\alpha^{\prime},\beta^{\prime}}$ (by the definition of the map
$\xi$).
\par
From $\mu\left(  \eta\right)  =\sigma_{I,\alpha,\beta}$, we obtain
$\sigma_{I,\alpha,\beta}=\mu\left(  \eta\right)  =\mu\left(  \xi\right)
=\sigma_{I,\alpha^{\prime},\beta^{\prime}}$.
\par
We have $\alpha\in S_{k}$. In other words, $\alpha$ is a permutation of
$\left\{  1,2,\ldots,k\right\}  $ (since $S_{k}$ is the set of all
permutations of $\left\{  1,2,\ldots,k\right\}  $). In other words, $\alpha$
is a bijective map $\left\{  1,2,\ldots,k\right\}  \rightarrow\left\{
1,2,\ldots,k\right\}  $.
\par
Let $i\in\left\{  1,2,\ldots,k\right\}  $. Then, $\alpha\left(  i\right)  $
and $\alpha^{\prime}\left(  i\right)  $ are elements of $\left\{
1,2,\ldots,k\right\}  $.
\par
Furthermore, (\ref{sol.Ialbe.c.aai=}) shows that $a_{\alpha\left(  i\right)
}=\underbrace{\sigma_{I,\alpha,\beta}}_{=\sigma_{I,\alpha^{\prime}%
,\beta^{\prime}}}\left(  i\right)  =\sigma_{I,\alpha^{\prime},\beta^{\prime}%
}\left(  i\right)  $. But (\ref{sol.Ialbe.c.aai=}) (applied to $\left(
\alpha^{\prime},\beta^{\prime}\right)  $ instead of $\left(  \alpha
,\beta\right)  $) yields $a_{\alpha^{\prime}\left(  i\right)  }=\sigma
_{I,\alpha^{\prime},\beta^{\prime}}\left(  i\right)  $. Comparing this with
$a_{\alpha\left(  i\right)  }=\sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(
i\right)  $, we obtain $a_{\alpha\left(  i\right)  }=a_{\alpha^{\prime}\left(
i\right)  }$.
\par
We know that $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is a list with no
repetitions. In other words, the elements $a_{1},a_{2},\ldots,a_{k}$ are
pairwise distinct. In other words, if $u$ and $v$ are two elements of
$\left\{  1,2,\ldots,k\right\}  $ such that $a_{u}=a_{v}$, then $u=v$.
Applying this to $u=\alpha\left(  i\right)  $ and $v=\alpha^{\prime}\left(
i\right)  $, we obtain $\alpha\left(  i\right)  =\alpha^{\prime}\left(
i\right)  $ (since $a_{\alpha\left(  i\right)  }=a_{\alpha^{\prime}\left(
i\right)  }$).
\par
Now, forget that we fixed $i$. We thus have shown that $\alpha\left(
i\right)  =\alpha^{\prime}\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,k\right\}  $. In other words, $\alpha=\alpha^{\prime}$ (since
$\alpha$ and $\alpha^{\prime}$ are maps $\left\{  1,2,\ldots,k\right\}
\rightarrow\left\{  1,2,\ldots,k\right\}  $).
\par
We have $\beta\in S_{n-k}$. In other words, $\beta$ is a permutation of
$\left\{  1,2,\ldots,n-k\right\}  $ (since $S_{n-k}$ is the set of all
permutations of $\left\{  1,2,\ldots,n-k\right\}  $). In other words, $\beta$
is a bijective map $\left\{  1,2,\ldots,n-k\right\}  \rightarrow\left\{
1,2,\ldots,n-k\right\}  $.
\par
Let $j\in\left\{  1,2,\ldots,n-k\right\}  $. Then, $\beta\left(  j\right)  $
and $\beta^{\prime}\left(  j\right)  $ are elements of $\left\{
1,2,\ldots,n-k\right\}  $.
\par
Furthermore, (\ref{sol.Ialbe.c.bbj=}) shows that $b_{\beta\left(  j\right)
}=\underbrace{\sigma_{I,\alpha,\beta}}_{=\sigma_{I,\alpha^{\prime}%
,\beta^{\prime}}}\left(  k+j\right)  =\sigma_{I,\alpha^{\prime},\beta^{\prime
}}\left(  k+j\right)  $. But (\ref{sol.Ialbe.c.bbj=}) (applied to $\left(
\alpha^{\prime},\beta^{\prime}\right)  $ instead of $\left(  \alpha
,\beta\right)  $) yields $b_{\beta^{\prime}\left(  j\right)  }=\sigma
_{I,\alpha^{\prime},\beta^{\prime}}\left(  k+j\right)  $. Comparing this with
$b_{\beta\left(  j\right)  }=\sigma_{I,\alpha^{\prime},\beta^{\prime}}\left(
k+j\right)  $, we obtain $b_{\beta\left(  j\right)  }=b_{\beta^{\prime}\left(
j\right)  }$.
\par
We know that $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is a list with no
repetitions. In other words, the elements $b_{1},b_{2},\ldots,b_{n-k}$ are
pairwise distinct. In other words, if $u$ and $v$ are two elements of
$\left\{  1,2,\ldots,n-k\right\}  $ such that $b_{u}=b_{v}$, then $u=v$.
Applying this to $u=\beta\left(  j\right)  $ and $v=\beta^{\prime}\left(
j\right)  $, we obtain $\beta\left(  j\right)  =\beta^{\prime}\left(
j\right)  $ (since $b_{\beta\left(  j\right)  }=b_{\beta^{\prime}\left(
j\right)  }$).
\par
Now, forget that we fixed $j$. We thus have shown that $\beta\left(  j\right)
=\beta^{\prime}\left(  j\right)  $ for every $j\in\left\{  1,2,\ldots
,n-k\right\}  $. In other words, $\beta=\beta^{\prime}$ (since $\beta$ and
$\beta^{\prime}$ are maps $\left\{  1,2,\ldots,n-k\right\}  \rightarrow
\left\{  1,2,\ldots,n-k\right\}  $).
\par
Now, we have shown that $\alpha=\alpha^{\prime}$ and $\beta=\beta^{\prime}$.
Now, $\eta=\left(  \underbrace{\alpha}_{=\alpha^{\prime}},\underbrace{\beta
}_{=\beta^{\prime}}\right)  =\left(  \alpha^{\prime},\beta^{\prime}\right)
=\xi$ (since $\xi=\left(  \alpha^{\prime},\beta^{\prime}\right)  $).
\par
Now, forget that we fixed $\eta$ and $\xi$. We thus have proven that if $\eta$
and $\xi$ are two elements of $S_{k}\times S_{n-k}$ such that $\mu\left(
\eta\right)  =\mu\left(  \xi\right)  $, then $\eta=\xi$. In other words, the
map $\mu$ is injective. Qed.}.
\end{verlong}

Our next goal is to show that the map $\mu$ is surjective.

In fact, let $\gamma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =I\right\}  $. We shall construct an $\left(
\alpha,\beta\right)  \in S_{k}\times S_{n-k}$ such that $\mu\left(
\alpha,\beta\right)  =\gamma$.

\begin{vershort}
We have $\gamma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =I\right\}  $. In other words, $\gamma$ is an
element of $S_{n}$ and satisfies $\gamma\left(  \left\{  1,2,\ldots,k\right\}
\right)  =I$.
\end{vershort}

\begin{verlong}
We have $\gamma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =I\right\}  $. In other words, $\gamma$ is an
element $\tau$ of $S_{n}$ satisfying $\tau\left(  \left\{  1,2,\ldots
,k\right\}  \right)  =I$. In other words, $\gamma$ is an element of $S_{n}$
and satisfies $\gamma\left(  \left\{  1,2,\ldots,k\right\}  \right)  =I$.
\end{verlong}

\begin{vershort}
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of
$\left\{  1,2,\ldots,n\right\}  $. Thus, $\gamma$ is a bijective map $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $, and
therefore also an injective map.
\end{vershort}

\begin{verlong}
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  $). In other words, $\gamma$
is a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. The map $\gamma$ is bijective and thus injective.
\end{verlong}

Every $i\in\left\{  1,2,\ldots,k\right\}  $ satisfies $\gamma\left(  i\right)
\in I$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots
,k\right\}  $. Then, $\gamma\left(  \underbrace{i}_{\in\left\{  1,2,\ldots
,k\right\}  }\right)  \in\gamma\left(  \left\{  1,2,\ldots,k\right\}  \right)
=I$, qed.}. Thus, $\gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  $ are $k$ elements of $I$.

\begin{vershort}
If $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots,k\right\}  $,
then $\gamma\left(  u\right)  \neq\gamma\left(  v\right)  $ (since the map
$\gamma$ is injective). In other words, the $k$ elements $\gamma\left(
1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  $ are
pairwise distinct.
\end{vershort}

\begin{verlong}
Now, if $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots
,k\right\}  $, then $\gamma\left(  u\right)  \neq\gamma\left(  v\right)
$\ \ \ \ \footnote{\textit{Proof.} Let $u$ and $v$ be two distinct elements of
$\left\{  1,2,\ldots,k\right\}  $. Then, $u\in\left\{  1,2,\ldots,k\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $ (since $k\leq n$) and $v\in\left\{
1,2,\ldots,k\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $. Now, $u\neq
v$ (since $u$ and $v$ are distinct), so that $\gamma\left(  u\right)
\neq\gamma\left(  v\right)  $ (since the map $\gamma$ is injective). Qed.}. In
other words, the elements $\gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  $ are pairwise distinct. Hence, $\gamma\left(
1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  $ are $k$
pairwise distinct elements of $I$ (since $\gamma\left(  1\right)
,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  $ are $k$ elements of
$I$).
\end{verlong}

On the other hand, recall that $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is
a list of all elements of $I$ (with no repetitions). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist2} (applied to $I$, $k$, $\left(  a_{1}%
,a_{2},\ldots,a_{k}\right)  $ and \newline$\left(  \gamma\left(  1\right)
,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  \right)  $ instead of
$S$, $s$, $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ and $\left(  p_{1}%
,p_{2},\ldots,p_{s}\right)  $) yields that there exists a $\pi\in S_{k}$ such
that $\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  k\right)  \right)  =\left(  a_{\pi\left(  1\right)  }%
,a_{\pi\left(  2\right)  },\ldots,a_{\pi\left(  k\right)  }\right)  $. Denote
this $\pi$ by $\alpha$. Thus, $\alpha$ is a $\pi\in S_{k}$ such that $\left(
\gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =\left(  a_{\pi\left(  1\right)  },a_{\pi\left(  2\right)
},\ldots,a_{\pi\left(  k\right)  }\right)  $. In other words, $\alpha$ is an
element of $S_{k}$ and satisfies
\begin{equation}
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  }\right)  .
\label{sol.Ialbe.c.gamma1}%
\end{equation}


\begin{vershort}
Every $j\in\left\{  1,2,\ldots,n-k\right\}  $ satisfies $\gamma\left(
k+j\right)  \in\left\{  1,2,\ldots,n\right\}  \setminus I$%
\ \ \ \ \footnote{\textit{Proof.} Let $j\in\left\{  1,2,\ldots,n-k\right\}  $.
Then, $k+j\in\left\{  k+1,k+2,\ldots,n\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $. Hence, $\gamma\left(  k+j\right)  $ is well-defined.
\par
We must show that $\gamma\left(  k+j\right)  \in\left\{  1,2,\ldots,n\right\}
\setminus I$.
\par
Indeed, assume the contrary. Thus, we don't have $\gamma\left(  k+j\right)
\in\left\{  1,2,\ldots,n\right\}  \setminus I$. In other words, we have
$\gamma\left(  k+j\right)  \notin\left\{  1,2,\ldots,n\right\}  \setminus I$.
Combining $\gamma\left(  k+j\right)  \in\left\{  1,2,\ldots,n\right\}  $ with
$\gamma\left(  k+j\right)  \notin\left\{  1,2,\ldots,n\right\}  \setminus I$,
we obtain
\[
\gamma\left(  k+j\right)  \in\left\{  1,2,\ldots,n\right\}  \setminus\left(
\left\{  1,2,\ldots,n\right\}  \setminus I\right)  \subseteq I=\gamma\left(
\left\{  1,2,\ldots,k\right\}  \right)
\]
(since $\gamma\left(  \left\{  1,2,\ldots,k\right\}  \right)  =I$). In other
words, there exists some $v\in\left\{  1,2,\ldots,k\right\}  $ such that
$\gamma\left(  k+j\right)  =\gamma\left(  v\right)  $. Consider this $v$.
Since $\gamma\left(  k+j\right)  =\gamma\left(  v\right)  $, we have $k+j=v$
(because $\gamma$ is injective). Thus, $k+j=v\in\left\{  1,2,\ldots,k\right\}
$, so that $k+j\leq k$. Therefore, $j\leq k-k=0$. This contradicts
$j\in\left\{  1,2,\ldots,n-k\right\}  $. This contradiction shows that our
assumption was wrong. Hence, $\gamma\left(  k+j\right)  \in\left\{
1,2,\ldots,n\right\}  \setminus I$ is proven. Qed.}. Thus, $\gamma\left(
k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  $ are
$n-k$ elements of $\left\{  1,2,\ldots,n\right\}  \setminus I$.
\end{vershort}

\begin{verlong}
Every $j\in\left\{  1,2,\ldots,n-k\right\}  $ satisfies $\gamma\left(
k+j\right)  \in\left\{  1,2,\ldots,n\right\}  \setminus I$%
\ \ \ \ \footnote{\textit{Proof.} Let $j\in\left\{  1,2,\ldots,n-k\right\}  $.
Then, $k+j\in\left\{  k+1,k+2,\ldots,n\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  $. Hence, $\gamma\left(  k+j\right)  $ is well-defined.
\par
We must show that $\gamma\left(  k+j\right)  \in\left\{  1,2,\ldots,n\right\}
\setminus I$.
\par
Indeed, assume the contrary. Thus, we don't have $\gamma\left(  k+j\right)
\in\left\{  1,2,\ldots,n\right\}  \setminus I$. In other words, we have
$\gamma\left(  k+j\right)  \notin\left\{  1,2,\ldots,n\right\}  \setminus I$.
Combining $\gamma\left(  k+j\right)  \in\left\{  1,2,\ldots,n\right\}  $
(since $\gamma$ is a map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $) with $\gamma\left(  k+j\right)  \notin\left\{
1,2,\ldots,n\right\}  \setminus I$, we obtain
\begin{align*}
\gamma\left(  k+j\right)   &  \in\left\{  1,2,\ldots,n\right\}  \setminus
\left(  \left\{  1,2,\ldots,n\right\}  \setminus I\right)
=I\ \ \ \ \ \ \ \ \ \ \left(  \text{since }I\subseteq\left\{  1,2,\ldots
,n\right\}  \right) \\
&  =\left\{  a_{1},a_{2},\ldots,a_{k}\right\}  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\left(  a_{1},a_{2},\ldots,a_{k}\right)  \text{ is a list of all
elements of }I\right)  .
\end{align*}
In other words, $\gamma\left(  k+j\right)  =a_{p}$ for some $p\in\left\{
1,2,\ldots,k\right\}  $. Consider this $p$. Clearly, $\alpha^{-1}\left(
p\right)  $ is a well-defined element of $\left\{  1,2,\ldots,k\right\}  $
(since $\alpha$ is a permutation of $\left\{  1,2,\ldots,k\right\}  $, while
$p$ is an element of $\left\{  1,2,\ldots,k\right\}  $).
\par
But recall that $\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  \right)  =\left(  a_{\alpha\left(  1\right)
},a_{\alpha\left(  2\right)  },\ldots,a_{\alpha\left(  k\right)  }\right)  $.
In other words, $\gamma\left(  u\right)  =a_{\alpha\left(  u\right)  }$ for
every $u\in\left\{  1,2,\ldots,k\right\}  $. Applying this to $u=\alpha
^{-1}\left(  p\right)  $, we obtain $\gamma\left(  \alpha^{-1}\left(
p\right)  \right)  =a_{\alpha\left(  \alpha^{-1}\left(  p\right)  \right)
}=a_{p}$ (since $\alpha\left(  \alpha^{-1}\left(  p\right)  \right)  =p$).
Comparing this with $\gamma\left(  k+j\right)  =a_{p}$, we obtain
$\gamma\left(  k+j\right)  =\gamma\left(  \alpha^{-1}\left(  p\right)
\right)  $.
\par
But the map $\gamma$ is injective. In other words, if $u$ and $v$ are two
elements of $\left\{  1,2,\ldots,n\right\}  $ such that $\gamma\left(
u\right)  =\gamma\left(  v\right)  $, then $u=v$. Applying this to $u=k+j$ and
$v=\alpha^{-1}\left(  p\right)  $, we obtain $k+j=\alpha^{-1}\left(  p\right)
$ (since $\gamma\left(  k+j\right)  =\gamma\left(  \alpha^{-1}\left(
p\right)  \right)  $). But $\alpha^{-1}\left(  p\right)  \in\left\{
1,2,\ldots,k\right\}  $, so that $\alpha^{-1}\left(  p\right)  \leq k$. Hence,
$k+j=\alpha^{-1}\left(  p\right)  \leq k$, so that $j\leq k-k=0<1$. This
contradicts $j\geq1$ (since $j\in\left\{  1,2,\ldots,n-k\right\}  $). This
contradiction shows that our assumption was wrong. Hence, $\gamma\left(
k+j\right)  \in\left\{  1,2,\ldots,n\right\}  \setminus I$ is proven. Qed.}.
Thus, $\gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  k+\left(  n-k\right)  \right)  $ are $n-k$ elements of
$\left\{  1,2,\ldots,n\right\}  \setminus I$.
\end{verlong}

\begin{vershort}
If $u$ and $v$ are two distinct elements of $\left\{  k+1,k+2,\ldots
,n\right\}  $, then $\gamma\left(  u\right)  \neq\gamma\left(  v\right)  $
(since the map $\gamma$ is injective). In other words, the $n-k$ elements
\newline$\gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  $ are pairwise distinct.
\end{vershort}

\begin{verlong}
Now, if $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots
,n-k\right\}  $, then $\gamma\left(  k+u\right)  \neq\gamma\left(  k+v\right)
$\ \ \ \ \footnote{\textit{Proof.} Let $u$ and $v$ be two distinct elements of
$\left\{  1,2,\ldots,n-k\right\}  $. Then, $u\in\left\{  1,2,\ldots
,n-k\right\}  $, so that $k+u\in\left\{  k+1,k+2,\ldots,n\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $.The same argument (applied to $v$
instead of $u$) shows that $k+v\in\left\{  1,2,\ldots,n\right\}  $. Now,
$u\neq v$ (since $u$ and $v$ are distinct), so that $k+u\neq k+v$. Hence,
$\gamma\left(  k+u\right)  \neq\gamma\left(  k+v\right)  $ (since the map
$\gamma$ is injective). Qed.}. In other words, the elements $\gamma\left(
k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  k+\left(
n-k\right)  \right)  $ are pairwise distinct. Hence, $\gamma\left(
k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  k+\left(
n-k\right)  \right)  $ are $n-k$ pairwise distinct elements of $\left\{
1,2,\ldots,n\right\}  \setminus I$ (since $\gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  k+\left(  n-k\right)
\right)  $ are $n-k$ elements of $\left\{  1,2,\ldots,n\right\}  \setminus I$).
\end{verlong}

\begin{vershort}
On the other hand, recall that $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $
is a list of all elements of $\left\{  1,2,\ldots,n\right\}  \setminus I$
(with no repetitions). Thus, Lemma \ref{lem.sol.exe.Ialbe.inclist2} (applied
to $\left\{  1,2,\ldots,n\right\}  \setminus I$, $n-k$, \newline$\left(
b_{1},b_{2},\ldots,b_{n-k}\right)  $ and $\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  \right)  $ instead
of $S$, $s$, $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $ and $\left(
p_{1},p_{2},\ldots,p_{s}\right)  $) yields that there exists a $\pi\in
S_{n-k}$ such that \newline$\left(  \gamma\left(  k+1\right)  ,\gamma\left(
k+2\right)  ,\ldots,\gamma\left(  n\right)  \right)  =\left(  b_{\pi\left(
1\right)  },b_{\pi\left(  2\right)  },\ldots,b_{\pi\left(  n-k\right)
}\right)  $. Denote this $\pi$ by $\beta$. Thus, $\beta$ is an element of
$S_{n-k}$ and satisfies
\begin{equation}
\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right)  =\left(  b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.short.gamma2}%
\end{equation}

\end{vershort}

\begin{verlong}
On the other hand, recall that $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $
is a list of all elements of $\left\{  1,2,\ldots,n\right\}  \setminus I$
(with no repetitions). Thus, Lemma \ref{lem.sol.exe.Ialbe.inclist2} (applied
to $\left\{  1,2,\ldots,n\right\}  \setminus I$, $n-k$, $\left(  b_{1}%
,b_{2},\ldots,b_{n-k}\right)  $ and $\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  k+\left(  n-k\right)
\right)  \right)  $ instead of $S$, $s$, \newline$\left(  c_{1},c_{2}%
,\ldots,c_{s}\right)  $ and $\left(  p_{1},p_{2},\ldots,p_{s}\right)  $)
yields that there exists a $\pi\in S_{n-k}$ such that $\left(  \gamma\left(
k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  k+\left(
n-k\right)  \right)  \right)  =\left(  b_{\pi\left(  1\right)  },b_{\pi\left(
2\right)  },\ldots,b_{\pi\left(  n-k\right)  }\right)  $. Denote this $\pi$ by
$\beta$. Thus, $\beta$ is a $\pi\in S_{n-k}$ such that $\left(  \gamma\left(
k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  k+\left(
n-k\right)  \right)  \right)  =\left(  b_{\pi\left(  1\right)  },b_{\pi\left(
2\right)  },\ldots,b_{\pi\left(  n-k\right)  }\right)  $. In other words,
$\beta$ is an element of $S_{n-k}$ and satisfies
\begin{equation}
\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  k+\left(  n-k\right)  \right)  \right)  =\left(  b_{\beta
\left(  1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(
n-k\right)  }\right)  . \label{sol.Ialbe.c.gamma2}%
\end{equation}

\end{verlong}

\begin{verlong}
The permutation $\sigma_{I,\alpha,\beta}$ is the unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(
2\right)  },\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)
},b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.sigma1}%
\end{equation}
Thus, $\sigma_{I,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{sol.Ialbe.c.sigma1}). In other words, we have%
\begin{align}
&  \left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\nonumber\\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right)  .
\label{sol.Ialbe.c.sigma2}%
\end{align}

\end{verlong}

\begin{vershort}
Now, let us introduce a notation: If $\left(  x_{1},x_{2},\ldots,x_{u}\right)
$ and $\left(  y_{1},y_{2},\ldots,y_{v}\right)  $ are two lists, then we shall
let $\left(  x_{1},x_{2},\ldots,x_{u}\right)  \ast\left(  y_{1},y_{2}%
,\ldots,y_{v}\right)  $ denote the list \newline$\left(  x_{1},x_{2}%
,\ldots,x_{u},y_{1},y_{2},\ldots,y_{v}\right)  $. Then,%
\begin{align*}
&  \left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  n\right)  \right) \\
&  =\underbrace{\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  \right)  }_{\substack{=\left(  a_{\alpha
\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots,a_{\alpha\left(
k\right)  }\right)  \\\text{(by (\ref{sol.Ialbe.c.gamma1}))}}}\ast
\underbrace{\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)
,\ldots,\gamma\left(  n\right)  \right)  }_{\substack{=\left(  b_{\beta\left(
1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)
}\right)  \\\text{(by (\ref{sol.Ialbe.c.short.gamma2}))}}}\\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right)  \ast\left(  b_{\beta\left(
1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)
}\right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right) \\
&  =\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.Ialbe.c.short.siIAB})}\right)
.
\end{align*}
In other words, $\gamma\left(  i\right)  =\sigma_{I,\alpha,\beta}\left(
i\right)  $ for every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words,
$\gamma=\sigma_{I,\alpha,\beta}$ (since $\gamma$ and $\sigma_{I,\alpha,\beta}$
are two maps $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $). But the definition of $\mu$ yields $\mu\left(
\alpha,\beta\right)  =\sigma_{I,\alpha,\beta}$. Comparing this with
$\gamma=\sigma_{I,\alpha,\beta}$, we obtain
\[
\gamma=\mu\underbrace{\left(  \alpha,\beta\right)  }_{\in S_{k}\times S_{n-k}%
}\in\mu\left(  S_{k}\times S_{n-k}\right)  .
\]

\end{vershort}

\begin{verlong}
Now, let us introduce a notation: If $\left(  x_{1},x_{2},\ldots,x_{u}\right)
$ and $\left(  y_{1},y_{2},\ldots,y_{v}\right)  $ are two lists, then we shall
let $\left(  x_{1},x_{2},\ldots,x_{u}\right)  \ast\left(  y_{1},y_{2}%
,\ldots,y_{v}\right)  $ denote the list \newline$\left(  x_{1},x_{2}%
,\ldots,x_{u},y_{1},y_{2},\ldots,y_{v}\right)  $. Then,%
\begin{align*}
&  \left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  n\right)  \right) \\
&  =\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  k\right)  \right)  \ast\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  \underbrace{n}_{=k+\left(
n-k\right)  }\right)  \right) \\
&  =\underbrace{\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  \right)  }_{\substack{=\left(  a_{\alpha
\left(  1\right)  },a_{\alpha\left(  2\right)  },\ldots,a_{\alpha\left(
k\right)  }\right)  \\\text{(by (\ref{sol.Ialbe.c.gamma1}))}}}\ast
\underbrace{\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)
,\ldots,\gamma\left(  k+\left(  n-k\right)  \right)  \right)  }%
_{\substack{=\left(  b_{\beta\left(  1\right)  },b_{\beta\left(  2\right)
},\ldots,b_{\beta\left(  n-k\right)  }\right)  \\\text{(by
(\ref{sol.Ialbe.c.gamma2}))}}}\\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  }\right)  \ast\left(  b_{\beta\left(
1\right)  },b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)
}\right) \\
&  =\left(  a_{\alpha\left(  1\right)  },a_{\alpha\left(  2\right)  }%
,\ldots,a_{\alpha\left(  k\right)  },b_{\beta\left(  1\right)  }%
,b_{\beta\left(  2\right)  },\ldots,b_{\beta\left(  n-k\right)  }\right) \\
&  =\left(  \sigma_{I,\alpha,\beta}\left(  1\right)  ,\sigma_{I,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{I,\alpha,\beta}\left(  n\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.Ialbe.c.sigma2})}\right)  .
\end{align*}
In other words, $\gamma\left(  i\right)  =\sigma_{I,\alpha,\beta}\left(
i\right)  $ for every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words,
$\gamma=\sigma_{I,\alpha,\beta}$ (since $\gamma$ and $\sigma_{I,\alpha,\beta}$
are two maps $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $). But the definition of $\mu$ yields $\mu\left(
\alpha,\beta\right)  =\sigma_{I,\alpha,\beta}$. Comparing this with
$\gamma=\sigma_{I,\alpha,\beta}$, we obtain
\[
\gamma=\mu\underbrace{\left(  \alpha,\beta\right)  }_{\in S_{k}\times S_{n-k}%
}\in\mu\left(  S_{k}\times S_{n-k}\right)  .
\]

\end{verlong}

Now, forget that we fixed $\gamma$. We thus have shown that every
\newline$\gamma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =I\right\}  $ satisfies $\gamma\in\mu\left(
S_{k}\times S_{n-k}\right)  $. In other words,%
\[
\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}
\right)  =I\right\}  \subseteq\mu\left(  S_{k}\times S_{n-k}\right)  .
\]
In other words, the map $\mu$ is surjective.

So the map $\mu$ is both injective and surjective. In other words, $\mu$ is
bijective. In other words, $\mu$ is a bijection. In other words, the map%
\begin{align*}
S_{k}\times S_{n-k}  &  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =I\right\}  ,\\
\left(  \alpha,\beta\right)   &  \mapsto\sigma_{I,\alpha,\beta}%
\end{align*}
is a bijection\footnote{since $\mu$ is the map
\begin{align*}
S_{k}\times S_{n-k}  &  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =I\right\}  ,\\
\left(  \alpha,\beta\right)   &  \mapsto\sigma_{I,\alpha,\beta}%
\end{align*}
}. This solves Exercise \ref{exe.Ialbe} \textbf{(c)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.perm.cycles}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.perm.cycles}.]\textbf{(a)} This proof is going
to be long, but most of it will be spent unraveling the notations. If you find
Exercise \ref{exe.perm.cycles} \textbf{(a)} obvious, don't let this proof cast
doubt on your understanding.

Let $\sigma\in S_{n}$. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements
of $\left[  n\right]  $.

The map $\sigma$ is a permutation (since $\sigma\in S_{n}$), and therefore
bijective. Hence, in particular, $\sigma$ is injective.

For every $p\in\left\{  1,2,\ldots,k\right\}  $, let $j_{p}$ be the element
$\sigma\left(  i_{p}\right)  \in\left[  n\right]  $. Then, $\left(
j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $.
Furthermore, $j_{1},j_{2},\ldots,j_{k}$ are $k$ distinct elements of $\left[
n\right]  $\ \ \ \ \footnote{\textit{Proof.} The map $\sigma$ is injective.
Hence, the elements $\sigma\left(  i_{1}\right)  ,\sigma\left(  i_{2}\right)
,\ldots,\sigma\left(  i_{k}\right)  $ are distinct (since the elements
$i_{1},i_{2},\ldots,i_{k}$ are distinct). Thus, $\sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  $ are $k$
distinct elements of $\left[  n\right]  $. In other words, $j_{1},j_{2}%
,\ldots,j_{k}$ are $k$ distinct elements of $\left[  n\right]  $ (since
$\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $%
).}. Therefore, $\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ is a
well-defined permutation in $S_{n}$.

We have defined $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ to
be the permutation in $S_{n}$ which sends $i_{1},i_{2},\ldots,i_{k}$ to
$i_{2},i_{3},\ldots,i_{k},i_{1}$, respectively, while leaving all other
elements of $\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ sends $i_{1},i_{2},\ldots,i_{k}$ to $i_{2},i_{3},\ldots,i_{k},i_{1}$,
respectively. In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  i_{p}\right)
=i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.short.a.cyc-manifest.1}%
\end{equation}
where $i_{k+1}$ means $i_{1}$.

\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ leaves all other elements of $\left[  n\right]  $ fixed (where
\textquotedblleft other\textquotedblright\ means \textquotedblleft other than
$i_{1},i_{2},\ldots,i_{k}$\textquotedblright). In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
i_{1},i_{2},\ldots,i_{k}\right\}  .
\label{sol.perm.cycles.short.a.cyc-manifest.2}%
\end{equation}

\end{itemize}

Similarly, we can say the same about $\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}$:

\begin{itemize}
\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  j_{p}\right)
=j_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.short.a.cyc-manifest.1'}%
\end{equation}
where $j_{k+1}$ means $j_{1}$.

\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
j_{1},j_{2},\ldots,j_{k}\right\}  .
\label{sol.perm.cycles.short.a.cyc-manifest.2'}%
\end{equation}

\end{itemize}

In the following, we shall use the notation $i_{k+1}$ as a synonym for $i_{1}%
$, and the notation $j_{k+1}$ as a synonym for $j_{1}$. Then,%
\begin{equation}
j_{p}=\sigma\left(  i_{p}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,k+1\right\}  . \label{sol.perm.cycles.short.a.jp}%
\end{equation}
(Indeed, in the case when $p\in\left\{  1,2,\ldots,k\right\}  $, this follows
from the definition of $j_{p}$; but in the remaining case when $p=k+1$, it
follows from $j_{k+1}=j_{1}=\sigma\left(  \underbrace{i_{1}}_{=i_{k+1}%
}\right)  =\sigma\left(  i_{k+1}\right)  $.)

Now, let us show that
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
\label{sol.perm.cycles.short.a.qq}%
\end{equation}
for every $q\in\left[  n\right]  $.

\textit{Proof of (\ref{sol.perm.cycles.short.a.qq}):} Let $q\in\left[
n\right]  $. We must prove (\ref{sol.perm.cycles.short.a.qq}). We are in one
of the following two cases:

\textit{Case 1:} We have $q\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

\textit{Case 2:} We have $q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

Let us first consider Case 1. In this case, we have $q\in\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Thus, $q=j_{p}$ for some $p\in\left\{
1,2,\ldots,k\right\}  $. Consider this $p$. Clearly, $p+1\in\left\{
2,3,\ldots,k+1\right\}  \subseteq\left\{  1,2,\ldots,k+1\right\}  $. Hence,
applying (\ref{sol.perm.cycles.short.a.jp}) to $p+1$ instead of $p$, we obtain
$j_{p+1}=\sigma\left(  i_{p+1}\right)  $. But $q=j_{p}=\sigma\left(
i_{p}\right)  $ (by the definition of $j_{p}$) and thus $\sigma^{-1}\left(
q\right)  =i_{p}$. Hence,
\begin{align*}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)   &  =\sigma\left(
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
\underbrace{\sigma^{-1}\left(  q\right)  }_{=i_{p}}\right)  \right)
=\sigma\left(  \underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}\left(  i_{p}\right)  }_{\substack{=i_{p+1}\\\text{(by
(\ref{sol.perm.cycles.short.a.cyc-manifest.1}))}}}\right) \\
&  =\sigma\left(  i_{p+1}\right)  .
\end{align*}
Compared with%
\begin{align*}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  \underbrace{q}%
_{=j_{p}}\right)   &  =\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}\left(  j_{p}\right)  =j_{p+1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.short.a.cyc-manifest.1'})}\right) \\
&  =\sigma\left(  i_{p+1}\right)  ,
\end{align*}
this yields $\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}\right)  \left(  q\right)
=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)  $.
Thus, (\ref{sol.perm.cycles.short.a.qq}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $q\notin\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Hence, $\sigma^{-1}\left(  q\right)
\notin\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $\sigma
^{-1}\left(  q\right)  \in\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $. In
other words, there exists a $p\in\left\{  1,2,\ldots,k\right\}  $ such that
$\sigma^{-1}\left(  q\right)  =i_{p}$. Consider this $p$. We have $\sigma
^{-1}\left(  q\right)  =i_{p}$, thus $q=\sigma\left(  i_{p}\right)  =j_{p}$
(since $j_{p}$ is defined as $\sigma\left(  i_{p}\right)  $). Thus,
$q=j_{p}\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $; but this contradicts
$q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $. This contradiction shows
that our assumption was wrong, qed.}. Thus, $\sigma^{-1}\left(  q\right)
\in\left[  n\right]  \setminus\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $.
Therefore, (\ref{sol.perm.cycles.short.a.cyc-manifest.2}) (applied to
$\sigma^{-1}\left(  q\right)  $ instead of $q$) yields $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  \sigma^{-1}\left(  q\right)
\right)  =\sigma^{-1}\left(  q\right)  $. Hence,%
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\sigma\left(
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
\sigma^{-1}\left(  q\right)  \right)  }_{=\sigma^{-1}\left(  q\right)
}\right)  =\sigma\left(  \sigma^{-1}\left(  q\right)  \right)  =q.
\label{sol.perm.cycles.short.a.qq.pf.5}%
\end{equation}
On the other hand, $q\in\left[  n\right]  \setminus\left\{  j_{1},j_{2}%
,\ldots,j_{k}\right\}  $ (since $q\notin\left\{  j_{1},j_{2},\ldots
,j_{k}\right\}  $) and thus $\operatorname*{cyc}\nolimits_{j_{1},j_{2}%
,\ldots,j_{k}}\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.short.a.cyc-manifest.2'})). Compared with
(\ref{sol.perm.cycles.short.a.qq.pf.5}), this yields \newline$\left(
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}\left(  q\right)  $. Thus,
(\ref{sol.perm.cycles.short.a.qq}) is proven in Case 2.

We have now proven (\ref{sol.perm.cycles.short.a.qq}) in each of the two Cases
1 and 2. Therefore, (\ref{sol.perm.cycles.short.a.qq}) always holds.

From this, we conclude that $\sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}=\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}$ (because both $\sigma\circ\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma^{-1}$ and $\operatorname*{cyc}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ are maps from $\left[  n\right]  $ to
$\left[  n\right]  $). Hence,%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}=\operatorname*{cyc}\nolimits_{\sigma\left(  i_{1}\right)  ,\sigma\left(
i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  }%
\]
(since $\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(
i_{1}\right)  ,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)
\right)  $). This solves Exercise \ref{exe.perm.cycles} \textbf{(a)}.

\textbf{(b)} This is again a straightforward argument whose complexity stems
only from the number of cases that need to be considered. We shall try to
reduce the amount of brainless verification using some tricks, although at the
cost of making parts of the solution appear unmotivated.

Let $p\in\left\{  0,1,\ldots,n-k\right\}  $. Then, the elements
$p+1,p+2,\ldots,p+k$ belong to $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $. Hence, $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is
well-defined. We let $\sigma$ denote the permutation $\operatorname*{cyc}%
\nolimits_{p+1,p+2,\ldots,p+k}$.

The permutation $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is
defined to be the permutation in $S_{n}$ which sends $p+1,p+2,\ldots,p+k$ to
$p+2,p+3,\ldots,p+k,p+1$, respectively, while leaving all other elements of
$\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\sigma$ sends $p+1,p+2,\ldots,p+k$ to $p+2,p+3,\ldots
,p+k,p+1$, respectively. In other words, we have%
\begin{equation}
\left(  \sigma\left(  p+i\right)  =p+\left(  i+1\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,k-1\right\}
\right)  \label{sol.perm.cycles.short.b.sigma-manifest.1a}%
\end{equation}
and%
\begin{equation}
\sigma\left(  p+k\right)  =p+1.
\label{sol.perm.cycles.short.b.sigma-manifest.1b}%
\end{equation}


\item The permutation $\sigma$ leaves all other elements of $\left[  n\right]
$ fixed (where \textquotedblleft other\textquotedblright\ means
\textquotedblleft other than $p+1,p+2,\ldots,p+k$\textquotedblright). In other
words,
\begin{equation}
\sigma\left(  q\right)  =q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \setminus\left\{  p+1,p+2,\ldots,p+k\right\}  .
\label{sol.perm.cycles.short.b.sigma-manifest.2}%
\end{equation}

\end{itemize}

We can now observe that
\begin{equation}
\sigma\left(  q\right)  \geq q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \text{ satisfying }q\neq p+k \label{sol.perm.cycles.short.b.sigma.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.short.b.sigma.1}):} Let
$q\in\left[  n\right]  $ be such that $q\neq p+k$. We must prove that
$\sigma\left(  q\right)  \geq q$.
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $q\in\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
\textit{Case 2:} We have $q\notin\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
Let us first consider Case 1. In this case, we have $q\in\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q=p+i$ for some $i\in\left\{
1,2,\ldots,k\right\}  $. Consider this $i$. We have $q\neq p+k$, so that
$q-p\neq k$ and thus $k\neq q-p=i$ (since $q=p+i$). Therefore, $i\neq k$.
Combined with $i\in\left\{  1,2,\ldots,k\right\}  $, this shows that
$i\in\left\{  1,2,\ldots,k\right\}  \setminus\left\{  k\right\}  =\left\{
1,2,\ldots,k-1\right\}  $. Hence,
(\ref{sol.perm.cycles.short.b.sigma-manifest.1a}) yields $\sigma\left(
p+i\right)  =p+\underbrace{\left(  i+1\right)  }_{\geq i}\geq p+i=q$. Hence,
$\sigma\left(  \underbrace{q}_{=p+i}\right)  =\sigma\left(  p+i\right)  \geq
q$. We thus have proven $\sigma\left(  q\right)  \geq q$ in Case 1.
\par
Let us now consider Case 2. In this case, we have $q\notin\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $. Therefore, $\sigma\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.short.b.sigma-manifest.2})). Thus, $\sigma\left(
q\right)  \geq q$ is proven in Case 2.
\par
We have now proven $\sigma\left(  q\right)  \geq q$ in both Cases 1 and 2.
Therefore, $\sigma\left(  q\right)  \geq q$ always holds. This proves
(\ref{sol.perm.cycles.short.b.sigma.1}).}. Furthermore,%
\begin{equation}
\sigma\left(  q\right)  \leq q+1\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left[  n\right]  \label{sol.perm.cycles.short.b.sigma.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.short.b.sigma.2}):} Let
$q\in\left[  n\right]  $. We must prove that $\sigma\left(  q\right)  \leq
q+1$.
\par
Assume the contrary (for the sake of contradiction). Thus, $\sigma\left(
q\right)  >q+1$. If we had $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $, then we would have $\sigma\left(  q\right)  =q$
(by (\ref{sol.perm.cycles.short.b.sigma-manifest.2})), which would contradict
$\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have $q\in\left[  n\right]
\setminus\left\{  p+1,p+2,\ldots,p+k\right\}  $. We therefore have%
\[
q\in\left[  n\right]  \setminus\left(  \left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  \right)  \subseteq\left\{  p+1,p+2,\ldots
,p+k\right\}  .
\]
Hence, $q=p+i$ for some $i\in\left\{  1,2,\ldots,k\right\}  $. Consider this
$i$. Clearly, $i\geq1$ and $i\leq k$.
\par
If we had $i=k$, then we would have%
\begin{align*}
\sigma\left(  \underbrace{q}_{=p+i}\right)   &  =\sigma\left(
p+\underbrace{i}_{=k}\right)  =\sigma\left(  p+k\right)  =p+\underbrace{1}%
_{\leq i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.short.b.sigma-manifest.1b})}\right) \\
&  \leq p+i=q,
\end{align*}
which would contradict $\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have
$i=k$. Hence, $i\in\left\{  1,2,\ldots,k-1\right\}  $ (since $i\geq1$ and
$i\leq k$). Therefore, (\ref{sol.perm.cycles.short.b.sigma-manifest.1a})
yields $\sigma\left(  p+i\right)  =p+\left(  i+1\right)  =\underbrace{p+i}%
_{=q}+1=q+1$. This contradicts $\sigma\left(  \underbrace{p+i}_{=q}\right)
=\sigma\left(  q\right)  >q+1$. This contradiction proves that our assumption
was wrong. Hence, $\sigma\left(  q\right)  \leq q+1$ is proven.}.

Now, set%
\[
A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  .
\]
In other words,%
\[
A=\left\{  \left(  p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(
p+\left(  k-1\right)  ,p+k\right)  \right\}  .
\]
Thus, the set $A$ has $k-1$ elements (since the $k-1$ pairs \newline$\left(
p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(  p+\left(  k-1\right)
,p+k\right)  $ are clearly distinct). In other words, $\left\vert A\right\vert
=k-1$.

Now, let $\operatorname*{Inv}\sigma$ denote the set of all inversions of
$\sigma$. Recall that $\ell\left(  \sigma\right)  $ was defined as the number
of inversions of $\sigma$. In other words, $\ell\left(  \sigma\right)
=\left\vert \operatorname*{Inv}\sigma\right\vert $.

But $A\subseteq\operatorname*{Inv}\sigma$\ \ \ \ \footnote{\textit{Proof.} Let
$c\in A$. We shall show that $c\in\operatorname*{Inv}\sigma$.
\par
We have $c\in A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{
1,2,\ldots,k-1\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  p+h,p+k\right)  $ for some $h\in\left\{  1,2,\ldots,k-1\right\}  $.
Consider this $h$. From $h\in\left\{  1,2,\ldots,k-1\right\}  $, we obtain
$1\leq h\leq k-1$, so that $h\leq k-1<k$. Thus, $p+\underbrace{h}_{<k}<p+k$.
Moreover, $1\leq p+h$ (since $\underbrace{p}_{\geq0}+\underbrace{h}_{\geq
1}\geq0+1=1$) and $p+k\leq n$ (since $p\leq n-k$). Thus, $1\leq p+h<p+k\leq
n$.
\par
Applying (\ref{sol.perm.cycles.short.b.sigma-manifest.1a}) to $i=h$, we obtain
$\sigma\left(  p+h\right)  =p+\left(  \underbrace{h}_{\geq1>0}+1\right)
>p+\left(  0+1\right)  =p+1=\sigma\left(  p+k\right)  $ (by
(\ref{sol.perm.cycles.short.b.sigma-manifest.1b})).
\par
Now, $\left(  p+h,p+k\right)  $ is a pair of integers satisfying $1\leq
p+h<p+k\leq n$ and $\sigma\left(  p+h\right)  >\sigma\left(  p+k\right)  $. In
other words, $\left(  p+h,p+k\right)  $ is a pair of integers $\left(
i,j\right)  $ satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. In other words, $\left(  p+h,p+k\right)  $ is an
inversion of $\sigma$ (by the definition of \textquotedblleft inversion of
$\sigma$\textquotedblright). In other words, $\left(  p+h,p+k\right)
\in\operatorname*{Inv}\sigma$. Thus, $c=\left(  p+h,p+k\right)  \in
\operatorname*{Inv}\sigma$.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\sigma$ for every $c\in A$. In other words, $A\subseteq
\operatorname*{Inv}\sigma$, qed.} and $\operatorname*{Inv}\sigma\subseteq
A$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\sigma$. We
shall show that $c\in A$.
\par
We have $c\in\operatorname*{Inv}\sigma$. In other words, $c$ is an inversion
of $\sigma$. In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. Consider this $\left(  i,j\right)  $.
\par
We have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, so that
$\sigma\left(  i\right)  \geq\sigma\left(  j\right)  +1$ (since $\sigma\left(
i\right)  $ and $\sigma\left(  j\right)  $ are integers). Also, $i<j$, so that
$i\leq j-1$ (since $i$ and $j$ are integers). In other words, $i+1\leq j$. But
(\ref{sol.perm.cycles.short.b.sigma.2}) (applied to $q=i$) yields
$\sigma\left(  i\right)  \leq i+1\leq j$.
\par
Let us first show that $j=p+k$. Indeed, let us assume the contrary (for the
sake of contradiction). Thus, $j\neq p+k$. Hence, $\sigma\left(  j\right)
\geq j$ (by (\ref{sol.perm.cycles.short.b.sigma.1}), applied to $q=j$). Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  \geq j$. This contradicts
$\sigma\left(  i\right)  \leq j$. This contradiction shows that our assumption
was wrong. Hence, $j=p+k$ is proven.
\par
Now, $\sigma\left(  \underbrace{j}_{=p+k}\right)  =\sigma\left(  p+k\right)
=p+1$ (by (\ref{sol.perm.cycles.short.b.sigma-manifest.1b})). Hence,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  =p+1$. Therefore,
$p+1<\sigma\left(  i\right)  \leq i+1$. Subtracting $1$ from both sides of
this inequality, we obtain $p<i$. Hence, $i>p$, so that $i\geq p+1$ (since $i$
and $p$ are integers). Combined with $i<j=p+k$, this yields $i\in\left\{
p+1,p+2,\ldots,p+k-1\right\}  $. Thus, $i-p\in\left\{  1,2,\ldots,k-1\right\}
$.
\par
So we know that the element $i-p\in\left\{  1,2,\ldots,k-1\right\}  $
satisfies $c=\left(  \underbrace{i}_{=p+\left(  i-p\right)  },\underbrace{j}%
_{=p+k}\right)  =\left(  p+\left(  i-p\right)  ,p+k\right)  $. Hence, there
exists an $h\in\left\{  1,2,\ldots,k-1\right\}  $ such that $c=\left(
p+h,p+k\right)  $ (namely, $h=i-p$). Thus,%
\[
c\in\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  =A.
\]
\par
Now, let us forget that we fixed $c$. We thus have proven that $c\in A$ for
every $c\in\operatorname*{Inv}\sigma$. In other words, $\operatorname*{Inv}%
\sigma\subseteq A$, qed.}. Combining these two relations, we obtain
$A=\operatorname*{Inv}\sigma$. Hence, $\left\vert A\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert $. Compared with $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert $, this yields
$\ell\left(  \sigma\right)  =\left\vert A\right\vert =k-1$. This rewrites as
$\ell\left(  \operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\right)  =k-1$
(since $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$). This
solves Exercise \ref{exe.perm.cycles} \textbf{(b)}.

\textbf{(c)} This one is tricky. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$
distinct elements of $\left[  n\right]  $. We extend the $k$-tuple $\left(
i_{1},i_{2},\ldots,i_{k}\right)  $ to an infinite sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ of elements of $\left[  n\right]  $ by
setting%
\[
\left(  i_{u}=i_{\left(  \text{the element }u^{\prime}\in\left\{
1,2,\ldots,k\right\}  \text{ satisfying }u^{\prime}\equiv u\operatorname{mod}%
k\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1\right)  .
\]
This sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic with
period $k$. In other words,%
\begin{equation}
i_{u}=i_{u+k}\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1.
\label{sol.perm.cycles.short.c.period}%
\end{equation}
From this, it is easy to obtain that%
\[
i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}+\cdots
+i_{k+2}=\cdots
\]
\footnote{\textit{Proof.} Every $u\in\left\{  1,2,3,\ldots\right\}  $
satisfies%
\begin{align*}
i_{u}+i_{u+1}+\cdots+i_{u+k-1}  &  =\underbrace{i_{u}}_{\substack{=i_{u+k}%
\\\text{(by (\ref{sol.perm.cycles.short.c.period}))}}}+\left(  i_{u+1}%
+i_{u+2}+\cdots+i_{u+k-1}\right) \\
&  =i_{u+k}+\left(  i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  =\left(
i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  +i_{u+k}\\
&  =i_{u+1}+i_{u+2}+\cdots+i_{u+k}.
\end{align*}
Thus, $i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}%
+\cdots+i_{k+2}=\cdots$, qed.}. Thus,%
\begin{equation}
i_{r+1}+i_{r+2}+\cdots+i_{r+k}=i_{1}+i_{2}+\cdots+i_{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }r\in\mathbb{N}.
\label{sol.perm.cycles.short.c.periodsum}%
\end{equation}


Let $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$. Let
$\operatorname*{Inv}\sigma$ denote the set of all inversions of $\sigma$.
Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $. (This can be seen as in the solution to Exercise
\ref{exe.perm.cycles} \textbf{(b)}.) Moreover, the definitions of the sequence
$\left(  i_{1},i_{2},i_{3},\ldots\right)  $ and of $\sigma$ show that%
\begin{equation}
\sigma\left(  i_{p}\right)  =i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\geq1. \label{sol.perm.cycles.short.c.sigma}%
\end{equation}


Now, fix $r\in\left\{  1,2,\ldots,k-1\right\}  $. We shall prove that
\begin{equation}
\text{there exists some }u\geq1\text{ such that }\left(  i_{u},i_{u+r}\right)
\in\operatorname*{Inv}\sigma. \label{sol.perm.cycles.short.c.mainclaim}%
\end{equation}


\textit{Proof of (\ref{sol.perm.cycles.short.c.mainclaim}):} The sequence
$\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic with period $k$, and
its first $k$ entries $i_{1},i_{2},\ldots,i_{k}$ are distinct. Hence, each
entry of this sequence repeats itself each $k$ steps, but not more often.
Hence, every integer $u\geq1$ satisfies $i_{u+r}\neq i_{u}$ (since
$r\in\left\{  1,2,\ldots,k-1\right\}  $). In other words, every $u\geq1$
satisfies%
\begin{equation}
i_{u+r}-i_{u}\neq0. \label{sol.perm.cycles.short.c.mainclaim.pf.4}%
\end{equation}
The $k$-tuple $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2},\ldots,i_{k+r}%
-i_{k}\right)  $ contains at least one positive entry\footnote{\textit{Proof.}
Assume the contrary. Thus, the $k$-tuple $\left(  i_{1+r}-i_{1},i_{2+r}%
-i_{2},\ldots,i_{k+r}-i_{k}\right)  $ contains no positive entries. In other
words, no $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies $i_{u+r}-i_{u}>0$.
In other words, every $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies
$i_{u+r}-i_{u}\leq0$. But (\ref{sol.perm.cycles.short.c.mainclaim.pf.4}) shows
that every $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies $i_{u+r}-i_{u}%
\neq0$. Thus, every $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies
$i_{u+r}-i_{u}<0$ (since $i_{u+r}-i_{u}\leq0$ and $i_{u+r}-i_{u}\neq0$).
Hence,%
\[
\sum_{u=1}^{k}\underbrace{\left(  i_{u+r}-i_{u}\right)  }_{<0}<\sum_{u=1}%
^{k}0=0.
\]
But this contradicts%
\begin{align*}
\sum_{u=1}^{k}\left(  \underbrace{i_{u+r}}_{=i_{r+u}}-i_{u}\right)   &
=\sum_{u=1}^{k}\left(  i_{r+u}-i_{u}\right)  =\left(  \sum_{u=1}^{k}%
i_{r+u}\right)  -\left(  \sum_{u=1}^{k}i_{u}\right) \\
&  =\left(  i_{r+1}+i_{r+2}+\cdots+i_{r+k}\right)  -\left(  i_{1}+i_{2}%
+\cdots+i_{k}\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.short.c.periodsum})}\right)  .
\end{align*}
This contradiction shows that our assumption was wrong, qed.}, and at least
one negative entry\footnote{This is proven similarly.}. Hence, there exists at
least one $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(  u+1\right)
+r}-i_{u+1}<0$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then,
there exists no $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(
u+1\right)  +r}-i_{u+1}<0$. Hence,%
\begin{equation}
\text{every }u\geq1\text{ satisfying }i_{u+r}-i_{u}>0\text{ must satisfy
}i_{\left(  u+1\right)  +r}-i_{u+1}\geq0.
\label{sol.perm.cycles.short.c.mainclaim.pf.5o}%
\end{equation}
Therefore,%
\begin{equation}
\text{every }u\geq1\text{ satisfying }i_{u+r}-i_{u}>0\text{ must satisfy
}i_{\left(  u+1\right)  +r}-i_{u+1}>0
\label{sol.perm.cycles.short.c.mainclaim.pf.5}%
\end{equation}
(because (\ref{sol.perm.cycles.short.c.mainclaim.pf.5o}) shows that
$i_{\left(  u+1\right)  +r}-i_{u+1}\geq0$; but combining this with $i_{\left(
u+1\right)  +r}-i_{u+1}\neq0$ (which follows from
(\ref{sol.perm.cycles.short.c.mainclaim.pf.4}), applied to $u+1$ instead of
$u$), we obtain $i_{\left(  u+1\right)  +r}-i_{u+1}>0$).
\par
But there exists some $v\in\left\{  1,2,\ldots,k\right\}  $ satisfying
$i_{v+r}-i_{v}>0$ (since the $k$-tuple $\left(  i_{1+r}-i_{1},i_{2+r}%
-i_{2},\ldots,i_{k+r}-i_{k}\right)  $ contains at least one positive entry).
Consider this $v$. Then, we have $i_{v+r}-i_{v}>0$, therefore $i_{\left(
v+1\right)  +r}-i_{v+1}>0$ (by (\ref{sol.perm.cycles.short.c.mainclaim.pf.5}),
applied to $u=v$), therefore $i_{\left(  v+2\right)  +r}-i_{v+2}>0$ (by
(\ref{sol.perm.cycles.short.c.mainclaim.pf.5}), applied to $u=v+1$), therefore
$i_{\left(  v+3\right)  +r}-i_{v+3}>0$ (by
(\ref{sol.perm.cycles.short.c.mainclaim.pf.5}), applied to $u=v+2$), and so
on. Altogether, we thus obtain
\[
i_{h+r}-i_{h}>0\ \ \ \ \ \ \ \ \ \ \text{for every }h\geq v.
\]
In other words, $i_{h}<i_{h+r}$ for every $h\geq v$. Hence, $i_{v}%
<i_{v+r}<i_{v+2r}<i_{v+3r}<\cdots$. Thus, the numbers $i_{v},i_{v+r}%
,i_{v+2r},i_{v+3r},\ldots$ are pairwise distinct; hence, the sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ contains infinitely many distinct entries.
But this contradicts the fact that this sequence is periodic. This
contradiction proves that our assumption was wrong, qed.}. Consider this $u$.
We have $i_{u}<i_{u+r}$ (since $i_{u+r}-i_{u}>0$), so that $1\leq
i_{u}<i_{u+r}\leq n$. Also, (\ref{sol.perm.cycles.short.c.sigma}) (applied to
$p=u$) yields $\sigma\left(  i_{u}\right)  =i_{u+1}$. Moreover,
(\ref{sol.perm.cycles.short.c.sigma}) (applied to $p=u+r$) yields
$\sigma\left(  i_{u+r}\right)  =i_{u+r+1}=i_{\left(  u+1\right)  +r}$. Hence,
\begin{align*}
\sigma\left(  i_{u}\right)   &  =i_{u+1}>i_{\left(  u+1\right)  +r}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i_{\left(  u+1\right)  +r}%
-i_{u+1}<0\right) \\
&  =\sigma\left(  i_{u+r}\right)  .
\end{align*}
So we know that $\left(  i_{u},i_{u+r}\right)  $ is a pair of integers
satisfying $1\leq i_{u}<i_{u+r}\leq n$ and $\sigma\left(  i_{u}\right)
>\sigma\left(  i_{u+r}\right)  $. In other words, $\left(  i_{u}%
,i_{u+r}\right)  $ is an inversion of $\sigma$. In other words, $\left(
i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Thus, we have found a
$u\geq1$ such that $\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}%
\sigma$. This proves (\ref{sol.perm.cycles.short.c.mainclaim}).

Now, let us forget that we fixed $r$. We have shown that, for every
$r\in\left\{  1,2,\ldots,k-1\right\}  $, there exists some $u\geq1$ such that
$\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Let us denote
this $u$ by $u_{r}$. Therefore, for every $r\in\left\{  1,2,\ldots
,k-1\right\}  $, we have found a $u_{r}\geq1$ such that $\left(  i_{u_{r}%
},i_{u_{r}+r}\right)  \in\operatorname*{Inv}\sigma$. The $k-1$ pairs%
\[
\left(  i_{u_{1}},i_{u_{1}+1}\right)  ,\ \left(  i_{u_{2}},i_{u_{2}+2}\right)
,\ \ldots,\ \left(  i_{u_{k-1}},i_{u_{k-1}+\left(  k-1\right)  }\right)
\]
are pairwise distinct\footnote{\textit{Proof.} Assume the contrary. Then,
there exist two distinct elements $x$ and $y$ of $\left\{  1,2,\ldots
,k-1\right\}  $ such that $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(
i_{u_{y}},i_{u_{y}+y}\right)  $. Consider these $x$ and $y$.
\par
We have $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(  i_{u_{y}},i_{u_{y}%
+y}\right)  $. In other words, $i_{u_{x}}=i_{u_{y}}$ and $i_{u_{x}+x}%
=i_{u_{y}+y}$. Since the numbers $i_{1},i_{2},\ldots,i_{k}$ are distinct (and
the sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ consists of these
numbers, repeated over and over), we obtain $u_{x}\equiv u_{y}%
\operatorname{mod}k$ from $i_{u_{x}}=i_{u_{y}}$, and we obtain $u_{x}+x\equiv
u_{y}+y\operatorname{mod}k$ from $i_{u_{x}+x}=i_{u_{y}+y}$. Subtracting the
congruence $u_{x}\equiv u_{y}\operatorname{mod}k$ from the congruence
$u_{x}+x\equiv u_{y}+y\operatorname{mod}k$, we obtain $x\equiv
y\operatorname{mod}k$. In light of $x,y\in\left\{  1,2,\ldots,k-1\right\}  $,
this shows that $x=y$. But this contradicts the fact that $x$ and $y$ are
distinct. This contradiction proves that our assumption was wrong, qed.}, and
all of them belong to $\operatorname*{Inv}\sigma$. Hence, the set
$\operatorname*{Inv}\sigma$ has at least $k-1$ elements. In other words,
$\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$. Thus, $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$.
Since $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$, this
rewrites as $\ell\left(  \operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}\right)  \geq k-1$. This solves Exercise \ref{exe.perm.cycles}
\textbf{(c)}.

\textbf{(d)} Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of
$\left[  n\right]  $. Hence, Proposition \ref{prop.perms.lists} \textbf{(c)}
(applied to $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  $) yields that there exists a permutation
$\sigma\in S_{n}$ such that $\left(  i_{1},i_{2},\ldots,i_{k}\right)  =\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
k\right)  \right)  $. Consider such a $\sigma$.

Exercise \ref{exe.perm.cycles} \textbf{(b)} yields $\ell\left(
\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)  =k-1$. But the definition
of $\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}$ yields
$\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}=\left(
-1\right)  ^{\ell\left(  \operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)
}=\left(  -1\right)  ^{k-1}$ (since $\ell\left(  \operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}\right)  =k-1$).

Exercise \ref{exe.perm.cycles} \textbf{(a)} (applied to $1,2,\ldots,k$ instead
of $i_{1},i_{2},\ldots,i_{k}$) yields%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma
^{-1}=\operatorname*{cyc}\nolimits_{\sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  k\right)  }=\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
\]
(since $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  k\right)  \right)  =\left(  i_{1},i_{2},\ldots
,i_{k}\right)  $). Hence,
\[
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}%
_{=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma^{-1}%
}\circ\sigma=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}%
\circ\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}}=\sigma
\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}.
\]
Thus,%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\sigma\circ\operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}}=\left(  -1\right)  ^{\sigma}\cdot\underbrace{\left(
-1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}}_{=\left(
-1\right)  ^{k-1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
\tau=\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{k-1}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}}\cdot\left(  -1\right)  ^{\sigma}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\text{ and }%
\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}},
\end{align*}
this yields $\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{k-1}$. We can cancel $\left(  -1\right)
^{\sigma}$ from this equality (since $\left(  -1\right)  ^{\sigma}\in\left\{
1,-1\right\}  $ is a nonzero integer), and thus obtain $\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{k-1}$. This solves Exercise \ref{exe.perm.cycles} \textbf{(d)}.

[\textit{Remark:} Exercise \ref{exe.perm.cycles} \textbf{(d)} can also be
solved in a different way, namely by arguing that%
\[
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}=t_{i_{1},i_{2}}\circ
t_{i_{2},i_{3}}\circ\cdots\circ t_{i_{k-1},i_{k}}%
\]
(using the notations of Definition \ref{def.transpos}) and then by applying
the equality $\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ multiple times. We leave the details
of this alternative proof to the curious reader. (That said, this alternative
proof is also the most popular proof, so it is easily found in textbooks.)]
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.perm.cycles}.]\textbf{(a)} Let $\sigma\in
S_{n}$. Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of $\left[
n\right]  $.

The map $\sigma$ is a permutation (since $\sigma\in S_{n}$), and therefore
bijective. Hence, in particular, $\sigma$ is injective.

For every $p\in\left\{  1,2,\ldots,k\right\}  $, let $j_{p}$ be the element
$\sigma\left(  i_{p}\right)  \in\left[  n\right]  $. Then, $\left(
j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $.
Furthermore, $j_{1},j_{2},\ldots,j_{k}$ are $k$ distinct elements of $\left[
n\right]  $\ \ \ \ \footnote{\textit{Proof.} The map $\sigma$ is injective.
Hence, the elements $\sigma\left(  i_{1}\right)  ,\sigma\left(  i_{2}\right)
,\ldots,\sigma\left(  i_{k}\right)  $ are distinct (since the elements
$i_{1},i_{2},\ldots,i_{k}$ are distinct). Thus, $\sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  $ are $k$
distinct elements of $\left[  n\right]  $. In other words, $j_{1},j_{2}%
,\ldots,j_{k}$ are $k$ distinct elements of $\left[  n\right]  $ (since
$\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(  i_{1}\right)
,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  \right)  $%
).}. Therefore, $\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ is a
well-defined permutation in $S_{n}$.

We have defined $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$ to
be the permutation in $S_{n}$ which sends $i_{1},i_{2},\ldots,i_{k}$ to
$i_{2},i_{3},\ldots,i_{k},i_{1}$, respectively, while leaving all other
elements of $\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ sends $i_{1},i_{2},\ldots,i_{k}$ to $i_{2},i_{3},\ldots,i_{k},i_{1}$,
respectively. In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  i_{p}\right)
=i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.a.cyc-manifest.1}%
\end{equation}
where $i_{k+1}$ means $i_{1}$.

\item The permutation $\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}$ leaves all other elements of $\left[  n\right]  $ fixed (where
\textquotedblleft other\textquotedblright\ means \textquotedblleft other than
$i_{1},i_{2},\ldots,i_{k}$\textquotedblright). In other words,%
\begin{equation}
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
i_{1},i_{2},\ldots,i_{k}\right\}  . \label{sol.perm.cycles.a.cyc-manifest.2}%
\end{equation}

\end{itemize}

Furthermore:

\begin{itemize}
\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  j_{p}\right)
=j_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{  1,2,\ldots
,k\right\}  , \label{sol.perm.cycles.a.cyc-manifest.1'}%
\end{equation}
where $j_{k+1}$ means $j_{1}$. (This is proven in the same way as
(\ref{sol.perm.cycles.a.cyc-manifest.1}), except that every $i_{x}$ is
replaced by $j_{x}$.)

\item We have%
\begin{equation}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
j_{1},j_{2},\ldots,j_{k}\right\}  . \label{sol.perm.cycles.a.cyc-manifest.2'}%
\end{equation}
(This is proven in the same way as (\ref{sol.perm.cycles.a.cyc-manifest.2}),
except that every $i_{x}$ is replaced by $j_{x}$.)
\end{itemize}

In the following, we shall use the notation $i_{k+1}$ as a synonym for $i_{1}%
$, and the notation $j_{k+1}$ as a synonym for $j_{1}$. Then,%
\begin{equation}
j_{p}=\sigma\left(  i_{p}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
p\in\left\{  1,2,\ldots,k+1\right\}  \label{sol.perm.cycles.a.jp}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.a.jp}):} Let $p\in\left\{
1,2,\ldots,k+1\right\}  $. We need to prove that $j_{p}=\sigma\left(
i_{p}\right)  $. If $p\in\left\{  1,2,\ldots,k\right\}  $, then this follows
from the definition of $j_{p}$. Hence, for the rest of this proof, we can WLOG
assume that we don't have $p\in\left\{  1,2,\ldots,k\right\}  $. Assume this.
\par
We have $p\in\left\{  1,2,\ldots,k+1\right\}  $, but we don't have
$p\in\left\{  1,2,\ldots,k\right\}  $. Hence, we have $p\in\left\{
1,2,\ldots,k+1\right\}  \setminus\left\{  1,2,\ldots,k\right\}  =\left\{
k+1\right\}  $. In other words, $p=k+1$, so that $i_{p}=i_{k+1}=i_{1}$ and
thus $\sigma\left(  i_{p}\right)  =\sigma\left(  i_{1}\right)  $. On the other
hand, from $p=k+1$, we obtain $j_{p}=j_{k+1}=j_{1}=\sigma\left(  i_{1}\right)
$ (by the definition of $j_{1}$). Compared with $\sigma\left(  i_{p}\right)
=\sigma\left(  i_{1}\right)  $, this yields $j_{p}=\sigma\left(  i_{p}\right)
$. Thus, $j_{p}=\sigma\left(  i_{p}\right)  $ is proven.}.

Now, let us show that
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)
\label{sol.perm.cycles.a.qq}%
\end{equation}
for every $q\in\left[  n\right]  $.

\textit{Proof of (\ref{sol.perm.cycles.a.qq}):} Let $q\in\left[  n\right]  $.
We must prove (\ref{sol.perm.cycles.a.qq}). We are in one of the following two cases:

\textit{Case 1:} We have $q\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

\textit{Case 2:} We have $q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $.

Let us first consider Case 1. In this case, we have $q\in\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Thus, $q=j_{p}$ for some $p\in\left\{
1,2,\ldots,k\right\}  $. Consider this $p$. Clearly, $p+1\in\left\{
2,3,\ldots,k+1\right\}  \subseteq\left\{  1,2,\ldots,k+1\right\}  $. Hence,
applying (\ref{sol.perm.cycles.a.jp}) to $p+1$ instead of $p$, we obtain
$j_{p+1}=\sigma\left(  i_{p+1}\right)  $. But $q=j_{p}=\sigma\left(
i_{p}\right)  $ (by the definition of $j_{p}$) and thus $\sigma^{-1}\left(
q\right)  =i_{p}$. Hence,
\begin{align*}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)   &  =\sigma\left(
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
\underbrace{\sigma^{-1}\left(  q\right)  }_{=i_{p}}\right)  \right)
=\sigma\left(  \underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}\left(  i_{p}\right)  }_{\substack{=i_{p+1}\\\text{(by
(\ref{sol.perm.cycles.a.cyc-manifest.1}))}}}\right) \\
&  =\sigma\left(  i_{p+1}\right)  .
\end{align*}
Compared with%
\begin{align*}
\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  \underbrace{q}%
_{=j_{p}}\right)   &  =\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}\left(  j_{p}\right)  =j_{p+1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.a.cyc-manifest.1'})}\right) \\
&  =\sigma\left(  i_{p+1}\right)  ,
\end{align*}
this yields $\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}\right)  \left(  q\right)
=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)  $.
Thus, (\ref{sol.perm.cycles.a.qq}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $q\notin\left\{  j_{1}%
,j_{2},\ldots,j_{k}\right\}  $. Hence, $\sigma^{-1}\left(  q\right)
\notin\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $\sigma
^{-1}\left(  q\right)  \in\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $. In
other words, there exists a $p\in\left\{  1,2,\ldots,k\right\}  $ such that
$\sigma^{-1}\left(  q\right)  =i_{p}$. Consider this $p$. We have $\sigma
^{-1}\left(  q\right)  =i_{p}$, thus $q=\sigma\left(  i_{p}\right)  =j_{p}$
(since $j_{p}$ is defined as $\sigma\left(  i_{p}\right)  $). Thus,
$q=j_{p}\in\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $; but this contradicts
$q\notin\left\{  j_{1},j_{2},\ldots,j_{k}\right\}  $. This contradiction shows
that our assumption was wrong, qed.}. Thus, $\sigma^{-1}\left(  q\right)
\in\left[  n\right]  \setminus\left\{  i_{1},i_{2},\ldots,i_{k}\right\}  $.
Therefore, (\ref{sol.perm.cycles.a.cyc-manifest.2}) (applied to $\sigma
^{-1}\left(  q\right)  $ instead of $q$) yields $\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(  \sigma^{-1}\left(  q\right)
\right)  =\sigma^{-1}\left(  q\right)  $. Hence,%
\begin{equation}
\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma^{-1}\right)  \left(  q\right)  =\sigma\left(
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\left(
\sigma^{-1}\left(  q\right)  \right)  }_{=\sigma^{-1}\left(  q\right)
}\right)  =\sigma\left(  \sigma^{-1}\left(  q\right)  \right)  =q.
\label{sol.perm.cycles.a.qq.pf.5}%
\end{equation}
On the other hand, $q\in\left[  n\right]  \setminus\left\{  j_{1},j_{2}%
,\ldots,j_{k}\right\}  $ (since $q\notin\left\{  j_{1},j_{2},\ldots
,j_{k}\right\}  $) and thus $\operatorname*{cyc}\nolimits_{j_{1},j_{2}%
,\ldots,j_{k}}\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.a.cyc-manifest.2'})). Compared with
(\ref{sol.perm.cycles.a.qq.pf.5}), this yields \newline$\left(  \sigma
\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma
^{-1}\right)  \left(  q\right)  =\operatorname*{cyc}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}\left(  q\right)  $. Thus, (\ref{sol.perm.cycles.a.qq}) is
proven in Case 2.

We have now proven (\ref{sol.perm.cycles.a.qq}) in each of the two Cases 1 and
2. Therefore, (\ref{sol.perm.cycles.a.qq}) always holds.

Thus, we know that $\left(  \sigma\circ\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}\circ\sigma^{-1}\right)  \left(  q\right)
=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}\left(  q\right)  $
for every $q\in\left[  n\right]  $. In other words, $\sigma\circ
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ\sigma
^{-1}=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ (because both
$\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}$ and $\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}}$ are
maps from $\left[  n\right]  $ to $\left[  n\right]  $). Hence,%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\circ
\sigma^{-1}=\operatorname*{cyc}\nolimits_{j_{1},j_{2},\ldots,j_{k}%
}=\operatorname*{cyc}\nolimits_{\sigma\left(  i_{1}\right)  ,\sigma\left(
i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)  }%
\]
(since $\left(  j_{1},j_{2},\ldots,j_{k}\right)  =\left(  \sigma\left(
i_{1}\right)  ,\sigma\left(  i_{2}\right)  ,\ldots,\sigma\left(  i_{k}\right)
\right)  $). This solves Exercise \ref{exe.perm.cycles} \textbf{(a)}.

\textbf{(b)} Let $p\in\left\{  0,1,\ldots,n-k\right\}  $. Then, $p\geq0$ and
$p\leq n-k$. Hence,
\[
\left\{  p+1,p+2,\ldots,p+k\right\}  \subseteq\left\{  1,2,\ldots,n\right\}
\]
(since $\underbrace{p}_{\geq0}+1\geq1$ and $\underbrace{p}_{\leq n-k}+k\leq
n-k+k=n$). Thus, the elements $p+1,p+2,\ldots,p+k$ belong to $\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence, $\operatorname*{cyc}%
\nolimits_{p+1,p+2,\ldots,p+k}$ is well-defined. We let $\sigma$ denote the
permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$.

The permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$ is defined
to be the permutation in $S_{n}$ which sends $p+1,p+2,\ldots,p+k$ to
$p+2,p+3,\ldots,p+k,p+1$, respectively, while leaving all other elements of
$\left[  n\right]  $ fixed. Therefore:

\begin{itemize}
\item The permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$
sends $p+1,p+2,\ldots,p+k$ to $p+2,p+3,\ldots,p+k,p+1$, respectively. In other
words, the permutation $\sigma$ sends $p+1,p+2,\ldots,p+k$ to $p+2,p+3,\ldots
,p+k,p+1$, respectively (since $\sigma=\operatorname*{cyc}%
\nolimits_{p+1,p+2,\ldots,p+k}$). In other words, we have%
\begin{equation}
\left(  \sigma\left(  p+i\right)  =p+\left(  i+1\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,k-1\right\}
\right)  \label{sol.perm.cycles.b.sigma-manifest.1a}%
\end{equation}
and%
\begin{equation}
\sigma\left(  p+k\right)  =p+1. \label{sol.perm.cycles.b.sigma-manifest.1b}%
\end{equation}


\item The permutation $\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$
leaves all other elements of $\left[  n\right]  $ fixed (where
\textquotedblleft other\textquotedblright\ means \textquotedblleft other than
$p+1,p+2,\ldots,p+k$\textquotedblright). In other words,
\[
\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\left(  q\right)
=q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  .
\]
In other words,%
\begin{equation}
\sigma\left(  q\right)  =q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \setminus\left\{  p+1,p+2,\ldots,p+k\right\}
\label{sol.perm.cycles.b.sigma-manifest.2}%
\end{equation}
(since $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$).
\end{itemize}

We can now observe that
\begin{equation}
\sigma\left(  q\right)  \geq q\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left[
n\right]  \text{ satisfying }q\neq p+k \label{sol.perm.cycles.b.sigma.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.b.sigma.1}):} Let
$q\in\left[  n\right]  $ be such that $q\neq p+k$. We must prove that
$\sigma\left(  q\right)  \geq q$.
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $q\in\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
\textit{Case 2:} We have $q\notin\left\{  p+1,p+2,\ldots,p+k\right\}  $.
\par
Let us first consider Case 1. In this case, we have $q\in\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q=p+i$ for some $i\in\left\{
1,2,\ldots,k\right\}  $. Consider this $i$. We have $q\neq p+k$, so that
$q-p\neq k$ and thus $k\neq q-p=i$ (since $q=p+i$). Therefore, $i\neq k$.
Combined with $i\in\left\{  1,2,\ldots,k\right\}  $, this shows that
$i\in\left\{  1,2,\ldots,k\right\}  \setminus\left\{  k\right\}  =\left\{
1,2,\ldots,k-1\right\}  $. Hence, (\ref{sol.perm.cycles.b.sigma-manifest.1a})
yields $\sigma\left(  p+i\right)  =p+\underbrace{\left(  i+1\right)  }_{\geq
i}\geq p+i=q$. Hence, $\sigma\left(  \underbrace{q}_{=p+i}\right)
=\sigma\left(  p+i\right)  \geq q$. We thus have proven $\sigma\left(
q\right)  \geq q$ in Case 1.
\par
Let us now consider Case 2. In this case, we have $q\notin\left\{
p+1,p+2,\ldots,p+k\right\}  $. Hence, $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $. Therefore, $\sigma\left(  q\right)  =q$ (by
(\ref{sol.perm.cycles.b.sigma-manifest.2})). Thus, $\sigma\left(  q\right)
\geq q$ is proven in Case 2.
\par
We have now proven $\sigma\left(  q\right)  \geq q$ in both Cases 1 and 2.
Therefore, $\sigma\left(  q\right)  \geq q$ always holds. This proves
(\ref{sol.perm.cycles.b.sigma.1}).}. Furthermore,%
\begin{equation}
\sigma\left(  q\right)  \leq q+1\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left[  n\right]  \label{sol.perm.cycles.b.sigma.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.perm.cycles.b.sigma.2}):} Let
$q\in\left[  n\right]  $. We must prove that $\sigma\left(  q\right)  \leq
q+1$.
\par
Assume the contrary (for the sake of contradiction). Thus, $\sigma\left(
q\right)  >q+1$. If we had $q\in\left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  $, then we would have $\sigma\left(  q\right)  =q$
(by (\ref{sol.perm.cycles.b.sigma-manifest.2})), which would contradict
$\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have $q\in\left[  n\right]
\setminus\left\{  p+1,p+2,\ldots,p+k\right\}  $. We therefore have%
\[
q\in\left[  n\right]  \setminus\left(  \left[  n\right]  \setminus\left\{
p+1,p+2,\ldots,p+k\right\}  \right)  \subseteq\left\{  p+1,p+2,\ldots
,p+k\right\}  .
\]
Hence, $q=p+i$ for some $i\in\left\{  1,2,\ldots,k\right\}  $. Consider this
$i$. Clearly, $i\geq1$ and $i\leq k$.
\par
If we had $i=k$, then we would have%
\begin{align*}
\sigma\left(  \underbrace{q}_{=p+i}\right)   &  =\sigma\left(
p+\underbrace{i}_{=k}\right)  =\sigma\left(  p+k\right)  =p+\underbrace{1}%
_{\leq i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.b.sigma-manifest.1b})}\right) \\
&  \leq p+i=q,
\end{align*}
which would contradict $\sigma\left(  q\right)  >q+1>q$. Thus, we cannot have
$i=k$. We thus have $i\neq k$. Combined with $i\in\left\{  1,2,\ldots
,k\right\}  $, this shows that $i\in\left\{  1,2,\ldots,k\right\}
\setminus\left\{  k\right\}  =\left\{  1,2,\ldots,k-1\right\}  $. Hence,
(\ref{sol.perm.cycles.b.sigma-manifest.1a}) yields $\sigma\left(  p+i\right)
=p+\left(  i+1\right)  =\underbrace{p+i}_{=q}+1=q+1$. This contradicts
$\sigma\left(  \underbrace{p+i}_{=q}\right)  =\sigma\left(  q\right)  >q+1$.
This contradiction proves that our assumption was wrong. Hence, $\sigma\left(
q\right)  \leq q+1$ is proven. We are thus done proving
(\ref{sol.perm.cycles.b.sigma.2}).}.

Now, set%
\[
A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  .
\]
In other words,%
\[
A=\left\{  \left(  p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(
p+\left(  k-1\right)  ,p+k\right)  \right\}  .
\]
Thus, the set $A$ has $k-1$ elements (since the $k-1$ pairs \newline$\left(
p+1,p+k\right)  ,\left(  p+2,p+k\right)  ,\ldots,\left(  p+\left(  k-1\right)
,p+k\right)  $ are clearly distinct). In other words, $\left\vert A\right\vert
=k-1$.

Now, let $\operatorname*{Inv}\sigma$ denote the set of all inversions of
$\sigma$. Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $\ \ \ \ \footnote{\textit{Proof.} Recall that $\ell\left(
\sigma\right)  $ is defined as the number of inversions of $\sigma$. Thus,%
\[
\ell\left(  \sigma\right)  =\left(  \text{the number of inversions of }%
\sigma\right)  =\left\vert \underbrace{\left(  \text{the set of all inversions
of }\sigma\right)  }_{=\operatorname*{Inv}\sigma}\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert ,
\]
qed.}.

But $A\subseteq\operatorname*{Inv}\sigma$\ \ \ \ \footnote{\textit{Proof.} Let
$c\in A$. We shall show that $c\in\operatorname*{Inv}\sigma$.
\par
We have $c\in A=\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{
1,2,\ldots,k-1\right\}  \right\}  $. Hence, $c$ can be written in the form
$c=\left(  p+h,p+k\right)  $ for some $h\in\left\{  1,2,\ldots,k-1\right\}  $.
Consider this $h$. From $h\in\left\{  1,2,\ldots,k-1\right\}  $, we obtain
$1\leq h\leq k-1$, so that $h\leq k-1<k$. Thus, $p+\underbrace{h}_{<k}<p+k$.
Moreover, $1\leq p+h$ (since $\underbrace{p}_{\geq0}+\underbrace{h}_{\geq
1}\geq0+1=1$) and $p+k\leq n$ (since $p\leq n-k$). Thus, $1\leq p+h<p+k\leq
n$.
\par
Applying (\ref{sol.perm.cycles.b.sigma-manifest.1a}) to $i=h$, we obtain
$\sigma\left(  p+h\right)  =p+\left(  \underbrace{h}_{\geq1>0}+1\right)
>p+\left(  0+1\right)  =p+1=\sigma\left(  p+k\right)  $ (by
(\ref{sol.perm.cycles.b.sigma-manifest.1b})).
\par
Now, $\left(  p+h,p+k\right)  $ is a pair of integers satisfying $1\leq
p+h<p+k\leq n$ and $\sigma\left(  p+h\right)  >\sigma\left(  p+k\right)  $. In
other words, $\left(  p+h,p+k\right)  $ is a pair of integers $\left(
i,j\right)  $ satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. In other words, $\left(  p+h,p+k\right)  $ is an
inversion of $\sigma$ (by the definition of \textquotedblleft inversion of
$\sigma$\textquotedblright). In other words, $\left(  p+h,p+k\right)
\in\operatorname*{Inv}\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of
all inversions of $\sigma$). Thus, $c=\left(  p+h,p+k\right)  \in
\operatorname*{Inv}\sigma$.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\sigma$ for every $c\in A$. In other words, $A\subseteq
\operatorname*{Inv}\sigma$, qed.} and $\operatorname*{Inv}\sigma\subseteq
A$\ \ \ \ \footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\sigma$. We
shall show that $c\in A$.
\par
We have $c\in\operatorname*{Inv}\sigma$. In other words, $c$ is an inversion
of $\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of all inversions of
$\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $. Consider this $\left(  i,j\right)  $. Thus, $c=\left(
i,j\right)  $.
\par
We have $\sigma\left(  i\right)  >\sigma\left(  j\right)  $, so that
$\sigma\left(  i\right)  \geq\sigma\left(  j\right)  +1$ (since $\sigma\left(
i\right)  $ and $\sigma\left(  j\right)  $ are integers). Also, $i<j$, so that
$i\leq j-1$ (since $i$ and $j$ are integers). In other words, $i+1\leq j$. But
(\ref{sol.perm.cycles.b.sigma.2}) (applied to $q=i$) yields $\sigma\left(
i\right)  \leq i+1\leq j$.
\par
Let us first show that $j=p+k$. Indeed, let us assume the contrary (for the
sake of contradiction). Thus, $j\neq p+k$. Hence, $\sigma\left(  j\right)
\geq j$ (by (\ref{sol.perm.cycles.b.sigma.1}), applied to $q=j$). Now,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  \geq j$. This contradicts
$\sigma\left(  i\right)  \leq j$. This contradiction shows that our assumption
was wrong. Hence, $j=p+k$ is proven.
\par
Now, $\sigma\left(  \underbrace{j}_{=p+k}\right)  =\sigma\left(  p+k\right)
=p+1$ (by (\ref{sol.perm.cycles.b.sigma-manifest.1b})). Hence, $\sigma\left(
i\right)  >\sigma\left(  j\right)  =p+1$. Therefore, $p+1<\sigma\left(
i\right)  \leq i+1$. Subtracting $1$ from both sides of this inequality, we
obtain $p<i$. Hence, $i>p$, so that $i\geq p+1$ (since $i$ and $p$ are
integers). Combined with $i<j=p+k$, this yields $i\in\left\{  p+1,p+2,\ldots
,p+k-1\right\}  $. Thus, $i-p\in\left\{  1,2,\ldots,k-1\right\}  $.
\par
So we know that the element $i-p\in\left\{  1,2,\ldots,k-1\right\}  $
satisfies $c=\left(  \underbrace{i}_{=p+\left(  i-p\right)  },\underbrace{j}%
_{=p+k}\right)  =\left(  p+\left(  i-p\right)  ,p+k\right)  $. Hence, there
exists an $h\in\left\{  1,2,\ldots,k-1\right\}  $ such that $c=\left(
p+h,p+k\right)  $ (namely, $h=i-p$). Thus,%
\[
c\in\left\{  \left(  p+h,p+k\right)  \ \mid\ h\in\left\{  1,2,\ldots
,k-1\right\}  \right\}  =A.
\]
\par
Now, let us forget that we fixed $c$. We thus have proven that $c\in A$ for
every $c\in\operatorname*{Inv}\sigma$. In other words, $\operatorname*{Inv}%
\sigma\subseteq A$, qed.}. Combining these two relations, we obtain
$A=\operatorname*{Inv}\sigma$. Hence, $\left\vert A\right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert $. Compared with $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert $, this yields
$\ell\left(  \sigma\right)  =\left\vert A\right\vert =k-1$. This rewrites as
$\ell\left(  \operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}\right)  =k-1$
(since $\sigma=\operatorname*{cyc}\nolimits_{p+1,p+2,\ldots,p+k}$). This
solves Exercise \ref{exe.perm.cycles} \textbf{(b)}.

\textbf{(c)} We shall only sketch the proof, since we will not use the result
in the following.

Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of $\left[  n\right]
$. We extend the $k$-tuple $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ to an
infinite sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ of elements of
$\left[  n\right]  $ by setting%
\[
\left(  i_{u}=i_{\left(  \text{the element }u^{\prime}\in\left\{
1,2,\ldots,k\right\}  \text{ satisfying }u^{\prime}\equiv u\operatorname{mod}%
k\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1\right)  .
\]
This sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ is periodic with
period $k$. In other words,%
\begin{equation}
i_{u}=i_{u+k}\ \ \ \ \ \ \ \ \ \ \text{for every }u\geq1.
\label{sol.perm.cycles.c.period}%
\end{equation}
From this, it is easy to obtain that%
\[
i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}+\cdots
+i_{k+2}=\cdots
\]
\footnote{\textit{Proof.} Every $u\in\left\{  1,2,3,\ldots\right\}  $
satisfies%
\begin{align*}
i_{u}+i_{u+1}+\cdots+i_{u+k-1}  &  =\underbrace{i_{u}}_{\substack{=i_{u+k}%
\\\text{(by (\ref{sol.perm.cycles.c.period}))}}}+\left(  i_{u+1}%
+i_{u+2}+\cdots+i_{u+k-1}\right) \\
&  =i_{u+k}+\left(  i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  =\left(
i_{u+1}+i_{u+2}+\cdots+i_{u+k-1}\right)  +i_{u+k}\\
&  =i_{u+1}+i_{u+2}+\cdots+i_{u+k}.
\end{align*}
Thus, $i_{1}+i_{2}+\cdots+i_{k}=i_{2}+i_{3}+\cdots+i_{k+1}=i_{3}+i_{4}%
+\cdots+i_{k+2}=\cdots$, qed.}. Thus,%
\begin{equation}
i_{r+1}+i_{r+2}+\cdots+i_{r+k}=i_{1}+i_{2}+\cdots+i_{k}%
\ \ \ \ \ \ \ \ \ \ \text{for every }r\in\mathbb{N}.
\label{sol.perm.cycles.c.periodsum}%
\end{equation}


Let $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$. Let
$\operatorname*{Inv}\sigma$ denote the set of all inversions of $\sigma$.
Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $. (This can be seen as in the solution to Exercise
\ref{exe.perm.cycles} \textbf{(b)}.) Moreover, the definitions of the sequence
$\left(  i_{1},i_{2},i_{3},\ldots\right)  $ and of $\sigma$ show that%
\begin{equation}
\sigma\left(  i_{p}\right)  =i_{p+1}\ \ \ \ \ \ \ \ \ \ \text{for every }%
p\geq1. \label{sol.perm.cycles.c.sigma}%
\end{equation}


Now, fix $r\in\left\{  1,2,\ldots,k-1\right\}  $. We shall prove that
\begin{equation}
\text{there exists some }u\geq1\text{ such that }\left(  i_{u},i_{u+r}\right)
\in\operatorname*{Inv}\sigma. \label{sol.perm.cycles.c.mainclaim}%
\end{equation}


\textit{Proof of (\ref{sol.perm.cycles.c.mainclaim}):} The sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ is periodic with period $k$, and its first
$k$ entries $i_{1},i_{2},\ldots,i_{k}$ are distinct. Hence, each entry of this
sequence repeats itself each $k$ steps, but not more often. Hence, every
integer $u\geq1$ satisfies $i_{u+r}\neq i_{u}$ (since $r\in\left\{
1,2,\ldots,k-1\right\}  $). In other words, every $u\geq1$ satisfies%
\begin{equation}
i_{u+r}-i_{u}\neq0. \label{sol.perm.cycles.c.mainclaim.pf.4}%
\end{equation}
The $k$-tuple $\left(  i_{1+r}-i_{1},i_{2+r}-i_{2},\ldots,i_{k+r}%
-i_{k}\right)  $ contains at least one positive entry\footnote{\textit{Proof.}
Assume the contrary. Thus, the $k$-tuple $\left(  i_{1+r}-i_{1},i_{2+r}%
-i_{2},\ldots,i_{k+r}-i_{k}\right)  $ contains no positive entries. In other
words, no $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies $i_{u+r}-i_{u}>0$.
In other words, every $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies
$i_{u+r}-i_{u}\leq0$. But (\ref{sol.perm.cycles.c.mainclaim.pf.4}) shows that
every $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies $i_{u+r}-i_{u}\neq0$.
Thus, every $u\in\left\{  1,2,\ldots,k\right\}  $ satisfies $i_{u+r}-i_{u}<0$
(since $i_{u+r}-i_{u}\leq0$ and $i_{u+r}-i_{u}\neq0$). Hence,%
\[
\sum_{u=1}^{k}\underbrace{\left(  i_{u+r}-i_{u}\right)  }_{<0}<\sum_{u=1}%
^{k}0=0.
\]
But this contradicts%
\begin{align*}
\sum_{u=1}^{k}\left(  \underbrace{i_{u+r}}_{=i_{r+u}}-i_{u}\right)   &
=\sum_{u=1}^{k}\left(  i_{r+u}-i_{u}\right)  =\left(  \sum_{u=1}^{k}%
i_{r+u}\right)  -\left(  \sum_{u=1}^{k}i_{u}\right) \\
&  =\left(  i_{r+1}+i_{r+2}+\cdots+i_{r+k}\right)  -\left(  i_{1}+i_{2}%
+\cdots+i_{k}\right)  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.perm.cycles.c.periodsum})}\right)  .
\end{align*}
This contradiction shows that our assumption was wrong, qed.}, and at least
one negative entry\footnote{This is proven similarly.}. Hence, there exists at
least one $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(  u+1\right)
+r}-i_{u+1}<0$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then,
there exists no $u\geq1$ such that $i_{u+r}-i_{u}>0$ but $i_{\left(
u+1\right)  +r}-i_{u+1}<0$. Hence,%
\begin{equation}
\text{every }u\geq1\text{ satisfying }i_{u+r}-i_{u}>0\text{ must satisfy
}i_{\left(  u+1\right)  +r}-i_{u+1}\geq0.
\label{sol.perm.cycles.c.mainclaim.pf.5o}%
\end{equation}
Therefore,%
\begin{equation}
\text{every }u\geq1\text{ satisfying }i_{u+r}-i_{u}>0\text{ must satisfy
}i_{\left(  u+1\right)  +r}-i_{u+1}>0 \label{sol.perm.cycles.c.mainclaim.pf.5}%
\end{equation}
(because (\ref{sol.perm.cycles.c.mainclaim.pf.5o}) shows that $i_{\left(
u+1\right)  +r}-i_{u+1}\geq0$; but combining this with $i_{\left(  u+1\right)
+r}-i_{u+1}\neq0$ (which follows from (\ref{sol.perm.cycles.c.mainclaim.pf.4}%
), applied to $u+1$ instead of $u$), we obtain $i_{\left(  u+1\right)
+r}-i_{u+1}>0$).
\par
But there exists some $v\in\left\{  1,2,\ldots,k\right\}  $ satisfying
$i_{v+r}-i_{v}>0$ (since the $k$-tuple $\left(  i_{1+r}-i_{1},i_{2+r}%
-i_{2},\ldots,i_{k+r}-i_{k}\right)  $ contains at least one positive entry).
Consider this $v$. Then, we have $i_{v+r}-i_{v}>0$, therefore $i_{\left(
v+1\right)  +r}-i_{v+1}>0$ (by (\ref{sol.perm.cycles.c.mainclaim.pf.5}),
applied to $u=v$), therefore $i_{\left(  v+2\right)  +r}-i_{v+2}>0$ (by
(\ref{sol.perm.cycles.c.mainclaim.pf.5}), applied to $u=v+1$), therefore
$i_{\left(  v+3\right)  +r}-i_{v+3}>0$ (by
(\ref{sol.perm.cycles.c.mainclaim.pf.5}), applied to $u=v+2$), and so on.
Altogether, we thus obtain
\[
i_{h+r}-i_{h}>0\ \ \ \ \ \ \ \ \ \ \text{for every }h\geq v.
\]
In other words, $i_{h}<i_{h+r}$ for every $h\geq v$. Hence, $i_{v}%
<i_{v+r}<i_{v+2r}<i_{v+3r}<\cdots$. Thus, the numbers $i_{v},i_{v+r}%
,i_{v+2r},i_{v+3r},\ldots$ are pairwise distinct; hence, the sequence $\left(
i_{1},i_{2},i_{3},\ldots\right)  $ contains infinitely many distinct entries.
But this contradicts the fact that this sequence is periodic. This
contradiction proves that our assumption was wrong, qed.}. Consider this $u$.
We have $i_{u}<i_{u+r}$ (since $i_{u+r}-i_{u}>0$), so that $1\leq
i_{u}<i_{u+r}\leq n$. Also, (\ref{sol.perm.cycles.c.sigma}) (applied to $p=u$)
yields $\sigma\left(  i_{u}\right)  =i_{u+1}$. Moreover,
(\ref{sol.perm.cycles.c.sigma}) (applied to $p=u+r$) yields $\sigma\left(
i_{u+r}\right)  =i_{u+r+1}=i_{\left(  u+1\right)  +r}$. Hence,
\begin{align*}
\sigma\left(  i_{u}\right)   &  =i_{u+1}>i_{\left(  u+1\right)  +r}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i_{\left(  u+1\right)  +r}%
-i_{u+1}<0\right) \\
&  =\sigma\left(  i_{u+r}\right)  .
\end{align*}
So we know that $\left(  i_{u},i_{u+r}\right)  $ is a pair of integers
satisfying $1\leq i_{u}<i_{u+r}\leq n$ and $\sigma\left(  i_{u}\right)
>\sigma\left(  i_{u+r}\right)  $. In other words, $\left(  i_{u}%
,i_{u+r}\right)  $ is an inversion of $\sigma$ (by the definition of an
``inversion''). In other words, $\left(  i_{u},i_{u+r}\right)  \in
\operatorname*{Inv}\sigma$. Thus, we have found a $u\geq1$ such that $\left(
i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. This proves
(\ref{sol.perm.cycles.c.mainclaim}).

Now, let us forget that we fixed $r$. We have shown that, for every
$r\in\left\{  1,2,\ldots,k-1\right\}  $, there exists some $u\geq1$ such that
$\left(  i_{u},i_{u+r}\right)  \in\operatorname*{Inv}\sigma$. Let us denote
this $u$ by $u_{r}$. Therefore, for every $r\in\left\{  1,2,\ldots
,k-1\right\}  $, we have found a $u_{r}\geq1$ such that $\left(  i_{u_{r}%
},i_{u_{r}+r}\right)  \in\operatorname*{Inv}\sigma$. The $k-1$ pairs%
\[
\left(  i_{u_{1}},i_{u_{1}+1}\right)  ,\ \left(  i_{u_{2}},i_{u_{2}+2}\right)
,\ \ldots,\ \left(  i_{u_{k-1}},i_{u_{k-1}+\left(  k-1\right)  }\right)
\]
are pairwise distinct\footnote{\textit{Proof.} Assume the contrary. Then,
there exist two distinct elements $x$ and $y$ of $\left\{  1,2,\ldots
,k-1\right\}  $ such that $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(
i_{u_{y}},i_{u_{y}+y}\right)  $. Consider these $x$ and $y$.
\par
We have $\left(  i_{u_{x}},i_{u_{x}+x}\right)  =\left(  i_{u_{y}},i_{u_{y}%
+y}\right)  $. In other words, $i_{u_{x}}=i_{u_{y}}$ and $i_{u_{x}+x}%
=i_{u_{y}+y}$. Since the numbers $i_{1},i_{2},\ldots,i_{k}$ are distinct (and
the sequence $\left(  i_{1},i_{2},i_{3},\ldots\right)  $ consists of these
numbers, repeated over and over), we obtain $u_{x}\equiv u_{y}%
\operatorname{mod}k$ from $i_{u_{x}}=i_{u_{y}}$, and we obtain $u_{x}+x\equiv
u_{y}+y\operatorname{mod}k$ from $i_{u_{x}+x}=i_{u_{y}+y}$. Subtracting the
congruence $u_{x}\equiv u_{y}\operatorname{mod}k$ from the congruence
$u_{x}+x\equiv u_{y}+y\operatorname{mod}k$, we obtain $x\equiv
y\operatorname{mod}k$. In light of $x,y\in\left\{  1,2,\ldots,k-1\right\}  $,
this shows that $x=y$. But this contradicts the fact that $x$ and $y$ are
distinct. This contradiction proves that our assumption was wrong, qed.}, and
all of them belong to $\operatorname*{Inv}\sigma$. Hence, the set
$\operatorname*{Inv}\sigma$ has at least $k-1$ elements. In other words,
$\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$. Thus, $\ell\left(
\sigma\right)  =\left\vert \operatorname*{Inv}\sigma\right\vert \geq k-1$.
Since $\sigma=\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}$, this
rewrites as $\ell\left(  \operatorname*{cyc}\nolimits_{i_{1},i_{2}%
,\ldots,i_{k}}\right)  \geq k-1$. This solves Exercise \ref{exe.perm.cycles}
\textbf{(c)}.

\textbf{(d)} Let $i_{1},i_{2},\ldots,i_{k}$ be $k$ distinct elements of
$\left[  n\right]  $. Thus, $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ is a
list of some elements of $\left[  n\right]  $ such that $i_{1},i_{2}%
,\ldots,i_{k}$ are distinct. Hence, Proposition \ref{prop.perms.lists}
\textbf{(c)} (applied to $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =\left(
i_{1},i_{2},\ldots,i_{k}\right)  $) yields that there exists a permutation
$\sigma\in S_{n}$ such that $\left(  i_{1},i_{2},\ldots,i_{k}\right)  =\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
k\right)  \right)  $. Consider such a $\sigma$.

Exercise \ref{exe.perm.cycles} \textbf{(b)} yields $\ell\left(
\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)  =k-1$. But the definition
of $\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}$ yields
$\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}=\left(
-1\right)  ^{\ell\left(  \operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right)
}=\left(  -1\right)  ^{k-1}$ (since $\ell\left(  \operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}\right)  =k-1$).

Exercise \ref{exe.perm.cycles} \textbf{(a)} (applied to $1,2,\ldots,k$ instead
of $i_{1},i_{2},\ldots,i_{k}$) yields%
\[
\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma
^{-1}=\operatorname*{cyc}\nolimits_{\sigma\left(  1\right)  ,\sigma\left(
2\right)  ,\ldots,\sigma\left(  k\right)  }=\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
\]
(since $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  k\right)  \right)  =\left(  i_{1},i_{2},\ldots
,i_{k}\right)  $). Hence,
\[
\underbrace{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}%
_{=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\circ\sigma^{-1}%
}\circ\sigma=\sigma\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}%
\circ\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}}=\sigma
\circ\operatorname*{cyc}\nolimits_{1,2,\ldots,k}.
\]
Thus,%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\sigma\circ\operatorname*{cyc}%
\nolimits_{1,2,\ldots,k}}=\left(  -1\right)  ^{\sigma}\cdot\underbrace{\left(
-1\right)  ^{\operatorname*{cyc}\nolimits_{1,2,\ldots,k}}}_{=\left(
-1\right)  ^{k-1}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to }%
\tau=\operatorname*{cyc}\nolimits_{1,2,\ldots,k}\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{k-1}.
\end{align*}
Compared with%
\begin{align*}
\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}%
}\circ\sigma}  &  =\left(  -1\right)  ^{\operatorname*{cyc}\nolimits_{i_{1}%
,i_{2},\ldots,i_{k}}}\cdot\left(  -1\right)  ^{\sigma}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sign.prod}), applied to
}\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}\text{ and }%
\sigma\text{ instead of }\sigma\text{ and }\tau\right) \\
&  =\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}},
\end{align*}
this yields $\left(  -1\right)  ^{\sigma}\cdot\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{k-1}$. We can cancel $\left(  -1\right)
^{\sigma}$ from this equality (since $\left(  -1\right)  ^{\sigma}\in\left\{
1,-1\right\}  $ is a nonzero integer), and thus obtain $\left(  -1\right)
^{\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}}=\left(  -1\right)
^{k-1}$. This solves Exercise \ref{exe.perm.cycles} \textbf{(d)}.

[\textit{Remark:} Exercise \ref{exe.perm.cycles} \textbf{(d)} can also be
solved in a different way, namely by arguing that%
\[
\operatorname*{cyc}\nolimits_{i_{1},i_{2},\ldots,i_{k}}=t_{i_{1},i_{2}}\circ
t_{i_{2},i_{3}}\circ\cdots\circ t_{i_{k-1},i_{k}}%
\]
(using the notations of Definition \ref{def.transpos}) and then by applying
the equality $\left(  -1\right)  ^{\sigma\circ\tau}=\left(  -1\right)
^{\sigma}\cdot\left(  -1\right)  ^{\tau}$ multiple times. We leave the details
of this alternative proof to the curious reader. (That said, this alternative
proof is also the most popular proof, so it is easily found in textbooks.)]
\end{proof}
\end{verlong}

\subsection{Solution to Additional exercise \ref{exeadd.perm.Inv.sub}}

We shall now prepare for the solution of Additional exercise
\ref{exeadd.perm.Inv.sub}. First, we introduce a notation:

\begin{definition}
\label{def.sol.exeadd.perm.Inv.sub.aXb}If $X$, $X^{\prime}$, $Y$ and
$Y^{\prime}$ are four sets and if $\alpha:X\rightarrow X^{\prime}$ and
$\beta:Y\rightarrow Y^{\prime}$ are two maps, then $\alpha\times\beta$ will
denote the map%
\begin{align*}
X\times Y  &  \rightarrow X^{\prime}\times Y^{\prime},\\
\left(  x,y\right)   &  \mapsto\left(  \alpha\left(  x\right)  ,\beta\left(
y\right)  \right)  .
\end{align*}

\end{definition}

\begin{lemma}
\label{lem.sol.exeadd.perm.Inv.sub.bijbij}Let $n\in\mathbb{N}$. Let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. Let $\alpha\in
S_{n}$ and $\beta\in S_{n}$. Then, the map $\alpha\times\beta:\left[
n\right]  \times\left[  n\right]  \rightarrow\left[  n\right]  \times\left[
n\right]  $ is invertible, and its inverse is $\alpha^{-1}\times\beta^{-1}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.perm.Inv.sub.bijbij}.]Recall that $S_{n}$
is the set of all permutations of the set $\left\{  1,2,\ldots,n\right\}  $.
In other words, $S_{n}$ is the set of all permutations of the set $\left[
n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $).

We know that $\alpha\in S_{n}$. In other words, $\alpha$ is a permutation of
the set $\left[  n\right]  $ (since $S_{n}$ is the set of all permutations of
the set $\left[  n\right]  $). In other words, $\alpha$ is a bijective map
$\left[  n\right]  \rightarrow\left[  n\right]  $. Similarly, $\beta$ is a
bijective map $\left[  n\right]  \rightarrow\left[  n\right]  $. Hence,
Definition \ref{def.sol.exeadd.perm.Inv.sub.aXb} defines a map $\alpha
\times\beta:\left[  n\right]  \times\left[  n\right]  \rightarrow\left[
n\right]  \times\left[  n\right]  $.

The map $\alpha$ is bijective, and thus invertible. Hence, its inverse map
$\alpha^{-1}:\left[  n\right]  \rightarrow\left[  n\right]  $ is well-defined.
Similarly, $\beta^{-1}:\left[  n\right]  \rightarrow\left[  n\right]  $ is
well-defined. Thus, Definition \ref{def.sol.exeadd.perm.Inv.sub.aXb} defines a
map $\alpha^{-1}\times\beta^{-1}:\left[  n\right]  \times\left[  n\right]
\rightarrow\left[  n\right]  \times\left[  n\right]  $.

\begin{vershort}
Now, every $\left(  i,j\right)  \in\left[  n\right]  \times\left[  n\right]  $
satisfies
\begin{align*}
&  \left(  \left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(
\alpha\times\beta\right)  \right)  \left(  i,j\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \underbrace{\left(
\alpha\times\beta\right)  \left(  i,j\right)  }_{\substack{=\left(
\alpha\left(  i\right)  ,\beta\left(  j\right)  \right)  \\\text{(by the
definition of }\alpha\times\beta\text{)}}}\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \alpha\left(
i\right)  ,\beta\left(  j\right)  \right)  =\left(  \underbrace{\alpha
^{-1}\left(  \alpha\left(  i\right)  \right)  }_{=i},\underbrace{\beta
^{-1}\left(  \beta\left(  j\right)  \right)  }_{=j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha^{-1}%
\times\beta^{-1}\right) \\
&  =\left(  i,j\right)  =\operatorname*{id}\left(  i,j\right)  .
\end{align*}
Thus, $\left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  =\operatorname*{id}$. Similarly, $\left(  \alpha
\times\beta\right)  \circ\left(  \alpha^{-1}\times\beta^{-1}\right)
=\operatorname*{id}$. These two equalities show that the maps $\alpha
\times\beta$ and $\alpha^{-1}\times\beta^{-1}$ are mutually inverse. Hence,
the map $\alpha\times\beta:\left[  n\right]  \times\left[  n\right]
\rightarrow\left[  n\right]  \times\left[  n\right]  $ is invertible, and its
inverse is $\alpha^{-1}\times\beta^{-1}$. This proves Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij}.
\end{vershort}

\begin{verlong}
Now, $\left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  =\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let
$z\in\left[  n\right]  \times\left[  n\right]  $. Thus, $z=\left(  i,j\right)
$ for some $\left(  i,j\right)  \in\left[  n\right]  \times\left[  n\right]
$. Consider this $\left(  i,j\right)  $.
\par
We have%
\begin{align*}
\left(  \left(  \alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha
\times\beta\right)  \right)  \left(  \underbrace{z}_{=\left(  i,j\right)
}\right)   &  =\left(  \left(  \alpha^{-1}\times\beta^{-1}\right)
\circ\left(  \alpha\times\beta\right)  \right)  \left(  i,j\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \underbrace{\left(
\alpha\times\beta\right)  \left(  i,j\right)  }_{\substack{=\left(
\alpha\left(  i\right)  ,\beta\left(  j\right)  \right)  \\\text{(by the
definition of }\alpha\times\beta\text{)}}}\right) \\
&  =\left(  \alpha^{-1}\times\beta^{-1}\right)  \left(  \alpha\left(
i\right)  ,\beta\left(  j\right)  \right)  =\left(  \underbrace{\alpha
^{-1}\left(  \alpha\left(  i\right)  \right)  }_{=i},\underbrace{\beta
^{-1}\left(  \beta\left(  j\right)  \right)  }_{=j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha^{-1}%
\times\beta^{-1}\right) \\
&  =\left(  i,j\right)  =z=\operatorname*{id}\left(  z\right)  .
\end{align*}
\par
Now, forget that we fixed $z$. We thus have shown that $\left(  \left(
\alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha\times\beta\right)
\right)  \left(  z\right)  =\operatorname*{id}\left(  z\right)  $ for every
$z\in\left[  n\right]  \times\left[  n\right]  $. In other words, $\left(
\alpha^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha\times\beta\right)
=\operatorname*{id}$, qed.} and $\left(  \alpha\times\beta\right)
\circ\left(  \alpha^{-1}\times\beta^{-1}\right)  =\operatorname*{id}%
$\ \ \ \ \footnote{\textit{Proof.} Let $z\in\left[  n\right]  \times\left[
n\right]  $. Thus, $z=\left(  i,j\right)  $ for some $\left(  i,j\right)
\in\left[  n\right]  \times\left[  n\right]  $. Consider this $\left(
i,j\right)  $.
\par
We have%
\begin{align*}
\left(  \left(  \alpha\times\beta\right)  \circ\left(  \alpha^{-1}\times
\beta^{-1}\right)  \right)  \left(  \underbrace{z}_{=\left(  i,j\right)
}\right)   &  =\left(  \left(  \alpha\times\beta\right)  \circ\left(
\alpha^{-1}\times\beta^{-1}\right)  \right)  \left(  i,j\right) \\
&  =\left(  \alpha\times\beta\right)  \left(  \underbrace{\left(  \alpha
^{-1}\times\beta^{-1}\right)  \left(  i,j\right)  }_{\substack{=\left(
\alpha^{-1}\left(  i\right)  ,\beta^{-1}\left(  j\right)  \right)  \\\text{(by
the definition of }\alpha^{-1}\times\beta^{-1}\text{)}}}\right) \\
&  =\left(  \alpha\times\beta\right)  \left(  \alpha^{-1}\left(  i\right)
,\beta^{-1}\left(  j\right)  \right)  =\left(  \underbrace{\alpha\left(
\alpha^{-1}\left(  i\right)  \right)  }_{=i},\underbrace{\beta\left(
\beta^{-1}\left(  j\right)  \right)  }_{=j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha\times
\beta\right) \\
&  =\left(  i,j\right)  =z=\operatorname*{id}\left(  z\right)  .
\end{align*}
\par
Now, forget that we fixed $z$. We thus have shown that $\left(  \left(
\alpha\times\beta\right)  \circ\left(  \alpha^{-1}\times\beta^{-1}\right)
\right)  \left(  z\right)  =\operatorname*{id}\left(  z\right)  $ for every
$z\in\left[  n\right]  \times\left[  n\right]  $. In other words, $\left(
\alpha\times\beta\right)  \circ\left(  \alpha^{-1}\times\beta^{-1}\right)
=\operatorname*{id}$, qed.}. Thus, the maps $\alpha\times\beta$ and
$\alpha^{-1}\times\beta^{-1}$ are mutually inverse (since $\left(  \alpha
^{-1}\times\beta^{-1}\right)  \circ\left(  \alpha\times\beta\right)
=\operatorname*{id}$ and $\left(  \alpha\times\beta\right)  \circ\left(
\alpha^{-1}\times\beta^{-1}\right)  =\operatorname*{id}$). Hence, the map
$\alpha\times\beta:\left[  n\right]  \times\left[  n\right]  \rightarrow
\left[  n\right]  \times\left[  n\right]  $ is invertible, and its inverse is
$\alpha^{-1}\times\beta^{-1}$. This proves Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij}.
\end{verlong}
\end{proof}

\begin{lemma}
\label{lem.sol.exeadd.perm.Inv.sub.subset}Let $n\in\mathbb{N}$. Let $\sigma\in
S_{n}$ and $\tau\in S_{n}$. Then:

\textbf{(a)} We have $\operatorname*{Inv}\left(  \sigma\circ\tau\right)
\setminus\operatorname*{Inv}\tau\subseteq\left(  \tau\times\tau\right)
^{-1}\left(  \operatorname*{Inv}\sigma\right)  $. (Here, $\tau\times\tau$ is
defined as in Definition \ref{def.sol.exeadd.perm.Inv.sub.aXb}.)

\textbf{(b)} We have $\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert \leq\left\vert
\operatorname*{Inv}\sigma\right\vert $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset}.]Let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij} (applied to $\alpha=\tau$ and
$\beta=\tau$) yields that the map $\tau\times\tau:\left[  n\right]
\times\left[  n\right]  \rightarrow\left[  n\right]  \times\left[  n\right]  $
is invertible, and its inverse is $\tau^{-1}\times\tau^{-1}$. Thus, $\left(
\tau\times\tau\right)  ^{-1}=\tau^{-1}\times\tau^{-1}$.

\textbf{(a)} Let $c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)
\setminus\operatorname*{Inv}\tau$. Thus, $c\in\operatorname*{Inv}\left(
\sigma\circ\tau\right)  $ but $c\notin\operatorname*{Inv}\tau$.

We have $c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. In other
words, $c$ is an inversion of $\sigma\circ\tau$ (since $\operatorname*{Inv}%
\left(  \sigma\circ\tau\right)  $ is the set of all inversions of $\sigma
\circ\tau$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\left(  \sigma\circ\tau\right)  \left(
i\right)  >\left(  \sigma\circ\tau\right)  \left(  j\right)  $ (by the
definition of an \textquotedblleft inversion of $\sigma\circ\tau
$\textquotedblright). In other words, there exists a pair $\left(  i,j\right)
$ of integers satisfying $1\leq i<j\leq n$, $\left(  \sigma\circ\tau\right)
\left(  i\right)  >\left(  \sigma\circ\tau\right)  \left(  j\right)  $ and
$c=\left(  i,j\right)  $. Let us denote this pair $\left(  i,j\right)  $ by
$\left(  u,v\right)  $. Thus, $\left(  u,v\right)  $ is a pair of integers
satisfying $1\leq u<v\leq n$, $\left(  \sigma\circ\tau\right)  \left(
u\right)  >\left(  \sigma\circ\tau\right)  \left(  v\right)  $ and $c=\left(
u,v\right)  $.

But $\tau\left(  u\right)  <\tau\left(  v\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $\tau\left(
u\right)  \geq\tau\left(  v\right)  $. But $\tau\in S_{n}$. In other words,
$\tau$ is a permutation of the set $\left\{  1,2,\ldots,n\right\}  $ (since
$S_{n}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $). Thus, the map $\tau$ is bijective, and therefore also
injective. But $u<v$, so that $u\neq v$ and therefore $\tau\left(  u\right)
\neq\tau\left(  v\right)  $ (since $\tau$ is injective). Combining this with
$\tau\left(  u\right)  \geq\tau\left(  v\right)  $, we obtain $\tau\left(
u\right)  >\tau\left(  v\right)  $.
\par
Now, we know that $\left(  u,v\right)  $ is a pair of integers and satisfies
$1\leq u<v\leq n$ and $\tau\left(  u\right)  >\tau\left(  v\right)  $. In
other words, $\left(  u,v\right)  $ is a pair $\left(  i,j\right)  $ of
integers satisfying $1\leq i<j\leq n$ and $\tau\left(  i\right)  >\tau\left(
j\right)  $. In other words, $\left(  u,v\right)  $ is an inversion of $\tau$
(by the definition of an \textquotedblleft inversion of $\tau$%
\textquotedblright). In other words, $\left(  u,v\right)  \in
\operatorname*{Inv}\tau$ (since $\operatorname*{Inv}\tau$ is the set of all
inversions of $\tau$). But this contradicts $\left(  u,v\right)
=c\notin\operatorname*{Inv}\tau$. This contradiction shows that our assumption
was false, qed.}. Also, $1\leq\tau\left(  u\right)  $ (since $\tau\left(
u\right)  \in\left\{  1,2,\ldots,n\right\}  $) and $\tau\left(  v\right)  \leq
n$ (since $\tau\left(  v\right)  \in\left\{  1,2,\ldots,n\right\}  $).
Finally, $\sigma\left(  \tau\left(  u\right)  \right)  =\left(  \sigma
\circ\tau\right)  \left(  u\right)  >\left(  \sigma\circ\tau\right)  \left(
v\right)  =\sigma\left(  \tau\left(  v\right)  \right)  $.

Thus, $\left(  \tau\left(  u\right)  ,\tau\left(  v\right)  \right)  $ is a
pair of integers satisfying $1\leq\tau\left(  u\right)  <\tau\left(  v\right)
\leq n$ and $\sigma\left(  \tau\left(  u\right)  \right)  >\sigma\left(
\tau\left(  v\right)  \right)  $. In other words, $\left(  \tau\left(
u\right)  ,\tau\left(  v\right)  \right)  $ is a pair $\left(  i,j\right)  $
of integers satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $. In other words, $\left(  \tau\left(  u\right)
,\tau\left(  v\right)  \right)  $ is an inversion of $\sigma$ (by the
definition of an \textquotedblleft inversion of $\sigma$\textquotedblright).
In other words, $\left(  \tau\left(  u\right)  ,\tau\left(  v\right)  \right)
\in\operatorname*{Inv}\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of
all inversions of $\sigma$).

Now, $c=\left(  u,v\right)  \in\left[  n\right]  \times\left[  n\right]  $
(since both $u$ and $v$ belong to $\left[  n\right]  $), and we have
\begin{align*}
\left(  \tau\times\tau\right)  \left(  \underbrace{c}_{=\left(  u,v\right)
}\right)   &  =\left(  \tau\times\tau\right)  \left(  u,v\right)  =\left(
\tau\left(  u\right)  ,\tau\left(  v\right)  \right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\tau\times\tau\right)
\\
&  \in\operatorname*{Inv}\sigma,
\end{align*}
so that $c\in\left(  \tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}%
\sigma\right)  $.

Now, forget that we fixed $c$. We thus have shown that $c\in\left(  \tau
\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)  $ for every
$c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau$. In other words, $\operatorname*{Inv}\left(
\sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau\subseteq\left(
\tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)  $. This
proves Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(a)}.

\textbf{(b)} Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(a)}
yields $\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\subseteq\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  $. Thus,%
\[
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert \leq\left\vert \left(  \tau\times
\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)  \right\vert
=\left\vert \operatorname*{Inv}\sigma\right\vert
\]
(since $\tau\times\tau$ is a bijection (since the map $\tau\times\tau$ is
invertible)). This proves Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset}
\textbf{(b)}.
\end{proof}

\begin{lemma}
\label{lem.sol.exeadd.perm.Inv.sub.l}Let $n\in\mathbb{N}$. Let $\sigma\in
S_{n}$. Then, $\ell\left(  \sigma\right)  =\left\vert \operatorname*{Inv}%
\sigma\right\vert $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}.]We have%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}.
\end{proof}

Before we step to the solution to Additional exercise
\ref{exeadd.perm.Inv.sub}, let us make a short digression and use Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.subset} to give a new solution of Exercise
\ref{exe.ps2.2.5} \textbf{(c)}:

\begin{proof}
[Second solution to Exercise \ref{exe.ps2.2.5} \textbf{(c)}.]Let $\sigma\in
S_{n}$ and $\tau\in S_{n}$. We have $\left\vert A\setminus B\right\vert
\geq\left\vert A\right\vert -\left\vert B\right\vert $ for any two finite sets
$A$ and $B$\ \ \ \ \footnote{\textit{Proof.} Let $A$ and $B$ be two finite
sets. Then, $A\setminus B=A\setminus\left(  A\cap B\right)  $, so that
\begin{align*}
\left\vert A\setminus B\right\vert  &  =\left\vert A\setminus\left(  A\cap
B\right)  \right\vert =\left\vert A\right\vert -\underbrace{\left\vert A\cap
B\right\vert }_{\substack{\leq\left\vert B\right\vert \\\text{(since }A\cap
B\subseteq B\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A\cap
B\subseteq A\right) \\
&  \geq\left\vert A\right\vert -\left\vert B\right\vert ,
\end{align*}
qed.}. Applying this to $A=\operatorname*{Inv}\left(  \sigma\circ\tau\right)
$ and $B=\operatorname*{Inv}\tau$, we obtain $\left\vert \operatorname*{Inv}%
\left(  \sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert
\geq\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert $. Thus,%
\begin{align*}
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert  &  \leq\left\vert
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert \leq\left\vert \operatorname*{Inv}%
\sigma\right\vert \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(b)}}\right) \\
&  =\ell\left(  \sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}}\right)  .
\end{align*}
Now,%
\[
\underbrace{\ell\left(  \sigma\circ\tau\right)  }_{\substack{=\left\vert
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert \\\text{(by
Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\sigma\circ
\tau\text{ instead of }\sigma\text{))}}}-\underbrace{\ell\left(  \tau\right)
}_{\substack{=\left\vert \operatorname*{Inv}\tau\right\vert \\\text{(by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\tau\text{ instead of
}\sigma\text{))}}}=\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right\vert -\left\vert \operatorname*{Inv}\tau\right\vert
\leq\ell\left(  \sigma\right)  .
\]
In other words, $\ell\left(  \sigma\circ\tau\right)  \leq\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $. Thus, Exercise \ref{exe.ps2.2.5}
\textbf{(c)} is solved again.
\end{proof}

Now, let us finally solve Additional exercise \ref{exeadd.perm.Inv.sub}:

\begin{proof}
[Solution to Additional exercise \ref{exeadd.perm.Inv.sub}.]\textbf{(a)} We
first observe that%
\begin{equation}
\underbrace{\ell\left(  \sigma\circ\tau\right)  }_{\substack{=\left\vert
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert \\\text{(by
Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\sigma\circ
\tau\text{ instead of }\sigma\text{))}}}-\underbrace{\ell\left(  \tau\right)
}_{\substack{=\left\vert \operatorname*{Inv}\tau\right\vert \\\text{(by Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.l}}\\\text{(applied to }\tau\text{ instead of
}\sigma\text{))}}}=\left\vert \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right\vert -\left\vert \operatorname*{Inv}\tau\right\vert .
\label{sol.exeadd.perm.Inv.sub.a.triv}%
\end{equation}


Let us now prove the logical implication%
\begin{equation}
\left(  \ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \right)  \ \Longrightarrow\ \left(
\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right)  . \label{sol.exeadd.perm.Inv.sub.a.1}%
\end{equation}


\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.a.1}):} Assume that
$\ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  $ holds. We will prove that $\operatorname*{Inv}\tau
\subseteq\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $.

If two finite sets $A$ and $B$ satisfy $\left\vert A\setminus B\right\vert
\leq\left\vert A\right\vert -\left\vert B\right\vert $, then%
\begin{equation}
B\subseteq A \label{sol.exeadd.perm.Inv.sub.a.1.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.a.1.pf.1}):} Let $A$
and $B$ be two finite sets satisfying $\left\vert A\setminus B\right\vert
\leq\left\vert A\right\vert -\left\vert B\right\vert $.
\par
We have $A\setminus\left(  A\cap B\right)  =A\setminus B$, so that $\left\vert
A\setminus\left(  A\cap B\right)  \right\vert =\left\vert A\setminus
B\right\vert \leq\left\vert A\right\vert -\left\vert B\right\vert $. Adding
$\left\vert B\right\vert $ to both sides of this inequality, we obtain
$\left\vert A\setminus\left(  A\cap B\right)  \right\vert +\left\vert
B\right\vert \leq\left\vert A\right\vert $. Hence,%
\[
\left\vert A\right\vert \geq\underbrace{\left\vert A\setminus\left(  A\cap
B\right)  \right\vert }_{\substack{=\left\vert A\right\vert -\left\vert A\cap
B\right\vert \\\text{(since }A\cap B\subseteq A\text{)}}}+\left\vert
B\right\vert =\left\vert A\right\vert -\left\vert A\cap B\right\vert
+\left\vert B\right\vert .
\]
Subtracting $\left\vert A\right\vert $ from both sides of this inequality, we
obtain $0\geq-\left\vert A\cap B\right\vert +\left\vert B\right\vert $. In
other words, $\left\vert A\cap B\right\vert \geq\left\vert B\right\vert $.
Also, clearly, $A\cap B$ is a subset of $B$.
\par
But $B$ is a finite set. Hence, the only subset of $B$ having size
$\geq\left\vert B\right\vert $ is $B$ itself. In other words, if $C$ is a
subset of $B$ satisfying $\left\vert C\right\vert \geq\left\vert B\right\vert
$, then $C=B$. Applying this to $C=A\cap B$, we obtain $A\cap B=B$ (since
$A\cap B$ is a subset of $B$ satisfying $\left\vert A\cap B\right\vert
\geq\left\vert B\right\vert $). Hence, $B=A\cap B\subseteq A$. This proves
(\ref{sol.exeadd.perm.Inv.sub.a.1.pf.1}).}.

Now, Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(b)} yields
\begin{align*}
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert  &  \leq\left\vert \operatorname*{Inv}%
\sigma\right\vert =\ell\left(  \sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}}\right) \\
&  =\ell\left(  \sigma\circ\tau\right)  -\ell\left(  \tau\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  \right) \\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.exeadd.perm.Inv.sub.a.triv})}\right)  .
\end{align*}
Thus, (\ref{sol.exeadd.perm.Inv.sub.a.1.pf.1}) (applied to
$A=\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ and
$B=\operatorname*{Inv}\tau$) yields $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $.

Now, forget our assumption that $\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  $. We thus have proven
that if $\ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  $, then $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. In other words, we have
proven the implication (\ref{sol.exeadd.perm.Inv.sub.a.1}).

Let us next prove the logical implication%
\begin{equation}
\left(  \operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma
\circ\tau\right)  \right)  \ \Longrightarrow\ \left(  \ell\left(  \sigma
\circ\tau\right)  =\ell\left(  \sigma\right)  +\ell\left(  \tau\right)
\right)  . \label{sol.exeadd.perm.Inv.sub.a.2}%
\end{equation}


\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.a.2}):} Assume that
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $ holds. We will prove that $\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \sigma\right)  +\ell\left(  \tau\right)  $.

Consider the map $\tau\times\tau$ defined as in Definition
\ref{def.sol.exeadd.perm.Inv.sub.aXb}. Let $\left[  n\right]  $ denote the set
$\left\{  1,2,\ldots,n\right\}  $. Lemma
\ref{lem.sol.exeadd.perm.Inv.sub.bijbij} (applied to $\alpha=\tau$ and
$\beta=\tau$) yields that the map $\tau\times\tau:\left[  n\right]
\times\left[  n\right]  \rightarrow\left[  n\right]  \times\left[  n\right]  $
is invertible, and its inverse is $\tau^{-1}\times\tau^{-1}$. Thus, $\left(
\tau\times\tau\right)  ^{-1}=\tau^{-1}\times\tau^{-1}$.

Lemma \ref{lem.sol.exeadd.perm.Inv.sub.subset} \textbf{(a)} shows that%
\begin{equation}
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\subseteq\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  . \label{sol.exeadd.perm.Inv.sub.a.2.pf.1}%
\end{equation}
We shall now prove the reverse inclusion, i.e., we shall prove that $\left(
\tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)
\subseteq\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau$.

Indeed, fix $c\in\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  $. Thus, $c\in\left[  n\right]
\times\left[  n\right]  $ and $\left(  \tau\times\tau\right)  \left(
c\right)  \in\operatorname*{Inv}\sigma$.

We have $\left(  \tau\times\tau\right)  \left(  c\right)  \in
\operatorname*{Inv}\sigma$. In other words, $\left(  \tau\times\tau\right)
\left(  c\right)  $ is an inversion of $\sigma$ (since $\operatorname*{Inv}%
\sigma$ is the set of all inversions of $\sigma$). In other words, $\left(
\tau\times\tau\right)  \left(  c\right)  $ is a pair $\left(  i,j\right)  $ of
integers satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)
>\sigma\left(  j\right)  $ (by the definition of an \textquotedblleft
inversion of $\sigma$\textquotedblright). In other words, there exists a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$,
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $ and $\left(  \tau
\times\tau\right)  \left(  c\right)  =\left(  i,j\right)  $. Let us denote
this pair $\left(  i,j\right)  $ by $\left(  u,v\right)  $. Thus, $\left(
u,v\right)  $ is a pair of integers satisfying $1\leq u<v\leq n$,
$\sigma\left(  u\right)  >\sigma\left(  v\right)  $ and $\left(  \tau
\times\tau\right)  \left(  c\right)  =\left(  u,v\right)  $.

From $\left(  \tau\times\tau\right)  \left(  c\right)  =\left(  u,v\right)  $,
we obtain%
\begin{align*}
c  &  =\underbrace{\left(  \tau\times\tau\right)  ^{-1}}_{=\tau^{-1}\times
\tau^{-1}}\left(  u,v\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the map
}\tau\times\tau\text{ is invertible}\right) \\
&  =\left(  \tau^{-1}\times\tau^{-1}\right)  \left(  u,v\right)  =\left(
\tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)
\end{align*}
(by the definition of $\tau^{-1}\times\tau^{-1}$).

Notice that%
\begin{equation}
\left(  \sigma\circ\tau\right)  \left(  \tau^{-1}\left(  v\right)  \right)
=\sigma\left(  \underbrace{\tau\left(  \tau^{-1}\left(  v\right)  \right)
}_{=v}\right)  =\sigma\left(  v\right)
\label{sol.exeadd.perm.Inv.sub.a.2.pf.4}%
\end{equation}
and%
\begin{align}
\left(  \sigma\circ\tau\right)  \left(  \tau^{-1}\left(  u\right)  \right)
&  =\sigma\left(  \underbrace{\tau\left(  \tau^{-1}\left(  u\right)  \right)
}_{=u}\right)  =\sigma\left(  u\right) \nonumber\\
&  >\sigma\left(  v\right)  =\left(  \sigma\circ\tau\right)  \left(  \tau
^{-1}\left(  v\right)  \right)  \label{sol.exeadd.perm.Inv.sub.a.2.pf.5}%
\end{align}
(by (\ref{sol.exeadd.perm.Inv.sub.a.2.pf.4})).

Let us now prove that $\tau^{-1}\left(  u\right)  <\tau^{-1}\left(  v\right)
$. Indeed, let us assume the contrary. Thus, $\tau^{-1}\left(  u\right)
\geq\tau^{-1}\left(  v\right)  $. But $u\neq v$ (since $u<v$), so that
$\tau^{-1}\left(  u\right)  \neq\tau^{-1}\left(  v\right)  $. Combined with
$\tau^{-1}\left(  u\right)  \geq\tau^{-1}\left(  v\right)  $, this yields
$\tau^{-1}\left(  u\right)  >\tau^{-1}\left(  v\right)  $. In other words,
$\tau^{-1}\left(  v\right)  <\tau^{-1}\left(  u\right)  $. Also, $1\leq
\tau^{-1}\left(  v\right)  $ (since $\tau^{-1}\left(  v\right)  \in\left\{
1,2,\ldots,n\right\}  $) and $\tau^{-1}\left(  u\right)  \leq n$ (since
$\tau^{-1}\left(  u\right)  \in\left\{  1,2,\ldots,n\right\}  $). Finally,
$\tau\left(  \tau^{-1}\left(  v\right)  \right)  =v>u=\tau\left(  \tau
^{-1}\left(  u\right)  \right)  $.

Thus, $\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)
$ is a pair of integers satisfying $1\leq\tau^{-1}\left(  v\right)  <\tau
^{-1}\left(  u\right)  \leq n$ and $\tau\left(  \tau^{-1}\left(  v\right)
\right)  >\tau\left(  \tau^{-1}\left(  u\right)  \right)  $. In other words,
$\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is
a pair $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\tau\left(  i\right)  >\tau\left(  j\right)  $. In other words, $\left(
\tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is an
inversion of $\tau$ (by the definition of an \textquotedblleft inversion of
$\tau$\textquotedblright). In other words, $\left(  \tau^{-1}\left(  v\right)
,\tau^{-1}\left(  u\right)  \right)  \in\operatorname*{Inv}\tau$ (since
$\operatorname*{Inv}\tau$ is the set of all inversions of $\tau$). Hence,
$\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)
\in\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $. In other words, $\left(  \tau^{-1}\left(  v\right)  ,\tau
^{-1}\left(  u\right)  \right)  $ is an inversion of $\sigma\circ\tau$ (since
$\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ is the set of all
inversions of $\sigma\circ\tau$). In other words, $\left(  \tau^{-1}\left(
v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is a pair $\left(
i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\left(
\sigma\circ\tau\right)  \left(  i\right)  >\left(  \sigma\circ\tau\right)
\left(  j\right)  $ (by the definition of an \textquotedblleft inversion of
$\sigma\circ\tau$\textquotedblright). In other words, $\left(  \tau
^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is a pair of
integers satisfying $1\leq\tau^{-1}\left(  v\right)  <\tau^{-1}\left(
u\right)  \leq n$ and $\left(  \sigma\circ\tau\right)  \left(  \tau
^{-1}\left(  v\right)  \right)  >\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  u\right)  \right)  $. But $\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  v\right)  \right)  >\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  u\right)  \right)  >\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  v\right)  \right)  $ (by
(\ref{sol.exeadd.perm.Inv.sub.a.2.pf.5})), which is absurd. This contradiction
proves that our assumption was wrong. Hence, $\tau^{-1}\left(  u\right)
<\tau^{-1}\left(  v\right)  $ is proven.

Now, $1\leq\tau^{-1}\left(  u\right)  $ (since $\tau^{-1}\left(  u\right)
\in\left\{  1,2,\ldots,n\right\}  $) and $\tau^{-1}\left(  v\right)  \leq n$
(since $\tau^{-1}\left(  v\right)  \in\left\{  1,2,\ldots,n\right\}  $).
Finally, recall that (\ref{sol.exeadd.perm.Inv.sub.a.2.pf.5}) holds. Thus,
$\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is
a pair of integers satisfying $1\leq\tau^{-1}\left(  u\right)  <\tau
^{-1}\left(  v\right)  \leq n$ and $\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  u\right)  \right)  >\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  v\right)  \right)  $. In other words, $\left(  \tau
^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\left(
\sigma\circ\tau\right)  \left(  i\right)  >\left(  \sigma\circ\tau\right)
\left(  j\right)  $. In other words, $\left(  \tau^{-1}\left(  u\right)
,\tau^{-1}\left(  v\right)  \right)  $ is an inversion of $\sigma\circ\tau$
(by the definition of an \textquotedblleft inversion of $\sigma\circ\tau
$\textquotedblright). In other words, $\left(  \tau^{-1}\left(  u\right)
,\tau^{-1}\left(  v\right)  \right)  \in\operatorname*{Inv}\left(  \sigma
\circ\tau\right)  $ (since $\operatorname*{Inv}\left(  \sigma\circ\tau\right)
$ is the set of all inversions of $\sigma\circ\tau$).

Hence, $c=\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)
\right)  \in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $.

Let us now prove that $c\notin\operatorname*{Inv}\tau$. Indeed, assume the
contrary. Thus, $c\in\operatorname*{Inv}\tau$. Thus, $\left(  \tau^{-1}\left(
u\right)  ,\tau^{-1}\left(  v\right)  \right)  =c\in\operatorname*{Inv}\tau$.
In other words, $\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(
v\right)  \right)  $ is an inversion of $\tau$ (since $\operatorname*{Inv}%
\tau$ is the set of all inversions of $\tau$). In other words, $\left(
\tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\tau\left(  i\right)  >\tau\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\tau$\textquotedblright). In other words,
$\left(  \tau^{-1}\left(  u\right)  ,\tau^{-1}\left(  v\right)  \right)  $ is
a pair of integers satisfying $1\leq\tau^{-1}\left(  u\right)  <\tau
^{-1}\left(  v\right)  \leq n$ and $\tau\left(  \tau^{-1}\left(  u\right)
\right)  >\tau\left(  \tau^{-1}\left(  v\right)  \right)  $. But $\tau\left(
\tau^{-1}\left(  u\right)  \right)  >\tau\left(  \tau^{-1}\left(  v\right)
\right)  =v$ contradicts $\tau\left(  \tau^{-1}\left(  u\right)  \right)
=u<v$. This contradiction proves that our assumption was wrong. Hence,
$c\notin\operatorname*{Inv}\tau$ is proven.

Combining $c\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ with
$c\notin\operatorname*{Inv}\tau$, we obtain $c\in\operatorname*{Inv}\left(
\sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau$.

Now, forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau$ for each $c\in\left(  \tau\times\tau\right)
^{-1}\left(  \operatorname*{Inv}\sigma\right)  $. In other words, we have
proven the inclusion%
\[
\left(  \tau\times\tau\right)  ^{-1}\left(  \operatorname*{Inv}\sigma\right)
\subseteq\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau.
\]
Combining this with the inclusion (\ref{sol.exeadd.perm.Inv.sub.a.2.pf.1}), we
obtain%
\[
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau=\left(  \tau\times\tau\right)  ^{-1}\left(
\operatorname*{Inv}\sigma\right)  .
\]
Hence,%
\[
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert =\left\vert \left(  \tau\times\tau\right)
^{-1}\left(  \operatorname*{Inv}\sigma\right)  \right\vert =\left\vert
\operatorname*{Inv}\sigma\right\vert
\]
(since the map $\tau\times\tau$ is a bijection (since $\tau\times\tau$ is
invertible)). Thus,
\[
\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \setminus
\operatorname*{Inv}\tau\right\vert =\left\vert \operatorname*{Inv}%
\sigma\right\vert =\ell\left(  \sigma\right)
\]
(by Lemma \ref{lem.sol.exeadd.perm.Inv.sub.l}). Hence,%
\begin{align*}
\ell\left(  \sigma\right)   &  =\left\vert \operatorname*{Inv}\left(
\sigma\circ\tau\right)  \setminus\operatorname*{Inv}\tau\right\vert
=\left\vert \operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right\vert
-\left\vert \operatorname*{Inv}\tau\right\vert \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(
\sigma\circ\tau\right)  \right) \\
&  =\ell\left(  \sigma\circ\tau\right)  -\ell\left(  \tau\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.exeadd.perm.Inv.sub.a.triv}%
)}\right)  .
\end{align*}
In other words, $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $.

Now, forget our assumption that $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. We thus have proven that
if $\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $, then $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $. In other words, we have proven the
implication (\ref{sol.exeadd.perm.Inv.sub.a.2}).

Combining the two implications (\ref{sol.exeadd.perm.Inv.sub.a.1}) and
(\ref{sol.exeadd.perm.Inv.sub.a.2}), we obtain the logical equivalence%
\[
\left(  \ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \right)  \ \Longleftrightarrow\ \left(
\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right)  .
\]
In other words, $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $. This solves Additional exercise \ref{exeadd.perm.Inv.sub}
\textbf{(a)}.

\textbf{(b)} Exercise \ref{exe.ps2.2.5} \textbf{(f)} (applied to $\sigma
\circ\tau$ instead of $\sigma$) yields $\ell\left(  \sigma\circ\tau\right)
=\ell\left(  \underbrace{\left(  \sigma\circ\tau\right)  ^{-1}}_{=\tau
^{-1}\circ\sigma^{-1}}\right)  =\ell\left(  \tau^{-1}\circ\sigma^{-1}\right)
$. Exercise \ref{exe.ps2.2.5} \textbf{(f)} (applied to $\tau$ instead of
$\sigma$) yields $\ell\left(  \tau\right)  =\ell\left(  \tau^{-1}\right)  $.
Exercise \ref{exe.ps2.2.5} \textbf{(f)} yields $\ell\left(  \sigma\right)
=\ell\left(  \sigma^{-1}\right)  $. Hence, $\underbrace{\ell\left(
\sigma\right)  }_{=\ell\left(  \sigma^{-1}\right)  }+\underbrace{\ell\left(
\tau\right)  }_{=\ell\left(  \tau^{-1}\right)  }=\ell\left(  \sigma
^{-1}\right)  +\ell\left(  \tau^{-1}\right)  =\ell\left(  \tau^{-1}\right)
+\ell\left(  \sigma^{-1}\right)  $.

Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(a)} (applied to
$\tau^{-1}$ and $\sigma^{-1}$ instead of $\sigma$ and $\tau$) yields that
$\ell\left(  \tau^{-1}\circ\sigma^{-1}\right)  =\ell\left(  \tau^{-1}\right)
+\ell\left(  \sigma^{-1}\right)  $ holds if and only if $\operatorname*{Inv}%
\left(  \sigma^{-1}\right)  \subseteq\operatorname*{Inv}\left(  \tau^{-1}%
\circ\sigma^{-1}\right)  $. In light of the equalities $\ell\left(  \tau
^{-1}\circ\sigma^{-1}\right)  =\ell\left(  \sigma\circ\tau\right)  $ and
$\ell\left(  \tau^{-1}\right)  +\ell\left(  \sigma^{-1}\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $, we can rewrite this as follows:%
\[
\ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)  +\ell\left(
\tau\right)  \text{ holds if and only if }\operatorname*{Inv}\left(
\sigma^{-1}\right)  \subseteq\operatorname*{Inv}\left(  \tau^{-1}\circ
\sigma^{-1}\right)  .
\]
This solves Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(b)}.

\textbf{(c)} Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(a)}
(applied to $\tau\circ\sigma^{-1}$ and $\sigma$ instead of $\sigma$ and $\tau
$) yields that $\ell\left(  \tau\circ\sigma^{-1}\circ\sigma\right)
=\ell\left(  \tau\circ\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $ holds
if and only if $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\left(
\tau\circ\sigma^{-1}\circ\sigma\right)  $. In light of $\tau\circ
\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}}=\tau\circ
\operatorname*{id}=\tau$, this rewrites as follows:%
\[
\ell\left(  \tau\right)  =\ell\left(  \tau\circ\sigma^{-1}\right)
+\ell\left(  \sigma\right)  \text{ holds if and only if }\operatorname*{Inv}%
\sigma\subseteq\operatorname*{Inv}\tau.
\]
In other words, $\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\tau$
holds if and only if $\ell\left(  \tau\right)  =\ell\left(  \tau\circ
\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $. This solves Additional
exercise \ref{exeadd.perm.Inv.sub} \textbf{(c)}.

\textbf{(d)} Assume that $\operatorname*{Inv}\sigma=\operatorname*{Inv}\tau$.
We WLOG assume that $\ell\left(  \sigma\right)  \geq\ell\left(  \tau\right)  $
(since otherwise, we can simply switch $\sigma$ and $\tau$). We have
$\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\tau$ (since
$\operatorname*{Inv}\sigma=\operatorname*{Inv}\tau$). But Additional exercise
\ref{exeadd.perm.Inv.sub} \textbf{(c)} shows that $\operatorname*{Inv}%
\sigma\subseteq\operatorname*{Inv}\tau$ holds if and only if $\ell\left(
\tau\right)  =\ell\left(  \tau\circ\sigma^{-1}\right)  +\ell\left(
\sigma\right)  $. Hence, we have $\ell\left(  \tau\right)  =\ell\left(
\tau\circ\sigma^{-1}\right)  +\ell\left(  \sigma\right)  $ (since
$\operatorname*{Inv}\sigma\subseteq\operatorname*{Inv}\tau$ holds). Thus,
$\ell\left(  \tau\right)  =\ell\left(  \tau\circ\sigma^{-1}\right)
+\underbrace{\ell\left(  \sigma\right)  }_{\geq\ell\left(  \tau\right)  }%
\geq\ell\left(  \tau\circ\sigma^{-1}\right)  +\ell\left(  \tau\right)  $.
Subtracting $\ell\left(  \tau\right)  $ from both sides of this inequality, we
obtain $0\geq\ell\left(  \tau\circ\sigma^{-1}\right)  $. In other words,
$\ell\left(  \tau\circ\sigma^{-1}\right)  \leq0$.

But $\ell\left(  \tau\circ\sigma^{-1}\right)  $ is the number of inversions of
$\tau\circ\sigma^{-1}$ (by the definition of $\ell\left(  \tau\circ\sigma
^{-1}\right)  $), and thus is a nonnegative integer. Hence, $\ell\left(
\tau\circ\sigma^{-1}\right)  \geq0$. Combining this with $\ell\left(
\tau\circ\sigma^{-1}\right)  \leq0$, we obtain $\ell\left(  \tau\circ
\sigma^{-1}\right)  =0$. Thus, Corollary \ref{cor.sol.exe.ps2.2.5.d2} (applied
to $\tau\circ\sigma^{-1}$ instead of $\sigma$) yields $\tau\circ\sigma
^{-1}=\operatorname*{id}$. Hence, $\underbrace{\tau\circ\sigma^{-1}%
}_{=\operatorname*{id}}\circ\sigma=\operatorname*{id}\circ\sigma=\sigma$, so
that $\sigma=\tau\circ\underbrace{\sigma^{-1}\circ\sigma}_{=\operatorname*{id}%
}=\tau\circ\operatorname*{id}=\tau$. This solves Additional exercise
\ref{exeadd.perm.Inv.sub} \textbf{(d)}.

\textbf{(e)} Let us first prove the logical implication%
\begin{equation}
\left(  \operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma
\circ\tau\right)  \right)  \ \Longrightarrow\ \left(  \left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  =\varnothing\right)  .
\label{sol.exeadd.perm.Inv.sub.e.1}%
\end{equation}


\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.e.1}):} Assume that
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $ holds. We will prove that $\left(  \operatorname*{Inv}%
\sigma\right)  \cap\left(  \operatorname*{Inv}\left(  \tau^{-1}\right)
\right)  =\varnothing$.

Indeed, fix $c\in\left(  \operatorname*{Inv}\sigma\right)  \cap\left(
\operatorname*{Inv}\left(  \tau^{-1}\right)  \right)  $. Thus, $c\in\left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  \subseteq\operatorname*{Inv}\sigma$ and
$c\in\left(  \operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}%
\left(  \tau^{-1}\right)  \right)  \subseteq\operatorname*{Inv}\left(
\tau^{-1}\right)  $.

We have $c\in\operatorname*{Inv}\sigma$. In other words, $c$ is an inversion
of $\sigma$ (since $\operatorname*{Inv}\sigma$ is the set of all inversions of
$\sigma$). In other words, $c$ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $\sigma\left(  i\right)  >\sigma\left(
j\right)  $ (by the definition of an \textquotedblleft inversion of $\sigma
$\textquotedblright). In other words, there exists a pair $\left(  i,j\right)
$ of integers satisfying $1\leq i<j\leq n$, $\sigma\left(  i\right)
>\sigma\left(  j\right)  $ and $c=\left(  i,j\right)  $. Let us denote this
pair $\left(  i,j\right)  $ by $\left(  u,v\right)  $. Thus, $\left(
u,v\right)  $ is a pair of integers satisfying $1\leq u<v\leq n$,
$\sigma\left(  u\right)  >\sigma\left(  v\right)  $ and $c=\left(  u,v\right)
$. We have $v>u$ (since $u<v$). Thus, $\tau\left(  \tau^{-1}\left(  v\right)
\right)  =v>u=\tau\left(  \tau^{-1}\left(  u\right)  \right)  $.

We have $\left(  u,v\right)  =c\in\operatorname*{Inv}\left(  \tau^{-1}\right)
$. In other words, $\left(  u,v\right)  $ is an inversion of $\tau^{-1}$
(since $\operatorname*{Inv}\left(  \tau^{-1}\right)  $ is the set of all
inversions of $\tau^{-1}$). In other words, $\left(  u,v\right)  $ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\tau
^{-1}\left(  i\right)  >\tau^{-1}\left(  j\right)  $ (by the definition of an
\textquotedblleft inversion of $\tau^{-1}$\textquotedblright). In other words,
$\left(  u,v\right)  $ is a pair of integers satisfying $1\leq u<v\leq n$ and
$\tau^{-1}\left(  u\right)  >\tau^{-1}\left(  v\right)  $. From $\tau
^{-1}\left(  u\right)  >\tau^{-1}\left(  v\right)  $, we obtain $\tau
^{-1}\left(  v\right)  <\tau^{-1}\left(  u\right)  $.

We have $\tau^{-1}\left(  u\right)  \in\left\{  1,2,\ldots,n\right\}  $ (since
$\tau\in S_{n}$), so that $\tau^{-1}\left(  u\right)  \leq n$. Also,
$\tau^{-1}\left(  v\right)  \in\left\{  1,2,\ldots,n\right\}  $ (since
$\tau\in S_{n}$) and thus $1\leq\tau^{-1}\left(  v\right)  $. Altogether, we
thus know that $\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)
\right)  $ is a pair of integers satisfying $1\leq\tau^{-1}\left(  v\right)
<\tau^{-1}\left(  u\right)  \leq n$ and $\tau\left(  \tau^{-1}\left(
v\right)  \right)  >\tau\left(  \tau^{-1}\left(  u\right)  \right)  $. In
other words, $\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)
\right)  $ is a pair $\left(  i,j\right)  $ of integers satisfying $1\leq
i<j\leq n$ and $\tau\left(  i\right)  >\tau\left(  j\right)  $. In other
words, $\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)
\right)  $ is an inversion of $\tau$ (by the definition of an
\textquotedblleft inversion of $\tau$\textquotedblright). In other words,
$\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)
\in\operatorname*{Inv}\tau$ (since $\operatorname*{Inv}\tau$ is the set of all
inversions of $\tau$).

Hence, $\left(  \tau^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)
\right)  \in\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(
\sigma\circ\tau\right)  $. In other words, $\left(  \tau^{-1}\left(  v\right)
,\tau^{-1}\left(  u\right)  \right)  $ is an inversion of $\sigma\circ\tau$
(since $\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ is the set of all
inversions of $\sigma\circ\tau$). In other words, $\left(  \tau^{-1}\left(
v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is a pair $\left(
i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\left(
\sigma\circ\tau\right)  \left(  i\right)  >\left(  \sigma\circ\tau\right)
\left(  j\right)  $ (by the definition of an \textquotedblleft inversion of
$\sigma\circ\tau$\textquotedblright). In other words, $\left(  \tau
^{-1}\left(  v\right)  ,\tau^{-1}\left(  u\right)  \right)  $ is a pair of
integers satisfying $1\leq\tau^{-1}\left(  v\right)  <\tau^{-1}\left(
u\right)  \leq n$ and $\left(  \sigma\circ\tau\right)  \left(  \tau
^{-1}\left(  v\right)  \right)  >\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  u\right)  \right)  $.

In particular, we have $\left(  \sigma\circ\tau\right)  \left(  \tau
^{-1}\left(  v\right)  \right)  >\left(  \sigma\circ\tau\right)  \left(
\tau^{-1}\left(  u\right)  \right)  $. This rewrites as $\sigma\left(
v\right)  >\sigma\left(  u\right)  $ (since $\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  v\right)  \right)  =\left(  \sigma\circ
\underbrace{\tau\circ\tau^{-1}}_{=\operatorname*{id}}\right)  \left(
v\right)  =\sigma\left(  v\right)  $ and $\left(  \sigma\circ\tau\right)
\left(  \tau^{-1}\left(  u\right)  \right)  =\left(  \sigma\circ
\underbrace{\tau\circ\tau^{-1}}_{=\operatorname*{id}}\right)  \left(
u\right)  =\sigma\left(  u\right)  $). But this contradicts $\sigma\left(
u\right)  >\sigma\left(  v\right)  $.

Now, forget that we fixed $c$. We thus have found a contradiction for each
$c\in\left(  \operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}%
\left(  \tau^{-1}\right)  \right)  $. Hence, there exists no $c\in\left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  $. In other words, $\left(  \operatorname*{Inv}%
\sigma\right)  \cap\left(  \operatorname*{Inv}\left(  \tau^{-1}\right)
\right)  $ is the empty set. In other words, $\left(  \operatorname*{Inv}%
\sigma\right)  \cap\left(  \operatorname*{Inv}\left(  \tau^{-1}\right)
\right)  =\varnothing$.

Now, forget our assumption that $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. We thus have proven that
if $\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $, then $\left(  \operatorname*{Inv}\sigma\right)  \cap\left(
\operatorname*{Inv}\left(  \tau^{-1}\right)  \right)  =\varnothing$. In other
words, we have proven the implication (\ref{sol.exeadd.perm.Inv.sub.e.1}).

Next, we shall prove the logical implication%
\begin{equation}
\left(  \left(  \operatorname*{Inv}\sigma\right)  \cap\left(
\operatorname*{Inv}\left(  \tau^{-1}\right)  \right)  =\varnothing\right)
\ \Longrightarrow\ \left(  \operatorname*{Inv}\tau\subseteq\operatorname*{Inv}%
\left(  \sigma\circ\tau\right)  \right)  . \label{sol.exeadd.perm.Inv.sub.e.2}%
\end{equation}


\textit{Proof of (\ref{sol.exeadd.perm.Inv.sub.e.2}):} Assume that $\left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  =\varnothing$ holds. We will prove that
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $.

Indeed, let $c\in\left(  \operatorname*{Inv}\tau\right)  \setminus\left(
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right)  $. We shall prove
a contradiction.

We have $c\in\left(  \operatorname*{Inv}\tau\right)  \setminus\left(
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right)  \subseteq
\operatorname*{Inv}\tau$. In other words, $c$ is an inversion of $\tau$ (since
$\operatorname*{Inv}\tau$ is the set of all inversions of $\tau$). In other
words, $c$ is a pair $\left(  i,j\right)  $ of integers satisfying $1\leq
i<j\leq n$ and $\tau\left(  i\right)  >\tau\left(  j\right)  $ (by the
definition of an \textquotedblleft inversion of $\tau$\textquotedblright). In
other words, there exists a pair $\left(  i,j\right)  $ of integers satisfying
$1\leq i<j\leq n$, $\tau\left(  i\right)  >\tau\left(  j\right)  $ and
$c=\left(  i,j\right)  $. Let us denote this pair $\left(  i,j\right)  $ by
$\left(  u,v\right)  $. Thus, $\left(  u,v\right)  $ is a pair of integers
satisfying $1\leq u<v\leq n$, $\tau\left(  u\right)  >\tau\left(  v\right)  $
and $c=\left(  u,v\right)  $. We have $v>u$ (since $u<v$) and $\tau\left(
v\right)  <\tau\left(  u\right)  $ (since $\tau\left(  u\right)  >\tau\left(
v\right)  $).

From $\tau\in S_{n}$, we obtain $\tau\left(  v\right)  \in\left\{
1,2,\ldots,n\right\}  $ and thus $1\leq\tau\left(  v\right)  $. From $\tau\in
S_{n}$, we obtain $\tau\left(  u\right)  \in\left\{  1,2,\ldots,n\right\}  $
and thus $\tau\left(  u\right)  \leq n$. Also, $\tau^{-1}\left(  \tau\left(
v\right)  \right)  =v>u=\tau^{-1}\left(  \tau\left(  u\right)  \right)  $.
Altogether, we thus know that $\left(  \tau\left(  v\right)  ,\tau\left(
u\right)  \right)  $ is a pair of integers satisfying $1\leq\tau\left(
v\right)  <\tau\left(  u\right)  \leq n$ and $\tau^{-1}\left(  \tau\left(
v\right)  \right)  >\tau^{-1}\left(  \tau\left(  u\right)  \right)  $. In
other words, $\left(  \tau\left(  v\right)  ,\tau\left(  u\right)  \right)  $
is a pair $\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\tau^{-1}\left(  i\right)  >\tau^{-1}\left(  j\right)  $. In other words,
$\left(  \tau\left(  v\right)  ,\tau\left(  u\right)  \right)  $ is an
inversion of $\tau^{-1}$ (by the definition of an \textquotedblleft inversion
of $\tau^{-1}$\textquotedblright). In other words, $\left(  \tau\left(
v\right)  ,\tau\left(  u\right)  \right)  \in\operatorname*{Inv}\left(
\tau^{-1}\right)  $ (since $\operatorname*{Inv}\left(  \tau^{-1}\right)  $ is
the set of all inversions of $\tau^{-1}$).

On the other hand, $\left(  u,v\right)  =c\in\left(  \operatorname*{Inv}%
\tau\right)  \setminus\left(  \operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right)  $, so that $\left(  u,v\right)  \notin%
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. From this, it is easy to
obtain that $\left(  \sigma\circ\tau\right)  \left(  u\right)  \leq\left(
\sigma\circ\tau\right)  \left(  v\right)  $\ \ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, we don't have $\left(  \sigma\circ\tau\right)
\left(  u\right)  \leq\left(  \sigma\circ\tau\right)  \left(  v\right)  $.
Hence, we have $\left(  \sigma\circ\tau\right)  \left(  u\right)  >\left(
\sigma\circ\tau\right)  \left(  v\right)  $. Thus, $\left(  u,v\right)  $ is a
pair of integers satisfying $1\leq u<v\leq n$ and $\left(  \sigma\circ
\tau\right)  \left(  u\right)  >\left(  \sigma\circ\tau\right)  \left(
v\right)  $. In other words, $\left(  u,v\right)  $ is a pair $\left(
i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and $\left(
\sigma\circ\tau\right)  \left(  i\right)  >\left(  \sigma\circ\tau\right)
\left(  j\right)  $. In other words, $\left(  u,v\right)  $ is an inversion of
$\sigma\circ\tau$ (by the definition of an \textquotedblleft inversion of
$\sigma\circ\tau$\textquotedblright). In other words, $\left(  u,v\right)
\in\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ (since
$\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $ is the set of all
inversions of $\sigma\circ\tau$). This contradicts $\left(  u,v\right)
\notin\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. This contradiction
proves that our assumption was wrong, qed.}. But the map $\sigma\circ\tau$ is
injective\footnote{\textit{Proof.} We have $\sigma\circ\tau\in S_{n}$. Hence,
$\sigma\circ\tau$ is a permutation of $\left\{  1,2,\ldots,n\right\}  $ (since
$S_{n}$ is the set of all permutations of $\left\{  1,2,\ldots,n\right\}  $).
Thus, $\sigma\circ\tau$ is a bijective map. So the map $\sigma\circ\tau$ is
bijective, and therefore injective. Qed.}. But $u<v$ and therefore $u\neq v$.
Hence, $\left(  \sigma\circ\tau\right)  \left(  u\right)  \neq\left(
\sigma\circ\tau\right)  \left(  v\right)  $ (since the map $\sigma\circ\tau$
is injective). Combining this with $\left(  \sigma\circ\tau\right)  \left(
u\right)  \leq\left(  \sigma\circ\tau\right)  \left(  v\right)  $, we obtain
$\left(  \sigma\circ\tau\right)  \left(  u\right)  <\left(  \sigma\circ
\tau\right)  \left(  v\right)  $. Hence, $\sigma\left(  \tau\left(  u\right)
\right)  =\left(  \sigma\circ\tau\right)  \left(  u\right)  <\left(
\sigma\circ\tau\right)  \left(  v\right)  =\sigma\left(  \tau\left(  v\right)
\right)  $. In other words, $\sigma\left(  \tau\left(  v\right)  \right)
>\sigma\left(  \tau\left(  u\right)  \right)  $.

Now, we know that $\left(  \tau\left(  v\right)  ,\tau\left(  u\right)
\right)  $ is a pair of integers satisfying $1\leq\tau\left(  v\right)
<\tau\left(  u\right)  \leq n$ and $\sigma\left(  \tau\left(  v\right)
\right)  >\sigma\left(  \tau\left(  u\right)  \right)  $. In other words,
$\left(  \tau\left(  v\right)  ,\tau\left(  u\right)  \right)  $ is a pair
$\left(  i,j\right)  $ of integers satisfying $1\leq i<j\leq n$ and
$\sigma\left(  i\right)  >\sigma\left(  j\right)  $. In other words, $\left(
\tau\left(  v\right)  ,\tau\left(  u\right)  \right)  $ is an inversion of
$\sigma$ (by the definition of an \textquotedblleft inversion of $\sigma
$\textquotedblright). In other words, $\left(  \tau\left(  v\right)
,\tau\left(  u\right)  \right)  \in\operatorname*{Inv}\sigma$ (since
$\operatorname*{Inv}\sigma$ is the set of all inversions of $\sigma$).
Combining this with $\left(  \tau\left(  v\right)  ,\tau\left(  u\right)
\right)  \in\operatorname*{Inv}\left(  \tau^{-1}\right)  $, we obtain $\left(
\tau\left(  v\right)  ,\tau\left(  u\right)  \right)  \in\left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  =\varnothing$. Thus, the set $\varnothing$ has at
least one element (namely, the element $\left(  \tau\left(  v\right)
,\tau\left(  u\right)  \right)  $). This contradicts the fact that the set
$\varnothing$ has no elements.

Now, forget that we fixed $c$. We thus have derived a contradiction for each
$c\in\left(  \operatorname*{Inv}\tau\right)  \setminus\left(
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right)  $. Hence, there
exists no $c\in\left(  \operatorname*{Inv}\tau\right)  \setminus\left(
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  \right)  $. In other words,
$\left(  \operatorname*{Inv}\tau\right)  \setminus\left(  \operatorname*{Inv}%
\left(  \sigma\circ\tau\right)  \right)  $ is the empty set. In other words,
$\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  $.

Now, forget our assumption that $\left(  \operatorname*{Inv}\sigma\right)
\cap\left(  \operatorname*{Inv}\left(  \tau^{-1}\right)  \right)
=\varnothing$. We thus have proven that if $\left(  \operatorname*{Inv}%
\sigma\right)  \cap\left(  \operatorname*{Inv}\left(  \tau^{-1}\right)
\right)  =\varnothing$, then $\operatorname*{Inv}\tau\subseteq
\operatorname*{Inv}\left(  \sigma\circ\tau\right)  $. In other words, we have
proven the implication (\ref{sol.exeadd.perm.Inv.sub.e.2}).

Combining the two implications (\ref{sol.exeadd.perm.Inv.sub.e.1}) and
(\ref{sol.exeadd.perm.Inv.sub.e.2}), we obtain the logical equivalence%
\[
\left(  \operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma
\circ\tau\right)  \right)  \ \Longleftrightarrow\ \left(  \left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  =\varnothing\right)  .
\]
On the other hand, Additional exercise \ref{exeadd.perm.Inv.sub} \textbf{(a)}
yields that we have the logical equivalence%
\[
\left(  \ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \right)  \ \Longleftrightarrow\ \left(
\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right)  .
\]
Hence, we have the following chain of logical equivalences:%
\begin{align*}
\left(  \ell\left(  \sigma\circ\tau\right)  =\ell\left(  \sigma\right)
+\ell\left(  \tau\right)  \right)  \  &  \Longleftrightarrow\ \left(
\operatorname*{Inv}\tau\subseteq\operatorname*{Inv}\left(  \sigma\circ
\tau\right)  \right) \\
\  &  \Longleftrightarrow\ \left(  \left(  \operatorname*{Inv}\sigma\right)
\cap\left(  \operatorname*{Inv}\left(  \tau^{-1}\right)  \right)
=\varnothing\right)  .
\end{align*}


In other words, $\ell\left(  \sigma\circ\tau\right)  =\ell\left(
\sigma\right)  +\ell\left(  \tau\right)  $ holds if and only if $\left(
\operatorname*{Inv}\sigma\right)  \cap\left(  \operatorname*{Inv}\left(
\tau^{-1}\right)  \right)  =\varnothing$. This solves Additional exercise
\ref{exeadd.perm.Inv.sub} \textbf{(e)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.prod(ai+bi)}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.prod(ai+bi)}.]\textbf{(a)} We shall solve
Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} by induction over $n$:

\textit{Induction base:} Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} holds in
the case when $n=0$\ \ \ \ \footnote{\textit{Proof.} Assume that $n=0$. We
must show that Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} holds in this case.
\par
We have $n=0$ and thus $\left[  n\right]  =\varnothing$. Hence, there is only
one subset of $\left[  n\right]  $ (namely, $\varnothing$). Therefore,%
\begin{align*}
\sum_{I\subseteq\left[  n\right]  }\left(  \prod_{i\in I}a_{i}\right)  \left(
\prod_{i\in\left[  n\right]  \setminus I}b_{i}\right)   &
=\underbrace{\left(  \prod_{i\in\varnothing}a_{i}\right)  }%
_{\substack{=\left(  \text{empty product}\right)  \\=1}}\underbrace{\left(
\prod_{i\in\left[  n\right]  \setminus\varnothing}b_{i}\right)  }%
_{\substack{=\prod_{i\in\varnothing}b_{i}\\\text{(since }\left[  n\right]
\setminus\varnothing=\left[  n\right]  =\varnothing\text{)}}}=\prod
_{i\in\varnothing}b_{i} =\left(  \text{empty product}\right)  =1.
\end{align*}
Comparing this with%
\begin{align*}
\prod_{i=1}^{n}\left(  a_{i}+b_{i}\right)   &  =\prod_{i=1}^{0}\left(
a_{i}+b_{i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=0\right) \\
&  =\left(  \text{empty product}\right)  =1,
\end{align*}
we obtain $\prod_{i=1}^{n}\left(  a_{i}+b_{i}\right)  =\sum_{I\subseteq\left[
n\right]  }\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[
n\right]  \setminus I}b_{i}\right)  $. Thus, Exercise \ref{exe.prod(ai+bi)}
\textbf{(a)} holds in the case when $n=0$.}. This completes the induction base.

\textit{Induction step:} Let $k$ be a positive integer. Assume that Exercise
\ref{exe.prod(ai+bi)} \textbf{(a)} holds in the case when $n=k-1$. We must
show that Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} holds in the case when
$n=k$.

Let $\mathbb{K}$ be a commutative ring. Let $a_{1},a_{2},\ldots,a_{k}$ be $k$
elements of $\mathbb{K}$. Let $b_{1},b_{2},\ldots,b_{k}$ be $k$ further
elements of $\mathbb{K}$. We have assumed that Exercise \ref{exe.prod(ai+bi)}
\textbf{(a)} holds in the case when $n=k-1$. Hence, we can apply Exercise
\ref{exe.prod(ai+bi)} \textbf{(a)} to $k-1$, $\left(  a_{1},a_{2}%
,\ldots,a_{k-1}\right)  $ and $\left(  b_{1},b_{2},\ldots,b_{k-1}\right)  $
instead of $n$, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ and $\left(
b_{1},b_{2},\ldots,b_{n}\right)  $. We thus obtain%
\begin{equation}
\prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)  =\sum_{I\subseteq\left[
k-1\right]  }\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[
k-1\right]  \setminus I}b_{i}\right)  . \label{sol.prod(ai+bi).short.a.iass}%
\end{equation}


Now, $k$ is a positive integer. Thus, we have $k\in\left[  k\right]  $ and
$\left[  k\right]  =\left[  k-1\right]  \cup\left\{  k\right\}  $, and
$\left[  k-1\right]  =\left[  k\right]  \setminus\left\{  k\right\}  $.

Let us introduce a notation: For any set $S$, we let $\mathcal{P}\left(
S\right)  $ denote the powerset of $S$ (that is, the set of all subsets of
$S$). Now, we observe the following fact:

\begin{statement}
\textit{Fact 1:} Let $S$ be any set. Let $s\in S$. Then, the map%
\begin{align}
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)   &  \rightarrow
\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  ,\nonumber\\
U  &  \mapsto U\cup\left\{  s\right\}
\label{sol.prod(ai+bi).short.a.fact1map}%
\end{align}
is well-defined and a bijection.
\end{statement}

Fact 1 is easy to prove\footnote{The idea behind it is that the subsets of $S$
which are \textbf{not} subsets of $S\setminus\left\{  s\right\}  $ are the
subsets of $S$ that contain $s$, and each such subset can be written in the
form $U\cup\left\{  s\right\}  $ for some $U\subseteq S\setminus\left\{
s\right\}  $.
\par
If you want to prove Fact 1 formally, you need to prove two statements:
\par
\begin{enumerate}
\item The map (\ref{sol.prod(ai+bi).short.a.fact1map}) is well-defined (i.e.,
we have $U\cup\left\{  s\right\}  \in\mathcal{P}\left(  S\right)
\setminus\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ for each
$U\in\mathcal{P}\left(  S\right)  $).
\par
\item This map is bijective.
\end{enumerate}
\par
Proving the first statement is straightforward. The best way to prove the
second statement is to show that the map
(\ref{sol.prod(ai+bi).short.a.fact1map}) has an inverse -- namely, the map
$\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  \rightarrow\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  ,\ V\mapsto V\setminus\left\{  s\right\}  $. Of course,
you would also have to show that this latter map is well-defined, too.}. We
can apply Fact 1 to $S=\left[  k\right]  $ and $s=k$; we thus conclude that
the map%
\begin{align*}
\mathcal{P}\left(  \left[  k\right]  \setminus\left\{  k\right\}  \right)   &
\rightarrow\mathcal{P}\left(  \left[  k\right]  \right)  \setminus
\mathcal{P}\left(  \left[  k\right]  \setminus\left\{  k\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  k\right\}
\end{align*}
is well-defined and a bijection. Since $\left[  k\right]  \setminus\left\{
k\right\}  =\left[  k-1\right]  $, this rewrites as follows: The map%
\begin{align*}
\mathcal{P}\left(  \left[  k-1\right]  \right)   &  \rightarrow\mathcal{P}%
\left(  \left[  k\right]  \right)  \setminus\mathcal{P}\left(  \left[
k-1\right]  \right)  ,\\
U  &  \mapsto U\cup\left\{  k\right\}
\end{align*}
is well-defined and a bijection.

But $\left[  k-1\right]  \subseteq\left[  k\right]  $. Hence, every subset of
$\left[  k-1\right]  $ is a subset of $\left[  k\right]  $.

Let us make another helpful observation:

\begin{statement}
\textit{Fact 2:} Let $I$ be a subset of $\left[  k-1\right]  $. Then,%
\begin{equation}
\prod_{i\in\left[  k\right]  \setminus I}b_{i}=b_{k}\prod_{i\in\left[
k-1\right]  \setminus I}b_{i} \label{sol.prod(ai+bi).short.a.fact2.1}%
\end{equation}
and%
\begin{equation}
\prod_{i\in I\cup\left\{  k\right\}  }a_{i}=a_{k}\prod_{i\in I}a_{i}
\label{sol.prod(ai+bi).short.a.fact2.2}%
\end{equation}
and%
\begin{equation}
\prod_{i\in\left[  k\right]  \setminus\left(  I\cup\left\{  k\right\}
\right)  }b_{i}=\prod_{i\in\left[  k-1\right]  \setminus I}b_{i}.
\label{sol.prod(ai+bi).short.a.fact2.3}%
\end{equation}

\end{statement}

\textit{Proof of Fact 2:} We have%
\[
\left(  \left[  k\right]  \setminus I\right)  \setminus\left\{  k\right\}
=\left[  k\right]  \setminus\underbrace{\left(  I\cup\left\{  k\right\}
\right)  }_{=\left\{  k\right\}  \cup I}=\left[  k\right]  \setminus\left(
\left\{  k\right\}  \cup I\right)  =\underbrace{\left(  \left[  k\right]
\setminus\left\{  k\right\}  \right)  }_{=\left[  k-1\right]  }\setminus
I=\left[  k-1\right]  \setminus I.
\]


If we had $k\in I$, then we would have $k\in I\subseteq\left[  k-1\right]  $,
which would contradict the fact that $k\notin\left[  k-1\right]  $. Thus, we
cannot have $k\in I$. In other words, we have $k\notin I$. Combining
$k\in\left[  k\right]  $ with $k\notin I$, we obtain $k\in\left[  k\right]
\setminus I$. Hence, we can split off the factor for $i=k$ from the product
$\prod_{i\in\left[  k\right]  \setminus I}b_{i}$. We thus obtain%
\[
\prod_{i\in\left[  k\right]  \setminus I}b_{i}=b_{k}\prod_{i\in\left(  \left[
k\right]  \setminus I\right)  \setminus\left\{  k\right\}  }b_{i}=b_{k}%
\prod_{i\in\left[  k-1\right]  \setminus I}b_{i}%
\]
(since $\left(  \left[  k\right]  \setminus I\right)  \setminus\left\{
k\right\}  =\left[  k-1\right]  \setminus I$). This proves
(\ref{sol.prod(ai+bi).short.a.fact2.1}).

We have $k\notin I$ and thus $\left(  I\cup\left\{  k\right\}  \right)
\setminus\left\{  k\right\}  =I$.

We have $k\in\left\{  k\right\}  \subseteq I\cup\left\{  k\right\}  $. Thus,
we can split off the factor for $i=k$ from the product $\prod_{i\in
I\cup\left\{  k\right\}  }a_{i}$. Thus, we obtain%
\[
\prod_{i\in I\cup\left\{  k\right\}  }a_{i}=a_{k}\prod_{i\in\left(
I\cup\left\{  k\right\}  \right)  \setminus\left\{  k\right\}  }a_{i}%
=a_{k}\prod_{i\in I}a_{i}%
\]
(since $\left(  I\cup\left\{  k\right\}  \right)  \setminus\left\{  k\right\}
=I$). This proves (\ref{sol.prod(ai+bi).short.a.fact2.2}).

Recall that $\left[  k\right]  \setminus\left(  I\cup\left\{  k\right\}
\right)  =\left[  k-1\right]  \setminus I$. Hence, $\prod_{i\in\left[
k\right]  \setminus\left(  I\cup\left\{  k\right\}  \right)  }b_{i}%
=\prod_{i\in\left[  k-1\right]  \setminus I}b_{i}$. This proves
(\ref{sol.prod(ai+bi).short.a.fact2.3}). Thus, the proof of Fact 2 is complete.

We have%
\begin{align}
&  \underbrace{\sum_{\substack{I\subseteq\left[  k\right]  ;\\I\in
\mathcal{P}\left(  \left[  k-1\right]  \right)  }}}_{\substack{=\sum
_{\substack{I\subseteq\left[  k\right]  ;\\I\subseteq\left[  k-1\right]
}}=\sum_{I\subseteq\left[  k-1\right]  }\\\text{(since every subset of
}\left[  k-1\right]  \\\text{is a subset of }\left[  k\right]  \text{)}%
}}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  k\right]
\setminus I}b_{i}\right) \nonumber\\
&  =\sum_{I\subseteq\left[  k-1\right]  }\left(  \prod_{i\in I}a_{i}\right)
\underbrace{\left(  \prod_{i\in\left[  k\right]  \setminus I}b_{i}\right)
}_{\substack{=b_{k}\prod_{i\in\left[  k-1\right]  \setminus I}b_{i}\\\text{(by
(\ref{sol.prod(ai+bi).short.a.fact2.1}))}}}=\sum_{I\subseteq\left[
k-1\right]  }\left(  \prod_{i\in I}a_{i}\right)  b_{k}\left(  \prod
_{i\in\left[  k-1\right]  \setminus I}b_{i}\right) \nonumber\\
&  =b_{k}\underbrace{\sum_{I\subseteq\left[  k-1\right]  }\left(  \prod_{i\in
I}a_{i}\right)  \left(  \prod_{i\in\left[  k-1\right]  \setminus I}%
b_{i}\right)  }_{\substack{=\prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)
\\\text{(by (\ref{sol.prod(ai+bi).short.a.iass}))}}}=b_{k}\prod_{i=1}%
^{k-1}\left(  a_{i}+b_{i}\right)  \label{sol.prod(ai+bi).short.a.add1}%
\end{align}
and%
\begin{align}
&  \underbrace{\sum_{\substack{I\subseteq\left[  k\right]  ;\\I\notin%
\mathcal{P}\left(  \left[  k-1\right]  \right)  }}}_{\substack{=\sum
_{\substack{I\in\mathcal{P}\left(  \left[  k\right]  \right)  ;\\I\notin%
\mathcal{P}\left(  \left[  k-1\right]  \right)  }}=\sum_{I\in\mathcal{P}%
\left(  \left[  k\right]  \right)  \setminus\mathcal{P}\left(  \left[
k-1\right]  \right)  }}}\left(  \prod_{i\in I}a_{i}\right)  \left(
\prod_{i\in\left[  k\right]  \setminus I}b_{i}\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}\left(  \left[  k\right]  \right)  \setminus
\mathcal{P}\left(  \left[  k-1\right]  \right)  }\left(  \prod_{i\in I}%
a_{i}\right)  \left(  \prod_{i\in\left[  k\right]  \setminus I}b_{i}\right)
\nonumber\\
&  =\sum_{U\in\mathcal{P}\left(  \left[  k-1\right]  \right)  }\left(
\prod_{i\in U\cup\left\{  k\right\}  }a_{i}\right)  \left(  \prod_{i\in\left[
k\right]  \setminus\left(  U\cup\left\{  k\right\}  \right)  }b_{i}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }U\cup\left\{  k\right\}  \text{ for }I\text{
in the sum,}\\
\text{since the map }\mathcal{P}\left(  \left[  k-1\right]  \right)
\rightarrow\mathcal{P}\left(  \left[  k\right]  \right)  \setminus
\mathcal{P}\left(  \left[  k-1\right]  \right)  ,\ U\mapsto U\cup\left\{
k\right\} \\
\text{is a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}\left(  \left[  k-1\right]  \right)  }%
\underbrace{\left(  \prod_{i\in I\cup\left\{  k\right\}  }a_{i}\right)
}_{\substack{=a_{k}\prod_{i\in I}a_{i}\\\text{(by
(\ref{sol.prod(ai+bi).short.a.fact2.2}))}}}\underbrace{\left(  \prod
_{i\in\left[  k\right]  \setminus\left(  I\cup\left\{  k\right\}  \right)
}b_{i}\right)  }_{\substack{=\prod_{i\in\left[  k-1\right]  \setminus I}%
b_{i}\\\text{(by (\ref{sol.prod(ai+bi).short.a.fact2.3}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}U\text{ as }I\right) \nonumber\\
&  =\sum_{I\subseteq\left[  k-1\right]  }a_{k}\left(  \prod_{i\in I}%
a_{i}\right)  \left(  \prod_{i\in\left[  k-1\right]  \setminus I}b_{i}\right)
\nonumber\\
&  =a_{k}\underbrace{\sum_{I\subseteq\left[  k-1\right]  }\left(  \prod_{i\in
I}a_{i}\right)  \left(  \prod_{i\in\left[  k-1\right]  \setminus I}%
b_{i}\right)  }_{\substack{=\prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)
\\\text{(by (\ref{sol.prod(ai+bi).short.a.iass}))}}}=a_{k}\prod_{i=1}%
^{k-1}\left(  a_{i}+b_{i}\right)  . \label{sol.prod(ai+bi).short.a.add2}%
\end{align}


Now, every subset $I$ of $\left[  k\right]  $ satisfies either $I\in
\mathcal{P}\left(  \left[  k-1\right]  \right)  $ or $I\notin\mathcal{P}%
\left(  \left[  k-1\right]  \right)  $ (but not both). Hence,%
\begin{align*}
&  \sum_{I\subseteq\left[  k\right]  }\left(  \prod_{i\in I}a_{i}\right)
\left(  \prod_{i\in\left[  k\right]  \setminus I}b_{i}\right) \\
&  =\underbrace{\sum_{\substack{I\subseteq\left[  k\right]  ;\\I\in
\mathcal{P}\left(  \left[  k-1\right]  \right)  }}\left(  \prod_{i\in I}%
a_{i}\right)  \left(  \prod_{i\in\left[  k\right]  \setminus I}b_{i}\right)
}_{\substack{=b_{k}\prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)  \\\text{(by
(\ref{sol.prod(ai+bi).short.a.add1}))}}}+\underbrace{\sum
_{\substack{I\subseteq\left[  k\right]  ;\\I\notin\mathcal{P}\left(  \left[
k-1\right]  \right)  }}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod
_{i\in\left[  k-1\right]  \setminus I}b_{i}\right)  }_{\substack{=a_{k}%
\prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)  \\\text{(by
(\ref{sol.prod(ai+bi).short.a.add2}))}}}\\
&  =b_{k}\prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)  +a_{k}\prod_{i=1}%
^{k-1}\left(  a_{i}+b_{i}\right)  =\left(  b_{k}+a_{k}\right)  \prod
_{i=1}^{k-1}\left(  a_{i}+b_{i}\right) \\
&  =\left(  a_{k}+b_{k}\right)  \prod_{i=1}^{k-1}\left(  a_{i}+b_{i}\right)
=\prod_{i=1}^{k}\left(  a_{i}+b_{i}\right)  .
\end{align*}
In other words,%
\begin{equation}
\prod_{i=1}^{k}\left(  a_{i}+b_{i}\right)  =\sum_{I\subseteq\left[  k\right]
}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  k\right]
\setminus I}b_{i}\right)  . \label{sol.prod(ai+bi).short.a.almostthere}%
\end{equation}


Now, forget that we fixed $\mathbb{K}$, $\left(  a_{1},a_{2},\ldots
,a_{k}\right)  $ and $\left(  b_{1},b_{2},\ldots,b_{k}\right)  $. We thus have
proven (\ref{sol.prod(ai+bi).short.a.almostthere}) for every commutative ring
$\mathbb{K}$, every $k$ elements $a_{1},a_{2},\ldots,a_{k}$ of $\mathbb{K}$,
and every $k$ elements $b_{1},b_{2},\ldots,b_{k}$ of $\mathbb{K}$. In other
words, we have proven that Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} holds
in the case when $n=k$. This completes the induction step. Exercise
\ref{exe.prod(ai+bi)} \textbf{(a)} is thus proven by induction.

\textbf{(b)} Let $a\in\mathbb{K}$, $b\in\mathbb{K}$ and $n\in\mathbb{N}$. We
must prove (\ref{eq.rings.(a+b)**n}).

For every $k\in\left\{  0,1,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the number of all }I\subseteq\left[  n\right]  \text{ satisfying
}\left\vert I\right\vert =k\right)  =\dbinom{n}{k}
\label{sol.prod(ai+bi).short.b.num}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.prod(ai+bi).short.b.num}):} Let
$k\in\left\{  0,1,\ldots,n\right\}  $.
\par
Clearly, $\left[  n\right]  $ is an $n$-element set. Thus, Proposition
\ref{prop.binom.subsets} (applied to $n$, $k$ and $\left[  n\right]  $ instead
of $m$, $n$ and $S$) shows that $\dbinom{n}{k}$ is the number of all
$k$-element subsets of $\left[  n\right]  $. In other words,%
\begin{align*}
\dbinom{n}{k}  &  =\left(  \text{the number of all }k\text{-element subsets of
}\left[  n\right]  \right) \\
&  =\left(  \text{the number of all }I\subseteq\left[  n\right]  \text{
satisfying }\left\vert I\right\vert =k\right)  .
\end{align*}
This proves (\ref{sol.prod(ai+bi).short.b.num}).}.

But Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} (applied to $a_{i}=a$ and
$b_{i}=b$) yields%
\[
\prod_{i=1}^{n}\left(  a+b\right)  =\sum_{I\subseteq\left[  n\right]
}\underbrace{\left(  \prod_{i\in I}a\right)  }_{=a^{\left\vert I\right\vert }%
}\underbrace{\left(  \prod_{i\in\left[  n\right]  \setminus I}b\right)
}_{\substack{=b^{\left\vert \left[  n\right]  \setminus I\right\vert
}\\=b^{\left\vert \left[  n\right]  \right\vert -\left\vert I\right\vert
}\\\text{(since }\left\vert \left[  n\right]  \setminus I\right\vert
=\left\vert \left[  n\right]  \right\vert -\left\vert I\right\vert
\\\text{(since }I\subseteq\left[  n\right]  \text{))}}}=\sum_{I\in
\mathcal{P}\left(  \left[  n\right]  \right)  }a^{\left\vert I\right\vert
}b^{\left\vert \left[  n\right]  \right\vert -\left\vert I\right\vert }.
\]
Comparing this with $\prod_{i=1}^{n}\left(  a+b\right)  =\left(  a+b\right)
^{n}$, we obtain%
\begin{align*}
\left(  a+b\right)  ^{n}  &  =\underbrace{\sum_{I\subseteq\left[  n\right]  }%
}_{\substack{=\sum_{k\in\left\{  0,1,\ldots,n\right\}  }\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=k}}\\\text{(since every subset }I\text{ of }\left[  n\right]
\\\text{satisfies }\left\vert I\right\vert \in\left\{  0,1,\ldots,n\right\}
\\\text{(because }\left\vert I\right\vert \leq\left\vert \left[  n\right]
\right\vert =n\text{))}}}a^{\left\vert I\right\vert }b^{\left\vert \left[
n\right]  \right\vert -\left\vert I\right\vert }=\sum_{k\in\left\{
0,1,\ldots,n\right\}  }\sum_{\substack{I\subseteq\left[  n\right]
;\\\left\vert I\right\vert =k}}\underbrace{a^{\left\vert I\right\vert }%
}_{\substack{=a^{k}\\\text{(since }\left\vert I\right\vert =k\text{)}%
}}\underbrace{b^{\left\vert \left[  n\right]  \right\vert -\left\vert
I\right\vert }}_{\substack{=b^{n-k}\\\text{(since }\left\vert \left[
n\right]  \right\vert =n\\\text{and }\left\vert I\right\vert =k\text{)}}}\\
&  =\underbrace{\sum_{k\in\left\{  0,1,\ldots,n\right\}  }}_{=\sum_{k=0}^{n}%
}\underbrace{\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert
I\right\vert =k}}a^{k}b^{n-k}}_{=\left(  \text{the number of all }%
I\subseteq\left[  n\right]  \text{ satisfying }\left\vert I\right\vert
=k\right)  a^{k}b^{n-k}}\\
&  =\sum_{k=0}^{n}\underbrace{\left(  \text{the number of all }I\subseteq
\left[  n\right]  \text{ satisfying }\left\vert I\right\vert =k\right)
}_{\substack{=\dbinom{n}{k}\\\text{(by (\ref{sol.prod(ai+bi).short.b.num}))}%
}}a^{k}b^{n-k}=\sum_{k=0}^{n}\dbinom{n}{k}a^{k}b^{n-k}.
\end{align*}
Thus, (\ref{eq.rings.(a+b)**n}) is proven. This solves Exercise
\ref{exe.prod(ai+bi)} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
Before we start solving Exercise \ref{exe.prod(ai+bi)}, let us introduce a notation:

\begin{definition}
\label{def.sol.prod(ai+bi).powerset}Let $S$ be a set. Then, $\mathcal{P}%
\left(  S\right)  $ will denote the powerset of $S$ (that is, the set of all
subsets of $S$).
\end{definition}

\begin{proposition}
\label{prop.sol.prod(ai+bi).powerset.lem}Let $S$ be a set. Let $s\in S$. Then,
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  \subseteq
\mathcal{P}\left(  S\right)  $. Furthermore, the map%
\begin{align*}
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)   &  \rightarrow
\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
is well-defined and a bijection.
\end{proposition}

Proposition \ref{prop.sol.prod(ai+bi).powerset.lem} is an analogue (more
precisely, a \textquotedblleft rougher\textquotedblright\ version) of
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(c)}; indeed, it
differs from Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(c)}
in that it concerns itself with arbitrary subsets rather than $m$-element and
$\left(  m-1\right)  $-element subsets. Unsurprisingly, the proof of
Proposition \ref{prop.sol.prod(ai+bi).powerset.lem} is analogous to that of
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem}. For the sake of
completeness, let me show this proof:

\begin{proof}
[Proof of Proposition \ref{prop.sol.prod(ai+bi).powerset.lem}.]We know that
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is the set of all
subsets of $S\setminus\left\{  s\right\}  $ (by the definition of
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $). Also,
$\mathcal{P}\left(  S\right)  $ is the set of all subsets of $S$ (by the
definition of $\mathcal{P}\left(  S\right)  $).

For every $U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $, we
have $U\in\mathcal{P}\left(  S\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. We must show
that $U\in\mathcal{P}\left(  S\right)  $.
\par
We have $U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. In
other words, $U$ is a subset of $S\setminus\left\{  s\right\}  $ (since
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is the set of all
subsets of $S\setminus\left\{  s\right\}  $). Thus, $U\subseteq S\setminus
\left\{  s\right\}  \subseteq S$. Therefore, $U$ is a subset of $S$. In other
words, $U\in\mathcal{P}\left(  S\right)  $ (since $\mathcal{P}\left(
S\right)  $ is the set of all subsets of $S$). Qed.}. In other words,
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  \subseteq
\mathcal{P}\left(  S\right)  $.

For every $U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $, we
have $U\cup\left\{  s\right\}  \in\mathcal{P}\left(  S\right)  \setminus
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $U\in\mathcal{P}\left(  S\setminus
\left\{  s\right\}  \right)  $. We must prove that $U\cup\left\{  s\right\}
\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  $.
\par
Let $V=U\cup\left\{  s\right\}  $.
\par
We have $U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. In
other words, $U$ is a subset of $S\setminus\left\{  s\right\}  $ (since
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is the set of all
subsets of $S\setminus\left\{  s\right\}  $). Thus, $U\subseteq S\setminus
\left\{  s\right\}  $. Now,
\[
V=\underbrace{U}_{\subseteq S\setminus\left\{  s\right\}  \subseteq S}%
\cup\underbrace{\left\{  s\right\}  }_{\substack{\subseteq S\\\text{(since
}s\in S\text{)}}}\subseteq S\cup S=S.
\]
In other words, $V$ is a subset of $S$. In other words, $V\in\mathcal{P}%
\left(  S\right)  $ (since $\mathcal{P}\left(  S\right)  $ is the set of all
subsets of $S$).
\par
Now, we shall prove that $V\notin\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  $. Indeed, assume the contrary (for the sake of
contradiction). Thus, $V\in\mathcal{P}\left(  S\setminus\left\{  s\right\}
\right)  $. In other words, $V$ is a subset of $S\setminus\left\{  s\right\}
$ (since $\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is the
set of all subsets of $S\setminus\left\{  s\right\}  $). In other words,
$V\subseteq S\setminus\left\{  s\right\}  $. Thus, $s\in\left\{  s\right\}
\subseteq U\cup\left\{  s\right\}  =V\subseteq S\setminus\left\{  s\right\}
$, which contradicts $s\notin S\setminus\left\{  s\right\}  $. This
contradiction proves that our assumption was false. Hence, $V\notin%
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is proven.
\par
Combining $V\in\mathcal{P}\left(  S\right)  $ with $V\notin\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $, we obtain $V\in\mathcal{P}\left(
S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)
$. Thus, $U\cup\left\{  s\right\}  =V\in\mathcal{P}\left(  S\right)
\setminus\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $, qed.}.
Hence, we can define a map%
\[
\alpha:\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)
\rightarrow\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)
\]
by
\begin{equation}
\left(  \alpha\left(  U\right)  =U\cup\left\{  s\right\}
\ \ \ \ \ \ \ \ \ \ \text{for every }U\in\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  \right)  .
\label{pf.prop.sol.prod(ai+bi).powerset.lem.alpha()=}%
\end{equation}
Consider this map $\alpha$.

For every $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $, we have $V\setminus\left\{
s\right\}  \in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)
$\ \ \ \ \footnote{\textit{Proof.} Let $V\in\mathcal{P}\left(  S\right)
\setminus\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. We must
prove that $V\setminus\left\{  s\right\}  \in\mathcal{P}\left(  S\setminus
\left\{  s\right\}  \right)  $.
\par
Let $W=V\setminus\left\{  s\right\}  $.
\par
We have $V\notin\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $
(since $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $).
\par
We have $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  \subseteq\mathcal{P}\left(  S\right)
$. In other words, $V$ is a subset of $S$ (since $\mathcal{P}\left(  S\right)
$ is the set of all subsets of $S$). In other words, $V\subseteq S$.
\par
Notice that $W=\underbrace{V}_{\subseteq S}\setminus\left\{  s\right\}
\subseteq S\setminus\left\{  s\right\}  $. In other words, $W$ is a subset of
$S\setminus\left\{  s\right\}  $. In other words, $W\in\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $ (since $\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $ is the set of all subsets of
$S\setminus\left\{  s\right\}  $). Hence, $V\setminus\left\{  s\right\}
=W\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. Qed.}.
Hence, we can define a map%
\[
\beta:\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus
\left\{  s\right\}  \right)  \rightarrow\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)
\]
by%
\[
\left(  \beta\left(  V\right)  =V\setminus\left\{  s\right\}
\ \ \ \ \ \ \ \ \ \ \text{for every }V\in\mathcal{P}\left(  S\right)
\setminus\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  \right)  .
\]
Consider this map $\beta$.

We have $\alpha\circ\beta=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $. We shall show that $\left(
\alpha\circ\beta\right)  \left(  V\right)  =\operatorname*{id}\left(
V\right)  $.
\par
We have $V\notin\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $
(since $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $).
\par
We have $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  \subseteq\mathcal{P}\left(  S\right)
$. In other words, $V$ is an subset of $S$ (since $\mathcal{P}\left(
S\right)  $ is the set of all subsets of $S$). In other words, $V\subseteq S$.
\par
Now, we shall prove that $s\in V$. Indeed, assume the contrary. Thus, $s\notin
V$. Hence, $V\setminus\left\{  s\right\}  =V$, so that $V=\underbrace{V}%
_{\subseteq S}\setminus\left\{  s\right\}  \subseteq S\setminus\left\{
s\right\}  $. Hence, $V$ is a subset of $S\setminus\left\{  s\right\}  $. In
other words, $V\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $
(since $\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is the set
of all subsets of $S\setminus\left\{  s\right\}  $). This contradicts
$V\notin\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. This
contradiction proves that our assumption was wrong.
\par
Hence, $s\in V$ is proven. Thus, $V\cup\left\{  s\right\}  =V$. Now,
\begin{align*}
\left(  \alpha\circ\beta\right)  \left(  V\right)   &  =\alpha\left(
\beta\left(  V\right)  \right)  =\underbrace{\beta\left(  V\right)
}_{\substack{=V\setminus\left\{  s\right\}  \\\text{(by the definition of
}\beta\text{)}}}\cup\left\{  s\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\alpha\right) \\
&  =\left(  V\setminus\left\{  s\right\}  \right)  \cup\left\{  s\right\}
=V\cup\left\{  s\right\}  =V=\operatorname*{id}\left(  V\right)  .
\end{align*}
\par
Now, forget that we fixed $V$. We thus have shown that $\left(  \alpha
\circ\beta\right)  \left(  V\right)  =\operatorname*{id}\left(  V\right)  $
for every $V\in\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $. In other words, $\alpha\circ
\beta=\operatorname*{id}$. Qed.} and $\beta\circ\alpha=\operatorname*{id}%
$\ \ \ \ \footnote{\textit{Proof.} Let $U\in\mathcal{P}\left(  S\setminus
\left\{  s\right\}  \right)  $. We shall prove that $\left(  \beta\circ
\alpha\right)  \left(  U\right)  =\operatorname*{id}\left(  U\right)  $.
\par
We have $U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. In
other words, $U$ is an subset of $S\setminus\left\{  s\right\}  $ (since
$\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $ is the set of all
subsets of $S\setminus\left\{  s\right\}  $). Hence, $U\subseteq
S\setminus\left\{  s\right\}  $.
\par
We have $s\in\left\{  s\right\}  $ and thus $s\notin S\setminus\left\{
s\right\}  $. If we had $s\in U$, then we would have $s\in U\subseteq
S\setminus\left\{  s\right\}  $, which would contradict $s\notin
S\setminus\left\{  s\right\}  $. Thus, we cannot have $s\in U$. In other
words, we have $s\notin U$. Hence, $U\setminus\left\{  s\right\}  =U$.
\par
Now,%
\begin{align*}
\left(  \beta\circ\alpha\right)  \left(  U\right)   &  =\beta\left(
\alpha\left(  U\right)  \right)  =\underbrace{\alpha\left(  U\right)
}_{\substack{=U\cup\left\{  s\right\}  \\\text{(by the definition of }%
\alpha\text{)}}}\setminus\left\{  s\right\}  \ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\beta\right) \\
&  =\left(  U\cup\left\{  s\right\}  \right)  \setminus\left\{  s\right\}
=U\setminus\left\{  s\right\}  =U=\operatorname*{id}\left(  U\right)  .
\end{align*}
\par
Now, forget that we fixed $U$. Thus, we have shown that $\left(  \beta
\circ\alpha\right)  \left(  U\right)  =\operatorname*{id}\left(  U\right)  $
for each $U\in\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)  $. In
other words, $\beta\circ\alpha=\operatorname*{id}$, qed.}. These two
equalities show that the maps $\alpha$ and $\beta$ are mutually inverse. Thus,
the map $\alpha$ is invertible, i.e., is a bijection.

But $\alpha$ is the map
\begin{align*}
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)   &  \rightarrow
\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
(because $\alpha$ is the map $\mathcal{P}\left(  S\setminus\left\{  s\right\}
\right)  \rightarrow\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(
S\setminus\left\{  s\right\}  \right)  $ defined by
(\ref{pf.prop.sol.prod(ai+bi).powerset.lem.alpha()=})).

We know that the map $\alpha$ is well-defined and a bijection. In other words,
the map
\begin{align*}
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)   &  \rightarrow
\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
is well-defined and a bijection (since $\alpha$ is the map
\begin{align*}
\mathcal{P}\left(  S\setminus\left\{  s\right\}  \right)   &  \rightarrow
\mathcal{P}\left(  S\right)  \setminus\mathcal{P}\left(  S\setminus\left\{
s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
). This proves Proposition \ref{prop.sol.prod(ai+bi).powerset.lem}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.prod(ai+bi)}.]\textbf{(a)} We shall prove that%
\begin{equation}
\prod_{i=1}^{k}\left(  a_{i}+b_{i}\right)  =\sum_{I\in\mathcal{P}\left(
\left[  k\right]  \right)  }\left(  \prod_{i\in I}a_{i}\right)  \left(
\prod_{i\in\left[  k\right]  \setminus I}b_{i}\right)
\label{sol.prod(ai+bi).a.goal}%
\end{equation}
for every $k\in\left\{  0,1,\ldots,n\right\}  $.

\textit{Proof of (\ref{sol.prod(ai+bi).a.goal}):} We shall prove
(\ref{sol.prod(ai+bi).a.goal}) by induction over $k$:

\textit{Induction base:} The equality (\ref{sol.prod(ai+bi).a.goal}) holds for
$k=0$\ \ \ \ \footnote{\textit{Proof.} Assume that $k=0$. We must prove that
the equality (\ref{sol.prod(ai+bi).a.goal}) holds.
\par
We have $k=0$, and thus
\begin{align*}
\left[  k\right]   &  =\left[  0\right]  =\left\{  1,2,\ldots,0\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left[  0\right]
\right) \\
&  =\varnothing.
\end{align*}
Hence, $\mathcal{P}\left(  \left[  k\right]  \right)  =\mathcal{P}\left(
\varnothing\right)  =\left\{  \varnothing\right\}  $. Thus, the sum
$\sum_{I\in\mathcal{P}\left(  \left[  k\right]  \right)  }\left(  \prod_{i\in
I}a_{i}\right)  \left(  \prod_{i\in\left[  k\right]  \setminus I}b_{i}\right)
$ has only one addend -- namely, the addend for $I=\varnothing$. Therefore,
this sum simplifies as follows:%
\begin{align*}
&  \sum_{I\in\mathcal{P}\left(  \left[  k\right]  \right)  }\left(
\prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  k\right]  \setminus
I}b_{i}\right) \\
&  =\underbrace{\left(  \prod_{i\in\varnothing}a_{i}\right)  }_{=\left(
\text{empty product}\right)  =1}\left(  \prod_{i\in\left[  k\right]
\setminus\varnothing}b_{i}\right)  =\prod_{i\in\left[  k\right]
\setminus\varnothing}b_{i}=\prod_{i\in\varnothing}b_{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  k\right]  \setminus
\varnothing=\left[  k\right]  =\varnothing\right) \\
&  =\left(  \text{empty product}\right)  =1.
\end{align*}
Comparing this with%
\begin{align*}
\prod_{i=1}^{k}\left(  a_{i}+b_{i}\right)   &  =\prod_{i=1}^{0}\left(
a_{i}+b_{i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k=0\right) \\
&  =\left(  \text{empty product}\right)  =1,
\end{align*}
we obtain $\prod_{i=1}^{k}\left(  a_{i}+b_{i}\right)  =\sum_{I\in
\mathcal{P}\left(  \left[  k\right]  \right)  }\left(  \prod_{i\in I}%
a_{i}\right)  \left(  \prod_{i\in\left[  k\right]  \setminus I}b_{i}\right)
$. In other words, the equality (\ref{sol.prod(ai+bi).a.goal}) holds. Qed.}.
This completes the induction base.

\textit{Induction step:} Let $K\in\left\{  0,1,\ldots,n\right\}  $ be
positive. Assume that the equality (\ref{sol.prod(ai+bi).a.goal}) holds for
$k=K-1$. We must prove that the equality (\ref{sol.prod(ai+bi).a.goal}) holds
for $k=K$.

We know that $K$ is a positive element of $\left\{  0,1,\ldots,n\right\}  $.
In other words, $K\in\left\{  1,2,\ldots,n\right\}  $. Hence, $K-1\in\left\{
0,1,\ldots,n-1\right\}  \subseteq\left\{  0,1,\ldots,n\right\}  $.

We also know that the equality (\ref{sol.prod(ai+bi).a.goal}) holds for
$k=K-1$. In other words, we have%
\begin{equation}
\prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right)  =\sum_{I\in\mathcal{P}\left(
\left[  K-1\right]  \right)  }\left(  \prod_{i\in I}a_{i}\right)  \left(
\prod_{i\in\left[  K-1\right]  \setminus I}b_{i}\right)  .
\label{sol.prod(ai+bi).a.goal.pf.iass}%
\end{equation}


But $\left[  K\right]  =\left\{  1,2,\ldots,K\right\}  $ (by the definition of
$\left[  K\right]  $) and $\left[  K-1\right]  =\left\{  1,2,\ldots
,K-1\right\}  $ (by the definition of $\left[  K-1\right]  $). Thus,%
\begin{align}
\underbrace{\left[  K\right]  }_{=\left\{  1,2,\ldots,K\right\}  }%
\setminus\left\{  K\right\}   &  =\left\{  1,2,\ldots,K\right\}
\setminus\left\{  K\right\} \nonumber\\
&  =\left\{  1,2,\ldots,K-1\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}K\text{ is a positive integer}\right) \nonumber\\
&  =\left[  K-1\right]  . \label{sol.prod(ai+bi).a.goal.pf.diff}%
\end{align}
Also, $K$ is a positive integer; hence, $K\in\left\{  1,2,\ldots,K\right\}
=\left[  K\right]  $. Thus, Proposition
\ref{prop.sol.prod(ai+bi).powerset.lem} (applied to $S=\left[  K\right]  $ and
$s=K$) yields the following two facts:

\begin{itemize}
\item We have $\mathcal{P}\left(  \left[  K\right]  \setminus\left\{
K\right\}  \right)  \subseteq\mathcal{P}\left(  \left[  K\right]  \right)  $.

\item The map%
\begin{align*}
\mathcal{P}\left(  \left[  K\right]  \setminus\left\{  K\right\}  \right)   &
\rightarrow\mathcal{P}\left(  \left[  K\right]  \right)  \setminus
\mathcal{P}\left(  \left[  K\right]  \setminus\left\{  K\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  K\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}

Using (\ref{sol.prod(ai+bi).a.goal.pf.diff}), we can rewrite these two facts
as follows:

\begin{itemize}
\item We have $\mathcal{P}\left(  \left[  K-1\right]  \right)  \subseteq
\mathcal{P}\left(  \left[  K\right]  \right)  $.

\item The map%
\begin{align*}
\mathcal{P}\left(  \left[  K-1\right]  \right)   &  \rightarrow\mathcal{P}%
\left(  \left[  K\right]  \right)  \setminus\mathcal{P}\left(  \left[
K-1\right]  \right)  ,\\
U  &  \mapsto U\cup\left\{  K\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}

Now, we shall prove a helpful observation:

\begin{statement}
\textit{Observation 1:} Let $I\in\mathcal{P}\left(  \left[  K-1\right]
\right)  $. Then,%
\begin{equation}
\prod_{i\in\left[  K\right]  \setminus I}b_{i}=b_{K}\prod_{i\in\left[
K-1\right]  \setminus I}b_{i} \label{sol.prod(ai+bi).a.goal.pf.term1}%
\end{equation}
and%
\begin{equation}
\prod_{i\in I\cup\left\{  K\right\}  }a_{i}=a_{K}\prod_{i\in I}a_{i}
\label{sol.prod(ai+bi).a.goal.pf.term2}%
\end{equation}
and%
\begin{equation}
\prod_{i\in\left[  K\right]  \setminus\left(  I\cup\left\{  K\right\}
\right)  }b_{i}=\prod_{i\in\left[  K-1\right]  \setminus I}b_{i}.
\label{sol.prod(ai+bi).a.goal.pf.term3}%
\end{equation}

\end{statement}

\textit{Proof of Observation 1:} Let $I\in\mathcal{P}\left(  \left[
K-1\right]  \right)  $. We must prove (\ref{sol.prod(ai+bi).a.goal.pf.term1}),
(\ref{sol.prod(ai+bi).a.goal.pf.term2}) and
(\ref{sol.prod(ai+bi).a.goal.pf.term3}).

We have $I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  $. In other
words, $I$ is a subset of $\left[  K-1\right]  $ (since $\mathcal{P}\left(
\left[  K-1\right]  \right)  $ is the set of all subsets of $\left[
K-1\right]  $ (by the definition of $\mathcal{P}\left(  \left[  K-1\right]
\right)  $)). In other words, $I\subseteq\left[  K-1\right]  $.

We have%
\[
\left(  \left[  K\right]  \setminus I\right)  \setminus\left\{  K\right\}
=\left[  K\right]  \setminus\underbrace{\left(  I\cup\left\{  K\right\}
\right)  }_{=\left\{  K\right\}  \cup I}=\left[  K\right]  \setminus\left(
\left\{  K\right\}  \cup I\right)  =\underbrace{\left(  \left[  K\right]
\setminus\left\{  K\right\}  \right)  }_{=\left[  K-1\right]  }\setminus
I=\left[  K-1\right]  \setminus I.
\]


If we had $K\in\left[  K-1\right]  $, then we would have $K\in\left[
K-1\right]  =\left\{  1,2,\ldots,K-1\right\}  $ and thus $K\leq K-1$; but this
would contradict $K>K-1$. Hence, we cannot have $K\in\left[  K-1\right]  $. In
other words, we have $K\notin\left[  K-1\right]  $.

If we had $K\in I$, then we would have $K\in I\subseteq\left[  K-1\right]  $,
which would contradict the fact that $K\notin\left[  K-1\right]  $. Thus, we
cannot have $K\in I$. In other words, we have $K\notin I$. Combining
$K\in\left[  K\right]  $ with $K\notin I$, we obtain $K\in\left[  K\right]
\setminus I$. Hence, we can split off the factor for $i=K$ from the product
$\prod_{i\in\left[  K\right]  \setminus I}b_{i}$. We thus obtain%
\[
\prod_{i\in\left[  K\right]  \setminus I}b_{i}=b_{K}\prod_{i\in\left(  \left[
K\right]  \setminus I\right)  \setminus\left\{  K\right\}  }b_{i}=b_{K}%
\prod_{i\in\left[  K-1\right]  \setminus I}b_{i}%
\]
(since $\left(  \left[  K\right]  \setminus I\right)  \setminus\left\{
K\right\}  =\left[  K-1\right]  \setminus I$). This proves
(\ref{sol.prod(ai+bi).a.goal.pf.term1}).

We have $K\notin I$ and thus $\left(  I\cup\left\{  K\right\}  \right)
\setminus\left\{  K\right\}  =I$.

We have $K\in\left\{  K\right\}  \subseteq I\cup\left\{  K\right\}  $. Thus,
we can split off the factor for $i=K$ from the product $\prod_{i\in
I\cup\left\{  K\right\}  }a_{i}$. Thus, we obtain%
\[
\prod_{i\in I\cup\left\{  K\right\}  }a_{i}=a_{K}\prod_{i\in\left(
I\cup\left\{  K\right\}  \right)  \setminus\left\{  K\right\}  }a_{i}%
=a_{K}\prod_{i\in I}a_{i}%
\]
(since $\left(  I\cup\left\{  K\right\}  \right)  \setminus\left\{  K\right\}
=I$). This proves (\ref{sol.prod(ai+bi).a.goal.pf.term2}).

Recall that $\left[  K\right]  \setminus\left(  I\cup\left\{  K\right\}
\right)  =\left[  K-1\right]  \setminus I$. Hence, $\prod_{i\in\left[
K\right]  \setminus\left(  I\cup\left\{  K\right\}  \right)  }b_{i}%
=\prod_{i\in\left[  K-1\right]  \setminus I}b_{i}$. This proves
(\ref{sol.prod(ai+bi).a.goal.pf.term3}). Thus, the proof of Observation 1 is complete.

Now,%
\begin{align}
&  \underbrace{\sum_{\substack{I\in\mathcal{P}\left(  \left[  K\right]
\right)  ;\\I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }%
}}_{\substack{=\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)
}\\\text{(since }\mathcal{P}\left(  \left[  K-1\right]  \right)
\subseteq\mathcal{P}\left(  \left[  K\right]  \right)  \text{)}}}\left(
\prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K\right]  \setminus
I}b_{i}\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }\left(
\prod_{i\in I}a_{i}\right)  \underbrace{\left(  \prod_{i\in\left[  K\right]
\setminus I}b_{i}\right)  }_{\substack{=b_{K}\prod_{i\in\left[  K-1\right]
\setminus I}b_{i}\\\text{(by (\ref{sol.prod(ai+bi).a.goal.pf.term1}))}}%
}=\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }\left(
\prod_{i\in I}a_{i}\right)  b_{K}\prod_{i\in\left[  K-1\right]  \setminus
I}b_{i}\nonumber\\
&  =b_{K}\underbrace{\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)
}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K-1\right]
\setminus I}b_{i}\right)  }_{\substack{=\prod_{i=1}^{K-1}\left(  a_{i}%
+b_{i}\right)  \\\text{(by (\ref{sol.prod(ai+bi).a.goal.pf.iass}))}}%
}=b_{K}\prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right)
\label{sol.prod(ai+bi).a.goal.pf.add1}%
\end{align}
and%
\begin{align}
&  \underbrace{\sum_{\substack{I\in\mathcal{P}\left(  \left[  K\right]
\right)  ;\\I\notin\mathcal{P}\left(  \left[  K-1\right]  \right)  }}}%
_{=\sum_{I\in\mathcal{P}\left(  \left[  K\right]  \right)  \setminus
\mathcal{P}\left(  \left[  K-1\right]  \right)  }}\left(  \prod_{i\in I}%
a_{i}\right)  \left(  \prod_{i\in\left[  K\right]  \setminus I}b_{i}\right)
\nonumber\\
&  =\sum_{I\in\mathcal{P}\left(  \left[  K\right]  \right)  \setminus
\mathcal{P}\left(  \left[  K-1\right]  \right)  }\left(  \prod_{i\in I}%
a_{i}\right)  \left(  \prod_{i\in\left[  K\right]  \setminus I}b_{i}\right)
\nonumber\\
&  =\sum_{U\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }\left(
\prod_{i\in U\cup\left\{  K\right\}  }a_{i}\right)  \left(  \prod_{i\in\left[
K\right]  \setminus\left(  U\cup\left\{  K\right\}  \right)  }b_{i}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }U\cup\left\{  K\right\}  \text{ for }I\text{
in the sum, since}\\
\text{the map }\mathcal{P}\left(  \left[  K-1\right]  \right)  \rightarrow
\mathcal{P}\left(  \left[  K\right]  \right)  \setminus\mathcal{P}\left(
\left[  K-1\right]  \right)  ,\ U\mapsto U\cup\left\{  K\right\} \\
\text{is a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }%
\underbrace{\left(  \prod_{i\in I\cup\left\{  K\right\}  }a_{i}\right)
}_{\substack{=a_{K}\prod_{i\in I}a_{i}\\\text{(by
(\ref{sol.prod(ai+bi).a.goal.pf.term2}))}}}\underbrace{\left(  \prod
_{i\in\left[  K\right]  \setminus\left(  I\cup\left\{  K\right\}  \right)
}b_{i}\right)  }_{\substack{=\prod_{i\in\left[  K-1\right]  \setminus I}%
b_{i}\\\text{(by (\ref{sol.prod(ai+bi).a.goal.pf.term3}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}U\text{ as }I\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }a_{K}\left(
\prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K-1\right]  \setminus
I}b_{i}\right) \nonumber\\
&  =a_{K}\underbrace{\sum_{I\in\mathcal{P}\left(  \left[  K-1\right]  \right)
}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K-1\right]
\setminus I}b_{i}\right)  }_{\substack{=\prod_{i=1}^{K-1}\left(  a_{i}%
+b_{i}\right)  \\\text{(by (\ref{sol.prod(ai+bi).a.goal.pf.iass}))}}%
}=a_{K}\prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right)  .
\label{sol.prod(ai+bi).a.goal.pf.add2}%
\end{align}


Now,%
\begin{align*}
&  \sum_{I\in\mathcal{P}\left(  \left[  K\right]  \right)  }\left(
\prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K\right]  \setminus
I}b_{i}\right) \\
&  =\underbrace{\sum_{\substack{I\in\mathcal{P}\left(  \left[  K\right]
\right)  ;\\I\in\mathcal{P}\left(  \left[  K-1\right]  \right)  }}\left(
\prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K\right]  \setminus
I}b_{i}\right)  }_{\substack{=b_{K}\prod_{i=1}^{K-1}\left(  a_{i}%
+b_{i}\right)  \\\text{(by (\ref{sol.prod(ai+bi).a.goal.pf.add1}))}%
}}+\underbrace{\sum_{\substack{I\in\mathcal{P}\left(  \left[  K\right]
\right)  ;\\I\notin\mathcal{P}\left(  \left[  K-1\right]  \right)  }}\left(
\prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  K\right]  \setminus
I}b_{i}\right)  }_{\substack{=a_{K}\prod_{i=1}^{K-1}\left(  a_{i}%
+b_{i}\right)  \\\text{(by (\ref{sol.prod(ai+bi).a.goal.pf.add2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every }I\in\mathcal{P}\left(  \left[  K\right]  \right)  \text{
satisfies either }I\in\mathcal{P}\left(  \left[  K-1\right]  \right) \\
\text{or }I\notin\mathcal{P}\left(  \left[  K-1\right]  \right)  \text{ (but
not both)}%
\end{array}
\right) \\
&  =b_{K}\prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right)  +a_{K}\prod_{i=1}%
^{K-1}\left(  a_{i}+b_{i}\right)  =\underbrace{\left(  b_{K}+a_{K}\right)
}_{=a_{K}+b_{K}}\prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right) \\
&  =\left(  a_{K}+b_{K}\right)  \prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right)  .
\end{align*}
Comparing this with%
\[
\prod_{i=1}^{K}\left(  a_{i}+b_{i}\right)  =\left(  a_{K}+b_{K}\right)
\prod_{i=1}^{K-1}\left(  a_{i}+b_{i}\right)
\]
(here, we have split off the factor for $i=K$ from the product, since
$K\in\left\{  1,2,\ldots,K\right\}  $), we obtain%
\[
\prod_{i=1}^{K}\left(  a_{i}+b_{i}\right)  =\sum_{I\in\mathcal{P}\left(
\left[  K\right]  \right)  }\left(  \prod_{i\in I}a_{i}\right)  \left(
\prod_{i\in\left[  K\right]  \setminus I}b_{i}\right)  .
\]
In other words, the equality (\ref{sol.prod(ai+bi).a.goal}) holds for $k=K$.
This completes the induction step. Thus, the induction proof of
(\ref{sol.prod(ai+bi).a.goal}) is complete.

Now, the equality (\ref{sol.prod(ai+bi).a.goal}) (applied to $k=n$) yields%
\[
\prod_{i=1}^{n}\left(  a_{i}+b_{i}\right)  =\underbrace{\sum_{I\in
\mathcal{P}\left(  \left[  n\right]  \right)  }}_{=\sum_{I\subseteq\left[
n\right]  }}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[
n\right]  \setminus I}b_{i}\right)  =\sum_{I\subseteq\left[  n\right]
}\left(  \prod_{i\in I}a_{i}\right)  \left(  \prod_{i\in\left[  n\right]
\setminus I}b_{i}\right)  .
\]
This solves Exercise \ref{exe.prod(ai+bi)} \textbf{(a)}.

\textbf{(b)} Let $a\in\mathbb{K}$, $b\in\mathbb{K}$ and $n\in\mathbb{N}$. We
must prove (\ref{eq.rings.(a+b)**n}).

The set $\mathcal{P}\left(  \left[  n\right]  \right)  $ is the set of all
subsets of $\left[  n\right]  $ (by the definition of $\mathcal{P}\left(
\left[  n\right]  \right)  $). Thus, the elements of $\mathcal{P}\left(
\left[  n\right]  \right)  $ are precisely the subsets of $\left[  n\right]  $.

Clearly, $\left\vert \underbrace{\left[  n\right]  }_{=\left\{  1,2,\ldots
,n\right\}  }\right\vert =\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert =n$. In other words, $\left[  n\right]  $ is an $n$-element set.

Every $I\in\mathcal{P}\left(  \left[  n\right]  \right)  $ satisfies
$\left\vert I\right\vert \in\left\{  0,1,\ldots,n\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Let $I\in\mathcal{P}\left(  \left[
n\right]  \right)  $. Thus, $I$ is a subset of $\left[  n\right]  $ (since
$\mathcal{P}\left(  \left[  n\right]  \right)  $ is the set of all subsets of
$\left[  n\right]  $). Thus, $I$ is a finite set (since $\left[  n\right]  $
is a finite set). Hence, $\left\vert I\right\vert \in\mathbb{N}$.
\par
But $I$ is a subset of $\left[  n\right]  $. In other words, $I\subseteq
\left[  n\right]  $, so that $\left\vert I\right\vert \leq\left\vert \left[
n\right]  \right\vert =n$. Combining this with $\left\vert I\right\vert
\in\mathbb{N}$, we obtain $\left\vert I\right\vert \in\left\{  0,1,\ldots
,n\right\}  $. Qed.}. Every $k\in\left\{  0,1,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left\vert \left\{  I\in\mathcal{P}\left(  \left[  n\right]  \right)
\ \mid\ \left\vert I\right\vert =k\right\}  \right\vert =\dbinom{n}{k}
\label{sol.prod(ai+bi).b.lem}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.prod(ai+bi).b.lem}):} Let $k\in\left\{
0,1,\ldots,n\right\}  $. Then, Proposition \ref{prop.binom.subsets} (applied
to $n$, $k$ and $\left[  n\right]  $ instead of $m$, $n$ and $S$) shows that
$\dbinom{n}{k}$ is the number of all $k$-element subsets of $\left[  n\right]
$. In other words,%
\[
\dbinom{n}{k}=\left(  \text{the number of all }k\text{-element subsets of
}\left[  n\right]  \right)  .
\]
Comparing this with%
\begin{align*}
&  \left\vert \left\{  I\in\mathcal{P}\left(  \left[  n\right]  \right)
\ \mid\ \left\vert I\right\vert =k\right\}  \right\vert \\
&  =\left(  \text{the number of all }I\in\mathcal{P}\left(  \left[  n\right]
\right)  \text{ satisfying }\left\vert I\right\vert =k\right) \\
&  =\left(  \text{the number of all subsets }I\text{ of }\left[  n\right]
\text{ satisfying }\left\vert I\right\vert =k\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the elements of }\mathcal{P}\left(
\left[  n\right]  \right)  \text{ are precisely the subsets of }\left[
n\right]  \right) \\
&  =\left(  \text{the number of all }k\text{-element subsets of }\left[
n\right]  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the subsets }I\text{ of }\left[  n\right]  \text{ satisfying
}\left\vert I\right\vert =k\text{ are}\\
\text{precisely the }k\text{-element subsets of }\left[  n\right]
\end{array}
\right)  ,
\end{align*}
we obtain $\left\vert \left\{  I\in\mathcal{P}\left(  \left[  n\right]
\right)  \ \mid\ \left\vert I\right\vert =k\right\}  \right\vert =\dbinom
{n}{k}$. This proves (\ref{sol.prod(ai+bi).b.lem}).}.

Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} (applied to $a_{i}=a$ and
$b_{i}=b$) yields%
\[
\prod_{i=1}^{n}\left(  a+b\right)  =\underbrace{\sum_{I\subseteq\left[
n\right]  }}_{=\sum_{I\in\mathcal{P}\left(  \left[  n\right]  \right)  }%
}\underbrace{\left(  \prod_{i\in I}a\right)  }_{=a^{\left\vert I\right\vert }%
}\underbrace{\left(  \prod_{i\in\left[  n\right]  \setminus I}b\right)
}_{\substack{=b^{\left\vert \left[  n\right]  \setminus I\right\vert
}\\=b^{\left\vert \left[  n\right]  \right\vert -\left\vert I\right\vert
}\\\text{(since }\left\vert \left[  n\right]  \setminus I\right\vert
=\left\vert \left[  n\right]  \right\vert -\left\vert I\right\vert
\\\text{(since }I\subseteq\left[  n\right]  \text{))}}}=\sum_{I\in
\mathcal{P}\left(  \left[  n\right]  \right)  }a^{\left\vert I\right\vert
}b^{\left\vert \left[  n\right]  \right\vert -\left\vert I\right\vert }.
\]
Comparing this with $\prod_{i=1}^{n}\left(  a+b\right)  =\left(  a+b\right)
^{n}$, we obtain%
\begin{align*}
\left(  a+b\right)  ^{n}  &  =\underbrace{\sum_{I\in\mathcal{P}\left(  \left[
n\right]  \right)  }}_{\substack{=\sum_{k\in\left\{  0,1,\ldots,n\right\}
}\sum_{\substack{I\in\mathcal{P}\left(  \left[  n\right]  \right)
;\\\left\vert I\right\vert =k}}\\\text{(since every }I\in\mathcal{P}\left(
\left[  n\right]  \right)  \\\text{satisfies }\left\vert I\right\vert
\in\left\{  0,1,\ldots,n\right\}  \text{)}}}a^{\left\vert I\right\vert
}\underbrace{b^{\left\vert \left[  n\right]  \right\vert -\left\vert
I\right\vert }}_{\substack{=b^{n-\left\vert I\right\vert }\\\text{(since
}\left\vert \left[  n\right]  \right\vert =n\text{)}}}=\sum_{k\in\left\{
0,1,\ldots,n\right\}  }\sum_{\substack{I\in\mathcal{P}\left(  \left[
n\right]  \right)  ;\\\left\vert I\right\vert =k}}\underbrace{a^{\left\vert
I\right\vert }}_{\substack{=a^{k}\\\text{(since }\left\vert I\right\vert
=k\text{)}}}\underbrace{b^{n-\left\vert I\right\vert }}_{\substack{=b^{n-k}%
\\\text{(since }\left\vert I\right\vert =k\text{)}}}\\
&  =\underbrace{\sum_{k\in\left\{  0,1,\ldots,n\right\}  }}_{=\sum_{k=0}^{n}%
}\underbrace{\sum_{\substack{I\in\mathcal{P}\left(  \left[  n\right]  \right)
;\\\left\vert I\right\vert =k}}a^{k}b^{n-k}}_{=\left\vert \left\{
I\in\mathcal{P}\left(  \left[  n\right]  \right)  \ \mid\ \left\vert
I\right\vert =k\right\}  \right\vert a^{k}b^{n-k}}\\
&  =\sum_{k=0}^{n}\underbrace{\left\vert \left\{  I\in\mathcal{P}\left(
\left[  n\right]  \right)  \ \mid\ \left\vert I\right\vert =k\right\}
\right\vert }_{\substack{=\dbinom{n}{k}\\\text{(by
(\ref{sol.prod(ai+bi).b.lem}))}}}a^{k}b^{n-k}=\sum_{k=0}^{n}\dbinom{n}{k}%
a^{k}b^{n-k}.
\end{align*}
Thus, (\ref{eq.rings.(a+b)**n}) is proven. This solves Exercise
\ref{exe.prod(ai+bi)} \textbf{(b)}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.multinom2}}

Let us first state a basic lemma about sets:

\begin{lemma}
\label{lem.prodrule.prod-assM}Let $M$ be a positive integer. For every
$i\in\left\{  1,2,\ldots,M\right\}  $, let $Z_{i}$ be a set. Then, the map
\begin{align*}
Z_{1}\times Z_{2}\times\cdots\times Z_{M}  &  \rightarrow\left(  Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.prodrule.prod-assM}.]This map is simply the canonical
bijection $Z_{1}\times Z_{2}\times\cdots\times Z_{M}\rightarrow\left(
Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M}$. Its inverse
map sends each $\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)
\in\left(  Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M}$
to $\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  \in Z_{1}\times Z_{2}%
\times\cdots\times Z_{M}$.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.prodrule.prod-assM}.]We define a map%
\[
\Phi:\left(  Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\right)  \times
Z_{M}\rightarrow Z_{1}\times Z_{2}\times\cdots\times Z_{M}%
\]
by%
\begin{equation}
\left(
\begin{array}
[c]{l}%
\Phi\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)  =\left(
s_{1},s_{2},\ldots,s_{M-1},t\right) \\
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \left(  s_{1},s_{2}%
,\ldots,s_{M-1}\right)  ,t\right)  \in\left(  Z_{1}\times Z_{2}\times
\cdots\times Z_{M-1}\right)  \times Z_{M}%
\end{array}
\right)  . \label{pf.lem.prodrule.S.Phidef}%
\end{equation}


We also define a map%
\[
\Psi:Z_{1}\times Z_{2}\times\cdots\times Z_{M}\rightarrow\left(  Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M}%
\]
by%
\begin{equation}
\left(
\begin{array}
[c]{l}%
\Psi\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right) \\
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  s_{1},s_{2},\ldots,s_{M}\right)
\in Z_{1}\times Z_{2}\times\cdots\times Z_{M}%
\end{array}
\right)  . \label{pf.lem.prodrule.S.Psidef}%
\end{equation}


We now claim that
\begin{equation}
\Psi\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  =\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,t\right)  \label{pf.lem.prodrule.S.Psi}%
\end{equation}
for every $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  \in Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}$ and $t\in Z_{M}$.

[\textit{Proof of (\ref{pf.lem.prodrule.S.Psi}):} Let $\left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}$
and $t\in Z_{M}$. We have $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  \in
Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}$; in other words,
\begin{equation}
s_{i}\in Z_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots
,M-1\right\}  . \label{pf.lem.prodrule.S.Psi.pf.1}%
\end{equation}


For every $i\in\left\{  1,2,\ldots,M\right\}  $, the element $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $Z_{i}$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\left\{  1,2,\ldots,M\right\}  $. We must prove that
\begin{equation}%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
\text{ is a well-defined element of }Z_{i}.
\label{pf.lem.prodrule.S.Psi.pf.fn1.1}%
\end{equation}
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq M$.
\par
\textit{Case 2:} We have $i=M$.
\par
Let us first consider Case 1. In this case, we have $i\neq M$. Combining this
with $i\in\left\{  1,2,\ldots,M\right\}  $, we obtain $i\in\left\{
1,2,\ldots,M\right\}  \setminus\left\{  M\right\}  =\left\{  1,2,\ldots
,M-1\right\}  $. Hence, $s_{i}\in Z_{i}$ (by (\ref{pf.lem.prodrule.S.Psi.pf.1}%
)). Thus, $s_{i}$ is a well-defined element of $Z_{i}$. In other words, $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $Z_{i}$ (since $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
=s_{i}$ (since $i<M$ (since $i\in\left\{  1,2,\ldots,M-1\right\}  $))). Thus,
(\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i=M$. But $t$ is a
well-defined element of $Z_{M}$. In other words, $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $Z_{i}$ (since $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
=t$ (since $i=M$) and $i=M$). Thus, (\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) is
proven in Case 2.
\par
We have now proven (\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) in each of the two
Cases 1 and 2. Thus, (\ref{pf.lem.prodrule.S.Psi.pf.fn1.1}) always holds
(since Cases 1 and 2 cover all possibilities). In other words, $%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
$ is a well-defined element of $Z_{i}$. Qed.}. Hence, we can define an
$M$-tuple $\left(  q_{1},q_{2},\ldots,q_{M}\right)  \in Z_{1}\times
Z_{2}\times\cdots\times Z_{M}$ by%
\[
\left(  q_{i}=%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,M\right\}
\right)  .
\]
Consider this $M$-tuple $\left(  q_{1},q_{2},\ldots,q_{M}\right)  $. The
definition of this $M$-tuple shows that%
\[
\left(  q_{1},q_{2},\ldots,q_{M}\right)  =\left(  s_{1},s_{2},\ldots
,s_{M-1},t\right)  .
\]


For every $i\in\left\{  1,2,\ldots,M-1\right\}  $, we have%
\[
q_{i}=%
\begin{cases}
s_{i}, & \text{if }i<M;\\
t, & \text{if }i=M
\end{cases}
=s_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<M\text{ (since }i\in\left\{
1,2,\ldots,M-1\right\}  \text{)}\right)  .
\]
In other words, $\left(  q_{1},q_{2},\ldots,q_{M-1}\right)  =\left(
s_{1},s_{2},\ldots,s_{M-1}\right)  $. Also, the definition of $q_{M}$ shows
that $q_{M}=%
\begin{cases}
s_{M}, & \text{if }M<M;\\
t, & \text{if }M=M
\end{cases}
=t$ (since $M=M$).

But the definition of $\Psi$ yields
\[
\Psi\left(  q_{1},q_{2},\ldots,q_{M}\right)  =\left(  \underbrace{\left(
q_{1},q_{2},\ldots,q_{M-1}\right)  }_{=\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  },\underbrace{q_{M}}_{=t}\right)  =\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)  .
\]
Comparing this with $\Psi\underbrace{\left(  q_{1},q_{2},\ldots,q_{M}\right)
}_{=\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  }=\Psi\left(  s_{1}%
,s_{2},\ldots,s_{M-1},t\right)  $, we obtain
\[
\Psi\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  =\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,t\right)  .
\]
This proves (\ref{pf.lem.prodrule.S.Psi}).]

Now, we have $\Phi\circ\Psi=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} Let $\alpha\in Z_{1}\times Z_{2}\times
\cdots\times Z_{M}$. Thus, we can write $\alpha$ in the form $\left(
s_{1},s_{2},\ldots,s_{M}\right)  $ for some $\left(  s_{1},s_{2},\ldots
,s_{M}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{M}$. Consider this
$\left(  s_{1},s_{2},\ldots,s_{M}\right)  $. Hence, $\alpha=\left(
s_{1},s_{2},\ldots,s_{M}\right)  $.
\par
Now,%
\begin{align*}
\left(  \Phi\circ\Psi\right)  \left(  \underbrace{\alpha}_{=\left(
s_{1},s_{2},\ldots,s_{M}\right)  }\right)   &  =\left(  \Phi\circ\Psi\right)
\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\Phi\left(  \underbrace{\Psi\left(
s_{1},s_{2},\ldots,s_{M}\right)  }_{\substack{=\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)  \\\text{(by the definition of
}\Psi\text{)}}}\right) \\
&  =\Phi\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
=\left(  s_{1},s_{2},\ldots,s_{M-1},s_{M}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right) \\
&  =\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\alpha=\operatorname*{id}\left(
\alpha\right)  .
\end{align*}
\par
Let us now forget that we fixed $\alpha$. We thus have shown that $\left(
\Phi\circ\Psi\right)  \left(  \alpha\right)  =\operatorname*{id}\left(
\alpha\right)  $ for every $\alpha\in Z_{1}\times Z_{2}\times\cdots\times
Z_{M}$. In other words, $\Phi\circ\Psi=\operatorname*{id}$, qed.} and
$\Psi\circ\Phi=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let
$\beta\in\left(  Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\right)  \times
Z_{M}$. Thus, we can write $\beta$ in the form $\left(  \gamma,t\right)  $ for
some $\gamma\in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}$ and $t\in Z_{M}$.
Consider these $\gamma$ and $t$. Thus, $\beta=\left(  \gamma,t\right)  $.
\par
We have $\gamma\in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}$. Thus, we can
write $\gamma$ in the form $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  $ for
some $\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  \in Z_{1}\times Z_{2}%
\times\cdots\times Z_{M-1}$. Consider this $\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  $. Hence, $\gamma=\left(  s_{1},s_{2},\ldots,s_{M-1}\right)
$.
\par
Now, $\beta=\left(  \underbrace{\gamma}_{=\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  },t\right)  =\left(  \left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  ,t\right)  $. Applying the map $\Psi\circ\Phi$ to both sides
of this equality, we obtain%
\begin{align*}
\left(  \Psi\circ\Phi\right)  \left(  \beta\right)   &  =\left(  \Psi\circ
\Phi\right)  \left(  \left(  s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)
=\Psi\left(  \underbrace{\Phi\left(  \left(  s_{1},s_{2},\ldots,s_{M-1}%
\right)  ,t\right)  }_{\substack{=\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)
\\\text{(by the definition of }\Phi\text{)}}}\right) \\
&  =\Psi\left(  s_{1},s_{2},\ldots,s_{M-1},t\right)  =\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,t\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.lem.prodrule.S.Psi})}\right) \\
&  =\beta=\operatorname*{id}\left(  \beta\right)  .
\end{align*}
\par
Let us now forget that we fixed $\beta$. We thus have shown that $\left(
\Psi\circ\Phi\right)  \left(  \beta\right)  =\operatorname*{id}\left(
\beta\right)  $ for every $\beta\in\left(  Z_{1}\times Z_{2}\times\cdots\times
Z_{M-1}\right)  \times Z_{M}$. In other words, $\Psi\circ\Phi
=\operatorname*{id}$, qed.}. These two equalities show that the maps $\Phi$
and $\Psi$ are mutually inverse. Thus, the map $\Psi$ is invertible. In other
words, the map $\Psi$ is a bijection.

Now, $\Psi$ is the map
\begin{align*}
Z_{1}\times Z_{2}\times\cdots\times Z_{M}  &  \rightarrow\left(  Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
(because $\Psi$ is the map $Z_{1}\times Z_{2}\times\cdots\times Z_{M}%
\rightarrow\left(  Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\right)  \times
Z_{M}$ satisfying (\ref{pf.lem.prodrule.S.Psidef})). Thus, the map
\begin{align*}
Z_{1}\times Z_{2}\times\cdots\times Z_{M}  &  \rightarrow\left(  Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection (since $\Psi$ is a bijection). This proves Lemma
\ref{lem.prodrule.prod-assM}.
\end{proof}
\end{verlong}

In the remainder of this section, we shall use the notation $\mathbf{m}\left(
k_{1},k_{2},\ldots,k_{m}\right)  $ as defined in Exercise \ref{exe.multinom2}.

Before we solve Exercise \ref{exe.multinom2}, let us record some really
trivial facts:

\begin{lemma}
\label{lem.sol.multinom2.0-tup-m}Each $0$-tuple $\left(  k_{1},k_{2}%
,\ldots,k_{0}\right)  \in\mathbb{N}^{0}$ satisfies $\mathbf{m}\left(
k_{1},k_{2},\ldots,k_{0}\right)  =1$.
\end{lemma}

(Note that there exists only one $0$-tuple $\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \in\mathbb{N}^{0}$, namely the empty list $\left(  {}\right)
$. Thus, the word \textquotedblleft each\textquotedblright\ in Lemma
\ref{lem.sol.multinom2.0-tup-m} is somewhat misleading.)

\begin{proof}
[Proof of Lemma \ref{lem.sol.multinom2.0-tup-m}.]Let $\left(  k_{1}%
,k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}$ be a $0$-tuple. Then, the
definition of $\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{0}\right)  $ yields%
\begin{align*}
\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{0}\right)   &  =\dfrac{\left(
k_{1}+k_{2}+\cdots+k_{0}\right)  !}{k_{1}!k_{2}!\cdots k_{0}!}\\
&  =\left(  \underbrace{k_{1}+k_{2}+\cdots+k_{0}}_{=\left(  \text{empty
sum}\right)  =0}\right)  !/\left(  \underbrace{k_{1}!k_{2}!\cdots k_{0}%
!}_{=\left(  \text{empty product}\right)  =1}\right) \\
&  =0!/1=0!=1.
\end{align*}
This proves Lemma \ref{lem.sol.multinom2.0-tup-m}.
\end{proof}

\begin{lemma}
\label{lem.sol.multinom2.prod}Let $M$ be a positive integer. Let $\left(
s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}^{M}$ and $n\in\mathbb{N}$ be
such that $s_{1}+s_{2}+\cdots+s_{M}=n$. Then,%
\[
\dbinom{n}{s_{M}}\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)
=\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.multinom2.prod}.]We have%
\[
\left(  s_{1}+s_{2}+\cdots+s_{M-1}\right)  +s_{M}=s_{1}+s_{2}+\cdots+s_{M}=n.
\]
Subtracting $s_{M}$ from both sides of this equality, we obtain $s_{1}%
+s_{2}+\cdots+s_{M-1}=n-s_{M}$.

The definition of $\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  $
yields%
\begin{equation}
\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  =\dfrac{\left(
s_{1}+s_{2}+\cdots+s_{M-1}\right)  !}{s_{1}!s_{2}!\cdots s_{M-1}!}%
=\dfrac{\left(  n-s_{M}\right)  !}{s_{1}!s_{2}!\cdots s_{M-1}!}
\label{pf.lem.sol.multinom2.prod.1}%
\end{equation}
(since $s_{1}+s_{2}+\cdots+s_{M-1}=n-s_{M}$).

The definition of $\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M}\right)  $ yields%
\begin{equation}
\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M}\right)  =\dfrac{\left(  s_{1}%
+s_{2}+\cdots+s_{M}\right)  !}{s_{1}!s_{2}!\cdots s_{M}!}=\dfrac{n!}%
{s_{1}!s_{2}!\cdots s_{M}!} \label{pf.lem.sol.multinom2.prod.2}%
\end{equation}
(since $s_{1}+s_{2}+\cdots+s_{M}=n$).

\begin{verlong}
On the other hand, $n=\underbrace{\left(  s_{1}+s_{2}+\cdots+s_{M-1}\right)
}_{\geq0}+s_{M}\geq s_{M}$. Hence, Proposition \ref{prop.binom.formula}
(applied to $n$ and $s_{M}$ instead of $m$ and $n$) yields $\dbinom{n}{s_{M}%
}=\dfrac{n!}{s_{M}!\left(  n-s_{M}\right)  !}$. Multiplying this equality with
(\ref{pf.lem.sol.multinom2.prod.1}), we obtain%
\begin{align*}
&  \dbinom{n}{s_{M}}\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right) \\
&  =\dfrac{n!}{s_{M}!\left(  n-s_{M}\right)  !}\cdot\dfrac{\left(
n-s_{M}\right)  !}{s_{1}!s_{2}!\cdots s_{M-1}!}=\dfrac{n!}{\left(  s_{1}%
!s_{2}!\cdots s_{M-1}!\right)  s_{M}!}=\dfrac{n!}{s_{1}!s_{2}!\cdots s_{M}!}\\
&  =\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.sol.multinom2.prod.2}%
)}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.multinom2.prod}.
\end{verlong}

\begin{verlong}
On the other hand, $s_{1},s_{2},\ldots,s_{M}$ are elements of $\mathbb{N}$
(since $\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}^{M}$). Hence,
$s_{1}+s_{2}+\cdots+s_{M-1}\in\mathbb{N}$ and $s_{M}\in\mathbb{N}$.

We have
\[
n=s_{1}+s_{2}+\cdots+s_{M}=\underbrace{\left(  s_{1}+s_{2}+\cdots
+s_{M-1}\right)  }_{\substack{\geq0\\\text{(since }s_{1}+s_{2}+\cdots
+s_{M-1}\in\mathbb{N}\text{)}}}+s_{M}\geq s_{M}.
\]
Hence, Proposition \ref{prop.binom.formula} (applied to $n$ and $s_{M}$
instead of $m$ and $n$) yields $\dbinom{n}{s_{M}}=\dfrac{n!}{s_{M}!\left(
n-s_{M}\right)  !}$. Multiplying this equality with
(\ref{pf.lem.sol.multinom2.prod.1}), we obtain%
\begin{align*}
&  \dbinom{n}{s_{M}}\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right) \\
&  =\dfrac{n!}{s_{M}!\left(  n-s_{M}\right)  !}\cdot\dfrac{\left(
n-s_{M}\right)  !}{s_{1}!s_{2}!\cdots s_{M-1}!}=\dfrac{n!}{s_{M}!}\cdot
\dfrac{1}{s_{1}!s_{2}!\cdots s_{M-1}!}\\
&  =\dfrac{n!}{\left(  s_{1}!s_{2}!\cdots s_{M-1}!\right)  s_{M}!}\\
&  =\dfrac{n!}{s_{1}!s_{2}!\cdots s_{M}!}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\left(  s_{1}!s_{2}!\cdots s_{M-1}!\right)  s_{M}!=s_{1}%
!s_{2}!\cdots s_{M}!\right) \\
&  =\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.sol.multinom2.prod.2}%
)}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.multinom2.prod}.
\end{verlong}
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.multinom2}.]We shall solve Exercise
\ref{exe.multinom2} by induction over $m$:

\begin{vershort}
\textit{Induction base:} Exercise \ref{exe.multinom2} holds for $m=0$%
\ \ \ \ \footnote{\textit{Proof.} Assume that $m=0$. We must then show that
Exercise \ref{exe.multinom2} holds.
\par
From $m=0$, we obtain $a_{1}+a_{2}+\cdots+a_{m}=a_{1}+a_{2}+\cdots
+a_{0}=\left(  \text{empty sum}\right)  =0$.
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $n=0$.
\par
\textit{Case 2:} We have $n\neq0$.
\par
Let us first consider Case 1. In this case, we have $n=0$. Hence, $0^{n}%
=0^{0}=1$. There exists exactly one $0$-tuple $\left(  k_{1},k_{2}%
,\ldots,k_{0}\right)  \in\mathbb{N}^{0}$ (namely, the empty list $\left(
{}\right)  $), and this $0$-tuple $\left(  k_{1},k_{2},\ldots,k_{0}\right)  $
satisfies $k_{1}+k_{2}+\cdots+k_{0}=\left(  \text{empty sum}\right)  =0=n$.
Hence, there exists exactly one $0$-tuple $\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \in\mathbb{N}^{0}$ satisfying $k_{1}+k_{2}+\cdots+k_{0}=n$. The
sum $\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in
\mathbb{N}^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}1$ therefore has exactly $1$
addend; thus, this sum rewrites as $\sum_{\substack{\left(  k_{1},k_{2}%
,\ldots,k_{0}\right)  \in\mathbb{N}^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}1=1$.
\par
But recall that $m=0$. Hence,%
\begin{align*}
&  \sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}%
^{m};\\k_{1}+k_{2}+\cdots+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{m}\right)  \prod_{i=1}^{m}a_{i}^{k_{i}}\\
&  =\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}%
^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}\underbrace{\mathbf{m}\left(  k_{1}%
,k_{2},\ldots,k_{0}\right)  }_{\substack{=1\\\text{(by Lemma
\ref{lem.sol.multinom2.0-tup-m})}}}\underbrace{\prod_{i=1}^{0}a_{i}^{k_{i}}%
}_{=\left(  \text{empty product}\right)  =1}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0};\\k_{1}+k_{2}+\cdots
+k_{0}=n}}1=1.
\end{align*}
Comparing this with%
\[
\left(  \underbrace{a_{1}+a_{2}+\cdots+a_{m}}_{=\left(  \text{empty
sum}\right)  =0}\right)  ^{n}=0^{n}=1,
\]
we obtain%
\[
\left(  a_{1}+a_{2}+\cdots+a_{m}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}^{m};\\k_{1}+k_{2}+\cdots
+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{m}\right)  \prod_{i=1}%
^{m}a_{i}^{k_{i}}.
\]
Hence, Exercise \ref{exe.multinom2} holds. We thus have solved Exercise
\ref{exe.multinom2} in Case 1.
\par
Let us now consider Case 2. In this case, we have $n\neq0$. Hence, $n>0$
(since $n\in\mathbb{N}$). Thus, $0^{n}=0$.
\par
Each $0$-tuple $\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}$
satisfies $k_{1}+k_{2}+\cdots+k_{0}=\left(  \text{empty sum}\right)  =0\neq
n$. In other words, no $\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in
\mathbb{N}^{0}$ satisfies $k_{1}+k_{2}+\cdots+k_{0}=n$. Hence, the sum
$\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}%
^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \prod_{i=1}^{0}a_{i}^{k_{i}}$ is an empty sum. Therefore, this
sum rewrites as follows:%
\[
\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}%
^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \prod_{i=1}^{0}a_{i}^{k_{i}}=\left(  \text{empty sum}\right)
=0.
\]
\par
Now, recall that $m=0$. Hence,%
\[
\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}%
^{m};\\k_{1}+k_{2}+\cdots+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{m}\right)  \prod_{i=1}^{m}a_{i}^{k_{i}}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0};\\k_{1}+k_{2}+\cdots
+k_{0}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{0}\right)  \prod_{i=1}%
^{0}a_{i}^{k_{i}}=0.
\]
Comparing this with%
\[
\left(  \underbrace{a_{1}+a_{2}+\cdots+a_{m}}_{=\left(  \text{empty
sum}\right)  =0}\right)  ^{n}=0^{n}=0,
\]
we obtain%
\[
\left(  a_{1}+a_{2}+\cdots+a_{m}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}^{m};\\k_{1}+k_{2}+\cdots
+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{m}\right)  \prod_{i=1}%
^{m}a_{i}^{k_{i}}.
\]
Hence, Exercise \ref{exe.multinom2} holds. We thus have solved Exercise
\ref{exe.multinom2} in Case 2.
\par
We thus have solved Exercise \ref{exe.multinom2} in each of the two Cases 1
and 2. Thus, Exercise \ref{exe.multinom2} always holds (under the assumption
that $m=0$). Qed.}. This completes the induction base.
\end{vershort}

\begin{verlong}
\textit{Induction base:} Exercise \ref{exe.multinom2} holds for $m=0$%
\ \ \ \ \footnote{\textit{Proof.} Assume that $m=0$. We must then show that
Exercise \ref{exe.multinom2} holds.
\par
From $m=0$, we obtain $a_{1}+a_{2}+\cdots+a_{m}=a_{1}+a_{2}+\cdots
+a_{0}=\left(  \text{empty sum}\right)  =0$.
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $n=0$.
\par
\textit{Case 2:} We have $n\neq0$.
\par
Let us first consider Case 1. In this case, we have $n=0$. There exists
exactly one $0$-tuple $\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in
\mathbb{N}^{0}$ (namely, the empty list $\left(  {}\right)  $). Hence, the sum
$\sum_{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}}1$ has
exactly one addend (namely, the addend corresponding to $\left(  k_{1}%
,k_{2},\ldots,k_{0}\right)  =\left(  {}\right)  $). Hence, this sum rewrites
as follows:%
\[
\sum_{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}}1=1.
\]
But recall that $m=0$. Hence,%
\begin{align*}
&  \sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}%
^{m};\\k_{1}+k_{2}+\cdots+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{m}\right)  \prod_{i=1}^{m}a_{i}^{k_{i}}\\
&  =\underbrace{\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)
\in\mathbb{N}^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}}_{\substack{=\sum_{\left(
k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}}\\\text{(since each
}\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}\text{
automatically}\\\text{satisfies }k_{1}+k_{2}+\cdots+k_{0}=n\\\text{(because
for each }\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}%
\text{,}\\\text{we have }k_{1}+k_{2}+\cdots+k_{0}=\left(  \text{empty
sum}\right)  =0=n\text{))}}}\underbrace{\mathbf{m}\left(  k_{1},k_{2}%
,\ldots,k_{0}\right)  }_{\substack{=1\\\text{(by Lemma
\ref{lem.sol.multinom2.0-tup-m})}}}\underbrace{\prod_{i=1}^{0}a_{i}^{k_{i}}%
}_{=\left(  \text{empty product}\right)  =1}\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}}1=1.
\end{align*}
Comparing this with%
\begin{align*}
\left(  \underbrace{a_{1}+a_{2}+\cdots+a_{m}}_{=\left(  \text{empty
sum}\right)  =0}\right)  ^{n}  &  =0^{n}=0^{0}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }n=0\right) \\
&  =1,
\end{align*}
we obtain%
\[
\left(  a_{1}+a_{2}+\cdots+a_{m}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}^{m};\\k_{1}+k_{2}+\cdots
+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{m}\right)  \prod_{i=1}%
^{m}a_{i}^{k_{i}}.
\]
Hence, Exercise \ref{exe.multinom2} holds. We thus have solved Exercise
\ref{exe.multinom2} in Case 1.
\par
Let us now consider Case 2. In this case, we have $n\neq0$. Hence, $n>0$
(since $n\in\mathbb{N}$).
\par
Each $0$-tuple $\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}^{0}$
satisfies $k_{1}+k_{2}+\cdots+k_{0}=\left(  \text{empty sum}\right)  =0\neq
n$. In other words, no $\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in
\mathbb{N}^{0}$ satisfies $k_{1}+k_{2}+\cdots+k_{0}=n$. Hence, the sum
$\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}%
^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \prod_{i=1}^{0}a_{i}^{k_{i}}$ is an empty sum. Therefore, this
sum rewrites as follows:%
\[
\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}%
^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \prod_{i=1}^{0}a_{i}^{k_{i}}=\left(  \text{empty sum}\right)
=0.
\]
\par
Now, recall that $m=0$. Hence,%
\begin{align*}
&  \sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}%
^{m};\\k_{1}+k_{2}+\cdots+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{m}\right)  \prod_{i=1}^{m}a_{i}^{k_{i}}\\
&  =\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in\mathbb{N}%
^{0};\\k_{1}+k_{2}+\cdots+k_{0}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \prod_{i=1}^{0}a_{i}^{k_{i}}=0.
\end{align*}
Comparing this with%
\[
\left(  \underbrace{a_{1}+a_{2}+\cdots+a_{m}}_{=\left(  \text{empty
sum}\right)  =0}\right)  ^{n}=0^{n}=0\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}n>0\right)  ,
\]
we obtain%
\[
\left(  a_{1}+a_{2}+\cdots+a_{m}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{m}\right)  \in\mathbb{N}^{m};\\k_{1}+k_{2}+\cdots
+k_{m}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{m}\right)  \prod_{i=1}%
^{m}a_{i}^{k_{i}}.
\]
Hence, Exercise \ref{exe.multinom2} holds. We thus have solved Exercise
\ref{exe.multinom2} in Case 2.
\par
We thus have solved Exercise \ref{exe.multinom2} in each of the two Cases 1
and 2. Thus, Exercise \ref{exe.multinom2} always holds (under the assumption
that $m=0$). Qed.}. This completes the induction base.
\end{verlong}

\textit{Induction step:} Fix a positive integer $M\in\left\{  0,1,\ldots
,m\right\}  $. Assume that Exercise \ref{exe.multinom2} holds for $m=M-1$. We
now must show that Exercise \ref{exe.multinom2} holds for $m=M$.

We have assumed that Exercise \ref{exe.multinom2} holds for $m=M-1$. In other
words, the following fact holds:

\begin{statement}
\textit{Fact 1:} Let $\mathbb{K}$ be a commutative ring. Let $a_{1}%
,a_{2},\ldots,a_{M-1}$ be $M-1$ elements of $\mathbb{K}$. Let $n\in\mathbb{N}%
$. Then,%
\[
\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1};\\k_{1}+k_{2}%
+\cdots+k_{M-1}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
\prod_{i=1}^{M-1}a_{i}^{k_{i}}.
\]

\end{statement}

We must show that Exercise \ref{exe.multinom2} holds for $m=M$. In other
words, we must prove the following fact:

\begin{statement}
\textit{Fact 2:} Let $\mathbb{K}$ be a commutative ring. Let $a_{1}%
,a_{2},\ldots,a_{M}$ be $M$ elements of $\mathbb{K}$. Let $n\in\mathbb{N}$.
Then,%
\[
\left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{M}\right)  \in\mathbb{N}^{M};\\k_{1}+k_{2}+\cdots
+k_{M}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M}\right)  \prod_{i=1}%
^{M}a_{i}^{k_{i}}.
\]

\end{statement}

\begin{vershort}
[\textit{Proof of Fact 2:} We have
\[
a_{1}+a_{2}+\cdots+a_{M}=\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)
+a_{M}=a_{M}+\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  .
\]
Taking both sides of this equality to the $n$-th power, we obtain%
\begin{align}
&  \left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}\nonumber\\
&  =\left(  a_{M}+\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  \right)
^{n}\nonumber\\
&  =\sum_{k=0}^{n}\dbinom{n}{k}a_{M}^{k}\left(  a_{1}+a_{2}+\cdots
+a_{M-1}\right)  ^{n-k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.rings.(a+b)**n}) (applied to
}a=a_{M}\text{ and }b=a_{1}+a_{2}+\cdots+a_{M-1}\text{)}\right) \nonumber\\
&  =\sum_{k=0}^{n}\dbinom{n}{k}\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)
^{n-k}a_{M}^{k}\nonumber\\
&  =\underbrace{\sum_{r=0}^{n}}_{=\sum_{\substack{r\in\mathbb{N};\\r\leq n}%
}}\dbinom{n}{r}\underbrace{\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  ^{n-r}%
}_{\substack{=\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
\in\mathbb{N}^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}\mathbf{m}\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\\\text{(by
Fact 1, applied to }n-r\text{ instead of }r\text{)}}}a_{M}^{r}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}k\text{ as }r\right) \nonumber\\
&  =\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\dbinom{n}{r}\left(
\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}%
^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}\mathbf{m}\left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}%
^{r}\nonumber\\
&  =\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1};\\k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{M-1}\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}.
\label{sol.multinom2.short.3}%
\end{align}


On the other hand, each $\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N}$ satisfying $k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r$ must automatically satisfy $r\leq n$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  \left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N}$ be
such that $k_{1}+k_{2}+\cdots+k_{M-1}=n-r$. Then, $n-r=k_{1}+k_{2}%
+\cdots+k_{M-1}\geq0$, so that $r\leq n$.}. Hence, we have the following
equality of summation signs:%
\[
\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)  ,r\right)
\in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r;\\r\leq
n}}=\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}.
\]
Now, we have the following equality of summation signs:%
\begin{align*}
\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\sum_{\substack{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}  &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
\in\mathbb{N}^{M-1}}\sum_{\substack{r\in\mathbb{N};\\k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r;\\r\leq n}}\\
&  =\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r;\\r\leq n}}=\sum_{\substack{\left(  \left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N}%
;\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}.
\end{align*}
Hence, (\ref{sol.multinom2.short.3}) becomes%
\begin{align}
&  \left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}\nonumber\\
&  =\underbrace{\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\sum
_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}%
^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}}_{=\sum_{\substack{\left(  \left(
k_{1},k_{2},\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}%
\times\mathbb{N};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}}\dbinom{n}{r}%
\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \left(  \prod_{i=1}%
^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}\nonumber\\
&  =\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}%
\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}.
\label{sol.multinom2.short.4}%
\end{align}


But Lemma \ref{lem.prodrule.prod-assM} (applied to $Z_{i}=\mathbb{N}$) shows
that the map%
\begin{align*}
\underbrace{\mathbb{N}\times\mathbb{N}\times\cdots\times\mathbb{N}}_{M\text{
factors}}  &  \rightarrow\left(  \underbrace{\mathbb{N}\times\mathbb{N}%
\times\cdots\times\mathbb{N}}_{M-1\text{ factors}}\right)  \times\mathbb{N},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection. Since $\underbrace{\mathbb{N}\times\mathbb{N}\times
\cdots\times\mathbb{N}}_{M\text{ factors}}=\mathbb{N}^{M}$ and
$\underbrace{\mathbb{N}\times\mathbb{N}\times\cdots\times\mathbb{N}%
}_{M-1\text{ factors}}=\mathbb{N}^{M-1}$, this can be rewritten as follows:
The map%
\begin{align*}
\mathbb{N}^{M}  &  \rightarrow\mathbb{N}^{M-1}\times\mathbb{N},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection. Hence, we can substitute $\left(  \left(  s_{1},s_{2}%
,\ldots,s_{M-1}\right)  ,s_{M}\right)  $ for $\left(  \left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  ,r\right)  $ in the sum on the right hand side
of (\ref{sol.multinom2.short.4}). Thus, we obtain%
\begin{align*}
&  \sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}%
\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}\\
&  =\underbrace{\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)
\in\mathbb{N}^{M};\\s_{1}+s_{2}+\cdots+s_{M-1}=n-s_{M}}}}_{\substack{=\sum
_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}^{M}%
;\\s_{1}+s_{2}+\cdots+s_{M}=n}}\\\text{(because for any }\left(  s_{1}%
,s_{2},\ldots,s_{M}\right)  \in\mathbb{N}^{M}\text{,}\\\text{the condition
}\left(  s_{1}+s_{2}+\cdots+s_{M-1}=n-s_{M}\right)  \\\text{is equivalent
to}\\\text{the condition }\left(  s_{1}+s_{2}+\cdots+s_{M}=n\right)  \text{)}%
}}\dbinom{n}{s_{M}}\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)
\underbrace{\left(  \prod_{i=1}^{M-1}a_{i}^{s_{i}}\right)  a_{M}^{s_{M}}%
}_{\substack{=\prod_{i=1}^{M}a_{i}^{s_{i}}}}\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}%
^{M};\\s_{1}+s_{2}+\cdots+s_{M}=n}}\underbrace{\dbinom{n}{s_{M}}%
\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  }_{\substack{=\mathbf{m}%
\left(  s_{1},s_{2},\ldots,s_{M}\right)  \\\text{(by Lemma
\ref{lem.sol.multinom2.prod})}}}\prod_{i=1}^{M}a_{i}^{s_{i}}\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}%
^{M};\\s_{1}+s_{2}+\cdots+s_{M}=n}}\mathbf{m}\left(  s_{1},s_{2},\ldots
,s_{M}\right)  \prod_{i=1}^{M}a_{i}^{s_{i}}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{M}\right)  \in\mathbb{N}^{M};\\k_{1}+k_{2}+\cdots
+k_{M}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M}\right)  \prod_{i=1}%
^{M}a_{i}^{k_{i}}%
\end{align*}
(here, we have renamed the summation index $\left(  s_{1},s_{2},\ldots
,s_{M}\right)  $ as $\left(  k_{1},k_{2},\ldots,k_{M}\right)  $). Hence,
(\ref{sol.multinom2.short.4}) becomes%
\begin{align*}
&  \left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}\\
&  =\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}%
\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}\\
&  =\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M}\right)  \in\mathbb{N}%
^{M};\\k_{1}+k_{2}+\cdots+k_{M}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{M}\right)  \prod_{i=1}^{M}a_{i}^{k_{i}}.
\end{align*}
This proves Fact 2.]
\end{vershort}

\begin{verlong}
[\textit{Proof of Fact 2:} We have
\[
a_{1}+a_{2}+\cdots+a_{M}=\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)
+a_{M}=a_{M}+\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  .
\]
Taking both sides of this equality to the $n$-th power, we obtain%
\begin{align*}
\left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}  &  =\left(  a_{M}+\left(
a_{1}+a_{2}+\cdots+a_{M-1}\right)  \right)  ^{n}\\
&  =\sum_{k=0}^{n}\dbinom{n}{k}a_{M}^{k}\left(  a_{1}+a_{2}+\cdots
+a_{M-1}\right)  ^{n-k}%
\end{align*}
(by (\ref{eq.rings.(a+b)**n}) (applied to $a=a_{M}$ and $b=a_{1}+a_{2}%
+\cdots+a_{M-1}$)). Thus,%
\begin{align}
\left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}  &  =\sum_{k=0}^{n}\dbinom{n}%
{k}a_{M}^{k}\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  ^{n-k}\nonumber\\
&  =\sum_{k=0}^{n}\dbinom{n}{k}\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)
^{n-k}a_{M}^{k}\nonumber\\
&  =\sum_{r=0}^{n}\dbinom{n}{r}\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)
^{n-r}a_{M}^{r} \label{sol.multinom2.1}%
\end{align}
(here, we have renamed the summation index $k$ as $r$).

But each $r\in\left\{  0,1,\ldots,n\right\}  $ satisfies%
\begin{align}
&  \left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  ^{n-r}\nonumber\\
&  =\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in
\mathbb{N}^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}\mathbf{m}\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \prod_{i=1}^{M-1}a_{i}^{k_{i}}
\label{sol.multinom2.2}%
\end{align}
\footnote{\textit{Proof of (\ref{sol.multinom2.2}):} Let $r\in\left\{
0,1,\ldots,n\right\}  $. Then, $r\leq n$, so that $n-r\in\mathbb{N}$. Hence,
Fact 1 (applied to $n-r$ instead of $n$) yields%
\[
\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  ^{n-r}=\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1};\\k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r}}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
\prod_{i=1}^{M-1}a_{i}^{k_{i}}.
\]
Qed.}. Hence, (\ref{sol.multinom2.1}) becomes%
\begin{align}
&  \left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}\nonumber\\
&  =\underbrace{\sum_{r=0}^{n}}_{=\sum_{\substack{r\in\mathbb{N};\\r\leq n}%
}}\dbinom{n}{r}\underbrace{\left(  a_{1}+a_{2}+\cdots+a_{M-1}\right)  ^{n-r}%
}_{\substack{=\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
\in\mathbb{N}^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}\mathbf{m}\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\\\text{(by
(\ref{sol.multinom2.2}))}}}a_{M}^{r}\nonumber\\
&  =\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\dbinom{n}{r}\left(
\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}%
^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}\mathbf{m}\left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}%
^{r}\nonumber\\
&  =\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\sum_{\substack{\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1};\\k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{M-1}\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}.
\label{sol.multinom2.3}%
\end{align}


On the other hand, each $\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N}$ satisfying $k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r$ must automatically satisfy $r\leq n$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  \left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N}$ be
such that $k_{1}+k_{2}+\cdots+k_{M-1}=n-r$. Then, $\left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1}$ (since $\left(  \left(
k_{1},k_{2},\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}%
\times\mathbb{N}$), so that $k_{1},k_{2},\ldots,k_{M-1}$ are elements of
$\mathbb{N}$. Thus, $k_{1}+k_{2}+\cdots+k_{M-1}\in\mathbb{N}$, so that
$k_{1}+k_{2}+\cdots+k_{M-1}\geq0$. Now, $n-r=k_{1}+k_{2}+\cdots+k_{M-1}\geq0$,
so that $r\leq n$. Qed.}. Hence, we have the following equality of summation
signs:%
\[
\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)  ,r\right)
\in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r;\\r\leq
n}}=\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}.
\]
Now, we have the following equality of summation signs:%
\begin{align*}
\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\sum_{\substack{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}^{M-1};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}  &  =\sum_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
\in\mathbb{N}^{M-1}}\sum_{\substack{r\in\mathbb{N};\\k_{1}+k_{2}%
+\cdots+k_{M-1}=n-r;\\r\leq n}}\\
&  =\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r;\\r\leq n}}=\sum_{\substack{\left(  \left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N}%
;\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}.
\end{align*}
Hence, (\ref{sol.multinom2.3}) becomes%
\begin{align}
&  \left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}\nonumber\\
&  =\underbrace{\sum_{\substack{r\in\mathbb{N};\\r\leq n}}\sum
_{\substack{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in\mathbb{N}%
^{M-1};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}}_{=\sum_{\substack{\left(  \left(
k_{1},k_{2},\ldots,k_{M-1}\right)  ,r\right)  \in\mathbb{N}^{M-1}%
\times\mathbb{N};\\k_{1}+k_{2}+\cdots+k_{M-1}=n-r}}}\dbinom{n}{r}%
\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \left(  \prod_{i=1}%
^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}\nonumber\\
&  =\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}%
\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}.
\label{sol.multinom2.4}%
\end{align}


But Lemma \ref{lem.prodrule.prod-assM} (applied to $Z_{i}=\mathbb{N}$) shows
that the map%
\begin{align*}
\underbrace{\mathbb{N}\times\mathbb{N}\times\cdots\times\mathbb{N}}_{M\text{
factors}}  &  \rightarrow\left(  \underbrace{\mathbb{N}\times\mathbb{N}%
\times\cdots\times\mathbb{N}}_{M-1\text{ factors}}\right)  \times\mathbb{N},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection. Since $\underbrace{\mathbb{N}\times\mathbb{N}\times
\cdots\times\mathbb{N}}_{M\text{ factors}}=\mathbb{N}^{M}$ and
$\underbrace{\mathbb{N}\times\mathbb{N}\times\cdots\times\mathbb{N}%
}_{M-1\text{ factors}}=\mathbb{N}^{M-1}$, this can be rewritten as follows:
The map%
\begin{align*}
\mathbb{N}^{M}  &  \rightarrow\mathbb{N}^{M-1}\times\mathbb{N},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection. Hence, we can substitute $\left(  \left(  s_{1},s_{2}%
,\ldots,s_{M-1}\right)  ,s_{M}\right)  $ for $\left(  \left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  ,r\right)  $ in the sum on the right hand side
of (\ref{sol.multinom2.4}). Thus, we obtain%
\begin{align*}
&  \sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}%
\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}\\
&  =\underbrace{\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)
\in\mathbb{N}^{M};\\s_{1}+s_{2}+\cdots+s_{M-1}=n-s_{M}}}}_{\substack{=\sum
_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}%
^{M};\\\left(  s_{1}+s_{2}+\cdots+s_{M-1}\right)  +s_{M}=n}}\\\text{(because
for any }\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}^{M}%
\text{,}\\\text{the condition }\left(  s_{1}+s_{2}+\cdots+s_{M-1}%
=n-s_{M}\right)  \\\text{is equivalent to}\\\text{the condition }\left(
\left(  s_{1}+s_{2}+\cdots+s_{M-1}\right)  +s_{M}=n\right)  \text{)}}%
}\dbinom{n}{s_{M}}\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)
\underbrace{\left(  \prod_{i=1}^{M-1}a_{i}^{s_{i}}\right)  a_{M}^{s_{M}}%
}_{\substack{=\prod_{i=1}^{M}a_{i}^{s_{i}}\\\text{(since }\prod_{i=1}^{M}%
a_{i}^{s_{i}}=\left(  \prod_{i=1}^{M-1}a_{i}^{s_{i}}\right)  a_{M}^{s_{M}%
}\\\text{(here, we have split off the}\\\text{factor for }i=M\text{ from the
product))}}}\\
&  =\underbrace{\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)
\in\mathbb{N}^{M};\\\left(  s_{1}+s_{2}+\cdots+s_{M-1}\right)  +s_{M}=n}%
}}_{\substack{=\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)
\in\mathbb{N}^{M};\\s_{1}+s_{2}+\cdots+s_{M}=n}}\\\text{(because for any
}\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}^{M}\text{,}\\\text{we
have }\left(  s_{1}+s_{2}+\cdots+s_{M-1}\right)  +s_{M}=s_{1}+s_{2}%
+\cdots+s_{M}\text{)}}}\dbinom{n}{s_{M}}\mathbf{m}\left(  s_{1},s_{2}%
,\ldots,s_{M-1}\right)  \prod_{i=1}^{M}a_{i}^{s_{i}}\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}%
^{M};\\s_{1}+s_{2}+\cdots+s_{M}=n}}\underbrace{\dbinom{n}{s_{M}}%
\mathbf{m}\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  }_{\substack{=\mathbf{m}%
\left(  s_{1},s_{2},\ldots,s_{M}\right)  \\\text{(by Lemma
\ref{lem.sol.multinom2.prod})}}}\prod_{i=1}^{M}a_{i}^{s_{i}}\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in\mathbb{N}%
^{M};\\s_{1}+s_{2}+\cdots+s_{M}=n}}\mathbf{m}\left(  s_{1},s_{2},\ldots
,s_{M}\right)  \prod_{i=1}^{M}a_{i}^{s_{i}}\\
&  =\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M}\right)  \in\mathbb{N}%
^{M};\\k_{1}+k_{2}+\cdots+k_{M}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{M}\right)  \prod_{i=1}^{M}a_{i}^{k_{i}}%
\end{align*}
(here, we have renamed the summation index $\left(  s_{1},s_{2},\ldots
,s_{M}\right)  $ as $\left(  k_{1},k_{2},\ldots,k_{M}\right)  $). Hence,
(\ref{sol.multinom2.4}) becomes%
\begin{align*}
&  \left(  a_{1}+a_{2}+\cdots+a_{M}\right)  ^{n}\\
&  =\sum_{\substack{\left(  \left(  k_{1},k_{2},\ldots,k_{M-1}\right)
,r\right)  \in\mathbb{N}^{M-1}\times\mathbb{N};\\k_{1}+k_{2}+\cdots
+k_{M-1}=n-r}}\dbinom{n}{r}\mathbf{m}\left(  k_{1},k_{2},\ldots,k_{M-1}%
\right)  \left(  \prod_{i=1}^{M-1}a_{i}^{k_{i}}\right)  a_{M}^{r}\\
&  =\sum_{\substack{\left(  k_{1},k_{2},\ldots,k_{M}\right)  \in\mathbb{N}%
^{M};\\k_{1}+k_{2}+\cdots+k_{M}=n}}\mathbf{m}\left(  k_{1},k_{2},\ldots
,k_{M}\right)  \prod_{i=1}^{M}a_{i}^{k_{i}}.
\end{align*}
This proves Fact 2.]
\end{verlong}

But Fact 2 is precisely the statement of Exercise \ref{exe.multinom2} for
$m=M$. Hence, Exercise \ref{exe.multinom2} holds for $m=M$ (since Fact 2 is
proven). This completes the induction step. Thus, Exercise \ref{exe.multinom2}
is proven by induction.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.3}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.3}.]We first notice a purely combinatorial
fact: For every $\sigma\in S_{n}$ satisfying $\sigma\neq\operatorname*{id}$,%
\begin{equation}
\text{there exists an }i\in\left\{  1,2,\ldots,n\right\}  \text{ such that
}\sigma\left(  i\right)  >i \label{sol.ps4.3.ineq}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.3.ineq}):} Let $\sigma\in S_{n}$ be
such that $\sigma\neq\operatorname*{id}$. We need to prove
(\ref{sol.ps4.3.ineq}).
\par
Assume the contrary. Thus, there exists no $i\in\left\{  1,2,\ldots,n\right\}
$ such that $\sigma\left(  i\right)  > i$. In other words, every $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies $\sigma\left(  i\right)  \leq i$.
\par
We shall now show that every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\sigma\left(  p\right)  =p. \label{sol.ps4.3.ineq.pf.goal}%
\end{equation}
\par
\textit{Proof of (\ref{sol.ps4.3.ineq.pf.goal}):} We will prove
(\ref{sol.ps4.3.ineq.pf.goal}) by strong induction over $p$. Thus, fix some
$P\in\left\{  1,2,\ldots,n\right\}  $. We assume that
(\ref{sol.ps4.3.ineq.pf.goal}) is proven for every $p<P$. We need to show that
(\ref{sol.ps4.3.ineq.pf.goal}) holds for $p=P$.
\par
We have assumed that (\ref{sol.ps4.3.ineq.pf.goal}) is proven for every $p<P$.
In other words,%
\begin{equation}
\sigma\left(  p\right)  =p\ \ \ \ \ \ \ \ \ \ \text{for every }p\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }p<P.
\label{sol.ps4.3.ineq.pf.goal.pf.hyp}%
\end{equation}
\par
Now, we assume (for the sake of contradiction) that $\sigma\left(  P\right)
\neq P$. Recall that every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
$\sigma\left(  i\right)  \leq i$. Applying this to $i=P$, we obtain
$\sigma\left(  P\right)  \leq P$. Combined with $\sigma\left(  P\right)  \neq
P$, this yields $\sigma\left(  P\right)  <P$. Hence,
(\ref{sol.ps4.3.ineq.pf.goal.pf.hyp}) (applied to $p=\sigma\left(  P\right)
$) yields $\sigma\left(  \sigma\left(  P\right)  \right)  =\sigma\left(
P\right)  $.
\par
But the map $\sigma$ is a permutation (since $\sigma\in S_{n}$), thus
injective. Hence, from $\sigma\left(  \sigma\left(  P\right)  \right)
=\sigma\left(  P\right)  $, we obtain $\sigma\left(  P\right)  =P$. This
contradicts $\sigma\left(  P\right)  \neq P$. Hence, we have obtained a
contradiction; thus, our assumption (that $\sigma\left(  P\right)  \neq P$)
must have been wrong. We thus have $\sigma\left(  P\right)  =P$. In other
words, (\ref{sol.ps4.3.ineq.pf.goal}) holds for $p=P$. This completes the
inductive proof of (\ref{sol.ps4.3.ineq.pf.goal}).
\par
Now, (\ref{sol.ps4.3.ineq.pf.goal}) shows that every $p\in\left\{
1,2,\ldots,n\right\}  $ satisfies $\sigma\left(  p\right)
=p=\operatorname*{id}\left(  p\right)  $. In other words, $\sigma
=\operatorname*{id}$. This contradicts $\sigma\neq\operatorname*{id}$. This
contradiction proves that our assumption was wrong. Hence,
(\ref{sol.ps4.3.ineq}) is proven.}. Thus, for every $\sigma\in S_{n}$
satisfying $\sigma\neq\operatorname*{id}$, we have%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.ps4.3.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.3.0}):} Let $\sigma\in S_{n}$ be such
that $\sigma\neq\operatorname*{id}$. According to (\ref{sol.ps4.3.ineq}), we
know that there exists an $i\in\left\{  1,2,\ldots,n\right\}  $ such that
$\sigma\left(  i\right)  >i$. Let $k$ be such an $i$. Thus, $k$ is an element
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\sigma\left(  k\right)  >k$.
Hence, $k<\sigma\left(  k\right)  $.
\par
Recall that $a_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$. Applying this to $i=k$ and
$j=\sigma\left(  k\right)  $, we obtain $a_{k,\sigma\left(  k\right)  }=0$
(since $k<\sigma\left(  k\right)  $). Hence, one factor of the product
$\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ is $0$ (namely, the factor
$a_{k,\sigma\left(  k\right)  }$). Therefore, the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$ is $0$ (because if one factor of a product
is $0$, then the whole product is $0$).}.

Now, (\ref{eq.det.eq.2}) yields%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}\prod_{i=1}%
^{n}\underbrace{a_{i,\operatorname*{id}\left(  i\right)  }}_{=a_{i,i}}%
+\sum_{\substack{\sigma\in S_{n};\\\sigma\neq\operatorname*{id}}}\left(
-1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=0\\\text{(by (\ref{sol.ps4.3.0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have moved the addend for
}\sigma=\operatorname*{id}\text{ out of the sum}\right) \\
&  =\prod_{i=1}^{n}a_{i,i}+\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\neq\operatorname*{id}}}\left(  -1\right)  ^{\sigma}0}%
_{=0}=\prod_{i=1}^{n}a_{i,i}=a_{1,1}a_{2,2}\cdots a_{n,n}.
\end{align*}
This solves Exercise \ref{exe.ps4.3}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.4}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.4}.]The map $S_{n}\rightarrow
S_{n},\ \sigma\mapsto\sigma^{-1}$ (that is, the map from $S_{n}$ to $S_{n}$
which sends every permutation to its inverse) is a bijection\footnote{In fact,
this map is its own inverse: Indeed, every $\sigma\in S_{n}$ satisfies
$\left(  \sigma^{-1}\right)  ^{-1}=\sigma$.}.

Write $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Thus, $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A^{T}$). Hence, (\ref{eq.det.eq.2}) (applied to $A^{T}$
and $a_{j,i}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det\left(  A^{T}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}
\label{sol.ps4.4.short.3a}%
\end{equation}
(here, we have substituted $\sigma^{-1}$ for $\sigma$ in the sum, since the
map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma^{-1}$ is a bijection). But
every $\sigma\in S_{n}$ satisfies%
\[
\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}=\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }%
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $\sigma$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $, thus a bijection from
$\left\{  1,2,\ldots,n\right\}  $ to $\left\{  1,2,\ldots,n\right\}  $. Hence,
we can substitute $\sigma\left(  i\right)  $ for $i$ in the product
$\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}$. We thus obtain%
\[
\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}=\prod_{i=1}^{n}%
\underbrace{a_{\sigma^{-1}\left(  \sigma\left(  i\right)  \right)
,\sigma\left(  i\right)  }}_{=a_{i,\sigma\left(  i\right)  }}=\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  },
\]
qed.}. Hence, (\ref{sol.ps4.4.short.3a}) becomes%
\[
\det\left(  A^{T}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}a_{\sigma^{-1}\left(  i\right)  ,i}}_{=\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\det
A\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.det.eq.2})}\right)  .
\]
This solves Exercise \ref{exe.ps4.4}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.ps4.4}.]Define a map $\Phi:S_{n}\rightarrow
S_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma^{-1}\ \ \ \ \ \ \ \ \ \ \text{for every
}\sigma\in S_{n}.
\]
Then, $\Phi\circ\Phi=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Every $\sigma\in S_{n}$ satisfies%
\begin{align*}
\left(  \Phi\circ\Phi\right)  \left(  \sigma\right)   &  =\Phi\left(
\underbrace{\Phi\left(  \sigma\right)  }_{=\sigma^{-1}}\right)  =\Phi\left(
\sigma^{-1}\right)  =\left(  \sigma^{-1}\right)  ^{-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right) \\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
Thus, $\Phi\circ\Phi=\operatorname*{id}$, qed.}. Hence, the maps $\Phi$ and
$\Phi$ are mutually inverse. Therefore, the map $\Phi$ is a bijection.

Write $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Thus, $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A^{T}$). Hence, (\ref{eq.det.eq.2}) (applied to $A^{T}$
and $a_{j,i}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  A^{T}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }}a_{\sigma\left(  i\right)  ,i}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\sigma
\left(  i\right)  ,i}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i\in\left\{
1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(  \sigma\right)  \right)  \left(
i\right)  ,i} \label{sol.ps4.4.3}%
\end{align}
(here, we have substituted $\Phi\left(  \sigma\right)  $ for $\sigma$ in the
sum, since the map $\Phi$ is a bijection). But every $\sigma\in S_{n}$
satisfies%
\[
\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  ,i}=\prod_{i\in\left\{
1,2,\ldots,n\right\}  }a_{i,\sigma\left(  i\right)  }%
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $\sigma$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $, thus a bijection from
$\left\{  1,2,\ldots,n\right\}  $ to $\left\{  1,2,\ldots,n\right\}  $. Hence,
we can substitute $\sigma\left(  i\right)  $ for $i$ in the product
$\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  ,i}$. We thus obtain%
\begin{align*}
\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(
\sigma\right)  \right)  \left(  i\right)  ,i}  &  =\prod_{i\in\left\{
1,2,\ldots,n\right\}  }a_{\left(  \Phi\left(  \sigma\right)  \right)  \left(
\sigma\left(  i\right)  \right)  ,\sigma\left(  i\right)  }=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }\underbrace{a_{\sigma^{-1}\left(
\sigma\left(  i\right)  \right)  ,\sigma\left(  i\right)  }}%
_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(since }\sigma^{-1}\left(
\sigma\left(  i\right)  \right)  =i\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\Phi\left(  \sigma\right)  =\sigma^{-1}\right) \\
&  =\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{i,\sigma\left(  i\right)  },
\end{align*}
qed.}. Hence, (\ref{sol.ps4.4.3}) becomes%
\begin{align*}
\det\left(  A^{T}\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }a_{\left(
\Phi\left(  \sigma\right)  \right)  \left(  i\right)  ,i}}_{=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }a_{i,\sigma\left(  i\right)  }}%
=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}}a_{i,\sigma\left(
i\right)  }\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }=\det A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{eq.det.eq.2})}\right)  .
\end{align*}
This solves Exercise \ref{exe.ps4.4}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.ps4.5}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.5}.]Of course, both parts of Exercise
\ref{exe.ps4.5} can be solved directly using (\ref{eq.det.eq.1}). This
solution, however, is tedious (particularly for part \textbf{(b)} of this
exercise). Let us show a smarter way.

\textbf{(a)} Let $A$ be the matrix $\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
l & 0 & 0 & e\\
k & 0 & 0 & f\\
j & i & h & g
\end{array}
\right)  $. We want to find $\det A$.

We write the matrix $A$ in the form $A=\left(  a_{u,v}\right)  _{1\leq
u\leq4,\ 1\leq v\leq4}$\ \ \ \ \footnote{We cannot write $A=\left(
a_{i,j}\right)  _{1\leq i\leq4,\ 1\leq j\leq4}$ because the letters $i$ and
$j$ are already taken for something different.}. Thus,%
\begin{align*}
a_{1,1}  &  =a,\ \ \ \ \ \ \ \ \ \ a_{1,2}=b,\ \ \ \ \ \ \ \ \ \ a_{1,3}%
=c,\ \ \ \ \ \ \ \ \ \ a_{1,4}=d,\\
a_{2,1}  &  =l,\ \ \ \ \ \ \ \ \ \ a_{2,2}=0,\ \ \ \ \ \ \ \ \ \ a_{2,3}%
=0,\ \ \ \ \ \ \ \ \ \ a_{2,4}=e,\\
a_{3,1}  &  =k,\ \ \ \ \ \ \ \ \ \ a_{3,2}=0,\ \ \ \ \ \ \ \ \ \ a_{3,3}%
=0,\ \ \ \ \ \ \ \ \ \ a_{3,4}=f,\\
a_{4,1}  &  =j,\ \ \ \ \ \ \ \ \ \ a_{4,2}=i,\ \ \ \ \ \ \ \ \ \ a_{4,3}%
=h,\ \ \ \ \ \ \ \ \ \ a_{4,4}=g.
\end{align*}
Now, applying (\ref{eq.det.eq.1}) to $n=4$, we obtain%
\begin{equation}
\det A=\sum_{\sigma\in S_{4}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(  3\right)
}a_{4,\sigma\left(  4\right)  }. \label{sol.ps4.5.a.1}%
\end{equation}


The sum on the right hand side of (\ref{sol.ps4.5.a.1}) has $\left\vert
S_{4}\right\vert =4!=24$ addends. However, some of them are $0$. Namely, every
addend corresponding to a permutation $\sigma\in S_{4}$ satisfying
$\sigma\left(  2\right)  \notin\left\{  1,4\right\}  $ must be $0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{4}$ be such that
$\sigma\left(  2\right)  \notin\left\{  1,4\right\}  $. We must then show that
the addend on the right hand side of (\ref{sol.ps4.5.a.1}) corresponding to
this $\sigma$ must be $0$. In other words, we have to show that $\left(
-1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)  }a_{2,\sigma\left(  2\right)
}a_{3,\sigma\left(  3\right)  }a_{4,\sigma\left(  4\right)  }=0$.
\par
We have $\sigma\left(  2\right)  \notin\left\{  1,4\right\}  $, and thus
$\sigma\left(  2\right)  \in\left\{  2,3\right\}  $. Hence, $a_{2,\sigma
\left(  2\right)  }=0$ (because $a_{2,2}=0$ and $a_{2,3}=0$), and thus
$\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(  1\right)  }%
\underbrace{a_{2,\sigma\left(  2\right)  }}_{=0}a_{3,\sigma\left(  3\right)
}a_{4,\sigma\left(  4\right)  }=0$, qed.}. Hence, all such addends can be
removed from the sum (without changing the value of this sum). Similarly, all
addends corresponding to permutations $\sigma\in S_{4}$ satisfying
$\sigma\left(  3\right)  \notin\left\{  1,4\right\}  $ must be $0$, and can
therefore also be removed from the sum. The addends that survive these two
removals are the ones that correspond to permutations $\sigma\in S_{4}$
satisfying $\sigma\left(  2\right)  \in\left\{  1,4\right\}  $ and
$\sigma\left(  3\right)  \in\left\{  1,4\right\}  $. It is easy to see that
there are exactly four such permutations: In one-line notation, these
permutations are $\left(  2,1,4,3\right)  $, $\left(  2,4,1,3\right)  $,
$\left(  3,1,4,2\right)  $ and $\left(  3,4,1,2\right)  $. The addends
corresponding to these permutations are $a_{1,2}a_{2,1}a_{3,4}a_{4,3}$,
$-a_{1,2}a_{2,4}a_{3,1}a_{4,3}$, $-a_{1,3}a_{2,1}a_{3,4}a_{4,2}$ and
$a_{1,3}a_{2,4}a_{3,1}a_{4,2}$. Hence, (\ref{sol.ps4.5.a.1}) simplifies to%
\begin{align*}
&  \det A\\
&  =\underbrace{a_{1,2}}_{=b}\underbrace{a_{2,1}}_{=l}\underbrace{a_{3,4}%
}_{=f}\underbrace{a_{4,3}}_{=h}-\underbrace{a_{1,2}}_{=b}\underbrace{a_{2,4}%
}_{=e}\underbrace{a_{3,1}}_{=k}\underbrace{a_{4,3}}_{=h}-\underbrace{a_{1,3}%
}_{=c}\underbrace{a_{2,1}}_{=l}\underbrace{a_{3,4}}_{=f}\underbrace{a_{4,2}%
}_{=i}+\underbrace{a_{1,3}}_{=c}\underbrace{a_{2,4}}_{=e}\underbrace{a_{3,1}%
}_{=k}\underbrace{a_{4,2}}_{=i}\\
&  =blfh-bekh-clfi+ceki.
\end{align*}
This is a simple enough formula to consider an answer to Exercise
\ref{exe.ps4.5} \textbf{(a)}, but we can simplify it even further. Namely,%
\[
\det A=blfh-bekh-clfi+ceki=\left(  bh-ci\right)  \left(  lf-ek\right)  .
\]
Exercise \ref{exe.ps4.5} \textbf{(a)} is solved.

\textbf{(b)} Let $A$ be the matrix $\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
f & 0 & 0 & 0 & g\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
\ell & m & n & o & p
\end{array}
\right)  $. We want to find $\det A$.

We write the matrix $A$ in the form $A=\left(  a_{u,v}\right)  _{1\leq
u\leq5,\ 1\leq v\leq5}$. Thus, $a_{1,1}=a$, $a_{1,2}=b$, etc.. For us, the
most important property of $A$ is that the $3\times3$-submatrix in the middle
of $A$ is filled with zeroes. In other words,%
\begin{equation}
a_{u,v}=0\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  2,3,4\right\}
\text{ and }v\in\left\{  2,3,4\right\}  . \label{sol.ps4.5.b.2}%
\end{equation}


Now, applying (\ref{eq.det.eq.1}) to $n=5$, we obtain%
\begin{equation}
\det A=\sum_{\sigma\in S_{5}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(  3\right)
}a_{4,\sigma\left(  4\right)  }a_{5,\sigma\left(  5\right)  }.
\label{sol.ps4.5.b.3}%
\end{equation}
But every $\sigma\in S_{5}$ satisfies $a_{2,\sigma\left(  2\right)
}a_{3,\sigma\left(  3\right)  }a_{4,\sigma\left(  4\right)  }=0$%
\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{5}$. Then, $\sigma$ is a
permutation of $\left\{  1,2,3,4,5\right\}  $, and thus an injective map.
Therefore, the numbers $\sigma\left(  2\right)  ,\sigma\left(  3\right)
,\sigma\left(  4\right)  $ are pairwise distinct.
\par
We now claim that there exists an $u\in\left\{  2,3,4\right\}  $ such that
$\sigma\left(  u\right)  \in\left\{  2,3,4\right\}  $. In order to prove this,
we assume the contrary. Thus, every $u\in\left\{  2,3,4\right\}  $ satisfies
$\sigma\left(  u\right)  \notin\left\{  2,3,4\right\}  $. Hence, every
$u\in\left\{  2,3,4\right\}  $ satisfies $\sigma\left(  u\right)  \in\left\{
1,5\right\}  $ (since $\sigma\left(  u\right)  \in\left\{  1,2,3,4,5\right\}
$ but $\sigma\left(  u\right)  \notin\left\{  2,3,4\right\}  $). In other
words, the numbers $\sigma\left(  2\right)  ,\sigma\left(  3\right)
,\sigma\left(  4\right)  $ belong to $\left\{  1,5\right\}  $. Hence,
$\sigma\left(  2\right)  ,\sigma\left(  3\right)  ,\sigma\left(  4\right)  $
are three distinct numbers belonging to the set $\left\{  1,5\right\}  $. But
this is absurd, since the set $\left\{  1,5\right\}  $ does not contain three
distinct numbers. Hence, we have obtained a contradiction. This shows that our
assumption was wrong.
\par
We thus have shown that there exists an $u\in\left\{  2,3,4\right\}  $ such
that $\sigma\left(  u\right)  \in\left\{  2,3,4\right\}  $. Consider such a
$u$. Applying (\ref{sol.ps4.5.b.2}) to $v=\sigma\left(  u\right)  $, we now
obtain $a_{u,\sigma\left(  u\right)  }=0$. But $u\in\left\{  2,3,4\right\}  $,
so that $a_{u,\sigma\left(  u\right)  }$ is a factor in the product
$a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(  3\right)  }a_{4,\sigma
\left(  4\right)  }$. Hence, the product $a_{2,\sigma\left(  2\right)
}a_{3,\sigma\left(  3\right)  }a_{4,\sigma\left(  4\right)  }$ is $0$ (since
its factor $a_{u,\sigma\left(  u\right)  }$ is $0$), qed.}. Hence,
(\ref{sol.ps4.5.b.3}) becomes%
\[
\det A=\sum_{\sigma\in S_{5}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }\underbrace{a_{2,\sigma\left(  2\right)  }a_{3,\sigma\left(
3\right)  }a_{4,\sigma\left(  4\right)  }}_{=0}a_{5,\sigma\left(  5\right)
}=\sum_{\sigma\in S_{5}}\left(  -1\right)  ^{\sigma}a_{1,\sigma\left(
1\right)  }0a_{5,\sigma\left(  5\right)  }=0.
\]
Exercise \ref{exe.ps4.5} \textbf{(b)} is thus solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.6}}

Our solution to Exercise \ref{exe.ps4.6} relies on Lemma \ref{lem.det.sigma}.
Thus, the reader is advised to read the proof of said lemma before the
solution. Furthermore, we shall use the following simple fact:

\begin{lemma}
\label{lem.colsA=rowsATT}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let $A$ be
an $n\times m$-matrix. Then, the columns of $A$ are the transposes of the
respective rows of $A^{T}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.colsA=rowsATT}.]Fix $k\in\left\{  1,2,\ldots
,m\right\}  $.

Write the $n\times m$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$. Then, $A^{T}=\left(  a_{j,i}\right)
_{1\leq i\leq m,\ 1\leq j\leq n}$ (by the definition of $A^{T}$). Hence,%
\[
\left(  \text{the }k\text{-th row of }A^{T}\right)  =\left(  a_{j,k}\right)
_{1\leq i\leq1,\ 1\leq j\leq n}.
\]
Thus,%
\begin{align}
&  \left(  \text{the transpose of }\underbrace{\text{the }k\text{-th row of
}A^{T}}_{=\left(  a_{j,k}\right)  _{1\leq i\leq1,\ 1\leq j\leq n}}\right)
\nonumber\\
&  =\left(  \text{the transpose of }\left(  a_{j,k}\right)  _{1\leq
i\leq1,\ 1\leq j\leq n}\right)  =\left(  \left(  a_{j,k}\right)  _{1\leq
i\leq1,\ 1\leq j\leq n}\right)  ^{T}\nonumber\\
&  =\left(  a_{i,k}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}
\label{pf.lem.colsA=rowsATT.1}%
\end{align}
(by the definition of $\left(  \left(  a_{j,k}\right)  _{1\leq i\leq1,\ 1\leq
j\leq n}\right)  ^{T}$). On the other hand, we have $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$, and thus%
\begin{align}
\left(  \text{the }k\text{-th column of }A\right)   &  =\left(  a_{i,k}%
\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\nonumber\\
&  =\left(  \text{the transpose of the }k\text{-th row of }A^{T}\right)
\label{pf.lem.colsA=rowsATT.claim}%
\end{align}
(by (\ref{pf.lem.colsA=rowsATT.1})).

Now, forget that we fixed $k$. We thus have proven that
(\ref{pf.lem.colsA=rowsATT.claim}) holds for each $k\in\left\{  1,2,\ldots
,m\right\}  $. In other words, for each $k\in\left\{  1,2,\ldots,m\right\}  $,
the $k$-th column of $A$ is the transpose of the $k$-th row of $A^{T}$. In
other words, the columns of $A$ are the transposes of the respective rows of
$A^{T}$. This proves Lemma \ref{lem.colsA=rowsATT}.
\end{proof}

\begin{remark}
\label{rmk.colsA=rowsATT.rmk}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Lemma \ref{lem.colsA=rowsATT} (applied to $m=n$) shows that the
columns of $A$ are the transposes of the respective rows of $A^{T}$. Thus, we
have a correspondence between the columns of $A$ and the rows of $A^{T}$. We
can use this correspondence to \textquotedblleft transport\textquotedblright%
\ information about the columns of $A$ to the rows of $A^{T}$ and vice versa;
for example:

\begin{itemize}
\item If a column of $A$ consists of zeroes, then the corresponding row of
$A^{T}$ consists of zeroes. The converse also holds.

\item If two columns of $A$ are equal, then the corresponding two rows of
$A^{T}$ are equal. The converse also holds.

\item Let $k\in\left\{  1,2,\ldots,n\right\}  $ and $\lambda\in\mathbb{K}$. If
$B$ is the $n\times n$-matrix obtained from $A$ by multiplying the $k$-th
column by $\lambda$, then $B^{T}$ is the $n\times n$-matrix obtained from
$A^{T}$ by multiplying the $k$-th row by $\lambda$. Again, the converse also holds.

\item Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $A^{\prime}$ be an
$n\times n$-matrix whose columns equal the corresponding columns of $A$ except
(perhaps) the $k$-th column. Then, $\left(  A^{\prime}\right)  ^{T}$ is an
$n\times n$-matrix whose rows equal the corresponding rows of $A^{T}$ except
(perhaps) the $k$-th row. Again, the converse also holds.

\item Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let $A^{\prime}$ be a further
$n\times n$-matrix. Let $B$ be the $n\times n$-matrix obtained from $A$ by
adding the $k$-th column of $A^{\prime}$ to the $k$-th column of $A$. Then,
$B^{T}$ is the $n\times n$-matrix obtained from $A^{T}$ by adding the $k$-th
row of $\left(  A^{\prime}\right)  ^{T}$ to the $k$-th row of $A^{\prime}$.
Again, the converse holds as well.
\end{itemize}
\end{remark}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.6}.]Let us write the matrix $A$ in the form
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A\right)  =a_{i,j}. \label{sol.ps4.6.aij}%
\end{equation}


We let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} Let $B$ be an $n\times n$-matrix obtained from $A$ by switching
two rows. Thus, there exist two distinct elements $u$ and $v$ of $\left\{
1,2,\ldots,n\right\}  $ such that $B$ is the $n\times n$-matrix obtained from
$A$ by switching the $u$-th row with the $v$-th row. Consider these $u$ and
$v$.

We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$.

\begin{vershort}
Consider the transposition $t_{u,v}$ in $S_{n}$ (defined according to
Definition \ref{def.transpos}). Clearly, $t_{u,v}$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, thus a map $\left[
n\right]  \rightarrow\left[  n\right]  $. Also, $\left(  -1\right)  ^{t_{u,v}%
}=-1$ (by Exercise \ref{exe.ps4.1ab} \textbf{(b)}, applied to $i=u$ and $j=v$).

Recall that $B$ is the $n\times n$-matrix obtained from $A$ by switching the
$u$-th row with the $v$-th row. In other words, for all $k\in\left\{
1,2,\ldots,n\right\}  $, we have
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }%
t_{u,v}\left(  k\right)  \text{-th row of }A\right)
\label{sol.ps4.6.a.short.krow2}%
\end{equation}
(because $t_{u,v}$ is the permutation of $\left\{  1,2,\ldots,n\right\}  $
that switches $u$ with $v$ while leaving all other numbers fixed). Therefore,
every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\[
b_{i,j}=a_{t_{u,v}\left(  i\right)  ,j}%
\]
\footnote{\textit{Proof.} We have $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Thus, every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.a.short.bij.pf.1}%
\end{equation}
Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,
\begin{align*}
b_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }B\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.a.short.bij.pf.1})}\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }i\text{-th
row of }B}_{\substack{=\left(  \text{the }t_{u,v}\left(  i\right)  \text{-th
row of }A\right)  \\\text{(by (\ref{sol.ps4.6.a.short.krow2}), applied to
}k=i\text{)}}}\right) \\
&  =\left(  \text{the }j\text{-th entry of the }t_{u,v}\left(  i\right)
\text{-th row of }A\right) \\
&  =a_{t_{u,v}\left(  i\right)  ,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.aij}), applied to }t_{u,v}\left(  i\right)  \text{ instead of
}i\right)  ,
\end{align*}
qed.}. Hence, $B=\left(  \underbrace{b_{i,j}}_{=a_{t_{u,v}\left(  i\right)
,j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{t_{u,v}\left(
i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Therefore, we can
apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $t_{u,v}$, $A$, $a_{i,j}$ and
$B$ instead of $\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain $\det
B=\underbrace{\left(  -1\right)  ^{t_{u,v}}}_{=-1}\cdot\det A=-\det A$.
Exercise \ref{exe.ps4.6} \textbf{(a)} is thus solved.
\end{vershort}

\begin{verlong}
We know that $B$ is the $n\times n$-matrix obtained from $A$ by switching the
$u$-th row with the $v$-th row. In other words,%
\begin{align*}
\left(  \text{the }u\text{-th row of }B\right)   &  =\left(  \text{the
}v\text{-th row of }A\right)  ,\\
\left(  \text{the }v\text{-th row of }B\right)   &  =\left(  \text{the
}u\text{-th row of }A\right)  ,
\end{align*}
and%
\begin{align}
&  \left(  \left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the
}k\text{-th row of }A\right)  \right. \label{sol.ps4.6.a.krow}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }k\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }k\notin\left\{  u,v\right\}  \right)
.\nonumber
\end{align}


Consider the transposition $t_{u,v}$ in $S_{n}$ (defined according to
Definition \ref{def.transpos}). Clearly, $t_{u,v}$ is a permutation of the set
$\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, thus a map $\left[
n\right]  \rightarrow\left[  n\right]  $. Also, $\left(  -1\right)  ^{t_{u,v}%
}=-1$ (by Exercise \ref{exe.ps4.1ab} \textbf{(b)}, applied to $i=u$ and $j=v$).

The permutation $t_{u,v}$ is the permutation in $S_{n}$ which switches $u$
with $v$ while leaving all other elements of $\left\{  1,2,\ldots,n\right\}  $
unchanged (according to the definition of $t_{u,v}$). In other words, we have
$t_{u,v}\left(  u\right)  =v$, $t_{u,v}\left(  v\right)  =u$ and%
\begin{equation}
\left(  t_{u,v}\left(  k\right)  =k\ \ \ \ \ \ \ \ \ \ \text{for all }%
k\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying }k\notin\left\{
u,v\right\}  \right)  . \label{sol.ps4.6.a.tuv}%
\end{equation}


Now, for all $k\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }%
t_{u,v}\left(  k\right)  \text{-th row of }A\right)  \label{sol.ps4.6.a.krow2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.a.krow2}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We need to prove (\ref{sol.ps4.6.a.krow2}). We are in
one of the following three cases:
\par
\textit{Case 1:} We have $k=u$.
\par
\textit{Case 2:} We have $k=v$.
\par
\textit{Case 3:} We have $k\notin\left\{  u,v\right\}  $.
\par
Let us first consider Case 1. In this case, we have $k=u$. Thus,
$t_{u,v}\left(  \underbrace{k}_{=u}\right)  =t_{u,v}\left(  u\right)  =v$, so
that $\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)
=\left(  \text{the }v\text{-th row of }A\right)  $. On the other hand,%
\begin{align*}
\left(  \text{the }\underbrace{k}_{=u}\text{-th row of }B\right)   &  =\left(
\text{the }u\text{-th row of }B\right)  =\left(  \text{the }v\text{-th row of
}A\right) \\
&  =\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.a.krow2}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $k=v$. Thus, $t_{u,v}\left(
\underbrace{k}_{=v}\right)  =t_{u,v}\left(  v\right)  =u$, so that $\left(
\text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  =\left(
\text{the }u\text{-th row of }A\right)  $. On the other hand,%
\begin{align*}
\left(  \text{the }\underbrace{k}_{=v}\text{-th row of }B\right)   &  =\left(
\text{the }v\text{-th row of }B\right)  =\left(  \text{the }u\text{-th row of
}A\right) \\
&  =\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.a.krow2}) is proven in Case 2.
\par
Finally, let us consider Case 3. In this case, we have $k\notin\left\{
u,v\right\}  $. Thus, $t_{u,v}\left(  k\right)  =k$ (by (\ref{sol.ps4.6.a.tuv}%
)), so that $\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of
}A\right)  =\left(  \text{the }k\text{-th row of }A\right)  $. On the other
hand,%
\begin{align*}
\left(  \text{the }k\text{-th row of }B\right)   &  =\left(  \text{the
}k\text{-th row of }A\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.a.krow})}\right) \\
&  =\left(  \text{the }t_{u,v}\left(  k\right)  \text{-th row of }A\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.a.krow2}) is proven in Case 3.
\par
We have now proven (\ref{sol.ps4.6.a.krow2}) in each of the three Cases 1, 2
and 3. Hence, (\ref{sol.ps4.6.a.krow2}) always holds, qed.}. Therefore, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
b_{i,j}=a_{t_{u,v}\left(  i\right)  ,j} \label{sol.ps4.6.a.bij}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.a.bij}):} We have $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.a.bij.pf.1}%
\end{equation}
Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,
\begin{align*}
b_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }B\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.a.bij.pf.1})}\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }i\text{-th
row of }B}_{\substack{=\left(  \text{the }t_{u,v}\left(  i\right)  \text{-th
row of }A\right)  \\\text{(by (\ref{sol.ps4.6.a.krow2}), applied to
}k=i\text{)}}}\right) \\
&  =\left(  \text{the }j\text{-th entry of the }t_{u,v}\left(  i\right)
\text{-th row of }A\right) \\
&  =a_{t_{u,v}\left(  i\right)  ,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6.aij}), applied to }t_{u,v}\left(  i\right)  \text{ instead of
}i\right)  ,
\end{align*}
qed.}. Hence, $B=\left(  \underbrace{b_{i,j}}_{=a_{t_{u,v}\left(  i\right)
,j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{t_{u,v}\left(
i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Therefore, we can
apply Lemma \ref{lem.det.sigma} \textbf{(a)} to $t_{u,v}$, $A$, $a_{i,j}$ and
$B$ instead of $\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain $\det
B=\underbrace{\left(  -1\right)  ^{t_{u,v}}}_{=-1}\cdot\det A=-\det A$.
Exercise \ref{exe.ps4.6} \textbf{(a)} is thus solved.
\end{verlong}

\textbf{(b)} Let $B$ be an $n\times n$-matrix obtained from $A$ by switching
two columns. Thus, $B^{T}$ is an $n\times n$-matrix obtained from $A^{T}$ by
switching two rows (because the columns of $A$ correspond to the rows of
$A^{T}$\ \ \ \ \footnote{See Remark \ref{rmk.colsA=rowsATT.rmk} for the
meaning of \textquotedblleft correspond\textquotedblright\ we are using
here.}). Hence, Exercise \ref{exe.ps4.6} \textbf{(a)} (applied to $A^{T}$ and
$B^{T}$ instead of $A$ and $B$) yields $\det\left(  B^{T}\right)
=-\det\left(  A^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. But recall that $\det\left(  B^{T}\right)
=-\det\left(  A^{T}\right)  $. This rewrites as $\det B=-\det A$ (since
$\det\left(  B^{T}\right)  =\det B$ and $\det\left(  A^{T}\right)  =\det A$).
This solves Exercise \ref{exe.ps4.6} \textbf{(b)}.

\textbf{(c)} Assume that a row of $A$ consists of zeroes. Thus, there exists a
$u\in\left\{  1,2,\ldots,n\right\}  $ such that the $u$-th row of $A$ consists
of zeroes. Consider this $u$.

\begin{vershort}
The $u$-th row of the matrix $A$ consists of zeroes. In other words,
\begin{equation}
a_{u,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,\ldots
,n\right\}  . \label{sol.ps4.6.c.short.1}%
\end{equation}
Now, every $\sigma\in S_{n}$ satisfies $\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }=0$\ \ \ \ \footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then,
the $u$-th factor of the product $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}$ is $a_{u,\sigma\left(  u\right)  }=0$ (by (\ref{sol.ps4.6.c.short.1}),
applied to $j=\sigma\left(  u\right)  $). Hence, the whole product is $0$. In
other words, we have $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0$, qed.}.
Thus, (\ref{eq.det.eq.2}) shows that%
\[
\det A=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{=0}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}0=0.
\]
This solves Exercise \ref{exe.ps4.6} \textbf{(c)}.
\end{vershort}

\begin{verlong}
We have $\left(  \text{the }u\text{-th row of }A\right)  =\underbrace{\left(
0,0,\ldots,0\right)  }_{n\text{ zeroes}}$ (since the $u$-th row of $A$
consists of zeroes). Thus,%
\begin{equation}
a_{u,j}=0\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{  1,2,\ldots
,n\right\}  \label{sol.ps4.6.c.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.c.1}):} Let every $j\in\left\{
1,2,\ldots,n\right\}  $. Applying (\ref{sol.ps4.6.aij}) to $i=u$, we obtain%
\[
\left(  \text{the }\left(  u,j\right)  \text{-th entry of the matrix
}A\right)  =a_{u,j}.
\]
Hence,%
\begin{align*}
a_{u,j}  &  =\left(  \text{the }\left(  u,j\right)  \text{-th entry of the
matrix }A\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }A}_{=\underbrace{\left(  0,0,\ldots,0\right)  }_{n\text{ zeroes}}%
}\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\left(  0,0,\ldots
,0\right)  }_{n\text{ zeroes}}\right)  =0.
\end{align*}
This proves (\ref{sol.ps4.6.c.1}).}. Now, every $\sigma\in S_{n}$ satisfies%
\[
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0
\]
\footnote{\textit{Proof.} Let $\sigma\in S_{n}$. Then, $a_{u,\sigma\left(
u\right)  }=0$ (by (\ref{sol.ps4.6.c.1})). But $a_{u,\sigma\left(  u\right)
}$ is a factor of the product $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$
(namely, the factor for $i=u$). Hence, one factor of the product $\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$ (since $a_{u,\sigma
\left(  u\right)  }=0$). Therefore, the whole product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$ (because if one factor of a
product equals $0$, then the whole product equals $0$). Qed.}. Now,
(\ref{eq.det.eq.2}) shows that%
\[
\det A=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{=0}=\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}0=0.
\]
This solves Exercise \ref{exe.ps4.6} \textbf{(c)}.
\end{verlong}

\textbf{(d)} Assume that a column of $A$ consists of zeroes. Thus, a row of
$A^{T}$ consists of zeroes (because the columns of $A$ correspond to the rows
of $A^{T}$\ \ \ \ \footnote{See Remark \ref{rmk.colsA=rowsATT.rmk} for the
meaning of \textquotedblleft correspond\textquotedblright\ we are using
here.}). Hence, Exercise \ref{exe.ps4.6} \textbf{(c)} (applied to $A^{T}$
instead of $A$) yields $\det\left(  A^{T}\right)  =0$.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$.
Hence, $\det A=\det\left(  A^{T}\right)  =0$. This solves Exercise
\ref{exe.ps4.6} \textbf{(d)}.

\textbf{(e)} Assume that $A$ has two equal rows. In other words, some two
distinct rows of $A$ are equal (where \textquotedblleft
distinct\textquotedblright\ means that these rows are in different positions,
not that they are distinct as row vectors). In other words, there exist two
distinct elements $u$ and $v$ of $\left\{  1,2,\ldots,n\right\}  $ such that%
\begin{equation}
\left(  \text{the }u\text{-th row of }A\right)  =\left(  \text{the }v\text{-th
row of }A\right)  . \label{sol.ps4.6.e.1}%
\end{equation}
Consider these $u$ and $v$.

\begin{vershort}
The equality (\ref{sol.ps4.6.e.1}) shows that%
\begin{equation}
a_{u,j}=a_{v,j}\ \ \ \ \ \ \ \ \ \ \text{for every }j\in\left\{
1,2,\ldots,n\right\}  \label{sol.ps4.6.e.short.1a}%
\end{equation}
(because $a_{u,j}$ is the $j$-th entry of the $u$-th row of $A$, while
$a_{v,j}$ is the $j$-th entry of the $v$-th row of $A$).

Define a map $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ by
\[
\left(  \kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n\right]  \right)
.
\]
The definition of $\kappa$ shows that $\kappa\left(  v\right)  =v$ (since
$v\neq u$) but also $\kappa\left(  u\right)  =v$. Thus, $\kappa\left(
u\right)  =v=\kappa\left(  v\right)  $, in spite of $u\neq v$. Therefore, the
map $\kappa$ is not injective, and thus not bijective; in particular, $\kappa$
is not a permutation. Thus, $\kappa\notin S_{n}$. But every $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy%
\begin{equation}
a_{\kappa\left(  i\right)  ,j}=a_{i,j} \label{sol.ps4.6.e.short.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.e.short.3}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We must
prove (\ref{sol.ps4.6.e.short.3}). We must be in one of the following two
cases:
\par
\textit{Case 1:} We have $i\neq u$.
\par
\textit{Case 2:} We have $i=u$.
\par
Let us first consider Case 1. In this case, we have $i\neq u$. Thus, the
definition of $\kappa$ yields $\kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  =i$ (since $i\neq u$), so that $a_{\kappa\left(  i\right)
,j}=a_{i,j}$. Thus, (\ref{sol.ps4.6.e.short.3}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i=u$. Thus, $\kappa\left(
i\right)  =\kappa\left(  u\right)  =v$ (since $i=u$). Hence,%
\begin{align*}
a_{\kappa\left(  i\right)  ,j}  &  =a_{v,j}=a_{u,j}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.ps4.6.e.short.1a})}\right) \\
&  =a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }u=i\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.e.short.3}) is proven in Case 2.
\par
We have now proven (\ref{sol.ps4.6.e.short.3}) in both Cases 1 and 2. Thus,
(\ref{sol.ps4.6.e.short.3}) always holds, qed.}. Thus, $\left(
\underbrace{a_{\kappa\left(  i\right)  ,j}}_{=a_{i,j}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
=A$. Therefore, we can apply Lemma \ref{lem.det.sigma} \textbf{(b)} to $A$,
$a_{i,j}$ and $A$ instead of $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain
$\det A=0$. Exercise \ref{exe.ps4.6} \textbf{(e)} is thus solved.
\end{vershort}

\begin{verlong}
Define a map $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ by
\[
\left(  \kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[  n\right]  \right)
.
\]
Then, $\kappa\notin S_{n}$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Then, $\kappa\in S_{n}$. Thus, $\kappa$ is a permutation. Hence, the
map $\kappa$ is injective. But the definition of $\kappa$ yields
$\kappa\left(  u\right)  =\left\{
\begin{array}
[c]{c}%
u,\ \ \ \ \ \ \ \ \ \ \text{if }u\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }u=u
\end{array}
\right.  =v$ (since $u=u$). Also, $u$ and $v$ are distinct, so that we have
$v\neq u$. Hence, $\kappa\left(  v\right)  \neq\kappa\left(  u\right)  $
(since $\kappa$ is injective). But the definition of $\kappa$ yields
$\kappa\left(  v\right)  =\left\{
\begin{array}
[c]{c}%
v,\ \ \ \ \ \ \ \ \ \ \text{if }v\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }v=u
\end{array}
\right.  =v=\kappa\left(  u\right)  $. This contradicts $\kappa\left(
v\right)  \neq\kappa\left(  u\right)  $. This contradiction proves that our
assumption was wrong, qed.}. But every $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy%
\begin{equation}
a_{\kappa\left(  i\right)  ,j}=a_{i,j} \label{sol.ps4.6.e.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.e.3}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We must
prove (\ref{sol.ps4.6.e.3}). We must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\neq u$.
\par
\textit{Case 2:} We have $i=u$.
\par
Let us first consider Case 1. In this case, we have $i\neq u$. Thus, the
definition of $\kappa$ yields $\kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  =i$ (since $i\neq u$), so that $a_{\kappa\left(  i\right)
,j}=a_{i,j}$. Thus, (\ref{sol.ps4.6.e.3}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i=u$. Thus, the definition
of $\kappa$ yields $\kappa\left(  i\right)  =\left\{
\begin{array}
[c]{c}%
i,\ \ \ \ \ \ \ \ \ \ \text{if }i\neq u;\\
v,\ \ \ \ \ \ \ \ \ \ \text{if }i=u
\end{array}
\right.  =v$ (since $i=u$). Hence,%
\begin{align*}
a_{\kappa\left(  i\right)  ,j}  &  =a_{v,j}=\left(  \text{the }\left(
v,j\right)  \text{-th entry of the matrix }A\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  \text{the }\left(  v,j\right)  \text{-th entry of the
matrix }A\right)  =a_{v,j}\\
\text{(by (\ref{sol.ps4.6.aij}), applied to }v\text{ instead of }i\text{)}%
\end{array}
\right) \\
&  =\left(  \text{the }j\text{-th entry of }\underbrace{\text{the }v\text{-th
row of }A}_{\substack{=\left(  \text{the }u\text{-th row of }A\right)
\\\text{(by (\ref{sol.ps4.6.e.1}))}}}\right)  =\left(  \text{the }j\text{-th
entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,j\right)  \text{-th entry of the matrix
}A\right)  =a_{u,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6.aij}),
applied to }u\text{ instead of }i\right) \\
&  =a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }u=i\right)  .
\end{align*}
Thus, (\ref{sol.ps4.6.e.3}) is proven in Case 2.
\par
We have now proven (\ref{sol.ps4.6.e.3}) in both Cases 1 and 2. Thus,
(\ref{sol.ps4.6.e.3}) always holds, qed.}. Thus, $\left(
\underbrace{a_{\kappa\left(  i\right)  ,j}}_{=a_{i,j}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
=A$. Therefore, we can apply Lemma \ref{lem.det.sigma} \textbf{(b)} to $A$,
$a_{i,j}$ and $A$ instead of $B$, $b_{i,j}$ and $B_{\kappa}$. We thus obtain
$\det A=0$. Exercise \ref{exe.ps4.6} \textbf{(e)} is thus solved.
\end{verlong}

\textbf{(f)} Assume that $A$ has two equal columns. Thus, $A^{T}$ has two
equal rows (because the columns of $A$ correspond to the rows of $A^{T}%
$\ \ \ \ \footnote{See Remark \ref{rmk.colsA=rowsATT.rmk} for the meaning of
\textquotedblleft correspond\textquotedblright\ we are using here.}). Hence,
Exercise \ref{exe.ps4.6} \textbf{(e)} (applied to $A^{T}$ instead of $A$)
yields $\det\left(  A^{T}\right)  =0$.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$.
Hence, $\det A=\det\left(  A^{T}\right)  =0$. This solves Exercise
\ref{exe.ps4.6} \textbf{(f)}.

\textbf{(g)} Let $B$ be the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th row by $\lambda$. Thus,
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\lambda\left(  \text{the
}k\text{-th row of }A\right)  , \label{sol.ps4.6.g.krow}%
\end{equation}
whereas%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6.g.urow}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  .\nonumber
\end{align}


We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Thus, every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.g.bij}%
\end{equation}


\begin{vershort}
For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.g.short.buv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.short.buv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.g.urow}) shows that the $v$-th entry of the
$u$-th row of $B$ equals the $v$-th entry of the $u$-th row of $A$. Since the
former entry is $b_{u,v}$, while the latter entry equals $a_{u,v}$, this
rewrites as $b_{u,v}=a_{u,v}$. This proves (\ref{sol.ps4.6.g.short.buv}).}.
For every $v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=\lambda a_{k,v} \label{sol.ps4.6.g.short.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.short.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.g.krow}) shows that the $v$-th
entry of the $k$-th row of $B$ equals $\lambda$ times the $v$-th entry of the
$k$-th row of $A$. Since the former entry is $b_{k,v}$, while the latter entry
equals $a_{k,v}$, this rewrites as $b_{k,v}=\lambda a_{k,v}$. This proves
(\ref{sol.ps4.6.g.short.bkv}).}. Now, it is easy to see that every $\sigma\in
S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  } \label{sol.ps4.6.g.short.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.short.sigma}):} Let $\sigma\in
S_{n}$. Taking the $k$-th factor out of the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$, we obtain
\[
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)
}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}a_{i,\sigma\left(  i\right)  }.
\]
Multiplying both sides of this equality by $\lambda$, we obtain%
\[
\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\lambda a_{k,\sigma
\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }.
\]
Compared with%
\begin{align*}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }  &  =\underbrace{b_{k,\sigma
\left(  k\right)  }}_{\substack{=\lambda a_{k,\sigma\left(  k\right)
}\\\text{(by (\ref{sol.ps4.6.g.short.bkv}), applied to}\\v=\sigma\left(
k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }}%
_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.g.short.buv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\in\left\{
1,2,\ldots,n\right\}  \right) \\
&  =\lambda a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  },
\end{align*}
this yields $\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$. This proves
(\ref{sol.ps4.6.g.short.sigma}).}. Now, recall that $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B$
and $b_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\lambda
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.g.short.sigma}))}}}=\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\lambda\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}=\lambda\det A.
\end{align*}
Thus, Exercise \ref{exe.ps4.6} \textbf{(g)} is solved.
\end{vershort}

\begin{verlong}
For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.g.buv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.buv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.g.bij}) (applied to $i=u$ and $j=v$) yields%
\[
\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}B\right)  =b_{u,v},
\]
so that%
\begin{align*}
b_{u,v}  &  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }B}_{\substack{=\left(  \text{the }u\text{-th row of }A\right)
\\\text{(by (\ref{sol.ps4.6.g.urow}))}}}\right) \\
&  =\left(  \text{the }v\text{-th entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A\right)  =a_{u,v}%
\end{align*}
(by (\ref{sol.ps4.6.aij}), applied to $i=u$ and $j=v$), qed.}. For every
$v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=\lambda a_{k,v} \label{sol.ps4.6.g.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.g.bij}) (applied to $i=k$ and
$j=v$) yields%
\[
\left(  \text{the }\left(  k,v\right)  \text{-th entry of the matrix
}B\right)  =b_{k,v},
\]
so that%
\begin{align*}
b_{k,v}  &  =\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }k\text{-th
row of }B}_{=\lambda\left(  \text{the }k\text{-th row of }A\right)  }\right)
\\
&  =\left(  \text{the }v\text{-th entry of }\lambda\left(  \text{the
}k\text{-th row of }A\right)  \right) \\
&  =\lambda\underbrace{\left(  \text{the }v\text{-th entry of the }k\text{-th
row of }A\right)  }_{=\left(  \text{the }\left(  k,v\right)  \text{-th entry
of the matrix }A\right)  }\\
&  =\lambda\underbrace{\left(  \text{the }\left(  k,v\right)  \text{-th entry
of the matrix }A\right)  }_{\substack{=a_{k,v}\\\text{(by (\ref{sol.ps4.6.aij}%
), applied to }i=k\text{ and }j=v\text{)}}}=\lambda a_{k,v},
\end{align*}
qed.}. Now, it is easy to see that every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  } \label{sol.ps4.6.g.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.g.sigma}):} Let $\sigma\in S_{n}$.
We have%
\[
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }=\prod_{i\in\left\{  1,2,\ldots,n\right\}
}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)  }\cdot
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma
\left(  i\right)  }%
\]
(since $k\in\left\{  1,2,\ldots,n\right\}  $). Multiplying both sides of this
equality by $\lambda$, we obtain%
\[
\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\lambda a_{k,\sigma
\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }.
\]
Compared with%
\begin{align*}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}b_{i,\sigma\left(  i\right)  }  &  =\prod_{i\in\left\{  1,2,\ldots,n\right\}
}b_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{b_{k,\sigma\left(  k\right)  }}_{\substack{=\lambda
a_{k,\sigma\left(  k\right)  }\\\text{(by (\ref{sol.ps4.6.g.bkv}), applied
to}\\v=\sigma\left(  k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by (\ref{sol.ps4.6.g.buv}%
), applied to}\\u=i\text{ and }v=\sigma\left(  i\right)  \text{)}%
}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\in\left\{  1,2,\ldots,n\right\}
\right) \\
&  =\lambda a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  },
\end{align*}
this yields $\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\lambda\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$. This proves
(\ref{sol.ps4.6.g.sigma}).}. Now, recall that $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, (\ref{eq.det.eq.2}) (applied to $B$
and $b_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\lambda
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.g.sigma}))}}}=\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\lambda\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\lambda\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}=\lambda\det A.
\end{align*}
Thus, Exercise \ref{exe.ps4.6} \textbf{(g)} is solved.
\end{verlong}

\textbf{(h)} Let $B$ be the $n\times n$-matrix obtained from $A$ by
multiplying the $k$-th column by $\lambda$. Thus, $B^{T}$ is the $n\times
n$-matrix obtained from $A^{T}$ by multiplying the $k$-th row by $\lambda$
(because the columns of $A$ correspond to the rows of $A^{T}$%
\ \ \ \ \footnote{See Remark \ref{rmk.colsA=rowsATT.rmk} for the meaning of
\textquotedblleft correspond\textquotedblright\ we are using here.}). Hence,
Exercise \ref{exe.ps4.6} \textbf{(g)} (applied to $A^{T}$ and $B^{T}$ instead
of $A$ and $B$) yields $\det\left(  B^{T}\right)  =\lambda\det\left(
A^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. But recall that $\det\left(  B^{T}\right)
=\lambda\det\left(  A^{T}\right)  $. This rewrites as $\det B=\lambda\det A$
(since $\det\left(  B^{T}\right)  =\det B$ and $\det\left(  A^{T}\right)
=\det A$). This solves Exercise \ref{exe.ps4.6} \textbf{(h)}.

\textbf{(i)} We know that the rows of the matrix $A^{\prime}$ equal the
corresponding rows of $A$ except (perhaps) the $k$-th row. In other words,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }A^{\prime}\right)  =\left(
\text{the }u\text{-th row of }A\right)  \right. \label{sol.ps4.6.i.urowA'}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  .\nonumber
\end{align}


We know that $B$ is the $n\times n$-matrix obtained from $A$ by adding the
$k$-th row of $A^{\prime}$ to the $k$-th row of $A$. Hence,%
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }k\text{-th
row of }A\right)  +\left(  \text{the }k\text{-th row of }A^{\prime}\right)
\label{sol.ps4.6.i.krowB}%
\end{equation}
and%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6.i.urowB}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  .\nonumber
\end{align}


\begin{vershort}
We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, and we write the matrix $A^{\prime}$ in the form
$A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
$. Then, for every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
a_{u,v}^{\prime}=a_{u,v} \label{sol.ps4.6.i.short.a'auv}%
\end{equation}
\footnote{This follows from (\ref{sol.ps4.6.i.urowA'}) in a similar way as
(\ref{sol.ps4.6.g.short.buv}) follows from (\ref{sol.ps4.6.g.urow}).} and%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.i.short.bauv}%
\end{equation}
\footnote{This follows from (\ref{sol.ps4.6.i.urowB}) in a similar way as
(\ref{sol.ps4.6.g.short.buv}) follows from (\ref{sol.ps4.6.g.urow}).}. Also,
for every $v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=a_{k,v}+a_{k,v}^{\prime} \label{sol.ps4.6.i.short.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.short.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.i.krowB}) shows that
\begin{align*}
&  \left(  \text{the }v\text{-th entry of the }k\text{-th row of }B\right) \\
&  =\left(  \text{the }v\text{-th entry of the }k\text{-th row of }A\right)
+\left(  \text{the }v\text{-th entry of the }k\text{-th row of }A^{\prime
}\right)  .
\end{align*}
But the three entries appearing in this equality are $b_{k,v}$, $a_{k,v}$ and
$a_{k,v}^{\prime}$ (in this order). Thus, this equality rewrites as
$b_{k,v}=a_{k,v}+a_{k,v}^{\prime}$. This proves (\ref{sol.ps4.6.i.short.bkv}%
).}. Now, it is easy to see that every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\prod_{i=1}^{n}a_{i,\sigma
\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}
\label{sol.ps4.6.i.short.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.short.sigma}):} Let $\sigma\in
S_{n}$. Pulling the $k$-th factor out of the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$, we obtain%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)
}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}a_{i,\sigma\left(  i\right)  } \label{sol.ps4.6.i.short.sigma.pf.1}%
\end{equation}
(since $k\in\left\{  1,2,\ldots,n\right\}  $). Similarly,%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}=a_{k,\sigma\left(
k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\underbrace{a_{i,\sigma\left(  i\right)  }^{\prime}}%
_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.short.a'auv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}=a_{k,\sigma\left(  k\right)  }^{\prime}\cdot
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma
\left(  i\right)  } \label{sol.ps4.6.i.short.sigma.pf.2}%
\end{equation}
and%
\begin{align*}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }  &  =\underbrace{b_{k,\sigma
\left(  k\right)  }}_{\substack{=a_{k,\sigma\left(  k\right)  }+a_{k,\sigma
\left(  k\right)  }^{\prime}\\\text{(by (\ref{sol.ps4.6.i.short.bkv}), applied
to}\\v=\sigma\left(  k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.short.bauv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}\\
&  =\left(  a_{k,\sigma\left(  k\right)  }+a_{k,\sigma\left(  k\right)
}^{\prime}\right)  \cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.short.sigma.pf.1}))}}}+\underbrace{a_{k,\sigma\left(
k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }}_{\substack{=\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }^{\prime}\\\text{(by
(\ref{sol.ps4.6.i.short.sigma.pf.2}))}}}\\
&  =\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }^{\prime}.
\end{align*}
This proves (\ref{sol.ps4.6.i.short.sigma}).}.
\end{vershort}

\begin{verlong}
We write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Thus, every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j}. \label{sol.ps4.6.i.bij}%
\end{equation}
We write the matrix $A^{\prime}$ in the form $A^{\prime}=\left(
a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Thus, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A^{\prime}\right)  =a_{i,j}^{\prime}. \label{sol.ps4.6.i.a'ij}%
\end{equation}


For every $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,n\right\}  $ satisfying $u\neq k$, we have%
\begin{equation}
b_{u,v}=a_{u,v} \label{sol.ps4.6.i.buv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.buv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.i.bij}) (applied to $i=u$ and $j=v$) yields%
\[
\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}B\right)  =b_{u,v},
\]
so that%
\begin{align*}
b_{u,v}  &  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }B}_{\substack{=\left(  \text{the }u\text{-th row of }A\right)
\\\text{(by (\ref{sol.ps4.6.i.urowB}))}}}\right) \\
&  =\left(  \text{the }v\text{-th entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A\right)  =a_{u,v}%
\end{align*}
(by (\ref{sol.ps4.6.aij}), applied to $i=u$ and $j=v$), qed.} and%
\begin{equation}
b_{u,v}=a_{u,v}^{\prime} \label{sol.ps4.6.i.a'uv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.a'uv}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}  $ be such that
$u\neq k$. Then, (\ref{sol.ps4.6.i.a'ij}) (applied to $i=u$ and $j=v$) yields%
\[
\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A^{\prime}\right)  =a_{u,v}^{\prime},
\]
so that%
\begin{align*}
a_{u,v}^{\prime}  &  =\left(  \text{the }\left(  u,v\right)  \text{-th entry
of the matrix }A^{\prime}\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }u\text{-th
row of }A^{\prime}}_{\substack{=\left(  \text{the }u\text{-th row of
}A\right)  \\\text{(by (\ref{sol.ps4.6.i.urowA'}))}}}\right) \\
&  =\left(  \text{the }v\text{-th entry of the }u\text{-th row of }A\right) \\
&  =\left(  \text{the }\left(  u,v\right)  \text{-th entry of the matrix
}A\right)  =a_{u,v}%
\end{align*}
(by (\ref{sol.ps4.6.aij}), applied to $i=u$ and $j=v$). Compared with
(\ref{sol.ps4.6.i.buv}), this yields $b_{u,v}=a_{u,v}^{\prime}$, qed.}. For
every $v\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{equation}
b_{k,v}=a_{k,v}+a_{k,v}^{\prime} \label{sol.ps4.6.i.bkv}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.bkv}):} Let $v\in\left\{
1,2,\ldots,n\right\}  $. Then, (\ref{sol.ps4.6.i.bij}) (applied to $i=k$ and
$j=v$) yields%
\[
\left(  \text{the }\left(  k,v\right)  \text{-th entry of the matrix
}B\right)  =b_{k,v},
\]
so that%
\begin{align*}
b_{k,v}  &  =\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }B\right) \\
&  =\left(  \text{the }v\text{-th entry of }\underbrace{\text{the }k\text{-th
row of }B}_{=\left(  \text{the }k\text{-th row of }A\right)  +\left(
\text{the }k\text{-th row of }A^{\prime}\right)  }\right) \\
&  =\left(  \text{the }v\text{-th entry of }\left(  \text{the }k\text{-th row
of }A\right)  +\left(  \text{the }k\text{-th row of }A^{\prime}\right)
\right) \\
&  =\underbrace{\left(  \text{the }v\text{-th entry of the }k\text{-th row of
}A\right)  }_{=\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }A\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  \text{the }v\text{-th entry of the
}k\text{-th row of }A^{\prime}\right)  }_{=\left(  \text{the }\left(
k,v\right)  \text{-th entry of the matrix }A^{\prime}\right)  }\\
&  =\underbrace{\left(  \text{the }\left(  k,v\right)  \text{-th entry of the
matrix }A\right)  }_{\substack{=a_{k,v}\\\text{(by (\ref{sol.ps4.6.aij}),
applied to }i=k\text{ and }j=v\text{)}}}+\underbrace{\left(  \text{the
}\left(  k,v\right)  \text{-th entry of the matrix }A^{\prime}\right)
}_{\substack{=a_{k,v}^{\prime}\\\text{(by (\ref{sol.ps4.6.i.a'ij}), applied to
}i=k\text{ and }j=v\text{)}}}\\
&  =a_{k,v}+a_{k,v}^{\prime},
\end{align*}
qed.}. Now, it is easy to see that every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }=\prod_{i=1}^{n}a_{i,\sigma
\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}
\label{sol.ps4.6.i.sigma}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6.i.sigma}):} Let $\sigma\in S_{n}$.
We have%
\begin{equation}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }=\prod_{i\in\left\{  1,2,\ldots,n\right\}
}a_{i,\sigma\left(  i\right)  }=a_{k,\sigma\left(  k\right)  }\cdot
\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma
\left(  i\right)  } \label{sol.ps4.6.i.sigma.pf.1}%
\end{equation}
(since $k\in\left\{  1,2,\ldots,n\right\}  $) and%
\begin{equation}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }^{\prime}=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }a_{i,\sigma\left(  i\right)  }^{\prime}=a_{k,\sigma\left(
k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}a_{i,\sigma\left(  i\right)  }^{\prime}
\label{sol.ps4.6.i.sigma.pf.2}%
\end{equation}
(since $k\in\left\{  1,2,\ldots,n\right\}  $). Furthermore,%
\begin{align*}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}b_{i,\sigma\left(  i\right)  }  &  =\prod_{i\in\left\{  1,2,\ldots,n\right\}
}b_{i,\sigma\left(  i\right)  }=\underbrace{b_{k,\sigma\left(  k\right)  }%
}_{\substack{=a_{k,\sigma\left(  k\right)  }+a_{k,\sigma\left(  k\right)
}^{\prime}\\\text{(by (\ref{sol.ps4.6.i.bkv}), applied to}\\v=\sigma\left(
k\right)  \text{)}}}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}b_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\in\left\{  1,2,\ldots,n\right\}
\right) \\
&  =\left(  a_{k,\sigma\left(  k\right)  }+a_{k,\sigma\left(  k\right)
}^{\prime}\right)  \cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}b_{i,\sigma\left(  i\right)  }\\
&  =a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }\\\text{(by (\ref{sol.ps4.6.i.buv}%
), applied to}\\u=i\text{ and }v=\sigma\left(  i\right)  \text{)}%
}}+a_{k,\sigma\left(  k\right)  }^{\prime}\cdot\prod_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\underbrace{b_{i,\sigma\left(  i\right)  }%
}_{\substack{=a_{i,\sigma\left(  i\right)  }^{\prime}\\\text{(by
(\ref{sol.ps4.6.i.a'uv}), applied to}\\u=i\text{ and }v=\sigma\left(
i\right)  \text{)}}}\\
&  =\underbrace{a_{k,\sigma\left(  k\right)  }\cdot\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq k}}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{sol.ps4.6.i.sigma.pf.1}))}}}+\underbrace{a_{k,\sigma\left(  k\right)
}^{\prime}\cdot\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}a_{i,\sigma\left(  i\right)  }^{\prime}}_{\substack{=\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }^{\prime}\\\text{(by
(\ref{sol.ps4.6.i.sigma.pf.2}))}}}\\
&  =\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }^{\prime}.
\end{align*}
This proves (\ref{sol.ps4.6.i.sigma}).}.
\end{verlong}

We have $A=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$;
therefore, (\ref{eq.det.eq.2}) (applied to $A^{\prime}$ and $a_{i,j}^{\prime}$
instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
\det A^{\prime}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}. \label{sol.ps4.6.i.detA'}%
\end{equation}


\begin{vershort}
Now, recall that $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Hence, (\ref{eq.det.eq.2}) (applied to $B$ and $b_{i,j}$ instead of $A$ and
$a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }^{\prime}\\\text{(by (\ref{sol.ps4.6.i.short.sigma}))}}%
}=\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}^{\prime}\right) \\
&  =\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}+\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}}%
_{\substack{=\det A^{\prime}\\\text{(by (\ref{sol.ps4.6.i.detA'}))}}}=\det
A+\det A^{\prime}.
\end{align*}
This solves Exercise \ref{exe.ps4.6} \textbf{(i)}.
\end{vershort}

\begin{verlong}
Now, recall that $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Hence, (\ref{eq.det.eq.2}) (applied to $B$ and $b_{i,j}$ instead of $A$ and
$a_{i,j}$) yields%
\begin{align*}
\det B  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\underbrace{\prod_{i=1}^{n}b_{i,\sigma\left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }^{\prime}\\\text{(by (\ref{sol.ps4.6.i.sigma}))}}}=\sum_{\sigma\in
S_{n}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }+\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}\right) \\
&  =\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\det A\\\text{(by
(\ref{eq.det.eq.2}))}}}+\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }^{\prime}}%
_{\substack{=\det A^{\prime}\\\text{(by (\ref{sol.ps4.6.i.detA'}))}}}=\det
A+\det A^{\prime}.
\end{align*}
This solves Exercise \ref{exe.ps4.6} \textbf{(i)}.
\end{verlong}

\textbf{(j)} We know that $A^{\prime}$ is an $n\times n$-matrix whose columns
equal the corresponding columns of $A$ except (perhaps) the $k$-th column.
Thus, $\left(  A^{\prime}\right)  ^{T}$ is an $n\times n$-matrix whose rows
equal the corresponding rows of $A^{T}$ except (perhaps) the $k$-th row
(because the columns of $A$ correspond to the rows of $A^{T}$%
\ \ \ \ \footnote{See Remark \ref{rmk.colsA=rowsATT.rmk} for the meaning of
\textquotedblleft correspond\textquotedblright\ we are using here.}, and
similarly the columns of $A^{\prime}$ correspond to the rows of $\left(
A^{\prime}\right)  ^{T}$).

Also, we know that $B$ is the $n\times n$-matrix obtained from $A$ by adding
the $k$-th column of $A^{\prime}$ to the $k$-th column of $A$. Hence, $B^{T}$
is the $n\times n$-matrix obtained from $A^{T}$ by adding the $k$-th row of
$\left(  A^{\prime}\right)  ^{T}$ to the $k$-th row of $A^{T}$ (because the
columns of $A$ correspond to the rows of $A^{T}$, and similarly the columns of
$A^{\prime}$ correspond to the rows of $\left(  A^{\prime}\right)  ^{T}$, and
similarly the columns of $B$ correspond to the rows of $B^{T}$).

Hence, Exercise \ref{exe.ps4.6} \textbf{(i)} (applied to $A^{T}$, $\left(
A^{\prime}\right)  ^{T}$ and $B^{T}$ instead of $A$, $A^{\prime}$ and $B$)
yields $\det\left(  B^{T}\right)  =\det\left(  A^{T}\right)  +\det\left(
\left(  A^{\prime}\right)  ^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. Finally, Exercise \ref{exe.ps4.4} (applied to
$A^{\prime}$ instead of $A$) yields $\det\left(  \left(  A^{\prime}\right)
^{T}\right)  =\det A^{\prime}$.

But recall that $\det\left(  B^{T}\right)  =\det\left(  A^{T}\right)
+\det\left(  \left(  A^{\prime}\right)  ^{T}\right)  $. This rewrites as $\det
B=\det A+\det A^{\prime}$ (since $\det\left(  B^{T}\right)  =\det B$ and
$\det\left(  A^{T}\right)  =\det A$ and $\det\left(  \left(  A^{\prime
}\right)  ^{T}\right)  =\det A^{\prime}$). This solves Exercise
\ref{exe.ps4.6} \textbf{(j)}.

[\textit{Remark:} It is tempting to regard Exercise \ref{exe.ps4.6}
\textbf{(e)} as a consequence of Exercise \ref{exe.ps4.6} \textbf{(a)},
because if a matrix $A$ has two equal rows, then switching these two rows does
not change the matrix $A$, and thus Exercise \ref{exe.ps4.6} \textbf{(a)}
(applied to $B=A$) yields that $\det A=-\det A$ in this case. However, we
cannot conclude $\det A=0$ from $\det A=-\det A$ in general, unless we know
that we can \textquotedblleft divide by $2$\textquotedblright\ (or at least
cancel a factor of $2$ from equalities) in the commutative ring $\mathbb{K}$.
For example, if $\mathbb{K}$ is the ring $\mathbb{Z}/2\mathbb{Z}$, then every
$a\in\mathbb{K}$ satisfies $a=-a$, but not every $a\in\mathbb{K}$ satisfies
$a=0$. Of course, if $\mathbb{K}=\mathbb{Z}$ or $\mathbb{K}=\mathbb{Q}$ or
$\mathbb{K}=\mathbb{R}$ or $\mathbb{K}=\mathbb{C}$, then this argument works
and thus Exercise \ref{exe.ps4.6} \textbf{(e)} does follow from Exercise
\ref{exe.ps4.6} \textbf{(a)} in these cases.]
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.6k}}

Our solution to Exercise \ref{exe.ps4.6k} relies on Lemma \ref{lem.det.sigma}
and on Lemma \ref{lem.colsA=rowsATT}. Thus, we advise the reader to read the
proofs of these two lemmas before the solution of the exercise.

Before we start solving Exercise \ref{exe.ps4.6k}, let us prove a simple fact:

\begin{lemma}
\label{lem.sol.ps4.6k.lem.1}Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix such that some row of the matrix $A$ equals a scalar multiple of
some other row of $A$. Then,%
\begin{equation}
\det A=0. \label{sol.ps4.6k.lem.1}%
\end{equation}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.ps4.6k.lem.1}.]Some row of the matrix $A$ equals
a scalar multiple of some other row of $A$. In other words, there exist two
distinct elements $k$ and $\ell$ of $\left\{  1,2,\ldots,n\right\}  $ such
that the $k$-th row of the matrix $A$ equals a scalar multiple of the $\ell
$-th row of $A$. Consider these $k$ and $\ell$.

\begin{vershort}
Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $k$-th
row of $A$ by the $\ell$-th row of $A$. Then, the matrix $C$ has two equal
rows (namely, its $k$-th row and its $\ell$-th row). Hence, $\det C=0$ (by
Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$ instead of $A$).

Recall that the $k$-th row of the matrix $A$ equals a scalar multiple of the
$\ell$-th row of $A$. In other words, there exists a $\lambda\in\mathbb{K}$
such that%
\begin{equation}
\left(  \text{the }k\text{-th row of }A\right)  =\lambda\left(  \text{the
}\ell\text{-th row of }A\right)  . \label{sol.ps4.6k.short.lem.1.pf.1}%
\end{equation}
Consider this $\lambda$. By the construction of $C$, we have
\begin{equation}
\left(  \text{the }k\text{-th row of }C\right)  =\left(  \text{the }%
\ell\text{-th row of }A\right)  . \label{sol.ps4.6k.short.lem.1.pf.2}%
\end{equation}
Thus, (\ref{sol.ps4.6k.short.lem.1.pf.1}) becomes%
\begin{equation}
\left(  \text{the }k\text{-th row of }A\right)  =\lambda\underbrace{\left(
\text{the }\ell\text{-th row of }A\right)  }_{\substack{=\left(  \text{the
}k\text{-th row of }C\right)  \\\text{(by (\ref{sol.ps4.6k.short.lem.1.pf.2}%
))}}}=\lambda\left(  \text{the }k\text{-th row of }C\right)  .
\label{sol.ps4.6k.short.lem.1.pf.3}%
\end{equation}
On the other hand, for every $u\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$u\neq k$, we have%
\[
\left(  \text{the }u\text{-th row of }C\right)  =\left(  \text{the }u\text{-th
row of }A\right)
\]
(since the construction of $C$ involves modifying only the $k$-th row of $A$)
and thus%
\begin{equation}
\left(  \text{the }u\text{-th row of }A\right)  =\left(  \text{the }u\text{-th
row of }C\right)  . \label{sol.ps4.6k.short.lem.1.pf.4}%
\end{equation}


Now, combining (\ref{sol.ps4.6k.short.lem.1.pf.3}) with
(\ref{sol.ps4.6k.short.lem.1.pf.4}), we obtain the following description of
$A$ from $C$: The matrix $A$ is the matrix obtained from $C$ by multiplying
the $k$-th row by $\lambda$. Then, $\det A=\lambda\det C$ (by Exercise
\ref{exe.ps4.6} \textbf{(g)}, applied to $C$ and $A$ instead of $A$ and $B$).
Thus, $\det A=\lambda\underbrace{\det C}_{=0}=0$. This proves Lemma
\ref{lem.sol.ps4.6k.lem.1}.
\end{vershort}

\begin{verlong}
Let $w$ be the $\ell$-th row of $A$ (regarded, as usual, as a row vector).
Thus, $w=\left(  \text{the }\ell\text{-th row of }A\right)  $.

Recall that the $k$-th row of $A$ equals a scalar multiple of the $\ell$-th
row of $A$. In other words, the $k$-th row of $A$ equals a scalar multiple of
$w$ (since $w$ is the $\ell$-th row of $A$). In other words, there exists some
$\lambda\in\mathbb{K}$ such that
\begin{equation}
\left(  \text{the }k\text{-th row of }A\right)  =\lambda w.
\label{sol.ps4.6k.lem.2}%
\end{equation}
Consider this $\lambda$.

Let $C$ be the $n\times n$-matrix obtained from $A$ by replacing the $k$-th
row of $A$ by the row vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }C\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.lem.3}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }k\text{-th row of }C\right)  =w. \label{sol.ps4.6k.lem.4}%
\end{equation}
The matrix $C$ has two equal rows\footnote{\textit{Proof.} The numbers $k$ and
$\ell$ are distinct. Thus, $\ell\neq k$. Hence, (\ref{sol.ps4.6k.lem.3})
(applied to $u=\ell$) yields%
\begin{align*}
\left(  \text{the }\ell\text{-th row of }C\right)   &  =\left(  \text{the
}\ell\text{-th row of }A\right)  =w\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}w=\left(  \text{the }\ell\text{-th row of }A\right)  \right) \\
&  =\left(  \text{the }k\text{-th row of }C\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.lem.4})}\right)  .
\end{align*}
In other words, the $\ell$-th row of $C$ and the $k$-th row of $C$ are equal.
Since $\ell\neq k$, this shows that the matrix $C$ has two equal rows. Qed.}.
Hence, $\det C=0$ (by Exercise \ref{exe.ps4.6} \textbf{(e)}, applied to $C$
instead of $A$).

On the other hand, let $C^{\prime}$ be the matrix obtained from $C$ by
multiplying the $k$-th row by $\lambda$. Then, $\det C^{\prime}=\lambda\det C$
(by Exercise \ref{exe.ps4.6} \textbf{(g)}, applied to $C$ and $C^{\prime}$
instead of $A$ and $B$). Thus, $\det C^{\prime}=\lambda\underbrace{\det
C}_{=0}=0$.

Recall that $C^{\prime}$ is the matrix obtained from $C$ by multiplying the
$k$-th row by $\lambda$. In other words,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }C^{\prime}\right)  =\left(
\text{the }u\text{-th row of }C\right)  \right. \label{sol.ps4.6k.lem.3'}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
while%
\begin{equation}
\left(  \text{the }k\text{-th row of }C^{\prime}\right)  =\lambda\left(
\text{the }k\text{-th row of }C\right)  . \label{sol.ps4.6k.lem.4'}%
\end{equation}


On the other hand, for every $u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th row of }A\right)  =\left(  \text{the }u\text{-th
row of }C^{\prime}\right)  \label{sol.ps4.6k.lem.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6k.lem.5}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $. We must prove (\ref{sol.ps4.6k.lem.5}).
\par
If $u\neq k$, then (\ref{sol.ps4.6k.lem.5}) follows from%
\begin{align*}
\left(  \text{the }u\text{-th row of }A\right)   &  =\left(  \text{the
}u\text{-th row of }C\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.lem.3})}\right) \\
&  =\left(  \text{the }u\text{-th row of }C^{\prime}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.lem.3'})}\right)  .
\end{align*}
Hence, if $u\neq k$, then (\ref{sol.ps4.6k.lem.5}) is proven. Therefore, for
the rest of this proof of (\ref{sol.ps4.6k.lem.5}), we can WLOG assume that we
don't have $u\neq k$. Assume this.
\par
We have $u=k$ (since we don't have $u\neq k$). Hence,%
\begin{align*}
\left(  \text{the }\underbrace{u}_{=k}\text{-th row of }A\right)   &  =\left(
\text{the }k\text{-th row of }A\right)  =\lambda\underbrace{w}%
_{\substack{=\left(  \text{the }k\text{-th row of }C\right)  \\\text{(by
(\ref{sol.ps4.6k.lem.4}))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.lem.2})}\right) \\
&  =\lambda\left(  \text{the }k\text{-th row of }C\right)  =\left(  \text{the
}\underbrace{k}_{=u}\text{-th row of }C^{\prime}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.lem.4'})}\right) \\
&  =\left(  \text{the }u\text{-th row of }C^{\prime}\right)  .
\end{align*}
Hence, (\ref{sol.ps4.6k.lem.5}) is proven.}. In other words, every row of $A$
equals the corresponding row of $C^{\prime}$. Thus, the matrices $A$ and
$C^{\prime}$ must be equal. In other words, $A=C^{\prime}$, so that $\det
A=\det C^{\prime}=0$. This proves Lemma \ref{lem.sol.ps4.6k.lem.1}.
\end{verlong}
\end{proof}

Now, let us come to the actual solution of Exercise \ref{exe.ps4.6k}.

\begin{proof}
[Solution to Exercise \ref{exe.ps4.6k}.]Let us write the $n\times n$-matrix
$A$ in the form \newline$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$.

\textbf{(a)} We need to prove that if we add a scalar multiple of a row of $A$
to another row of $A$, then the determinant of $A$ does not change. In other
words, we need to prove that if $B$ is an $n\times n$-matrix obtained from $A$
by adding a scalar multiple of a row of $A$ to another row of $A$, then $\det
B=\det A$.

So let $B$ be an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a row of $A$ to another row of $A$. We then need to show that
$\det B=\det A$.

We know that $B$ is an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a row of $A$ to another row of $A$. In other words, there exist
two distinct elements $\ell$ and $k$ of $\left\{  1,2,\ldots,n\right\}  $ such
that $B$ is the $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of the $\ell$-th row of $A$ to the $k$-th row of $A$. Consider these
$\ell$ and $k$.

\begin{vershort}
We know that $B$ is the $n\times n$-matrix obtained from $A$ by adding a
scalar multiple of the $\ell$-th row of $A$ to the $k$-th row of $A$. In other
words, there exists a $\lambda\in\mathbb{K}$ such that $B$ is $n\times
n$-matrix obtained from $A$ by adding $\lambda\cdot\left(  \text{the }%
\ell\text{-th row of }A\right)  $ to the $k$-th row of $A$. Consider this
$\lambda$.

Let $A^{\prime}$ be the $n\times n$-matrix obtained from $A$ by replacing the
$k$-th row of $A$ by $\lambda\cdot\left(  \text{the }\ell\text{-th row of
}A\right)  $. Then,%
\begin{align}
\left(  \text{the }k\text{-th row of }A^{\prime}\right)   &  =\lambda
\cdot\underbrace{\left(  \text{the }\ell\text{-th row of }A\right)
}_{\substack{=\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)
\\\text{(since the }\ell\text{-th row of }A^{\prime}\text{ has been}%
\\\text{taken over from }A\text{ unchanged)}}}\label{sol.ps4.6k.a.short.1}\\
&  =\lambda\cdot\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)
.\nonumber
\end{align}
Thus, the $k$-th row of $A^{\prime}$ equals a scalar multiple of the $\ell$-th
row of $A^{\prime}$. Thus, some row of the matrix $A^{\prime}$ equals a scalar
multiple of some other row of $A^{\prime}$. Hence, (\ref{sol.ps4.6k.lem.1})
(applied to $A^{\prime}$ instead of $A$) shows that $\det A^{\prime}=0$.

On the other hand, $A^{\prime}$ is an $n\times n$-matrix whose rows equal the
corresponding rows of $A$ except (perhaps) the $k$-th row (because of the
construction of $A^{\prime}$). Also, $B$ is the $n\times n$-matrix obtained
from $A$ by adding $\lambda\cdot\left(  \text{the }\ell\text{-th row of
}A\right)  $ to the $k$-th row of $A$. Because of (\ref{sol.ps4.6k.a.short.1}%
), this can be rewritten as follows: $B$ is the $n\times n$-matrix obtained
from $A$ by adding the $k$-th row of $A^{\prime}$ to the $k$-th row of $A$.
Exercise \ref{exe.ps4.6} \textbf{(i)} thus yields $\det B=\det
A+\underbrace{\det A^{\prime}}_{=0}=\det A$.
\end{vershort}

\begin{verlong}
We have $\ell\neq k$ (since $\ell$ and $k$ are distinct).

We know that $B$ is the $n\times n$-matrix obtained from $A$ by adding a
scalar multiple of the $\ell$-th row of $A$ to the $k$-th row of $A$. In other
words, we have%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B\right)  =\left(  \text{the
}u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.a.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
while the $k$-th row of $B$ is the sum of the $k$-th row of $A$ with a scalar
multiple of the $\ell$-th row of $A$.

We know that the $k$-th row of $B$ is the sum of the $k$-th row of $A$ with a
scalar multiple of the $\ell$-th row of $A$. In other words,%
\begin{equation}
\left(  \text{the }k\text{-th row of }B\right)  =\left(  \text{the }k\text{-th
row of }A\right)  +w, \label{sol.ps4.6k.a.2}%
\end{equation}
where $w$ is some scalar multiple of the $\ell$-th row of $A$. Consider this
$w$. There exists a $\lambda\in\mathbb{K}$ such that
\begin{equation}
w=\lambda\left(  \text{the }\ell\text{-th row of }A\right)
\label{sol.ps4.6k.a.3}%
\end{equation}
(since $w$ is some scalar multiple of the $\ell$-th row of $A$). Consider this
$\lambda$.

Let $A^{\prime}$ be the $n\times n$-matrix obtained from $A$ by replacing the
$k$-th row of $A$ by the row vector $w$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }A^{\prime}\right)  =\left(
\text{the }u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.a.5}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }k\text{-th row of }A^{\prime}\right)  =w.
\label{sol.ps4.6k.a.6}%
\end{equation}
Then, the $k$-th row of the matrix $A^{\prime}$ equals a scalar multiple of
the $\ell$-th row of $A^{\prime}$\ \ \ \ \footnote{\textit{Proof.} We have
$\ell\neq k$. Thus, (\ref{sol.ps4.6k.a.5}) (applied to $u=\ell$) yields
$\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)  =\left(
\text{the }\ell\text{-th row of }A\right)  $. Hence, $\lambda
\underbrace{\left(  \text{the }\ell\text{-th row of }A^{\prime}\right)
}_{=\left(  \text{the }\ell\text{-th row of }A\right)  }=\lambda\left(
\text{the }\ell\text{-th row of }A\right)  =w$ (by (\ref{sol.ps4.6k.a.3})).
Compared with (\ref{sol.ps4.6k.a.6}), this yields $\left(  \text{the
}k\text{-th row of }A^{\prime}\right)  =\lambda\left(  \text{the }%
\ell\text{-th row of }A^{\prime}\right)  $. Hence, the $k$-th row of the
matrix $A^{\prime}$ equals a scalar multiple of the $\ell$-th row of
$A^{\prime}$. Qed.}. Therefore, some row of the matrix $A^{\prime}$ equals a
scalar multiple of some other row of $A^{\prime}$. Hence,
(\ref{sol.ps4.6k.lem.1}) (applied to $A^{\prime}$ instead of $A$) yields $\det
A^{\prime}=0$.

On the other hand, $A^{\prime}$ is an $n\times n$-matrix whose rows equal the
corresponding rows of $A$ except (perhaps) the $k$-th row\footnote{In fact,
(\ref{sol.ps4.6k.a.5}) says precisely that the rows of $A^{\prime}$ equal the
corresponding rows of $A$ except (perhaps) the $k$-th row.}.

Now, let $B^{\prime}$ be the $n\times n$-matrix obtained from $A$ by adding
the $k$-th row of $A^{\prime}$ to the $k$-th row of $A$. Thus,%
\begin{align}
&  \left(  \left(  \text{the }u\text{-th row of }B^{\prime}\right)  =\left(
\text{the }u\text{-th row of }A\right)  \right. \label{sol.ps4.6k.a.11}\\
&  \ \ \ \ \ \ \ \ \ \ \left.  \text{for all }u\in\left\{  1,2,\ldots
,n\right\}  \text{ satisfying }u\neq k\right)  ,\nonumber
\end{align}
whereas%
\begin{equation}
\left(  \text{the }k\text{-th row of }B^{\prime}\right)  =\left(  \text{the
}k\text{-th row of }A\right)  +\left(  \text{the }k\text{-th row of }%
A^{\prime}\right)  . \label{sol.ps4.6k.a.12}%
\end{equation}


Exercise \ref{exe.ps4.6} \textbf{(i)} (applied to $B^{\prime}$ instead of $B$)
yields $\det B^{\prime}=\det A+\underbrace{\det A^{\prime}}_{=0}=\det A$.

Now, for every $u\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }u\text{-th row of }B^{\prime}\right)  =\left(  \text{the
}u\text{-th row of }B\right)  \label{sol.ps4.6k.a.13}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.6k.a.13}):} Let $u\in\left\{
1,2,\ldots,n\right\}  $. We must prove (\ref{sol.ps4.6k.a.13}).
\par
If $u\neq k$, then (\ref{sol.ps4.6k.a.13}) follows from%
\begin{align*}
\left(  \text{the }u\text{-th row of }B^{\prime}\right)   &  =\left(
\text{the }u\text{-th row of }A\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.a.11})}\right) \\
&  =\left(  \text{the }u\text{-th row of }B\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.6k.a.1})}\right)  .
\end{align*}
Hence, if $u\neq k$, then (\ref{sol.ps4.6k.a.13}) is proven. Therefore, for
the rest of this proof of (\ref{sol.ps4.6k.a.13}), we can WLOG assume that we
don't have $u\neq k$. Assume this.
\par
We have $u=k$ (since we don't have $u\neq k$). Hence,%
\begin{align*}
\left(  \text{the }\underbrace{u}_{=k}\text{-th row of }B^{\prime}\right)   &
=\left(  \text{the }k\text{-th row of }B^{\prime}\right) \\
&  =\left(  \text{the }k\text{-th row of }A\right)  +\underbrace{\left(
\text{the }k\text{-th row of }A^{\prime}\right)  }_{\substack{=w\\\text{(by
(\ref{sol.ps4.6k.a.6}))}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.ps4.6k.lem.2})}\right) \\
&  =\left(  \text{the }k\text{-th row of }A\right)  +w=\left(  \text{the
}\underbrace{k}_{=u}\text{-th row of }B\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{sol.ps4.6k.a.2})}\right) \\
&  =\left(  \text{the }u\text{-th row of }B\right)  .
\end{align*}
Hence, (\ref{sol.ps4.6k.a.13}) is proven.}. In other words, every row of
$B^{\prime}$ equals the corresponding row of $B$. Thus, the matrices
$B^{\prime}$ and $B$ must be equal. In other words, $B^{\prime}=B$, so that
$\det B^{\prime}=\det B$.

Compared with $\det B^{\prime}=\det A$, this yields $\det B=\det A$.
\end{verlong}

Thus, we have shown that%
\begin{equation}
\det B=\det A. \label{sol.ps4.6k.a.result}%
\end{equation}
This completes our solution to Exercise \ref{exe.ps4.6k} \textbf{(a)}.

\textbf{(b)} We need to prove that if we add a scalar multiple of a column of
$A$ to another column of $A$, then the determinant of $A$ does not change. In
other words, we need to prove that if $B$ is an $n\times n$-matrix obtained
from $A$ by adding a scalar multiple of a column of $A$ to another column of
$A$, then $\det B=\det A$.

So let $B$ be an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a column of $A$ to another column of $A$. We then need to show
that $\det B=\det A$.

We know that $B$ is an $n\times n$-matrix obtained from $A$ by adding a scalar
multiple of a column of $A$ to another column of $A$. Therefore, $B^{T}$ is an
$n\times n$-matrix obtained from $A^{T}$ by adding a scalar multiple of a row
of $A^{T}$ to another row of $A^{T}$ (because the columns of $A$ correspond to
the rows of $A^{T}$\ \ \ \ \footnote{See Remark \ref{rmk.colsA=rowsATT.rmk}
for the meaning of \textquotedblleft correspond\textquotedblright\ we are
using here.}). Therefore, (\ref{sol.ps4.6k.a.result}) (applied to $A^{T}$ and
$B^{T}$ instead of $A$ and $B$) yields $\det\left(  B^{T}\right)  =\det\left(
A^{T}\right)  $.

But Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Also,
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) yields $\det\left(
B^{T}\right)  =\det B$. But recall that $\det\left(  B^{T}\right)
=\det\left(  A^{T}\right)  $. This rewrites as $\det B=\det A$ (since
$\det\left(  B^{T}\right)  =\det B$ and $\det\left(  A^{T}\right)  =\det A$).
This solves Exercise \ref{exe.ps4.6k} \textbf{(b)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.prodrule}}

Our goal is now to prove Lemma \ref{lem.prodrule}. Actually, we will prove a
more general result:

\begin{lemma}
\label{lem.prodrule.S}Let $n\in\mathbb{N}$. For every $i\in\left\{
1,2,\ldots,n\right\}  $, let $Z_{i}$ be a finite set. For every $i\in\left\{
1,2,\ldots,n\right\}  $ and every $k\in Z_{i}$, let $p_{i,k}$ be an element of
$\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k\in Z_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{n}}\prod_{i=1}%
^{n}p_{i,k_{i}}.
\]

\end{lemma}

Let us first show the particular case of this lemma for $n=2$:

\begin{lemma}
\label{lem.prodrule.S.n=2}Let $X$ and $Y$ be two finite sets. For every $x\in
X$, let $q_{x}$ be an element of $\mathbb{K}$. For every $y\in Y$, let $r_{y}$
be an element of $\mathbb{K}$. Then,%
\[
\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}r_{y}\right)
=\sum_{\left(  x,y\right)  \in X\times Y}q_{x}r_{y}.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.prodrule.S.n=2}.]We have%
\[
\underbrace{\sum_{\left(  x,y\right)  \in X\times Y}}_{=\sum_{x\in X}%
\sum_{y\in Y}}q_{x}r_{y}=\sum_{x\in X}\underbrace{\sum_{y\in Y}q_{x}r_{y}%
}_{=q_{x}\sum_{y\in Y}r_{y}}=\sum_{x\in X}\left(  q_{x}\sum_{y\in Y}%
r_{y}\right)  =\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}%
r_{y}\right)  .
\]
This proves Lemma \ref{lem.prodrule.S.n=2}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.prodrule.S.n=2}.]The equalities (\ref{eq.sum.linear2}%
) and (\ref{eq.sum.fubini}) (and most other properties of finite sums) remain
valid if $\mathbb{A}$ is replaced by $\mathbb{K}$ throughout them. It is in
this variant that they will be used in the following proof.

For every $\lambda\in\mathbb{K}$, we have%
\begin{align}
\sum_{y\in Y}\lambda r_{y}  &  =\sum_{s\in Y}\lambda r_{s}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}y\text{ as }s\right) \nonumber\\
&  =\lambda\sum_{s\in Y}r_{s}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.sum.linear2}) (applied to }Y\text{, }r_{s}\text{ and
}\mathbb{K}\\
\text{instead of }S\text{, }a_{s}\text{ and }\mathbb{A}\text{)}%
\end{array}
\right) \nonumber\\
&  =\lambda\sum_{y\in Y}r_{y}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
renamed the summation index }s\text{ as }y\right) \nonumber\\
&  =\left(  \sum_{y\in Y}r_{y}\right)  \lambda.
\label{pf.lem.prodrule.S.n=2.1}%
\end{align}
For every $\lambda\in\mathbb{K}$, we have%
\begin{align}
\sum_{x\in X}\lambda q_{x}  &  =\sum_{s\in X}\lambda q_{s}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}x\text{ as }s\right) \nonumber\\
&  =\lambda\sum_{s\in X}q_{s}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.sum.linear2}) (applied to }X\text{, }q_{s}\text{ and
}\mathbb{K}\\
\text{instead of }S\text{, }a_{s}\text{ and }\mathbb{A}\text{)}%
\end{array}
\right) \nonumber\\
&  =\lambda\sum_{x\in X}q_{x}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
renamed the summation index }s\text{ as }x\right) \nonumber\\
&  =\left(  \sum_{x\in X}q_{x}\right)  \lambda.
\label{pf.lem.prodrule.S.n=2.2}%
\end{align}


From (\ref{eq.sum.fubini}) (applied to $\mathbb{K}$ and $q_{x}r_{y}$ instead
of $\mathbb{A}$ and $a_{\left(  x,y\right)  }$), we obtain%
\[
\sum_{x\in X}\sum_{y\in Y}q_{x}r_{y}=\sum_{\left(  x,y\right)  \in X\times
Y}q_{x}r_{y}=\sum_{y\in Y}\sum_{x\in X}q_{x}r_{y}.
\]
Hence,%
\begin{align*}
\sum_{\left(  x,y\right)  \in X\times Y}q_{x}r_{y}  &  =\sum_{x\in
X}\underbrace{\sum_{y\in Y}q_{x}r_{y}}_{\substack{=\left(  \sum_{y\in Y}%
r_{y}\right)  q_{x}\\\text{(by (\ref{pf.lem.prodrule.S.n=2.1}) (applied to
}\lambda=q_{x}\text{))}}}=\sum_{x\in X}\left(  \sum_{y\in Y}r_{y}\right)
q_{x}\\
&  =\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}r_{y}\right)
\end{align*}
(by (\ref{pf.lem.prodrule.S.n=2.2}) (applied to $\lambda=\sum_{y\in Y}r_{y}%
$)). This proves Lemma \ref{lem.prodrule.S.n=2}.
\end{proof}
\end{verlong}

\begin{proof}
[Proof of Lemma \ref{lem.prodrule.S}.]We claim that%
\begin{equation}
\prod_{i=1}^{m}\sum_{k\in Z_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{m}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{m}}\prod_{i=1}%
^{m}p_{i,k_{i}}. \label{pf.lem.prodrule.S.claim}%
\end{equation}
for every $m\in\left\{  0,1,\ldots,n\right\}  $.

\textit{Proof of (\ref{pf.lem.prodrule.S.claim}):} We shall prove
(\ref{pf.lem.prodrule.S.claim}) by induction over $m$:

\textit{Induction base:} Comparing
\[
\prod_{i=1}^{0}\sum_{k\in Z_{i}}p_{i,k}=\left(  \text{empty product}\right)
=1
\]
with%
\begin{align*}
\sum_{\left(  k_{1},k_{2},\ldots,k_{0}\right)  \in Z_{1}\times Z_{2}%
\times\cdots\times Z_{0}}\underbrace{\prod_{i=1}^{0}p_{i,k_{i}}}_{=\left(
\text{empty product}\right)  =1}  &  =\sum_{\left(  k_{1},k_{2},\ldots
,k_{0}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{0}}1\\
&  =\underbrace{\left\vert Z_{1}\times Z_{2}\times\cdots\times Z_{0}%
\right\vert }_{\substack{=1\\\text{(since }Z_{1}\times Z_{2}\times\cdots\times
Z_{0}\text{ is an}\\\text{empty Cartesian product)}}}\cdot1=1,
\end{align*}
we obtain $\prod_{i=1}^{0}\sum_{k\in Z_{i}}p_{i,k}=\sum_{\left(  k_{1}%
,k_{2},\ldots,k_{0}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{0}%
}\prod_{i=1}^{0}p_{i,k_{i}}$. In other words, (\ref{pf.lem.prodrule.S.claim})
holds for $m=0$. This completes the induction base.

\textit{Induction step:} Let $M\in\left\{  0,1,\ldots,n\right\}  $ be
positive. Assume that (\ref{pf.lem.prodrule.S.claim}) holds for $m=M-1$. We
now must show that (\ref{pf.lem.prodrule.S.claim}) holds for $m=M$.

We have assumed that (\ref{pf.lem.prodrule.S.claim}) holds for $m=M-1$. In
other words, we have%
\begin{equation}
\prod_{i=1}^{M-1}\sum_{k\in Z_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2}%
,\ldots,k_{M-1}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}}%
\prod_{i=1}^{M-1}p_{i,k_{i}}. \label{pf.lem.prodrule.S.indhyp}%
\end{equation}


Lemma \ref{lem.prodrule.prod-assM} shows that the map
\begin{align*}
Z_{1}\times Z_{2}\times\cdots\times Z_{M}  &  \rightarrow\left(  Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right)
\end{align*}
is a bijection.

For every $\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}$, we define an element \newline$g_{\left(
k_{1},k_{2},\ldots,k_{M-1}\right)  }\in\mathbb{K}$ by
\begin{equation}
g_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  }=\prod_{i=1}^{M-1}p_{i,k_{i}}.
\label{pf.lem.prodrule.S.g=}%
\end{equation}


Now, (\ref{pf.lem.prodrule.S.indhyp}) becomes%
\begin{align}
\prod_{i=1}^{M-1}\sum_{k\in Z_{i}}p_{i,k}  &  =\sum_{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}%
}\underbrace{\prod_{i=1}^{M-1}p_{i,k_{i}}}_{\substack{=g_{\left(  k_{1}%
,k_{2},\ldots,k_{M-1}\right)  }\\\text{(by (\ref{pf.lem.prodrule.S.g=}))}%
}}\nonumber\\
&  =\sum_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)  \in Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}}g_{\left(  k_{1},k_{2},\ldots,k_{M-1}\right)
}\nonumber\\
&  =\sum_{x\in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}}g_{x}
\label{pf.lem.prodrule.S.L1}%
\end{align}
(here, we have renamed the summation index $\left(  k_{1},k_{2},\ldots
,k_{M-1}\right)  $ as $x$).

The sets $Z_{1},Z_{2},\ldots,Z_{M-1}$ are finite. Hence, their Cartesian
product $Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}$ is also finite. Every
$\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in Z_{1}\times Z_{2}\times
\cdots\times Z_{M}$ satisfies%
\begin{equation}
\prod_{i=1}^{M}p_{i,s_{i}}=\left(  \prod_{i=1}^{M-1}p_{i,s_{i}}\right)
p_{M,s_{M}} \label{pf.lem.prodrule.S.splioff}%
\end{equation}
(indeed, this follows by splitting off the factor for $i=M$ from the product
$\prod_{i=1}^{M}p_{i,s_{i}}$).

Now, if we split off the factor for $i=M$ from the product $\prod_{i=1}%
^{M}\sum_{k\in Z_{i}}p_{i,k}$, we obtain
\begin{align*}
\prod_{i=1}^{M}\sum_{k\in Z_{i}}p_{i,k}  &  =\underbrace{\left(  \prod
_{i=1}^{M-1}\sum_{k\in Z_{i}}p_{i,k}\right)  }_{\substack{=\sum_{x\in
Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}}g_{x}\\\text{(by
(\ref{pf.lem.prodrule.S.L1}))}}}\underbrace{\left(  \sum_{k\in Z_{M}}%
p_{M,k}\right)  }_{\substack{=\sum_{y\in Z_{M}}p_{M,y}\\\text{(here, we
renamed the}\\\text{summation index }k\text{ as }y\text{)}}}\\
&  =\left(  \sum_{x\in Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}}%
g_{x}\right)  \left(  \sum_{y\in Z_{M}}p_{M,y}\right)  =\sum_{\left(
x,y\right)  \in\left(  Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\right)
\times Z_{M}}g_{x}p_{M,y}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Lemma \ref{lem.prodrule.S.n=2}, applied to}\\
X=Z_{1}\times Z_{2}\times\cdots\times Z_{M-1}\text{, }Y=Z_{M}\text{, }%
q_{x}=g_{x}\text{ and }r_{y}=p_{M,y}%
\end{array}
\right) \\
&  =\sum_{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in Z_{1}\times Z_{2}%
\times\cdots\times Z_{M}}\underbrace{g_{\left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  }}_{\substack{=\prod_{i=1}^{M-1}p_{i,s_{i}}\\\text{(by the
definition of }g_{\left(  s_{1},s_{2},\ldots,s_{M-1}\right)  }\text{)}%
}}p_{M,s_{M}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  \left(  s_{1},s_{2},\ldots
,s_{M-1}\right)  ,s_{M}\right)  \text{ for }\left(  x,y\right) \\
\text{in the sum, since the map}\\
Z_{1}\times Z_{2}\times\cdots\times Z_{M}\rightarrow\left(  Z_{1}\times
Z_{2}\times\cdots\times Z_{M-1}\right)  \times Z_{M},\\
\left(  s_{1},s_{2},\ldots,s_{M}\right)  \mapsto\left(  \left(  s_{1}%
,s_{2},\ldots,s_{M-1}\right)  ,s_{M}\right) \\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in Z_{1}\times Z_{2}%
\times\cdots\times Z_{M}}\underbrace{\left(  \prod_{i=1}^{M-1}p_{i,s_{i}%
}\right)  p_{M,s_{M}}}_{\substack{=\prod_{i=1}^{M}p_{i,s_{i}}\\\text{(by
(\ref{pf.lem.prodrule.S.splioff}))}}}\\
&  =\sum_{\left(  s_{1},s_{2},\ldots,s_{M}\right)  \in Z_{1}\times Z_{2}%
\times\cdots\times Z_{M}}\prod_{i=1}^{M}p_{i,s_{i}}=\sum_{\left(  k_{1}%
,k_{2},\ldots,k_{M}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{M}%
}\prod_{i=1}^{M}p_{i,k_{i}}%
\end{align*}
(here, we have renamed the summation index $\left(  s_{1},s_{2},\ldots
,s_{M}\right)  $ as $\left(  k_{1},k_{2},\ldots,k_{M}\right)  $). In other
words, (\ref{pf.lem.prodrule.S.claim}) holds for $m=M$. This completes the
induction step. Thus, (\ref{pf.lem.prodrule.S.claim}) is proven by induction.

Now, (\ref{pf.lem.prodrule.S.claim}) (applied to $m=n$) yields%
\[
\prod_{i=1}^{n}\sum_{k\in Z_{i}}p_{i,k}=\sum_{\left(  k_{1},k_{2},\ldots
,k_{n}\right)  \in Z_{1}\times Z_{2}\times\cdots\times Z_{n}}\prod_{i=1}%
^{n}p_{i,k_{i}}.
\]
This proves Lemma \ref{lem.prodrule.S}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.prodrule}.]For every $i\in\left[  n\right]  $, the
set $\left[  m_{i}\right]  $ is clearly a finite set. For every $i\in\left[
n\right]  $, we know that $p_{i,1},p_{i,2},\ldots,p_{i,m_{i}}$ are elements of
$\mathbb{K}$. In other words, for every $i\in\left[  n\right]  $ and every
$k\in\left[  m_{i}\right]  $, we know that $p_{i,k}$ is an element of
$\mathbb{K}$. In other words, for every $i\in\left\{  1,2,\ldots,n\right\}  $
and every $k\in\left[  m_{i}\right]  $, we know that $p_{i,k}$ is an element
of $\mathbb{K}$ (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).
Hence, Lemma \ref{lem.prodrule.S} (applied to $Z_{i}=\left[  m_{i}\right]  $)
yields%
\[
\prod_{i=1}^{n}\sum_{k\in\left[  m_{i}\right]  }p_{i,k}=\sum_{\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]  \times\left[
m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  }\prod_{i=1}%
^{n}p_{i,k_{i}}.
\]
Thus,%
\[
\sum_{\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m_{1}\right]
\times\left[  m_{2}\right]  \times\cdots\times\left[  m_{n}\right]  }%
\prod_{i=1}^{n}p_{i,k_{i}}=\prod_{i=1}^{n}\underbrace{\sum_{k\in\left[
m_{i}\right]  }}_{=\sum_{k=1}^{m_{i}}}p_{i,k}=\prod_{i=1}^{n}\sum_{k=1}%
^{m_{i}}p_{i,k}.
\]
This proves Lemma \ref{lem.prodrule}.
\end{proof}

We have now proven Lemma \ref{lem.prodrule}, and thus solved Exercise
\ref{exe.prodrule}.

\subsection{Solution to Exercise \ref{exe.ps4.det.fibo}}

We shall solve Exercise \ref{exe.ps4.det.fibo} more or less as you would
expect, by modifying our proof of (\ref{eq.exam.det(AB).fibo.1}) in some places.

\begin{proof}
[Solution to Exercise \ref{exe.ps4.det.fibo}.]Let $B$ be the $2\times2$-matrix
$\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right)  $. Thus,%
\[
\det B=\det\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right)  =a\cdot0-1\cdot b=-b.
\]


Let $A$ be the $2\times2$-matrix $\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  $. Then, $\det A=\det\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  =x_{k+2}x_{0}-x_{k+1}x_{1}$.

We now claim that%
\begin{equation}
AB^{m}=\left(
\begin{array}
[c]{cc}%
x_{m+k+2} & x_{m+k+1}\\
x_{m+1} & x_{m}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }m\in\mathbb{N}.
\label{sol.ps4.det.fibo.BmC}%
\end{equation}


\textit{Proof of (\ref{sol.ps4.det.fibo.BmC}):} We shall prove
(\ref{sol.ps4.det.fibo.BmC}) by induction over $m$:

\textit{Induction base:} We have $A\underbrace{B^{0}}_{=I_{2}}=AI_{2}%
=A=\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  $. Compared with $\left(
\begin{array}
[c]{cc}%
x_{0+k+2} & x_{0+k+1}\\
x_{0+1} & x_{0}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
x_{k+2} & x_{k+1}\\
x_{1} & x_{0}%
\end{array}
\right)  $, this yields $AB^{0}=\left(
\begin{array}
[c]{cc}%
x_{0+k+2} & x_{0+k+1}\\
x_{0+1} & x_{0}%
\end{array}
\right)  $. In other words, (\ref{sol.ps4.det.fibo.BmC}) holds for $m=0$. This
completes the induction base.

\textit{Induction step:} Let $M$ be a positive integer. Assume that
(\ref{sol.ps4.det.fibo.BmC}) holds for $m=M-1$. We need to show that
(\ref{sol.ps4.det.fibo.BmC}) holds for $m=M$.

We have assumed that (\ref{sol.ps4.det.fibo.BmC}) holds for $m=M-1$. In other
words,%
\[
AB^{M-1}=\left(
\begin{array}
[c]{cc}%
x_{\left(  M-1\right)  +k+2} & x_{\left(  M-1\right)  +k+1}\\
x_{\left(  M-1\right)  +1} & x_{M-1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
x_{M+k+1} & x_{M+k}\\
x_{M} & x_{M-1}%
\end{array}
\right)  .
\]
Now,%
\begin{align}
A\underbrace{B^{M}}_{=B^{M-1}\cdot B}  &  =\underbrace{AB^{M-1}}_{=\left(
\begin{array}
[c]{cc}%
x_{M+k+1} & x_{M+k}\\
x_{M} & x_{M-1}%
\end{array}
\right)  }\cdot\underbrace{B}_{=\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right)  }\nonumber\\
&  =\left(
\begin{array}
[c]{cc}%
x_{M+k+1} & x_{M+k}\\
x_{M} & x_{M-1}%
\end{array}
\right)  \cdot\left(
\begin{array}
[c]{cc}%
a & 1\\
b & 0
\end{array}
\right) \nonumber\\
&  =\left(
\begin{array}
[c]{cc}%
x_{M+k+1}\cdot a+x_{M+k}\cdot b & x_{M+k+1}\cdot1+x_{M+k}\cdot0\\
x_{M}\cdot a+x_{M-1}\cdot b & x_{M}\cdot1+x_{M-1}\cdot0
\end{array}
\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of a product of two
matrices}\right) \nonumber\\
&  =\left(
\begin{array}
[c]{cc}%
ax_{M+k+1}+bx_{M+k} & x_{M+k+1}\\
ax_{M}+bx_{M-1} & x_{M}%
\end{array}
\right)  . \label{sol.ps4.det.fibo.BmC.4}%
\end{align}


But (\ref{eq.det.fibo.rec}) (applied to $n=M+k+2$) yields%
\[
x_{M+k+2}=a\underbrace{x_{\left(  M+k+2\right)  -1}}_{=x_{M+k+1}%
}+b\underbrace{x_{\left(  M+k+2\right)  -2}}_{=x_{M+k}}=ax_{M+k+1}+bx_{M+k}.
\]
Also, (\ref{eq.det.fibo.rec}) (applied to $n=M+1$) yields $x_{M+1}%
=a\underbrace{x_{\left(  M+1\right)  -1}}_{=x_{M}}+b\underbrace{x_{\left(
M+1\right)  -2}}_{=x_{M-1}}=ax_{M}+bx_{M-1}$. Now,%
\[
\left(
\begin{array}
[c]{cc}%
x_{M+k+2} & x_{M+k+1}\\
x_{M+1} & x_{M}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
ax_{M+k+1}+bx_{M+k} & x_{M+k+1}\\
ax_{M}+bx_{M-1} & x_{M}%
\end{array}
\right)
\]
(since $x_{M+k+2}=ax_{M+k+1}+bx_{M+k}$ and $x_{M+1}=ax_{M}+bx_{M-1}$).
Compared with (\ref{sol.ps4.det.fibo.BmC.4}), this yields $AB^{M}=\left(
\begin{array}
[c]{cc}%
x_{M+k+2} & x_{M+k+1}\\
x_{M+1} & x_{M}%
\end{array}
\right)  $. In other words, (\ref{sol.ps4.det.fibo.BmC}) holds for $m=M$. This
completes the induction step. Thus, (\ref{sol.ps4.det.fibo.BmC}) is proven by induction.

Now, let $n>k$ be an integer. Then, $n-k>0$, so that $n-k-1\in\mathbb{N}$.
Hence, (\ref{sol.ps4.det.fibo.BmC}) (applied to $m=n-k-1$) yields%
\[
AB^{n-k-1}=\left(
\begin{array}
[c]{cc}%
x_{\left(  n-k-1\right)  +k+2} & x_{\left(  n-k-1\right)  +k+1}\\
x_{\left(  n-k-1\right)  +1} & x_{n-k-1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
x_{n+1} & x_{n}\\
x_{n-k} & x_{n-k-1}%
\end{array}
\right)  .
\]
Taking determinants on both sides of this equality, we obtain%
\[
\det\left(  AB^{n-k-1}\right)  =\det\left(
\begin{array}
[c]{cc}%
x_{n+1} & x_{n}\\
x_{n-k} & x_{n-k-1}%
\end{array}
\right)  =x_{n+1}x_{n-k-1}-x_{n}x_{n-k}.
\]
Hence,%
\begin{align*}
&  x_{n+1}x_{n-k-1}-x_{n}x_{n-k}\\
&  =\det\left(  AB^{n-k-1}\right)  =\det A\cdot\underbrace{\det\left(
B^{n-k-1}\right)  }_{\substack{=\left(  \det B\right)  ^{n-k-1}\\\text{(by
Corollary \ref{cor.det.product} \textbf{(b)}, applied}\\\text{to }2\text{ and
}n-k-1\text{ instead of }n\text{ and }k\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}2\text{ and }B^{n-k-1}\text{ instead of }n\text{ and }B\right) \\
&  =\underbrace{\det A}_{=x_{k+2}x_{0}-x_{k+1}x_{1}}\cdot\left(
\underbrace{\det B}_{=-b}\right)  ^{n-k-1}=\left(  x_{k+2}x_{0}-x_{k+1}%
x_{1}\right)  \cdot\left(  -b\right)  ^{n-k-1}\\
&  =\left(  -b\right)  ^{n-k-1}\left(  x_{k+2}x_{0}-x_{k+1}x_{1}\right)  .
\end{align*}
This solves Exercise \ref{exe.ps4.det.fibo}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.ps4.pascal}}

\begin{proof}
[Solution to Exercise \ref{exe.ps4.pascal}.]We let $B$ be the $n\times
n$-matrix $\left(  \dbinom{i-1}{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
$. We have $\dbinom{i-1}{j-1}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.}
Let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ be such that
$i<j$. Then, $i-1\in\mathbb{N}$ (since $i\geq1$) and $j-1\in\mathbb{N}$ (since
$j\geq1$) and $i-1<j-1$ (since $i<j$). Hence, (\ref{eq.binom.0}) (applied to
$i-1$ and $j-1$ instead of $m$ and $n$) yields $\dbinom{i-1}{j-1}=0$, qed.}.
Therefore, Exercise \ref{exe.ps4.3} (applied to $B$ and $\dbinom{i-1}{j-1}$
instead of $A$ and $a_{i,j}$) yields
\[
\det B=\underbrace{\dbinom{1-1}{1-1}}_{=1}\underbrace{\dbinom{2-1}{2-1}}%
_{=1}\cdots\underbrace{\dbinom{n-1}{n-1}}_{=1}=1\cdot1\cdot\cdots\cdot1=1.
\]
Exercise \ref{exe.ps4.4} (applied to $B$ instead of $A$) shows that
$\det\left(  B^{T}\right)  =\det B=1$.

Now, we are going to show that $A=BB^{T}$.

Indeed, let us show that%
\begin{equation}
\sum_{k=1}^{n}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}=\dbinom{i+j-2}{i-1}
\label{sol.ps4.pascal.1}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

\textit{Proof of (\ref{sol.ps4.pascal.1}):} Let $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then, $i\geq1$, so that
$i-1\in\mathbb{N}$. Hence, we can apply Theorem \ref{thm.vandermonde} to
$n=i-1$. We thus obtain%
\[
\dbinom{X+Y}{i-1}=\sum_{k=0}^{i-1}\dbinom{X}{k}\dbinom{Y}{i-1-k}.
\]
Substituting $j-1$ and $i-1$ for $X$ and $Y$ in this polynomial identity, we
obtain%
\begin{equation}
\dbinom{\left(  j-1\right)  +\left(  i-1\right)  }{i-1}=\sum_{k=0}%
^{i-1}\dbinom{j-1}{k}\dbinom{i-1}{i-1-k}. \label{sol.ps4.pascal.2}%
\end{equation}
On the other hand, for every $k\in\left\{  i+1,i+2,\ldots,n\right\}  $, we
have%
\begin{equation}
\dbinom{i-1}{k-1}=0 \label{sol.ps4.pascal.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps4.pascal.4}):} Let $k\in\left\{
i+1,i+2,\ldots,n\right\}  $. Then, $k>i\geq1$ and thus $k-1\in\mathbb{N}$.
Also, $k>i$, so that $k-i>i-1$ and thus $i-1<k-1$. Hence, (\ref{eq.binom.0})
(applied to $i-1$ and $k-1$ instead of $m$ and $n$) yields $\dbinom{i-1}%
{k-1}=0$, qed.}. Now, $i\leq n$, so that%
\begin{align*}
&  \sum_{k=1}^{n}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}\\
&  =\sum_{k=1}^{i}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}+\sum_{k=i+1}%
^{n}\underbrace{\dbinom{i-1}{k-1}}_{\substack{=0\\\text{(by
(\ref{sol.ps4.pascal.4}))}}}\dbinom{j-1}{k-1}\\
&  =\sum_{k=1}^{i}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}+\underbrace{\sum
_{k=i+1}^{n}0\dbinom{j-1}{k-1}}_{=0}=\sum_{k=1}^{i}\dbinom{i-1}{k-1}%
\dbinom{j-1}{k-1}\\
&  =\sum_{k=0}^{i-1}\underbrace{\dbinom{i-1}{k}}_{\substack{=\dbinom
{i-1}{i-1-k}\\\text{(by (\ref{eq.binom.symm}), applied to }i-1\text{ and
}k\text{ instead of }m\text{ and }n\text{)}}}\dbinom{j-1}{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}k-1\text{ in the sum}\right) \\
&  =\sum_{k=0}^{i-1}\dbinom{i-1}{i-1-k}\dbinom{j-1}{k}=\sum_{k=0}^{i-1}%
\dbinom{j-1}{k}\dbinom{i-1}{i-1-k}\\
&  =\dbinom{\left(  j-1\right)  +\left(  i-1\right)  }{i-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.ps4.pascal.2})}\right) \\
&  =\dbinom{i+j-2}{i-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(
j-1\right)  +\left(  i-1\right)  =i+j-2\right)  .
\end{align*}
This proves (\ref{sol.ps4.pascal.1}).

Now, we have $B=\left(  \dbinom{i-1}{j-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ and therefore \newline$B^{T}=\left(  \dbinom{j-1}{i-1}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of $B^{T}$). Hence, the
definition of the product $BB^{T}$ shows that%
\[
BB^{T}=\left(  \underbrace{\sum_{k=1}^{n}\dbinom{i-1}{k-1}\dbinom{j-1}{k-1}%
}_{\substack{=\dbinom{i+j-2}{i-1}\\\text{(by (\ref{sol.ps4.pascal.1}))}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \dbinom{i+j-2}%
{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A.
\]
Hence, $A=BB^{T}$, so that%
\begin{align*}
\det A  &  =\det\left(  BB^{T}\right)  =\underbrace{\det B}_{=1}%
\cdot\underbrace{\det\left(  B^{T}\right)  }_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}B\text{ and }B^{T}\text{ instead of }A\text{ and }B\right) \\
&  =1.
\end{align*}
This solves Exercise \ref{exe.ps4.pascal}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.lem.increasing-sequences}}

\begin{proof}
[Proof of Lemma \ref{lem.increasing-sequences}.]\textbf{(b)} Let $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}$ be an
$n$-tuple satisfying $g_{1}<g_{2}<\cdots<g_{n}$. We shall derive a contradiction.

We have $m<n$, so that $n>m\geq0$. Thus, $n\geq1$ (since $n$ is an integer).
Therefore, the elements $g_{1}$ and $g_{n}$ of $\left\{  1,2,\ldots,m\right\}
$ are well-defined. Every $i\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies
$g_{i}-i\leq g_{i+1}-\left(  i+1\right)  $\ \ \ \ \footnote{\textit{Proof.}
Let $i\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $g_{i}<g_{i+1}$ (since
$g_{1}<g_{2}<\cdots<g_{n}$). Hence, $g_{i}\leq g_{i+1}-1$ (since $g_{i}$ and
$g_{i+1}$ are integers). Hence, $\underbrace{g_{i}}_{\leq g_{i+1}-1}-i\leq
g_{i+1}-1-i=g_{i+1}-\left(  i+1\right)  $, qed.}. In other words, we have
$g_{1}-1\leq g_{2}-2\leq\cdots\leq g_{n}-n$. Hence, $g_{1}-1\leq g_{n}-n$.

But $g_{n}\in\left\{  1,2,\ldots,m\right\}  $, so that $g_{n}\leq m<n$ and
thus $\underbrace{g_{n}}_{<n}-n<n-n=0$. Hence, $g_{1}-1\leq g_{n}-n<0$. On the
other hand, $g_{1}\in\left\{  1,2,\ldots,m\right\}  $, so that $g_{1}\geq1$
and thus $g_{1}-1\geq0$. This contradicts $g_{1}-1<0$.

Now, let us forget that we fixed $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $.
We thus have derived a contradiction for every $n$-tuple $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n}$. Hence, there exists no $n$-tuple $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,m\right\}  ^{n}$
satisfying $g_{1}<g_{2}<\cdots<g_{n}$. This proves Lemma
\ref{lem.increasing-sequences} \textbf{(b)}.

\textbf{(a)} The $n$-tuple $\left(  1,2,\ldots,n\right)  $ clearly belongs to
$\left\{  1,2,\ldots,n\right\}  ^{n}$, and satisfies $1<2<\cdots<n$. Hence,
there exists an $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$,
namely the $n$-tuple $\left(  1,2,\ldots,n\right)  $. We shall now show that
$\left(  1,2,\ldots,n\right)  $ is the only such $n$-tuple.

Indeed, let $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{
1,2,\ldots,n\right\}  ^{n}$ be an $n$-tuple satisfying $g_{1}<g_{2}%
<\cdots<g_{n}$. We shall prove that $\left(  g_{1},g_{2},\ldots,g_{n}\right)
=\left(  1,2,\ldots,n\right)  $.

Every $i\in\left\{  1,2,\ldots,n-1\right\}  $ satisfies $g_{i}-i\leq
g_{i+1}-\left(  i+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $g_{i}<g_{i+1}$ (since
$g_{1}<g_{2}<\cdots<g_{n}$). Hence, $g_{i}\leq g_{i+1}-1$ (since $g_{i}$ and
$g_{i+1}$ are integers). Hence, $\underbrace{g_{i}}_{\leq g_{i+1}-1}-i\leq
g_{i+1}-1-i=g_{i+1}-\left(  i+1\right)  $, qed.}. In other words, we have
$g_{1}-1\leq g_{2}-2\leq\cdots\leq g_{n}-n$. In other words,%
\begin{equation}
g_{u}-u\leq g_{v}-v \label{pf.lem.increasing-sequences.b.1}%
\end{equation}
for any two elements $u$ and $v$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $u\leq v$.

Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $1\leq i\leq n$, so that
$1\leq n$. Hence, the elements $g_{1}$ and $g_{n}$ of $\left\{  1,2,\ldots
,n\right\}  $ are well-defined.

Now, $1\leq i$. Hence, (\ref{pf.lem.increasing-sequences.b.1}) (applied to
$u=1$ and $v=i$) yields $g_{1}-1\leq g_{i}-i$. But $g_{1}\in\left\{
1,2,\ldots,n\right\}  $, so that $g_{1}\geq1$ and thus $g_{1}-1\geq0$. Hence,
$0\leq g_{1}-1\leq g_{i}-i$.

On the other hand, $i\leq n$. Therefore,
(\ref{pf.lem.increasing-sequences.b.1}) (applied to $u=i$ and $v=n$) yields
$g_{i}-i\leq g_{n}-n$. But $g_{n}\in\left\{  1,2,\ldots,n\right\}  $, so that
$g_{n}\leq n$ and thus $g_{n}-n\leq0$. Hence, $g_{i}-i\leq g_{n}-n\leq0$.
Combined with $0\leq g_{i}-i$, this yields $g_{i}-i=0$, so that $g_{i}=i$.

Now, let us forget that we fixed $i$. We thus have shown that $g_{i}=i$ for
every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words, $\left(
g_{1},g_{2},\ldots,g_{n}\right)  =\left(  1,2,\ldots,n\right)  $.

Let us now forget that we fixed $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $.
We thus have shown that if $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n}$ is an $n$-tuple satisfying
$g_{1}<g_{2}<\cdots<g_{n}$, then $\left(  g_{1},g_{2},\ldots,g_{n}\right)
=\left(  1,2,\ldots,n\right)  $. In other words, every $n$-tuple $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$
satisfying $g_{1}<g_{2}<\cdots<g_{n}$ must be equal to $\left(  1,2,\ldots
,n\right)  $. Hence, there exists at most one $n$-tuple $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n}$ (namely, the $n$-tuple $\left(  1,2,\ldots
,n\right)  $).

We now know the following two facts:

\begin{itemize}
\item There exists an $n$-tuple $\left(  g_{1},g_{2},\ldots,g_{n}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$,
namely the $n$-tuple $\left(  1,2,\ldots,n\right)  $.

\item There exists at most one $n$-tuple $\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n}$ satisfying $g_{1}%
<g_{2}<\cdots<g_{n}$.
\end{itemize}

Combining these two facts, we conclude that there exists exactly one $n$-tuple
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\left\{  1,2,\ldots,n\right\}
^{n}$ satisfying $g_{1}<g_{2}<\cdots<g_{n}$, namely the $n$-tuple $\left(
1,2,\ldots,n\right)  $. This proves Lemma \ref{lem.increasing-sequences}
\textbf{(a)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.sorting.basics}}

Before we give a formal proof of Proposition \ref{prop.sorting}, let us
outline the main ideas of this proof: We shall define the notion of
\textit{inversion} of an $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $
of integers\footnote{It will be defined as a pair $\left(  i,j\right)  $ of
integers satisfying $1\leq i<j\leq n$ and $a_{i}>a_{j}$. The analogy with the
notion of \textquotedblleft inversion\textquotedblright\ of a permutation is
intentional.}; then we will argue that switching two adjacent entries $a_{k}$
and $a_{k+1}$ of an $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $
which are \textquotedblleft out of order\textquotedblright\ (i.e., satisfy
$a_{k}>a_{k+1}$) reduces the number of inversions by $1$ (this is our equality
(\ref{pf.prop.sorting.a.fact5}) further below), and thus a sequence of such
switches will eventually end and therefore bring the tuple into weakly
increasing order. (This is similar to an argument in the solution of Exercise
\ref{exe.ps2.2.5} \textbf{(e)}.) Other proofs of Proposition
\ref{prop.sorting} \textbf{(a)} are possible\footnote{Roughly speaking, to
each \href{https://en.wikipedia.org/wiki/Sorting_algorithm}{sorting algorithm}
corresponds at least one proof of Proposition \ref{prop.sorting} \textbf{(a)}.
(\textquotedblleft At least\textquotedblright\ because there are often several
ways to prove the correctness of a given sorting algorithm.) The proof we just
outlined corresponds to \textquotedblleft bubble sort\textquotedblright.}.
Proposition \ref{prop.sorting} \textbf{(b)} will then easily follow (indeed,
we will argue that $a_{\sigma\left(  i\right)  }$ is the smallest integer $x$
such that at least $i$ different elements $j\in\left\{  1,2,\ldots,n\right\}
$ satisfy $a_{j}\leq x$), and Proposition \ref{prop.sorting} \textbf{(c)} will
finally follow from parts \textbf{(a)} and \textbf{(b)}.

Here is the proof in detail:

\begin{proof}
[Proof of Proposition \ref{prop.sorting}.]\textbf{(a)} Let us forget that we
fixed $a_{1},a_{2},\ldots,a_{n}$.

We shall first introduce some notations.

We let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.

If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an $n$-tuple of
integers, then:

\begin{itemize}
\item An \textit{inversion} of $\mathbf{a}$ will mean a pair $\left(
i,j\right)  \in\left[  n\right]  ^{2}$ satisfying $i<j$ and $a_{i}>a_{j}$.

\item We denote by $\operatorname*{Inv}\left(  \mathbf{a}\right)  $ the set of
all inversions of $\mathbf{a}$. Thus, $\operatorname*{Inv}\left(
\mathbf{a}\right)  \subseteq\left[  n\right]  ^{2}$. More precisely,%
\begin{align}
\operatorname*{Inv}\left(  \mathbf{a}\right)   &  =\left(  \text{the set of
all inversions of }\mathbf{a}\right) \nonumber\\
&  =\left\{  \left(  i,j\right)  \in\left[  n\right]  ^{2}\ \mid\ i<j\text{
and }a_{i}>a_{j}\right\} \label{pf.prop.sorting.a.Inv.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the inversions of }\mathbf{a}\text{ are the pairs }\left(
i,j\right)  \in\left[  n\right]  ^{2}\text{ satisfying}\\
i<j\text{ and }a_{i}>a_{j}\text{ (by the definition of an \textquotedblleft
inversion\textquotedblright)}%
\end{array}
\right) \nonumber\\
&  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{
and }a_{u}>a_{v}\right\} \label{pf.prop.sorting.a.Inv.2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the index }\left(
i,j\right)  \text{ as }\left(  u,v\right)  \right)  .\nonumber
\end{align}


\item We denote by $\ell\left(  \mathbf{a}\right)  $ the number $\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\right)  \right\vert $. (This is
well-defined because $\operatorname*{Inv}\left(  \mathbf{a}\right)  $ is
finite (since $\operatorname*{Inv}\left(  \mathbf{a}\right)  \subseteq\left[
n\right]  ^{2}$).) Thus,%
\begin{align}
\ell\left(  \mathbf{a}\right)   &  =\left\vert \underbrace{\operatorname*{Inv}%
\left(  \mathbf{a}\right)  }_{=\left(  \text{the set of all inversions of
}\mathbf{a}\right)  }\right\vert =\left\vert \left(  \text{the set of all
inversions of }\mathbf{a}\right)  \right\vert \nonumber\\
&  =\left(  \text{the number of all inversions of }\mathbf{a}\right)  .
\label{pf.prop.sorting.a.l(sigma)}%
\end{align}


\item For every permutation $\tau\in S_{n}$, we denote by $\mathbf{a}\circ
\tau$ the $n$-tuple \newline$\left(  a_{\tau\left(  1\right)  },a_{\tau\left(
2\right)  },\ldots,a_{\tau\left(  n\right)  }\right)  $ of integers.
\end{itemize}

We notice that if $\mathbf{a}$ is any $n$-tuple of integers, then%
\begin{equation}
\mathbf{a}\circ\left(  \sigma\circ\tau\right)  =\left(  \mathbf{a}\circ
\sigma\right)  \circ\tau\ \ \ \ \ \ \ \ \ \ \text{for any }\sigma\in
S_{n}\text{ and }\tau\in S_{n} \label{pf.prop.sorting.a.sigmatau}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.sigmatau}):} Let
$\mathbf{a}$ be any $n$-tuple of integers. Let $\sigma\in S_{n}$ and $\tau\in
S_{n}$. We must show that $\mathbf{a}\circ\left(  \sigma\circ\tau\right)
=\left(  \mathbf{a}\circ\sigma\right)  \circ\tau$.
\par
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
Thus, the definition of $\mathbf{a}\circ\left(  \sigma\circ\tau\right)  $
yields%
\[
\mathbf{a}\circ\left(  \sigma\circ\tau\right)  =\left(  a_{\left(  \sigma
\circ\tau\right)  \left(  1\right)  },a_{\left(  \sigma\circ\tau\right)
\left(  2\right)  },\ldots,a_{\left(  \sigma\circ\tau\right)  \left(
n\right)  }\right)  =\left(  a_{\sigma\left(  \tau\left(  1\right)  \right)
},a_{\sigma\left(  \tau\left(  2\right)  \right)  },\ldots,a_{\sigma\left(
\tau\left(  n\right)  \right)  }\right)
\]
(since $a_{\left(  \sigma\circ\tau\right)  \left(  i\right)  }=a_{\sigma
\left(  \tau\left(  i\right)  \right)  }$ for every $i\in\left\{
1,2,\ldots,n\right\}  $). On the other hand, the definition of $\mathbf{a}%
\circ\sigma$ yields $\mathbf{a}\circ\sigma=\left(  a_{\sigma\left(  1\right)
},a_{\sigma\left(  2\right)  },\ldots,a_{\sigma\left(  n\right)  }\right)  $
(since $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $). Therefore, the
definition of $\left(  \mathbf{a}\circ\sigma\right)  \circ\tau$ yields
$\left(  \mathbf{a}\circ\sigma\right)  \circ\tau=\left(  a_{\sigma\left(
\tau\left(  1\right)  \right)  },a_{\sigma\left(  \tau\left(  2\right)
\right)  },\ldots,a_{\sigma\left(  \tau\left(  n\right)  \right)  }\right)  $.
Compared with $\mathbf{a}\circ\left(  \sigma\circ\tau\right)  =\left(
a_{\sigma\left(  \tau\left(  1\right)  \right)  },a_{\sigma\left(  \tau\left(
2\right)  \right)  },\ldots,a_{\sigma\left(  \tau\left(  n\right)  \right)
}\right)  $, this yields $\mathbf{a}\circ\left(  \sigma\circ\tau\right)
=\left(  \mathbf{a}\circ\sigma\right)  \circ\tau$. This proves
(\ref{pf.prop.sorting.a.sigmatau}).}. Also, if $\mathbf{a}$ is any $n$-tuple
of integers, then%
\begin{equation}
\mathbf{a}\circ\operatorname*{id}=\mathbf{a} \label{pf.prop.sorting.a.id}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.id}):} Let $\mathbf{a}$ be
any $n$-tuple of integers.
\par
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
Thus, the definition of $\mathbf{a}\circ\operatorname*{id}$ yields%
\[
\mathbf{a}\circ\operatorname*{id}=\left(  a_{\operatorname*{id}\left(
1\right)  },a_{\operatorname*{id}\left(  2\right)  },\ldots
,a_{\operatorname*{id}\left(  n\right)  }\right)  =\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)
\]
(since $a_{\operatorname*{id}\left(  i\right)  }=a_{i}$ for every
$i\in\left\{  1,2,\ldots,n\right\}  $). Compared with $\mathbf{a}=\left(
a_{1},a_{2},\ldots,a_{n}\right)  $, this yields $\mathbf{a}\circ
\operatorname*{id}=\mathbf{a}$. This proves (\ref{pf.prop.sorting.a.id}).}.

If $X$, $X^{\prime}$, $Y$ and $Y^{\prime}$ are four sets and if $\alpha
:X\rightarrow X^{\prime}$ and $\beta:Y\rightarrow Y^{\prime}$ are two maps,
then $\alpha\times\beta$ will denote the map%
\begin{align*}
X\times Y  &  \rightarrow X^{\prime}\times Y^{\prime},\\
\left(  x,y\right)   &  \mapsto\left(  \alpha\left(  x\right)  ,\beta\left(
y\right)  \right)  .
\end{align*}


Recall that, for each $k\in\left\{  1,2,\ldots,n-1\right\}  $, we have defined
$s_{k}$ to be the permutation in $S_{n}$ that switches $k$ with $k+1$ but
leaves all other numbers unchanged. This permutation $s_{k}$ is a map $\left[
n\right]  \rightarrow\left[  n\right]  $ and satisfies $s_{k}^{2}%
=\operatorname*{id}$. For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, the
map $s_{k}\times s_{k}:\left[  n\right]  \times\left[  n\right]
\rightarrow\left[  n\right]  \times\left[  n\right]  $ satisfies $\left(
s_{k}\times s_{k}\right)  ^{2}=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\left[  n\right]  \times\left[
n\right]  $. Then, we can write $u$ in the form $\left(  i,j\right)  $ for
some $i\in\left[  n\right]  $ and $j\in\left[  n\right]  $. Consider these $i$
and $j$. We have%
\begin{align*}
\underbrace{\left(  s_{k}\times s_{k}\right)  ^{2}}_{=\left(  s_{k}\times
s_{k}\right)  \circ\left(  s_{k}\times s_{k}\right)  }\left(  \underbrace{u}%
_{=\left(  i,j\right)  }\right)   &  =\left(  \left(  s_{k}\times
s_{k}\right)  \circ\left(  s_{k}\times s_{k}\right)  \right)  \left(  \left(
i,j\right)  \right)  =\left(  s_{k}\times s_{k}\right)  \left(
\underbrace{\left(  s_{k}\times s_{k}\right)  \left(  \left(  i,j\right)
\right)  }_{\substack{=\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)
\right)  \\\text{(by the definition of }s_{k}\times s_{k}\text{)}}}\right) \\
&  =\left(  s_{k}\times s_{k}\right)  \left(  \left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  \right)  =\left(  \underbrace{s_{k}\left(
s_{k}\left(  i\right)  \right)  }_{=s_{k}^{2}\left(  i\right)  }%
,\underbrace{s_{k}\left(  s_{k}\left(  j\right)  \right)  }_{=s_{k}^{2}\left(
j\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }s_{k}\times
s_{k}\right) \\
&  =\left(  \underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(  i\right)
,\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(  j\right)  \right)
=\left(  \underbrace{\operatorname*{id}\left(  i\right)  }_{=i}%
,\underbrace{\operatorname*{id}\left(  j\right)  }_{=j}\right)  =\left(
i,j\right)  =u=\operatorname*{id}\left(  u\right)  .
\end{align*}
\par
Now, let us forget that we fixed $u$. We thus have shown that $\left(
s_{k}\times s_{k}\right)  ^{2}\left(  u\right)  =\operatorname*{id}\left(
u\right)  $ for every $u\in\left[  n\right]  \times\left[  n\right]  $. In
other words, $\left(  s_{k}\times s_{k}\right)  ^{2}=\operatorname*{id}$,
qed.}, and thus is a bijection from $\left[  n\right]  ^{2}$ to $\left[
n\right]  ^{2}$\ \ \ \ \footnote{\textit{Proof.} Let $k\in\left\{
1,2,\ldots,n-1\right\}  $. We have $\left(  s_{k}\times s_{k}\right)
\circ\left(  s_{k}\times s_{k}\right)  =\left(  s_{k}\times s_{k}\right)
^{2}=\operatorname*{id}$. Hence, the maps $s_{k}\times s_{k}$ and $s_{k}\times
s_{k}$ are mutually inverse. Hence, the map $s_{k}\times s_{k}$ is invertible,
thus a bijection. Therefore, this map $s_{k}\times s_{k}$ is a bijection from
$\left[  n\right]  ^{2}$ to $\left[  n\right]  ^{2}$ (because its domain is
$\left[  n\right]  \times\left[  n\right]  =\left[  n\right]  ^{2}$, and its
codomain is $\left[  n\right]  \times\left[  n\right]  =\left[  n\right]
^{2}$). Qed.}.

Let us now recall a simple fact: If $u$ and $v$ are two integers such that
$1\leq u<v\leq n$, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $ is such that
$\left(  u,v\right)  \neq\left(  k,k+1\right)  $, then%
\begin{equation}
s_{k}\left(  u\right)  <s_{k}\left(  v\right)  . \label{pf.prop.sorting.a.2.5}%
\end{equation}
(This was proven in the solution of Exercise \ref{exe.ps2.2.5} \textbf{(a)}.)

Now, we notice the following facts:

\begin{itemize}
\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  =\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{s_{k}\left(
u\right)  }>a_{s_{k}\left(  v\right)  }\right\}
\label{pf.prop.sorting.a.fact1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact1}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $. The definition of
$\mathbf{a}\circ s_{k}$ yields $\mathbf{a}\circ s_{k}=\left(  a_{s_{k}\left(
1\right)  },a_{s_{k}\left(  2\right)  },\ldots,a_{s_{k}\left(  n\right)
}\right)  $ (since $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $).
Hence, (\ref{pf.prop.sorting.a.Inv.2}) (applied to $\mathbf{a}\circ s_{k}$ and
$a_{s_{k}\left(  i\right)  }$ instead of $\mathbf{a}$ and $a_{i}$) yields
$\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  =\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{s_{k}\left(
u\right)  }>a_{s_{k}\left(  v\right)  }\right\}  $. This proves
(\ref{pf.prop.sorting.a.fact1}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, and if $k\in\left\{
1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
\subseteq\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}
\label{pf.prop.sorting.a.fact2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact2}):} Let $\mathbf{a}$
be an $n$-tuple of integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
\par
Let $c\in\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}%
\left(  \mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}
\right)  $. Thus, $c\in\left[  n\right]  ^{2}$, so that we can write $c$ in
the form $c=\left(  i,j\right)  $ for some $i\in\left[  n\right]  $ and
$j\in\left[  n\right]  $. Consider these $i$ and $j$.
\par
We have $\left(  i,j\right)  =c\in\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  $, so that $\left(  s_{k}\times
s_{k}\right)  \left(  \left(  i,j\right)  \right)  \in\operatorname*{Inv}%
\left(  \mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}
$. Since $\left(  s_{k}\times s_{k}\right)  \left(  \left(  i,j\right)
\right)  =\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  $
(by the definition of $\left(  s_{k}\times s_{k}\right)  $), this rewrites as
$\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)
\in\operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{  \left(
k,k+1\right)  \right\}  $. In other words, $\left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  \in\operatorname*{Inv}\left(  \mathbf{a}%
\right)  $ and $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)
\right)  \neq\left(  k,k+1\right)  $.
\par
We have $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)
\in\operatorname*{Inv}\left(  \mathbf{a}\right)  =\left\{  \left(  u,v\right)
\in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  $ (by
(\ref{pf.prop.sorting.a.Inv.2})). In other words, $\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  $ is an element of $\left[
n\right]  ^{2}$ and satisfies $s_{k}\left(  i\right)  <s_{k}\left(  j\right)
$ and $a_{s_{k}\left(  i\right)  }>a_{s_{k}\left(  j\right)  }$. We have
$s_{k}\left(  i\right)  <s_{k}\left(  j\right)  $ and $\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  \neq\left(  k,k+1\right)  $. Thus,
we can apply (\ref{pf.prop.sorting.a.2.5}) to $u=s_{k}\left(  i\right)  $ and
$v=s_{k}\left(  j\right)  $. We thus conclude $s_{k}\left(  s_{k}\left(
i\right)  \right)  <s_{k}\left(  s_{k}\left(  j\right)  \right)  $. In other
words, $i<j$ (since $s_{k}\left(  s_{k}\left(  i\right)  \right)
=\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(  i\right)
=\operatorname*{id}\left(  i\right)  =i$ and $s_{k}\left(  s_{k}\left(
j\right)  \right)  =\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(
j\right)  =\operatorname*{id}\left(  j\right)  =j$).
\par
Now, we know that $\left(  i,j\right)  $ is an element $\left(  u,v\right)  $
of $\left[  n\right]  ^{2}$ satisfying $u<v$ and $a_{s_{k}\left(  u\right)
}>a_{s_{k}\left(  v\right)  }$ (since $i<j$ and $a_{s_{k}\left(  i\right)
}>a_{s_{k}\left(  j\right)  }$). In other words,%
\[
\left(  i,j\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(
v\right)  }\right\}  =\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\]
(by (\ref{pf.prop.sorting.a.fact1})).
\par
Next, let us assume (for the sake of contradiction) that $\left(  i,j\right)
=\left(  k,k+1\right)  $. Thus, $i=k$ and $j=k+1$. Thus, $s_{k}\left(
\underbrace{i}_{=k}\right)  =s_{k}\left(  k\right)  =k+1>k=s_{k}\left(
\underbrace{k+1}_{=j}\right)  =s_{k}\left(  j\right)  $; but this contradicts
$s_{k}\left(  i\right)  <s_{k}\left(  j\right)  $. This contradiction shows
that our assumption (that $\left(  i,j\right)  =\left(  k,k+1\right)  $) was
wrong. Hence, we cannot have $\left(  i,j\right)  =\left(  k,k+1\right)  $. In
other words, we must have $\left(  i,j\right)  \neq\left(  k,k+1\right)  $.
Combined with $\left(  i,j\right)  \in\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  $, this yields $\left(  i,j\right)
\in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  $. Thus, $c=\left(  i,j\right)  \in
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  $.
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  $ for every $c\in\left(  s_{k}\times
s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  \right)  $. In other words,%
\[
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
\subseteq\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  .
\]
This proves (\ref{pf.prop.sorting.a.fact2}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, and if $k\in\left\{
1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \subseteq\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  \label{pf.prop.sorting.a.fact3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact3}):} Let $\mathbf{a}$
be an $n$-tuple of integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Write the $n$-tuple $\mathbf{a}$ in the form $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $ for some integers $a_{1},a_{2},\ldots,a_{n}$.
\par
Let $c\in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  $. Thus, $c\in
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \subseteq\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  \subseteq\left[  n\right]  ^{2}$, so that we can
write $c$ in the form $c=\left(  i,j\right)  $ for some $i\in\left[  n\right]
$ and $j\in\left[  n\right]  $. Consider these $i$ and $j$.
\par
We have $\left(  i,j\right)  =c\in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  $. In other
words, $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  $ and $\left(  i,j\right)  \neq\left(  k,k+1\right)  $.
\par
We have $\left(  i,j\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\text{ and }a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(  v\right)
}\right\}  $ (by (\ref{pf.prop.sorting.a.fact1})). In other words, $\left(
i,j\right)  $ is an element of $\left[  n\right]  ^{2}$ and satisfies $i<j$
and $a_{s_{k}\left(  i\right)  }>a_{s_{k}\left(  j\right)  }$. Applying
(\ref{pf.prop.sorting.a.2.5}) to $u=i$ and $v=j$, we obtain $s_{k}\left(
i\right)  <s_{k}\left(  j\right)  $ (since $i<j$ and $\left(  i,j\right)
\neq\left(  k,k+1\right)  $).
\par
The pair $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  $ is
a pair $\left(  u,v\right)  \in\left[  n\right]  ^{2}$ satisfying $u<v$ and
$a_{u}>a_{v}$ (since $s_{k}\left(  i\right)  <s_{k}\left(  j\right)  $ and
$a_{s_{k}\left(  i\right)  }>a_{s_{k}\left(  j\right)  }$). In other words,
$\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  \in\left\{
\left(  u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }%
a_{u}>a_{v}\right\}  =\operatorname*{Inv}\left(  \mathbf{a}\right)  $ (by
(\ref{pf.prop.sorting.a.Inv.2})).
\par
Next, let us assume (for the sake of contradiction) that $\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  =\left(  k,k+1\right)  $. Thus,
$s_{k}\left(  i\right)  =k$ and $s_{k}\left(  j\right)  =k+1$. Hence,
$k+1=s_{k}\left(  \underbrace{k}_{=s_{k}\left(  i\right)  }\right)
=s_{k}\left(  s_{k}\left(  i\right)  \right)  =\underbrace{s_{k}^{2}%
}_{=\operatorname*{id}}\left(  i\right)  =i$ and $k=s_{k}\left(
\underbrace{k+1}_{=s_{k}\left(  j\right)  }\right)  =s_{k}\left(  s_{k}\left(
j\right)  \right)  =\underbrace{s_{k}^{2}}_{=\operatorname*{id}}\left(
j\right)  =\operatorname*{id}\left(  j\right)  =j$, so that $i=k+1>k=j$. This
contradicts $i<j$. This contradiction shows that our assumption (that $\left(
s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)  =\left(  k,k+1\right)
$) was wrong. Hence, we cannot have $\left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  =\left(  k,k+1\right)  $. In other words, we
must have $\left(  s_{k}\left(  i\right)  ,s_{k}\left(  j\right)  \right)
\neq\left(  k,k+1\right)  $. Combined with $\left(  s_{k}\left(  i\right)
,s_{k}\left(  j\right)  \right)  \in\operatorname*{Inv}\left(  \mathbf{a}%
\right)  $, this yields $\left(  s_{k}\left(  i\right)  ,s_{k}\left(
j\right)  \right)  \in\operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  $.
\par
The definition of $s_{k}\times s_{k}$ yields $\left(  s_{k}\times
s_{k}\right)  \left(  \left(  i,j\right)  \right)  =\left(  s_{k}\left(
i\right)  ,s_{k}\left(  j\right)  \right)  \in\operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  $. In
other words, $\left(  i,j\right)  \in\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  $. Hence,%
\[
c=\left(  i,j\right)  \in\left(  s_{k}\times s_{k}\right)  ^{-1}\left(
\operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{  \left(
k,k+1\right)  \right\}  \right)  .
\]
\par
Now, let us forget that we fixed $c$. We thus have shown that $c\in\left(
s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
$ for every $c\in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  $. In other words,%
\[
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \subseteq\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  .
\]
This proves (\ref{pf.prop.sorting.a.fact3}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, and if $k\in\left\{
1,2,\ldots,n-1\right\}  $, then%
\begin{equation}
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
=\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \label{pf.prop.sorting.a.fact4}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact4}):} Let $\mathbf{a}$
be an $n$-tuple of integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $.
Then, combining (\ref{pf.prop.sorting.a.fact2}) with
(\ref{pf.prop.sorting.a.fact3}), we obtain $\left(  s_{k}\times s_{k}\right)
^{-1}\left(  \operatorname*{Inv}\left(  \mathbf{a}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  \right)  =\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  \setminus\left\{  \left(  k,k+1\right)
\right\}  $. This proves (\ref{pf.prop.sorting.a.fact4}).}.

\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers, and if $k\in\left\{  1,2,\ldots,n-1\right\}  $ is such
that $a_{k}>a_{k+1}$, then%
\begin{equation}
\ell\left(  \mathbf{a}\circ s_{k}\right)  =\ell\left(  \mathbf{a}\right)  -1
\label{pf.prop.sorting.a.fact5}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact5}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers, and let $k\in\left\{  1,2,\ldots,n-1\right\}  $ be such that
$a_{k}>a_{k+1}$.
\par
The definition of $\ell\left(  \mathbf{a}\right)  $ yields $\ell\left(
\mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert $.
\par
The pair $\left(  k,k+1\right)  $ is a pair $\left(  u,v\right)  \in\left[
n\right]  ^{2}$ such that $u<v$ and $a_{u}>a_{v}$ (since $k<k+1$ and
$a_{k}>a_{k+1}$). In other words,%
\[
\left(  k,k+1\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  =\operatorname*{Inv}\left(
\mathbf{a}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.sorting.a.Inv.2})}\right)  .
\]
Hence, $\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  \right\vert
=\underbrace{\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert }_{=\ell\left(  \mathbf{a}\right)  }-1=\ell\left(  \mathbf{a}%
\right)  -1$. The map $s_{k}\times s_{k}$ is a bijection, and thus we have
$\left\vert \left(  s_{k}\times s_{k}\right)  ^{-1}\left(  X\right)
\right\vert =\left\vert X\right\vert $ for every subset $X$ of $\left[
n\right]  ^{2}$. Applying this to $X=\operatorname*{Inv}\left(  \mathbf{a}%
\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  $, we obtain%
\begin{equation}
\left\vert \left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}%
\left(  \mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}
\right)  \right\vert =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\setminus\left\{  \left(  k,k+1\right)  \right\}  \right\vert =\ell\left(
\mathbf{a}\right)  -1. \label{pf.prop.sorting.a.fact5.pf.1}%
\end{equation}
\par
On the other hand, let us assume (for the sake of contradiction) that $\left(
k,k+1\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  $.
Thus,%
\[
\left(  k,k+1\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  =\left\{  \left(  u,v\right)  \in\left[  n\right]  ^{2}%
\ \mid\ u<v\text{ and }a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(  v\right)
}\right\}
\]
(by (\ref{pf.prop.sorting.a.fact1})). In other words, $\left(  k,k+1\right)  $
is a pair $\left(  u,v\right)  \in\left[  n\right]  ^{2}$ such that $u<v$ and
$a_{s_{k}\left(  u\right)  }>a_{s_{k}\left(  v\right)  }$. In other words,
$k<k+1$ and $a_{s_{k}\left(  k\right)  }>a_{s_{k}\left(  k+1\right)  }$. Now,
$a_{s_{k}\left(  k\right)  }>a_{s_{k}\left(  k+1\right)  }$. In other words,
$a_{k+1}>a_{k}$ (since $s_{k}\left(  k\right)  =k+1$ and $s_{k}\left(
k+1\right)  =k$). This contradicts $a_{k}>a_{k+1}$. This contradiction shows
that our assumption (that $\left(  k,k+1\right)  \in\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  $) was wrong. Hence, we cannot have $\left(
k,k+1\right)  \in\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  $.
We thus have $\left(  k,k+1\right)  \notin\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  $. Hence, $\operatorname*{Inv}\left(
\mathbf{a}\circ s_{k}\right)  \setminus\left\{  \left(  k,k+1\right)
\right\}  =\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  $. Now,
(\ref{pf.prop.sorting.a.fact4}) becomes%
\[
\left(  s_{k}\times s_{k}\right)  ^{-1}\left(  \operatorname*{Inv}\left(
\mathbf{a}\right)  \setminus\left\{  \left(  k,k+1\right)  \right\}  \right)
=\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \setminus\left\{
\left(  k,k+1\right)  \right\}  =\operatorname*{Inv}\left(  \mathbf{a}\circ
s_{k}\right)  .
\]
Therefore, (\ref{pf.prop.sorting.a.fact5.pf.1}) rewrites as $\left\vert
\operatorname*{Inv}\left(  \mathbf{a}\circ s_{k}\right)  \right\vert
=\ell\left(  \mathbf{a}\right)  -1$.
\par
But the definition of $\ell\left(  \mathbf{a}\circ s_{k}\right)  $ yields
$\ell\left(  \mathbf{a}\circ s_{k}\right)  =\left\vert \operatorname*{Inv}%
\left(  \mathbf{a}\circ s_{k}\right)  \right\vert =\ell\left(  \mathbf{a}%
\right)  -1$. This proves (\ref{pf.prop.sorting.a.fact5}).}.

\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers satisfying $\ell\left(  \mathbf{a}\right)  =0$, then%
\begin{equation}
a_{1}\leq a_{2}\leq\cdots\leq a_{n} \label{pf.prop.sorting.a.fact7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact7}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers such that $\ell\left(  \mathbf{a}\right)  =0$.
\par
We assume (for the sake of contradiction) that there exists some $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $a_{k}>a_{k+1}$. Consider this $k$. Then,
$\left(  k,k+1\right)  $ is an element $\left(  u,v\right)  \in\left[
n\right]  ^{2}$ satisfying $u<v$ and $a_{u}>a_{v}$ (since $k<k+1$ and
$a_{k}>a_{k+1}$). In other words,%
\[
\left(  k,k+1\right)  \in\left\{  \left(  u,v\right)  \in\left[  n\right]
^{2}\ \mid\ u<v\text{ and }a_{u}>a_{v}\right\}  =\operatorname*{Inv}\left(
\mathbf{a}\right)
\]
(by (\ref{pf.prop.sorting.a.Inv.2})). Hence, the set $\operatorname*{Inv}%
\left(  \mathbf{a}\right)  $ contains at least one element (namely, $\left(
k,k+1\right)  $). In other words, $\left\vert \operatorname*{Inv}\left(
\mathbf{a}\right)  \right\vert \geq1$.
\par
But the definition of $\ell\left(  \mathbf{a}\right)  $ yields $\ell\left(
\mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert \geq1$. This contradicts $\ell\left(  \mathbf{a}\right)  =0$. This
contradiction shows that our assumption (that there exists some $k\in\left\{
1,2,\ldots,n-1\right\}  $ such that $a_{k}>a_{k+1}$) was wrong.
\par
Therefore, there exists no $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$a_{k}>a_{k+1}$. In other words, every $k\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies $a_{k}\leq a_{k+1}$. In other words, $a_{1}\leq a_{2}\leq\cdots\leq
a_{n}$. This proves (\ref{pf.prop.sorting.a.fact7}).}.

\item If $\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is an
$n$-tuple of integers satisfying $a_{1}\leq a_{2}\leq\cdots\leq a_{n}$, then%
\begin{equation}
\ell\left(  \mathbf{a}\right)  =0 \label{pf.prop.sorting.a.fact8}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact8}):} Let
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ be an $n$-tuple of
integers satisfying $a_{1}\leq a_{2}\leq\cdots\leq a_{n}$. We must prove that
$\ell\left(  \mathbf{a}\right)  =0$.
\par
Indeed, assume the contrary. Thus, $\ell\left(  \mathbf{a}\right)  \neq0$. But
the definition of $\ell\left(  \mathbf{a}\right)  $ yields $\ell\left(
\mathbf{a}\right)  =\left\vert \operatorname*{Inv}\left(  \mathbf{a}\right)
\right\vert $, so that $\left\vert \operatorname*{Inv}\left(  \mathbf{a}%
\right)  \right\vert =\ell\left(  \mathbf{a}\right)  \neq0$. Hence, the set
$\operatorname*{Inv}\left(  \mathbf{a}\right)  $ is nonempty. In other words,
there exists some $c\in\operatorname*{Inv}\left(  \mathbf{a}\right)  $.
Consider this $c$.
\par
Recall that $a_{1}\leq a_{2}\leq\cdots\leq a_{n}$. In other words,%
\begin{equation}
a_{u}\leq a_{v} \label{pf.prop.sorting.a.fact8.pf.leq}%
\end{equation}
for any $u\in\left[  n\right]  $ and $v\in\left[  n\right]  $ satisfying
$u<v$.
\par
But%
\[
c\in\operatorname*{Inv}\left(  \mathbf{a}\right)  =\left\{  \left(
u,v\right)  \in\left[  n\right]  ^{2}\ \mid\ u<v\text{ and }a_{u}%
>a_{v}\right\}
\]
(by (\ref{pf.prop.sorting.a.Inv.2})). In other words, $c$ has the form
$c=\left(  u,v\right)  $ for some $\left(  u,v\right)  \in\left[  n\right]
^{2}$ satisfying $u<v$ and $a_{u}>a_{v}$. Consider this $\left(  u,v\right)
\in\left[  n\right]  ^{2}$. We have $u<v$, and thus $a_{u}\leq a_{v}$ (by
(\ref{pf.prop.sorting.a.fact8.pf.leq})). This contradicts $a_{u}>a_{v}$. This
contradiction proves that our assumption was wrong.
\par
Hence, we have shown that $\ell\left(  \mathbf{a}\right)  =0$. This proves
(\ref{pf.prop.sorting.a.fact8}).}.

\item If $\mathbf{a}$ is an $n$-tuple of integers, then%
\begin{equation}
\text{there exists a }\sigma\in S_{n}\text{ such that }\ell\left(
\mathbf{a}\circ\sigma\right)  =0 \label{pf.prop.sorting.a.fact6}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sorting.a.fact6}):} We shall prove
(\ref{pf.prop.sorting.a.fact6}) by induction over $\ell\left(  \mathbf{a}%
\right)  $:
\par
\textit{Induction base:} If $\mathbf{a}$ is an $n$-tuple of integers
satisfying $\ell\left(  \mathbf{a}\right)  =0$, then we have $\ell\left(
\underbrace{\mathbf{a}\circ\operatorname*{id}}_{\substack{=\mathbf{a}%
\\\text{(by (\ref{pf.prop.sorting.a.id}))}}}\right)  =\ell\left(
\mathbf{a}\right)  =0$. Hence, if $\mathbf{a}$ is an $n$-tuple of integers
satisfying $\ell\left(  \mathbf{a}\right)  =0$, then there exists a $\sigma\in
S_{n}$ such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$ (namely,
$\sigma=\operatorname*{id}$). In other words, (\ref{pf.prop.sorting.a.fact6})
holds for $\ell\left(  \mathbf{a}\right)  =0$. This completes the induction
base.
\par
\textit{Induction step:} Let $L$ be a positive integer. Assume that
(\ref{pf.prop.sorting.a.fact6}) holds for $\ell\left(  \mathbf{a}\right)
=L-1$. We need to prove that (\ref{pf.prop.sorting.a.fact6}) holds for
$\ell\left(  \mathbf{a}\right)  =L$.
\par
We have assumed that (\ref{pf.prop.sorting.a.fact6}) holds for $\ell\left(
\mathbf{a}\right)  =L-1$. In other words, if $\mathbf{a}$ is an $n$-tuple of
integers satisfying $\ell\left(  \mathbf{a}\right)  =L-1$, then%
\begin{equation}
\text{there exists a }\sigma\in S_{n}\text{ such that }\ell\left(
\mathbf{a}\circ\sigma\right)  =0. \label{pf.prop.sorting.a.fact6.pf.hyp}%
\end{equation}
\par
Now, let $\mathbf{a}$ be an $n$-tuple of integers satisfying $\ell\left(
\mathbf{a}\right)  =L$. We shall show that there exists a $\sigma\in S_{n}$
such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$.
\par
Let us first prove that there exists some $k\in\left\{  1,2,\ldots
,n-1\right\}  $ such that $a_{k}>a_{k+1}$. In fact, assume the contrary. Thus,
there exists no $k\in\left\{  1,2,\ldots,n-1\right\}  $ such that
$a_{k}>a_{k+1}$. In other words, every $k\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies $a_{k}\leq a_{k+1}$. In other words, $a_{1}\leq a_{2}\leq\cdots\leq
a_{n}$. Thus, (\ref{pf.prop.sorting.a.fact8}) shows that $\ell\left(
\mathbf{a}\right)  =0$. Hence, $0=\ell\left(  \mathbf{a}\right)  =L>0$ (since
$L$ is positive). This is absurd. This contradiction proves that our
assumption was wrong.
\par
Hence, we have proven that there exists some $k\in\left\{  1,2,\ldots
,n-1\right\}  $ such that $a_{k}>a_{k+1}$. Consider this $k$. Then,
(\ref{pf.prop.sorting.a.fact5}) shows that $\ell\left(  \mathbf{a}\circ
s_{k}\right)  =\underbrace{\ell\left(  \mathbf{a}\right)  }_{=L}-1=L-1$.
Therefore, (\ref{pf.prop.sorting.a.fact6.pf.hyp}) (applied to $\mathbf{a}\circ
s_{k}$ instead of $\mathbf{a}$) shows that there exists a $\sigma\in S_{n}$
such that $\ell\left(  \left(  \mathbf{a}\circ s_{k}\right)  \circ
\sigma\right)  =0$. Let $\tau$ be such a $\sigma$. Thus, $\tau$ is an element
of $S_{n}$ and satisfies $\ell\left(  \left(  \mathbf{a}\circ s_{k}\right)
\circ\tau\right)  =0$.
\par
Now, (\ref{pf.prop.sorting.a.sigmatau}) (applied to $s_{k}$ instead of
$\sigma$) yields $\mathbf{a}\circ\left(  s_{k}\circ\tau\right)  =\left(
\mathbf{a}\circ s_{k}\right)  \circ\tau$. Hence, $\ell\left(  \mathbf{a}%
\circ\left(  s_{k}\circ\tau\right)  \right)  =\ell\left(  \left(
\mathbf{a}\circ s_{k}\right)  \circ\tau\right)  =0$. Thus, there exists a
$\sigma\in S_{n}$ such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$
(namely, $\sigma=s_{k}\circ\tau$).
\par
Now, let us forget that we fixed $\mathbf{a}$. We thus have shown that if
$\mathbf{a}$ is an $n$-tuple of integers satisfying $\ell\left(
\mathbf{a}\right)  =L$, then there exists a $\sigma\in S_{n}$ such that
$\ell\left(  \mathbf{a}\circ\sigma\right)  =0$. In other words,
(\ref{pf.prop.sorting.a.fact6}) holds for $\ell\left(  \mathbf{a}\right)  =L$.
This completes the induction step. Thus, the induction proof of
(\ref{pf.prop.sorting.a.fact6}) is complete.}.
\end{itemize}

Now, let us prove Proposition \ref{prop.sorting} \textbf{(a)}. Let
$a_{1},a_{2},\ldots,a_{n}$ be $n$ integers. Let $\mathbf{a}$ be the $n$-tuple
$\left(  a_{1},a_{2},\ldots,a_{n}\right)  $. Then, there exists a $\sigma\in
S_{n}$ such that $\ell\left(  \mathbf{a}\circ\sigma\right)  =0$ (by
(\ref{pf.prop.sorting.a.fact6})). Consider this $\sigma$. The definition of
$\mathbf{a}\circ\sigma$ shows that $\mathbf{a}\circ\sigma=\left(
a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  },\ldots
,a_{\sigma\left(  n\right)  }\right)  $ (since $\mathbf{a}=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $). Therefore, (\ref{pf.prop.sorting.a.fact7})
(applied to $\mathbf{a}\circ\sigma$ and $a_{\sigma\left(  i\right)  }$ instead
of $\mathbf{a}$ and $a_{i}$) yields $a_{\sigma\left(  1\right)  }\leq
a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$.

Let us now forget that we defined $\sigma$. We thus have constructed a
$\sigma\in S_{n}$ satisfying $a_{\sigma\left(  1\right)  }\leq a_{\sigma
\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$. Therefore,
there exists a permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(
1\right)  }\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(
n\right)  }$. This proves Proposition \ref{prop.sorting} \textbf{(a)}.

\textbf{(b)} Let $\sigma\in S_{n}$ be such that $a_{\sigma\left(  1\right)
}\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }%
$. Let $i\in\left\{  1,2,\ldots,n\right\}  $. We shall now show that%
\begin{equation}
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  . \label{pf.prop.sorting.bclaim}%
\end{equation}
(This statement includes the tacit claim that the right hand side of
(\ref{pf.prop.sorting.bclaim}) is well-defined.)

\textit{Proof of (\ref{pf.prop.sorting.bclaim}):} Define a subset $X$ of
$\mathbb{Z}$ by%
\[
X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  .
\]
We shall show that $a_{\sigma\left(  i\right)  }=\min X$.

We have $a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }%
\leq\cdots\leq a_{\sigma\left(  n\right)  }$. In other words,%
\begin{equation}
a_{\sigma\left(  u\right)  }\leq a_{\sigma\left(  v\right)  }
\label{pf.prop.sorting.bclaim.pf.1}%
\end{equation}
for any two elements $u$ and $v$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $u\leq v$.

We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation, thus a bijective
map, thus an injective map.

Every $k\in\left\{  1,2,\ldots,i\right\}  $ satisfies $\sigma\left(  k\right)
\in\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
a_{\sigma\left(  i\right)  }\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  1,2,\ldots,i\right\}  $. Then, $k\leq i$ and thus $a_{\sigma
\left(  k\right)  }\leq a_{\sigma\left(  i\right)  }$ (by
(\ref{pf.prop.sorting.bclaim.pf.1}), applied to $u=k$ and $v=i$). Hence,
$\sigma\left(  k\right)  $ is an element $j$ of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $a_{j}\leq a_{\sigma\left(  i\right)  }$ (because
$a_{\sigma\left(  k\right)  }\leq a_{\sigma\left(  i\right)  }$). In other
words, $\sigma\left(  k\right)  \in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq a_{\sigma\left(  i\right)  }\right\}  $, qed.}. In other
words,%
\[
\left\{  \sigma\left(  k\right)  \ \mid\ k\in\left\{  1,2,\ldots,i\right\}
\right\}  \subseteq\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid
\ a_{j}\leq a_{\sigma\left(  i\right)  }\right\}  .
\]
Hence,%
\[
\left\vert \left\{  \sigma\left(  k\right)  \ \mid\ k\in\left\{
1,2,\ldots,i\right\}  \right\}  \right\vert \leq\left\vert \left\{
j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq a_{\sigma\left(
i\right)  }\right\}  \right\vert ,
\]
so that
\begin{align*}
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
a_{\sigma\left(  i\right)  }\right\}  \right\vert  &  \geq\left\vert
\underbrace{\left\{  \sigma\left(  k\right)  \ \mid\ k\in\left\{
1,2,\ldots,i\right\}  \right\}  }_{=\sigma\left(  \left\{  1,2,\ldots
,i\right\}  \right)  }\right\vert \\
&  =\left\vert \sigma\left(  \left\{  1,2,\ldots,i\right\}  \right)
\right\vert =\left\vert \left\{  1,2,\ldots,i\right\}  \right\vert \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the map }\sigma\text{ is
injective}\right) \\
&  =i.
\end{align*}
In other words, at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfy $a_{j}\leq a_{\sigma\left(  i\right)  }$. In other words,
$a_{\sigma\left(  i\right)  }$ is an element $x$ of $\mathbb{Z}$ such that at
least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq
x$. In other words,%
\[
a_{\sigma\left(  i\right)  }\in\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  =X.
\]


On the other hand, let $y$ be any element of $X$. We shall show that
$a_{\sigma\left(  i\right)  }\leq y$. Indeed, assume the contrary. Thus,
$a_{\sigma\left(  i\right)  }>y$. Hence,%
\begin{equation}
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
y\right\}  \right\vert <i \label{pf.prop.sorting.bclaim.pf.5}%
\end{equation}
\footnote{\textit{Proof.} Let $p\in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq y\right\}  $. Thus, $p$ is an element $j$ of $\left\{
1,2,\ldots,n\right\}  $ such that $a_{j}\leq y$. In other words, $p$ is an
element of $\left\{  1,2,\ldots,n\right\}  $ and satisfies $a_{p}\leq y$.
\par
The permutation $\sigma$ has an inverse $\sigma^{-1}$. Let $q=\sigma
^{-1}\left(  p\right)  $. Thus, $p=\sigma\left(  q\right)  $. Hence,
$a_{p}=a_{\sigma\left(  q\right)  }$, so that $a_{\sigma\left(  q\right)
}=a_{p}\leq y<a_{\sigma\left(  i\right)  }$ (since $a_{\sigma\left(  i\right)
}>y$).
\par
If we had $i\leq q$, then we would have $a_{\sigma\left(  i\right)  }\leq
a_{\sigma\left(  q\right)  }$ (by (\ref{pf.prop.sorting.bclaim.pf.1}), applied
to $u=i$ and $v=q$), which would contradict $a_{\sigma\left(  q\right)
}<a_{\sigma\left(  i\right)  }$. Hence, we cannot have $i\leq q$. Thus, we
must have $q<i$. Hence, $q\in\left\{  1,2,\ldots,i-1\right\}  $. Thus,
$p=\sigma\left(  \underbrace{q}_{\in\left\{  1,2,\ldots,i-1\right\}  }\right)
\in\sigma\left(  \left\{  1,2,\ldots,i-1\right\}  \right)  $.
\par
Let us now forget that we fixed $p$. We thus have shown that every
$p\in\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq y\right\}
$ satisfies $p\in\sigma\left(  \left\{  1,2,\ldots,i-1\right\}  \right)  $. In
other words,%
\[
\left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq y\right\}
\subseteq\sigma\left(  \left\{  1,2,\ldots,i-1\right\}  \right)  .
\]
Hence,%
\begin{align*}
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
y\right\}  \right\vert  &  \leq\left\vert \sigma\left(  \left\{
1,2,\ldots,i-1\right\}  \right)  \right\vert =\left\vert \left\{
1,2,\ldots,i-1\right\}  \right\vert \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since the map }\sigma\text{ is
injective}\right) \\
&  =i-1<i,
\end{align*}
qed.}.

But%
\[
y\in X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  .
\]
In other words, $y$ is an element $x$ of $\mathbb{Z}$ such that at least $i$
elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq x$. In other
words, $y$ is an element of $\mathbb{Z}$, and at least $i$ elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq y$. We have%
\[
\left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
y\right\}  \right\vert \geq i
\]
(since at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy
$a_{j}\leq y$). This contradicts (\ref{pf.prop.sorting.bclaim.pf.5}). This
contradiction proves that our assumption was wrong. Hence, $a_{\sigma\left(
i\right)  }\leq y$ is proven.

Now, let us forget that we fixed $y$. We thus have shown that every $y\in X$
satisfies $a_{\sigma\left(  i\right)  }\leq y$. Altogether, we thus have shown
the following two facts:

\begin{itemize}
\item We have $a_{\sigma\left(  i\right)  }\in X$.

\item Every $y\in X$ satisfies $a_{\sigma\left(  i\right)  }\leq y$.
\end{itemize}

Combining these two facts, we obtain $a_{\sigma\left(  i\right)  }=\min X$
(and, in particular, this shows that $\min X$ is well-defined). Since
\newline$X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements
}j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  $,
this rewrites as follows:%
\[
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  .
\]
Thus, (\ref{pf.prop.sorting.bclaim}) is proven.

Now, the value $a_{\sigma\left(  i\right)  }$ depends only on $a_{1}%
,a_{2},\ldots,a_{n}$ and $i$ (but not on $\sigma$) (because
(\ref{pf.prop.sorting.bclaim}) gives a description of $a_{\sigma\left(
i\right)  }$ which involves $a_{1},a_{2},\ldots,a_{n}$ and $i$, but not
$\sigma$). Thus, Proposition \ref{prop.sorting} \textbf{(b)} is proven.

\textbf{(c)} The integers $a_{1},a_{2},\ldots,a_{n}$ are distinct. In other
words, if $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots
,n\right\}  $, then%
\begin{equation}
a_{u}\neq a_{v}. \label{pf.prop.sorting.c.distinct}%
\end{equation}


We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation, thus a bijective
map, thus an injective map.

Now, we can see that:

\begin{itemize}
\item There is \textbf{at least one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.sorting} \textbf{(a)} shows that there exists a permutation
$\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(
2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$. Consider this
$\sigma$.
\par
Let $k\in\left\{  1,2,\ldots,n-1\right\}  $. Then, $a_{\sigma\left(  k\right)
}\leq a_{\sigma\left(  k+1\right)  }$ (since $a_{\sigma\left(  1\right)  }\leq
a_{\sigma\left(  2\right)  }\leq\cdots\leq a_{\sigma\left(  n\right)  }$). But
on the other hand, $k\neq k+1$. Hence, $\sigma\left(  k\right)  \neq
\sigma\left(  k+1\right)  $ (since the map $\sigma$ is injective). Hence,
$a_{\sigma\left(  k\right)  }\neq a_{\sigma\left(  k+1\right)  }$ (by
(\ref{pf.prop.sorting.c.distinct}), applied to $u=\sigma\left(  k\right)  $
and $v=\sigma\left(  k+1\right)  $). Combined with $a_{\sigma\left(  k\right)
}\leq a_{\sigma\left(  k+1\right)  }$, this yields $a_{\sigma\left(  k\right)
}<a_{\sigma\left(  k+1\right)  }$.
\par
Let us now forget that we fixed $k$. We thus have shown that $a_{\sigma\left(
k\right)  }<a_{\sigma\left(  k+1\right)  }$ for every $k\in\left\{
1,2,\ldots,n-1\right\}  $. In other words, $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$.
\par
Let us now forget that we defined $\sigma$. We thus have constructed a
permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. Hence,
there is \textbf{at least one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. Qed.}.

\item There is \textbf{at most one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$\ \ \ \ \footnote{\textit{Proof.} Let
$\sigma_{1}$ and $\sigma_{2}$ be two permutations $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. We shall show that $\sigma_{1}=\sigma_{2}$.
\par
Fix $i\in\left\{  1,2,\ldots,n\right\}  $.
\par
We know that $\sigma_{1}$ is a permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. In other words, $\sigma_{1}$ is a permutation
in $S_{n}$ such that $a_{\sigma_{1}\left(  1\right)  }<a_{\sigma_{1}\left(
2\right)  }<\cdots<a_{\sigma_{1}\left(  n\right)  }$. Thus,
(\ref{pf.prop.sorting.bclaim}) (applied to $\sigma=\sigma_{1}$) yields%
\begin{equation}
a_{\sigma_{1}\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy
}a_{j}\leq x\right\}  . \label{pf.prop.sorting.c.5}%
\end{equation}
The same argument (applied to $\sigma_{2}$ instead of $\sigma_{1}$) yields%
\[
a_{\sigma_{2}\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy
}a_{j}\leq x\right\}  .
\]
Comparing this with (\ref{pf.prop.sorting.c.5}), we obtain $a_{\sigma
_{1}\left(  i\right)  }=a_{\sigma_{2}\left(  i\right)  }$.
\par
Now, if we had $\sigma_{1}\left(  i\right)  \neq\sigma_{2}\left(  i\right)  $,
then we would have $a_{\sigma_{1}\left(  i\right)  }\neq a_{\sigma_{2}\left(
i\right)  }$ (by (\ref{pf.prop.sorting.c.distinct}), applied to $u=\sigma
_{1}\left(  i\right)  $ and $v=\sigma_{2}\left(  i\right)  $), which would
contradict $a_{\sigma_{1}\left(  i\right)  }=a_{\sigma_{2}\left(  i\right)  }%
$. Thus, we cannot have $\sigma_{1}\left(  i\right)  \neq\sigma_{2}\left(
i\right)  $. Hence, we must have $\sigma_{1}\left(  i\right)  =\sigma
_{2}\left(  i\right)  $.
\par
Let us now forget that we fixed $i$. We thus have shown that $\sigma
_{1}\left(  i\right)  =\sigma_{2}\left(  i\right)  $ for every $i\in\left\{
1,2,\ldots,n\right\}  $. In other words, $\sigma_{1}=\sigma_{2}$.
\par
Let us now forget that we fixed $\sigma_{1}$ and $\sigma_{2}$. We thus have
shown that $\sigma_{1}=\sigma_{2}$ whenever $\sigma_{1}$ and $\sigma_{2}$ are
two permutations $\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. In other
words, there is \textbf{at most one} permutation $\sigma\in S_{n}$ such that
$a_{\sigma\left(  1\right)  }<a_{\sigma\left(  2\right)  }<\cdots
<a_{\sigma\left(  n\right)  }$. Qed.}.
\end{itemize}

Combining these two statements, we conclude that there is a \textbf{unique}
permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. This
proves Proposition \ref{prop.sorting} \textbf{(c)}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.cauchy-binet.EI}.]We have%
\[
\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots,k_{n}\text{ are
distinct}\right\}
\]
and%
\[
\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[
m\right]  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}  .
\]


\begin{vershort}
Clearly, every element of $\mathbf{I}\times S_{n}$ can be written in the form
$\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}$.
\end{vershort}

\begin{verlong}
Every element of $\mathbf{I}\times S_{n}$ can be written in the form $\left(
\left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\alpha\in\mathbf{I}\times S_{n}$.
Then, $\alpha$ can be written in the form $\alpha=\left(  \beta,\gamma\right)
$ for some $\beta\in\mathbf{I}$ and $\gamma\in S_{n}$ (since $\alpha
\in\mathbf{I}\times S_{n}$). Consider these $\beta$ and $\gamma$. We have
$\beta\in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}\ \mid\ k_{1}<k_{2}<\cdots<k_{n}\right\}
\subseteq\left[  m\right]  ^{n}$. Hence, $\beta$ can be written in the form
$\beta=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ for some $\left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$. Consider this
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $. Clearly, $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  =\beta\in\mathbf{I}$.
\par
But $\alpha=\left(  \underbrace{\beta}_{=\left(  k_{1},k_{2},\ldots
,k_{n}\right)  },\gamma\right)  =\left(  \left(  k_{1},k_{2},\ldots
,k_{n}\right)  ,\gamma\right)  $. Hence, $\alpha$ can be written in the form
$\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}$
(namely, for $\left(  g_{1},g_{2},\ldots,g_{n}\right)  =\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  $ and $\sigma=\gamma$).
\par
Now, let us forget that we fixed $\alpha$. We thus have proven that every
$\alpha\in\mathbf{I}\times S_{n}$ can be written in the form $\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some $\left(
g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}$. In
other words, every element of $\mathbf{I}\times S_{n}$ can be written in the
form $\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for
some $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in
S_{n}$. Qed.}.
\end{verlong}

For every $\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)
\in\mathbf{I}\times S_{n}$, we have $\left(  g_{\sigma\left(  1\right)
},g_{\sigma\left(  2\right)  },\ldots,g_{\sigma\left(  n\right)  }\right)
\in\mathbf{E}$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  \in\mathbf{I}\times S_{n}$.
Thus, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in
S_{n}$.
\par
We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation, and thus a
bijective map, hence an injective map.
\par
We have
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $g_{1}<g_{2}<\cdots<g_{n}$. Hence, the integers $g_{1},g_{2}%
,\ldots,g_{n}$ are distinct (since $g_{1}<g_{2}<\cdots<g_{n}$). In other
words, any two distinct elements $u$ and $v$ of $\left[  n\right]  $ satisfy%
\begin{equation}
g_{u}\neq g_{v}. \label{pf.lem.cauchy-binet.EI.1.pf.1}%
\end{equation}
\par
Now, let $u$ and $v$ be two distinct elements of $\left[  n\right]  $. Thus,
$u\neq v$ (since $u$ and $v$ are distinct), so that $\sigma\left(  u\right)
\neq\sigma\left(  v\right)  $ (since $\sigma$ is injective). Hence,
$g_{\sigma\left(  u\right)  }\neq g_{\sigma\left(  v\right)  }$ (by
(\ref{pf.lem.cauchy-binet.EI.1.pf.1}), applied to $\sigma\left(  u\right)  $
and $\sigma\left(  v\right)  $ instead of $u$ and $v$).
\par
Let us now forget that we fixed $u$ and $v$. We thus have shown that any two
distinct elements $u$ and $v$ of $\left[  n\right]  $ satisfy $g_{\sigma
\left(  u\right)  }\neq g_{\sigma\left(  v\right)  }$. In other words, the
integers $g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  }%
,\ldots,g_{\sigma\left(  n\right)  }$ are distinct. Hence, $\left(
g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots
,g_{\sigma\left(  n\right)  }\right)  $ is an element $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that the integers
$k_{1},k_{2},\ldots,k_{n}$ are distinct. In other words,%
\[
\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  }%
,\ldots,g_{\sigma\left(  n\right)  }\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ \text{the
integers }k_{1},k_{2},\ldots,k_{n}\text{ are distinct}\right\}  =\mathbf{E},
\]
qed.}. Hence, we can define a map $\Phi:\mathbf{I}\times S_{n}\rightarrow
\mathbf{E}$ by setting%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\Phi\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  =\left(
g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)  },\ldots
,g_{\sigma\left(  n\right)  }\right) \\
\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  \left(  g_{1},g_{2},\ldots
,g_{n}\right)  ,\sigma\right)  \in\mathbf{I}\times S_{n}%
\end{array}
\right)  \label{pf.lem.cauchy-binet.EI.defPhi}%
\end{equation}
(since every element of $\mathbf{I}\times S_{n}$ can be written in the form
$\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)  $ for some
$\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\sigma\in S_{n}%
$). Consider this map $\Phi$.

The map $\Phi$ is the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
(since $\Phi$ is defined by (\ref{pf.lem.cauchy-binet.EI.defPhi})). In
particular, the latter map is well-defined.

The map $\Phi$ is injective\footnote{\textit{Proof.} Let $\alpha$ and $\beta$
be two elements of $\mathbf{I}\times S_{n}$ such that $\Phi\left(
\alpha\right)  =\Phi\left(  \beta\right)  $. We shall show that $\alpha=\beta
$.
\par
Let $\gamma$ be the element $\Phi\left(  \alpha\right)  =\Phi\left(
\beta\right)  $ of $\mathbf{E}$. Thus, $\gamma=\Phi\left(  \alpha\right)
=\Phi\left(  \beta\right)  $.
\par
Write $\alpha\in\mathbf{I}\times S_{n}$ in the form $\alpha=\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\pi\right)  $ for some $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  \in\mathbf{I}$ and $\pi\in S_{n}$.
\par
Write $\beta\in\mathbf{I}\times S_{n}$ in the form $\beta=\left(  \left(
h_{1},h_{2},\ldots,h_{n}\right)  ,\tau\right)  $ for some $\left(  h_{1}%
,h_{2},\ldots,h_{n}\right)  \in\mathbf{I}$ and $\tau\in S_{n}$.
\par
We have%
\[
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ of $\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  g_{1}%
,g_{2},\ldots,g_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $g_{1}<g_{2}<\cdots<g_{n}$.
\par
We have%
\[
\left(  h_{1},h_{2},\ldots,h_{n}\right)  \in\mathbf{I}=\left\{  \left(
k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  .
\]
In other words, $\left(  h_{1},h_{2},\ldots,h_{n}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ of $\left[  m\right]  ^{n}$
satisfying $k_{1}<k_{2}<\cdots<k_{n}$. In other words, $\left(  h_{1}%
,h_{2},\ldots,h_{n}\right)  $ is an element of $\left[  m\right]  ^{n}$ and
satisfies $h_{1}<h_{2}<\cdots<h_{n}$.
\par
We have $\gamma\in\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots
,k_{n}\text{ are distinct}\right\}  $. In other words, we can write $\gamma$
in the form $\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ for some
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that
the integers $k_{1},k_{2},\ldots,k_{n}$ are distinct. Consider this $\left(
k_{1},k_{2},\ldots,k_{n}\right)  $.
\par
Applying the map $\Phi$ to both sides of the equality $\alpha=\left(  \left(
g_{1},g_{2},\ldots,g_{n}\right)  ,\pi\right)  $, we obtain%
\[
\Phi\left(  \alpha\right)  =\Phi\left(  \left(  g_{1},g_{2},\ldots
,g_{n}\right)  ,\pi\right)  =\left(  g_{\pi\left(  1\right)  },g_{\pi\left(
2\right)  },\ldots,g_{\pi\left(  n\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right)  .
\]
Thus, $\left(  g_{\pi\left(  1\right)  },g_{\pi\left(  2\right)  }%
,\ldots,g_{\pi\left(  n\right)  }\right)  =\Phi\left(  \alpha\right)
=\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $. In other words, every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{equation}
g_{\pi\left(  i\right)  }=k_{i}. \label{pf.lem.cauchy-binet.EI.2.pf.1}%
\end{equation}
Thus, every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align}
g_{j}  &  =g_{\pi\left(  \pi^{-1}\left(  j\right)  \right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=\pi\left(  \pi^{-1}\left(
j\right)  \right)  \right) \nonumber\\
&  =k_{\pi^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.1}), applied to }i=\pi^{-1}\left(  j\right)
\right)  . \label{pf.lem.cauchy-binet.EI.2.pf.1a}%
\end{align}
\par
We have $g_{1}<g_{2}<\cdots<g_{n}$. This rewrites as
\[
k_{\pi^{-1}\left(  1\right)  }<k_{\pi^{-1}\left(  2\right)  }<\cdots
<k_{\pi^{-1}\left(  n\right)  }%
\]
(because every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies $g_{j}%
=k_{\pi^{-1}\left(  j\right)  }$).
\par
Applying the map $\Phi$ to both sides of the equality $\beta=\left(  \left(
h_{1},h_{2},\ldots,h_{n}\right)  ,\tau\right)  $, we obtain%
\[
\Phi\left(  \beta\right)  =\Phi\left(  \left(  h_{1},h_{2},\ldots
,h_{n}\right)  ,\tau\right)  =\left(  h_{\tau\left(  1\right)  }%
,h_{\tau\left(  2\right)  },\ldots,h_{\tau\left(  n\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\Phi\right)  .
\]
Thus, $\left(  h_{\tau\left(  1\right)  },h_{\tau\left(  2\right)  }%
,\ldots,h_{\tau\left(  n\right)  }\right)  =\Phi\left(  \beta\right)
=\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $. In other words, every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{equation}
h_{\tau\left(  i\right)  }=k_{i}. \label{pf.lem.cauchy-binet.EI.2.pf.2}%
\end{equation}
Thus, every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies
\begin{align}
h_{j}  &  =h_{\tau\left(  \tau^{-1}\left(  j\right)  \right)  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j=\tau\left(  \tau^{-1}\left(
j\right)  \right)  \right) \nonumber\\
&  =k_{\tau^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.2}), applied to }i=\pi^{-1}\left(  j\right)
\right)  . \label{pf.lem.cauchy-binet.EI.2.pf.2a}%
\end{align}
\par
We have $h_{1}<h_{2}<\cdots<h_{n}$. This rewrites as%
\[
k_{\tau^{-1}\left(  1\right)  }<k_{\tau^{-1}\left(  2\right)  }<\cdots
<k_{\tau^{-1}\left(  n\right)  }%
\]
(because every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies $h_{j}%
=k_{\tau^{-1}\left(  j\right)  }$).
\par
Proposition \ref{prop.sorting} \textbf{(c)} (applied to $a_{i}=k_{i}$) yields
that there is a \textbf{unique} permutation $\sigma\in S_{n}$ such that
$k_{\sigma\left(  1\right)  }<k_{\sigma\left(  2\right)  }<\cdots
<k_{\sigma\left(  n\right)  }$ (since the integers $k_{1},k_{2},\ldots,k_{n}$
are distinct). In particular, there exists \textbf{at most one} such
permutation. In other words, if $\sigma_{1}$ and $\sigma_{2}$ are two
permutations $\sigma\in S_{n}$ satisfying $k_{\sigma\left(  1\right)
}<k_{\sigma\left(  2\right)  }<\cdots<k_{\sigma\left(  n\right)  }$, then%
\begin{equation}
\sigma_{1}=\sigma_{2}. \label{pf.lem.cauchy-binet.EI.2.pf.4}%
\end{equation}
\par
Now, $\pi^{-1}$ is a permutation $\sigma\in S_{n}$ satisfying $k_{\sigma
\left(  1\right)  }<k_{\sigma\left(  2\right)  }<\cdots<k_{\sigma\left(
n\right)  }$ (since $k_{\pi^{-1}\left(  1\right)  }<k_{\pi^{-1}\left(
2\right)  }<\cdots<k_{\pi^{-1}\left(  n\right)  }$). Also, $\tau^{-1}$ is a
permutation $\sigma\in S_{n}$ satisfying $k_{\sigma\left(  1\right)
}<k_{\sigma\left(  2\right)  }<\cdots<k_{\sigma\left(  n\right)  }$ (since
$k_{\tau^{-1}\left(  1\right)  }<k_{\tau^{-1}\left(  2\right)  }%
<\cdots<k_{\tau^{-1}\left(  n\right)  }$). Hence, we can apply
(\ref{pf.lem.cauchy-binet.EI.2.pf.4}) to $\sigma_{1}=\pi^{-1}$ and $\sigma
_{2}=\tau^{-1}$. As a result, we obtain $\pi^{-1}=\tau^{-1}$. Hence, $\pi
=\tau$. Now, every $j\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align*}
g_{j}  &  =k_{\pi^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.1a})}\right) \\
&  =k_{\tau^{-1}\left(  j\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\pi=\tau\right) \\
&  =h_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.cauchy-binet.EI.2.pf.2a})}\right)  .
\end{align*}
In other words, $\left(  g_{1},g_{2},\ldots,g_{n}\right)  =\left(  h_{1}%
,h_{2},\ldots,h_{n}\right)  $. Thus,
\[
\alpha=\left(  \underbrace{\left(  g_{1},g_{2},\ldots,g_{n}\right)
}_{=\left(  h_{1},h_{2},\ldots,h_{n}\right)  },\underbrace{\pi}_{=\tau
}\right)  =\left(  \left(  h_{1},h_{2},\ldots,h_{n}\right)  ,\tau\right)
=\beta.
\]
\par
Now, let us forget that we fixed $\alpha$ and $\beta$. We thus have shown that
if $\alpha$ and $\beta$ are two elements of $\mathbf{I}\times S_{n}$ such that
$\Phi\left(  \alpha\right)  =\Phi\left(  \beta\right)  $, then $\alpha=\beta$.
In other words, the map $\Phi$ is injective, qed.} and
surjective\footnote{\textit{Proof.} Let $\gamma\in\mathbf{E}$. We shall prove
that $\gamma\in\Phi\left(  \mathbf{I}\times S_{n}\right)  $.
\par
We have $\gamma\in\mathbf{E}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n}\right)
\in\left[  m\right]  ^{n}\ \mid\ \text{the integers }k_{1},k_{2},\ldots
,k_{n}\text{ are distinct}\right\}  $. In other words, we can write $\gamma$
in the form $\gamma=\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ for some
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}$ such that
the integers $k_{1},k_{2},\ldots,k_{n}$ are distinct. Let us denote this
$\left(  k_{1},k_{2},\ldots,k_{n}\right)  $ by $\left(  a_{1},a_{2}%
,\ldots,a_{n}\right)  $. Thus, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ is
an element of $\left[  m\right]  ^{n}$ such that the integers $a_{1}%
,a_{2},\ldots,a_{n}$ are distinct, and we have $\gamma=\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  $.
\par
Proposition \ref{prop.sorting} \textbf{(c)} yields that there is a
\textbf{unique} permutation $\sigma\in S_{n}$ such that $a_{\sigma\left(
1\right)  }<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$
(since the integers $a_{1},a_{2},\ldots,a_{n}$ are distinct). In particular,
there exists \textbf{at least one} such permutation. Consider such a
permutation $\sigma$. Thus, $\sigma\in S_{n}$ and $a_{\sigma\left(  1\right)
}<a_{\sigma\left(  2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$.
\par
Now, $\left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  }%
,\ldots,a_{\sigma\left(  n\right)  }\right)  $ is an element of $\left[
m\right]  ^{n}$ satisfying $a_{\sigma\left(  1\right)  }<a_{\sigma\left(
2\right)  }<\cdots<a_{\sigma\left(  n\right)  }$. In other words, $\left(
a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  },\ldots
,a_{\sigma\left(  n\right)  }\right)  $ is an element $\left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  $ of $\left[  m\right]  ^{n}$ satisfying
$k_{1}<k_{2}<\cdots<k_{n}$. In other words,
\[
\left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)  }%
,\ldots,a_{\sigma\left(  n\right)  }\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n}\right)  \in\left[  m\right]  ^{n}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n}\right\}  =\mathbf{I}.
\]
Hence, $\Phi\left(  \left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(
2\right)  },\ldots,a_{\sigma\left(  n\right)  }\right)  ,\sigma^{-1}\right)  $
is well-defined. The definition of $\Phi\left(  \left(  a_{\sigma\left(
1\right)  },a_{\sigma\left(  2\right)  },\ldots,a_{\sigma\left(  n\right)
}\right)  ,\sigma^{-1}\right)  $ yields%
\begin{align*}
&  \Phi\left(  \left(  a_{\sigma\left(  1\right)  },a_{\sigma\left(  2\right)
},\ldots,a_{\sigma\left(  n\right)  }\right)  ,\sigma^{-1}\right) \\
&  =\left(  a_{\sigma\left(  \sigma^{-1}\left(  1\right)  \right)  }%
,a_{\sigma\left(  \sigma^{-1}\left(  2\right)  \right)  },\ldots
,a_{\sigma\left(  \sigma^{-1}\left(  n\right)  \right)  }\right)  =\left(
a_{1},a_{2},\ldots,a_{n}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }a_{\sigma\left(  \sigma
^{-1}\left(  i\right)  \right)  }=a_{i}\text{ for every }i\in\left\{
1,2,\ldots,n\right\}  \right) \\
&  =\gamma.
\end{align*}
Thus, $\gamma=\Phi\left(  \underbrace{\left(  a_{\sigma\left(  1\right)
},a_{\sigma\left(  2\right)  },\ldots,a_{\sigma\left(  n\right)  }\right)
}_{\in\mathbf{I}},\underbrace{\sigma^{-1}}_{\in S_{n}}\right)  \in\Phi\left(
\mathbf{I}\times S_{n}\right)  $.
\par
Now, let us forget that we fixed $\gamma$. We thus have shown that $\gamma
\in\Phi\left(  \mathbf{I}\times S_{n}\right)  $ for every $\gamma\in
\mathbf{E}$. In other words, $\mathbf{E}\subseteq\Phi\left(  \mathbf{I}\times
S_{n}\right)  $. In other words, the map $\Phi$ is surjective. Qed.}. Hence,
the map $\Phi$ is bijective. In other words, the map $\Phi$ is a bijection. In
other words, the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
is a bijection (because the map $\Phi$ is the map
\begin{align*}
\mathbf{I}\times S_{n}  &  \rightarrow\mathbf{E},\\
\left(  \left(  g_{1},g_{2},\ldots,g_{n}\right)  ,\sigma\right)   &
\mapsto\left(  g_{\sigma\left(  1\right)  },g_{\sigma\left(  2\right)
},\ldots,g_{\sigma\left(  n\right)  }\right)
\end{align*}
). This concludes the proof of Lemma \ref{lem.cauchy-binet.EI}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.sorting.nmu}}

Before we start solving Exercise \ref{exe.sorting.nmu}, let us isolate a
useful fact that was proven in our above proof of Proposition
\ref{prop.sorting} \textbf{(b)}:

\begin{lemma}
\label{lem.sorting.nmu.uniprop}Let $n\in\mathbb{N}$. Let $a_{1},a_{2}%
,\ldots,a_{n}$ be $n$ integers. Let $\sigma\in S_{n}$ be such that
$a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }\leq\cdots\leq
a_{\sigma\left(  n\right)  }$. Let $i\in\left\{  1,2,\ldots,n\right\}  $.
Then,%
\begin{equation}
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  \label{eq.lem.sorting.nmu.uniprop.claim}%
\end{equation}
(and, in particular, the right hand side of
(\ref{eq.lem.sorting.nmu.uniprop.claim}) is well-defined).
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sorting.nmu.uniprop}.]The equality
(\ref{eq.lem.sorting.nmu.uniprop.claim}) is precisely the equality
(\ref{pf.prop.sorting.bclaim}) that was proven during our above proof of
Proposition \ref{prop.sorting} \textbf{(b)}. Hence, we do not need to prove it
again. Thus, Lemma \ref{lem.sorting.nmu.uniprop} is proven.
\end{proof}

\begin{proof}
[First solution to Exercise \ref{exe.sorting.nmu}.]Let $i\in\left\{
1,2,\ldots,m\right\}  $. We must prove that $a_{\sigma\left(  i\right)  }\leq
b_{\tau\left(  i\right)  }$.

We have $n\geq m$, so that $m\leq n$. Now, $i\in\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $ (since $m\leq n$). Hence,
$\sigma\left(  i\right)  $ is well-defined (since $\sigma$ is a map $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $ (since
$\sigma\in S_{n}$)).

Define a subset $X$ of $\mathbb{Z}$ by%
\begin{equation}
X=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }a_{j}\leq x\right\}  .
\label{sol.sorting.nmu.X=}%
\end{equation}
Define a subset $Y$ of $\mathbb{Z}$ by%
\begin{equation}
Y=\left\{  x\in\mathbb{Z}\ \mid\ \text{at least }i\text{ elements }%
j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy }b_{j}\leq x\right\}  .
\label{sol.sorting.nmu.Y=}%
\end{equation}


Lemma \ref{lem.sorting.nmu.uniprop} yields that
\begin{equation}
a_{\sigma\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  \label{sol.sorting.nmu.uniprop1}%
\end{equation}
(and, in particular, the right hand side of (\ref{sol.sorting.nmu.uniprop1})
is well-defined). Thus,%
\begin{align}
a_{\sigma\left(  i\right)  }  &  =\min\underbrace{\left\{  x\in\mathbb{Z}%
\ \mid\ \text{at least }i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}
\text{ satisfy }a_{j}\leq x\right\}  }_{\substack{=X\\\text{(by
(\ref{sol.sorting.nmu.X=}))}}}\nonumber\\
&  =\min X. \label{sol.sorting.nmu.uniprop1'}%
\end{align}


But $\tau\in S_{m}$ satisfies $b_{\tau\left(  1\right)  }\leq b_{\tau\left(
2\right)  }\leq\cdots\leq b_{\tau\left(  m\right)  }$, and we have
$i\in\left\{  1,2,\ldots,m\right\}  $. Thus, Lemma
\ref{lem.sorting.nmu.uniprop} (applied to $m$, $b_{k}$ and $\tau$ instead of
$n$, $a_{k}$ and $\sigma$) yields that%
\begin{equation}
b_{\tau\left(  i\right)  }=\min\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy }%
b_{j}\leq x\right\}  \label{sol.sorting.nmu.uniprop2}%
\end{equation}
(and, in particular, the right hand side of (\ref{sol.sorting.nmu.uniprop2})
is well-defined). Thus,%
\begin{align}
b_{\tau\left(  i\right)  }  &  =\min\underbrace{\left\{  x\in\mathbb{Z}%
\ \mid\ \text{at least }i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}
\text{ satisfy }b_{j}\leq x\right\}  }_{\substack{=Y\\\text{(by
(\ref{sol.sorting.nmu.Y=}))}}}\nonumber\\
&  =\min Y. \label{sol.sorting.nmu.uniprop2'}%
\end{align}


\begin{vershort}
But clearly, $\min Y\in Y$. Hence,
\[
b_{\tau\left(  i\right)  }=\min Y\in Y=\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy
}b_{j}\leq x\right\}  .
\]
In other words, $b_{\tau\left(  i\right)  }$ is an element of $\mathbb{Z}$
such that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $ satisfy
$b_{j}\leq b_{\tau\left(  i\right)  }$.
\end{vershort}

\begin{verlong}
But if $S$ is a subset of $\mathbb{Z}$ for which $\min S$ is well-defined,
then $\min S\in S$ (since the minimum of a set must lie inside the set).
Applying this to $S=Y$, we obtain $\min Y\in Y$. Now,
(\ref{sol.sorting.nmu.uniprop2'}) becomes%
\[
b_{\tau\left(  i\right)  }=\min Y\in Y=\left\{  x\in\mathbb{Z}\ \mid\ \text{at
least }i\text{ elements }j\in\left\{  1,2,\ldots,m\right\}  \text{ satisfy
}b_{j}\leq x\right\}  .
\]
In other words, $b_{\tau\left(  i\right)  }$ is an element $x$ of $\mathbb{Z}$
such that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $ satisfy
$b_{j}\leq x$. In other words, $b_{\tau\left(  i\right)  }$ is an element of
$\mathbb{Z}$ such that at least $i$ elements $j\in\left\{  1,2,\ldots
,m\right\}  $ satisfy $b_{j}\leq b_{\tau\left(  i\right)  }$.
\end{verlong}

\begin{vershort}
We now know that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $
satisfy $b_{j}\leq b_{\tau\left(  i\right)  }$. Each of these $i$ elements $j$
must also be an element of $\left\{  1,2,\ldots,n\right\}  $ (since
$j\in\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $)
which satisfies $a_{j}\leq b_{\tau\left(  i\right)  }$ (because
(\ref{eq.exe.sorting.nmu.ass}) (applied to $j$ instead of $i$) shows that
$a_{j}\leq b_{j}\leq b_{\tau\left(  i\right)  }$). Thus, at least $i$ elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq b_{\tau\left(
i\right)  }$.
\end{vershort}

\begin{verlong}
We now know that at least $i$ elements $j\in\left\{  1,2,\ldots,m\right\}  $
satisfy $b_{j}\leq b_{\tau\left(  i\right)  }$. In other words,%
\[
\left\vert \left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \right\vert \geq i.
\]


But $\left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \subseteq\left\{  j\in\left\{
1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $g\in\left\{  j\in\left\{
1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq b_{\tau\left(  i\right)  }\right\}  $.
We shall prove that $g\in\left\{  j\in\left\{  1,2,\ldots,m\right\}
\ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}  $.
\par
We have $g\in\left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  $. In other words, $g$ is an element $j$
of $\left\{  1,2,\ldots,m\right\}  $ satisfying $b_{j}\leq b_{\tau\left(
i\right)  }$. In other words, $g$ is an element of $\left\{  1,2,\ldots
,m\right\}  $ satisfying $b_{g}\leq b_{\tau\left(  i\right)  }$. Now,
$g\in\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $.
Also, (\ref{eq.exe.sorting.nmu.ass}) (applied to $g$ instead of $i$) shows
that $a_{g}\leq b_{g}\leq b_{\tau\left(  i\right)  }$. Hence, $g$ is an
element of $\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{g}\leq
b_{\tau\left(  i\right)  }$. In other words, $g$ is an element $j$ of
$\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{j}\leq b_{\tau\left(
i\right)  }$. In other words, $g\in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}  $.
\par
Now, forget that we fixed $g$. We thus have shown that $g\in\left\{
j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ a_{j}\leq b_{\tau\left(  i\right)
}\right\}  $ for each $g\in\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ b_{j}\leq b_{\tau\left(  i\right)  }\right\}  $. In other words,
$\left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq b_{\tau\left(
i\right)  }\right\}  \subseteq\left\{  j\in\left\{  1,2,\ldots,n\right\}
\ \mid\ a_{j}\leq b_{\tau\left(  i\right)  }\right\}  $. Qed.}. Hence,%
\[
\left\vert \left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid\ b_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \right\vert \leq\left\vert \left\{
j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq b_{\tau\left(  i\right)
}\right\}  \right\vert .
\]
Thus,%
\begin{align*}
&  \left\vert \left\{  j\in\left\{  1,2,\ldots,n\right\}  \ \mid\ a_{j}\leq
b_{\tau\left(  i\right)  }\right\}  \right\vert \\
&  \geq\left\vert \left\{  j\in\left\{  1,2,\ldots,m\right\}  \ \mid
\ b_{j}\leq b_{\tau\left(  i\right)  }\right\}  \right\vert \geq i.
\end{align*}
In other words, at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfy $a_{j}\leq b_{\tau\left(  i\right)  }$.
\end{verlong}

Now, we know that $b_{\tau\left(  i\right)  }$ is an element of $\mathbb{Z}$
such that at least $i$ elements $j\in\left\{  1,2,\ldots,n\right\}  $ satisfy
$a_{j}\leq b_{\tau\left(  i\right)  }$. In other words, $b_{\tau\left(
i\right)  }$ is an element $x$ of $\mathbb{Z}$ such that at least $i$ elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfy $a_{j}\leq x$. In other words,%
\[
b_{\tau\left(  i\right)  }\in\left\{  x\in\mathbb{Z}\ \mid\ \text{at least
}i\text{ elements }j\in\left\{  1,2,\ldots,n\right\}  \text{ satisfy }%
a_{j}\leq x\right\}  .
\]
In light of (\ref{sol.sorting.nmu.X=}), this rewrites as $b_{\tau\left(
i\right)  }\in X$.

\begin{vershort}
But every $s\in X$ satisfies $\min X\leq s$ (since the minimum of a set is
$\leq$ to each element of this set). Applying this to $s=b_{\tau\left(
i\right)  }$, we obtain $\min X\leq b_{\tau\left(  i\right)  }$ (since
$b_{\tau\left(  i\right)  }\in X$). Thus, $b_{\tau\left(  i\right)  }\geq\min
X=a_{\sigma\left(  i\right)  }$ (by (\ref{sol.sorting.nmu.uniprop1'})). This
solves Exercise \ref{exe.sorting.nmu}.
\end{vershort}

\begin{verlong}
But if $S$ is a subset of $\mathbb{Z}$ for which $\min S$ is well-defined,
then every $s\in S$ satisfies $\min S\leq s$ (since the minimum of a set is
$\leq$ to each element of this set). Applying this to $S=X$, we conclude that
every $s\in X$ satisfies $\min X\leq s$. Applying this to $s=b_{\tau\left(
i\right)  }$, we obtain $\min X\leq b_{\tau\left(  i\right)  }$ (since
$b_{\tau\left(  i\right)  }\in X$). Thus, $b_{\tau\left(  i\right)  }\geq\min
X=a_{\sigma\left(  i\right)  }$ (by (\ref{sol.sorting.nmu.uniprop1'})). In
other words, $a_{\sigma\left(  i\right)  }\leq b_{\tau\left(  i\right)  }$.
This solves Exercise \ref{exe.sorting.nmu}.
\end{verlong}
\end{proof}

In order to give a second solution to Exercise \ref{exe.sorting.nmu}, let us
prove a lemma:

\begin{lemma}
\label{lem.sorting.nmu.stronger}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$ be
such that $n\geq m$. Let $a_{1},a_{2},\ldots,a_{n}$ be $n$ integers. Let
$b_{1},b_{2},\ldots,b_{m}$ be $m$ integers. Assume that%
\begin{equation}
a_{i}\leq b_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,m\right\}  . \label{eq.lem.sorting.nmu.stronger.ass1}%
\end{equation}


Let $\sigma\in S_{n}$ and $\tau\in S_{m}$. Let $i\in\left\{  1,2,\ldots
,m\right\}  $. Assume that%
\begin{equation}
a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }v\in\left\{  i,i+1,\ldots,n\right\}  .
\label{eq.lem.sorting.nmu.stronger.ass2}%
\end{equation}
Furthermore, assume that%
\begin{equation}
b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  1,2,\ldots,i\right\}  .
\label{eq.lem.sorting.nmu.stronger.ass3}%
\end{equation}
Then, $a_{\sigma\left(  i\right)  }\leq b_{\tau\left(  i\right)  }$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sorting.nmu.stronger}.]We have $i\in\left\{
1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $ (since $m\leq
n$).

We have $\sigma\in S_{n}$. Thus, $\sigma$ is a permutation of $\left\{
1,2,\ldots,n\right\}  $, hence an injective map. Thus, the $n$ integers
$\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  $ are distinct. In particular, the $n-i+1$ integers $\sigma\left(
i\right)  ,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  $ are distinct.

Also, $\tau\in S_{m}$. Therefore, $\tau$ is a permutation of $\left\{
1,2,\ldots,m\right\}  $, hence an injective map. Therefore, the $m$ integers
$\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  m\right)  $
are distinct. In particular, the $i$ integers $\tau\left(  1\right)
,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  $ are distinct.

If $A$ and $B$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$, then%
\begin{equation}
A\cap B\neq\varnothing\label{pf.lem.sorting.nmu.stronger.short.AcutB}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sorting.nmu.stronger.short.AcutB}):}
Let $A$ and $B$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$. We must prove that
$A\cap B\neq\varnothing$.
\par
Indeed, assume the contrary. Thus, $A\cap B=\varnothing$. We know that $A$ and
$B$ are subsets of $\left\{  1,2,\ldots,n\right\}  $. Hence, $A\cup B$ is a
subset of $\left\{  1,2,\ldots,n\right\}  $. Thus, $\left\vert A\cup
B\right\vert \leq\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert =n$.
But the sets $A$ and $B$ are disjoint (since $A\cap B=\varnothing$), and thus
we have $\left\vert A\cup B\right\vert =\left\vert A\right\vert +\left\vert
B\right\vert $ (since the size of the union of two disjoint sets is the sum of
their sizes). Thus, $\left\vert A\right\vert +\left\vert B\right\vert
=\left\vert A\cup B\right\vert \leq n$. This contradicts $\left\vert
A\right\vert +\left\vert B\right\vert >n$. This contradiction proves that our
assumption was wrong. Hence, $A\cap B\neq\varnothing$ is proven. This proves
(\ref{pf.lem.sorting.nmu.stronger.short.AcutB}).}.

Now, let $A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $ and $B=\left\{  \tau\left(
1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \right\}  $.

Clearly,
\begin{equation}
A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)  ,\ldots
,\sigma\left(  n\right)  \right\}  \subseteq\left\{  1,2,\ldots,n\right\}
\label{pf.lem.sorting.nmu.stronger.short.subset1}%
\end{equation}
(since $\sigma$ is a map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $) and $B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $ (since $\tau$ is a map $\left\{  1,2,\ldots,m\right\}
\rightarrow\left\{  1,2,\ldots,m\right\}  $). Thus,
\begin{equation}
B\subseteq\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  . \label{pf.lem.sorting.nmu.stronger.short.subset2}%
\end{equation}


Now, from (\ref{pf.lem.sorting.nmu.stronger.short.subset1}) and
(\ref{pf.lem.sorting.nmu.stronger.short.subset2}), we see that $A$ and $B$ are
two subsets of $\left\{  1,2,\ldots,n\right\}  $. These two subsets satisfy%
\begin{align*}
&  \left\vert \underbrace{A}_{=\left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  }\right\vert +\left\vert
\underbrace{B}_{=\left\{  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  i\right)  \right\}  }\right\vert \\
&  =\underbrace{\left\vert \left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  \right\vert
}_{\substack{=n-i+1\\\text{(since the }n-i+1\text{ integers}\\\sigma\left(
i\right)  ,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \text{
are distinct)}}}+\underbrace{\left\vert \left\{  \tau\left(  1\right)
,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \right\vert
}_{\substack{=i\\\text{(since the }i\text{ integers}\\\tau\left(  1\right)
,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \text{ are distinct)}}}\\
&  =n-i+1+i=n+1>n.
\end{align*}
Hence, (\ref{pf.lem.sorting.nmu.stronger.short.AcutB}) shows that $A\cap
B\neq\varnothing$. In other words, there exists a $g\in A\cap B$. Consider
this $g$.

We have $g\in A\cap B\subseteq B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $. Hence, $b_{g}$ is well-defined. Also, $g\in\left\{
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)
\right\}  $. In other words, there exists a $u\in\left\{  1,2,\ldots
,i\right\}  $ satisfying $g=\tau\left(  u\right)  $. Consider this $u$. From
(\ref{eq.lem.sorting.nmu.stronger.ass3}), we obtain $b_{\tau\left(  u\right)
}\leq b_{\tau\left(  i\right)  }$. But $g=\tau\left(  u\right)  $ shows that
$b_{g}=b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }$.

On the other hand, $g\in A\cap B\subseteq A=\left\{  \sigma\left(  i\right)
,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{g}$ is well-defined.
Also, $g\in\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $. In other words, there exists a
$v\in\left\{  i,i+1,\ldots,n\right\}  $ such that $g=\sigma\left(  v\right)
$. Consider this $v$. From (\ref{eq.lem.sorting.nmu.stronger.ass2}), we obtain
$a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }=a_{g}$ (since
$\sigma\left(  v\right)  =g$).

But (\ref{eq.lem.sorting.nmu.stronger.ass1}) (applied to $g$ instead of $i$)
shows that $a_{g}\leq b_{g}$ (since $g\in\left\{  1,2,\ldots,m\right\}  $).
Hence, $a_{\sigma\left(  i\right)  }\leq a_{g}\leq b_{g}\leq b_{\tau\left(
i\right)  }$. This proves Lemma \ref{lem.sorting.nmu.stronger}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sorting.nmu.stronger}.]We have $\sigma\in S_{n}$. In
other words, $\sigma$ belongs to the set $S_{n}$. In other words, $\sigma$
belongs to the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of the set
$\left\{  1,2,\ldots,n\right\}  $). In other words, $\sigma$ is a permutation
of the set $\left\{  1,2,\ldots,n\right\}  $. In other words, $\sigma$ is a
bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. The map $\sigma$ is thus bijective, hence both
surjective and injective.

We have $\tau\in S_{m}$. In other words, $\tau$ belongs to the set $S_{m}$. In
other words, $\tau$ belongs to the set of all permutations of the set
$\left\{  1,2,\ldots,m\right\}  $ (since $S_{m}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,m\right\}  $). In other words,
$\tau$ is a permutation of the set $\left\{  1,2,\ldots,m\right\}  $. In other
words, $\tau$ is a bijective map $\left\{  1,2,\ldots,m\right\}
\rightarrow\left\{  1,2,\ldots,m\right\}  $. The map $\tau$ is thus bijective,
hence both surjective and injective.

We have $n\geq m$, thus $m\leq n$, hence $\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Now, $i\in\left\{  1,2,\ldots
,m\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $; thus, $\sigma\left(
i\right)  $ is well-defined.

If $A$ and $B$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$, then%
\begin{equation}
A\cap B\neq\varnothing\label{pf.lem.sorting.nmu.stronger.AcutB}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sorting.nmu.stronger.AcutB}):} Let $A$
and $B$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert A\right\vert +\left\vert B\right\vert >n$. We must prove that
$A\cap B\neq\varnothing$.
\par
Indeed, assume the contrary. Thus, $A\cap B=\varnothing$. We know that $A$ and
$B$ are subsets of $\left\{  1,2,\ldots,n\right\}  $. Hence, $A\cup B$ is a
subset of $\left\{  1,2,\ldots,n\right\}  $. In other words, $A\cup
B\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, $\left\vert A\cup
B\right\vert \leq\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert =n$.
But the sets $A$ and $B$ are disjoint (since $A\cap B=\varnothing$), and thus
we have $\left\vert A\cup B\right\vert =\left\vert A\right\vert +\left\vert
B\right\vert $ (since the size of the union of two disjoint sets is the sum of
their sizes). Thus, $\left\vert A\right\vert +\left\vert B\right\vert
=\left\vert A\cup B\right\vert \leq n$. This contradicts $\left\vert
A\right\vert +\left\vert B\right\vert >n$. This contradiction proves that our
assumption was wrong. Hence, $A\cap B\neq\varnothing$ is proven. This proves
(\ref{pf.lem.sorting.nmu.stronger.AcutB}).}.

Now, let $A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $ and $B=\left\{  \tau\left(
1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)  \right\}  $.

Clearly,
\begin{equation}
A=\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)  ,\ldots
,\sigma\left(  n\right)  \right\}  \subseteq\left\{  1,2,\ldots,n\right\}
\label{pf.lem.sorting.nmu.stronger.subset1}%
\end{equation}
(since $\sigma$ is a map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $) and $B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $ (since $\tau$ is a map $\left\{  1,2,\ldots,m\right\}
\rightarrow\left\{  1,2,\ldots,m\right\}  $). Thus,
\begin{equation}
B\subseteq\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  . \label{pf.lem.sorting.nmu.stronger.subset2}%
\end{equation}


The $n-i+1$ integers $\sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  $ are pairwise
distinct\footnote{\textit{Proof.} Assume the contrary. Thus, the $n-i+1$
integers $\sigma\left(  i\right)  ,\sigma\left(  i+1\right)  ,\ldots
,\sigma\left(  n\right)  $ are \textbf{not} pairwise distinct. In other words,
two of these integers are equal. In other words, there exist two distinct
elements $u$ and $v$ of $\left\{  i,i+1,\ldots,n\right\}  $ such that
$\sigma\left(  u\right)  =\sigma\left(  v\right)  $. Consider these $u$ and
$v$.
\par
From $\sigma\left(  u\right)  =\sigma\left(  v\right)  $, we obtain $u=v$
(since the map $\sigma$ is injective). This contradicts the assumption that
$u$ and $v$ are distinct. This contradiction shows that our assumption was
wrong, qed.}. Hence,
\[
\left\vert \left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  \right\vert =n-i+1.
\]


The $i$ integers $\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots
,\tau\left(  i\right)  $ are pairwise distinct\footnote{\textit{Proof.} Assume
the contrary. Thus, the $i$ integers $\tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  $ are \textbf{not} pairwise distinct.
In other words, two of these integers are equal. In other words, there exist
two distinct elements $u$ and $v$ of $\left\{  1,2,\ldots,i\right\}  $ such
that $\tau\left(  u\right)  =\tau\left(  v\right)  $. Consider these $u$ and
$v$.
\par
From $\tau\left(  u\right)  =\tau\left(  v\right)  $, we obtain $u=v$ (since
the map $\tau$ is injective). This contradicts the assumption that $u$ and $v$
are distinct. This contradiction shows that our assumption was wrong, qed.}.
Hence,
\[
\left\vert \left\{  \tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots
,\tau\left(  i\right)  \right\}  \right\vert =i.
\]


Now, from (\ref{pf.lem.sorting.nmu.stronger.subset1}) and
(\ref{pf.lem.sorting.nmu.stronger.subset2}), we see that $A$ and $B$ are two
subsets of $\left\{  1,2,\ldots,n\right\}  $. These two subsets satisfy%
\begin{align*}
&  \left\vert \underbrace{A}_{=\left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  }\right\vert +\left\vert
\underbrace{B}_{=\left\{  \tau\left(  1\right)  ,\tau\left(  2\right)
,\ldots,\tau\left(  i\right)  \right\}  }\right\vert \\
&  =\underbrace{\left\vert \left\{  \sigma\left(  i\right)  ,\sigma\left(
i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}  \right\vert }%
_{=n-i+1}+\underbrace{\left\vert \left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \right\vert }_{=i}\\
&  =n-i+1+i=n+1>n.
\end{align*}
Hence, (\ref{pf.lem.sorting.nmu.stronger.AcutB}) shows that $A\cap
B\neq\varnothing$. In other words, the set $A\cap B$ is nonempty. In other
words, the set $A\cap B$ has at least one element. In other words, there
exists a $g\in A\cap B$. Consider this $g$.

We have $g\in A\cap B\subseteq B=\left\{  \tau\left(  1\right)  ,\tau\left(
2\right)  ,\ldots,\tau\left(  i\right)  \right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $. Hence, $b_{g}$ is well-defined. Also, $g\in\left\{
\tau\left(  1\right)  ,\tau\left(  2\right)  ,\ldots,\tau\left(  i\right)
\right\}  $. In other words, there exists a $u\in\left\{  1,2,\ldots
,i\right\}  $ satisfying $g=\tau\left(  u\right)  $. Consider this $u$. From
(\ref{eq.lem.sorting.nmu.stronger.ass3}), we obtain $b_{\tau\left(  u\right)
}\leq b_{\tau\left(  i\right)  }$. But $g=\tau\left(  u\right)  $ shows that
$b_{g}=b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }$.

On the other hand, $g\in A\cap B\subseteq A=\left\{  \sigma\left(  i\right)
,\sigma\left(  i+1\right)  ,\ldots,\sigma\left(  n\right)  \right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{g}$ is well-defined.
Also, $g\in\left\{  \sigma\left(  i\right)  ,\sigma\left(  i+1\right)
,\ldots,\sigma\left(  n\right)  \right\}  $. In other words, there exists a
$v\in\left\{  i,i+1,\ldots,n\right\}  $ such that $g=\sigma\left(  v\right)
$. Consider this $v$. From (\ref{eq.lem.sorting.nmu.stronger.ass2}), we obtain
$a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }=a_{g}$ (since
$\sigma\left(  v\right)  =g$).

But (\ref{eq.lem.sorting.nmu.stronger.ass1}) (applied to $g$ instead of $i$)
shows that $a_{g}\leq b_{g}$ (since $g\in\left\{  1,2,\ldots,m\right\}  $).
Hence, $a_{\sigma\left(  i\right)  }\leq a_{g}\leq b_{g}\leq b_{\tau\left(
i\right)  }$. This proves Lemma \ref{lem.sorting.nmu.stronger}.
\end{proof}
\end{verlong}

\begin{proof}
[Second solution to Exercise \ref{exe.sorting.nmu}.]Let $i\in\left\{
1,2,\ldots,m\right\}  $. We must prove that $a_{\sigma\left(  i\right)  }\leq
b_{\tau\left(  i\right)  }$.

We have $a_{\sigma\left(  1\right)  }\leq a_{\sigma\left(  2\right)  }%
\leq\cdots\leq a_{\sigma\left(  n\right)  }$. In other words, for every
$u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{  1,2,\ldots,n\right\}
$ satisfying $u\leq v$, we have%
\begin{equation}
a_{\sigma\left(  u\right)  }\leq a_{\sigma\left(  v\right)  }.
\label{sol.sorting.nmu.sol2.1}%
\end{equation}
Also, we have $b_{\tau\left(  1\right)  }\leq b_{\tau\left(  2\right)  }%
\leq\cdots\leq b_{\tau\left(  m\right)  }$. In other words, for every
$u\in\left\{  1,2,\ldots,m\right\}  $ and $v\in\left\{  1,2,\ldots,m\right\}
$ satisfying $u\leq v$, we have%
\begin{equation}
b_{\tau\left(  u\right)  }\leq b_{\tau\left(  v\right)  }.
\label{sol.sorting.nmu.sol2.2}%
\end{equation}


Now,%
\[
a_{\sigma\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }v\in\left\{  i,i+1,\ldots,n\right\}
\]
\footnote{\textit{Proof.} Let $v\in\left\{  i,i+1,\ldots,n\right\}  $. Then,
$v\geq i$, so that $i\leq v$. Also, $i\in\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $ (since $m\leq n$ (since $n\geq m$))
and $v\in\left\{  i,i+1,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $ (since $i\geq1$ (since $i\in\left\{  1,2,\ldots,m\right\}  $)).
Hence, (\ref{sol.sorting.nmu.sol2.1}) (applied to $u=i$) yields $a_{\sigma
\left(  i\right)  }\leq a_{\sigma\left(  v\right)  }$. Qed.}. Furthermore,%
\[
b_{\tau\left(  u\right)  }\leq b_{\tau\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  1,2,\ldots,i\right\}
\]
\footnote{\textit{Proof.} Let $u\in\left\{  1,2,\ldots,i\right\}  $. Then,
$u\leq i$. Also, $u\in\left\{  1,2,\ldots,i\right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $ (since $i\leq m$ (since $i\in\left\{  1,2,\ldots
,m\right\}  $)) and $i\in\left\{  1,2,\ldots,m\right\}  $. Hence,
(\ref{sol.sorting.nmu.sol2.2}) (applied to $v=i$) yields $b_{\tau\left(
u\right)  }\leq b_{\tau\left(  i\right)  }$. Qed.}. Hence, Lemma
\ref{lem.sorting.nmu.stronger} shows that $a_{\sigma\left(  i\right)  }\leq
b_{\tau\left(  i\right)  }$. This solves Exercise \ref{exe.sorting.nmu} again.
\end{proof}

\subsection{Solution to Exercise \ref{exe.subsect.vandermonde.factoring}}

Definition \ref{def.vandermonde.factoring.h} shall be used throughout this section.

\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.h.0}.]\textbf{(a)} Let $k$ be a
negative integer. Then,%
\begin{equation}
h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n};\\a_{1}+a_{2}+\cdots
+a_{n}=k}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}
\label{pf.lem.vandermonde.factoring.h.0.a.1}%
\end{equation}
(by the definition of $h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $).

\begin{vershort}
But $k<0$. Hence, there exists no $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{N}^{n}$ satisfying $a_{1}+a_{2}+\cdots+a_{n}=k$ (because every
$\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}$ satisfies
$a_{1}+a_{2}+\cdots+a_{n}\geq0$, whereas $k<0$). Therefore, the sum on the
right hand side of (\ref{pf.lem.vandermonde.factoring.h.0.a.1}) is an empty
sum and therefore equals $0$. Therefore,
(\ref{pf.lem.vandermonde.factoring.h.0.a.1}) simplifies to $h_{k}\left(
x_{1},x_{2},\ldots,x_{n}\right)  =0$. This proves Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(a)}.
\end{vershort}

\begin{verlong}
But there exists no $\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in
\mathbb{N}^{n}$ satisfying $a_{1}+a_{2}+\cdots+a_{n}=k$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  \in\mathbb{N}^{n}$ be such that $a_{1}+a_{2}+\cdots+a_{n}=k$.
We shall derive a contradiction.
\par
We have $\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}$. In other
words, $a_{1},a_{2},\ldots,a_{n}$ are $n$ elements of $\mathbb{N}$. In other
words, $a_{1},a_{2},\ldots,a_{n}$ are $n$ nonnegative integers. Hence, their
sum $a_{1}+a_{2}+\cdots+a_{n}$ also is a nonnegative integer. Thus,
$a_{1}+a_{2}+\cdots+a_{n}\geq0$. But $a_{1}+a_{2}+\cdots+a_{n}=k<0$ (since $k$
is negative). This contradicts $a_{1}+a_{2}+\cdots+a_{n}\geq0$.
\par
Now, forget that we fixed $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $. We thus
have derived a contradiction for each $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  \in\mathbb{N}^{n}$ satisfying $a_{1}+a_{2}+\cdots+a_{n}=k$.
Hence, there exists no $\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in
\mathbb{N}^{n}$ satisfying $a_{1}+a_{2}+\cdots+a_{n}=k$. Qed.}. Hence, the sum
$\sum_{\substack{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}%
^{n};\\a_{1}+a_{2}+\cdots+a_{n}=k}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots
x_{n}^{a_{n}}$ is an empty sum. Thus,
\[
\sum_{\substack{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}%
^{n};\\a_{1}+a_{2}+\cdots+a_{n}=k}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots
x_{n}^{a_{n}}=\left(  \text{empty sum}\right)  =0.
\]
Hence, (\ref{pf.lem.vandermonde.factoring.h.0.a.1}) becomes%
\[
h_{k}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n};\\a_{1}+a_{2}+\cdots
+a_{n}=k}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}=0.
\]
This proves Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)}.
\end{verlong}

\textbf{(b)} The definition of $h_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)
$ yields
\begin{equation}
h_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n};\\a_{1}+a_{2}+\cdots
+a_{n}=0}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}.
\label{pf.lem.vandermonde.factoring.h.0.b.1}%
\end{equation}


\begin{vershort}
But the only $\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}$
satisfying $a_{1}+a_{2}+\cdots+a_{n}=0$ is $\left(  \underbrace{0,0,\ldots
,0}_{n\text{ zeroes}}\right)  $ (since a sum of nonnegative integers can only
be $0$ if all addends are $0$). Therefore, the sum on the right hand side of
(\ref{pf.lem.vandermonde.factoring.h.0.b.1}) has exactly one addend -- namely,
the addend for $\left(  a_{1},a_{2},\ldots,a_{n}\right)  =\left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  $. Thus,
(\ref{pf.lem.vandermonde.factoring.h.0.b.1}) simplifies to $h_{0}\left(
x_{1},x_{2},\ldots,x_{n}\right)  =\underbrace{x_{1}^{0}}_{=1}\underbrace{x_{2}%
^{0}}_{=1}\cdots\underbrace{x_{n}^{0}}_{=1}=1$. This proves Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(b)}.
\end{vershort}

\begin{verlong}
Define a subset $A$ of $\mathbb{N}^{n}$ by%
\begin{equation}
A=\left\{  \left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}%
\ \mid\ a_{1}+a_{2}+\cdots+a_{n}=0\right\}  .
\label{pf.lem.vandermonde.factoring.h.0.b.defA}%
\end{equation}
Thus,%
\begin{equation}
\sum_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in A}=\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n};\\a_{1}+a_{2}+\cdots
+a_{n}=0}} \label{pf.lem.vandermonde.factoring.h.0.b.sums1}%
\end{equation}
(an equality between summation signs). But $A=\left\{  \left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \right\}  $%
\ \ \ \ \footnote{\textit{Proof.} The $n$-tuple $\left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  $ belongs to
$\mathbb{N}^{n}$ and satisfies $\underbrace{0+0+\cdots+0}_{n\text{ zeroes}}%
=0$. In other words, $\left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}%
}\right)  $ is an element $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{N}^{n}$ satisfying $a_{1}+a_{2}+\cdots+a_{n}=0$. In other words,%
\[
\left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \in\left\{
\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}\ \mid\ a_{1}%
+a_{2}+\cdots+a_{n}=0\right\}  .
\]
In light of (\ref{pf.lem.vandermonde.factoring.h.0.b.defA}), this rewrites as
$\left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \in A$. Thus,
$\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)
\right\}  \subseteq A$.
\par
Now, let $\mathbf{a}\in A$. We shall show that $\mathbf{a}=\left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  $.
\par
Indeed, $\mathbf{a}\in A=\left\{  \left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{N}^{n}\ \mid\ a_{1}+a_{2}+\cdots+a_{n}=0\right\}  $. In other
words, $\mathbf{a}$ is an element $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{N}^{n}$ satisfying $a_{1}+a_{2}+\cdots+a_{n}=0$. In other words,
$\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  $ for some $\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}$ satisfying $a_{1}%
+a_{2}+\cdots+a_{n}=0$. Consider this $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  $.
\par
We have $\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}$. In other
words, $a_{1},a_{2},\ldots,a_{n}$ are $n$ elements of $\mathbb{N}$.
\par
Fix $i\in\left\{  1,2,\ldots,n\right\}  $. We shall show that $a_{i}=0$.
\par
Splitting off the addend for $k=i$ from the sum $\sum_{k\in\left\{
1,2,\ldots,n\right\}  }a_{k}$, we obtain%
\[
\sum_{k\in\left\{  1,2,\ldots,n\right\}  }a_{k}=a_{i}+\sum_{\substack{k\in
\left\{  1,2,\ldots,n\right\}  ;\\k\neq i}}\underbrace{a_{k}}_{\substack{\geq
0\\\text{(since }a_{k}\in\mathbb{N}\\\text{(since }a_{1},a_{2},\ldots
,a_{n}\\\text{are }n\text{ elements of }\mathbb{N}\text{))}}}\geq
a_{i}+\underbrace{\sum_{\substack{k\in\left\{  1,2,\ldots,n\right\}  ;\\k\neq
i}}0}_{=0}=a_{i}.
\]
Thus, $a_{i}\leq\sum_{k\in\left\{  1,2,\ldots,n\right\}  }a_{k}=a_{1}%
+a_{2}+\cdots+a_{n}=0$. But $a_{i}\in\mathbb{N}$ (since $a_{1},a_{2}%
,\ldots,a_{n}$ are $n$ elements of $\mathbb{N}$), so that $a_{i}\geq0$.
Combining this with $a_{i}\leq0$, we obtain $a_{i}=0$.
\par
Now, forget that we fixed $i$. We thus have shown that $a_{i}=0$ for each
$i\in\left\{  1,2,\ldots,n\right\}  $. In other words, $\left(  a_{1}%
,a_{2},\ldots,a_{n}\right)  =\left(  \underbrace{0,0,\ldots,0}_{n\text{
zeroes}}\right)  $. Thus,
\[
\mathbf{a}=\left(  a_{1},a_{2},\ldots,a_{n}\right)  =\left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \in\left\{  \left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \right\}  .
\]
\par
Now, forget that we fixed $\mathbf{a}$. We thus have proven that
$\mathbf{a}\in\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}%
}\right)  \right\}  $ for each $\mathbf{a}\in A$. In other words,
$A\subseteq\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}%
}\right)  \right\}  $. Combining this with $\left\{  \left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \right\}  \subseteq A$, we
obtain $A=\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)
\right\}  $. Qed.}.

From $A=\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)
\right\}  $, we obtain
\[
\sum_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in A}=\sum_{\left(
a_{1},a_{2},\ldots,a_{n}\right)  \in\left\{  \left(  \underbrace{0,0,\ldots
,0}_{n\text{ zeroes}}\right)  \right\}  }%
\]
(an equality between summation signs). Comparing this with
(\ref{pf.lem.vandermonde.factoring.h.0.b.sums1}), we obtain%
\[
\sum_{\substack{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}%
^{n};\\a_{1}+a_{2}+\cdots+a_{n}=0}}=\sum_{\left(  a_{1},a_{2},\ldots
,a_{n}\right)  \in\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}%
}\right)  \right\}  }%
\]
(an equality between summation signs). Now,
(\ref{pf.lem.vandermonde.factoring.h.0.b.1}) becomes%
\begin{align*}
h_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)   &  =\underbrace{\sum
_{\substack{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\mathbb{N}^{n}%
;\\a_{1}+a_{2}+\cdots+a_{n}=0}}}_{=\sum_{\left(  a_{1},a_{2},\ldots
,a_{n}\right)  \in\left\{  \left(  \underbrace{0,0,\ldots,0}_{n\text{ zeroes}%
}\right)  \right\}  }}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}\\
&  =\sum_{\left(  a_{1},a_{2},\ldots,a_{n}\right)  \in\left\{  \left(
\underbrace{0,0,\ldots,0}_{n\text{ zeroes}}\right)  \right\}  }x_{1}^{a_{1}%
}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}=x_{1}^{0}x_{2}^{0}\cdots x_{n}^{0}%
=\prod_{i=1}^{n}\underbrace{x_{i}^{0}}_{=1}\\
&  =\prod_{i=1}^{n}1=1.
\end{align*}
This proves Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(b)}.
\end{verlong}
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.h.hq1}.]The definition of
$h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)  $ yields%
\begin{align}
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{k}\right)  \in\mathbb{N}^{k};\\a_{1}+a_{2}+\cdots
+a_{k}=q}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{k}^{a_{k}}\nonumber\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}%
^{k};\\s_{1}+s_{2}+\cdots+s_{k}=q}}x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots
x_{k}^{s_{k}} \label{pf.lem.vandermonde.factoring.h.hq1.hq=}%
\end{align}
(here, we have renamed the summation index $\left(  a_{1},a_{2},\ldots
,a_{k}\right)  $ as $\left(  s_{1},s_{2},\ldots,s_{k}\right)  $).

Every $r\in\mathbb{Z}$ satisfies%
\begin{align}
h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)   &  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{k-1}\right)  \in\mathbb{N}^{k-1};\\a_{1}+a_{2}%
+\cdots+a_{k-1}=q-r}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{k-1}^{a_{k-1}%
}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }h_{q-r}\left(
x_{1},x_{2},\ldots,x_{k-1}\right)  \right) \nonumber\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{k-1}\right)  \in
\mathbb{N}^{k-1};\\s_{1}+s_{2}+\cdots+s_{k-1}=q-r}}x_{1}^{s_{1}}x_{2}^{s_{2}%
}\cdots x_{k-1}^{s_{k-1}} \label{pf.lem.vandermonde.factoring.h.hq1.hq-r=}%
\end{align}
(here, we have renamed the summation index $\left(  a_{1},a_{2},\ldots
,a_{k-1}\right)  $ as $\left(  s_{1},s_{2},\ldots,s_{k-1}\right)  $).
Moreover, every $r\in\mathbb{Z}$ satisfying $r>q$ satisfies%
\begin{equation}
h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  =0
\label{pf.lem.vandermonde.factoring.h.hq1.hq-r=0}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.vandermonde.factoring.h.hq1.hq-r=0}):}
Let $r\in\mathbb{Z}$ be such that $r>q$. The integer $q-r$ is negative (since
$r>q$). Hence, Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied
to $k-1$ and $q-r$ instead of $n$ and $k$) yields $h_{q-r}\left(  x_{1}%
,x_{2},\ldots,x_{k-1}\right)  =0$. This proves
(\ref{pf.lem.vandermonde.factoring.h.hq1.hq-r=0}).}.

But Lemma \ref{lem.prodrule.prod-assM} (applied to $M=k$ and $Z_{i}%
=\mathbb{N}$) shows that the map%
\begin{align*}
\underbrace{\mathbb{N}\times\mathbb{N}\times\cdots\times\mathbb{N}}_{k\text{
factors}}  &  \rightarrow\left(  \underbrace{\mathbb{N}\times\mathbb{N}%
\times\cdots\times\mathbb{N}}_{k-1\text{ factors}}\right)  \times\mathbb{N},\\
\left(  s_{1},s_{2},\ldots,s_{k}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{k-1}\right)  ,s_{k}\right)
\end{align*}
is a bijection. Since $\underbrace{\mathbb{N}\times\mathbb{N}\times
\cdots\times\mathbb{N}}_{k\text{ factors}}=\mathbb{N}^{k}$ and
$\underbrace{\mathbb{N}\times\mathbb{N}\times\cdots\times\mathbb{N}%
}_{k-1\text{ factors}}=\mathbb{N}^{k-1}$, this can be rewritten as follows:
The map%
\begin{align*}
\mathbb{N}^{k}  &  \rightarrow\mathbb{N}^{k-1}\times\mathbb{N},\\
\left(  s_{1},s_{2},\ldots,s_{k}\right)   &  \mapsto\left(  \left(
s_{1},s_{2},\ldots,s_{k-1}\right)  ,s_{k}\right)
\end{align*}
is a bijection.

\begin{vershort}
Now, (\ref{pf.lem.vandermonde.factoring.h.hq1.hq=}) becomes%
\begin{align*}
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\underbrace{\sum
_{\substack{\left(  s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}^{k}%
;\\s_{1}+s_{2}+\cdots+s_{k}=q}}}_{\substack{=\sum_{\substack{\left(
s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}^{k};\\s_{1}+s_{2}%
+\cdots+s_{k-1}=q-s_{k}}}\\\text{(because for any }\left(  s_{1},s_{2}%
,\ldots,s_{k}\right)  \in\mathbb{N}^{k}\text{,}\\\text{the condition }%
s_{1}+s_{2}+\cdots+s_{k}=q\\\text{is equivalent to }s_{1}+s_{2}+\cdots
+s_{k-1}=q-s_{k}\text{)}}}\underbrace{x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots
x_{k}^{s_{k}}}_{=\left(  x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}%
}\right)  x_{k}^{s_{k}}}\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}%
^{k};\\s_{1}+s_{2}+\cdots+s_{k-1}=q-s_{k}}}\left(  x_{1}^{s_{1}}x_{2}^{s_{2}%
}\cdots x_{k-1}^{s_{k-1}}\right)  x_{k}^{s_{k}}\\
&  =\underbrace{\sum_{\substack{\left(  \left(  s_{1},s_{2},\ldots
,s_{k-1}\right)  ,r\right)  \in\mathbb{N}^{k-1}\times\mathbb{N};\\s_{1}%
+s_{2}+\cdots+s_{k-1}=q-r}}}_{=\sum_{r\in\mathbb{N}}\sum_{\substack{\left(
s_{1},s_{2},\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\s_{1}+s_{2}%
+\cdots+s_{k-1}=q-r}}}\left(  x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}%
^{s_{k-1}}\right)  x_{k}^{r}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  \left(  s_{1},s_{2},\ldots
,s_{k-1}\right)  ,r\right)  \text{ for}\\
\left(  \left(  s_{1},s_{2},\ldots,s_{k-1}\right)  ,s_{k}\right)  \text{,
since the map}\\
\mathbb{N}^{k}\rightarrow\mathbb{N}^{k-1}\times\mathbb{N},\ \left(
s_{1},s_{2},\ldots,s_{k}\right)  \mapsto\left(  \left(  s_{1},s_{2}%
,\ldots,s_{k-1}\right)  ,s_{k}\right) \\
\text{is a bijection}%
\end{array}
\right)
\end{align*}%
\begin{align*}
&  =\sum_{r\in\mathbb{N}}\sum_{\substack{\left(  s_{1},s_{2},\ldots
,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\s_{1}+s_{2}+\cdots+s_{k-1}=q-r}}\left(
x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}\right)  x_{k}^{r}\\
&  =\sum_{r\in\mathbb{N}}x_{k}^{r}\underbrace{\sum_{\substack{\left(
s_{1},s_{2},\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\s_{1}+s_{2}%
+\cdots+s_{k-1}=q-r}}x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}%
}_{\substack{=h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.hq1.hq-r=}))}}}\\
&  =\sum_{r\in\mathbb{N}}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots
,x_{k-1}\right) \\
&  =\underbrace{\sum_{\substack{r\in\mathbb{N};\\r\leq q}}}_{=\sum_{r=0}^{q}%
}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +\sum
_{\substack{r\in\mathbb{N};\\r>q}}x_{k}^{r}\underbrace{h_{q-r}\left(
x_{1},x_{2},\ldots,x_{k-1}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.hq1.hq-r=0}))}}}\\
&  =\sum_{r=0}^{q}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
+\underbrace{\sum_{\substack{r\in\mathbb{N};\\r>q}}x_{k}^{r}0}_{=0}=\sum
_{r=0}^{q}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  .
\end{align*}
This proves Lemma \ref{lem.vandermonde.factoring.h.hq1}.
\end{vershort}

\begin{verlong}
Now, (\ref{pf.lem.vandermonde.factoring.h.hq1.hq=}) becomes%
\begin{align*}
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\underbrace{\sum
_{\substack{\left(  s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}^{k}%
;\\s_{1}+s_{2}+\cdots+s_{k}=q}}}_{\substack{=\sum_{\substack{\left(
s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}^{k};\\\left(  s_{1}%
+s_{2}+\cdots+s_{k-1}\right)  +s_{k}=q}}\\\text{(since }s_{1}+s_{2}%
+\cdots+s_{k}=\left(  s_{1}+s_{2}+\cdots+s_{k-1}\right)  +s_{k}\\\text{for
every }\left(  s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}^{k}\text{)}%
}}\underbrace{x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k}^{s_{k}}}_{=\left(
x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}\right)  x_{k}^{s_{k}}}\\
&  =\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{k}\right)  \in\mathbb{N}%
^{k};\\\left(  s_{1}+s_{2}+\cdots+s_{k-1}\right)  +s_{k}=q}}\left(
x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}\right)  x_{k}^{s_{k}}\\
&  =\underbrace{\sum_{\substack{\left(  \left(  s_{1},s_{2},\ldots
,s_{k-1}\right)  ,r\right)  \in\mathbb{N}^{k-1}\times\mathbb{N};\\\left(
s_{1}+s_{2}+\cdots+s_{k-1}\right)  +r=q}}}_{\substack{=\sum_{\left(
s_{1},s_{2},\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1}}\sum_{\substack{r\in
\mathbb{N};\\\left(  s_{1}+s_{2}+\cdots+s_{k-1}\right)  +r=q}}\\=\sum
_{r\in\mathbb{N}}\sum_{\substack{\left(  s_{1},s_{2},\ldots,s_{k-1}\right)
\in\mathbb{N}^{k-1};\\\left(  s_{1}+s_{2}+\cdots+s_{k-1}\right)  +r=q}%
}}}\left(  x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}\right)
x_{k}^{r}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  \left(  s_{1},s_{2},\ldots
,s_{k-1}\right)  ,r\right)  \text{ for}\\
\left(  \left(  s_{1},s_{2},\ldots,s_{k-1}\right)  ,s_{k}\right)  \text{,
since the map}\\
\mathbb{N}^{k}\rightarrow\mathbb{N}^{k-1}\times\mathbb{N},\ \left(
s_{1},s_{2},\ldots,s_{k}\right)  \mapsto\left(  \left(  s_{1},s_{2}%
,\ldots,s_{k-1}\right)  ,s_{k}\right) \\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{r\in\mathbb{N}}\underbrace{\sum_{\substack{\left(  s_{1}%
,s_{2},\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\\left(  s_{1}+s_{2}%
+\cdots+s_{k-1}\right)  +r=q}}}_{\substack{=\sum_{\substack{\left(
s_{1},s_{2},\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\s_{1}+s_{2}%
+\cdots+s_{k-1}=q-r}}\\\text{(because for each }\left(  s_{1},s_{2}%
,\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1}\text{,}\\\text{the condition
}\left(  s_{1}+s_{2}+\cdots+s_{k-1}\right)  +r=q\\\text{is equivalent to
}s_{1}+s_{2}+\cdots+s_{k-1}=q-r\text{)}}}\left(  x_{1}^{s_{1}}x_{2}^{s_{2}%
}\cdots x_{k-1}^{s_{k-1}}\right)  x_{k}^{r}%
\end{align*}%
\begin{align*}
&  =\sum_{r\in\mathbb{N}}\sum_{\substack{\left(  s_{1},s_{2},\ldots
,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\s_{1}+s_{2}+\cdots+s_{k-1}=q-r}}\left(
x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}\right)  x_{k}^{r}\\
&  =\sum_{r\in\mathbb{N}}x_{k}^{r}\underbrace{\sum_{\substack{\left(
s_{1},s_{2},\ldots,s_{k-1}\right)  \in\mathbb{N}^{k-1};\\s_{1}+s_{2}%
+\cdots+s_{k-1}=q-r}}x_{1}^{s_{1}}x_{2}^{s_{2}}\cdots x_{k-1}^{s_{k-1}}%
}_{\substack{=h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.hq1.hq-r=}))}}}\\
&  =\sum_{r\in\mathbb{N}}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots
,x_{k-1}\right) \\
&  =\underbrace{\sum_{\substack{r\in\mathbb{N};\\r\leq q}}}_{=\sum_{r=0}^{q}%
}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +\sum
_{\substack{r\in\mathbb{N};\\r>q}}x_{k}^{r}\underbrace{h_{q-r}\left(
x_{1},x_{2},\ldots,x_{k-1}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.hq1.hq-r=0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }r\in\mathbb{N}\text{ satisfies either }r\leq q\text{ or
}r>q\\
\text{(but not both)}%
\end{array}
\right) \\
&  =\sum_{r=0}^{q}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
+\underbrace{\sum_{\substack{r\in\mathbb{N};\\r>q}}x_{k}^{r}0}_{=0}=\sum
_{r=0}^{q}x_{k}^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  .
\end{align*}
This proves Lemma \ref{lem.vandermonde.factoring.h.hq1}.
\end{verlong}
\end{proof}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.h.hq2}.]If $q<0$, then Lemma
\ref{lem.vandermonde.factoring.h.hq2} holds\footnote{\textit{Proof.} Assume
that $q<0$. Thus, both $q-1$ and $q$ are negative integers. Hence, Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied to $q-1$ and $k$
instead of $k$ and $n$) yields $h_{q-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  =0$. Also, Lemma \ref{lem.vandermonde.factoring.h.0}
\textbf{(a)} (applied to $q$ and $k-1$ instead of $k$ and $n$) yields
$h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  =0$.
\par
Now, Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied to $q$
and $k$ instead of $k$ and $n$) yields $h_{q}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  =0$. Comparing this with%
\[
\underbrace{h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  }_{=0}%
+x_{k}\underbrace{h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  }%
_{=0}=0+x_{k}0=0,
\]
we obtain
\[
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)  =h_{q}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  +x_{k}h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
.
\]
Thus, Lemma \ref{lem.vandermonde.factoring.h.hq2} holds. Qed.}. Hence, for the
rest of this proof, we WLOG assume that we don't have $q<0$. Thus, $q\geq0$.

Lemma \ref{lem.vandermonde.factoring.h.hq1} (applied to $q-1$ instead of $q$)
yields%
\begin{align}
h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\sum_{r=0}^{q-1}x_{k}%
^{r}h_{\left(  q-1\right)  -r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
\nonumber\\
&  =\sum_{r=1}^{q}x_{k}^{r-1}\underbrace{h_{\left(  q-1\right)  -\left(
r-1\right)  }\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  }_{\substack{=h_{q-r}%
\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \\\text{(since }\left(  q-1\right)
-\left(  r-1\right)  =q-r\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }r-1\text{ for
}r\text{ in the sum}\right) \nonumber\\
&  =\sum_{r=1}^{q}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
. \label{pf.lem.vandermonde.factoring.h.hq2.short.1}%
\end{align}
But Lemma \ref{lem.vandermonde.factoring.h.hq1} yields%
\begin{align*}
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\sum_{r=0}^{q}x_{k}%
^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right) \\
&  =\underbrace{x_{k}^{0}}_{=1}\underbrace{h_{q-0}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  }_{=h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
}+\sum_{r=1}^{q}\underbrace{x_{k}^{r}}_{=x_{k}x_{k}^{r-1}}h_{q-r}\left(
x_{1},x_{2},\ldots,x_{k-1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }r=0\text{ from}\\
\text{the sum (since }q\geq0\text{)}%
\end{array}
\right) \\
&  =h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +\underbrace{\sum
_{r=1}^{q}x_{k}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
}_{=x_{k}\sum_{r=1}^{q}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots
,x_{k-1}\right)  }\\
&  =h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +x_{k}\underbrace{\sum
_{r=1}^{q}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
}_{\substack{=h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.hq2.short.1}))}}}\\
&  =h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +x_{k}h_{q-1}\left(
x_{1},x_{2},\ldots,x_{k}\right)  .
\end{align*}
This proves Lemma \ref{lem.vandermonde.factoring.h.hq2}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.h.hq2}.]If $q<0$, then Lemma
\ref{lem.vandermonde.factoring.h.hq2} holds\footnote{\textit{Proof.} Assume
that $q<0$. Thus, $q-1<q<0$ as well. In other words, $q-1$ is a negative
integer. Hence, Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)}
(applied to $q-1$ and $k$ instead of $k$ and $n$) yields $h_{q-1}\left(
x_{1},x_{2},\ldots,x_{k}\right)  =0$. Also, $q$ is a negative integer (since
$q<0$). Furthermore, $k-1\in\mathbb{N}$ (since $k$ is a positive integer).
Hence, Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied to $q$
and $k-1$ instead of $k$ and $n$) yields $h_{q}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  =0$.
\par
Now, Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied to $q$
and $k$ instead of $k$ and $n$) yields $h_{q}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  =0$. Comparing this with%
\[
\underbrace{h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  }_{=0}%
+x_{k}\underbrace{h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  }%
_{=0}=0+x_{k}0=0,
\]
we obtain
\[
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)  =h_{q}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  +x_{k}h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
.
\]
Thus, Lemma \ref{lem.vandermonde.factoring.h.hq2} holds. Qed.}. Hence, for the
rest of this proof, we can WLOG assume that we don't have $q<0$. Assume this.

We have $q\geq0$ (since we don't have $q<0$). Thus, $q\in\mathbb{N}$.

Lemma \ref{lem.vandermonde.factoring.h.hq1} (applied to $q-1$ instead of $q$)
yields%
\begin{align}
h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\sum_{r=0}^{q-1}x_{k}%
^{r}h_{\left(  q-1\right)  -r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
\nonumber\\
&  =\sum_{r=1}^{q}x_{k}^{r-1}\underbrace{h_{\left(  q-1\right)  -\left(
r-1\right)  }\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  }_{\substack{=h_{q-r}%
\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \\\text{(since }\left(  q-1\right)
-\left(  r-1\right)  =q-r\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }r-1\text{ for
}r\text{ in the sum}\right) \nonumber\\
&  =\sum_{r=1}^{q}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
. \label{pf.lem.vandermonde.factoring.h.hq2.1}%
\end{align}
But Lemma \ref{lem.vandermonde.factoring.h.hq1} yields%
\begin{align*}
h_{q}\left(  x_{1},x_{2},\ldots,x_{k}\right)   &  =\sum_{r=0}^{q}x_{k}%
^{r}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right) \\
&  =\underbrace{x_{k}^{0}}_{=1}\underbrace{h_{q-0}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  }_{\substack{=h_{q}\left(  x_{1},x_{2},\ldots
,x_{k-1}\right)  \\\text{(since }q-0=q\text{)}}}+\sum_{r=1}^{q}%
\underbrace{x_{k}^{r}}_{=x_{k}x_{k}^{r-1}}h_{q-r}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }r=0\text{ from the sum}\\
\text{(since }q\geq0\text{)}%
\end{array}
\right) \\
&  =h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +\underbrace{\sum
_{r=1}^{q}x_{k}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
}_{=x_{k}\sum_{r=1}^{q}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots
,x_{k-1}\right)  }\\
&  =h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +x_{k}\underbrace{\sum
_{r=1}^{q}x_{k}^{r-1}h_{q-r}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
}_{\substack{=h_{q-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.hq2.1}))}}}\\
&  =h_{q}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +x_{k}h_{q-1}\left(
x_{1},x_{2},\ldots,x_{k}\right)  .
\end{align*}
This proves Lemma \ref{lem.vandermonde.factoring.h.hq2}.
\end{proof}
\end{verlong}

\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.h.sum-u}.]We first notice that
every positive integer $q$ satisfies%
\begin{equation}
h_{q}\left(  x_{1},x_{2},\ldots,x_{0}\right)  =0.
\label{pf.lem.vandermonde.factoring.h.sum-u.0}%
\end{equation}
(Here, as usual, $\left(  x_{1},x_{2},\ldots,x_{0}\right)  $ stands for the
$0$-tuple $\left(  {}\right)  $.)

\begin{vershort}
[\textit{Proof of (\ref{pf.lem.vandermonde.factoring.h.sum-u.0}):} Let $q$ be
a positive integer. The definition of $h_{q}\left(  x_{1},x_{2},\ldots
,x_{0}\right)  $ yields%
\begin{equation}
h_{q}\left(  x_{1},x_{2},\ldots,x_{0}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0};\\a_{1}+a_{2}+\cdots
+a_{0}=q}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{0}^{a_{0}}.
\label{pf.lem.vandermonde.factoring.h.sum-u.0.pf.short.0}%
\end{equation}
But there is only one $0$-tuple $\left(  a_{1},a_{2},\ldots,a_{0}\right)
\in\mathbb{N}^{0}$, and this $0$-tuple satisfies $a_{1}+a_{2}+\cdots
+a_{0}=\left(  \text{empty sum}\right)  =0\neq q$ (since $q$ is positive).
Hence, there exists no $\left(  a_{1},a_{2},\ldots,a_{0}\right)  \in
\mathbb{N}^{0}$ satisfying $a_{1}+a_{2}+\cdots+a_{0}=q$. Thus, the sum on the
right hand side of (\ref{pf.lem.vandermonde.factoring.h.sum-u.0.pf.short.0})
is empty and equals $0$. Therefore,
(\ref{pf.lem.vandermonde.factoring.h.sum-u.0.pf.short.0}) simplifies to
$h_{q}\left(  x_{1},x_{2},\ldots,x_{0}\right)  =0$. This proves
(\ref{pf.lem.vandermonde.factoring.h.sum-u.0}).]
\end{vershort}

\begin{verlong}
[\textit{Proof of (\ref{pf.lem.vandermonde.factoring.h.sum-u.0}):} Let $q$ be
a positive integer. The definition of $h_{q}\left(  x_{1},x_{2},\ldots
,x_{0}\right)  $ yields%
\begin{equation}
h_{q}\left(  x_{1},x_{2},\ldots,x_{0}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0};\\a_{1}+a_{2}+\cdots
+a_{0}=q}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{0}^{a_{0}}.
\label{pf.lem.vandermonde.factoring.h.sum-u.0.pf.0}%
\end{equation}
But there exists no $\left(  a_{1},a_{2},\ldots,a_{0}\right)  \in
\mathbb{N}^{0}$ satisfying $a_{1}+a_{2}+\cdots+a_{0}=q$%
\ \ \ \ \footnote{\textit{Proof.} We have $q\neq0$ (since $q$ is positive).
Every $\left(  a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0}$ satisfies
$a_{1}+a_{2}+\cdots+a_{0}=\left(  \text{empty sum}\right)  =0\neq q$. Hence,
no $\left(  a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0}$ satisfies
$a_{1}+a_{2}+\cdots+a_{0}=q$. In other words, there exists no $\left(
a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0}$ satisfying $a_{1}%
+a_{2}+\cdots+a_{0}=q$. Qed.}. Hence, the sum $\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0};\\a_{1}+a_{2}+\cdots
+a_{0}=q}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{0}^{a_{0}}$ is empty. Thus,%
\[
\sum_{\substack{\left(  a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}%
^{0};\\a_{1}+a_{2}+\cdots+a_{0}=q}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots
x_{0}^{a_{0}}=\left(  \text{empty sum}\right)  =0.
\]
Thus, (\ref{pf.lem.vandermonde.factoring.h.sum-u.0.pf.0}) becomes%
\[
h_{q}\left(  x_{1},x_{2},\ldots,x_{0}\right)  =\sum_{\substack{\left(
a_{1},a_{2},\ldots,a_{0}\right)  \in\mathbb{N}^{0};\\a_{1}+a_{2}+\cdots
+a_{0}=q}}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{0}^{a_{0}}=0.
\]
This proves (\ref{pf.lem.vandermonde.factoring.h.sum-u.0.pf.0}).]
\end{verlong}

We shall now show that%
\begin{equation}
\sum_{k=1}^{j}h_{j-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right)  =u^{j-1}
\label{pf.lem.vandermonde.factoring.h.sum-u.main}%
\end{equation}
for each $j\in\left\{  1,2,\ldots,i\right\}  $.

\textit{Proof of (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}):} We shall
prove (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) by induction over $j$:

\begin{vershort}
\textit{Induction base:} We have%
\begin{align*}
\sum_{k=1}^{1}h_{1-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right)   &  =\underbrace{h_{1-1}\left(
x_{1},x_{2},\ldots,x_{1}\right)  }_{\substack{=h_{0}\left(  x_{1},x_{2}%
,\ldots,x_{1}\right)  =1\\\text{(by Lemma \ref{lem.vandermonde.factoring.h.0}
\textbf{(b)}}\\\text{(applied to }1\text{ instead of }n\text{))}%
}}\underbrace{\prod_{p=1}^{1-1}\left(  u-x_{p}\right)  }_{=\left(  \text{empty
product}\right)  =1}\\
&  =1\cdot1=1=u^{1-1}%
\end{align*}
(since $u^{1-1}=u^{0}=1$). In other words,
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) holds for $j=1$. This
completes the induction base.
\end{vershort}

\begin{verlong}
\textit{Induction base:} We have%
\begin{align*}
\sum_{k=1}^{1}h_{1-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right)   &  =\underbrace{h_{1-1}\left(
x_{1},x_{2},\ldots,x_{1}\right)  }_{\substack{=h_{0}\left(  x_{1},x_{2}%
,\ldots,x_{1}\right)  \\\text{(since }1-1=0\text{)}}}\underbrace{\prod
_{p=1}^{1-1}\left(  u-x_{p}\right)  }_{\substack{=\left(  \text{empty
product}\right)  \\\text{(since }1-1<1\text{)}}}\\
&  =\underbrace{h_{0}\left(  x_{1},x_{2},\ldots,x_{1}\right)  }%
_{\substack{=1\\\text{(by Lemma \ref{lem.vandermonde.factoring.h.0}
\textbf{(b)}}\\\text{(applied to }1\text{ instead of }n\text{))}}%
}\cdot\underbrace{\left(  \text{empty product}\right)  }_{=1}\\
&  =1\cdot1=1.
\end{align*}
Comparing this with $u^{1-1}=u^{0}=1$, we obtain%
\[
\sum_{k=1}^{1}h_{1-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right)  =u^{1-1}.
\]
In other words, (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) holds for
$j=1$. This completes the induction base.
\end{verlong}

\textit{Induction step:} Let $m\in\left\{  1,2,\ldots,i\right\}  $ be such
that $m>1$. Assume that (\ref{pf.lem.vandermonde.factoring.h.sum-u.main})
holds for $j=m-1$. We must prove that
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) holds for $j=m$.

We have assumed that (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) holds
for $j=m-1$. In other words, we have%
\begin{equation}
\sum_{k=1}^{m-1}h_{\left(  m-1\right)  -k}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  =u^{\left(
m-1\right)  -1}. \label{pf.lem.vandermonde.factoring.h.sum-u.main.pf.indhyp}%
\end{equation}


\begin{vershort}
Now, $m>1\geq0$. Moreover,
\begin{align}
&  \sum_{k=1}^{m}\underbrace{h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)
}_{\substack{=h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +x_{k}%
h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \\\text{(by Lemma
\ref{lem.vandermonde.factoring.h.hq2} (applied to }q=m-k\text{))}}}\prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\sum_{k=1}^{m}\left(  h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
+x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=1}^{m}x_{k}h_{m-k-1}\left(  x_{1},x_{2}%
,\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  .
\label{pf.lem.vandermonde.factoring.h.sum-u.main.pf.short.1}%
\end{align}

\end{vershort}

\begin{verlong}
Now, $m>1\geq0$. Moreover,
\begin{align}
&  \sum_{k=1}^{m}\underbrace{h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)
}_{\substack{=h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  +x_{k}%
h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \\\text{(by Lemma
\ref{lem.vandermonde.factoring.h.hq2} (applied to }q=m-k\text{))}}}\prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\sum_{k=1}^{m}\underbrace{\left(  h_{m-k}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  +x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  }%
_{=h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \prod_{p=1}^{k-1}\left(
u-x_{p}\right)  +x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right)  }\nonumber\\
&  =\sum_{k=1}^{m}\left(  h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right)  +x_{k}h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \right)
\nonumber\\
&  =\sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=1}^{m}x_{k}h_{m-k-1}\left(  x_{1},x_{2}%
,\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  .
\label{pf.lem.vandermonde.factoring.h.sum-u.main.pf.1}%
\end{align}

\end{verlong}

We have $h_{m-1}\left(  x_{1},x_{2},\ldots,x_{1-1}\right)  =0$%
\ \ \ \ \footnote{\textit{Proof.} We have $m>1$, and thus $m-1>0$. Hence,
$m-1$ is a positive integer. Thus,
(\ref{pf.lem.vandermonde.factoring.h.sum-u.0}) (applied to $q=m-1$) yields
$h_{m-1}\left(  x_{1},x_{2},\ldots,x_{0}\right)  =0$. Now, $1-1=0$ and thus
$h_{m-1}\left(  x_{1},x_{2},\ldots,x_{1-1}\right)  =h_{m-1}\left(  x_{1}%
,x_{2},\ldots,x_{0}\right)  =0$. Qed.}. But $m>1$. Hence, we can split off the
addend for $k=1$ from the sum $\sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2}%
,\ldots,x_{k-1}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  $. We thus
obtain
\begin{align}
&  \sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\underbrace{h_{m-1}\left(  x_{1},x_{2},\ldots,x_{1-1}\right)  }_{=0}%
\prod_{p=1}^{1-1}\left(  u-x_{p}\right)  +\sum_{k=2}^{m}h_{m-k}\left(
x_{1},x_{2},\ldots,x_{k-1}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)
\nonumber\\
&  =\underbrace{0\prod_{p=1}^{1-1}\left(  u-x_{p}\right)  }_{=0}+\sum
_{k=2}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  \prod_{p=1}%
^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\sum_{k=2}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\sum_{k=1}^{m-1}\underbrace{h_{m-\left(  k+1\right)  }\left(  x_{1}%
,x_{2},\ldots,x_{\left(  k+1\right)  -1}\right)  }_{\substack{=h_{m-k-1}%
\left(  x_{1},x_{2},\ldots,x_{k}\right)  \\\text{(since }m-\left(  k+1\right)
=m-k-1\\\text{and }\left(  k+1\right)  -1=k\text{)}}}\underbrace{\prod
_{p=1}^{\left(  k+1\right)  -1}}_{\substack{=\prod_{p=1}^{k}\\\text{(since
}\left(  k+1\right)  -1=k\text{)}}}\left(  u-x_{p}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+1\text{ for
}k\text{ in the sum}\right) \nonumber\\
&  =\sum_{k=1}^{m-1}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\underbrace{\prod_{p=1}^{k}\left(  u-x_{p}\right)  }_{\substack{=\left(
u-x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \\\text{(here, we
have split off the factor for }p=k\text{ from}\\\text{the product (since }%
k\in\left\{  1,2,\ldots,k\right\}  \text{))}}}\nonumber\\
&  =\sum_{k=1}^{m-1}\underbrace{h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \cdot\left(  u-x_{k}\right)  }_{=\left(  u-x_{k}\right)
h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  }\prod_{p=1}^{k-1}\left(
u-x_{p}\right) \nonumber\\
&  =\sum_{k=1}^{m-1}\left(  u-x_{k}\right)  h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  .
\label{pf.lem.vandermonde.factoring.h.sum-u.main.pf.2}%
\end{align}
On the other hand, $h_{m-m-1}\left(  x_{1},x_{2},\ldots,x_{m}\right)
=0$\ \ \ \ \footnote{\textit{Proof.} Clearly, $m-m-1=-1$ is a negative
integer. Hence, Lemma \ref{lem.vandermonde.factoring.h.0} \textbf{(a)}
(applied to $n=m$ and $k=m-m-1$) yields $h_{m-m-1}\left(  x_{1},x_{2}%
,\ldots,x_{m}\right)  =0$. Qed.}. But $m>1$. Hence, we can split off the
addend for $k=m$ from the sum $\sum_{k=1}^{m}x_{k}h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  $. We
thus obtain%
\begin{align}
&  \sum_{k=1}^{m}x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =x_{m}\underbrace{h_{m-m-1}\left(  x_{1},x_{2},\ldots,x_{m}\right)  }%
_{=0}\prod_{p=1}^{m-1}\left(  u-x_{p}\right)  +\sum_{k=1}^{m-1}x_{k}%
h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(
u-x_{p}\right) \nonumber\\
&  =\underbrace{x_{m}0\prod_{p=1}^{m-1}\left(  u-x_{p}\right)  }_{=0}%
+\sum_{k=1}^{m-1}x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\sum_{k=1}^{m-1}x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right)  .
\label{pf.lem.vandermonde.factoring.h.sum-u.main.pf.3}%
\end{align}


\begin{vershort}
Now, (\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.short.1}) becomes%
\begin{align*}
&  \sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =\underbrace{\sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}%
\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  }_{\substack{=\sum
_{k=1}^{m-1}\left(  u-x_{k}\right)  h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{k=1}^{m}x_{k}h_{m-k-1}\left(
x_{1},x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)
}_{\substack{=\sum_{k=1}^{m-1}x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.3}))}}}\\
&  =\sum_{k=1}^{m-1}\left(  u-x_{k}\right)  h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=1}^{m-1}x_{k}h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =\sum_{k=1}^{m-1}\underbrace{\left(  \left(  u-x_{k}\right)  +x_{k}\right)
}_{=u}\underbrace{h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
}_{\substack{=h_{\left(  m-1\right)  -k}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \\\text{(since }m-k-1=\left(  m-1\right)  -k\text{)}}%
}\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =\sum_{k=1}^{m-1}uh_{\left(  m-1\right)  -k}\left(  x_{1},x_{2}%
,\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =u\underbrace{\sum_{k=1}^{m-1}h_{\left(  m-1\right)  -k}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)
}_{\substack{=u^{\left(  m-1\right)  -1}\\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.indhyp}))}}}=uu^{\left(
m-1\right)  -1}=u^{m-1}.
\end{align*}
In other words, (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) holds for
$j=m$. This completes the induction step.
\end{vershort}

\begin{verlong}
Now, (\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.1}) becomes%
\begin{align*}
&  \sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =\underbrace{\sum_{k=1}^{m}h_{m-k}\left(  x_{1},x_{2},\ldots,x_{k-1}%
\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  }_{\substack{=\sum
_{k=1}^{m-1}\left(  u-x_{k}\right)  h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{k=1}^{m}x_{k}h_{m-k-1}\left(
x_{1},x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)
}_{\substack{=\sum_{k=1}^{m-1}x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.3}))}}}\\
&  =\sum_{k=1}^{m-1}\left(  u-x_{k}\right)  h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=1}^{m-1}x_{k}h_{m-k-1}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =\sum_{k=1}^{m-1}\underbrace{\left(  \left(  u-x_{k}\right)  h_{m-k-1}%
\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(
u-x_{p}\right)  +x_{k}h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right)  \right)  }_{=\left(  \left(
u-x_{k}\right)  +x_{k}\right)  h_{m-k-1}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)  }\\
&  =\sum_{k=1}^{m-1}\underbrace{\left(  \left(  u-x_{k}\right)  +x_{k}\right)
}_{=u}\underbrace{h_{m-k-1}\left(  x_{1},x_{2},\ldots,x_{k}\right)
}_{\substack{=h_{\left(  m-1\right)  -k}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \\\text{(since }m-k-1=\left(  m-1\right)  -k\text{)}}%
}\prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =\sum_{k=1}^{m-1}uh_{\left(  m-1\right)  -k}\left(  x_{1},x_{2}%
,\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right) \\
&  =u\underbrace{\sum_{k=1}^{m-1}h_{\left(  m-1\right)  -k}\left(  x_{1}%
,x_{2},\ldots,x_{k}\right)  \prod_{p=1}^{k-1}\left(  u-x_{p}\right)
}_{\substack{=u^{\left(  m-1\right)  -1}\\\text{(by
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main.pf.indhyp}))}}}=uu^{\left(
m-1\right)  -1}=u^{m-1}.
\end{align*}
In other words, (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) holds for
$j=m$. This completes the induction step.
\end{verlong}

Thus, we have proven (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) by induction.

\begin{vershort}
Lemma \ref{lem.vandermonde.factoring.h.sum-u} now follows from
(\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) (applied to $j=i$).
\end{vershort}

\begin{verlong}
Now, (\ref{pf.lem.vandermonde.factoring.h.sum-u.main}) (applied to $j=i$)
yields%
\[
\sum_{k=1}^{i}h_{i-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right)  =u^{i-1}.
\]
This proves Lemma \ref{lem.vandermonde.factoring.h.sum-u}.
\end{verlong}
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.U}.]For every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, set
\begin{equation}
a_{i,j}=\prod_{p=1}^{j-1}\left(  x_{i}-x_{p}\right)  .
\label{pf.lem.vandermonde.factoring.U.1}%
\end{equation}
Recall that $U=\left(  \prod_{p=1}^{i-1}\left(  x_{j}-x_{p}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the definition of $U^{T}$ yields%
\[
U^{T}=\left(  \underbrace{\prod_{p=1}^{j-1}\left(  x_{i}-x_{p}\right)
}_{\substack{=a_{i,j}\\\text{(by (\ref{pf.lem.vandermonde.factoring.U.1}))}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}.
\]


\begin{vershort}
We have $a_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ be such that
$i<j$. We want to show that $a_{i,j}=0$.
\par
We have $i<j$, and thus $i\leq j-1$ (since $i$ and $j$ are integers). Thus,
$i\in\left\{  1,2,\ldots,j-1\right\}  $. Thus, the product $\prod_{p=1}%
^{j-1}\left(  x_{i}-x_{p}\right)  $ has a factor for $p=i$. This factor is
$x_{i}-x_{i}=0$. Hence, at least one factor of the product $\prod_{p=1}%
^{j-1}\left(  x_{i}-x_{p}\right)  $ equals $0$ (namely, the factor for $p=i$).
Thus, the whole product $\prod_{p=1}^{j-1}\left(  x_{i}-x_{p}\right)  $ equals
$0$ (because if at least one factor of a product equals $0$, then the whole
product must equal $0$). In other words, $\prod_{p=1}^{j-1}\left(  x_{i}%
-x_{p}\right)  =0$.
\par
But the definition of $a_{i,j}$ yields $a_{i,j}=\prod_{p=1}^{j-1}\left(
x_{i}-x_{p}\right)  =0$. Qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to
$U^{T}$ instead of $A$) yields
\begin{align*}
\det\left(  U^{T}\right)   &  =a_{1,1}a_{2,2}\cdots a_{n,n}=\prod_{i=1}%
^{n}\underbrace{a_{i,i}}_{\substack{=\prod_{p=1}^{i-1}\left(  x_{i}%
-x_{p}\right)  \\\text{(by the definition of }a_{i,i}\text{)}}%
}=\underbrace{\prod_{i=1}^{n}\prod_{p=1}^{i-1}}_{=\prod_{1\leq p<i\leq n}%
}\left(  x_{i}-x_{p}\right) \\
&  =\prod_{1\leq p<i\leq n}\left(  x_{i}-x_{p}\right)  =\prod_{1\leq j<i\leq
n}\left(  x_{i}-x_{j}\right)
\end{align*}
(here, we have renamed the index $\left(  p,i\right)  $ as $\left(
j,i\right)  $ in the product).
\end{vershort}

\begin{verlong}
We have $a_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ be such that
$i<j$. We want to show that $a_{i,j}=0$.
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. In other
words, $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,n\right\}  $. From $i\in\left\{  1,2,\ldots,n\right\}  $, we obtain $i\geq1$.
\par
We have $i<j$, and thus $i\leq j-1$ (since $i$ and $j$ are integers). Thus,
$i\in\left\{  1,2,\ldots,j-1\right\}  $ (since $i\geq1$ and $i\leq j-1$).
Thus, the product $\prod_{p=1}^{j-1}\left(  x_{i}-x_{p}\right)  $ has a factor
for $p=i$. This factor is $x_{i}-x_{i}=0$. Hence, at least one factor of the
product $\prod_{p=1}^{j-1}\left(  x_{i}-x_{p}\right)  $ equals $0$ (namely,
the factor for $p=i$). Thus, the whole product $\prod_{p=1}^{j-1}\left(
x_{i}-x_{p}\right)  $ equals $0$ (because if at least one factor of a product
equals $0$, then the whole product must equal $0$). In other words,
$\prod_{p=1}^{j-1}\left(  x_{i}-x_{p}\right)  =0$.
\par
But the definition of $a_{i,j}$ yields $a_{i,j}=\prod_{p=1}^{j-1}\left(
x_{i}-x_{p}\right)  =0$. Qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to
$U^{T}$ instead of $A$) yields
\begin{align*}
\det\left(  U^{T}\right)   &  =a_{1,1}a_{2,2}\cdots a_{n,n}=\prod_{i=1}%
^{n}\underbrace{a_{i,i}}_{\substack{=\prod_{p=1}^{i-1}\left(  x_{i}%
-x_{p}\right)  \\\text{(by the definition of }a_{i,i}\text{)}}%
}=\underbrace{\prod_{i=1}^{n}\prod_{p=1}^{i-1}}_{=\prod_{1\leq p<i\leq n}%
}\left(  x_{i}-x_{p}\right) \\
&  =\prod_{1\leq p<i\leq n}\left(  x_{i}-x_{p}\right)  =\prod_{1\leq j<i\leq
n}\left(  x_{i}-x_{j}\right)
\end{align*}
(here, we have renamed the index $\left(  p,i\right)  $ as $\left(
j,i\right)  $ in the product).
\end{verlong}

But Exercise \ref{exe.ps4.4} (applied to $A=U$) yields $\det\left(
U^{T}\right)  =\det U$. Hence,%
\[
\det U=\det\left(  U^{T}\right)  =\prod_{1\leq j<i\leq n}\left(  x_{i}%
-x_{j}\right)  .
\]
This proves Lemma \ref{lem.vandermonde.factoring.U}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.L}.]For every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, set
\begin{equation}
a_{i,j}=h_{i-j}\left(  x_{1},x_{2},\ldots,x_{j}\right)  .
\label{pf.lem.vandermonde.factoring.L.1}%
\end{equation}
Then,%
\[
L=\left(  \underbrace{h_{i-j}\left(  x_{1},x_{2},\ldots,x_{j}\right)
}_{\substack{=a_{i,j}\\\text{(by (\ref{pf.lem.vandermonde.factoring.L.1}))}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}.
\]


We have $a_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$ satisfying $i<j$\ \ \ \ \footnote{\textit{Proof.} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ be such that
$i<j$. We want to show that $a_{i,j}=0$.
\par
We have $i-j<0$ (since $i<j$). Thus, $i-j$ is a negative integer. Hence, Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied to $j$ and $i-j$
instead of $n$ and $k$) yields $h_{i-j}\left(  x_{1},x_{2},\ldots
,x_{j}\right)  =0$. But the definition of $a_{i,j}$ yields $a_{i,j}%
=h_{i-j}\left(  x_{1},x_{2},\ldots,x_{j}\right)  =0$. Qed.}. Hence, Exercise
\ref{exe.ps4.3} (applied to $L$ instead of $A$) yields
\begin{align*}
\det L  &  =a_{1,1}a_{2,2}\cdots a_{n,n}=\prod_{i=1}^{n}\underbrace{a_{i,i}%
}_{\substack{=h_{i-i}\left(  x_{1},x_{2},\ldots,x_{i}\right)  \\\text{(by the
definition of }a_{i,i}\text{)}}}=\prod_{i=1}^{n}\underbrace{h_{i-i}\left(
x_{1},x_{2},\ldots,x_{i}\right)  }_{\substack{=h_{0}\left(  x_{1},x_{2}%
,\ldots,x_{i}\right)  \\\text{(since }i-i=0\text{)}}}\\
&  =\prod_{i=1}^{n}\underbrace{h_{0}\left(  x_{1},x_{2},\ldots,x_{i}\right)
}_{\substack{=1\\\text{(by Lemma \ref{lem.vandermonde.factoring.h.0}
\textbf{(b)}}\\\text{(applied to }i\text{ instead of }n\text{))}}}=\prod
_{i=1}^{n}1=1.
\end{align*}
This proves Lemma \ref{lem.vandermonde.factoring.L}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.vandermonde.factoring.A=LU}.]If $i\in\mathbb{Z}$ and
$k\in\left\{  1,2,\ldots,n\right\}  $ are such that $k>i$, then%
\begin{equation}
h_{i-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  =0
\label{pf.lem.vandermonde.factoring.A=LU.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.vandermonde.factoring.A=LU.0}):} Let
$i\in\mathbb{Z}$ and $k\in\left\{  1,2,\ldots,n\right\}  $ be such that $k>i$.
Then, $i-k<0$ (since $k>i$). Thus, $i-k$ is a negative integer. Hence, Lemma
\ref{lem.vandermonde.factoring.h.0} \textbf{(a)} (applied to $k$ and $i-k$
instead of $n$ and $k$) yields $h_{i-k}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  =0$. This proves (\ref{pf.lem.vandermonde.factoring.A=LU.0}).}.

Now, for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $u\in\mathbb{K}$, we
have%
\begin{align}
&  \sum_{k=1}^{n}h_{i-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  \prod
_{p=1}^{k-1}\left(  u-x_{p}\right) \nonumber\\
&  =\underbrace{\sum_{k=1}^{i}h_{i-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)
\prod_{p=1}^{k-1}\left(  u-x_{p}\right)  }_{\substack{=u^{i-1}\\\text{(by
Lemma \ref{lem.vandermonde.factoring.h.sum-u})}}}+\sum_{k=i+1}^{n}%
\underbrace{h_{i-k}\left(  x_{1},x_{2},\ldots,x_{k}\right)  }%
_{\substack{=0\\\text{(by (\ref{pf.lem.vandermonde.factoring.A=LU.0}%
)}\\\text{(since }k\geq i+1>i\text{))}}}\prod_{p=1}^{k-1}\left(
u-x_{p}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq i\leq n\text{ (since }%
i\in\left\{  1,2,\ldots,n\right\}  \text{)}\right) \nonumber\\
&  =u^{i-1}+\underbrace{\sum_{k=i+1}^{n}0\prod_{p=1}^{k-1}\left(
u-x_{p}\right)  }_{=0}=u^{i-1}. \label{pf.lem.vandermonde.factoring.A=LU.2}%
\end{align}


But recall that $L=\left(  h_{i-j}\left(  x_{1},x_{2},\ldots,x_{j}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and \newline$U=\left(  \prod
_{p=1}^{i-1}\left(  x_{j}-x_{p}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Hence, the definition of the product $LU$ yields%
\[
LU=\left(  \underbrace{\sum_{k=1}^{n}h_{i-k}\left(  x_{1},x_{2},\ldots
,x_{k}\right)  \prod_{p=1}^{k-1}\left(  x_{j}-x_{p}\right)  }%
_{\substack{=x_{j}^{i-1}\\\text{(by (\ref{pf.lem.vandermonde.factoring.A=LU.2}%
) (applied to }u=x_{j}\text{))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  x_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
This proves Lemma \ref{lem.vandermonde.factoring.A=LU}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.subsect.vandermonde.factoring}.]We have proven
Lemma \ref{lem.vandermonde.factoring.h.0}, Lemma
\ref{lem.vandermonde.factoring.h.hq1}, Lemma
\ref{lem.vandermonde.factoring.h.hq2}, Lemma
\ref{lem.vandermonde.factoring.h.sum-u}, Lemma
\ref{lem.vandermonde.factoring.U}, Lemma \ref{lem.vandermonde.factoring.L} and
Lemma \ref{lem.vandermonde.factoring.A=LU}. Thus, Exercise
\ref{exe.subsect.vandermonde.factoring} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.vander-det.s1}}

\begin{proof}
[First solution to Exercise \ref{exe.vander-det.s1}.]Our solution will imitate
our First proof of Theorem \ref{thm.vander-det} \textbf{(a)} (but it will
involve some additional complications).

For every $u\in\left\{  0,1,\ldots,n\right\}  $ and $\left(  i,j\right)
\in\left\{  1,2,\ldots,u\right\}  ^{2}$, define $a_{i,j,u}\in\mathbb{K}$ by%
\[
a_{i,j,u}=%
\begin{cases}
x_{i}^{u-j}, & \text{if }j>1;\\
x_{i}^{u}, & \text{if }j=1
\end{cases}
.
\]


For every $u\in\left\{  1,2,\ldots,n\right\}  $, let $A_{u}$ be the $u\times
u$-matrix $\left(  a_{i,j,u}\right)  _{1\leq i\leq u,\ 1\leq j\leq u}$. Then,
our goal is to prove that $\det\left(  A_{n}\right)  =\left(  x_{1}%
+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
$ (because $A_{n}$ is precisely the matrix $\left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ which appears in the statement of
the exercise).

Now, let us show that%
\begin{equation}
\det\left(  A_{u}\right)  =\left(  x_{1}+x_{2}+\cdots+x_{u}\right)
\prod_{1\leq i<j\leq u}\left(  x_{i}-x_{j}\right)
\label{sol.vander-det.s1.goal}%
\end{equation}
for every $u\in\left\{  1,2,\ldots,n\right\}  $.

\textit{Proof of (\ref{sol.vander-det.s1.goal}):} We will prove
(\ref{sol.vander-det.s1.goal}) by induction over $u$:

\textit{Induction base:} The definition of $A_{1}$ yields $A_{1}=\left(
\begin{array}
[c]{c}%
x_{1}^{1}%
\end{array}
\right)  =\left(
\begin{array}
[c]{c}%
x_{1}%
\end{array}
\right)  $ and thus $\det\left(  A_{1}\right)  =x_{1}$. Compared with%
\[
\underbrace{\left(  x_{1}+x_{2}+\cdots+x_{1}\right)  }_{=x_{1}}%
\underbrace{\prod_{1\leq i<j\leq1}\left(  x_{i}-x_{j}\right)  }_{=\left(
\text{empty product}\right)  =1}=x_{1},
\]
this yields $\det\left(  A_{1}\right)  =\left(  x_{1}+x_{2}+\cdots
+x_{1}\right)  \prod_{1\leq i<j\leq1}\left(  x_{i}-x_{j}\right)  $. In other
words, (\ref{sol.vander-det.s1.goal}) holds for $u=1$. The induction base is
thus complete.

\textit{Induction step:} Let $U\in\left\{  2,3,\ldots,n\right\}  $. Assume
that (\ref{sol.vander-det.s1.goal}) holds for $u=U-1$. We need to prove that
(\ref{sol.vander-det.s1.goal}) holds for $u=U$.

Recall that $A_{U}=\left(  a_{i,j,U}\right)  _{1\leq i\leq U,\ 1\leq j\leq U}$
(by the definition of $A_{U}$). The matrix $A_{U}$ looks as follows:%
\[
A_{U}=\left(
\begin{array}
[c]{cccccc}%
x_{1}^{U} & x_{1}^{U-2} & x_{1}^{U-3} & \cdots & x_{1} & 1\\
x_{2}^{U} & x_{2}^{U-2} & x_{2}^{U-3} & \cdots & x_{2} & 1\\
x_{3}^{U} & x_{3}^{U-2} & x_{3}^{U-3} & \cdots & x_{3} & 1\\
x_{4}^{U} & x_{4}^{U-2} & x_{4}^{U-3} & \cdots & x_{4} & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
x_{U}^{U} & x_{U}^{U-2} & x_{U}^{U-3} & \cdots & x_{U} & 1
\end{array}
\right)  .
\]


For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, define
$b_{i,j}\in\mathbb{K}$ by%
\[
b_{i,j}=%
\begin{cases}
x_{i}^{U}-x_{U}^{2}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }1<j<U;\\
1, & \text{if }j=U
\end{cases}
.
\]
Let $B$ be the $U\times U$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$. Here is how $B$ looks like:%
\[
B=\left(
\begin{array}
[c]{cccccc}%
x_{1}^{U}-x_{U}^{2}x_{1}^{U-2} & x_{1}^{U-2}-x_{U}x_{1}^{U-3} & x_{1}%
^{U-3}-x_{U}x_{1}^{U-4} & \cdots & x_{1}-x_{U} & 1\\
x_{2}^{U}-x_{U}^{2}x_{2}^{U-2} & x_{2}^{U-2}-x_{U}x_{2}^{U-3} & x_{2}%
^{U-3}-x_{U}x_{2}^{U-4} & \cdots & x_{2}-x_{U} & 1\\
x_{3}^{U}-x_{U}^{2}x_{3}^{U-2} & x_{3}^{U-2}-x_{U}x_{3}^{U-3} & x_{3}%
^{U-3}-x_{U}x_{3}^{U-4} & \cdots & x_{3}-x_{U} & 1\\
x_{4}^{U}-x_{U}^{2}x_{4}^{U-2} & x_{4}^{U-2}-x_{U}x_{4}^{U-3} & x_{4}%
^{U-3}-x_{U}x_{4}^{U-4} & \cdots & x_{4}-x_{U} & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2} & x_{U}^{U-2}-x_{U}x_{U}^{U-3} & x_{U}%
^{U-3}-x_{U}x_{U}^{U-4} & \cdots & x_{U}-x_{U} & 1
\end{array}
\right)  .
\]
We claim that $\det B=\det\left(  A_{U}\right)  $. Indeed, here are two ways
to prove this:

\textit{First proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Exercise
\ref{exe.ps4.6k} \textbf{(b)} shows that the determinant of a $U\times
U$-matrix does not change if we subtract a multiple of one of its columns from
another column. Now, let us do the following steps (in this order):

\begin{itemize}
\item subtract $x_{U}^{2}$ times the $2$-nd column of $A_{U}$ from the $1$-st column;

\item subtract $x_{U}$ times the $3$-rd column of the resulting matrix from
the $2$-nd column;

\item subtract $x_{U}$ times the $4$-th column of the resulting matrix from
the $3$-rd column;

\item and so on, all the way until we finally subtract $x_{U}$ times the
$U$-th column of the matrix from the $\left(  U-1\right)  $-st column.
\end{itemize}

Yes, you are reading this right: At the first step we subtract $x_{U}^{2}$
times (not $x_{U}$ times) the $2$-nd column from the $1$-st column; but at all
further steps, we subtract $x_{U}$ times a column from another. Having done
all this, the resulting matrix is $B$ (according to our definition of $B$).
Thus, $\det B=\det\left(  A_{U}\right)  $ (since our subtractions never change
the determinant). This proves $\det B=\det\left(  A_{U}\right)  $.

\textit{Second proof of }$\det B=\det\left(  A_{U}\right)  $\textit{:} Here is
another way to prove that $\det B=\det\left(  A_{U}\right)  $, with some less handwaving.

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U\right\}  ^{2}$, we
define $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
-x_{U}^{2}, & \text{if }i=j+1\text{ and }j=1;\\
-x_{U}, & \text{if }i=j+1\text{ and }j>1;\\
0, & \text{otherwise}%
\end{cases}
.
\]
Let $C$ be the $U\times U$-matrix $\left(  c_{i,j}\right)  _{1\leq i\leq
U,\ 1\leq j\leq U}$. Here is how $C$ looks like:%
\[
C=\left(
\begin{array}
[c]{cccccc}%
1 & 0 & 0 & \cdots & 0 & 0\\
-x_{U}^{2} & 1 & 0 & \cdots & 0 & 0\\
0 & -x_{U} & 1 & \cdots & 0 & 0\\
0 & 0 & -x_{U} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & -x_{U} & 1
\end{array}
\right)  ,
\]
where the only $-x_{U}^{2}$ is in the $\left(  2,1\right)  $-th cell.

The matrix $C$ is lower-triangular, and thus Exercise \ref{exe.ps4.3} shows
that its determinant is $\det C=\underbrace{c_{1,1}}_{=1}\underbrace{c_{2,2}%
}_{=1}\cdots\underbrace{c_{U,U}}_{=1}=1$.

On the other hand, it is easy to see that $B=A_{U}C$ (check this!). Thus,
Theorem \ref{thm.det(AB)} yields $\det B=\det\left(  A_{U}\right)
\cdot\underbrace{\det C}_{=1}=\det\left(  A_{U}\right)  $. So we have proven
$\det B=\det\left(  A_{U}\right)  $ again.

Next, we observe that for every $j\in\left\{  1,2,\ldots,U-1\right\}  $, we
have%
\begin{align*}
b_{U,j}  &  =%
\begin{cases}
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2}, & \text{if }j=1;\\
x_{U}^{U-j}-x_{U}x_{U}^{U-j-1}, & \text{if }1<j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,j}\right) \\
&  =%
\begin{cases}
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2}, & \text{if }j=1;\\
x_{U}^{U-j}-x_{U}x_{U}^{U-j-1}, & \text{if }1<j<U
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<U\text{ (since }j\in\left\{
1,2,\ldots,U-1\right\}  \text{)}\right) \\
&  =%
\begin{cases}
x_{U}^{U}-x_{U}^{U}, & \text{if }j=1;\\
x_{U}^{U-j}-x_{U}^{U-j}, & \text{if }1<j<U
\end{cases}
=%
\begin{cases}
0, & \text{if }j=1;\\
0, & \text{if }1<j<U
\end{cases}
=0.
\end{align*}
Hence, Theorem \ref{thm.laplace.pre} (applied to $U$, $B$ and $b_{i,j}$
instead of $n$, $A$ and $a_{i,j}$) yields%
\begin{equation}
\det B=b_{U,U}\cdot\det\left(  \left(  b_{i,j}\right)  _{1\leq i\leq
U-1,\ 1\leq j\leq U-1}\right)  . \label{sol.vander-det.s1.detB=prod}%
\end{equation}
Let $B^{\prime}$ denote the $\left(  U-1\right)  \times\left(  U-1\right)
$-matrix $\left(  b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$.

The definition of $b_{U,U}$ yields%
\begin{align*}
b_{U,U}  &  =%
\begin{cases}
x_{U}^{U}-x_{U}^{2}x_{U}^{U-2}, & \text{if }U=1;\\
x_{U}^{U-U}-x_{U}x_{U}^{U-U-1}, & \text{if }1<U<U;\\
1, & \text{if }U=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{U,U}\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }U=U\right)  .
\end{align*}
Thus, (\ref{sol.vander-det.s1.detB=prod}) becomes%
\[
\det B=\underbrace{b_{U,U}}_{=1}\cdot\det\left(  \underbrace{\left(
b_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}}_{=B^{\prime}}\right)
=\det\left(  B^{\prime}\right)  .
\]
Compared with $\det B=\det\left(  A_{U}\right)  $, this yields%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  .
\label{sol.vander-det.s1.detAU=detB'}%
\end{equation}


Now, let us take a closer look at $B^{\prime}$. Indeed, every $\left(
i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$ satisfies%
\begin{align}
b_{i,j}  &  =%
\begin{cases}
x_{i}^{U}-x_{U}^{2}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }1<j<U;\\
1, & \text{if }j=U
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }b_{i,j}\right)
\nonumber\\
&  =%
\begin{cases}
x_{i}^{U}-x_{U}^{2}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{U-j}-x_{U}x_{i}^{U-j-1}, & \text{if }1<j<U
\end{cases}
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j<U\text{ (since }j\in\left\{  1,2,\ldots,U-1\right\} \\
\text{(since }\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}
^{2}\text{))}%
\end{array}
\right) \nonumber\\
&  =%
\begin{cases}
\left(  x_{i}^{2}-x_{U}^{2}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
\left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}, & \text{if }1<j<U
\end{cases}
=%
\begin{cases}
\left(  x_{i}-x_{U}\right)  \left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, &
\text{if }j=1;\\
\left(  x_{i}-x_{U}\right)  x_{i}^{\left(  U-1\right)  -j}, & \text{if }1<j<U
\end{cases}
\nonumber\\
&  =\left(  x_{i}-x_{U}\right)
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }1<j<U
\end{cases}
\nonumber\\
&  =\left(  x_{i}-x_{U}\right)
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
\label{sol.vander-det.s1.bij-small}%
\end{align}
(since $1<j<U$ is equivalent to $j>1$).

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$, we
define $g_{i,j}\in\mathbb{K}$ by%
\[
g_{i,j}=%
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
.
\]
Let $G$ be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix
$\left(  g_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$. Here is how
$G$ looks like:%
\[
G=\left(
\begin{array}
[c]{cccccc}%
\left(  x_{1}+x_{U}\right)  x_{1}^{U-2} & x_{1}^{U-3} & x_{1}^{U-4} & \cdots &
x_{1} & 1\\
\left(  x_{2}+x_{U}\right)  x_{2}^{U-2} & x_{2}^{U-3} & x_{2}^{U-4} & \cdots &
x_{2} & 1\\
\left(  x_{3}+x_{U}\right)  x_{3}^{U-2} & x_{3}^{U-3} & x_{3}^{U-4} & \cdots &
x_{3} & 1\\
\left(  x_{4}+x_{U}\right)  x_{4}^{U-2} & x_{4}^{U-3} & x_{4}^{U-4} & \cdots &
x_{4} & 1\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
\left(  x_{U-1}+x_{U}\right)  x_{U-1}^{U-2} & x_{U-1}^{U-3} & x_{U-1}^{U-4} &
\cdots & x_{U-1} & 1
\end{array}
\right)  .
\]
Now, (\ref{sol.vander-det.s1.bij-small}) becomes%
\begin{equation}
b_{i,j}=\left(  x_{i}-x_{U}\right)  \underbrace{%
\begin{cases}
\left(  x_{i}+x_{U}\right)  x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
}_{=g_{i,j}}=\left(  x_{i}-x_{U}\right)  g_{i,j}
\label{sol.vander-det.s1.bij-small-2}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$.
Hence,%
\begin{equation}
B^{\prime}=\left(  \underbrace{b_{i,j}}_{\substack{=\left(  x_{i}%
-x_{U}\right)  g_{i,j}\\\text{(by (\ref{sol.vander-det.s1.bij-small-2}))}%
}}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}=\left(  \left(  x_{i}%
-x_{U}\right)  g_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}.
\label{sol.vander-det.s1.B'}%
\end{equation}
On the other hand, the definition of $G$ yields
\begin{equation}
G=\left(  g_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}.
\label{sol.vander-det.s1.AU-1}%
\end{equation}


Now, we claim that%
\begin{equation}
\det\left(  B^{\prime}\right)  =\det G\cdot\prod_{i=1}^{U-1}\left(
x_{i}-x_{U}\right)  . \label{sol.vander-det.s1.detB'=}%
\end{equation}
This can be proven similarly to how we proved (\ref{pf.thm.vander-det.detB'=})
back in our First proof of Theorem \ref{thm.vander-det}. (Of course, this
time, $G$ plays the role of $A_{U-1}$.) Now,
(\ref{sol.vander-det.s1.detAU=detB'}) becomes%
\begin{equation}
\det\left(  A_{U}\right)  =\det\left(  B^{\prime}\right)  =\det G\cdot
\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right)  .
\label{sol.vander-det.s1.detAU=}%
\end{equation}


Now, we want to compute $\det G$. (This part is harder than the analogous part
of our First proof of Theorem \ref{thm.vander-det}, because back then the role
of $G$ was played by $A_{U-1}$, and we knew $\det\left(  A_{U-1}\right)  $
directly from our induction hypothesis.)

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$, we
define two elements $p_{i,j}\in\mathbb{K}$ and $q_{i,j}\in\mathbb{K}$ by%
\[
p_{i,j}=%
\begin{cases}
x_{i}^{U-1}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
\]
and%
\[
q_{i,j}=%
\begin{cases}
x_{U}x_{i}^{U-2}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
.
\]
We let $P$ be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix
$\left(  p_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$, and we let $Q$
be the $\left(  U-1\right)  \times\left(  U-1\right)  $-matrix $\left(
q_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$. We now make the
following observations:

\begin{itemize}
\item The columns of the matrix $Q$ equal the corresponding columns of $P$
except (perhaps) the $1$-st column. The matrix $G$ is the $\left(  U-1\right)
\times\left(  U-1\right)  $-matrix obtained from $P$ by adding the $1$-st
column of $Q$ to the $1$-st column of $P$. Thus, Exercise \ref{exe.ps4.6}
\textbf{(j)} (applied to $U-1$, $1$, $P$, $Q$ and $G$ instead of $n$, $k$,
$A$, $A^{\prime}$ and $B$) yields $\det G=\det P+\det Q$.

\item We have $P=\left(  p_{i,j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$
and $A_{U-1}=\left(  a_{i,j,U-1}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}%
$. But a quick look at the definitions reveals that
\[
p_{i,j}=%
\begin{cases}
x_{i}^{U-1}, & \text{if }j=1;\\
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1
\end{cases}
=%
\begin{cases}
x_{i}^{\left(  U-1\right)  -j}, & \text{if }j>1;\\
x_{i}^{U-1}, & \text{if }j=1
\end{cases}
=a_{i,j,U-1}%
\]
for all $\left(  i,j\right)  \in\left\{  1,2,\ldots,U-1\right\}  ^{2}$. Hence,
$P=A_{U-1}$.

\item The matrix $Q$ is obtained from the matrix $\left(  x_{i}^{\left(
U-1\right)  -j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$ by multiplying
its $1$-st column by $x_{U}$. Hence, Exercise \ref{exe.ps4.6} \textbf{(h)}
(applied to $U-1$, $x_{U}$, $1$, $\left(  x_{i}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}$ and $Q$ instead of $n$,
$\lambda$, $k$, $A$ and $B$) yields%
\begin{align*}
\det Q  &  =x_{U}\underbrace{\det\left(  \left(  x_{i}^{\left(  U-1\right)
-j}\right)  _{1\leq i\leq U-1,\ 1\leq j\leq U-1}\right)  }_{\substack{=\prod
_{1\leq i<j\leq U-1}\left(  x_{i}-x_{j}\right)  \\\text{(by Theorem
\ref{thm.vander-det} \textbf{(a)}, applied to }n=U-1\text{)}}}\\
&  =x_{U}\prod_{1\leq i<j\leq U-1}\left(  x_{i}-x_{j}\right)  .
\end{align*}

\end{itemize}

Combining this, we obtain%
\begin{align*}
\det G  &  =\det\underbrace{P}_{=A_{U-1}}+\underbrace{\det Q}_{=x_{U}%
\prod_{1\leq i<j\leq U-1}\left(  x_{i}-x_{j}\right)  }\\
&  =\underbrace{\det\left(  A_{U-1}\right)  }_{\substack{=\left(  x_{1}%
+x_{2}+\cdots+x_{U-1}\right)  \prod_{1\leq i<j\leq U-1}\left(  x_{i}%
-x_{j}\right)  \\\text{(since we assumed that (\ref{sol.vander-det.s1.goal})
holds for }u=U-1\text{)}}}+x_{U}\prod_{1\leq i<j\leq U-1}\left(  x_{i}%
-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U-1}\right)  \prod_{1\leq i<j\leq
U-1}\left(  x_{i}-x_{j}\right)  +x_{U}\prod_{1\leq i<j\leq U-1}\left(
x_{i}-x_{j}\right) \\
&  =\underbrace{\left(  \left(  x_{1}+x_{2}+\cdots+x_{U-1}\right)
+x_{U}\right)  }_{=x_{1}+x_{2}+\cdots+x_{U}}\prod_{1\leq i<j\leq U-1}\left(
x_{i}-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \prod_{1\leq i<j\leq U-1}\left(
x_{i}-x_{j}\right)  .
\end{align*}
Hence, (\ref{sol.vander-det.s1.detAU=}) yields%
\begin{align*}
\det\left(  A_{U}\right)   &  =\underbrace{\det G}_{=\left(  x_{1}%
+x_{2}+\cdots+x_{U}\right)  \prod_{1\leq i<j\leq U-1}\left(  x_{i}%
-x_{j}\right)  }\cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \underbrace{\prod_{1\leq i<j\leq
U-1}}_{=\prod_{j=1}^{U-1}\prod_{i=1}^{j-1}}\left(  x_{i}-x_{j}\right)
\cdot\prod_{i=1}^{U-1}\left(  x_{i}-x_{U}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \left(  \prod_{j=1}^{U-1}%
\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  \right)  \cdot\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right)  .
\end{align*}
Compared with%
\begin{align*}
&  \left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \underbrace{\prod_{1\leq i<j\leq
U}}_{=\prod_{j=1}^{U}\prod_{i=1}^{j-1}}\left(  x_{i}-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \prod_{j=1}^{U}\prod_{i=1}%
^{j-1}\left(  x_{i}-x_{j}\right) \\
&  =\left(  x_{1}+x_{2}+\cdots+x_{U}\right)  \left(  \prod_{j=1}^{U-1}%
\prod_{i=1}^{j-1}\left(  x_{i}-x_{j}\right)  \right)  \cdot\prod_{i=1}%
^{U-1}\left(  x_{i}-x_{U}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the factor for
}j=U\text{ from the product}\right)  ,
\end{align*}
this yields $\det\left(  A_{U}\right)  =\left(  x_{1}+x_{2}+\cdots
+x_{U}\right)  \prod_{1\leq i<j\leq U}\left(  x_{i}-x_{j}\right)  $. In other
words, (\ref{sol.vander-det.s1.goal}) holds for $u=U$. This completes the
induction step.

Now, (\ref{sol.vander-det.s1.goal}) is proven by induction. Hence, we can
apply (\ref{sol.vander-det.s1.goal}) to $u=n$. As the result, we obtain
$\det\left(  A_{n}\right)  =\left(  x_{1}+x_{2}+\cdots+x_{n}\right)
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  $. Since $A_{n}=\left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of $A_{n}$),
this rewrites as
\[
\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\left(  x_{1}+x_{2}%
+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  .
\]
This solves Exercise \ref{exe.vander-det.s1}.
\end{proof}

We shall give a second solution for Exercise \ref{exe.vander-det.s1} later (in
Section \ref{sect.sols.vander-det.s1.sol2}).

\subsection{Solution to Exercise \ref{exe.vander-det.xi+yj}}

We shall first sketch short solutions to parts \textbf{(a)} and \textbf{(b)}
of Exercise \ref{exe.vander-det.xi+yj}. Then, we will show a solution to part
\textbf{(c)} (which is a more elaborate and subtler version of our solution to
part \textbf{(b)}), and finally derive parts \textbf{(a)} and \textbf{(b)}
from part \textbf{(c)}. Thus, parts \textbf{(a)} and \textbf{(b)} of Exercise
\ref{exe.vander-det.xi+yj} will be solved twice (though the solutions cannot
really be called different).

\begin{proof}
[Solution sketch to parts \textbf{(a)} and \textbf{(b)} of Exercise
\ref{exe.vander-det.xi+yj}.]\textbf{(a)} Let $m\in\left\{  0,1,\ldots
,n-2\right\}  $. Thus, $m+1<n$.

Define an $n\times\left(  m+1\right)  $-matrix $B$ by%
\[
B=\left(  \dbinom{m}{j-1}x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m+1}.
\]
Define an $\left(  m+1\right)  \times n$-matrix $C$ by%
\[
C=\left(  y_{j}^{m-\left(  i-1\right)  }\right)  _{1\leq i\leq m+1,\ 1\leq
j\leq n}.
\]
Then, the definition of the product of two matrices shows that%
\[
BC=\left(  \sum_{k=1}^{m+1}\dbinom{m}{k-1}x_{i}^{k-1}y_{j}^{m-\left(
k-1\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \sum_{k=1}^{m+1}\dbinom{m}{k-1}x_{i}^{k-1}y_{j}^{m-\left(  k-1\right)  }\\
&  =\sum_{k=0}^{m}\dbinom{m}{k}x_{i}^{k}y_{j}^{m-k}\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we have substituted }k\text{ for }k-1\text{ in the sum}\right) \\
&  =\left(  x_{i}+y_{j}\right)  ^{m}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the binomial formula yields}\\
\left(  x_{i}+y_{j}\right)  ^{m}=\sum_{k=0}^{m}\dbinom{m}{k}x_{i}^{k}%
y_{j}^{m-k}%
\end{array}
\right)  ,
\end{align*}
this rewrites as%
\[
BC=\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]


But recall that $m+1<n$. Hence, (\ref{eq.exam.cauchy-binet.0}) (applied to
$m+1$, $B$ and $C$ instead of $m$, $A$ and $B$) shows that $\det\left(
BC\right)  =0$. Since $BC=\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$, this rewrites as $\det\left(  \left(
\left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =0$. This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

\textbf{(b)} Define an $n\times n$-matrix $B$ by%
\[
B=\left(  \dbinom{n-1}{j-1}x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Define an $n\times n$-matrix $C$ by%
\[
C=\left(  y_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then, the definition of the product of two matrices shows that%
\[
BC=\left(  \sum_{k=1}^{n}\dbinom{n-1}{k-1}x_{i}^{k-1}y_{j}^{n-k}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Since every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \sum_{k=1}^{n}\dbinom{n-1}{k-1}x_{i}^{k-1}\underbrace{y_{j}^{n-k}}%
_{=y_{j}^{\left(  n-1\right)  -\left(  k-1\right)  }}=\sum_{k=1}^{n}%
\dbinom{n-1}{k-1}x_{i}^{k-1}y_{j}^{\left(  n-1\right)  -\left(  k-1\right)
}\\
&  =\sum_{k=0}^{n-1}\dbinom{n-1}{k}x_{i}^{k}y_{j}^{\left(  n-1\right)
-k}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}k-1\text{ in the sum}\right) \\
&  =\left(  x_{i}+y_{j}\right)  ^{n-1}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the binomial formula yields}\\
\left(  x_{i}+y_{j}\right)  ^{n-1}=\sum_{k=0}^{n-1}\dbinom{n-1}{k}x_{i}%
^{k}y_{j}^{\left(  n-1\right)  -k}%
\end{array}
\right)  ,
\end{align*}
this rewrites as%
\[
BC=\left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]


Theorem \ref{thm.det(AB)} shows that $\det\left(  BC\right)  =\det B\cdot\det
C$. But what are $\det B$ and $\det C$ ?

Finding $\det C$ is easy: We have $C=\left(  y_{j}^{n-i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ and thus
\[
\det C=\det\left(  \left(  y_{j}^{n-i}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)
\]
(by Theorem \ref{thm.vander-det} \textbf{(b)}, applied to $y_{k}$ instead of
$x_{k}$).

To find $\det B$, we first observe that $\det\left(  \left(  x_{i}%
^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  =\prod_{1\leq j<i\leq
n}\left(  x_{i}-x_{j}\right)  $ (by Theorem \ref{thm.vander-det}
\textbf{(c)}). But $B=\left(  \dbinom{n-1}{j-1}x_{i}^{j-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. In other words, the matrix $B$ is obtained from the
matrix $\left(  x_{i}^{j-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ by
multiplying the whole $j$-th column with $\dbinom{n-1}{j-1}$ for every
$j\in\left\{  1,2,\ldots,n\right\}  $. Therefore, the determinant $\det B$ is
obtained by successively multiplying $\det\left(  \left(  x_{i}^{j-1}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  $ with $\dbinom{n-1}{j-1}$ for every
$j\in\left\{  1,2,\ldots,n\right\}  $ (since Exercise \ref{exe.ps4.6}
\textbf{(h)} tells us that multiplying a single column of a square matrix by a
scalar $\lambda$ results in the determinant getting multiplied by $\lambda$).
In other words,%
\begin{align*}
\det B  &  =\underbrace{\det\left(  \left(  x_{i}^{j-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  }_{\substack{=\prod_{1\leq j<i\leq n}\left(
x_{i}-x_{j}\right)  \\=\prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)
\\\text{(here, we renamed the}\\\text{index }\left(  j,i\right)  \text{ as
}\left(  i,j\right)  \text{)}}}\cdot\underbrace{\prod_{j=1}^{n}\dbinom
{n-1}{j-1}}_{\substack{=\prod_{k=0}^{n-1}\dbinom{n-1}{k}\\\text{(here, we
substituted }k\text{ for }j-1\text{)}}}\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)  \right)
\cdot\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  .
\end{align*}


Now,%
\begin{align*}
\det\left(  BC\right)   &  =\underbrace{\det B}_{=\left(  \prod_{1\leq i<j\leq
n}\left(  x_{j}-x_{i}\right)  \right)  \cdot\left(  \prod_{k=0}^{n-1}%
\dbinom{n-1}{k}\right)  }\cdot\underbrace{\det C}_{=\prod_{1\leq i<j\leq
n}\left(  y_{i}-y_{j}\right)  }\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)  \right)
\cdot\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)  \right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\underbrace{\left(
\prod_{1\leq i<j\leq n}\left(  x_{j}-x_{i}\right)  \right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)  \right)  }_{=\prod_{1\leq
i<j\leq n}\left(  \left(  x_{j}-x_{i}\right)  \left(  y_{i}-y_{j}\right)
\right)  }\\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\prod_{1\leq i<j\leq
n}\underbrace{\left(  \left(  x_{j}-x_{i}\right)  \left(  y_{i}-y_{j}\right)
\right)  }_{=\left(  x_{i}-x_{j}\right)  \left(  y_{j}-y_{i}\right)  }\\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\underbrace{\prod
_{1\leq i<j\leq n}\left(  \left(  x_{i}-x_{j}\right)  \left(  y_{j}%
-y_{i}\right)  \right)  }_{=\left(  \prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}%
-y_{i}\right)  \right)  }\\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
Since $BC=\left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, this rewrites as%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(b)}.
\end{proof}

Now, as promised, we shall start from scratch, and solve Exercise
\ref{exe.vander-det.xi+yj} \textbf{(c)} first, and then solve Exercise
\ref{exe.vander-det.xi+yj} \textbf{(a)} and \textbf{(b)} again using Exercise
\ref{exe.vander-det.xi+yj} \textbf{(c)}.

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.vander-det.xi+yj}.]\textbf{(c)} We extend the
$n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  $ to an infinite
sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{K}^{\infty}$ by
setting%
\begin{equation}
p_{\ell}=0\ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\left\{
n,n+1,n+2,\ldots\right\}  . \label{sol.vander-det.xi+yj.short.c.pk=0}%
\end{equation}


Next, we define three $n\times n$-matrices $B$, $C$ and $D$:

\begin{itemize}
\item Define an $n\times n$-matrix $B$ by
\[
B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{equation}
\det\underbrace{B}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}=\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\label{sol.vander-det.xi+yj.short.c.detB}%
\end{equation}
(by Theorem \ref{thm.vander-det} \textbf{(a)}).

\item For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
the element $\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$ of $\mathbb{K}$ is
well-defined\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $j-1\geq0$. Hence, the binomial coefficient
$\dbinom{n-1-i+j}{j-1}\in\mathbb{Z}$ is well-defined. Moreover,
$n-1-\underbrace{i}_{\leq n}+\underbrace{j}_{\geq1}\geq n-1-n+1=0$, so that
$n-1-i+j\in\mathbb{N}$. Thus, $p_{n-1-i+j}\in\mathbb{K}$ is well-defined
(since we have an infinite sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)
\in\mathbb{K}^{\infty}$). Hence, the element $\dbinom{n-1-i+j}{j-1}%
p_{n-1-i+j}$ of $\mathbb{K}$ is well-defined. Qed.}. Thus, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, we can define an element
$c_{i,j}\in\mathbb{K}$ by $c_{i,j}=\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$. Consider
these elements $c_{i,j}$. Define an $n\times n$-matrix $C$ by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Consider this matrix $C$. We have $c_{i,j}=0$ for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ be such that $i<j$. From $i<j$, we obtain $i\leq
j-1$ (since $i$ and $j$ are integers). Thus, $n-1-\underbrace{i}_{\leq
j-1}+j\geq n-1-\left(  j-1\right)  +j=n$. In other words, $n-1-i+j\in\left\{
n,n+1,n+2,\ldots\right\}  $. Hence, $p_{n-1-i+j}=0$ (by
(\ref{sol.vander-det.xi+yj.short.c.pk=0}), applied to $\ell=n-1-i+j$). Hence,
$c_{i,j}=\dbinom{n-1-i+j}{j-1}\underbrace{p_{n-1-i+j}}_{=0}=0$, qed.}. Hence,
Exercise \ref{exe.ps4.3} (applied to $C$ and $\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ instead of $A$ and $\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n,n}=\prod_{i=1}^{n}\underbrace{c_{i,i}%
}_{\substack{=\dbinom{n-1-i+i}{i-1}p_{n-1-i+i}\\\text{(by the definition of
}c_{i,i}\text{)}}}\nonumber\\
&  =\prod_{i=1}^{n}\left(  \underbrace{\dbinom{n-1-i+i}{i-1}}_{=\dbinom
{n-1}{i-1}}\underbrace{p_{n-1-i+i}}_{=p_{n-1}}\right) \nonumber\\
&  =\prod_{i=1}^{n}\left(  \dbinom{n-1}{i-1}p_{n-1}\right)  =\left(
\prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  p_{n-1}^{n}\nonumber\\
&  =p_{n-1}^{n}\left(  \prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  =p_{n-1}%
^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)
\label{sol.vander-det.xi+yj.short.c.detC}%
\end{align}
(here, we have substituted $k$ for $i-1$ in the product).

\item Define an $n\times n$-matrix $D$ by
\[
D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{align}
\det\underbrace{D}_{=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}  &  =\det\left(  \left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  y_{i}-y_{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.vander-det}
\textbf{(d)}, applied to }y_{k}\text{ instead of }x_{k}\right) \nonumber\\
&  =\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)
\label{sol.vander-det.xi+yj.short.c.detD}%
\end{align}
(here, we have renamed the index $\left(  j,i\right)  $ as $\left(
i,j\right)  $ in the product).
\end{itemize}

Our goal is to prove that $\left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}=BCD$. Once this is done, we will be able to
compute $\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  $ by applying Theorem \ref{thm.det(AB)} twice.

We have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the
definition of the product of two matrices shows that%
\begin{equation}
CD=\left(  \sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}. \label{sol.vander-det.xi+yj.short.c.CD.1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
\sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}  &  =\sum_{\ell=0}^{n-1}\underbrace{c_{i,\ell
+1}}_{\substack{=\dbinom{n-1-i+\left(  \ell+1\right)  }{\left(  \ell+1\right)
-1}p_{n-1-i+\left(  \ell+1\right)  }\\\text{(by the definition of }%
c_{i,\ell+1}\text{)}}}y_{j}^{\left(  \ell+1\right)  -1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\ell+1\text{
for }k\text{ in the sum}\right) \\
&  =\sum_{\ell=0}^{n-1}\underbrace{\dbinom{n-1-i+\left(  \ell+1\right)
}{\left(  \ell+1\right)  -1}}_{\substack{=\dbinom{n-i+\ell}{\ell}%
}}\underbrace{p_{n-1-i+\left(  \ell+1\right)  }}_{=p_{n-i+\ell}}%
\underbrace{y_{j}^{\left(  \ell+1\right)  -1}}_{=y_{j}^{\ell}}\\
&  =\sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell}.
\end{align*}
Hence, (\ref{sol.vander-det.xi+yj.short.c.CD.1}) rewrites as%
\[
CD=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]


Now, $B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$CD=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the definition of the
product of two matrices shows that%
\begin{equation}
B\left(  CD\right)  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell
=0}^{n-1}\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}. \label{sol.vander-det.xi+yj.short.c.BCD.1}%
\end{equation}


Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,%
\begin{align}
&  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}{\ell
}p_{n-k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}x_{i}^{k}\left(  \sum_{\ell=0}^{n-1}\dbinom{k+\ell}{\ell
}p_{k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}n-k\text{ in the first sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=0}^{n-1}x_{i}^{k}\dbinom{k+\ell}{\ell}%
p_{k+\ell}y_{j}^{\ell}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1+k}x_{i}^{k}\underbrace{\dbinom
{k+\left(  \ell-k\right)  }{\ell-k}}_{=\dbinom{\ell}{\ell-k}}%
\underbrace{p_{k+\left(  \ell-k\right)  }}_{=p_{\ell}}y_{j}^{\ell
-k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\ell-k\text{
for }\ell\text{ in the second sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\underbrace{\sum_{\ell=k}^{n-1+k}x_{i}^{k}\dbinom{\ell
}{\ell-k}p_{\ell}y_{j}^{\ell-k}}_{\substack{=\sum_{\ell=k}^{n-1}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}\\\text{(since }k\leq n\leq
n-1+k+1\text{)}}}\nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}^{k}\dbinom{\ell}{\ell
-k}\underbrace{p_{\ell}}_{\substack{=0\\\text{(by
(\ref{sol.vander-det.xi+yj.short.c.pk=0})}\\\text{(since }\ell\in\left\{
n,n+1,\ldots,n-1+k\right\}  \\\subseteq\left\{  n,n+1,n+2,\ldots\right\}
\text{))}}}y_{j}^{\ell-k}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\underbrace{\sum_{\ell=n}^{n-1+k}x_{i}^{k}%
\dbinom{\ell}{\ell-k}0y_{j}^{\ell-k}}_{=0}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell
}y_{j}^{\ell-k}. \label{sol.vander-det.xi+yj.short.c.BCD.2}%
\end{align}
On the other hand, $P\left(  X\right)  =\sum_{k=0}^{n-1}p_{k}X^{k}=\sum
_{\ell=0}^{n-1}p_{\ell}X^{\ell}$ (here, we renamed the summation index $k$ as
$\ell$). Hence,%
\begin{align}
P\left(  x_{i}+y_{j}\right)   &  =\sum_{\ell=0}^{n-1}p_{\ell}%
\underbrace{\left(  x_{i}+y_{j}\right)  ^{\ell}}_{\substack{=\sum_{k=0}^{\ell
}\dbinom{\ell}{k}x_{i}^{k}y_{j}^{\ell-k}\\\text{(by the binomial formula)}%
}}=\sum_{\ell=0}^{n-1}p_{\ell}\sum_{k=0}^{\ell}\dbinom{\ell}{k}x_{i}^{k}%
y_{j}^{\ell-k}\nonumber\\
&  =\underbrace{\sum_{\ell=0}^{n-1}\sum_{k=0}^{\ell}}_{=\sum
_{\substack{\left(  \ell,k\right)  \in\left\{  0,1,\ldots,n-1\right\}
^{2};\\k\leq\ell}}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}}p_{\ell}\dbinom{\ell
}{k}x_{i}^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}p_{\ell}\underbrace{\dbinom{\ell}{k}%
}_{\substack{=\dbinom{\ell}{\ell-k}\\\text{(by (\ref{eq.binom.symm}), applied
to }\ell\text{ and }k\\\text{instead of }m\text{ and }n\text{)}}}x_{i}%
^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}p_{\ell}\dbinom{\ell}{\ell-k}x_{i}%
^{k}y_{j}^{\ell-k}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell
}{\ell-k}p_{\ell}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}%
{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.short.c.BCD.2})}\right)  .
\label{sol.vander-det.xi+yj.short.c.BCD.3}%
\end{align}


Now, let us forget that we fixed $\left(  i,j\right)  $. We thus have proven
(\ref{sol.vander-det.xi+yj.short.c.BCD.3}) for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence,%
\begin{align*}
&  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\\
&  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}%
\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=B\left(  CD\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.short.c.BCD.1})}\right) \\
&  =BCD.
\end{align*}
Hence,%
\begin{align*}
&  \det\underbrace{\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{=BCD}\\
&  =\det\left(  BCD\right)  =\det B\cdot\underbrace{\det\left(  CD\right)
}_{\substack{=\det C\cdot\det D\\\text{(by Theorem \ref{thm.det(AB)}, applied
to }C\text{ and }D\\\text{instead of }A\text{ and }B\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}B\text{ and }CD\text{ instead of }A\text{ and }B\right) \\
&  =\underbrace{\det B}_{\substack{=\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.short.c.detB}))}}%
}\cdot\underbrace{\det C}_{\substack{=p_{n-1}^{n}\left(  \prod_{k=0}%
^{n-1}\dbinom{n-1}{k}\right)  \\\text{(by
(\ref{sol.vander-det.xi+yj.short.c.detC}))}}}\cdot\underbrace{\det
D}_{\substack{=\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \\\text{(by
(\ref{sol.vander-det.xi+yj.short.c.detD}))}}}\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \cdot
p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(c)}.

\textbf{(a)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be
the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

Let $m\in\left\{  0,1,\ldots,n-1\right\}  $. (You are reading this right: We
are not requiring $m$ to belong to $\left\{  0,1,\ldots,n-2\right\}  $; the
purpose of this is to obtain a result that will bring us close to solving both
parts \textbf{(a)} and \textbf{(b)} simultaneously.)

Define an $n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  \in
\mathbb{K}^{n}$ by%
\[
\left(  p_{k}=\delta_{k,m}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
0,1,\ldots,n-1\right\}  \right)  .
\]
Let $P\left(  X\right)  \in\mathbb{K}\left[  X\right]  $ be the polynomial
$\sum_{k=0}^{n-1}p_{k}X^{k}$.

We have%
\begin{align*}
P\left(  X\right)   &  =\underbrace{\sum_{k=0}^{n-1}}_{=\sum_{k\in\left\{
0,1,\ldots,n-1\right\}  }}\underbrace{p_{k}}_{\substack{=\delta_{k,m}}%
}X^{k}=\sum_{k\in\left\{  0,1,\ldots,n-1\right\}  }\delta_{k,m}X^{k}\\
&  =\underbrace{\delta_{m,m}}_{\substack{=1\\\text{(since }m=m\text{)}}%
}X^{m}+\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}  ;\\k\neq
m}}\underbrace{\delta_{k,m}}_{\substack{=0\\\text{(since }k\neq m\text{)}%
}}X^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=m\text{ from the sum}\\
\text{(since }m\in\left\{  0,1,\ldots,n-1\right\}  \text{)}%
\end{array}
\right) \\
&  =X^{m}+\underbrace{\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}
;\\k\neq m}}0X^{k}}_{=0}=X^{m}.
\end{align*}
Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies
$P\left(  x_{i}+y_{j}\right)  =\left(  x_{i}+y_{j}\right)  ^{m}$ (since
$P\left(  X\right)  =X^{m}$). In other words, $\left(  P\left(  x_{i}%
+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \left(
x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Taking determinants on both sides of this equality, we obtain%
\begin{align}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\underbrace{p_{n-1}^{n}}_{\substack{=\delta_{n-1,m}^{n}\\\text{(since
}p_{n-1}=\delta_{n-1,m}\\\text{(by the definition of }p_{n-1}\text{))}%
}}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.vander-det.xi+yj}
\textbf{(c)}}\right) \nonumber\\
&  =\delta_{n-1,m}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\label{sol.vander-det.xi+yj.short.a.almost}%
\end{align}


Now, let us forget that we fixed $m$. We thus have proven
(\ref{sol.vander-det.xi+yj.short.a.almost}) for every $m\in\left\{
0,1,\ldots,n-1\right\}  $.

Now, let $m\in\left\{  0,1,\ldots,n-2\right\}  $. Thus, $m\neq n-1$, so that
$\delta_{n-1,m}=0$. Hence, $\delta_{n-1,m}^{n}=0^{n}=0$ (since $n$ is a
positive integer).

On the other hand, $m\in\left\{  0,1,\ldots,n-2\right\}  \subseteq\left\{
0,1,\ldots,n-1\right\}  $, and therefore
(\ref{sol.vander-det.xi+yj.short.a.almost}) holds. Hence,%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,m}^{n}}_{=0}\left(  \prod_{k=0}^{n-1}\dbinom
{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =0.
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

\textbf{(b)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ as in
the solution to Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

We have $\delta_{n-1,n-1}=1$ and thus $\delta_{n-1,n-1}^{n}=1^{n}=1$.

In our solution of Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}, we have
proven the equality (\ref{sol.vander-det.xi+yj.short.a.almost}) for every
$m\in\left\{  0,1,\ldots,n-1\right\}  $. Thus, we can apply this equality to
$m=n-1$. As a result, we obtain
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,n-1}^{n}}_{=1}\left(  \prod_{k=0}^{n-1}%
\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}%
-y_{i}\right)  \right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.vander-det.xi+yj}.]\textbf{(c)} We extend the
$n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  $ to an infinite
sequence $\left(  p_{0},p_{1},p_{2},\ldots\right)  \in\mathbb{K}^{\infty}$ by
setting%
\begin{equation}
p_{\ell}=0\ \ \ \ \ \ \ \ \ \ \text{for every }\ell\in\left\{
n,n+1,n+2,\ldots\right\}  . \label{sol.vander-det.xi+yj.c.pk=0}%
\end{equation}


Next, we define three $n\times n$-matrices $B$, $C$ and $D$:

\begin{itemize}
\item Define an $n\times n$-matrix $B$ by
\[
B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{equation}
\det\underbrace{B}_{=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}=\det\left(  \left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\label{sol.vander-det.xi+yj.c.detB}%
\end{equation}
(by Theorem \ref{thm.vander-det} \textbf{(a)}).

\item For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
the element $\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$ of $\mathbb{K}$ is
well-defined\footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,n\right\}  $. Hence, $1\leq i\leq n$ (since
$i\in\left\{  1,2,\ldots,n\right\}  $) and $1\leq j\leq n$ (since
$j\in\left\{  1,2,\ldots,n\right\}  $). From $1\leq j$, we obtain $j-1\geq0$.
Hence, the binomial coefficient $\dbinom{n-1-i+j}{j-1}\in\mathbb{Z}$ is
well-defined.
\par
Moreover, $n-1-\underbrace{i}_{\leq n}+\underbrace{j}_{\geq1}\geq n-1-n+1=0$,
so that $n-1-i+j\in\mathbb{N}$. Thus, $p_{n-1-i+j}\in\mathbb{K}$ is
well-defined (since we have an infinite sequence $\left(  p_{0},p_{1}%
,p_{2},\ldots\right)  \in\mathbb{K}^{\infty}$). Hence, the element
$\dbinom{n-1-i+j}{j-1}p_{n-1-i+j}$ of $\mathbb{K}$ is well-defined. Qed.}.
Thus, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we can define an element $c_{i,j}\in\mathbb{K}$ by $c_{i,j}=\dbinom
{n-1-i+j}{j-1}p_{n-1-i+j}$. Consider these elements $c_{i,j}$. Define an
$n\times n$-matrix $C$ by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Consider this matrix $C$. We have $c_{i,j}=0$ for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ be such that $i<j$. From $i<j$, we obtain $i\leq
j-1$ (since $i$ and $j$ are integers). Thus, $n-1-\underbrace{i}_{\leq
j-1}+j\geq n-1-\left(  j-1\right)  +j=n$. In other words, $n-1-i+j\in\left\{
n,n+1,n+2,\ldots\right\}  $. Hence, $p_{n-1-i+j}=0$ (by
(\ref{sol.vander-det.xi+yj.c.pk=0}), applied to $\ell=n-1-i+j$). Hence,
$c_{i,j}=\dbinom{n-1-i+j}{j-1}\underbrace{p_{n-1-i+j}}_{=0}=0$, qed.}. Hence,
Exercise \ref{exe.ps4.3} (applied to $C$ and $\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ instead of $A$ and $\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n,n}=\prod_{i=1}^{n}\underbrace{c_{i,i}%
}_{\substack{=\dbinom{n-1-i+i}{i-1}p_{n-1-i+i}\\\text{(by the definition of
}c_{i,i}\text{)}}}\nonumber\\
&  =\prod_{i=1}^{n}\left(  \underbrace{\dbinom{n-1-i+i}{i-1}}_{=\dbinom
{n-1}{i-1}}\underbrace{p_{n-1-i+i}}_{\substack{=p_{n-1}\\\text{(since
}n-1-i+i=n-1\text{)}}}\right) \nonumber\\
&  =\prod_{i=1}^{n}\left(  \dbinom{n-1}{i-1}p_{n-1}\right)  =\left(
\prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  p_{n-1}^{n}\nonumber\\
&  =p_{n-1}^{n}\left(  \prod_{i=1}^{n}\dbinom{n-1}{i-1}\right)  =p_{n-1}%
^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)
\label{sol.vander-det.xi+yj.c.detC}%
\end{align}
(here, we have substituted $k$ for $i-1$ in the product).

\item Define an $n\times n$-matrix $D$ by
\[
D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Then,%
\begin{align}
\det\underbrace{D}_{=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}}  &  =\det\left(  \left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq j<i\leq n}\left(  y_{i}-y_{j}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.vander-det}
\textbf{(d)}, applied to }y_{k}\text{ instead of }x_{k}\right) \nonumber\\
&  =\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)
\label{sol.vander-det.xi+yj.c.detD}%
\end{align}
(here, we have renamed the index $\left(  j,i\right)  $ as $\left(
i,j\right)  $ in the product).
\end{itemize}

Our goal is to prove that $\left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}=BCD$. Once this is done, we will be able to
compute $\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right)  $ by applying Theorem \ref{thm.det(AB)} twice.

We have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$D=\left(  y_{j}^{i-1}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the
definition of the product of two matrices shows that%
\begin{equation}
CD=\left(  \sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}. \label{sol.vander-det.xi+yj.c.CD.1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
\sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}  &  =\sum_{k=0}^{n-1}\underbrace{c_{i,k+1}%
}_{\substack{=\dbinom{n-1-i+\left(  k+1\right)  }{\left(  k+1\right)
-1}p_{n-1-i+\left(  k+1\right)  }\\\text{(by the definition of }%
c_{i,k+1}\text{)}}}y_{j}^{\left(  k+1\right)  -1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k+1\text{ for
}k\text{ in the sum}\right) \\
&  =\sum_{k=0}^{n-1}\underbrace{\dbinom{n-1-i+\left(  k+1\right)  }{\left(
k+1\right)  -1}}_{\substack{=\dbinom{n-i+k}{k}\\\text{(since }n-1-i+\left(
k+1\right)  =n-i+k\\\text{and }\left(  k+1\right)  -1=k\text{)}}%
}\underbrace{p_{n-1-i+\left(  k+1\right)  }}_{\substack{=p_{n-i+k}%
\\\text{(since }n-1-i+\left(  k+1\right)  =n-i+k\text{)}}}\underbrace{y_{j}%
^{\left(  k+1\right)  -1}}_{\substack{=y_{j}^{k}\\\text{(since }\left(
k+1\right)  -1=k\text{)}}}\\
&  =\sum_{k=0}^{n-1}\dbinom{n-i+k}{k}p_{n-i+k}y_{j}^{k}=\sum_{\ell=0}%
^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell}%
\end{align*}
(here, we renamed the summation index $k$ as $\ell$). Hence,
(\ref{sol.vander-det.xi+yj.c.CD.1}) becomes%
\begin{align*}
CD  &  =\left(  \underbrace{\sum_{k=1}^{n}c_{i,k}y_{j}^{k-1}}_{=\sum_{\ell
=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell}}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\\
&  =\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\end{align*}


Now, $B=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$CD=\left(  \sum_{\ell=0}^{n-1}\dbinom{n-i+\ell}{\ell}p_{n-i+\ell}y_{j}^{\ell
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence, the definition of the
product of two matrices shows that%
\begin{equation}
B\left(  CD\right)  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell
=0}^{n-1}\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}. \label{sol.vander-det.xi+yj.c.BCD.1}%
\end{equation}


Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Then,%
\begin{align}
&  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}{\ell
}p_{n-k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}x_{i}^{k}\left(  \sum_{\ell=0}^{n-1}\dbinom{k+\ell}{\ell
}p_{k+\ell}y_{j}^{\ell}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }k\text{ for
}n-k\text{ in the first sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=0}^{n-1}x_{i}^{k}\dbinom{k+\ell}{\ell}%
p_{k+\ell}y_{j}^{\ell}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1+k}x_{i}^{k}\underbrace{\dbinom
{k+\left(  \ell-k\right)  }{\ell-k}}_{\substack{=\dbinom{\ell}{\ell
-k}\\\text{(since }k+\left(  \ell-k\right)  =\ell\text{)}}%
}\underbrace{p_{k+\left(  \ell-k\right)  }}_{\substack{=p_{\ell}\\\text{(since
}k+\left(  \ell-k\right)  =\ell\text{)}}}y_{j}^{\ell-k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\ell-k\text{
for }\ell\text{ in the second sum}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\underbrace{\sum_{\ell=k}^{n-1+k}x_{i}^{k}\dbinom{\ell
}{\ell-k}p_{\ell}y_{j}^{\ell-k}}_{\substack{=\sum_{\ell=k}^{n-1}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}%
^{k}\dbinom{\ell}{\ell-k}p_{\ell}y_{j}^{\ell-k}\\\text{(since }k\leq n\leq
n-1+k+1\text{)}}}\nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\sum_{\ell=n}^{n-1+k}x_{i}^{k}\dbinom{\ell}{\ell
-k}\underbrace{p_{\ell}}_{\substack{=0\\\text{(by
(\ref{sol.vander-det.xi+yj.c.pk=0})}\\\text{(since }\ell\in\left\{
n,n+1,\ldots,n-1+k\right\}  \\\subseteq\left\{  n,n+1,n+2,\ldots\right\}
\text{))}}}y_{j}^{\ell-k}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\left(  \sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell
-k}p_{\ell}y_{j}^{\ell-k}+\underbrace{\sum_{\ell=n}^{n-1+k}x_{i}^{k}%
\dbinom{\ell}{\ell-k}0y_{j}^{\ell-k}}_{=0}\right) \nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell
}y_{j}^{\ell-k}. \label{sol.vander-det.xi+yj.c.BCD.2}%
\end{align}
On the other hand, $P\left(  X\right)  =\sum_{k=0}^{n-1}p_{k}X^{k}=\sum
_{\ell=0}^{n-1}p_{\ell}X^{\ell}$ (here, we renamed the summation index $k$ as
$\ell$). Hence,%
\begin{align}
P\left(  x_{i}+y_{j}\right)   &  =\sum_{\ell=0}^{n-1}p_{\ell}%
\underbrace{\left(  x_{i}+y_{j}\right)  ^{\ell}}_{\substack{=\sum_{k=0}^{\ell
}\dbinom{\ell}{k}x_{i}^{k}y_{j}^{\ell-k}\\\text{(by the binomial formula)}%
}}=\sum_{\ell=0}^{n-1}p_{\ell}\sum_{k=0}^{\ell}\dbinom{\ell}{k}x_{i}^{k}%
y_{j}^{\ell-k}\nonumber\\
&  =\underbrace{\sum_{\ell=0}^{n-1}\sum_{k=0}^{\ell}}_{=\sum
_{\substack{\left(  \ell,k\right)  \in\left\{  0,1,\ldots,n-1\right\}
^{2};\\k\leq\ell}}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}}p_{\ell}\dbinom{\ell
}{k}x_{i}^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}p_{\ell}\underbrace{\dbinom{\ell}{k}%
}_{\substack{=\dbinom{\ell}{\ell-k}\\\text{(by (\ref{eq.binom.symm}) (applied
to }\ell\text{ and }k\\\text{instead of }m\text{ and }n\text{))}}}x_{i}%
^{k}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}\underbrace{p_{\ell}\dbinom{\ell}%
{\ell-k}x_{i}^{k}}_{=x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell}}y_{j}^{\ell
-k}=\sum_{k=0}^{n-1}\sum_{\ell=k}^{n-1}x_{i}^{k}\dbinom{\ell}{\ell-k}p_{\ell
}y_{j}^{\ell-k}\nonumber\\
&  =\sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}%
{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.c.BCD.2})}\right)  .
\label{sol.vander-det.xi+yj.c.BCD.3}%
\end{align}


Now, let us forget that we fixed $\left(  i,j\right)  $. We thus have proven
(\ref{sol.vander-det.xi+yj.c.BCD.3}) for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence,%
\begin{align*}
&  \left(  \underbrace{P\left(  x_{i}+y_{j}\right)  }_{\substack{=\sum
_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}\dbinom{n-k+\ell}{\ell
}p_{n-k+\ell}y_{j}^{\ell}\right)  \\\text{(by
(\ref{sol.vander-det.xi+yj.c.BCD.3}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\\
&  =\left(  \sum_{k=1}^{n}x_{i}^{n-k}\left(  \sum_{\ell=0}^{n-1}%
\dbinom{n-k+\ell}{\ell}p_{n-k+\ell}y_{j}^{\ell}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=B\left(  CD\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.vander-det.xi+yj.c.BCD.1})}\right) \\
&  =BCD.
\end{align*}
Hence,%
\begin{align*}
&  \det\underbrace{\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{=BCD}\\
&  =\det\left(  BCD\right)  =\det B\cdot\underbrace{\det\left(  CD\right)
}_{\substack{=\det C\cdot\det D\\\text{(by Theorem \ref{thm.det(AB)}, applied
to }C\text{ and }D\\\text{instead of }A\text{ and }B\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}B\text{ and }CD\text{ instead of }A\text{ and }B\right) \\
&  =\underbrace{\det B}_{\substack{=\prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.c.detB}))}}%
}\cdot\underbrace{\det C}_{\substack{=p_{n-1}^{n}\left(  \prod_{k=0}%
^{n-1}\dbinom{n-1}{k}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.c.detC}%
))}}}\cdot\underbrace{\det D}_{\substack{=\prod_{1\leq i<j\leq n}\left(
y_{j}-y_{i}\right)  \\\text{(by (\ref{sol.vander-det.xi+yj.c.detD}))}}}\\
&  =\left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \cdot
p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \cdot\left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =p_{n-1}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(c)}.

\textbf{(a)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be
the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

Let $m\in\left\{  0,1,\ldots,n-1\right\}  $. (You are reading this right: We
are not requiring $m$ to belong to $\left\{  0,1,\ldots,n-2\right\}  $; the
purpose of this is to obtain a result that will bring us close to solving both
parts \textbf{(a)} and \textbf{(b)} simultaneously.)

Define an $n$-tuple $\left(  p_{0},p_{1},\ldots,p_{n-1}\right)  \in
\mathbb{K}^{n}$ by%
\[
\left(  p_{k}=\delta_{k,m}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{
0,1,\ldots,n-1\right\}  \right)  .
\]
Let $P\left(  X\right)  \in\mathbb{K}\left[  X\right]  $ be the polynomial
$\sum_{k=0}^{n-1}p_{k}X^{k}$.

We have%
\begin{align*}
P\left(  X\right)   &  =\underbrace{\sum_{k=0}^{n-1}}_{=\sum_{k\in\left\{
0,1,\ldots,n-1\right\}  }}\underbrace{p_{k}}_{\substack{=\delta_{k,m}%
\\\text{(by the definition}\\\text{of }p_{k}\text{)}}}X^{k}=\sum_{k\in\left\{
0,1,\ldots,n-1\right\}  }\delta_{k,m}X^{k}\\
&  =\underbrace{\delta_{m,m}}_{\substack{=1\\\text{(since }m=m\text{)}}%
}X^{m}+\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}  ;\\k\neq
m}}\underbrace{\delta_{k,m}}_{\substack{=0\\\text{(since }k\neq m\text{)}%
}}X^{k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }k=m\text{ from the sum}\\
\text{(since }m\in\left\{  0,1,\ldots,n-1\right\}  \text{)}%
\end{array}
\right) \\
&  =X^{m}+\underbrace{\sum_{\substack{k\in\left\{  0,1,\ldots,n-1\right\}
;\\k\neq m}}0X^{k}}_{=0}=X^{m}.
\end{align*}
Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies
$P\left(  x_{i}+y_{j}\right)  =\left(  x_{i}+y_{j}\right)  ^{m}$ (since
$P\left(  X\right)  =X^{m}$). In other words, $\left(  P\left(  x_{i}%
+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  \left(
x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,%
\[
\left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Taking determinants on both sides of this equality, we obtain%
\begin{align}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\det\left(  \left(  P\left(  x_{i}+y_{j}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \nonumber\\
&  =\underbrace{p_{n-1}^{n}}_{\substack{=\delta_{n-1,m}^{n}\\\text{(since
}p_{n-1}=\delta_{n-1,m}\\\text{(by the definition of }p_{n-1}\text{))}%
}}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.vander-det.xi+yj}
\textbf{(c)}}\right) \nonumber\\
&  =\delta_{n-1,m}^{n}\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(
\prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(
\prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right)  .
\label{sol.vander-det.xi+yj.a.almost}%
\end{align}


Now, let us forget that we fixed $m$. We thus have proven
(\ref{sol.vander-det.xi+yj.a.almost}) for every $m\in\left\{  0,1,\ldots
,n-1\right\}  $.

Now, let $m\in\left\{  0,1,\ldots,n-2\right\}  $. Thus, $m\neq n-1$, so that
$n-1\neq m$. Thus, $\delta_{n-1,m}=0$.

But $n$ is a positive integer. Hence, $0^{n}=0$. Now, taking the equality
$\delta_{n-1,m}=0$ to the $n$-th power, we obtain $\delta_{n-1,m}^{n}=0^{n}=0$.

On the other hand, $m\in\left\{  0,1,\ldots,n-2\right\}  \subseteq\left\{
0,1,\ldots,n-1\right\}  $, and therefore (\ref{sol.vander-det.xi+yj.a.almost})
holds. Hence,%
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{m}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,m}^{n}}_{=0}\left(  \prod_{k=0}^{n-1}\dbinom
{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)
\right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}-y_{i}\right)  \right) \\
&  =0.
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}.

\textbf{(b)} For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be
the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

We have $\delta_{n-1,n-1}=1$ (since $n-1=n-1$) and thus $\delta_{n-1,n-1}%
^{n}=1^{n}=1$.

In our solution of Exercise \ref{exe.vander-det.xi+yj} \textbf{(a)}, we have
proven the equality (\ref{sol.vander-det.xi+yj.a.almost}) for every
$m\in\left\{  0,1,\ldots,n-1\right\}  $. Thus, we can apply this equality to
$m=n-1$. As a result, we obtain
\begin{align*}
&  \det\left(  \left(  \left(  x_{i}+y_{j}\right)  ^{n-1}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\right) \\
&  =\underbrace{\delta_{n-1,n-1}^{n}}_{=1}\left(  \prod_{k=0}^{n-1}%
\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq i<j\leq n}\left(  x_{i}%
-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq n}\left(  y_{j}%
-y_{i}\right)  \right) \\
&  =\left(  \prod_{k=0}^{n-1}\dbinom{n-1}{k}\right)  \left(  \prod_{1\leq
i<j\leq n}\left(  x_{i}-x_{j}\right)  \right)  \left(  \prod_{1\leq i<j\leq
n}\left(  y_{j}-y_{i}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.xi+yj} \textbf{(b)}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.cauchy-det}}

Exercise \ref{exe.cauchy-det} is obtained from \cite[Exercise 2.67(a)]{Reiner}
by setting $a_{i}=x_{i}$ and $b_{j}=-y_{j}$. (See the ancillary PDF file of
the arXiv version of \cite{Reiner} for the solutions to the exercises.)

Exercise \ref{exe.cauchy-det} can also be obtained from \cite[Theorem
2]{Gri-19.9} by setting $k=\mathbb{K}$, $m=n$, $a_{j}=y_{j}$ and $b_{i}%
=-x_{i}$ (and observing that
\begin{align*}
&  \prod_{\substack{\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\i>j}}\underbrace{\left(  \left(  y_{i}-y_{j}\right)  \left(  \left(
-x_{j}\right)  -\left(  -x_{i}\right)  \right)  \right)  }_{\substack{=\left(
y_{j}-y_{i}\right)  \left(  x_{j}-x_{i}\right)  \\=\left(  x_{j}-x_{i}\right)
\left(  y_{j}-y_{i}\right)  }}\\
&  =\prod_{\substack{\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\i>j}}\left(  \left(  x_{j}-x_{i}\right)  \left(  y_{j}-y_{i}\right)
\right)  =\prod_{\substack{\left(  j,i\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2};\\j>i}}\left(  \left(  x_{i}-x_{j}\right)  \left(
y_{i}-y_{j}\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
i,j\right)  \text{ as }\left(  j,i\right)  \right) \\
&  =\prod_{1\leq i<j\leq n}\left(  \left(  x_{i}-x_{j}\right)  \left(
y_{i}-y_{j}\right)  \right)
\end{align*}
). The statement of \cite[Theorem 2]{Gri-19.9} makes the requirement that $k$
be a field; however, this is easily seen to be unnecessary for the proof.

\subsection{Solution to Exercise \ref{exe.cauchy-det-lem}}

Exercise \ref{exe.cauchy-det-lem} is the equality (12.218) in \cite[ancillary
PDF file]{Reiner}.

\subsection{Solution to Exercise \ref{exe.det.schur-lem}}

\begin{proof}
[Solution to Exercise \ref{exe.det.schur-lem}.]For every $n\times n$-matrix
$\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, we have%
\begin{equation}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod\limits_{i=1}%
^{n}c_{\sigma\left(  i\right)  ,i} \label{sol.det.schur-lem.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.schur-lem.1}):} Let $\left(
c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be any $n\times n$-matrix.
Then, Exercise \ref{exe.ps4.4} (applied to $A=\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$) yields%
\[
\det\left(  \left(  \left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  ^{T}\right)  =\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  .
\]
Hence,%
\begin{align*}
\det\left(  \left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
&  =\det\left(  \underbrace{\left(  \left(  c_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  ^{T}}_{\substack{=\left(  c_{j,i}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\\\text{(by the definition of the}\\\text{transpose of
a matrix)}}}\right)  =\det\left(  \left(  c_{j,i}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right) \\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
c_{\sigma\left(  i\right)  ,i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.det.eq.2}), applied to }\left(  c_{j,i}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\text{ and }c_{j,i}\\
\text{instead of }A\text{ and }a_{i,j}%
\end{array}
\right)  .
\end{align*}
This proves (\ref{sol.det.schur-lem.1}).}. Applied to $\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, this yields%
\begin{equation}
\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod\limits_{i=1}%
^{n}a_{\sigma\left(  i\right)  ,i}. \label{sol.det.schur-lem.2}%
\end{equation}


Now, let $k\in\left\{  1,2,\ldots,n\right\}  $. For every $\sigma\in S_{n}$,
we have%
\begin{align}
\underbrace{\prod\limits_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }}b_{\sigma\left(  i\right)  }^{\delta_{i,k}}  &  =\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{\sigma\left(  i\right)  }%
^{\delta_{i,k}}=\underbrace{b_{\sigma\left(  k\right)  }^{\delta_{k,k}}%
}_{\substack{=b_{\sigma\left(  k\right)  }^{1}\\\text{(since }\delta
_{k,k}=1\text{)}}}\prod\limits_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\underbrace{b_{\sigma\left(  i\right)  }^{\delta_{i,k}}%
}_{\substack{=b_{\sigma\left(  i\right)  }^{0}\\\text{(since }\delta
_{i,k}=0\\\text{(since }i\neq k\text{))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the factor for
}i=k\text{ from the product}\right) \nonumber\\
&  =\underbrace{b_{\sigma\left(  k\right)  }^{1}}_{=b_{\sigma\left(  k\right)
}}\prod\limits_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
k}}\underbrace{b_{\sigma\left(  i\right)  }^{0}}_{=1}=b_{\sigma\left(
k\right)  }\underbrace{\prod\limits_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq k}}1}_{=1}=b_{\sigma\left(  k\right)  }.
\label{sol.det.schur-lem.prod}%
\end{align}
But we can apply (\ref{sol.det.schur-lem.1}) to $\left(  c_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$, and thus obtain%
\begin{align}
\det\left(  \left(  a_{i,j}b_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)   &  =\sum\limits_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\underbrace{\prod\limits_{i=1}^{n}\left(  a_{\sigma\left(
i\right)  ,i}b_{\sigma\left(  i\right)  }^{\delta_{i,k}}\right)  }_{=\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  \left(
\prod\limits_{i=1}^{n}b_{\sigma\left(  i\right)  }^{\delta_{i,k}}\right)
}\nonumber\\
&  =\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)
\underbrace{\left(  \prod\limits_{i=1}^{n}b_{\sigma\left(  i\right)  }%
^{\delta_{i,k}}\right)  }_{\substack{=b_{\sigma\left(  k\right)  }\\\text{(by
(\ref{sol.det.schur-lem.prod}))}}}\nonumber\\
&  =\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  b_{\sigma\left(
k\right)  }. \label{sol.det.schur-lem.k-th-term}%
\end{align}


Now, let us forget that we fixed $k$. We thus have proven
(\ref{sol.det.schur-lem.k-th-term}) for every $k\in\left\{  1,2,\ldots
,n\right\}  $. On the other hand, for every $\sigma\in S_{n}$, we have%
\begin{align}
\underbrace{\sum_{k=1}^{n}}_{=\sum_{k\in\left\{  1,2,\ldots,n\right\}  }%
}b_{\sigma\left(  k\right)  }  &  =\sum_{k\in\left\{  1,2,\ldots,n\right\}
}b_{\sigma\left(  k\right)  }=\underbrace{\sum_{k\in\left\{  1,2,\ldots
,n\right\}  }}_{=\sum_{k=1}^{n}}b_{k}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }k\text{ for }\sigma\left(  k\right)  \text{
in the sum,}\\
\text{since }\sigma\text{ is a bijection }\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\} \\
\text{(since }\sigma\text{ is a permutation of }\left\{  1,2,\ldots,n\right\}
\text{ (since }\sigma\in S_{n}\text{))}%
\end{array}
\right) \nonumber\\
&  =\sum_{k=1}^{n}b_{k}=b_{1}+b_{2}+\cdots+b_{n}.
\label{sol.det.schur-lem.sum-over-k}%
\end{align}
Now,%
\begin{align*}
&  \sum\limits_{k=1}^{n}\underbrace{\det\left(  \left(  a_{i,j}b_{i}%
^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)
}_{\substack{=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  b_{\sigma\left(
k\right)  }\\\text{(by (\ref{sol.det.schur-lem.k-th-term}))}}}\\
&  =\sum_{k=1}^{n}\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma
}\left(  \prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)
b_{\sigma\left(  k\right)  }=\sum\limits_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\left(  \prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)
\underbrace{\sum_{k=1}^{n}b_{\sigma\left(  k\right)  }}_{\substack{=b_{1}%
+b_{2}+\cdots+b_{n}\\\text{(by (\ref{sol.det.schur-lem.sum-over-k}))}}}\\
&  =\sum\limits_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(
\prod\limits_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}\right)  \left(
b_{1}+b_{2}+\cdots+b_{n}\right) \\
&  =\left(  b_{1}+b_{2}+\cdots+b_{n}\right)  \underbrace{\sum\limits_{\sigma
\in S_{n}}\left(  -1\right)  ^{\sigma}\prod\limits_{i=1}^{n}a_{\sigma\left(
i\right)  ,i}}_{\substack{=\det\left(  \left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  \\\text{(by (\ref{sol.det.schur-lem.2}))}}}\\
&  =\left(  b_{1}+b_{2}+\cdots+b_{n}\right)  \det\left(  \left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This solves Exercise \ref{exe.det.schur-lem}.
\end{proof}

\subsection{\label{sect.sols.vander-det.s1.sol2}Second solution to Exercise
\ref{exe.vander-det.s1}}

Exercise \ref{exe.det.schur-lem} can be used to give a new (and simpler)
solution to Exercise \ref{exe.vander-det.s1}, suggested by Karthik Karnik:

\begin{vershort}
\begin{proof}
[Second solution to Exercise \ref{exe.vander-det.s1}.]Exercise
\ref{exe.det.schur-lem} (applied to $a_{i,j}=x_{i}^{n-j}$ and $b_{i}=x_{i}$)
shows that%
\begin{align}
\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)   &  =\left(  x_{1}%
+x_{2}+\cdots+x_{n}\right)  \underbrace{\det\left(  \left(  x_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{\substack{=\prod
_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \\\text{(by Theorem
\ref{thm.vander-det} \textbf{(a)})}}}\nonumber\\
&  =\left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  . \label{sol.vander-det.s1.sol2.short.1}%
\end{align}


The left hand side of this equality is a sum of $n$ determinants. We shall now
show that $n-1$ of these determinants (namely, the ones that appear as addends
for $k>1$ in the sum) are $0$.

Indeed, every $k\in\left\{  2,3,\ldots,n\right\}  $ satisfies%
\begin{equation}
\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0 \label{sol.vander-det.s1.sol2.short.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.short.2}):} Let
$k\in\left\{  2,3,\ldots,n\right\}  $. Let $A$ be the $n\times n$-matrix
$\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$.
\par
Let $h\in\left\{  1,2,\ldots,n\right\}  $. Since $A=\left(  x_{i}^{n-j}%
x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, we have%
\begin{equation}
\left(  \text{the }\left(  h,k\right)  \text{-th entry of }A\right)
=x_{h}^{n-k}\underbrace{x_{h}^{\delta_{k,k}}}_{\substack{=x_{h}^{1}%
\\\text{(since }\delta_{k,k}=1\text{)}}}=x_{h}^{n-k}x_{h}^{1}=x_{h}^{n-k+1}.
\label{sol.vander-det.s1.sol2.short.2.pf.0}%
\end{equation}
On the other hand, $k-1\in\left\{  1,2,\ldots,n\right\}  $ (since
$k\in\left\{  2,3,\ldots,n\right\}  $), and thus the $\left(  h,k-1\right)
$-th entry of $A$ exists. Since $A=\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, this entry satisfies%
\[
\left(  \text{the }\left(  h,k-1\right)  \text{-th entry of }A\right)
=x_{h}^{n-\left(  k-1\right)  }\underbrace{x_{h}^{\delta_{k-1,k}}%
}_{\substack{=x_{h}^{0}\\\text{(since }\delta_{k-1,k}=0\text{)}}%
}=x_{h}^{n-\left(  k-1\right)  }x_{h}^{0}=x_{h}^{n-\left(  k-1\right)  }%
=x_{h}^{n-k+1}.
\]
Comparing this with (\ref{sol.vander-det.s1.sol2.short.2.pf.0}), we obtain%
\[
\left(  \text{the }\left(  h,k\right)  \text{-th entry of }A\right)  =\left(
\text{the }\left(  h,k-1\right)  \text{-th entry of }A\right)  .
\]
\par
Now, let us forget that we fixed $h$. We thus have shown that $\left(
\text{the }\left(  h,k\right)  \text{-th entry of }A\right)  =\left(
\text{the }\left(  h,k-1\right)  \text{-th entry of }A\right)  $ for every
$h\in\left\{  1,2,\ldots,n\right\}  $. In other words, the $k$-th column of
$A$ equals the $\left(  k-1\right)  $-st column of $A$. Thus, the matrix $A$
has two equal columns (since $k-1\neq k$). Therefore, Exercise \ref{exe.ps4.6}
\textbf{(f)} shows that $\det A=0$. Since $A=\left(  x_{i}^{n-j}x_{i}%
^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, this rewrites as
$\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0$. Qed.}. Furthermore, every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\label{sol.vander-det.s1.sol2.short.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.short.4}):} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. We need to prove
the equality (\ref{sol.vander-det.s1.sol2.short.4}). To do so, it clearly
satisfies to prove the following two claims:
\par
\textit{Claim 1:} If $j>1$, then $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=x_{i}^{n-j}%
$.
\par
\textit{Claim 2:} If $j=1$, then $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=x_{i}^{n}$.
\par
\textit{Proof of Claim 1:} Assume that $j>1$. Thus, $j\neq1$, so that
$\delta_{j,1}=0$, so that $x_{i}^{\delta_{j,1}}=x_{i}^{0}=1$. Hence,
$x_{i}^{n-j}\underbrace{x_{i}^{\delta_{j,1}}}_{=1}=x_{i}^{n-j}$, and thus
Claim 1 is proven.
\par
\textit{Proof of Claim 2:} Assume that $j=1$. Thus, $\delta_{j,1}=1$, so that
$x_{i}^{\delta_{j,1}}=x_{i}^{1}$ and thus $x_{i}^{n-j}\underbrace{x_{i}%
^{\delta_{j,1}}}_{=x_{i}^{1}}=x_{i}^{n-j}x_{i}^{1}=x_{i}^{\left(  n-j\right)
+1}=x_{i}^{\left(  n-1\right)  +1}$ (since $j=1$), so that $x_{i}^{n-j}%
x_{i}^{\delta_{j,1}}=x_{i}^{\left(  n-1\right)  +1}=x_{i}^{n}$. This proves
Claim 2.
\par
Hence, (\ref{sol.vander-det.s1.sol2.short.4}) is proven (since we have proven
Claims 1 and 2).}. Now, (\ref{sol.vander-det.s1.sol2.short.1}) shows that%
\begin{align*}
&  \left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right) \\
&  =\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\det\left(  \left(  \underbrace{x_{i}^{n-j}x_{i}^{\delta_{j,1}}%
}_{\substack{=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\\\text{(by (\ref{sol.vander-det.s1.sol2.short.4}))}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  +\sum\limits_{k=2}^{n}\underbrace{\det\left(
\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  }_{\substack{=0\\\text{(by (\ref{sol.vander-det.s1.sol2.short.2}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=1\text{ from the sum}\right) \\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  +\underbrace{\sum
\limits_{k=2}^{n}0}_{=0}\\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.s1} again.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Second solution to Exercise \ref{exe.vander-det.s1}.]Exercise
\ref{exe.det.schur-lem} (applied to $a_{i,j}=x_{i}^{n-j}$ and $b_{i}=x_{i}$)
shows that%
\begin{align}
\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)   &  =\left(  x_{1}%
+x_{2}+\cdots+x_{n}\right)  \underbrace{\det\left(  \left(  x_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  }_{\substack{=\prod
_{1\leq i<j\leq n}\left(  x_{i}-x_{j}\right)  \\\text{(by Theorem
\ref{thm.vander-det} \textbf{(a)})}}}\nonumber\\
&  =\left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right)  . \label{sol.vander-det.s1.sol2.1}%
\end{align}


Now, every $k\in\left\{  2,3,\ldots,n\right\}  $ satisfies%
\begin{equation}
\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  =0 \label{sol.vander-det.s1.sol2.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.2}):} Let
$k\in\left\{  2,3,\ldots,n\right\}  $. Let $A$ be the $n\times n$-matrix
$\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Thus,%
\begin{align}
\left(  \text{the }k\text{-th column of }A\right)   &  =\left(
\begin{array}
[c]{c}%
x_{1}^{n-k}x_{1}^{\delta_{k,k}}\\
x_{2}^{n-k}x_{2}^{\delta_{k,k}}\\
\vdots\\
x_{n}^{n-k}x_{n}^{\delta_{k,k}}%
\end{array}
\right)  =\left(  x_{i}^{n-k}\underbrace{x_{i}^{\delta_{k,k}}}%
_{\substack{=x_{i}^{1}\\\text{(since }\delta_{k,k}=1\\\text{(since
}k=k\text{))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}\nonumber\\
&  =\left(  \underbrace{x_{i}^{n-k}x_{i}^{1}}_{=x_{i}^{\left(  n-k\right)
+1}}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}=\left(  x_{i}^{\left(
n-k\right)  +1}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\label{sol.vander-det.s1.sol2.2.pf.1}%
\end{align}
On the other hand, we have $k\in\left\{  2,3,\ldots,n\right\}  $, so that
$k-1\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  $. Hence, the $\left(  k-1\right)  $-th column of $A$ exists. From
$A=\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$, we obtain%
\begin{align*}
\left(  \text{the }\left(  k-1\right)  \text{-th column of }A\right)   &
=\left(
\begin{array}
[c]{c}%
x_{1}^{n-\left(  k-1\right)  }x_{1}^{\delta_{k-1,k}}\\
x_{2}^{n-\left(  k-1\right)  }x_{2}^{\delta_{k-1,k}}\\
\vdots\\
x_{n}^{n-\left(  k-1\right)  }x_{n}^{\delta_{k-1,k}}%
\end{array}
\right)  =\left(  x_{i}^{n-\left(  k-1\right)  }\underbrace{x_{i}%
^{\delta_{k-1,k}}}_{\substack{=x_{i}^{0}\\\text{(since }\delta_{k-1,k}%
=0\\\text{(since }k-1\neq k\text{))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
1}\\
&  =\left(  \underbrace{x_{i}^{n-\left(  k-1\right)  }}_{=x_{i}^{\left(
n-k\right)  +1}}\underbrace{x_{i}^{0}}_{=1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}=\left(  x_{i}^{\left(  n-k\right)  +1}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
&  =\left(  \text{the }k\text{-th column of }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.vander-det.s1.sol2.2.pf.1}%
)}\right)  .
\end{align*}
Thus, the matrix $A$ has two equal columns (since $k-1\neq k$). Therefore,
Exercise \ref{exe.ps4.6} \textbf{(f)} shows that $\det A=0$. Since $A=\left(
x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
this rewrites as $\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right)  =0$. Qed.}. Furthermore, every
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfies%
\begin{equation}
x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\label{sol.vander-det.s1.sol2.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-det.s1.sol2.4}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Thus, $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. We need to
prove the equality (\ref{sol.vander-det.s1.sol2.4}).
\par
We distinguish between the following two cases:
\par
\textit{Case 1:} We have $j\neq1$.
\par
\textit{Case 2:} We have $j=1$.
\par
Let us first consider Case 1. In this case, we have $j\neq1$. Thus,
$\delta_{j,1}=0$, so that $x_{i}^{\delta_{j,1}}=x_{i}^{0}=1$. Since
$j\in\left\{  1,2,\ldots,n\right\}  $ and $j\neq1$, we have $j\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  1\right\}  =\left\{  2,3,\ldots
,n\right\}  $, so that $j>1$. Thus, $%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
=x_{i}^{n-j}$. Compared with $x_{i}^{n-j}\underbrace{x_{i}^{\delta_{j,1}}%
}_{=1}=x_{i}^{n-j}$, this yields $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
$. Thus, (\ref{sol.vander-det.s1.sol2.4}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $j=1$. Thus, $\delta
_{j,1}=1$, so that $x_{i}^{\delta_{j,1}}=x_{i}^{1}$ and thus $x_{i}%
^{n-j}\underbrace{x_{i}^{\delta_{j,1}}}_{=x_{i}^{1}}=x_{i}^{n-j}x_{i}%
^{1}=x_{i}^{\left(  n-j\right)  +1}=x_{i}^{\left(  n-1\right)  +1}$ (since
$j=1$), so that $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=x_{i}^{\left(  n-1\right)
+1}=x_{i}^{n}$. Compared with $%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
=x_{i}^{n}$ (since $j=1$), this yields $x_{i}^{n-j}x_{i}^{\delta_{j,1}}=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
$. Thus, (\ref{sol.vander-det.s1.sol2.4}) is proven in Case 2.
\par
Now, we have proven (\ref{sol.vander-det.s1.sol2.4}) in each of the two Cases
1 and 2. Since these two Cases cover all possibilities, we thus see that
(\ref{sol.vander-det.s1.sol2.4}) always holds. Qed.}. Now,
(\ref{sol.vander-det.s1.sol2.1}) shows that%
\begin{align*}
&  \left(  x_{1}+x_{2}+\cdots+x_{n}\right)  \prod_{1\leq i<j\leq n}\left(
x_{i}-x_{j}\right) \\
&  =\sum\limits_{k=1}^{n}\det\left(  \left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\det\left(  \left(  \underbrace{x_{i}^{n-j}x_{i}^{\delta_{j,1}}%
}_{\substack{=%
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\\\text{(by (\ref{sol.vander-det.s1.sol2.4}))}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}\right)  +\sum\limits_{k=2}^{n}\underbrace{\det\left(
\left(  x_{i}^{n-j}x_{i}^{\delta_{j,k}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  }_{\substack{=0\\\text{(by (\ref{sol.vander-det.s1.sol2.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=1\text{ from the sum}\right) \\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  +\underbrace{\sum
\limits_{k=2}^{n}0}_{=0}\\
&  =\det\left(  \left(
\begin{cases}
x_{i}^{n-j}, & \text{if }j>1;\\
x_{i}^{n}, & \text{if }j=1
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\end{align*}
This solves Exercise \ref{exe.vander-det.s1} again.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.det.a1a2anx}}

\begin{proof}
[First solution to Exercise \ref{exe.det.a1a2anx}.]The following solution will
rely on Exercise \ref{exe.ps4.6k} and \ref{exe.ps4.3}. Since this is not the
first time (nor the second) that we are using these exercises, I shall be brief.

Let $A$ be the $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix
$\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  $. We need to prove that $\det A=\left(  x+\sum_{i=1}^{n}%
a_{i}\right)  \prod_{i=1}^{n}\left(  x-a_{i}\right)  $.

We perform the following operations on the matrix $A$ (in this order):

\begin{itemize}
\item We subtract the $2$-nd row from the $1$-st row.

\item We subtract the $3$-rd row from the $2$-nd row.

\item $\ldots$

\item We subtract the $\left(  n+1\right)  $-th row from the $n$-th row.
\end{itemize}

As we know from Exercise \ref{exe.ps4.6k} \textbf{(a)}, these operations do
not change the determinant of the matrix. Thus, if we denote by $B$ the result
of these operations, then $\det B=\det A$. On the other hand, it is easy to
see that%
\[
B=\left(
\begin{array}
[c]{cccccc}%
x-a_{1} & a_{1}-x & 0 & \cdots & 0 & 0\\
0 & x-a_{2} & a_{2}-x & \cdots & 0 & 0\\
0 & 0 & x-a_{3} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & x-a_{n} & a_{n}-x\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  .
\]
(Here, for every $i\in\left\{  1,2,\ldots,n\right\}  $, the $i$-th row of $B$
has $i$-th entry $x-a_{i}$ and $\left(  i+1\right)  $-th entry $a_{i}-x$; all
other entries in this row are $0$. The $\left(  n+1\right)  $-th row of $B$ is
$\left(  a_{1},a_{2},\ldots,a_{n},x\right)  $.)

Next, we apply the following operations to the matrix $B$ (in this order):

\begin{itemize}
\item We add the $1$-st column to the $2$-nd column.

\item We add the $2$-nd column to the $3$-rd column.

\item $\ldots$

\item We add the $n$-th column to the $\left(  n+1\right)  $-th column.
\end{itemize}

(Notice that the order in which we are performing these operations forces
their effects to accumulate; namely, at every step, the column that we are
adding has already been modified by the previous step.) As we know from
Exercise \ref{exe.ps4.6k} \textbf{(b)}, these operations do not change the
determinant of the matrix. Thus, if we denote by $C$ the result of these
operations, then $\det C=\det B=\det A$. On the other hand, it is easy to see
that%
\begin{align*}
&  C\\
&  =\left(
\begin{array}
[c]{cccccc}%
x-a_{1} & 0 & 0 & \cdots & 0 & 0\\
0 & x-a_{2} & 0 & \cdots & 0 & 0\\
0 & 0 & x-a_{3} & \cdots & 0 & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & 0 & \cdots & x-a_{n} & 0\\
a_{1} & a_{1}+a_{2} & a_{1}+a_{2}+a_{3} & \cdots & a_{1}+a_{2}+\cdots+a_{n} &
x+a_{1}+a_{2}+\cdots+a_{n}%
\end{array}
\right)  .
\end{align*}
This is a lower-triangular matrix. Thus, Exercise \ref{exe.ps4.3} shows that
its determinant is the product of its diagonal entries. In other words,%
\begin{align*}
\det C  &  =\left(  x-a_{1}\right)  \left(  x-a_{2}\right)  \cdots\left(
x-a_{n}\right)  \left(  x+a_{1}+a_{2}+\cdots+a_{n}\right) \\
&  =\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\end{align*}
Compared with $\det C=\det A$, this yields $\det A=\left(  x+\sum_{i=1}%
^{n}a_{i}\right)  \prod_{i=1}^{n}\left(  x-a_{i}\right)  $. This solves
Exercise \ref{exe.det.a1a2anx}.
\end{proof}

\begin{vershort}
\begin{proof}
[Second solution to Exercise \ref{exe.det.a1a2anx}.]For any $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define an element
$u_{i,j}\in\mathbb{K}$ by%
\[
u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
.
\]
This $u_{i,j}$ is well-defined\footnote{\textit{Proof.} It is sufficient to
check that $a_{j}$ is well-defined when $j<i$, and that $a_{j-1}$ is
well-defined when $j>i$ (because $x$ is always well-defined). But this is
easy:
\par
\begin{itemize}
\item If $j<i$, then $j\in\left\{  1,2,\ldots,n\right\}  $ (since $j<i\leq
n+1$ yields $j\leq n$), and thus $a_{j}$ is well-defined.
\par
\item If $j>i$, then $j\in\left\{  2,3,\ldots,n+1\right\}  $ (since $j>i\geq1$
yields $j\geq2$), and thus $a_{j-1}$ is well-defined.
\end{itemize}
}. Now, we define an $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix
$U$ by $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Thus,%
\[
U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  .
\]
Our goal is now to prove that $\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  $.

For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define
an element $s_{i,j}\in\mathbb{K}$ by%
\[
s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
.
\]
Now, we define an $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix $S$
by $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. The
matrix $S$ is upper-triangular\footnote{It looks as follows: $S=\left(
\begin{array}
[c]{cccc}%
1 & 1 & \cdots & 1\\
0 & 1 & \cdots & 1\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1
\end{array}
\right)  $.}. Since the determinant of an upper-triangular matrix is the
product of its diagonal entries, we thus obtain $\det S=1\cdot1\cdot
\cdots\cdot1=1$.

We extend the $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{K}^{n}$ to an $\left(  n+1\right)  $-tuple $\left(  a_{1}%
,a_{2},\ldots,a_{n+1}\right)  \in\mathbb{K}^{n+1}$ by setting $a_{n+1}=0$.
Thus, an element $a_{k}\in\mathbb{K}$ is defined for every $k\in\left\{
1,2,\ldots,n+1\right\}  $.

Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.short.US}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.US}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.short.US}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$1\leq i\leq n+1$ and $1\leq j\leq n+1$. Now,%
\begin{align}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}  &  =\sum_{k=1}^{j}u_{i,k}\underbrace{s_{k,j}%
}_{\substack{=1\\\text{(by the definition of}\\s_{k,j}\text{, since }k\leq
j\text{)}}}+\sum_{k=j+1}^{n+1}u_{i,k}\underbrace{s_{k,j}}%
_{\substack{=0\\\text{(by the definition of}\\s_{k,j}\text{, since
}k>j\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq j\leq n+1\right)
\nonumber\\
&  =\sum_{k=1}^{j}u_{i,k}1+\underbrace{\sum_{k=j+1}^{n+1}u_{i,k}0}_{=0}%
=\sum_{k=1}^{j}u_{i,k}1=\sum_{k=1}^{j}u_{i,k}.
\label{sol.det.a1a2anx.short.US.pf.1}%
\end{align}
\par
Now, we must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq j$.
\par
\textit{Case 2:} We have $i>j$.
\par
Let us first consider Case 1. In this case, we have $i\leq j$. The definition
of $s_{i,j}$ shows that $s_{i,j}=1$ (since $i\leq j$). Therefore,%
\begin{equation}
\underbrace{\sum_{k=1}^{j}a_{k}}_{\substack{=\sum_{k=1}^{j-1}a_{k}%
+a_{j}\\\text{(here, we have split off the}\\\text{addend for }k=j\text{ from
the sum)}}}+\underbrace{s_{i,j}}_{=1}\left(  x-a_{j}\right)  =\sum_{k=1}%
^{j-1}a_{k}+a_{j}+\left(  x-a_{j}\right)  =\sum_{k=1}^{j-1}a_{k}+x.
\label{sol.det.a1a2anx.short.US.pf.3}%
\end{equation}
\par
We have $1\leq i\leq j$. Thus, $0\leq i-1\leq j-1$. Now,
(\ref{sol.det.a1a2anx.short.US.pf.1}) becomes%
\begin{align*}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}  &  =\sum_{k=1}^{j}u_{i,k}=\underbrace{\sum
_{k=1}^{i}u_{i,k}}_{\substack{=\sum_{k=1}^{i-1}u_{i,k}+u_{i,i}\\\text{(here,
we have split off the}\\\text{addend for }k=i\text{ from the sum)}}%
}+\sum_{k=i+1}^{j}u_{i,k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq i\leq
j\right) \\
&  =\sum_{k=1}^{i-1}\underbrace{u_{i,k}}_{\substack{=a_{k}\\\text{(by the
definition of }u_{i,k}\text{,}\\\text{since }k<i\text{)}}}+\underbrace{u_{i,i}%
}_{\substack{=x\\\text{(by the definition of }u_{i,i}\text{,}\\\text{since
}i=i\text{)}}}+\sum_{k=i+1}^{j}\underbrace{u_{i,k}}_{\substack{=a_{k-1}%
\\\text{(by the definition of }u_{i,k}\text{,}\\\text{since }k>i\text{)}}}\\
&  =\sum_{k=1}^{i-1}a_{k}+x+\underbrace{\sum_{k=i+1}^{j}a_{k-1}}%
_{\substack{=\sum_{k=i}^{j-1}a_{k}\\\text{(here, we have substituted }k\text{
for }k-1\text{ in the sum)}}}=\sum_{k=1}^{i-1}a_{k}+x+\sum_{k=i}^{j-1}a_{k}\\
&  =\underbrace{\sum_{k=1}^{i-1}a_{k}+\sum_{k=i}^{j-1}a_{k}}_{\substack{=\sum
_{k=1}^{j-1}a_{k}\\\text{(since }0\leq i-1\leq j-1\text{)}}}+x=\sum
_{k=1}^{j-1}a_{k}+x.
\end{align*}
Compared with (\ref{sol.det.a1a2anx.short.US.pf.3}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.short.US}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i>j$. Hence, $j<i$. The
definition of $s_{i,j}$ therefore shows that $s_{i,j}=0$. Hence,%
\begin{equation}
\sum_{k=1}^{j}a_{k}+\underbrace{s_{i,j}}_{=0}\left(  x-a_{j}\right)
=\sum_{k=1}^{j}a_{k}+\underbrace{0\left(  x-a_{j}\right)  }_{=0}=\sum
_{k=1}^{j}a_{k}. \label{sol.det.a1a2anx.short.US.pf.5}%
\end{equation}
\par
But (\ref{sol.det.a1a2anx.short.US.pf.1}) becomes%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}\underbrace{u_{i,k}%
}_{\substack{=a_{k}\\\text{(by the definition of }u_{i,k}\text{,}\\\text{since
}k\leq j<i\text{)}}}=\sum_{k=1}^{j}a_{k}.
\]
Compared with (\ref{sol.det.a1a2anx.short.US.pf.5}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.short.US}) is proven in Case 2.
\par
We have thus proven (\ref{sol.det.a1a2anx.short.US}) in each of the two Cases
1 and 2. Hence, (\ref{sol.det.a1a2anx.short.US}) always holds.}.

For any two objects $i$ and $j$, we define an element $\delta_{i,j}%
\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
define an element $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=\delta_{i,j}\left(  x-a_{j}\right)  +\delta_{i,n+1}\sum_{k=1}^{j}%
a_{k}.
\]
Let $C$ be the $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix defined
by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\]


Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.short.SC}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.SC}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.short.SC}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$1\leq i\leq n+1$ and $1\leq j\leq n+1$.
\par
The definition of $s_{i,n+1}$ yields $s_{i,n+1}=1$ (since $i\leq n+1$). Now,%
\begin{align*}
&  \sum_{k=1}^{n+1}s_{i,k}c_{k,j}\\
&  =\underbrace{\sum_{r=1}^{n+1}}_{=\sum_{r\in\left\{  1,2,\ldots,n+1\right\}
}}s_{i,r}\underbrace{c_{r,j}}_{\substack{=\delta_{r,j}\left(  x-a_{j}\right)
+\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\\\text{(by the definition of }%
c_{r,j}\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the
summation index }k\text{ as }r\right) \\
&  =\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}\left(  \delta
_{r,j}\left(  x-a_{j}\right)  +\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\right) \\
&  =\underbrace{\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}%
\delta_{r,j}\left(  x-a_{j}\right)  }_{\substack{=s_{i,j}\delta_{j,j}\left(
x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
j}}s_{i,r}\delta_{r,j}\left(  x-a_{j}\right)  \\\text{(here, we have split off
the addend for }r=j\text{ from the sum)}}}+\underbrace{\sum_{r\in\left\{
1,2,\ldots,n+1\right\}  }s_{i,r}\delta_{r,n+1}\sum_{k=1}^{j}a_{k}%
}_{\substack{=s_{i,n+1}\delta_{n+1,n+1}\sum_{k=1}^{j}a_{k}+\sum
_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}%
\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\\\text{(here, we have split off the addend
for }r=n+1\text{ from the sum)}}}\\
&  =s_{i,j}\underbrace{\delta_{j,j}}_{\substack{=1\\\text{(since }j=j\text{)}%
}}\left(  x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots
,n+1\right\}  ;\\r\neq j}}s_{i,r}\underbrace{\delta_{r,j}}%
_{\substack{=0\\\text{(since }r\neq j\text{)}}}\left(  x-a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{s_{i,n+1}}_{=1}\underbrace{\delta
_{n+1,n+1}}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{j}%
a_{k}+\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
n+1}}s_{i,r}\underbrace{\delta_{r,n+1}}_{\substack{=0\\\text{(since }r\neq
n+1\text{)}}}\sum_{k=1}^{j}a_{k}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq j}}s_{i,r}0\left(  x-a_{j}\right)  }%
_{=0}+\sum_{k=1}^{j}a_{k}+\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}0\sum_{k=1}^{j}a_{k}}_{=0}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\sum_{k=1}^{j}a_{k}=\sum_{k=1}^{j}%
a_{k}+s_{i,j}\left(  x-a_{j}\right)  .
\end{align*}
Thus, (\ref{sol.det.a1a2anx.short.SC}) is proven.}. Thus, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  =\sum_{k=1}^{n+1}u_{i,k}s_{k,j}
\label{sol.det.a1a2anx.short.SCvsUS}%
\end{equation}
(by (\ref{sol.det.a1a2anx.short.US})).

For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
c_{i,i}=x-a_{i} \label{sol.det.a1a2anx.short.cii}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.cii}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Thus, $i\neq n+1$, so that
$\delta_{i,n+1}=0$. Now, the definition of $c_{i,i}$ yields%
\[
c_{i,i}=\underbrace{\delta_{i,i}}_{\substack{=1\\\text{(since }i=i\text{)}%
}}\left(  x-a_{i}\right)  +\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{i}%
a_{k}=\left(  x-a_{i}\right)  +\underbrace{0\sum_{k=1}^{i}a_{k}}_{=0}%
=x-a_{i}.
\]
This proves (\ref{sol.det.a1a2anx.short.cii}).}. Also,%
\begin{equation}
c_{n+1,n+1}=x+\sum_{i=1}^{n}a_{i} \label{sol.det.a1a2anx.short.cn+1n+1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.short.cn+1n+1}):} The
definition of $c_{n+1,n+1}$ yields%
\begin{align*}
c_{n+1,n+1}  &  =\underbrace{\delta_{n+1,n+1}}_{\substack{=1\\\text{(since
}n+1=n+1\text{)}}}\left(  x-a_{n+1}\right)  +\underbrace{\delta_{n+1,n+1}%
}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{n+1}a_{k}=\left(
x-a_{n+1}\right)  +\sum_{k=1}^{n+1}a_{k}\\
&  =x+\underbrace{\left(  \sum_{k=1}^{n+1}a_{k}-a_{n+1}\right)  }_{=\sum
_{k=1}^{n}a_{k}}=x+\sum_{k=1}^{n}a_{k}=x+\sum_{i=1}^{n}a_{i}%
\end{align*}
(here, we have renamed the summation index $k$ as $i$). This proves
(\ref{sol.det.a1a2anx.short.cn+1n+1}).}.

But we have $c_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ be such that $i<j$. Thus, $1\leq i<j\leq n+1$,
so that $i<n+1$. Hence, $i\neq n+1$ and thus $\delta_{i,n+1}=0$. Also, $i<j$,
so that $i\neq j$ and thus $\delta_{i,j}=0$. Now, the definition of $c_{i,j}$
yields $c_{i,j}=\underbrace{\delta_{i,j}}_{=0}\left(  x-a_{j}\right)
+\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{j}a_{k}=0\left(  x-a_{j}\right)
+0\sum_{k=1}^{j}a_{k}=0$, qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to
$n+1$, $C$ and $c_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n+1,n+1}=\prod_{i=1}^{n+1}c_{i,i}%
\nonumber\\
&  =\left(  \prod_{i=1}^{n}\underbrace{c_{i,i}}_{\substack{=x-a_{i}\\\text{(by
(\ref{sol.det.a1a2anx.short.cii}))}}}\right)  \underbrace{c_{n+1,n+1}%
}_{\substack{=x+\sum_{i=1}^{n}a_{i}\\\text{(by
(\ref{sol.det.a1a2anx.short.cn+1n+1}))}}}\nonumber\\
&  =\left(  \prod_{i=1}^{n}\left(  x-a_{i}\right)  \right)  \left(
x+\sum_{i=1}^{n}a_{i}\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  . \label{sol.det.a1a2anx.short.detC}%
\end{align}


But recall that $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\begin{equation}
SC=\left(  \underbrace{\sum_{k=1}^{n+1}s_{i,k}c_{k,j}}_{\substack{=\sum
_{k=1}^{n+1}u_{i,k}s_{k,j}\\\text{(by (\ref{sol.det.a1a2anx.short.SCvsUS}))}%
}}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(  \sum_{k=1}%
^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\label{sol.det.a1a2anx.short.3}%
\end{equation}


On the other hand, $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\[
US=\left(  \sum_{k=1}^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq
j\leq n+1}=SC\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.a1a2anx.short.3})}\right)  .
\]


Thus,%
\begin{align*}
\det\left(  \underbrace{US}_{=SC}\right)   &  =\det\left(  SC\right)
=\underbrace{\det S}_{=1}\cdot\det C\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)}, applied to }n+1\text{, }S\text{ and }C\\
\text{instead of }n\text{, }A\text{ and }B
\end{array}
\right) \\
&  =\det C=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.a1a2anx.short.detC})}\right)  .
\end{align*}
Compared with%
\begin{align*}
\det\left(  US\right)   &  =\det U\cdot\underbrace{\det S}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }U\text{ and }S\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det U,
\end{align*}
this yields
\[
\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]
This solves Exercise \ref{exe.det.a1a2anx}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Second solution to Exercise \ref{exe.det.a1a2anx}.]For any $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define an element
$u_{i,j}\in\mathbb{K}$ by%
\[
u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
.
\]
This $u_{i,j}$ is well-defined\footnote{\textit{Proof.} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus, $i\in\left\{
1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots,n+1\right\}  $. We must
prove that $u_{i,j}$ is well-defined.
\par
We are in one of the following three cases:
\par
\textit{Case 1:} We have $j<i$.
\par
\textit{Case 2:} We have $j=i$.
\par
\textit{Case 3:} We have $j>i$.
\par
Let us first consider Case 1. In this case, we have $j<i$. Thus, $j<i\leq n+1$
(since $i\in\left\{  1,2,\ldots,n+1\right\}  $), so that $j\leq\left(
n+1\right)  -1$ (since $j$ and $n+1$ are integers). Thus, $j\leq\left(
n+1\right)  -1=n$. Combined with $j\geq1$ (since $j\in\left\{  1,2,\ldots
,n+1\right\}  $), this shows that $1\leq j\leq n$. In other words,
$j\in\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{j}$ is well-defined. Now,
the definition of $u_{i,j}$ says that $u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
=a_{j}$ (since $j<i$). Hence, $u_{i,j}$ is well-defined (since $a_{j}$ is
well-defined). Thus, in Case 1, we have proven that $u_{i,j}$ is well-defined.
\par
Let us next consider Case 2. In this case, we have $j=i$. Thus, the definition
of $u_{i,j}$ says that $u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
=x$ (since $j=i$). Hence, $u_{i,j}$ is well-defined (since $x$ is
well-defined). Thus, in Case 2, we have proven that $u_{i,j}$ is well-defined.
\par
Let us finally consider Case 3. In this case, we have $j>i$. Thus, $j>i\geq1$
(since $i\in\left\{  1,2,\ldots,n+1\right\}  $), so that $j-1>0$ and thus
$j-1\geq1$ (since $j-1$ is an integer). Also, $j\in\left\{  1,2,\ldots
,n+1\right\}  $, so that $j\leq n+1$ and thus $j-1\leq n$. Combined with
$j-1\geq1$, this yields $1\leq j-1\leq n$. In other words, $j-1\in\left\{
1,2,\ldots,n\right\}  $. Hence, $a_{j-1}$ is well-defined. Now, the definition
of $u_{i,j}$ says that $u_{i,j}=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
=a_{j-1}$ (since $j>i$). Hence, $u_{i,j}$ is well-defined (since $a_{j-1}$ is
well-defined). Thus, in Case 3, we have proven that $u_{i,j}$ is well-defined.
\par
Now, in each of our three Cases 1, 2 and 3, we have shown that $u_{i,j}$ is
well-defined. Since these three Cases cover all possibilities, we thus
conclude that $u_{i,j}$ is always well-defined. Qed.}. Now, we define an
$\left(  n+1\right)  \times\left(  n+1\right)  $-matrix $U$ by $U=\left(
u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Thus,%
\begin{align*}
U  &  =\left(  \underbrace{u_{i,j}}_{=%
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(
\begin{cases}
a_{j}, & \text{if }j<i;\\
x, & \text{if }j=i;\\
a_{j-1}, & \text{if }j>i
\end{cases}
\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}\\
&  =\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  .
\end{align*}
Our goal is now to prove that $\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  $.

For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, define
an element $s_{i,j}\in\mathbb{K}$ by%
\[
s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
.
\]
Now, we define an $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix $S$
by $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Then,
$\det S=1$\ \ \ \ \footnote{\textit{Proof.} We have $S=\left(  s_{i,j}\right)
_{1\leq i\leq n+1,\ 1\leq j\leq n+1}$. Therefore, the definition of the
transpose of a matrix yields $S^{T}=\left(  s_{j,i}\right)  _{1\leq i\leq
n+1,\ 1\leq j\leq n+1}$.
\par
Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$ be
such that $i<j$. Then, $j>i$ (since $i<j$). Now, the definition of $s_{j,i}$
yields $s_{j,i}=%
\begin{cases}
1, & \text{if }j\leq i;\\
0, & \text{if }j>i
\end{cases}
=0$ (since $j>i$).
\par
Let us now forget that we fixed $\left(  i,j\right)  $. We thus have shown
that $s_{j,i}=0$ for every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n+1\right\}  ^{2}$ satisfying $i<j$. Therefore, Exercise \ref{exe.ps4.3}
(applied to $n+1$, $S^{T}$ and $s_{j,i}$ instead of $n$, $A$ and $a_{i,j}$)
shows that
\[
\det\left(  S^{T}\right)  =s_{1,1}s_{2,2}\cdots s_{n+1,n+1}=\prod_{i=1}%
^{n+1}\underbrace{s_{i,i}}_{\substack{=%
\begin{cases}
1, & \text{if }i\leq i;\\
0, & \text{if }i>i
\end{cases}
\\\text{(by the definition of }s_{i,i}\text{)}}}=\prod_{i=1}^{n+1}\underbrace{%
\begin{cases}
1, & \text{if }i\leq i;\\
0, & \text{if }i>i
\end{cases}
}_{\substack{=1\\\text{(since }i\leq i\text{)}}}=\prod_{i=1}^{n+1}1=1.
\]
But Exercise \ref{exe.ps4.4} (applied to $n+1$ and $S$ instead of $n$ and $A$)
shows that $\det\left(  S^{T}\right)  =\det S$. Comparing this with
$\det\left(  S^{T}\right)  =1$, we obtain $\det S=1$, qed.}.

We extend the $n$-tuple $\left(  a_{1},a_{2},\ldots,a_{n}\right)
\in\mathbb{K}^{n}$ to an $\left(  n+1\right)  $-tuple $\left(  a_{1}%
,a_{2},\ldots,a_{n+1}\right)  \in\mathbb{K}^{n+1}$ by setting $a_{n+1}=0$.
Thus, an element $a_{k}\in\mathbb{K}$ is defined for every $k\in\left\{
1,2,\ldots,n+1\right\}  $.

Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.US}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.US}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.US}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots
,n+1\right\}  $. From $j\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq j\leq n+1$. From $i\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq i\leq n+1$. Now,%
\begin{align}
\sum_{k=1}^{n+1}u_{i,k}\underbrace{s_{k,j}}_{\substack{=%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
\\\text{(by the definition of }s_{k,j}\text{)}}}  &  =\sum_{k=1}^{n+1}u_{i,k}%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
\nonumber\\
&  =\sum_{k=1}^{j}u_{i,k}\underbrace{%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
}_{\substack{=1\\\text{(since }k\leq j\text{)}}}+\sum_{k=j+1}^{n+1}%
u_{i,k}\underbrace{%
\begin{cases}
1, & \text{if }k\leq j;\\
0, & \text{if }k>j
\end{cases}
}_{\substack{=0\\\text{(since }k>j\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq j\leq n+1\right) \nonumber\\
&  =\sum_{k=1}^{j}u_{i,k}1+\underbrace{\sum_{k=j+1}^{n+1}u_{i,k}0}_{=0}%
=\sum_{k=1}^{j}u_{i,k}1=\sum_{k=1}^{j}u_{i,k}. \label{sol.det.a1a2anx.US.pf.1}%
\end{align}
\par
Now, we must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq j$.
\par
\textit{Case 2:} We have $i>j$.
\par
Let us first consider Case 1. In this case, we have $i\leq j$. The definition
of $s_{i,j}$ shows that $s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
=1$ (since $i\leq j$). Therefore,%
\begin{equation}
\underbrace{\sum_{k=1}^{j}a_{k}}_{\substack{=\sum_{k=1}^{j-1}a_{k}%
+a_{j}\\\text{(here, we have split off the}\\\text{addend for }k=j\text{ from
the sum)}}}+\underbrace{s_{i,j}}_{=1}\left(  x-a_{j}\right)  =\sum_{k=1}%
^{j-1}a_{k}+a_{j}+\left(  x-a_{j}\right)  =\sum_{k=1}^{j-1}a_{k}+x.
\label{sol.det.a1a2anx.US.pf.3}%
\end{equation}
\par
We have $1\leq i\leq j$. Thus, $0\leq i-1\leq j-1$. Now,
(\ref{sol.det.a1a2anx.US.pf.1}) becomes%
\begin{align*}
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}  &  =\sum_{k=1}^{j}u_{i,k}=\underbrace{\sum
_{k=1}^{i}u_{i,k}}_{\substack{=\sum_{k=1}^{i-1}u_{i,k}+u_{i,i}\\\text{(here,
we have split off the}\\\text{addend for }k=i\text{ from the sum)}}%
}+\sum_{k=i+1}^{j}u_{i,k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }1\leq i\leq
j\right) \\
&  =\sum_{k=1}^{i-1}\underbrace{u_{i,k}}_{\substack{=%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
\\\text{(by the definition of }u_{i,k}\text{)}}}+\underbrace{u_{i,i}%
}_{\substack{=%
\begin{cases}
a_{i}, & \text{if }i<i;\\
x, & \text{if }i=i;\\
a_{i-1}, & \text{if }i>i
\end{cases}
\\\text{(by the definition of }u_{i,i}\text{)}}}+\sum_{k=i+1}^{j}%
\underbrace{u_{i,k}}_{\substack{=%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
\\\text{(by the definition of }u_{i,k}\text{)}}}\\
&  =\sum_{k=1}^{i-1}\underbrace{%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
}_{\substack{=a_{k}\\\text{(since }k<i\text{)}}}+\underbrace{%
\begin{cases}
a_{i}, & \text{if }i<i;\\
x, & \text{if }i=i;\\
a_{i-1}, & \text{if }i>i
\end{cases}
}_{\substack{=x\\\text{(since }i=i\text{)}}}+\sum_{k=i+1}^{j}\underbrace{%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
}_{\substack{=a_{k-1}\\\text{(since }k>i\text{)}}}\\
&  =\sum_{k=1}^{i-1}a_{k}+x+\underbrace{\sum_{k=i+1}^{j}a_{k-1}}%
_{\substack{=\sum_{k=i}^{j-1}a_{k}\\\text{(here, we have substituted }k\text{
for }k-1\text{ in the sum)}}}=\sum_{k=1}^{i-1}a_{k}+x+\sum_{k=i}^{j-1}a_{k}\\
&  =\underbrace{\sum_{k=1}^{i-1}a_{k}+\sum_{k=i}^{j-1}a_{k}}_{\substack{=\sum
_{k=1}^{j-1}a_{k}\\\text{(since }0\leq i-1\leq j-1\text{)}}}+x=\sum
_{k=1}^{j-1}a_{k}+x.
\end{align*}
Compared with (\ref{sol.det.a1a2anx.US.pf.3}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.US}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i>j$. Hence, $j<i$. The
definition of $s_{i,j}$ shows that $s_{i,j}=%
\begin{cases}
1, & \text{if }i\leq j;\\
0, & \text{if }i>j
\end{cases}
=0$ (since $i>j$). Therefore,%
\begin{equation}
\sum_{k=1}^{j}a_{k}+\underbrace{s_{i,j}}_{=0}\left(  x-a_{j}\right)
=\sum_{k=1}^{j}a_{k}+\underbrace{0\left(  x-a_{j}\right)  }_{=0}=\sum
_{k=1}^{j}a_{k}. \label{sol.det.a1a2anx.US.pf.5}%
\end{equation}
\par
But (\ref{sol.det.a1a2anx.US.pf.1}) becomes%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}\underbrace{u_{i,k}}_{\substack{=%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
\\\text{(by the definition of }u_{i,k}\text{)}}}=\sum_{k=1}^{j}\underbrace{%
\begin{cases}
a_{k}, & \text{if }k<i;\\
x, & \text{if }k=i;\\
a_{k-1}, & \text{if }k>i
\end{cases}
}_{\substack{=a_{k}\\\text{(since }k\leq j<i\text{)}}}=\sum_{k=1}^{j}a_{k}.
\]
Compared with (\ref{sol.det.a1a2anx.US.pf.5}), this yields%
\[
\sum_{k=1}^{n+1}u_{i,k}s_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  .
\]
Thus, (\ref{sol.det.a1a2anx.US}) is proven in Case 2.
\par
We have thus proven (\ref{sol.det.a1a2anx.US}) in each of the two Cases 1 and
2. Since these two Cases cover all possibilities, this shows that
(\ref{sol.det.a1a2anx.US}) always holds. Qed.}.

For any two objects $i$ and $j$, we define an element $\delta_{i,j}%
\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. For any $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
define an element $c_{i,j}\in\mathbb{K}$ by%
\[
c_{i,j}=\delta_{i,j}\left(  x-a_{j}\right)  +\delta_{i,n+1}\sum_{k=1}^{j}%
a_{k}.
\]
Let $C$ be the $\left(  n+1\right)  \times\left(  n+1\right)  $-matrix defined
by%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\]


Now, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$,
we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  \label{sol.det.a1a2anx.SC}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.SC}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. We need to prove the
equality (\ref{sol.det.a1a2anx.SC}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots
,n+1\right\}  $. From $j\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq j\leq n+1$. From $i\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain
$1\leq i\leq n+1$.
\par
The definition of $s_{i,n+1}$ yields $s_{i,n+1}=%
\begin{cases}
1, & \text{if }i\leq n+1;\\
0, & \text{if }i>n+1
\end{cases}
=1$ (since $i\leq n+1$). Now,%
\begin{align*}
&  \sum_{k=1}^{n+1}s_{i,k}c_{k,j}\\
&  =\underbrace{\sum_{r=1}^{n+1}}_{=\sum_{r\in\left\{  1,2,\ldots,n+1\right\}
}}s_{i,r}\underbrace{c_{r,j}}_{\substack{=\delta_{r,j}\left(  x-a_{j}\right)
+\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\\\text{(by the definition of }%
c_{r,j}\text{)}}}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the
summation index }k\text{ as }r\right) \\
&  =\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}\left(  \delta
_{r,j}\left(  x-a_{j}\right)  +\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\right) \\
&  =\underbrace{\sum_{r\in\left\{  1,2,\ldots,n+1\right\}  }s_{i,r}%
\delta_{r,j}\left(  x-a_{j}\right)  }_{\substack{=s_{i,j}\delta_{j,j}\left(
x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
j}}s_{i,r}\delta_{r,j}\left(  x-a_{j}\right)  \\\text{(here, we have split off
the addend for }r=j\text{ from the sum)}}}+\underbrace{\sum_{r\in\left\{
1,2,\ldots,n+1\right\}  }s_{i,r}\delta_{r,n+1}\sum_{k=1}^{j}a_{k}%
}_{\substack{=s_{i,n+1}\delta_{n+1,n+1}\sum_{k=1}^{j}a_{k}+\sum
_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}%
\delta_{r,n+1}\sum_{k=1}^{j}a_{k}\\\text{(here, we have split off the addend
for }r=n+1\text{ from the sum)}}}\\
&  =s_{i,j}\underbrace{\delta_{j,j}}_{\substack{=1\\\text{(since }j=j\text{)}%
}}\left(  x-a_{j}\right)  +\sum_{\substack{r\in\left\{  1,2,\ldots
,n+1\right\}  ;\\r\neq j}}s_{i,r}\underbrace{\delta_{r,j}}%
_{\substack{=0\\\text{(since }r\neq j\text{)}}}\left(  x-a_{j}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{s_{i,n+1}}_{=1}\underbrace{\delta
_{n+1,n+1}}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{j}%
a_{k}+\sum_{\substack{r\in\left\{  1,2,\ldots,n+1\right\}  ;\\r\neq
n+1}}s_{i,r}\underbrace{\delta_{r,n+1}}_{\substack{=0\\\text{(since }r\neq
n+1\text{)}}}\sum_{k=1}^{j}a_{k}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq j}}s_{i,r}0\left(  x-a_{j}\right)  }%
_{=0}+\sum_{k=1}^{j}a_{k}+\underbrace{\sum_{\substack{r\in\left\{
1,2,\ldots,n+1\right\}  ;\\r\neq n+1}}s_{i,r}0\sum_{k=1}^{j}a_{k}}_{=0}\\
&  =s_{i,j}\left(  x-a_{j}\right)  +\sum_{k=1}^{j}a_{k}=\sum_{k=1}^{j}%
a_{k}+s_{i,j}\left(  x-a_{j}\right)  .
\end{align*}
Thus, (\ref{sol.det.a1a2anx.SC}) is proven.}. Thus, for every $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, we have%
\begin{equation}
\sum_{k=1}^{n+1}s_{i,k}c_{k,j}=\sum_{k=1}^{j}a_{k}+s_{i,j}\left(
x-a_{j}\right)  =\sum_{k=1}^{n+1}u_{i,k}s_{k,j} \label{sol.det.a1a2anx.SCvsUS}%
\end{equation}
(by (\ref{sol.det.a1a2anx.US})).

For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
c_{i,i}=x-a_{i} \label{sol.det.a1a2anx.cii}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.cii}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $. Thus, $1\leq i\leq n$, so that $i\leq n<n+1$, and
therefore $i\neq n+1$. Hence, $\delta_{i,n+1}=0$. Now, the definition of
$c_{i,i}$ yields%
\[
c_{i,i}=\underbrace{\delta_{i,i}}_{\substack{=1\\\text{(since }i=i\text{)}%
}}\left(  x-a_{i}\right)  +\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{i}%
a_{k}=\left(  x-a_{i}\right)  +\underbrace{0\sum_{k=1}^{i}a_{k}}_{=0}%
=x-a_{i}.
\]
This proves (\ref{sol.det.a1a2anx.cii}).}. Also,%
\begin{equation}
c_{n+1,n+1}=x+\sum_{i=1}^{n}a_{i} \label{sol.det.a1a2anx.cn+1n+1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.a1a2anx.cn+1n+1}):} The definition of
$c_{n+1,n+1}$ yields%
\begin{align*}
c_{n+1,n+1}  &  =\underbrace{\delta_{n+1,n+1}}_{\substack{=1\\\text{(since
}n+1=n+1\text{)}}}\left(  x-a_{n+1}\right)  +\underbrace{\delta_{n+1,n+1}%
}_{\substack{=1\\\text{(since }n+1=n+1\text{)}}}\sum_{k=1}^{n+1}a_{k}=\left(
x-a_{n+1}\right)  +\underbrace{\sum_{k=1}^{n+1}a_{k}}_{\substack{=\sum
_{k=1}^{n}a_{k}+a_{n+1}\\\text{(here, we have split off the}\\\text{addend for
}k=n+1\text{ from the sum)}}}\\
&  =\left(  x-a_{n+1}\right)  +\sum_{k=1}^{n}a_{k}+a_{n+1}=x+\sum_{k=1}%
^{n}a_{k}=x+\sum_{i=1}^{n}a_{i}%
\end{align*}
(here, we have renamed the summation index $k$ as $i$). This proves
(\ref{sol.det.a1a2anx.cn+1n+1}).}.

But we have $c_{i,j}=0$ for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+1\right\}  ^{2}$ be such that $i<j$. We have $\left(  i,j\right)
\in\left\{  1,2,\ldots,n+1\right\}  ^{2}$, so that $i\in\left\{
1,2,\ldots,n+1\right\}  $ and $j\in\left\{  1,2,\ldots,n+1\right\}  $. From
$j\in\left\{  1,2,\ldots,n+1\right\}  $, we obtain $1\leq j\leq n+1$. Thus,
$i<j\leq n+1$, so that $i\neq n+1$ and thus $\delta_{i,n+1}=0$. Also, $i<j$,
so that $i\neq j$ and thus $\delta_{i,j}=0$. Now, the definition of $c_{i,j}$
yields $c_{i,j}=\underbrace{\delta_{i,j}}_{=0}\left(  x-a_{j}\right)
+\underbrace{\delta_{i,n+1}}_{=0}\sum_{k=1}^{j}a_{k}=0\left(  x-a_{j}\right)
+0\sum_{k=1}^{j}a_{k}=0$, qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to
$n+1$, $C$ and $c_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) shows that%
\begin{align}
\det C  &  =c_{1,1}c_{2,2}\cdots c_{n+1,n+1}=\prod_{i=1}^{n+1}c_{i,i}%
\nonumber\\
&  =\left(  \prod_{i=1}^{n}\underbrace{c_{i,i}}_{\substack{=x-a_{i}\\\text{(by
(\ref{sol.det.a1a2anx.cii}))}}}\right)  \underbrace{c_{n+1,n+1}}%
_{\substack{=x+\sum_{i=1}^{n}a_{i}\\\text{(by (\ref{sol.det.a1a2anx.cn+1n+1}%
))}}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the factor}\\
\text{for }i=n+1\text{ from the product}%
\end{array}
\right) \nonumber\\
&  =\left(  \prod_{i=1}^{n}\left(  x-a_{i}\right)  \right)  \left(
x+\sum_{i=1}^{n}a_{i}\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)
\prod_{i=1}^{n}\left(  x-a_{i}\right)  . \label{sol.det.a1a2anx.detC}%
\end{align}


But recall that $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $C=\left(  c_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\begin{equation}
SC=\left(  \underbrace{\sum_{k=1}^{n+1}s_{i,k}c_{k,j}}_{\substack{=\sum
_{k=1}^{n+1}u_{i,k}s_{k,j}\\\text{(by (\ref{sol.det.a1a2anx.SCvsUS}))}%
}}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}=\left(  \sum_{k=1}%
^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}.
\label{sol.det.a1a2anx.3}%
\end{equation}


On the other hand, $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq
n+1}$ and $S=\left(  s_{i,j}\right)  _{1\leq i\leq n+1,\ 1\leq j\leq n+1}$.
Hence, the definition of the product of two matrices shows that
\[
US=\left(  \sum_{k=1}^{n+1}u_{i,k}s_{k,j}\right)  _{1\leq i\leq n+1,\ 1\leq
j\leq n+1}=SC\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.det.a1a2anx.3}%
)}\right)  .
\]


Thus,%
\begin{align*}
\det\left(  \underbrace{US}_{=SC}\right)   &  =\det\left(  SC\right)
=\underbrace{\det S}_{=1}\cdot\det C\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }S\text{ and }C\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det C=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.a1a2anx.detC})}\right)  .
\end{align*}
Compared with%
\begin{align*}
\det\left(  US\right)   &  =\det U\cdot\underbrace{\det S}_{=1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.det(AB)}, applied to
}n+1\text{, }U\text{ and }S\text{ instead of }n\text{, }A\text{ and }B\right)
\\
&  =\det U,
\end{align*}
this yields
\[
\det U=\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]
Since $U=\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  $, this rewrites as%
\[
\det\left(
\begin{array}
[c]{cccccc}%
x & a_{1} & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & x & a_{2} & \cdots & a_{n-1} & a_{n}\\
a_{1} & a_{2} & x & \cdots & a_{n-1} & a_{n}\\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
a_{1} & a_{2} & a_{3} & \cdots & x & a_{n}\\
a_{1} & a_{2} & a_{3} & \cdots & a_{n} & x
\end{array}
\right)  =\left(  x+\sum_{i=1}^{n}a_{i}\right)  \prod_{i=1}^{n}\left(
x-a_{i}\right)  .
\]
This solves Exercise \ref{exe.det.a1a2anx}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.det.2diags}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.det.2diags}.]Let $z$ be the permutation
$\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$ (where we are using the
notations of Definition \ref{def.perm.cycles}). Then, every $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
z\left(  i\right)  =%
\begin{cases}
i-1, & \text{if }i>1;\\
n, & \text{if }i=1
\end{cases}
. \label{sol.det.2diags.short.z}%
\end{equation}
(This follows easily from the definition of $z$.) In particular, $z\left(
1\right)  =n\neq1$ (since $n>1$), and thus $z\neq\operatorname*{id}$. Every
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align}
z\left(  i\right)   &  =%
\begin{cases}
i-1, & \text{if }i>1;\\
n, & \text{if }i=1
\end{cases}
\nonumber\\
&  \equiv%
\begin{cases}
i-1, & \text{if }i>1;\\
i-1, & \text{if }i=1
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }n\equiv0=\underbrace{1}_{=i}-1=i-1\operatorname{mod}n\\
\text{in the case when }i=1
\end{array}
\right) \nonumber\\
&  =i-1\operatorname{mod}n. \label{sol.det.2diags.short.zmod}%
\end{align}
Notice also that $z=\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$, so that
$\left(  -1\right)  ^{z}=\left(  -1\right)  ^{\operatorname*{cyc}%
\nolimits_{n,n-1,\ldots,1}}=\left(  -1\right)  ^{n-1}$ (by Exercise
\ref{exe.perm.cycles} \textbf{(d)}, applied to $k=n$ and $\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  =\left(  n,n-1,\ldots,1\right)  $).

We recall the following simple fact: If $p$ and $q$ are two elements of
$\left\{  1,2,\ldots,n\right\}  $ such that $p\equiv q\operatorname{mod}n$,
then $p=q$. We shall use this fact several times (tacitly) in the following arguments.

Now, I claim that if $\sigma\in S_{n}$ satisfies $\sigma\notin\left\{
\operatorname*{id},z\right\}  $, then%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists an }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfying}\\
i\neq\sigma\left(  i\right)  \text{ and }i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n
\end{array}
\right)  . \label{sol.det.2diags.short.exi}%
\end{equation}


\textit{Proof of (\ref{sol.det.2diags.short.exi}):} Let $\sigma\in S_{n}$ be
such that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $. Thus,
$\sigma\neq\operatorname*{id}$ and $\sigma\neq z$.

We need to prove (\ref{sol.det.2diags.short.exi}). Indeed, let us assume the
contrary (for the sake of contradiction). Thus,%
\begin{equation}
\text{every }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfies either
}i=\sigma\left(  i\right)  \text{ or }i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n. \label{sol.det.2diags.short.exi.pf.ass}%
\end{equation}
\footnote{I use the words \textquotedblleft either\textquotedblleft%
/\textquotedblleft or\textquotedblright\ in a non-exclusive meaning (i.e.,
when I say \textquotedblleft either $\mathcal{A}$ or $\mathcal{B}%
$\textquotedblright, I mean to include also the case when both $\mathcal{A}$
and $\mathcal{B}$ hold simultaneously), but here it does not matter (because
we cannot have $i=\sigma\left(  i\right)  $ and $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$ at the same time).}

There exists a $J\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(
J\right)  \neq J$ (since $\sigma\neq\operatorname*{id}$). Let $j$ be the
smallest such $J$. Thus, $\sigma\left(  j\right)  \neq j$, but every $J<j$
satisfies $\sigma\left(  J\right)  =J$.

Applying (\ref{sol.det.2diags.short.exi.pf.ass}) to $i=j$, we see that either
$j=\sigma\left(  j\right)  $ or $j\equiv\sigma\left(  j\right)
+1\operatorname{mod}n$. Since $j=\sigma\left(  j\right)  $ cannot hold
(because we have $\sigma\left(  j\right)  \neq j$), we thus have
$j\equiv\sigma\left(  j\right)  +1\operatorname{mod}n$. In other words,
$\sigma\left(  j\right)  \equiv j-1\operatorname{mod}n$.

We have $j=1$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$j\neq1$, so that $j>1$. Hence, $j-1\in\left\{  1,2,\ldots,n\right\}  $. Also,
$j-1<j$. Hence, $\sigma\left(  j-1\right)  =j-1$ (since every $J<j$ satisfies
$\sigma\left(  J\right)  =J$). Thus, $\sigma\left(  j-1\right)  =j-1\equiv
\sigma\left(  j\right)  \operatorname{mod}n$. Since both $\sigma\left(
j-1\right)  $ and $\sigma\left(  j\right)  $ are elements of $\left\{
1,2,\ldots,n\right\}  $, this shows that $\sigma\left(  j-1\right)
=\sigma\left(  j\right)  $, and thus $j-1=j$ (since $\sigma$ is injective).
But this is absurd. This contradiction shows that our assumption was wrong,
qed.}. Thus, $\sigma\left(  \underbrace{1}_{=j}\right)  =\sigma\left(
j\right)  \equiv\underbrace{j}_{=1}-1=0\equiv n\operatorname{mod}n$. Since
both $\sigma\left(  1\right)  $ and $n$ belong to $\left\{  1,2,\ldots
,n\right\}  $, this shows that $\sigma\left(  1\right)  =n$.

There exists a $K\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(
K\right)  =K$\ \ \ \ \footnote{\textit{Proof.} We have $\sigma\neq z$. Hence,
there exists an $i\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(
i\right)  \neq z\left(  i\right)  $. Consider this $i$.
\par
If we had $i\equiv\sigma\left(  i\right)  +1\operatorname{mod}n$, then we
would have $\sigma\left(  i\right)  \equiv i-1\equiv z\left(  i\right)
\operatorname{mod}n$ (by (\ref{sol.det.2diags.short.zmod})), which would
entail $\sigma\left(  i\right)  =z\left(  i\right)  $ (since both
$\sigma\left(  i\right)  $ and $z\left(  i\right)  $ belong to $\left\{
1,2,\ldots,n\right\}  $); but this would contradict $\sigma\left(  i\right)
\neq z\left(  i\right)  $. Hence, we cannot have $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$.
\par
We have either $i=\sigma\left(  i\right)  $ or $i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n$ (because of (\ref{sol.det.2diags.short.exi.pf.ass})).
Thus, $i=\sigma\left(  i\right)  $ (since we cannot have $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$). Hence, there exists a $K\in\left\{
1,2,\ldots,n\right\}  $ such that $\sigma\left(  K\right)  =K$ (namely,
$K=i$). Qed.}. Let $k$ be the largest such $K$. Thus, $\sigma\left(  k\right)
=k$, but every $K>k$ satisfies $\sigma\left(  K\right)  \neq K$.

We have $n\neq1$ and thus $\sigma\left(  n\right)  \neq\sigma\left(  1\right)
$ (since $\sigma$ is injective). Thus, $\sigma\left(  n\right)  \neq
\sigma\left(  1\right)  =n$.

We cannot have $k=n$ (because otherwise, we would have $\sigma\left(
\underbrace{n}_{=k}\right)  =\sigma\left(  k\right)  =k=n$, which would
contradict $\sigma\left(  n\right)  \neq n$). Thus, $k<n$. Hence,
$k+1\in\left\{  1,2,\ldots,n\right\}  $. Therefore, $\sigma\left(  k+1\right)
\neq k+1$ (since every $K>k$ satisfies $\sigma\left(  K\right)  \neq K$, and
since $k+1>k$). Now, applying (\ref{sol.det.2diags.short.exi.pf.ass}) to
$i=k+1$, we conclude that either $k+1=\sigma\left(  k+1\right)  $ or
$k+1\equiv\sigma\left(  k+1\right)  +1\operatorname{mod}n$. Since we cannot
have $k+1=\sigma\left(  k+1\right)  $ (because $\sigma\left(  k+1\right)  \neq
k+1$), we thus must have $k+1\equiv\sigma\left(  k+1\right)
+1\operatorname{mod}n$. In other words, $k\equiv\sigma\left(  k+1\right)
\operatorname{mod}n$. Hence, $k=\sigma\left(  k+1\right)  $ (since both $k$
and $\sigma\left(  k+1\right)  $ belong to $\left\{  1,2,\ldots,n\right\}  $),
so that $\sigma\left(  k+1\right)  =k=\sigma\left(  k\right)  $. Since
$\sigma$ is injective, this yields $k+1=k$, which is absurd. This
contradiction shows that our assumption was wrong. Hence,
(\ref{sol.det.2diags.short.exi}) is proven.

Let us now write our matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. Then,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A=\left(
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
In other words, we have%
\begin{equation}
a_{i,j}=%
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\label{sol.det.2diags.short.aij}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

For every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{align}
a_{i,i}  &  =%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.det.2diags.short.aij}), applied
to }\left(  i,i\right)  \text{ instead of }\left(  i,j\right)  \right)
\nonumber\\
&  =a_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i=i\right)
\label{sol.det.2diags.short.aii}%
\end{align}
and%
\begin{align}
a_{i,z\left(  i\right)  }  &  =%
\begin{cases}
a_{z\left(  i\right)  }, & \text{if }i=z\left(  i\right)  ;\\
b_{z\left(  i\right)  }, & \text{if }i\equiv z\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.det.2diags.short.aij}),
applied to }\left(  i,z\left(  i\right)  \right)  \text{ instead of }\left(
i,j\right)  \right) \nonumber\\
&  =a_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\equiv z\left(  i\right)
+1\operatorname{mod}n\text{ (by (\ref{sol.det.2diags.short.zmod}))}\right)  .
\label{sol.det.2diags.short.aizi}%
\end{align}


It is now easy to see that if $\sigma\in S_{n}$ satisfies $\sigma
\notin\left\{  \operatorname*{id},z\right\}  $, then
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.det.2diags.short.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.short.0}):} Let $\sigma\in
S_{n}$ be such that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $.
Thus, there exists an $i\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$i\neq\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n$ (because of (\ref{sol.det.2diags.short.exi})). Consider
this $i$.
\par
From (\ref{sol.det.2diags.short.aij}) (applied to $\left(  i,\sigma\left(
i\right)  \right)  $ instead of $\left(  i,j\right)  $), we obtain%
\[
a_{i,\sigma\left(  i\right)  }=%
\begin{cases}
a_{\sigma\left(  i\right)  }, & \text{if }i=\sigma\left(  i\right)  ;\\
b_{\sigma\left(  i\right)  }, & \text{if }i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
=0
\]
(since $i\neq\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(
i\right)  +1\operatorname{mod}n$).
\par
Now, let us forget that we fixed $i$. We thus have shown that there exists an
$i\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i,\sigma\left(  i\right)
}=0$. In other words, at least one factor of the product $\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$. Therefore, the whole product
$\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ equals $0$. This proves
(\ref{sol.det.2diags.short.0}).}.

Now, (\ref{eq.det.eq.2}) becomes%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}\prod_{i=1}%
^{n}\underbrace{a_{i,\operatorname*{id}\left(  i\right)  }}_{=a_{i,i}%
}+\underbrace{\left(  -1\right)  ^{z}}_{=\left(  -1\right)  ^{n-1}}\prod
_{i=1}^{n}a_{i,z\left(  i\right)  }+\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=0\\\text{(by (\ref{sol.det.2diags.short.0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }\sigma=\operatorname*{id}\text{
and}\\
\text{for }\sigma=z\text{ from the sum (since }z\neq\operatorname*{id}\text{)}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n}a_{i,i}+\left(  -1\right)  ^{n-1}\prod_{i=1}^{n}%
a_{i,z\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)
^{\sigma}0}_{=0}\\
&  =\prod_{i=1}^{n}\underbrace{a_{i,i}}_{\substack{=a_{i}\\\text{(by
(\ref{sol.det.2diags.short.aii}))}}}+\left(  -1\right)  ^{n-1}\prod_{i=1}%
^{n}\underbrace{a_{i,z\left(  i\right)  }}_{\substack{=b_{z\left(  i\right)
}\\\text{(by (\ref{sol.det.2diags.short.aizi}))}}}\\
&  =\prod_{i=1}^{n}a_{i}+\left(  -1\right)  ^{n-1}\underbrace{\prod_{i=1}%
^{n}b_{z\left(  i\right)  }}_{\substack{=\prod_{i=1}^{n}b_{i}\\\text{(here, we
have substituted }i\\\text{for }z\left(  i\right)  \text{ in the
product,}\\\text{since }z:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  \\\text{is a bijection)}}}=\underbrace{\prod_{i=1}%
^{n}a_{i}}_{=a_{1}a_{2}\cdots a_{n}}+\left(  -1\right)  ^{n-1}%
\underbrace{\prod_{i=1}^{n}b_{i}}_{=b_{1}b_{2}\cdots b_{n}}\\
&  =a_{1}a_{2}\cdots a_{n}+\left(  -1\right)  ^{n-1}b_{1}b_{2}\cdots b_{n}.
\end{align*}
This solves Exercise \ref{exe.det.2diags}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.det.2diags}.]We first make some preliminary definitions.

Let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. The
set $S_{n}$ is the set of all permutations of $\left\{  1,2,\ldots,n\right\}
$. In other words, the set $S_{n}$ is the set of all permutations of $\left[
n\right]  $ (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).

The integers $1,2,\ldots,n$ take all possible residues modulo $n$, and each of
them exactly once. In other words: For every $h\in\mathbb{Z}$, there exists a
unique element $g\in\left\{  1,2,\ldots,n\right\}  $ satisfying $g\equiv
h\operatorname{mod}n$. Since $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $, this rewrites as follows: For every $h\in\mathbb{Z}$, there
exists a unique element $g\in\left[  n\right]  $ satisfying $g\equiv
h\operatorname{mod}n$. We shall denote this $g$ by $\operatorname*{posrem}h$.
Thus, we have the following facts:

\begin{itemize}
\item For every $h\in\mathbb{Z}$, we have%
\begin{equation}
\operatorname*{posrem}h\in\left[  n\right]  \ \ \ \ \ \ \ \ \ \ \text{and}%
\ \ \ \ \ \ \ \ \ \ \operatorname*{posrem}h\equiv h\operatorname{mod}n
\label{sol.det.2diags.posrem.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.posrem.1}):} Let
$h\in\mathbb{Z}$. We know that $\operatorname*{posrem}h$ is the unique element
$g\in\left[  n\right]  $ satisfying $g\equiv h\operatorname{mod}n$ (because
this is how we have defined $\operatorname*{posrem}h$). Thus,
$\operatorname*{posrem}h$ is an element $g\in\left[  n\right]  $ satisfying
$g\equiv h\operatorname{mod}n$. In other words, $\operatorname*{posrem}h$ is
an element of $\left[  n\right]  $ and satisfies $\operatorname*{posrem}%
h\equiv h\operatorname{mod}n$. In other words, $\operatorname*{posrem}%
h\in\left[  n\right]  $ and $\operatorname*{posrem}h\equiv h\operatorname{mod}%
n$. This proves (\ref{sol.det.2diags.posrem.1}).}.

\item If $h\in\mathbb{Z}$ and $k\in\left[  n\right]  $ are such that $k\equiv
h\operatorname{mod}n$, then%
\begin{equation}
k=\operatorname*{posrem}h \label{sol.det.2diags.posrem.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.posrem.2}):} Let
$h\in\mathbb{Z}$ and $k\in\left[  n\right]  $ be such that $k\equiv
h\operatorname{mod}n$.
\par
We know that $\operatorname*{posrem}h$ is the unique element $g\in\left[
n\right]  $ satisfying $g\equiv h\operatorname{mod}n$ (because this is how we
have defined $\operatorname*{posrem}h$). Thus, in particular,
$\operatorname*{posrem}h$ is the only such element. In other words, if
$g\in\left[  n\right]  $ is any element satisfying $g\equiv
h\operatorname{mod}n$, then $g=\operatorname*{posrem}h$. Applying this to
$g=k$, we obtain $k=\operatorname*{posrem}h$ (since $k\in\left[  n\right]  $
is an element satisfying $k\equiv h\operatorname{mod}n$). This proves
(\ref{sol.det.2diags.posrem.2}).}.

\item If $a\in\left[  n\right]  $ and $b\in\left[  n\right]  $ are such that
$a\equiv b\operatorname{mod}n$, then%
\begin{equation}
a=b \label{sol.det.2diags.posrem.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.posrem.3}):} Let $a\in\left[
n\right]  $ and $b\in\left[  n\right]  $ be such that $a\equiv
b\operatorname{mod}n$. Applying (\ref{sol.det.2diags.posrem.2}) to $h=b$ and
$k=a$, we obtain $a=\operatorname*{posrem}b$ (since $a\equiv
b\operatorname{mod}n$). Applying (\ref{sol.det.2diags.posrem.2}) to $h=b$ and
$k=b$, we obtain $b=\operatorname*{posrem}b$ (since $b\equiv
b\operatorname{mod}n$). Thus, $a=\operatorname*{posrem}b=b$. This proves
(\ref{sol.det.2diags.posrem.3}).}.
\end{itemize}

Now, fix $a\in\mathbb{Z}$. Every $g\in\left[  n\right]  $ satisfies
$\operatorname*{posrem}\left(  g+a\right)  \in\left[  n\right]  $%
\ \ \ \ \footnote{\textit{Proof.} Let $g\in\left[  n\right]  $. Then,
(\ref{sol.det.2diags.posrem.1}) (applied to $h=g+a$) shows that
$\operatorname*{posrem}\left(  g+a\right)  \in\left[  n\right]  $ and
$\operatorname*{posrem}\left(  g+a\right)  \equiv g+a\operatorname{mod}n$. In
particular, $\operatorname*{posrem}\left(  g+a\right)  \in\left[  n\right]  $,
qed.}. Hence, we can define a map $\operatorname*{shift}\nolimits_{a}:\left[
n\right]  \rightarrow\left[  n\right]  $ by setting%
\[
\left(  \operatorname*{shift}\nolimits_{a}\left(  g\right)
=\operatorname*{posrem}\left(  g+a\right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}g\in\left[  n\right]  \right)  .
\]
Consider this map $\operatorname*{shift}\nolimits_{a}$.

Let us now forget that we fixed $a\in\mathbb{Z}$. We thus have defined a map
$\operatorname*{shift}\nolimits_{a}:\left[  n\right]  \rightarrow\left[
n\right]  $ for every $a\in\mathbb{Z}$. (For example, if $n=5$ and $a=2$, then
the map $\operatorname*{shift}\nolimits_{a}$ sends $1,2,3,4,5$ to $3,4,5,1,2$, respectively.)

We observe the following facts:

\begin{itemize}
\item We have%
\begin{equation}
\operatorname*{shift}\nolimits_{0}=\operatorname*{id}
\label{sol.det.2diags.shift.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.0}):} Let $g\in\left[
n\right]  $. Then, $g\equiv g\operatorname{mod}n$. Hence,
(\ref{sol.det.2diags.posrem.2}) (applied to $h=g$ and $k=g$) shows that
$g=\operatorname*{posrem}g$. Now, the definition of $\operatorname*{shift}%
\nolimits_{0}$ shows that $\operatorname*{shift}\nolimits_{0}\left(  g\right)
=\operatorname*{posrem}\left(  \underbrace{g+0}_{=g}\right)
=\operatorname*{posrem}g$. Compared with $\operatorname*{id}\left(  g\right)
=g=\operatorname*{posrem}g$, this yields $\operatorname*{shift}\nolimits_{0}%
\left(  g\right)  =\operatorname*{id}\left(  g\right)  $.
\par
Now, let us forget that we fixed $g$. We thus have proven that
$\operatorname*{shift}\nolimits_{0}\left(  g\right)  =\operatorname*{id}%
\left(  g\right)  $ for every $g\in\left[  n\right]  $. In other words,
$\operatorname*{shift}\nolimits_{0}=\operatorname*{id}$. This proves
(\ref{sol.det.2diags.shift.0}).}.

\item We have%
\begin{equation}
\operatorname*{shift}\nolimits_{a+b}=\operatorname*{shift}\nolimits_{a}%
\circ\operatorname*{shift}\nolimits_{b}\ \ \ \ \ \ \ \ \ \ \text{for any }%
a\in\mathbb{Z}\text{ and }b\in\mathbb{Z} \label{sol.det.2diags.shift.a+b}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.a+b}):} Let
$a\in\mathbb{Z}$ and $b\in\mathbb{Z}$.
\par
Let $g\in\left[  n\right]  $. Then, $\operatorname*{shift}\nolimits_{a+b}%
\left(  g\right)  =\operatorname*{posrem}\left(  g+a+b\right)  $ (by the
definition of $\operatorname*{shift}\nolimits_{a+b}$).
\par
On the other hand, $\operatorname*{shift}\nolimits_{b}\left(  g\right)
=\operatorname*{posrem}\left(  g+b\right)  $ (by the definition of
$\operatorname*{shift}\nolimits_{b}$). Applying (\ref{sol.det.2diags.posrem.1}%
) to $h=g+b$, we obtain $\operatorname*{posrem}\left(  g+b\right)  \in\left[
n\right]  $ and $\operatorname*{posrem}\left(  g+b\right)  \equiv
g+b\operatorname{mod}n$.
\par
Moreover, $\left(  \operatorname*{shift}\nolimits_{a}\circ
\operatorname*{shift}\nolimits_{b}\right)  \left(  g\right)
=\operatorname*{shift}\nolimits_{a}\left(  \operatorname*{shift}%
\nolimits_{b}\left(  g\right)  \right)  =\operatorname*{posrem}\left(
\operatorname*{shift}\nolimits_{b}\left(  g\right)  +a\right)  $ (by the
definition of $\operatorname*{shift}\nolimits_{a}$). Applying
(\ref{sol.det.2diags.posrem.1}) to $h=\operatorname*{shift}\nolimits_{b}%
\left(  g\right)  +a$, we obtain $\operatorname*{posrem}\left(
\operatorname*{shift}\nolimits_{b}\left(  g\right)  +a\right)  \in\left[
n\right]  $ and $\operatorname*{posrem}\left(  \operatorname*{shift}%
\nolimits_{b}\left(  g\right)  +a\right)  \equiv\operatorname*{shift}%
\nolimits_{b}\left(  g\right)  +a\operatorname{mod}n$.
\par
Now,%
\begin{align*}
&  \left(  \operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}%
\nolimits_{b}\right)  \left(  g\right) \\
&  =\operatorname*{posrem}\left(  \operatorname*{shift}\nolimits_{b}\left(
g\right)  +a\right)  \equiv\underbrace{\operatorname*{shift}\nolimits_{b}%
\left(  g\right)  }_{=\operatorname*{posrem}\left(  g+b\right)  \equiv
g+b\operatorname{mod}n}+a\\
&  \equiv g+b+a=g+a+b\operatorname{mod}n.
\end{align*}
Thus, we can apply (\ref{sol.det.2diags.posrem.3}) to $h=g+a+b$ and $k=\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  $ (since $\left(  \operatorname*{shift}%
\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}\right)  \left(  g\right)
=\operatorname*{posrem}\left(  \operatorname*{shift}\nolimits_{b}\left(
g\right)  +a\right)  \in\left[  n\right]  $). As a result, we obtain $\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  =\operatorname*{posrem}\left(  g+a+b\right)  $.
Comparing this with $\operatorname*{shift}\nolimits_{a+b}\left(  g\right)
=\operatorname*{posrem}\left(  g+a+b\right)  $, we obtain $\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  =\operatorname*{shift}\nolimits_{a+b}\left(
g\right)  $.
\par
Now, let us forget that we fixed $g$. We thus have shown that $\left(
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
\right)  \left(  g\right)  =\operatorname*{shift}\nolimits_{a+b}\left(
g\right)  $ for every $g\in\left[  n\right]  $. In other words,
$\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{b}%
=\operatorname*{shift}\nolimits_{a+b}$. This proves
(\ref{sol.det.2diags.shift.a+b}).}.

\item We have%
\begin{equation}
\operatorname*{shift}\nolimits_{a}\in S_{n}\ \ \ \ \ \ \ \ \ \ \text{for every
}a\in\mathbb{Z} \label{sol.det.2diags.shift.Sn}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.Sn}):} Let
$a\in\mathbb{Z}$. From (\ref{sol.det.2diags.shift.a+b}) (applied to $b=-a$),
we obtain $\operatorname*{shift}\nolimits_{a+\left(  -a\right)  }%
=\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{-a}$.
Hence,%
\begin{align*}
\operatorname*{shift}\nolimits_{a}\circ\operatorname*{shift}\nolimits_{-a}  &
=\operatorname*{shift}\nolimits_{a+\left(  -a\right)  }=\operatorname*{shift}%
\nolimits_{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }a+\left(  -a\right)
=0\right) \\
&  =\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.2diags.shift.0})}\right)  .
\end{align*}
\par
From (\ref{sol.det.2diags.shift.a+b}) (applied to $-a$ and $a$ instead of $a$
and $b$), we obtain $\operatorname*{shift}\nolimits_{\left(  -a\right)
+a}=\operatorname*{shift}\nolimits_{-a}\circ\operatorname*{shift}%
\nolimits_{a}$. Hence,%
\begin{align*}
\operatorname*{shift}\nolimits_{-a}\circ\operatorname*{shift}\nolimits_{a}  &
=\operatorname*{shift}\nolimits_{\left(  -a\right)  +a}=\operatorname*{shift}%
\nolimits_{0}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  -a\right)
+a=0\right) \\
&  =\operatorname*{id}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.det.2diags.shift.0})}\right)  .
\end{align*}
\par
The maps $\operatorname*{shift}\nolimits_{a}$ and $\operatorname*{shift}%
\nolimits_{-a}$ are mutually inverse (since $\operatorname*{shift}%
\nolimits_{a}\circ\operatorname*{shift}\nolimits_{-a}=\operatorname*{id}$ and
$\operatorname*{shift}\nolimits_{-a}\circ\operatorname*{shift}\nolimits_{a}%
=\operatorname*{id}$). Thus, the map $\operatorname*{shift}\nolimits_{a}$ is
invertible, and therefore a bijection.
\par
So the map $\operatorname*{shift}\nolimits_{a}$ is a bijection $\left[
n\right]  \rightarrow\left[  n\right]  $. In other words, the map
$\operatorname*{shift}\nolimits_{a}$ is a permutation of the set $\left[
n\right]  $. In other words, the map $\operatorname*{shift}\nolimits_{a}$ is a
permutation of the set $\left\{  1,2,\ldots,n\right\}  $ (since $\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $). In other words,
$\operatorname*{shift}\nolimits_{a}\in S_{n}$ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). This proves
(\ref{sol.det.2diags.shift.Sn}).}.
\end{itemize}

Let $z=\operatorname*{shift}\nolimits_{-1}$. Then, $z=\operatorname*{shift}%
\nolimits_{-1}\in S_{n}$ (by (\ref{sol.det.2diags.shift.Sn}), applied to
$a=-1$). In other words, $z$ is a permutation of $\left[  n\right]  $ (since
$S_{n}$ is the set of all permutations of $\left[  n\right]  $). We observe
some properties of $z$:

\begin{itemize}
\item We have $z=\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$, where we are
using the notations of Definition \ref{def.perm.cycles}. We will not use this
fact, and thus we will not prove it (but the proof is almost trivial).

\item We have%
\begin{equation}
z\left(  i\right)  \equiv i-1\operatorname{mod}n\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left[  n\right]  \label{sol.det.2diags.zmod}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.zmod}):} Let $i\in\left[
n\right]  $. Applying (\ref{sol.det.2diags.posrem.1}) to $h=i-1$, we obtain
$\operatorname*{posrem}\left(  i-1\right)  \in\left[  n\right]  $ and
$\operatorname*{posrem}\left(  i-1\right)  \equiv i-1\operatorname{mod}n$.
Now,%
\begin{align*}
\underbrace{z}_{=\operatorname*{shift}\nolimits_{-1}}\left(  i\right)   &
=\operatorname*{shift}\nolimits_{-1}\left(  i\right)  =\operatorname*{posrem}%
\left(  \underbrace{i+\left(  -1\right)  }_{=i-1}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{shift}%
\nolimits_{-1}\right) \\
&  =\operatorname*{posrem}\left(  i-1\right)  \equiv i-1\operatorname{mod}n.
\end{align*}
This proves (\ref{sol.det.2diags.zmod}).}. In other words,%
\begin{equation}
z\left(  i\right)  +1\equiv i\operatorname{mod}n\ \ \ \ \ \ \ \ \ \ \text{for
every }i\in\left[  n\right]  . \label{sol.det.2diags.zmod2}%
\end{equation}
In particular,%
\begin{equation}
i\neq z\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left[
n\right]  \label{sol.det.2diags.zmod0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.zmod0}):} Let $i\in\left[
n\right]  $. We have $i\equiv z\left(  i\right)  +1\operatorname{mod}n$ (by
(\ref{sol.det.2diags.zmod2})), so that $i-z\left(  i\right)  \equiv
1\not \equiv 0\operatorname{mod}n$ (since $n>1$). In other words,
$i\not \equiv z\left(  i\right)  \operatorname{mod}n$. Hence, $i\neq z\left(
i\right)  $. This proves (\ref{sol.det.2diags.zmod0}).}.

\item We have
\begin{equation}
z\left(  1\right)  =n \label{sol.det.2diags.shift.z.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.shift.z.1}):} We have
$n\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $ and $n\equiv
0\operatorname{mod}n$. Hence, $n=\operatorname*{posrem}0$ (by
(\ref{sol.det.2diags.posrem.2}), applied to $k=n$ and $h=0$). Now,
\begin{align*}
\underbrace{z}_{=\operatorname*{shift}\nolimits_{-1}}\left(  1\right)   &
=\operatorname*{shift}\nolimits_{-1}\left(  1\right)  =\operatorname*{posrem}%
\underbrace{\left(  1+\left(  -1\right)  \right)  }_{=0}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{shift}%
\nolimits_{-1}\right) \\
&  =\operatorname*{posrem}0=n\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}n=\operatorname*{posrem}0\right)  .
\end{align*}
This proves (\ref{sol.det.2diags.shift.z.1}).}.

\item We have%
\begin{equation}
z\left(  i\right)  =i-1 \label{sol.det.2diags.shift.z.i}%
\end{equation}
for every $i\in\left[  n\right]  $ satisfying $i\neq1$%
\ \ \ \ \footnote{\textit{Proof of (\ref{sol.det.2diags.shift.z.i}):} Let
$i\in\left[  n\right]  $ be such that $i\neq1$. We have $i\in\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $. Combining this with $i\neq1$, we obtain
$i\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  1\right\}  =\left\{
2,3,\ldots,n\right\}  $, so that $i-1\in\left\{  1,2,\ldots,n-1\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $.
\par
We have $i-1\in\left[  n\right]  $ and $i-1\equiv i-1\operatorname{mod}n$.
Hence, $i-1=\operatorname*{posrem}\left(  i-1\right)  $ (by
(\ref{sol.det.2diags.posrem.2}), applied to $k=i-1$ and $h=i-1$). Now,
\begin{align*}
\underbrace{z}_{=\operatorname*{shift}\nolimits_{-1}}\left(  i\right)   &
=\operatorname*{shift}\nolimits_{-1}\left(  i\right)  =\operatorname*{posrem}%
\underbrace{\left(  i+\left(  -1\right)  \right)  }_{=i-1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{shift}%
\nolimits_{-1}\right) \\
&  =\operatorname*{posrem}\left(  i-1\right)  =i-1\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i-1=\operatorname*{posrem}\left(  i-1\right)  \right)  .
\end{align*}
This proves (\ref{sol.det.2diags.shift.z.i}).}.
\end{itemize}

For every $\sigma\in S_{n}$, we let $\operatorname*{Inv}\left(  \sigma\right)
$ be the set of all inversions of the permutation $\sigma$. Thus, for every
$\sigma\in S_{n}$, we have%
\begin{align}
\ell\left(  \sigma\right)   &  =\left(  \text{the number of inversions of
}\sigma\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\ell\left(  \sigma\right)  \right) \nonumber\\
&  =\left\vert \underbrace{\left(  \text{the set of all inversions of }%
\sigma\right)  }_{\substack{=\operatorname*{Inv}\left(  \sigma\right)
\\\text{(since }\operatorname*{Inv}\left(  \sigma\right)  \text{ is the set of
all inversions of }\sigma\text{)}}}\right\vert \nonumber\\
&  =\left\vert \operatorname*{Inv}\left(  \sigma\right)  \right\vert .
\label{sol.det.2diags.invl}%
\end{align}


Now, we continue stating properties of $z$:

\begin{itemize}
\item The set $\operatorname*{Inv}\left(  z\right)  $ is the set of all
inversions of $z$ (by the definition of $\operatorname*{Inv}\left(  z\right)
$). For every $u\in\left\{  2,3,\ldots,n\right\}  $, we have $\left(
1,u\right)  \in\operatorname*{Inv}\left(  z\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $u\in\left\{  2,3,\ldots,n\right\}  $.
Thus, $2\leq u\leq n$. Now, $1<2\leq u$. Thus, $1\leq1<u\leq n$. Moreover,
$u\in\left\{  2,3,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots,n\right\}
=\left[  n\right]  $ and $u\neq1$ (since $1<u$). Hence,
(\ref{sol.det.2diags.shift.z.i}) (applied to $i=u$) shows that $z\left(
u\right)  =u-1<u\leq n$. But (\ref{sol.det.2diags.shift.z.1}) shows that
$z\left(  1\right)  =n>z\left(  u\right)  $ (since $z\left(  u\right)  <n$).
\par
Thus, $\left(  1,u\right)  $ is a pair $\left(  i,j\right)  $ of integers
satisfying $1\leq i<j\leq n$ and $z\left(  i\right)  >z\left(  j\right)  $
(since $1\leq1<u\leq n$ and $z\left(  1\right)  >z\left(  u\right)  $). In
other words, $\left(  1,u\right)  $ is an inversion of $z$ (by the definition
of an \textquotedblleft inversion of $z$\textquotedblright). In other words,
$\left(  1,u\right)  \in\operatorname*{Inv}\left(  z\right)  $ (since
$\operatorname*{Inv}\left(  z\right)  $ is the set of all inversions of $z$).
Qed.}. Hence, we can define a map $\rho:\left\{  2,3,\ldots,n\right\}
\rightarrow\operatorname*{Inv}\left(  z\right)  $ by%
\[
\left(  \rho\left(  u\right)  =\left(  1,u\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }u\in\left\{  2,3,\ldots,n\right\}
\right)  .
\]
Consider this map $\rho$. The map $\rho$ is injective\footnote{\textit{Proof.}
Let $u_{1}$ and $u_{2}$ be two elements of $\left\{  2,3,\ldots,n\right\}  $
such that $\rho\left(  u_{1}\right)  =\rho\left(  u_{2}\right)  $. Then,
$\rho\left(  u_{1}\right)  =\left(  1,u_{1}\right)  $ (by the definition of
$\rho$) and $\rho\left(  u_{2}\right)  =\left(  1,u_{2}\right)  $ (by the
definition of $\rho$). Now, $\left(  1,u_{1}\right)  =\rho\left(
u_{1}\right)  =\rho\left(  u_{2}\right)  =\left(  1,u_{2}\right)  $. In other
words, $1=1$ and $u_{1}=u_{2}$.
\par
Let us now forget that we fixed $u_{1}$ and $u_{2}$. We thus have shown that
if $u_{1}$ and $u_{2}$ are two elements of $\left\{  2,3,\ldots,n\right\}  $
such that $\rho\left(  u_{1}\right)  =\rho\left(  u_{2}\right)  $, then
$u_{1}=u_{2}$. In other words, the map $\rho$ is injective, qed.} and
surjective\footnote{\textit{Proof.} Let $c\in\operatorname*{Inv}\left(
z\right)  $. Thus, $c$ is an element of the set $\operatorname*{Inv}\left(
z\right)  $. In other words, $c$ is an inversion of $z$ (since
$\operatorname*{Inv}\left(  z\right)  $ is the set of all inversions of $z$).
In other words, $c$ is a pair $\left(  i,j\right)  $ of integers satisfying
$1\leq i<j\leq n$ and $z\left(  i\right)  >z\left(  j\right)  $ (by the
definition of an \textquotedblleft inversion of $z$\textquotedblright).
Consider this pair $\left(  i,j\right)  $. Thus, $c=\left(  i,j\right)  $.
\par
We have $i\in\left[  n\right]  $ (since $1\leq i\leq n$) and $j\in\left[
n\right]  $ (since $1\leq j\leq n$). Also, $1\leq i<j$; thus, $j>1$, so that
$j\neq1$. Hence, $z\left(  j\right)  =j-1$ (by (\ref{sol.det.2diags.shift.z.i}%
), applied to $j$ instead of $i$). From $j>1$, we obtain $j\geq2$ (since $j$
is an integer). Hence, $j\in\left\{  2,3,\ldots,n\right\}  $ (since $j\leq
n$). Thus, $\rho\left(  j\right)  $ is well-defined. The definition of $\rho$
shows that $\rho\left(  j\right)  =\left(  1,j\right)  $.
\par
We assume (for the sake of contradiction) that $i\neq1$. Thus, $z\left(
i\right)  =i-1$ (by (\ref{sol.det.2diags.shift.z.i})). Hence, $z\left(
i\right)  =\underbrace{i}_{<j}-1<j-1=z\left(  j\right)  $, which contradicts
$z\left(  i\right)  >z\left(  j\right)  $. This contradiction shows that our
assumption (that $i\neq1$) was wrong. Hence, we cannot have $i\neq1$. We thus
have $i=1$. Now, $c=\left(  \underbrace{i}_{=1},j\right)  =\left(  1,j\right)
=\rho\left(  \underbrace{j}_{\in\left\{  2,3,\ldots,n\right\}  }\right)
\in\rho\left(  \left\{  2,3,\ldots,n\right\}  \right)  $.
\par
Let us now forget that we fixed $c$. We thus have shown that $c\in\rho\left(
\left\{  2,3,\ldots,n\right\}  \right)  $ for every $c\in\operatorname*{Inv}%
\left(  z\right)  $. In other words, $\operatorname*{Inv}\left(  z\right)
\subseteq\rho\left(  \left\{  2,3,\ldots,n\right\}  \right)  $. In other
words, the map $\rho$ is surjective, qed.}. Hence, the map $\rho$ is
bijective. Thus, we have found a bijective map between the sets $\left\{
2,3,\ldots,n\right\}  $ and $\operatorname*{Inv}\left(  z\right)  $ (namely,
$\rho$). Consequently,%
\[
\left\vert \operatorname*{Inv}\left(  z\right)  \right\vert =\left\vert
\left\{  2,3,\ldots,n\right\}  \right\vert =n-1.
\]
Now, (\ref{sol.det.2diags.invl}) (applied to $\sigma=z$) shows that
$\ell\left(  z\right)  =\left\vert \operatorname*{Inv}\left(  z\right)
\right\vert =n-1$. The definition of $\left(  -1\right)  ^{z}$ now shows that
\begin{equation}
\left(  -1\right)  ^{z}=\left(  -1\right)  ^{\ell\left(  z\right)  }=\left(
-1\right)  ^{n-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell\left(
z\right)  =n-1\right)  . \label{sol.det.2diags.signz}%
\end{equation}
(Alternatively, we could have obtained this equality from Exercise
\ref{exe.perm.cycles} \textbf{(d)} (applied to $k=n$ and $\left(  i_{1}%
,i_{2},\ldots,i_{k}\right)  =\left(  n,n-1,\ldots,1\right)  $) using the
observation that $z=\operatorname*{cyc}\nolimits_{n,n-1,\ldots,1}$.)

\item Now, let us recall that $n>1$; hence, $n\neq1$. We have $z\neq
\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, $z=\operatorname*{id}$. Hence, $\underbrace{z}_{=\operatorname*{id}%
}\left(  1\right)  =\operatorname*{id}\left(  1\right)  =1$. This contradicts
$z\left(  1\right)  =n\neq1$. This contradiction proves that our assumption
was wrong; qed.}.
\end{itemize}

Now, I claim that if $\sigma\in S_{n}$ satisfies $\sigma\notin\left\{
\operatorname*{id},z\right\}  $, then%
\begin{equation}
\left(
\begin{array}
[c]{c}%
\text{there exists an }i\in\left[  n\right]  \text{ satisfying}\\
i\neq\sigma\left(  i\right)  \text{ and }i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n
\end{array}
\right)  . \label{sol.det.2diags.exi}%
\end{equation}


\textit{Proof of (\ref{sol.det.2diags.exi}):} Let $\sigma\in S_{n}$ be such
that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $. Thus, $\sigma
\neq\operatorname*{id}$ and $\sigma\neq z$.

The map $\sigma$ is an element of $S_{n}$, thus a permutation of the set
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of the set $\left\{  1,2,\ldots,n\right\}  $). In other words,
$\sigma$ is a permutation of the set $\left[  n\right]  $ (since $\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  $). Thus, $\sigma$ is a bijection
$\left[  n\right]  \rightarrow\left[  n\right]  $. Hence, the map $\sigma$ is
bijective, and therefore injective and surjective.

There exists a $J\in\left[  n\right]  $ such that $\sigma\left(  J\right)
\neq J$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, there
exists no $J\in\left[  n\right]  $ such that $\sigma\left(  J\right)  \neq J$.
In other words, every $J\in\left[  n\right]  $ satisfies $\sigma\left(
J\right)  =J$. Thus, every $J\in\left[  n\right]  $ satisfies $\sigma\left(
J\right)  =J=\operatorname*{id}\left(  J\right)  $. In other words,
$\sigma=\operatorname*{id}$. This contradicts $\sigma\neq\operatorname*{id}$.
This contradiction proves that our assumption was wrong. Qed.}. Let $j$ be the
smallest such $J$. Thus, $j$ is an element of $\left[  n\right]  $ satisfying
$\sigma\left(  j\right)  \neq j$\ \ \ \ \footnote{\textit{Proof.} We know that
$j$ is the smallest $J\in\left[  n\right]  $ such that $\sigma\left(
J\right)  \neq J$ (by the definition of $j$). Hence, $j$ is an element $J$ of
$\left[  n\right]  $ such that $\sigma\left(  J\right)  \neq J$. In other
words, $j$ is an element of $\left[  n\right]  $ satisfying $\sigma\left(
j\right)  \neq j$. Qed.}. Moreover,
\begin{equation}
\text{every }J\in\left[  n\right]  \text{ satisfying }\sigma\left(  J\right)
\neq J\text{ must satisfy }J\geq j \label{sol.det.2diags.exi.pf.min}%
\end{equation}
\ \ \ \ \footnote{\textit{Proof of (\ref{sol.det.2diags.exi.pf.min}):} We know
that $j$ is the \textbf{smallest} $J\in\left[  n\right]  $ such that
$\sigma\left(  J\right)  \neq J$ (by the definition of $j$). Hence, no
$J\in\left[  n\right]  $ such that $\sigma\left(  J\right)  \neq J$ can be
smaller than $j$. In other words, every $J\in\left[  n\right]  $ such that
$\sigma\left(  J\right)  \neq J$ must be $\geq j$. In other words, every
$J\in\left[  n\right]  $ satisfying $\sigma\left(  J\right)  \neq J$ must
satisfy $J\geq j$. This proves (\ref{sol.det.2diags.exi.pf.min}).}.

If $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$, then
(\ref{sol.det.2diags.exi}) holds\footnote{\textit{Proof.} Assume the contrary.
Thus, $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$. So we know
that $j\neq\sigma\left(  j\right)  $ (since $\sigma\left(  j\right)  \neq j$)
and $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$.
Consequently, there exists an $i\in\left[  n\right]  $ satisfying $i\neq
\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(  i\right)
+1\operatorname{mod}n$ (namely, $i=j$). In other words,
(\ref{sol.det.2diags.exi}) holds. Qed.}. Hence, for the rest of this proof of
(\ref{sol.det.2diags.exi}), we can WLOG assume that we don't have
$j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$. Assume this.

We have $j\equiv\sigma\left(  j\right)  +1\operatorname{mod}n$ (since we don't
have $j\not \equiv \sigma\left(  j\right)  +1\operatorname{mod}n$). In other
words, $\sigma\left(  j\right)  \equiv j-1\operatorname{mod}n$.

We have $j=1$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
$j\neq1$. Combining this with $j\in\left[  n\right]  =\left\{  1,2,\ldots
,n\right\}  $, we obtain $j\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{
1\right\}  =\left\{  2,3,\ldots,n\right\}  $. Hence, $j-1\in\left\{
1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $. Also, $j-1<j$.
\par
If we had $\sigma\left(  j-1\right)  \neq j-1$, then we would obtain $j-1\geq
j$ (by (\ref{sol.det.2diags.exi.pf.min}), applied to $J=j-1$); but this would
contradict $j-1<j$. Hence, we cannot have $\sigma\left(  j-1\right)  \neq
j-1$. In other words, we must have $\sigma\left(  j-1\right)  =j-1$. Thus,
$\sigma\left(  j-1\right)  =j-1\equiv\sigma\left(  j\right)
\operatorname{mod}n$ (since $\sigma\left(  j\right)  \equiv
j-1\operatorname{mod}n$). Since both $\sigma\left(  j-1\right)  $ and
$\sigma\left(  j\right)  $ are elements of $\left[  n\right]  $, we can now
apply (\ref{sol.det.2diags.posrem.3}) to $\sigma\left(  j-1\right)  $ and
$\sigma\left(  j\right)  $ instead of $a$ and $b$. As a result, we obtain
$\sigma\left(  j-1\right)  =\sigma\left(  j\right)  $. Since the map $\sigma$
is injective, this shows that $j-1=j$. But this contradicts $j-1\neq j$. This
contradiction shows that our assumption was wrong, qed.}. Thus, $\sigma\left(
\underbrace{1}_{=j}\right)  =\sigma\left(  j\right)  \equiv\underbrace{j}%
_{=1}-1=0\equiv n\operatorname{mod}n$. Since both $\sigma\left(  1\right)  $
and $n$ belong to $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, we can
now apply (\ref{sol.det.2diags.posrem.3}) to $\sigma\left(  1\right)  $ and
$n$ instead of $a$ and $b$. As a result, we obtain $\sigma\left(  1\right)
=n$.

There exists a $p\in\left[  n\right]  $ such that $\sigma\left(  p\right)
\neq z\left(  p\right)  $\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary. Thus, there exists no $p\in\left[  n\right]  $ such that
$\sigma\left(  p\right)  \neq z\left(  p\right)  $. In other words, every
$p\in\left[  n\right]  $ satisfies $\sigma\left(  p\right)  =z\left(
p\right)  $. In other words, $\sigma=z$. This contradicts $\sigma\neq z$. This
contradiction shows that our assumption was wrong. Qed.}. Consider this $p$.

We have $p\not \equiv \sigma\left(  p\right)  +1\operatorname{mod}%
n$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, $p\equiv
\sigma\left(  p\right)  +1\operatorname{mod}n$. Hence, $\sigma\left(
p\right)  \equiv p-1\operatorname{mod}n$. But (\ref{sol.det.2diags.zmod})
(applied to $i=p$) yields $z\left(  p\right)  \equiv p-1\operatorname{mod}n$.
Hence, $\sigma\left(  p\right)  \equiv p-1\equiv z\left(  p\right)
\operatorname{mod}n$. Since both $\sigma$ and $z$ are elements of $S_{n}$, we
have $\sigma\left(  p\right)  \in\left[  n\right]  $ and $z\left(  p\right)
\in\left[  n\right]  $. Thus, (\ref{sol.det.2diags.posrem.3}) (applied to
$\sigma\left(  p\right)  $ and $z\left(  p\right)  $ instead of $a$ and $b$)
shows that $\sigma\left(  p\right)  =z\left(  p\right)  $ (since
$\sigma\left(  p\right)  \equiv z\left(  p\right)  \operatorname{mod}n$). This
contradicts $\sigma\left(  p\right)  \neq z\left(  p\right)  $. This
contradiction proves that our assumption was wrong, qed.}. If we have
$p\neq\sigma\left(  p\right)  $, then (\ref{sol.det.2diags.exi})
holds\footnote{\textit{Proof.} Assume that $p\neq\sigma\left(  p\right)  $. So
we know that $p\neq\sigma\left(  p\right)  $ and $p\not \equiv \sigma\left(
p\right)  +1\operatorname{mod}n$. Consequently, there exists an $i\in\left[
n\right]  $ satisfying $i\neq\sigma\left(  i\right)  $ and $i\not \equiv
\sigma\left(  i\right)  +1\operatorname{mod}n$ (namely, $i=p$). In other
words, (\ref{sol.det.2diags.exi}) holds. Qed.}. Hence, for the rest of this
proof of (\ref{sol.det.2diags.exi}), we can WLOG assume that we don't have
$p\neq\sigma\left(  p\right)  $. Assume this.

We have $p=\sigma\left(  p\right)  $ (since we don't have $p\neq\sigma\left(
p\right)  $). In other words, $\sigma\left(  p\right)  =p$. Hence, there
exists a $K\in\left[  n\right]  $ such that $\sigma\left(  K\right)  =K$
(namely, $K=p$). Let $k$ be the largest such $K$. Thus, $k$ is an element of
$\left[  n\right]  $ satisfying $\sigma\left(  k\right)  =k$%
\ \ \ \ \footnote{\textit{Proof.} We know that $k$ is the largest $K\in\left[
n\right]  $ such that $\sigma\left(  K\right)  =K$ (by the definition of $k$).
Hence, $k$ is an element $K$ of $\left[  n\right]  $ such that $\sigma\left(
K\right)  =K$. In other words, $k$ is an element of $\left[  n\right]  $
satisfying $\sigma\left(  k\right)  =k$. Qed.}. Moreover,
\begin{equation}
\text{every }K\in\left[  n\right]  \text{ satisfying }\sigma\left(  K\right)
=K\text{ must satisfy }K\leq k \label{sol.det.2diags.exi.pf.max}%
\end{equation}
\ \ \ \ \footnote{\textit{Proof of (\ref{sol.det.2diags.exi.pf.max}):} We know
that $k$ is the \textbf{largest} $K\in\left[  n\right]  $ such that
$\sigma\left(  K\right)  =K$ (by the definition of $k$). Hence, no
$K\in\left[  n\right]  $ such that $\sigma\left(  K\right)  =K$ can be larger
than $k$. In other words, every $K\in\left[  n\right]  $ such that
$\sigma\left(  K\right)  =K$ must be $\leq k$. In other words, every
$K\in\left[  n\right]  $ satisfying $\sigma\left(  K\right)  =K$ must satisfy
$K\leq k$. This proves (\ref{sol.det.2diags.exi.pf.max}).}.

We have $n\neq1$ and thus $\sigma\left(  n\right)  \neq\sigma\left(  1\right)
$ (since the map $\sigma$ is injective). Thus, $\sigma\left(  n\right)
\neq\sigma\left(  1\right)  =n$.

If we had $k=n$, then we would have $\sigma\left(  \underbrace{n}_{=k}\right)
=\sigma\left(  k\right)  =k=n$, which would contradict $\sigma\left(
n\right)  \neq n$. Thus, we cannot have $k=n$. Hence, we have $k\neq n$. Thus,
$k+1\in\left[  n\right]  $\ \ \ \ \footnote{\textit{Proof.} Combining
$k\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $ with $k\neq n$, we
obtain $k\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  n\right\}
=\left\{  1,2,\ldots,n-1\right\}  $, so that $k+1\in\left\{  2,3,\ldots
,n\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $,
qed.}. Hence, $\sigma\left(  k+1\right)  $ is well-defined. If we had
$\sigma\left(  k+1\right)  =k+1$, then we would obtain $k+1\leq k$ (by
(\ref{sol.det.2diags.exi.pf.max}), applied to $K=k+1$), which would contradict
$k+1>k$. Hence, we cannot have $\sigma\left(  k+1\right)  =k+1$. We therefore
must have $\sigma\left(  k+1\right)  \neq k+1$. In other words, $k+1\neq
\sigma\left(  k+1\right)  $.

But we also have $k+1\not \equiv \sigma\left(  k+1\right)
+1\operatorname{mod}n$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary.
Thus, $k+1\equiv\sigma\left(  k+1\right)  +1\operatorname{mod}n$. Subtracting
$1$ from both sides of this congruence, we obtain $k\equiv\sigma\left(
k+1\right)  \operatorname{mod}n$. Since both $k$ and $\sigma\left(
k+1\right)  $ belong to $\left[  n\right]  $, we can therefore apply
(\ref{sol.det.2diags.posrem.3}) to $k$ and $\sigma\left(  k+1\right)  $
instead of $a$ and $b$. As a result, we obtain $k=\sigma\left(  k+1\right)  $.
Hence, $\sigma\left(  k+1\right)  =k=\sigma\left(  k\right)  $ (since
$\sigma\left(  k\right)  =k$). Since the map $\sigma$ is injective, this
yields that $k+1=k$. But this contradicts $k+1>k$. This contradiction proves
that our assumption was false, qed.}. Hence, there exists an $i\in\left[
n\right]  $ satisfying $i\neq\sigma\left(  i\right)  $ and $i\not \equiv
\sigma\left(  i\right)  +1\operatorname{mod}n$ (namely, $i=k+1$). In other
words, (\ref{sol.det.2diags.exi}) holds. This completes the proof of
(\ref{sol.det.2diags.exi}).

Let us now write our matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$. Then,%
\[
\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A=\left(
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
In other words, we have%
\begin{equation}
a_{i,j}=%
\begin{cases}
a_{j}, & \text{if }i=j;\\
b_{j}, & \text{if }i\equiv j+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\label{sol.det.2diags.aij}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$.

It is now easy to see that if $\sigma\in S_{n}$ satisfies $\sigma
\notin\left\{  \operatorname*{id},z\right\}  $, then
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.det.2diags.0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.0}):} Let $\sigma\in S_{n}$ be
such that $\sigma\notin\left\{  \operatorname*{id},z\right\}  $. Thus, there
exists an $i\in\left[  n\right]  $ satisfying $i\neq\sigma\left(  i\right)  $
and $i\not \equiv \sigma\left(  i\right)  +1\operatorname{mod}n$ (because of
(\ref{sol.det.2diags.exi})). Consider this $i$. We have neither $i=\sigma
\left(  i\right)  $ nor $i\equiv\sigma\left(  i\right)  +1\operatorname{mod}n$
(since we have $i\neq\sigma\left(  i\right)  $ and $i\not \equiv \sigma\left(
i\right)  +1\operatorname{mod}n$).
\par
From (\ref{sol.det.2diags.aij}) (applied to $\left(  i,\sigma\left(  i\right)
\right)  $ instead of $\left(  i,j\right)  $), we obtain%
\[
a_{i,\sigma\left(  i\right)  }=%
\begin{cases}
a_{\sigma\left(  i\right)  }, & \text{if }i=\sigma\left(  i\right)  ;\\
b_{\sigma\left(  i\right)  }, & \text{if }i\equiv\sigma\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
=0
\]
(since we have neither $i=\sigma\left(  i\right)  $ nor $i\equiv\sigma\left(
i\right)  +1\operatorname{mod}n$).
\par
Now, let us forget that we fixed $i$. We thus have found an $i\in\left[
n\right]  $ satisfying $a_{i,\sigma\left(  i\right)  }=0$. Thus, there exists
an $i\in\left[  n\right]  $ satisfying $a_{i,\sigma\left(  i\right)  }=0$. In
other words, at least one factor of the product $\prod_{i\in\left[  n\right]
}a_{i,\sigma\left(  i\right)  }$ equals $0$. Therefore, the whole product
$\prod_{i\in\left[  n\right]  }a_{i,\sigma\left(  i\right)  }$ equals $0$
(because if at least one factor of a product equals $0$, then the whole
product equals $0$). In other words, $\prod_{i\in\left[  n\right]
}a_{i,\sigma\left(  i\right)  }=0$.
\par
Now,
\[
\underbrace{\prod_{i=1}^{n}}_{\substack{=\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }=\prod_{i\in\left[  n\right]  }\\\text{(since }\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}}}a_{i,\sigma\left(
i\right)  }=\prod_{i\in\left[  n\right]  }a_{i,\sigma\left(  i\right)  }=0.
\]
This proves (\ref{sol.det.2diags.0}).}.

Let us also notice that%
\begin{equation}
\prod_{i=1}^{n}a_{i,z\left(  i\right)  }=\prod_{i=1}^{n}b_{i}
\label{sol.det.2diags.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det.2diags.1}):} The map $z$ is a
permutation of $\left[  n\right]  $. In other words, the map $z$ is a
permutation of $\left\{  1,2,\ldots,n\right\}  $ (since $\left[  n\right]
=\left\{  1,2,\ldots,n\right\}  $), thus a bijection $\left\{  1,2,\ldots
,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $. Hence, we can
substitute $i$ for $z\left(  i\right)  $ in the product $\prod_{i\in\left\{
1,2,\ldots,n\right\}  }b_{z\left(  i\right)  }$. We thus obtain $\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{z\left(  i\right)  }=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{i}$.
\par
But for every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{align*}
a_{i,z\left(  i\right)  }  &  =%
\begin{cases}
a_{z\left(  i\right)  }, & \text{if }i=z\left(  i\right)  ;\\
b_{z\left(  i\right)  }, & \text{if }i\equiv z\left(  i\right)
+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.det.2diags.aij}), applied to
}\left(  i,z\left(  i\right)  \right)  \text{ instead of }\left(  i,j\right)
\right) \\
&  =b_{z\left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since we don't have }i=z\left(  i\right)  \text{ (because of
(\ref{sol.det.2diags.zmod0})),}\\
\text{but we do have }i\equiv z\left(  i\right)  +1\operatorname{mod}n\text{
(by (\ref{sol.det.2diags.zmod}))}%
\end{array}
\right)  .
\end{align*}
Thus,%
\[
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}\underbrace{a_{i,z\left(  i\right)  }}_{=b_{z\left(  i\right)  }}=\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }b_{z\left(  i\right)  }%
=\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}%
}b_{i}=\prod_{i=1}^{n}b_{i}.
\]
This proves (\ref{sol.det.2diags.1}).}.

Now, (\ref{eq.det.eq.2}) becomes%
\begin{align*}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\operatorname*{id}}}_{=1}\prod_{i=1}%
^{n}\underbrace{a_{i,\operatorname*{id}\left(  i\right)  }}%
_{\substack{=a_{i,i}\\\text{(since }\operatorname*{id}\left(  i\right)
=i\text{)}}}+\underbrace{\left(  -1\right)  ^{z}}_{\substack{=\left(
-1\right)  ^{n-1}\\\text{(by (\ref{sol.det.2diags.signz}))}}}\prod_{i=1}%
^{n}a_{i,z\left(  i\right)  }+\sum_{\substack{\sigma\in S_{n};\\\sigma
\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)  ^{\sigma
}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}%
_{\substack{=0\\\text{(by (\ref{sol.det.2diags.0}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addends for }\sigma=\operatorname*{id}\text{
and}\\
\text{for }\sigma=z\text{ from the sum (since }z\neq\operatorname*{id}\text{)}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n}a_{i,i}+\left(  -1\right)  ^{n-1}\prod_{i=1}^{n}%
a_{i,z\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\notin\left\{  \operatorname*{id},z\right\}  }}\left(  -1\right)
^{\sigma}0}_{=0}\\
&  =\prod_{i=1}^{n}\underbrace{a_{i,i}}_{\substack{=%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
\\\text{(by (\ref{sol.det.2diags.aij}), applied to }\left(  i,i\right)  \text{
instead of }\left(  i,j\right)  \text{)}}}+\left(  -1\right)  ^{n-1}%
\underbrace{\prod_{i=1}^{n}a_{i,z\left(  i\right)  }}_{\substack{=\prod
_{i=1}^{n}b_{i}\\\text{(by (\ref{sol.det.2diags.1}))}}}\\
&  =\prod_{i=1}^{n}\underbrace{%
\begin{cases}
a_{i}, & \text{if }i=i;\\
b_{i}, & \text{if }i\equiv i+1\operatorname{mod}n;\\
0, & \text{otherwise}%
\end{cases}
}_{\substack{=a_{i}\\\text{(since }i=i\text{)}}}+\left(  -1\right)
^{n-1}\prod_{i=1}^{n}b_{i}\\
&  =\underbrace{\prod_{i=1}^{n}a_{i}}_{=a_{1}a_{2}\cdots a_{n}}+\left(
-1\right)  ^{n-1}\underbrace{\prod_{i=1}^{n}b_{i}}_{=b_{1}b_{2}\cdots b_{n}}\\
&  =a_{1}a_{2}\cdots a_{n}+\left(  -1\right)  ^{n-1}b_{1}b_{2}\cdots b_{n}.
\end{align*}
This solves Exercise \ref{exe.det.2diags}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.altern.STAS}}

Before we come to the solution of Exercise \ref{exe.altern.STAS}, let us first
state some simple lemmas:

\begin{lemma}
\label{lem.sol.altern.STAS.1}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$,
$p\in\mathbb{N}$ and $q\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\in\mathbb{K}^{n\times m}$, $B=\left(  b_{i,j}\right)
_{1\leq i\leq m,\ 1\leq j\leq p}\in\mathbb{K}^{m\times p}$ and $C=\left(
c_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq q}\in\mathbb{K}^{p\times q}$.
Then,%
\[
ABC=\left(  \sum_{k=1}^{m}\sum_{\ell=1}^{p}a_{i,k}b_{k,\ell}c_{\ell,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq q}.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.STAS.1}.]We have $B=\left(  b_{i,j}%
\right)  _{1\leq i\leq m,\ 1\leq j\leq p}$ and $C=\left(  c_{i,j}\right)
_{1\leq i\leq p,\ 1\leq j\leq q}$. Thus, the definition of the product $BC$
yields%
\[
BC=\left(  \underbrace{\sum_{k=1}^{p}b_{i,k}c_{k,j}}_{\substack{=\sum_{\ell
=1}^{p}b_{i,\ell}c_{\ell,j}\\\text{(here, we renamed the}\\\text{summation
index }k\text{ as }\ell\text{)}}}\right)  _{1\leq i\leq m,\ 1\leq j\leq
q}=\left(  \sum_{\ell=1}^{p}b_{i,\ell}c_{\ell,j}\right)  _{1\leq i\leq
m,\ 1\leq j\leq q}.
\]


Now, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ and
$BC=\left(  \sum_{\ell=1}^{p}b_{i,\ell}c_{\ell,j}\right)  _{1\leq i\leq
m,\ 1\leq j\leq q}$. Hence, the definition of the product $A\left(  BC\right)
$ yields%
\[
A\left(  BC\right)  =\left(  \sum_{k=1}^{m}\underbrace{a_{i,k}\left(
\sum_{\ell=1}^{p}b_{k,\ell}c_{\ell,j}\right)  }_{=\sum_{\ell=1}^{p}%
a_{i,k}b_{k,\ell}c_{\ell,j}}\right)  _{1\leq i\leq n,\ 1\leq j\leq q}=\left(
\sum_{k=1}^{m}\sum_{\ell=1}^{p}a_{i,k}b_{k,\ell}c_{\ell,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq q}.
\]
Thus,%
\[
ABC=A\left(  BC\right)  =\left(  \sum_{k=1}^{m}\sum_{\ell=1}^{p}%
a_{i,k}b_{k,\ell}c_{\ell,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq q}.
\]
This proves Lemma \ref{lem.sol.altern.STAS.1}.
\end{proof}

\begin{lemma}
\label{lem.sol.altern.STAS.2}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times
n$-matrix. Let $S=\left(  s_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ be
an $n\times m$-matrix. Then,%
\[
S^{T}AS=\left(  \sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}s_{k,i}s_{\ell,j}a_{k,\ell}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.STAS.2}.]We have $S=\left(  s_{i,j}%
\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Thus, $S^{T}=\left(
s_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$ (by the definition of
$S^{T}$). Recall also that $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ and $S=\left(  s_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.
Hence, Lemma \ref{lem.sol.altern.STAS.1} (applied to $m$, $n$, $n$, $m$,
$S^{T}$, $A$, $S$, $s_{j,i}$, $a_{i,j}$ and $s_{i,j}$ instead of $n$, $m$,
$p$, $q$, $A$, $B$, $C$, $a_{i,j}$, $b_{i,j}$ and $c_{i,j}$) yields%
\begin{align*}
S^{T}AS  &  =\left(  \underbrace{\sum_{k=1}^{n}}_{=\sum_{k\in\left\{
1,2,\ldots,n\right\}  }}\underbrace{\sum_{\ell=1}^{n}}_{=\sum_{\ell\in\left\{
1,2,\ldots,n\right\}  }}s_{k,i}\underbrace{a_{k,\ell}s_{\ell,j}}_{=s_{\ell
,j}a_{k,\ell}}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}\\
&  =\left(  \underbrace{\sum_{k\in\left\{  1,2,\ldots,n\right\}  }\sum
_{\ell\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}}}s_{k,i}s_{\ell,j}a_{k,\ell}\right)
_{1\leq i\leq m,\ 1\leq j\leq m}\\
&  =\left(  \sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}s_{k,i}s_{\ell,j}a_{k,\ell}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}.
\end{align*}
This proves Lemma \ref{lem.sol.altern.STAS.2}.
\end{proof}

\begin{lemma}
\label{lem.sol.altern.STAS.3}Let $n\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an alternating $n\times
n$-matrix.

\textbf{(a)} Every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies $a_{i,i}=0$.

\textbf{(b)} Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}$ satisfy $a_{i,j}=-a_{j,i}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.STAS.3}.]Recall that the matrix $A$ is
alternating if and only if it satisfies $A^{T}=-A$ and $\left(  a_{i,i}%
=0\text{ for all }i\in\left\{  1,2,\ldots,n\right\}  \right)  $ (by the
definition of \textquotedblleft alternating\textquotedblright). Hence, the
matrix $A$ satisfies $A^{T}=-A$ and $\left(  a_{i,i}=0\text{ for all }%
i\in\left\{  1,2,\ldots,n\right\}  \right)  $ (since $A$ is alternating).

We have $\left(  a_{i,i}=0\text{ for all }i\in\left\{  1,2,\ldots,n\right\}
\right)  $. In other words, every $i\in\left\{  1,2,\ldots,n\right\}  $
satisfies $a_{i,i}=0$. This proves Lemma \ref{lem.sol.altern.STAS.3}
\textbf{(a)}.

\textbf{(b)} We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$, and thus $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $A^{T}$). Hence,%
\[
\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A^{T}=-\underbrace{A}%
_{=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}}=-\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  -a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}.
\]
In other words,%
\begin{equation}
a_{j,i}=-a_{i,j}\ \ \ \ \ \ \ \ \ \ \text{for every }\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}. \label{pf.lem.sol.altern.STAS.3.1}%
\end{equation}


Now, let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. Hence,
(\ref{pf.lem.sol.altern.STAS.3.1}) yields $a_{j,i}=-a_{i,j}$. In other words,
$a_{i,j}=-a_{j,i}$. This proves Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.altern.STAS}.]Let $B$ be the $m\times m$-matrix
$S^{T}AS$. Thus, $B=S^{T}AS$. Write the $m\times m$-matrix $B$ in the form
$B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$.

Write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Write the $n\times m$-matrix $S$ in the
form $S=\left(  s_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Then,%
\begin{equation}
B=S^{T}AS=\left(  \sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}}s_{k,i}s_{\ell,j}a_{k,\ell}\right)  _{1\leq i\leq m,\ 1\leq
j\leq m} \label{sol.altern.STAS.1}%
\end{equation}
(by Lemma \ref{lem.sol.altern.STAS.2}).

But $B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$, so that%
\[
\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}=B=\left(
\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}%
s_{k,i}s_{\ell,j}a_{k,\ell}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}%
\]
(by (\ref{sol.altern.STAS.1})). In other words,%
\begin{equation}
b_{i,j}=\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}%
}s_{k,i}s_{\ell,j}a_{k,\ell}\ \ \ \ \ \ \ \ \ \ \text{for every }\left(
i,j\right)  \in\left\{  1,2,\ldots,m\right\}  ^{2}. \label{sol.altern.STAS.4}%
\end{equation}


\begin{vershort}
But
\[
\left(  b_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}=\left(
-b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}%
\]
\footnote{\textit{Proof:} Every $\left(  i,j\right)  \in\left\{
1,2,\ldots,m\right\}  ^{2}$ satisfies
\begin{align*}
b_{j,i}  &  =\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}s_{k,j}s_{\ell,i}a_{k,\ell}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.altern.STAS.4}) (applied to }\left(  j,i\right)  \text{ instead of
}\left(  i,j\right)  \text{)}\right) \\
&  =\underbrace{\sum_{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}}_{\substack{=\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}}}}\underbrace{s_{\ell,j}s_{k,i}}_{=s_{k,i}s_{\ell,j}%
}\underbrace{a_{\ell,k}}_{\substack{=-a_{k,\ell}\\\text{(by Lemma
\ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied to }\left(
\ell,k\right)  \\\text{instead of }\left(  i,j\right)  \text{))}%
}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the}\\
\text{summation index }\left(  k,\ell\right)  \text{ as }\left(
\ell,k\right)
\end{array}
\right) \\
&  =\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}%
}s_{k,i}s_{\ell,j}\left(  -a_{k,\ell}\right)  =-\underbrace{\sum_{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}s_{k,i}s_{\ell
,j}a_{k,\ell}}_{\substack{=b_{i,j}\\\text{(by (\ref{sol.altern.STAS.4}))}%
}}=-b_{i,j}.
\end{align*}
In other words, $\left(  b_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq
m}=\left(  -b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$.}. But
$B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$, and thus
$B^{T}=\left(  b_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$ (by the
definition of $B^{T}$). Hence,%
\[
B^{T}=\left(  b_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}=\left(
-b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}.
\]

\end{vershort}

\begin{verlong}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,m\right\}  ^{2}$
satisfies%
\begin{equation}
b_{j,i}=-b_{i,j} \label{sol.altern.STAS.5}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.altern.STAS.5}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,m\right\}  ^{2}$. Thus, $i\in\left\{
1,2,\ldots,m\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}  $. Hence,
$j\in\left\{  1,2,\ldots,m\right\}  $ and $i\in\left\{  1,2,\ldots,m\right\}
$. In other words, $\left(  j,i\right)  \in\left\{  1,2,\ldots,m\right\}
^{2}$. Now, (\ref{sol.altern.STAS.4}) (applied to $\left(  j,i\right)  $
instead of $\left(  i,j\right)  $) yields%
\begin{align*}
b_{j,i}  &  =\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}s_{k,j}s_{\ell,i}a_{k,\ell}\\
&  =\underbrace{\sum_{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}}_{\substack{=\sum_{\ell\in\left\{  1,2,\ldots,n\right\}  }\sum
_{k\in\left\{  1,2,\ldots,n\right\}  }\\=\sum_{k\in\left\{  1,2,\ldots
,n\right\}  }\sum_{\ell\in\left\{  1,2,\ldots,n\right\}  }\\=\sum_{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}}}\underbrace{s_{\ell
,j}s_{k,i}}_{=s_{k,i}s_{\ell,j}}\underbrace{a_{\ell,k}}_{\substack{=-a_{k,\ell
}\\\text{(by Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied
to }\left(  \ell,k\right)  \\\text{instead of }\left(  i,j\right)  \text{))}%
}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the}\\
\text{summation index }\left(  k,\ell\right)  \text{ as }\left(
\ell,k\right)
\end{array}
\right) \\
&  =\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}%
}s_{k,i}s_{\ell,j}\left(  -a_{k,\ell}\right)  =-\underbrace{\sum_{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}s_{k,i}s_{\ell
,j}a_{k,\ell}}_{\substack{=b_{i,j}\\\text{(by (\ref{sol.altern.STAS.4}))}%
}}=-b_{i,j}.
\end{align*}
This proves (\ref{sol.altern.STAS.5}).}. But $B=\left(  b_{i,j}\right)
_{1\leq i\leq m,\ 1\leq j\leq m}$, and thus $B^{T}=\left(  b_{j,i}\right)
_{1\leq i\leq m,\ 1\leq j\leq m}$ (by the definition of $B^{T}$). Hence,%
\[
B^{T}=\left(  \underbrace{b_{j,i}}_{=-b_{i,j}}\right)  _{1\leq i\leq m,\ 1\leq
j\leq m}=\left(  -b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}.
\]

\end{verlong}

Comparing this with%
\[
-\underbrace{B}_{=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}%
}=-\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}=\left(
-b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m},
\]
we obtain $B^{T}=-B$.

\begin{vershort}
Let $i\in\left\{  1,2,\ldots,m\right\}  $. Then, (\ref{sol.altern.STAS.4})
(applied to $\left(  i,i\right)  $ instead of $\left(  i,j\right)  $) yields%
\begin{align*}
b_{i,i}  &  =\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}s_{k,i}s_{\ell,i}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,i}a_{k,\ell}+\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,i}\underbrace{a_{k,\ell}}_{\substack{=a_{\ell,\ell}\\\text{(since
}k=\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}\underbrace{s_{k,i}s_{\ell,i}}%
_{=s_{\ell,i}s_{k,i}}\underbrace{a_{k,\ell}}_{\substack{=-a_{\ell
,k}\\\text{(by Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied
to }\left(  k,\ell\right)  \text{ instead of }\left(  i,j\right)  \text{))}%
}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}\text{ satisfies exactly one of}\\
\text{the three assertions }\left(  k<\ell\right)  \text{, }\left(
k=\ell\right)  \text{ and }\left(  k>\ell\right)
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{k,i}s_{\ell,i}a_{k,\ell}%
}_{\substack{=\sum_{\substack{\left(  \ell,k\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}\\\text{(here,
we have renamed the}\\\text{summation index }\left(  k,\ell\right)  \text{ as
}\left(  \ell,k\right)  \text{)}}}+\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}s_{\ell,i}%
\underbrace{a_{\ell,\ell}}_{\substack{=0\\\text{(by Lemma
\ref{lem.sol.altern.STAS.3} \textbf{(a)}}\\\text{(applied to }\ell\text{
instead of }i\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell,i}s_{k,i}\left(
-a_{\ell,k}\right)  }_{=-\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell,i}s_{k,i}a_{\ell,k}}\\
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}+\underbrace{\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,i}0}_{=0}+\left(  -\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell,i}s_{k,i}a_{\ell,k}\right)
\end{align*}%
\begin{align*}
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}-\underbrace{\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}}%
_{\substack{=\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2};\\k>\ell}}=\sum_{\substack{\left(  \ell,k\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\\ell<k}}\\\text{(because for every }\left(
\ell,k\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}\text{,}\\\text{the
statement }\left(  k>\ell\right)  \text{ is equivalent to }\left(
\ell<k\right)  \text{)}}}s_{\ell,i}s_{k,i}a_{\ell,k}\\
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}-\sum_{\substack{\left(
\ell,k\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\\ell<k}}s_{\ell
,i}s_{k,i}a_{\ell,k}=0.
\end{align*}

\end{vershort}

\begin{verlong}
Let $i\in\left\{  1,2,\ldots,m\right\}  $. Thus, $\left(  i,i\right)
\in\left\{  1,2,\ldots,m\right\}  ^{2}$. Hence, (\ref{sol.altern.STAS.4})
(applied to $\left(  i,i\right)  $ instead of $\left(  i,j\right)  $) yields%
\begin{align*}
b_{i,i}  &  =\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}}s_{k,i}s_{\ell,i}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,i}a_{k,\ell}+\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,i}\underbrace{a_{k,\ell}}_{\substack{=a_{\ell,\ell}\\\text{(since
}k=\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}\underbrace{s_{k,i}s_{\ell,i}}%
_{=s_{\ell,i}s_{k,i}}\underbrace{a_{k,\ell}}_{\substack{=-a_{\ell
,k}\\\text{(by Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied
to }\left(  k,\ell\right)  \text{ instead of }\left(  i,j\right)  \text{))}%
}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}\text{ satisfies exactly one of}\\
\text{the three assertions }\left(  k<\ell\right)  \text{, }\left(
k=\ell\right)  \text{ and }\left(  k>\ell\right)
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{k,i}s_{\ell,i}a_{k,\ell}%
}_{\substack{=\sum_{\substack{\left(  \ell,k\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}\\\text{(here,
we have renamed the}\\\text{summation index }\left(  k,\ell\right)  \text{ as
}\left(  \ell,k\right)  \text{)}}}+\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}s_{\ell,i}%
\underbrace{a_{\ell,\ell}}_{\substack{=0\\\text{(by Lemma
\ref{lem.sol.altern.STAS.3} \textbf{(a)}}\\\text{(applied to }\ell\text{
instead of }i\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell,i}s_{k,i}\left(
-a_{\ell,k}\right)  }_{=-\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell,i}s_{k,i}a_{\ell,k}}\\
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}+\underbrace{\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,i}0}_{=0}+\left(  -\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell,i}s_{k,i}a_{\ell,k}\right) \\
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}+\left(  -\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{\ell
,i}s_{k,i}a_{\ell,k}\right)
\end{align*}%
\begin{align*}
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}-\underbrace{\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}}%
_{\substack{=\sum_{k\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\ell
\in\left\{  1,2,\ldots,n\right\}  ;\\k>\ell}}\\=\sum_{\ell\in\left\{
1,2,\ldots,n\right\}  }\sum_{\substack{k\in\left\{  1,2,\ldots,n\right\}
;\\k>\ell}}\\=\sum_{\substack{\left(  \ell,k\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k>\ell}}=\sum_{\substack{\left(  \ell,k\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\\ell<k}}\\\text{(because for every
}\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}\text{,}%
\\\text{the statement }\left(  k>\ell\right)  \text{ is equivalent to }\left(
\ell<k\right)  \text{)}}}s_{\ell,i}s_{k,i}a_{\ell,k}\\
&  =\sum_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell<k}}s_{\ell,i}s_{k,i}a_{\ell,k}-\sum_{\substack{\left(
\ell,k\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\\ell<k}}s_{\ell
,i}s_{k,i}a_{\ell,k}=0.
\end{align*}

\end{verlong}

Now, forget that we fixed $i$. We thus have shown that $\left(  b_{i,i}%
=0\text{ for all }i\in\left\{  1,2,\ldots,m\right\}  \right)  $.

Now, recall that $B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$.
Hence, the $m\times m$-matrix $B$ is alternating if and only if it satisfies
$B^{T}=-B$ and $\left(  b_{i,i}=0\text{ for all }i\in\left\{  1,2,\ldots
,m\right\}  \right)  $ (by the definition of \textquotedblleft
alternating\textquotedblright). Thus, the $m\times m$-matrix $B$ is
alternating (since it satisfies $B^{T}=-B$ and $\left(  b_{i,i}=0\text{ for
all }i\in\left\{  1,2,\ldots,m\right\}  \right)  $). Since $B=S^{T}AS$, this
rewrites as follows: The $m\times m$-matrix $S^{T}AS$ is alternating. This
solves Exercise \ref{exe.altern.STAS}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.altern.det}}

Before we solve Exercise \ref{exe.altern.det}, let us show a combinatorial lemma:

\begin{lemma}
\label{lem.sol.altern.det.mod2}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}$ be
such that $\sigma^{-1}=\sigma$. Let $X=\left\{  i\in\left\{  1,2,\ldots
,n\right\}  \ \mid\ \sigma\left(  i\right)  =i\right\}  $. Then,%
\[
\left\vert X\right\vert \equiv n\operatorname{mod}2.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.det.mod2}.]Define two further sets $Y$ and
$Z$ by
\[
Y=\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(  i\right)
<i\right\}
\]
and%
\[
Z=\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(  i\right)
>i\right\}  .
\]


Every $j\in Y$ satisfies $\sigma\left(  j\right)  \in Z$%
\ \ \ \ \footnote{\textit{Proof.} Let $j\in Y$. Then,%
\[
j\in Y=\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(
i\right)  <i\right\}  .
\]
In other words, $j$ is an element $i$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $\sigma\left(  i\right)  <i$. In other words, $j$ is an element of
$\left\{  1,2,\ldots,n\right\}  $ and satisfies $\sigma\left(  j\right)  <j$.
\par
Clearly, $\sigma\left(  j\right)  $ is an element of $\left\{  1,2,\ldots
,n\right\}  $ and satisfies $\sigma\left(  \sigma\left(  j\right)  \right)
>\sigma\left(  j\right)  $ (since $\sigma\left(  j\right)
<j=\underbrace{\sigma^{-1}}_{=\sigma}\left(  \sigma\left(  j\right)  \right)
=\sigma\left(  \sigma\left(  j\right)  \right)  $). In other words,
$\sigma\left(  j\right)  $ is an element $i$ of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $\sigma\left(  i\right)  >i$. In other words,
$\sigma\left(  j\right)  \in\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  >i\right\}  $. This rewrites as $\sigma\left(
j\right)  \in Z$ (since $Z=\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  >i\right\}  $). Qed.}. Thus, we can define a
map $\alpha:Y\rightarrow Z$ by
\[
\left(  \alpha\left(  j\right)  =\sigma\left(  j\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in Y\right)  .
\]
Consider this map $\alpha$. Similarly, define a map $\beta:Z\rightarrow Y$ by
\[
\left(  \beta\left(  j\right)  =\sigma\left(  j\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in Z\right)  .
\]


We have $\alpha\circ\beta=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $j\in Z$. The definition of $\beta$ yields $\beta\left(  j\right)
=\sigma\left(  j\right)  $. Comparing this with $\underbrace{\sigma^{-1}%
}_{=\sigma}\left(  j\right)  =\sigma\left(  j\right)  $, we obtain
$\beta\left(  j\right)  =\sigma^{-1}\left(  j\right)  $. Now,%
\begin{align*}
\left(  \alpha\circ\beta\right)  \left(  j\right)   &  =\alpha\left(
\beta\left(  j\right)  \right)  =\sigma\left(  \underbrace{\beta\left(
j\right)  }_{=\sigma^{-1}\left(  j\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha\right) \\
&  =\sigma\left(  \sigma^{-1}\left(  j\right)  \right)  =j=\operatorname*{id}%
\left(  j\right)  .
\end{align*}
\par
Now, forget that we fixed $j$. We thus have shown that $\left(  \alpha
\circ\beta\right)  \left(  j\right)  =\operatorname*{id}\left(  j\right)  $
for each $j\in Z$. In other words, $\alpha\circ\beta=\operatorname*{id}$.
Qed.} and $\beta\circ\alpha=\operatorname*{id}$\ \ \ \ \footnote{for similar
reasons}. Thus, the two maps $\alpha$ and $\beta$ are mutually inverse. Hence,
the map $\alpha$ is invertible, i.e., is a bijection. Thus, there exists a
bijection $Y\rightarrow Z$ (namely, the map $\alpha$). Hence, $\left\vert
Y\right\vert =\left\vert Z\right\vert $.

Now,%
\[
\sum_{i\in\left\{  1,2,\ldots,n\right\}  }1=\underbrace{\left\vert \left\{
1,2,\ldots,n\right\}  \right\vert }_{=n}\cdot1=n\cdot1=n.
\]
Hence,%
\begin{align*}
n  &  =\sum_{i\in\left\{  1,2,\ldots,n\right\}  }1\\
&  =\underbrace{\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\\sigma\left(  i\right)  <i}}}_{\substack{=\sum_{i\in Y}\\\text{(since
}\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(  i\right)
<i\right\}  =Y\text{)}}}1+\underbrace{\sum_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\\sigma\left(  i\right)  =i}}}_{\substack{=\sum_{i\in
X}\\\text{(since }\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid
\ \sigma\left(  i\right)  =i\right\}  =X\text{)}}}1\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\\sigma\left(  i\right)  >i}}}_{\substack{=\sum_{i\in
Z}\\\text{(since }\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid
\ \sigma\left(  i\right)  >i\right\}  =Z\text{)}}}1\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfies exactly
one of the three}\\
\text{statements }\left(  \sigma\left(  i\right)  <i\right)  \text{, }\left(
\sigma\left(  i\right)  =i\right)  \text{ and }\left(  \sigma\left(  i\right)
>i\right)
\end{array}
\right) \\
&  =\underbrace{\sum_{i\in Y}1}_{=\left\vert Y\right\vert \cdot1=\left\vert
Y\right\vert }+\underbrace{\sum_{i\in X}1}_{=\left\vert X\right\vert
\cdot1=\left\vert X\right\vert }+\underbrace{\sum_{i\in Z}1}_{=\left\vert
Z\right\vert \cdot1=\left\vert Z\right\vert }=\underbrace{\left\vert
Y\right\vert }_{=\left\vert Z\right\vert }+\left\vert X\right\vert +\left\vert
Z\right\vert \\
&  =\left\vert Z\right\vert +\left\vert X\right\vert +\left\vert Z\right\vert
=\left\vert X\right\vert +2\cdot\left\vert Z\right\vert \equiv\left\vert
X\right\vert \operatorname{mod}2.
\end{align*}
This proves Lemma \ref{lem.sol.altern.det.mod2}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.det.mod2}.]We have $X=\left\{
i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(  i\right)  =i\right\}
$. Thus,%
\begin{equation}
\sum_{i\in X}=\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\\sigma\left(  i\right)  =i}} \label{pf.lem.sol.altern.det.mod2.X}%
\end{equation}
(an equality between summation signs).

Define a further set $Y$ by $Y=\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  <i\right\}  $. Thus,%
\begin{equation}
\sum_{i\in Y}=\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\\sigma\left(  i\right)  <i}} \label{pf.lem.sol.altern.det.mod2.Y}%
\end{equation}
(an equality between summation signs).

Define a further set $Z$ by $Z=\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  >i\right\}  $. Thus,%
\begin{equation}
\sum_{i\in Z}=\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\\sigma\left(  i\right)  >i}} \label{pf.lem.sol.altern.det.mod2.Z}%
\end{equation}
(an equality between summation signs).

Every $j\in Y$ satisfies $\sigma\left(  j\right)  \in Z$%
\ \ \ \ \footnote{\textit{Proof.} Let $j\in Y$. Then,%
\[
j\in Y=\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(
i\right)  <i\right\}  .
\]
In other words, $j$ is an element $i$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $\sigma\left(  i\right)  <i$. In other words, $j$ is an element of
$\left\{  1,2,\ldots,n\right\}  $ and satisfies $\sigma\left(  j\right)  <j$.
\par
Clearly, $\sigma\left(  j\right)  $ is an element of $\left\{  1,2,\ldots
,n\right\}  $ and satisfies $\sigma\left(  \sigma\left(  j\right)  \right)
>\sigma\left(  j\right)  $ (since $\sigma\left(  j\right)
<j=\underbrace{\sigma^{-1}}_{=\sigma}\left(  \sigma\left(  j\right)  \right)
=\sigma\left(  \sigma\left(  j\right)  \right)  $). In other words,
$\sigma\left(  j\right)  $ is an element $i$ of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $\sigma\left(  i\right)  >i$. In other words,
$\sigma\left(  j\right)  \in\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  >i\right\}  $. This rewrites as $\sigma\left(
j\right)  \in Z$ (since $Z=\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  >i\right\}  $). Qed.}. Thus, we can define a
map $\alpha:Y\rightarrow Z$ by
\[
\left(  \alpha\left(  j\right)  =\sigma\left(  j\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in Y\right)  .
\]
Consider this map $\alpha$.

Every $j\in Z$ satisfies $\sigma\left(  j\right)  \in Y$%
\ \ \ \ \footnote{\textit{Proof.} Let $j\in Z$. Then,%
\[
j\in Z=\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(
i\right)  >i\right\}  .
\]
In other words, $j$ is an element $i$ of $\left\{  1,2,\ldots,n\right\}  $
satisfying $\sigma\left(  i\right)  >i$. In other words, $j$ is an element of
$\left\{  1,2,\ldots,n\right\}  $ and satisfies $\sigma\left(  j\right)  >j$.
\par
Clearly, $\sigma\left(  j\right)  $ is an element of $\left\{  1,2,\ldots
,n\right\}  $ and satisfies $\sigma\left(  \sigma\left(  j\right)  \right)
<\sigma\left(  j\right)  $ (since $\sigma\left(  j\right)
>j=\underbrace{\sigma^{-1}}_{=\sigma}\left(  \sigma\left(  j\right)  \right)
=\sigma\left(  \sigma\left(  j\right)  \right)  $). In other words,
$\sigma\left(  j\right)  $ is an element $i$ of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $\sigma\left(  i\right)  <i$. In other words,
$\sigma\left(  j\right)  \in\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  <i\right\}  $. This rewrites as $\sigma\left(
j\right)  \in Y$ (since $Y=\left\{  i\in\left\{  1,2,\ldots,n\right\}
\ \mid\ \sigma\left(  i\right)  <i\right\}  $). Qed.}. Thus, we can define a
map $\beta:Z\rightarrow Y$ by
\[
\left(  \beta\left(  j\right)  =\sigma\left(  j\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }j\in Z\right)  .
\]
Consider this map $\beta$.

We have $\alpha\circ\beta=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $j\in Z$. The definition of $\beta$ yields $\beta\left(  j\right)
=\sigma\left(  j\right)  $. Comparing this with $\underbrace{\sigma^{-1}%
}_{=\sigma}\left(  j\right)  =\sigma\left(  j\right)  $, we obtain
$\beta\left(  j\right)  =\sigma^{-1}\left(  j\right)  $. Now,%
\begin{align*}
\left(  \alpha\circ\beta\right)  \left(  j\right)   &  =\alpha\left(
\beta\left(  j\right)  \right)  =\sigma\left(  \underbrace{\beta\left(
j\right)  }_{=\sigma^{-1}\left(  j\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\alpha\right) \\
&  =\sigma\left(  \sigma^{-1}\left(  j\right)  \right)  =j=\operatorname*{id}%
\left(  j\right)  .
\end{align*}
\par
Now, forget that we fixed $j$. We thus have shown that $\left(  \alpha
\circ\beta\right)  \left(  j\right)  =\operatorname*{id}\left(  j\right)  $
for each $j\in Z$. In other words, $\alpha\circ\beta=\operatorname*{id}$.
Qed.} and $\beta\circ\alpha=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} Let $j\in Y$. The definition of $\alpha$
yields $\alpha\left(  j\right)  =\sigma\left(  j\right)  $. Comparing this
with $\underbrace{\sigma^{-1}}_{=\sigma}\left(  j\right)  =\sigma\left(
j\right)  $, we obtain $\alpha\left(  j\right)  =\sigma^{-1}\left(  j\right)
$. Now,%
\begin{align*}
\left(  \beta\circ\alpha\right)  \left(  j\right)   &  =\beta\left(
\alpha\left(  j\right)  \right)  =\sigma\left(  \underbrace{\alpha\left(
j\right)  }_{=\sigma^{-1}\left(  j\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\beta\right) \\
&  =\sigma\left(  \sigma^{-1}\left(  j\right)  \right)  =j=\operatorname*{id}%
\left(  j\right)  .
\end{align*}
\par
Now, forget that we fixed $j$. We thus have shown that $\left(  \beta
\circ\alpha\right)  \left(  j\right)  =\operatorname*{id}\left(  j\right)  $
for each $j\in Y$. In other words, $\beta\circ\alpha=\operatorname*{id}$.
Qed.}. Thus, the two maps $\alpha$ and $\beta$ are mutually inverse. Hence,
the map $\alpha$ is invertible, i.e., is a bijection. Thus, there exists a
bijection $Y\rightarrow Z$ (namely, the map $\alpha$). Hence, $\left\vert
Y\right\vert =\left\vert Z\right\vert $.

Now,%
\[
\sum_{i\in\left\{  1,2,\ldots,n\right\}  }1=\underbrace{\left\vert \left\{
1,2,\ldots,n\right\}  \right\vert }_{=n}\cdot1=n\cdot1=n.
\]
Hence,%
\begin{align*}
n  &  =\sum_{i\in\left\{  1,2,\ldots,n\right\}  }1=\underbrace{\sum
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\\sigma\left(  i\right)
<i}}}_{\substack{=\sum_{i\in Y}\\\text{(by (\ref{pf.lem.sol.altern.det.mod2.Y}%
))}}}1+\underbrace{\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\\sigma\left(  i\right)  =i}}}_{\substack{=\sum_{i\in X}\\\text{(by
(\ref{pf.lem.sol.altern.det.mod2.X}))}}}1+\underbrace{\sum_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\\sigma\left(  i\right)  >i}}}%
_{\substack{=\sum_{i\in Z}\\\text{(by (\ref{pf.lem.sol.altern.det.mod2.Z}))}%
}}1\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfies exactly
one of the three}\\
\text{statements }\left(  \sigma\left(  i\right)  <i\right)  \text{, }\left(
\sigma\left(  i\right)  =i\right)  \text{ and }\left(  \sigma\left(  i\right)
>i\right)
\end{array}
\right) \\
&  =\underbrace{\sum_{i\in Y}1}_{=\left\vert Y\right\vert \cdot1=\left\vert
Y\right\vert }+\underbrace{\sum_{i\in X}1}_{=\left\vert X\right\vert
\cdot1=\left\vert X\right\vert }+\underbrace{\sum_{i\in Z}1}_{=\left\vert
Z\right\vert \cdot1=\left\vert Z\right\vert }=\underbrace{\left\vert
Y\right\vert }_{=\left\vert Z\right\vert }+\left\vert X\right\vert +\left\vert
Z\right\vert \\
&  =\left\vert Z\right\vert +\left\vert X\right\vert +\left\vert Z\right\vert
=\left\vert X\right\vert +2\cdot\left\vert Z\right\vert \equiv\left\vert
X\right\vert \operatorname{mod}2.
\end{align*}
This proves Lemma \ref{lem.sol.altern.det.mod2}.
\end{proof}
\end{verlong}

\begin{corollary}
\label{cor.sol.altern.det.sigma0}Let $n\in\mathbb{N}$ be odd. Let $\sigma\in
S_{n}$ be such that $\sigma^{-1}=\sigma$. Let $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$ be an alternating $n\times n$-matrix. Then,%
\[
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0.
\]

\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.sol.altern.det.sigma0}.]Let $X=\left\{
i\in\left\{  1,2,\ldots,n\right\}  \ \mid\ \sigma\left(  i\right)  =i\right\}
$. Then, Lemma \ref{lem.sol.altern.det.mod2} yields $\left\vert X\right\vert
\equiv n\equiv1\operatorname{mod}2$ (since $n$ is odd). If we had
$X=\varnothing$, then we would have $\left\vert \underbrace{X}_{=\varnothing
}\right\vert =\left\vert \varnothing\right\vert =0\not \equiv
1\operatorname{mod}2$; this would contradict $\left\vert X\right\vert
\equiv1\operatorname{mod}2$. Hence, we cannot have $X=\varnothing$. Thus, we
have $X\neq\varnothing$. In other words, the set $X$ is nonempty. Hence, there
exists some $x\in X$. Consider this $x$.

We have $x\in X=\left\{  i\in\left\{  1,2,\ldots,n\right\}  \ \mid
\ \sigma\left(  i\right)  =i\right\}  $. In other words, $x$ is an element $i$
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\sigma\left(  i\right)  =i$.
In other words, $x$ is an element of $\left\{  1,2,\ldots,n\right\}  $ and
satisfies $\sigma\left(  x\right)  =x$. Now, Lemma \ref{lem.sol.altern.STAS.3}
\textbf{(a)} (applied to $i=x$) yields $a_{x,x}=0$. Since $\sigma\left(
x\right)  =x$, we have $a_{x,\sigma\left(  x\right)  }=a_{x,x}=0$.

But $x\in\left\{  1,2,\ldots,n\right\}  $. Hence, $a_{x,\sigma\left(
x\right)  }$ is a factor of the product $\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }$ (namely, the factor for $i=x$). This factor is $0$ (since
$a_{x,\sigma\left(  x\right)  }=0$). Hence, one factor of the product
$\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ is $0$. Thus, the whole
product $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ must be $0$ (because
if one factor of a product is $0$, then the whole product must be $0$). In
other words, $\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0$. This proves
Corollary \ref{cor.sol.altern.det.sigma0}.
\end{proof}

\begin{lemma}
\label{lem.sol.altern.det.sigma-1}Let $n\in\mathbb{N}$ be odd. Let $\sigma\in
S_{n}$. Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an
alternating $n\times n$-matrix. Then,%
\[
\prod_{i=1}^{n}a_{i,\sigma^{-1}\left(  i\right)  }=-\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.det.sigma-1}.]We have $\sigma\in S_{n}$.
In other words, $\sigma$ is a permutation of the set $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of the set
$\left\{  1,2,\ldots,n\right\}  $). In other words, $\sigma$ is a bijection
$\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $.

Now,
\begin{align*}
\prod_{i=1}^{n}a_{i,\sigma^{-1}\left(  i\right)  }  &  =\prod_{i=1}%
^{n}\underbrace{a_{\sigma\left(  i\right)  ,\sigma^{-1}\left(  \sigma\left(
i\right)  \right)  }}_{\substack{=a_{\sigma\left(  i\right)  ,i}\\\text{(since
}\sigma^{-1}\left(  \sigma\left(  i\right)  \right)  =i\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma\left(  i\right)  \text{ for }i\text{
in the product,}\\
\text{since }\sigma:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n}\underbrace{a_{\sigma\left(  i\right)  ,i}}%
_{\substack{=-a_{i,\sigma\left(  i\right)  }\\\text{(by Lemma
\ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied to }\left(
\sigma\left(  i\right)  ,i\right)  \text{ instead of }\left(  i,j\right)
\text{))}}}=\prod_{i=1}^{n}\left(  -a_{i,\sigma\left(  i\right)  }\right) \\
&  =\underbrace{\left(  -1\right)  ^{n}}_{\substack{=-1\\\text{(since }n\text{
is odd)}}}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=-\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }.
\end{align*}
This proves Lemma \ref{lem.sol.altern.det.sigma-1}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.altern.det.sigma-1}.]We have $\sigma\in S_{n}$.
In other words, $\sigma$ is a permutation of the set $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of the set
$\left\{  1,2,\ldots,n\right\}  $). In other words, $\sigma$ is a bijection
$\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $.

We have%
\begin{equation}
a_{\sigma\left(  i\right)  ,i}=-a_{i,\sigma\left(  i\right)  }%
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n\right\}
\label{pf.lem.sol.altern.det.sigma-1.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.altern.det.sigma-1.1}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\sigma\left(  i\right)
\in\left\{  1,2,\ldots,n\right\}  $. Hence, $\left(  \sigma\left(  i\right)
,i\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$ (since $\sigma\left(
i\right)  \in\left\{  1,2,\ldots,n\right\}  $ and $i\in\left\{  1,2,\ldots
,n\right\}  $). Thus, Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)} (applied
to $\left(  \sigma\left(  i\right)  ,i\right)  $ instead of $\left(
i,j\right)  $) yields $a_{\sigma\left(  i\right)  ,i}=-a_{i,\sigma\left(
i\right)  }$. This proves (\ref{pf.lem.sol.altern.det.sigma-1.1}).}.

Now,%
\begin{align*}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma^{-1}\left(  i\right)  }  &  =\prod_{i\in\left\{  1,2,\ldots
,n\right\}  }a_{i,\sigma^{-1}\left(  i\right)  }=\prod_{i\in\left\{
1,2,\ldots,n\right\}  }\underbrace{a_{\sigma\left(  i\right)  ,\sigma
^{-1}\left(  \sigma\left(  i\right)  \right)  }}_{\substack{=a_{\sigma\left(
i\right)  ,i}\\\text{(since }\sigma^{-1}\left(  \sigma\left(  i\right)
\right)  =i\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma\left(  i\right)  \text{ for }i\text{
in the product,}\\
\text{since }\sigma:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\prod_{i\in\left\{  1,2,\ldots,n\right\}  }\underbrace{a_{\sigma\left(
i\right)  ,i}}_{\substack{=-a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.sol.altern.det.sigma-1.1}))}}}=\underbrace{\prod_{i\in\left\{
1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}}\underbrace{\left(  -a_{i,\sigma
\left(  i\right)  }\right)  }_{=\left(  -1\right)  a_{i,\sigma\left(
i\right)  }}\\
&  =\prod_{i=1}^{n}\left(  \left(  -1\right)  a_{i,\sigma\left(  i\right)
}\right)  =\underbrace{\left(  -1\right)  ^{n}}_{\substack{=-1\\\text{(since
}n\text{ is odd)}}}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\left(
-1\right)  \prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =-\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }.
\end{align*}
This proves Lemma \ref{lem.sol.altern.det.sigma-1}.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Exercise \ref{exe.altern.det}.]\textbf{(a)} Assume that $A$ is antisymmetric.

Recall that the matrix $A$ is antisymmetric if and only if $A^{T}=-A$ (by the
definition of \textquotedblleft antisymmetric\textquotedblright). Hence,
$A^{T}=-A$ (since the matrix $A$ is antisymmetric).

Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Hence,%
\begin{align*}
\det A  &  =\det\left(  \underbrace{A^{T}}_{=-A=\left(  -1\right)  A}\right)
=\det\left(  \left(  -1\right)  A\right)  =\underbrace{\left(  -1\right)
^{n}}_{\substack{=-1\\\text{(since }n\text{ is odd)}}}\det A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.det.scale}
(applied to }\lambda=-1\text{)}\right) \\
&  =\left(  -1\right)  \det A=-\det A.
\end{align*}
Adding $\det A$ to both sides of this equality, we obtain $\det A+\det A=0$.
Thus, $0=\det A+\det A=2\det A$. This solves Exercise \ref{exe.altern.det}
\textbf{(a)}.

\textbf{(b)} Assume that $A$ is alternating.

Write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

The set $S_{n}$ is finite. Hence, there exists a bijection $\beta
:S_{n}\rightarrow\left\{  1,2,\ldots,\left\vert S_{n}\right\vert \right\}  $.
Consider this bijection $\beta$. (Of course, $\left\vert S_{n}\right\vert
=n!$; but we will not use this.)

Recall that%
\begin{equation}
\left(  -1\right)  ^{\sigma^{-1}}=\left(  -1\right)  ^{\sigma}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in S_{n}.
\label{sol.altern.det.b.(-1)sigma}%
\end{equation}


\begin{vershort}
The map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma^{-1}$ is a bijection
(indeed, it is its own inverse).
\end{vershort}

\begin{verlong}
Define a map $\mathbf{j}:S_{n}\rightarrow S_{n}$ by $\left(  \mathbf{j}\left(
\sigma\right)  =\sigma^{-1}\text{ for every }\sigma\in S_{n}\right)  $. Then,
$\mathbf{j}$ is the map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma^{-1}$.

We have $\mathbf{j}\circ\mathbf{j}=\operatorname*{id}$%
\ \ \ \ \footnote{\textit{Proof.} Every $\sigma\in S_{n}$ satisfies%
\begin{align*}
\left(  \mathbf{j}\circ\mathbf{j}\right)  \left(  \sigma\right)   &
=\mathbf{j}\left(  \underbrace{\mathbf{j}\left(  \sigma\right)  }%
_{\substack{=\sigma^{-1}\\\text{(by the definition of }\mathbf{j}\text{)}%
}}\right)  =\mathbf{j}\left(  \sigma^{-1}\right)  =\left(  \sigma^{-1}\right)
^{-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{j}\right)
\\
&  =\sigma=\operatorname*{id}\left(  \sigma\right)  .
\end{align*}
In other words, $\mathbf{j}\circ\mathbf{j}=\operatorname*{id}$. Qed.}. The two
maps $\mathbf{j}$ and $\mathbf{j}$ are mutually inverse (since $\mathbf{j}%
\circ\mathbf{j}=\operatorname*{id}$ and $\mathbf{j}\circ\mathbf{j}%
=\operatorname*{id}$). Hence, the map $\mathbf{j}$ is invertible. In other
words, the map $\mathbf{j}$ is a bijection. In other words, the map
$S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma^{-1}$ is a bijection (since the
map $\mathbf{j}$ is the map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma
^{-1}$).
\end{verlong}

Now,%
\begin{align}
&  \sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)  <\beta\left(
\sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }\nonumber\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma
^{-1}\right)  <\beta\left(  \left(  \sigma^{-1}\right)  ^{-1}\right)  }%
}}_{\substack{=\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma
^{-1}\right)  <\beta\left(  \sigma\right)  }}\\\text{(since }\left(
\sigma^{-1}\right)  ^{-1}=\sigma\text{)}}}\underbrace{\left(  -1\right)
^{\sigma^{-1}}}_{\substack{=\left(  -1\right)  ^{\sigma}\\\text{(by
(\ref{sol.altern.det.b.(-1)sigma}))}}}\underbrace{\prod_{i=1}^{n}%
a_{i,\sigma^{-1}\left(  i\right)  }}_{\substack{=-\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }\\\text{(by Lemma
\ref{lem.sol.altern.det.sigma-1})}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma^{-1}\text{ for }\sigma\text{ in the
sum,}\\
\text{since the map }S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma^{-1}\text{
is a bijection}%
\end{array}
\right) \nonumber\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma
^{-1}\right)  <\beta\left(  \sigma\right)  }}}_{\substack{=\sum
_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)  >\beta\left(
\sigma^{-1}\right)  }}\\\text{(since for every }\sigma\in S_{n}\text{,
the}\\\text{statement }\left(  \beta\left(  \sigma^{-1}\right)  <\beta\left(
\sigma\right)  \right)  \text{ is}\\\text{equivalent to }\left(  \beta\left(
\sigma\right)  >\beta\left(  \sigma^{-1}\right)  \right)  \text{)}}}\left(
-1\right)  ^{\sigma}\left(  -\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}\right)  =\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)
>\beta\left(  \sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\left(
-\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right) \nonumber\\
&  =-\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)
>\beta\left(  \sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }. \label{sol.altern.det.b.1}%
\end{align}


Also, every $\sigma\in S_{n}$ satisfying $\beta\left(  \sigma\right)
=\beta\left(  \sigma^{-1}\right)  $ must satisfy
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0 \label{sol.altern.det.b.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.altern.det.b.2}):} Let $\sigma\in S_{n}$
be such that $\beta\left(  \sigma\right)  =\beta\left(  \sigma^{-1}\right)  $.
\par
Recall that $\beta$ is a bijection. Hence, a map $\beta^{-1}$ is well-defined.
Now, $\beta^{-1}\left(  \underbrace{\beta\left(  \sigma\right)  }%
_{=\beta\left(  \sigma^{-1}\right)  }\right)  =\beta^{-1}\left(  \beta\left(
\sigma^{-1}\right)  \right)  =\sigma^{-1}$, so that $\sigma^{-1}=\beta
^{-1}\left(  \beta\left(  \sigma\right)  \right)  =\sigma$. Thus, Corollary
\ref{cor.sol.altern.det.sigma0} yields $\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }=0$. This proves (\ref{sol.altern.det.b.2}).}.

Now, recall that $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Hence, the definition of $\det A$ yields%
\begin{align*}
&  \det A\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)
<\beta\left(  \sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=-\sum_{\substack{\sigma
\in S_{n};\\\beta\left(  \sigma\right)  >\beta\left(  \sigma^{-1}\right)
}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}\\\text{(by (\ref{sol.altern.det.b.1}))}}}+\sum_{\substack{\sigma\in
S_{n};\\\beta\left(  \sigma\right)  =\beta\left(  \sigma^{-1}\right)
}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(
i\right)  }}_{\substack{=0\\\text{(by (\ref{sol.altern.det.b.2}))}}%
}+\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)  >\beta\left(
\sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}%
a_{i,\sigma\left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }\sigma\in S_{n}\text{ satisfies exactly one of the three}\\
\text{statements }\left(  \beta\left(  \sigma\right)  <\beta\left(
\sigma^{-1}\right)  \right)  \text{, }\left(  \beta\left(  \sigma\right)
=\beta\left(  \sigma^{-1}\right)  \right)  \text{ and }\left(  \beta\left(
\sigma\right)  >\beta\left(  \sigma^{-1}\right)  \right)
\end{array}
\right) \\
&  =-\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)
>\beta\left(  \sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\beta\left(  \sigma\right)  =\beta\left(  \sigma^{-1}\right)
}}\left(  -1\right)  ^{\sigma}0}_{=0}+\sum_{\substack{\sigma\in S_{n}%
;\\\beta\left(  \sigma\right)  >\beta\left(  \sigma^{-1}\right)  }}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =-\sum_{\substack{\sigma\in S_{n};\\\beta\left(  \sigma\right)
>\beta\left(  \sigma^{-1}\right)  }}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\sum_{\substack{\sigma\in
S_{n};\\\beta\left(  \sigma\right)  >\beta\left(  \sigma^{-1}\right)
}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0.
\end{align*}
This solves Exercise \ref{exe.altern.det} \textbf{(b)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.tridiag.isl}}

\begin{proof}
[Solution to Exercise \ref{exe.tridiag.isl}.]Define $n-1$ elements
$c_{1},c_{2},\ldots,c_{n-1}$ of $\mathbb{K}$ by%
\[
\left(  c_{i}=1\text{ for every }i\in\left\{  1,2,\ldots,n-1\right\}  \right)
.
\]
Define an $n\times n$-matrix $A$ as in Definition \ref{def.tridiag}. For every
two elements $x$ and $y$ of $\left\{  0,1,\ldots,n\right\}  $ satisfying
$x\leq y$, we define a $\left(  y-x\right)  \times\left(  y-x\right)  $-matrix
$A_{x,y}$ as in Proposition \ref{prop.tridiag.rec}.

Now, we claim that%
\begin{equation}
u_{i}=\det\left(  A_{0,i}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  0,1,\ldots,n\right\}  . \label{sol.tridiag.isl.u}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.isl.u}):} We shall prove
(\ref{sol.tridiag.isl.u}) by strong induction over $i$. Thus, we fix some
$k\in\left\{  0,1,\ldots,n\right\}  $. We assume that (\ref{sol.tridiag.isl.u}%
) holds for all $i<k$. We now must show that (\ref{sol.tridiag.isl.u}) holds
for $i=k$. In other words, we must show that $u_{k}=\det\left(  A_{0,k}%
\right)  $.

This holds if $k\leq1$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=0$) yields $\det\left(
A_{0,0}\right)  =1$. Compared with $u_{0}=1$, this yields $u_{0}=\det\left(
A_{0,0}\right)  $.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=0$) yields
$\det\left(  A_{0,1}\right)  =a_{1}$. Compared with $u_{1}=a_{1}$, this yields
$u_{1}=\det\left(  A_{0,1}\right)  $.
\par
Now, $u_{k}=\det\left(  A_{0,k}\right)  $ holds if $k=0$ (because we have
$u_{0}=\det\left(  A_{0,0}\right)  $), and also holds if $k=1$ (since
$u_{1}=\det\left(  A_{0,1}\right)  $). Therefore, $u_{k}=\det\left(
A_{0,k}\right)  $ holds if $k\leq1$. Qed.}. Hence, we can WLOG assume that we
don't have $k\leq1$. Assume this.

We have $k>1$ (since we don't have $k\leq1$). Thus, $k\geq2$. Hence, $k-1$ and
$k-2$ are nonnegative integers satisfying $k-1<k$ and $k-2<k$. Hence, we can
apply (\ref{sol.tridiag.isl.u}) to $i=k-1$ (since we have assumed that
(\ref{sol.tridiag.isl.u}) holds for all $i<k$). As a result, we obtain
$u_{k-1}=\det\left(  A_{0,k-1}\right)  $. Also, we can apply
(\ref{sol.tridiag.isl.u}) to $i=k-2$ (since we have assumed that
(\ref{sol.tridiag.isl.u}) holds for all $i<k$). As a result, we obtain
$u_{k-2}=\det\left(  A_{0,k-2}\right)  $.

We have $c_{k-1}=1$ (by the definition of $c_{k-1}$).

Now, $0\leq k-2$ (since $k\geq2$). Hence, Proposition \ref{prop.tridiag.rec}
\textbf{(c)} (applied to $x=0$ and $y=k$) yields%
\begin{align*}
\det\left(  A_{0,k}\right)   &  =a_{k}\det\left(  A_{0,k-1}\right)
-b_{k-1}\underbrace{c_{k-1}}_{=1}\det\left(  A_{0,k-2}\right) \\
&  =a_{k}\det\left(  A_{0,k-1}\right)  -b_{k-1}\det\left(  A_{0,k-2}\right)  .
\end{align*}
Comparing this with%
\begin{align*}
u_{k}  &  =a_{k}\underbrace{u_{k-1}}_{=\det\left(  A_{0,k-1}\right)  }%
-b_{k-1}\underbrace{u_{k-2}}_{=\det\left(  A_{0,k-2}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursive definition of }\left(
u_{0},u_{1},\ldots,u_{n}\right)  \right) \\
&  =a_{k}\det\left(  A_{0,k-1}\right)  -b_{k-1}\det\left(  A_{0,k-2}\right)  ,
\end{align*}
we obtain $u_{k}=\det\left(  A_{0,k}\right)  $. In other words,
(\ref{sol.tridiag.isl.u}) holds for $i=k$. This completes the induction step.
Thus, (\ref{sol.tridiag.isl.u}) is proven.

Next, we claim that%
\begin{equation}
v_{i}=\det\left(  A_{n-i,n}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  0,1,\ldots,n\right\}  . \label{sol.tridiag.isl.v}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.isl.v}):} We shall prove
(\ref{sol.tridiag.isl.v}) by strong induction over $i$. Thus, we fix some
$k\in\left\{  0,1,\ldots,n\right\}  $. We assume that (\ref{sol.tridiag.isl.v}%
) holds for all $i<k$. We now must show that (\ref{sol.tridiag.isl.v}) holds
for $i=k$. In other words, we must show that $v_{k}=\det\left(  A_{n-k,n}%
\right)  $.

This holds if $k\leq1$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=n$) yields $\det\left(
A_{n,n}\right)  =1$. Thus, $\det\left(  A_{n-0,n}\right)  =\det\left(
A_{n,n}\right)  =1$. Compared with $v_{0}=1$, this yields $v_{0}=\det\left(
A_{n-0,n}\right)  $.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=n-1$) yields
$\det\left(  A_{n-1,\left(  n-1\right)  +1}\right)  =a_{\left(  n-1\right)
+1}$. This rewrites as $\det\left(  A_{n-1,n}\right)  =a_{n}$ (since $\left(
n-1\right)  +1=n$). Compared with $v_{1}=a_{n}$, this yields $v_{1}%
=\det\left(  A_{n-1,n}\right)  $.
\par
Now, $v_{k}=\det\left(  A_{n-k,n}\right)  $ holds if $k=0$ (because we have
$v_{0}=\det\left(  A_{n-0,n}\right)  $), and also holds if $k=1$ (since
$v_{1}=\det\left(  A_{n-1,n}\right)  $). Therefore, $v_{k}=\det\left(
A_{n-k,n}\right)  $ holds if $k\leq1$. Qed.}. Hence, we can WLOG assume that
we don't have $k\leq1$. Assume this.

We have $k>1$ (since we don't have $k\leq1$). Thus, $k\geq2$. Hence, $k-1$ and
$k-2$ are nonnegative integers satisfying $k-1<k$ and $k-2<k$. Hence, we can
apply (\ref{sol.tridiag.isl.v}) to $i=k-1$ (since we have assumed that
(\ref{sol.tridiag.isl.v}) holds for all $i<k$). As a result, we obtain
$v_{k-1}=\det\left(  \underbrace{A_{n-\left(  k-1\right)  ,n}}_{=A_{n-k+1,n}%
}\right)  =\det\left(  A_{n-k+1,n}\right)  $. Also, we can apply
(\ref{sol.tridiag.isl.v}) to $i=k-2$ (since we have assumed that
(\ref{sol.tridiag.isl.v}) holds for all $i<k$). As a result, we obtain
$v_{k-2}=\det\left(  \underbrace{A_{n-\left(  k-2\right)  ,n}}_{=A_{n-k+2,n}%
}\right)  =\det\left(  A_{n-k+2,n}\right)  $.

We have $c_{n-k+1}=1$ (by the definition of $c_{n-k+1}$).

Now, $n-k\leq n-2$ (since $k\geq2$). Hence, Proposition \ref{prop.tridiag.rec}
\textbf{(d)} (applied to $x=n-k$ and $y=n$) yields%
\begin{align*}
\det\left(  A_{n-k,n}\right)   &  =a_{n-k+1}\det\left(  A_{n-k+1,n}\right)
-b_{n-k+1}\underbrace{c_{n-k+1}}_{=1}\det\left(  A_{n-k+2,n}\right) \\
&  =a_{n-k+1}\det\left(  A_{n-k+1,n}\right)  -b_{n-k+1}\det\left(
A_{n-k+2,n}\right)  .
\end{align*}
Comparing this with%
\begin{align*}
v_{k}  &  =a_{n-k+1}\underbrace{v_{k-1}}_{=\det\left(  A_{n-k+1,n}\right)
}-b_{n-k+1}\underbrace{v_{k-2}}_{=\det\left(  A_{n-k+2,n}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursive definition of }\left(
v_{0},v_{1},\ldots,v_{n}\right)  \right) \\
&  =a_{n-k+1}\det\left(  A_{n-k+1,n}\right)  -b_{n-k+1}\det\left(
A_{n-k+2,n}\right)  ,
\end{align*}
we obtain $v_{k}=\det\left(  A_{n-k,n}\right)  $. In other words,
(\ref{sol.tridiag.isl.v}) holds for $i=k$. This completes the induction step.
Thus, (\ref{sol.tridiag.isl.v}) is proven.

Now, we are almost done. In fact, applying (\ref{sol.tridiag.isl.u}) to $i=n$,
we obtain $u_{n}=\det\left(  A_{0,n}\right)  $. On the other hand, applying
(\ref{sol.tridiag.isl.v}) to $i=n$, we obtain $v_{n}=\det\left(
A_{n-n,n}\right)  =\det\left(  A_{0,n}\right)  $ (since $n-n=0$). Comparing
this with $u_{n}=\det\left(  A_{0,n}\right)  $, we obtain $u_{n}=v_{n}$.
Exercise \ref{exe.tridiag.isl} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.tridiag.cf}}

\begin{proof}
[Solution to Exercise \ref{exe.tridiag.cf}.]Assume that all denominators
appearing in Exercise \ref{exe.tridiag.cf} are invertible. For every
$k\in\left\{  1,2,\ldots,n\right\}  $, define an element $p_{k}$ of
$\mathbb{K}$ by%
\[
p_{k}=a_{k}-\dfrac{b_{k}c_{k}}{a_{k+1}-\dfrac{b_{k+1}c_{k+1}}{a_{k+2}%
-\dfrac{b_{k+2}c_{k+2}}{%
\begin{array}
[c]{ccc}%
a_{k+3}- &  & \\
& \ddots & \\
&  & -\dfrac{b_{n-2}c_{n-2}}{a_{n-1}-\dfrac{b_{n-1}c_{n-1}}{a_{n}}}%
\end{array}
}}}%
\]
\footnote{If $k=n$, then this should be interpreted as saying that
$p_{n}=a_{n}$.}. This definition of $p_{k}$ immediately gives a recursion:

\begin{itemize}
\item We have $p_{n}=a_{n}$.

\item For every $k\in\left\{  1,2,\ldots,n-1\right\}  $, we have%
\begin{equation}
p_{k}=a_{k}-\dfrac{b_{k}c_{k}}{p_{k+1}}. \label{sol.tridiag.cf.1}%
\end{equation}

\end{itemize}

Now, we shall show that
\begin{equation}
\det\left(  A_{n-k-1,n}\right)  =p_{n-k}\det\left(  A_{n-k,n}\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  0,1,\ldots,n-1\right\}  .
\label{sol.tridiag.cf.main}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.cf.main}):} We shall prove
(\ref{sol.tridiag.cf.main}) by induction over $k$:

\textit{Induction base:} It is easy to see that (\ref{sol.tridiag.cf.main})
holds for $k=0$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=n$) yields $\det\left(
A_{n,n}\right)  =1$. Hence, $\underbrace{p_{n-0}}_{=p_{n}=a_{n}}\det\left(
\underbrace{A_{n-0,n}}_{=A_{n,n}}\right)  =a_{n}$.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=n-1$) yields
$\det\left(  A_{n-1,\left(  n-1\right)  +1}\right)  =a_{\left(  n-1\right)
+1}$. This rewrites as $\det\left(  A_{n-1,n}\right)  =a_{n}$ (since $\left(
n-1\right)  +1=n$). Thus, $\det\left(  \underbrace{A_{n-0-1,n}}_{=A_{n-1,n}%
}\right)  =\det\left(  A_{n-1,n}\right)  =a_{n}$. Comparing this with
$\underbrace{p_{n-0}}_{=p_{n}=a_{n}}\det\left(  \underbrace{A_{n-0,n}%
}_{=A_{n,n}}\right)  =a_{n}\underbrace{\det\left(  A_{n,n}\right)  }%
_{=1}=a_{n}$, we obtain $\det\left(  A_{n-0-1,n}\right)  =p_{n-0}\det\left(
A_{n-0,n}\right)  $. In other words, (\ref{sol.tridiag.cf.main}) holds for
$k=0$. Qed.}. This completes the induction base.

\textit{Induction step:} Let $K\in\left\{  0,1,\ldots,n-1\right\}  $ be
positive. Assume that (\ref{sol.tridiag.cf.main}) holds for $k=K-1$. We shall
show that (\ref{sol.tridiag.cf.main}) holds for $k=K$.

We have assumed that (\ref{sol.tridiag.cf.main}) holds for $k=K-1$. In other
words, we have $\det\left(  A_{n-\left(  K-1\right)  -1,n}\right)
=p_{n-\left(  K-1\right)  }\det\left(  A_{n-\left(  K-1\right)  ,n}\right)  $.
Since $n-\left(  K-1\right)  =n-K+1$, this rewrites as $\det\left(
A_{n-K+1-1,n}\right)  =p_{n-K+1}\det\left(  A_{n-K+1,n}\right)  $. Solving
this for $\det\left(  A_{n-K+1,n}\right)  $, we obtain%
\begin{align}
\det\left(  A_{n-K+1,n}\right)   &  =\dfrac{\det\left(  A_{n-K+1-1,n}\right)
}{p_{n-K+1}}=\dfrac{1}{p_{n-K+1}}\det\left(  \underbrace{A_{n-K+1-1,n}%
}_{=A_{n-K,n}}\right) \nonumber\\
&  =\dfrac{1}{p_{n-K+1}}\det\left(  A_{n-K,n}\right)  .
\label{sol.tridiag.cf.main.pf.2}%
\end{align}


We have $K\in\left\{  1,2,\ldots,n-1\right\}  $ (since $K$ is positive and
belongs to $\left\{  0,1,\ldots,n-1\right\}  $). Hence, applying
(\ref{sol.tridiag.cf.1}) to $k=n-K$, we obtain%
\begin{equation}
p_{n-K}=a_{n-K}-\dfrac{b_{n-K}c_{n-K}}{p_{n-K+1}}.
\label{sol.tridiag.cf.main.pf.4}%
\end{equation}


We have $K\leq n-1$ and thus $n-K-1\geq0$. Moreover, $K\geq1$ (since $K$ is
positive), thus $n-\underbrace{K}_{\geq1}-1\leq n-1-1=n-2$. Hence, Proposition
\ref{prop.tridiag.rec} \textbf{(d)} (applied to $x=n-K-1$ and $y=n$) shows
that%
\begin{align*}
&  \det\left(  A_{n-K-1,n}\right) \\
&  =\underbrace{a_{\left(  n-K-1\right)  +1}}_{=a_{n-K}}\det\left(
\underbrace{A_{\left(  n-K-1\right)  +1,n}}_{=A_{n-K,n}}\right)
-\underbrace{b_{\left(  n-K-1\right)  +1}}_{=b_{n-K}}\underbrace{c_{\left(
n-K-1\right)  +1}}_{=c_{n-K}}\det\left(  \underbrace{A_{\left(  n-K-1\right)
+2,n}}_{=A_{n-K+1,n}}\right) \\
&  =a_{n-K}\det\left(  A_{n-K,n}\right)  -b_{n-K}c_{n-K}\underbrace{\det
\left(  A_{n-K+1,n}\right)  }_{\substack{=\dfrac{1}{p_{n-K+1}}\det\left(
A_{n-K,n}\right)  \\\text{(by (\ref{sol.tridiag.cf.main.pf.2}))}}}\\
&  =a_{n-K}\det\left(  A_{n-K,n}\right)  -b_{n-K}c_{n-K}\cdot\dfrac
{1}{p_{n-K+1}}\det\left(  A_{n-K,n}\right) \\
&  =\underbrace{\left(  a_{n-K}-b_{n-K}c_{n-K}\cdot\dfrac{1}{p_{n-K+1}%
}\right)  }_{\substack{=a_{n-K}-\dfrac{b_{n-K}c_{n-K}}{p_{n-K+1}}%
=p_{n-K}\\\text{(by (\ref{sol.tridiag.cf.main.pf.4}))}}}\det\left(
A_{n-K,n}\right)  =p_{n-K}\det\left(  A_{n-K,n}\right)  .
\end{align*}
In other words, (\ref{sol.tridiag.cf.main}) holds for $k=K$. This completes
the induction step. Thus, (\ref{sol.tridiag.cf.main}) is proven by induction.

Now, we can apply (\ref{sol.tridiag.cf.main}) to $k=n-1$. This gives us%
\[
\det\left(  A_{n-\left(  n-1\right)  -1,n}\right)  =\underbrace{p_{n-\left(
n-1\right)  }}_{=p_{1}}\det\left(  \underbrace{A_{n-\left(  n-1\right)  ,n}%
}_{=A_{1,n}}\right)  =p_{1}\det\left(  A_{1,n}\right)  .
\]
Since $n-\left(  n-1\right)  -1=0$, this rewrites as $\det\left(
A_{0,n}\right)  =p_{1}\det\left(  A_{1,n}\right)  $. Since $A_{0,n}=A$ (by
Proposition \ref{prop.tridiag.rec} \textbf{(e)}), this simplifies to $\det
A=p_{1}\det\left(  A_{1,n}\right)  $. Hence,%
\[
\dfrac{\det A}{\det\left(  A_{1,n}\right)  }=p_{1}=a_{1}-\dfrac{b_{1}c_{1}%
}{a_{2}-\dfrac{b_{2}c_{2}}{a_{3}-\dfrac{b_{3}c_{3}}{%
\begin{array}
[c]{ccc}%
a_{4}- &  & \\
& \ddots & \\
&  & -\dfrac{b_{n-2}c_{n-2}}{a_{n-1}-\dfrac{b_{n-1}c_{n-1}}{a_{n}}}%
\end{array}
}}}%
\]
(by the definition of $p_{1}$).
\end{proof}

\subsection{Solution to Exercise \ref{exe.tridiag.fib}}

\begin{proof}
[First solution to Exercise \ref{exe.tridiag.fib}.]We claim that%
\begin{equation}
\det\left(  A_{0,i}\right)  =f_{i+1}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  0,1,\ldots,n\right\}  . \label{sol.tridiag.fib.main}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.fib.main}):} We shall prove
(\ref{sol.tridiag.fib.main}) by strong induction over $i$. Thus, we fix some
$k\in\left\{  0,1,\ldots,n\right\}  $. We assume that
(\ref{sol.tridiag.fib.main}) holds for all $i<k$. We now must show that
(\ref{sol.tridiag.fib.main}) holds for $i=k$. In other words, we must show
that $\det\left(  A_{0,k}\right)  =f_{k+1}$.

This holds if $k\leq1$\ \ \ \ \footnote{\textit{Proof.} Proposition
\ref{prop.tridiag.rec} \textbf{(a)} (applied to $x=0$) yields $\det\left(
A_{0,0}\right)  =1$. Compared with $f_{0+1}=f_{1}=1$, this yields $\det\left(
A_{0,0}\right)  =f_{0+1}$.
\par
Proposition \ref{prop.tridiag.rec} \textbf{(b)} (applied to $x=0$) yields
$\det\left(  A_{0,1}\right)  =a_{1}=1$. Compared with $f_{1+1}=f_{2}=1$, this
yields $\det\left(  A_{0,1}\right)  =f_{1+1}$.
\par
Now, $\det\left(  A_{0,k}\right)  =f_{k+1}$ holds if $k=0$ (because we have
$\det\left(  A_{0,0}\right)  =f_{0+1}$), and also holds if $k=1$ (since
$\det\left(  A_{0,1}\right)  =f_{1+1}$). Therefore, $\det\left(
A_{0,k}\right)  =f_{k+1}$ holds if $k\leq1$. Qed.}. Hence, we can WLOG assume
that we don't have $k\leq1$. Assume this.

We have $k>1$ (since we don't have $k\leq1$). Thus, $k\geq2$. Hence, $k-1$ and
$k-2$ are nonnegative integers satisfying $k-1<k$ and $k-2<k$. Hence, we can
apply (\ref{sol.tridiag.fib.main}) to $i=k-1$ (since we have assumed that
(\ref{sol.tridiag.fib.main}) holds for all $i<k$). As a result, we obtain
$\det\left(  A_{0,k-1}\right)  =f_{\left(  k-1\right)  +1}$. Also, we can
apply (\ref{sol.tridiag.fib.main}) to $i=k-2$ (since we have assumed that
(\ref{sol.tridiag.fib.main}) holds for all $i<k$). As a result, we obtain
$\det\left(  A_{0,k-2}\right)  =f_{\left(  k-2\right)  +1}$.

Now, $0\leq k-2$ (since $k\geq2$). Hence, Proposition \ref{prop.tridiag.rec}
\textbf{(c)} (applied to $x=0$ and $y=k$) yields%
\begin{align*}
\det\left(  A_{0,k}\right)   &  =\underbrace{a_{k}}_{\substack{=1\\\text{(by
the}\\\text{definition of }a_{k}\text{)}}}\underbrace{\det\left(
A_{0,k-1}\right)  }_{\substack{=f_{\left(  k-1\right)  +1}\\=f_{k}%
}}-\underbrace{b_{k-1}}_{\substack{=1\\\text{(by the}\\\text{definition of
}b_{k-1}\text{)}}}\underbrace{c_{k-1}}_{\substack{=-1\\\text{(by
the}\\\text{definition of }c_{k-1}\text{)}}}\underbrace{\det\left(
A_{0,k-2}\right)  }_{\substack{=f_{\left(  k-2\right)  +1}\\=f_{k-1}}}\\
&  =f_{k}-\left(  -1\right)  f_{k-1}=f_{k}+f_{k-1}.
\end{align*}
Comparing this with%
\[
f_{k+1}=f_{k}+f_{k-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the recursive
definition of the Fibonacci numbers}\right)  ,
\]
we obtain $\det\left(  A_{0,k}\right)  =f_{k+1}$. In other words,
(\ref{sol.tridiag.fib.main}) holds for $i=k$. This completes the induction
step. Thus, (\ref{sol.tridiag.fib.main}) is proven.

Now, applying \ref{sol.tridiag.fib.main} to $i=n$, we obtain $\det\left(
A_{0,n}\right)  =f_{n+1}$. Since $A_{0,n}=A$ (by Proposition
\ref{prop.tridiag.rec} \textbf{(e)}), we can rewrite this as $\det A=f_{n+1}$.
This solves Exercise \ref{exe.tridiag.fib}.
\end{proof}

\begin{proof}
[Second solution to Exercise \ref{exe.tridiag.fib} (sketched).]Here is a
different solution for Exercise \ref{exe.tridiag.fib}, which is far more
complicated than the previous one, but has the pedagogical advantage of
illuminating the connection between determinants and permutations, and the
combinatorics of the latter.

Exercise \ref{exe.ps2.2.3} (applied to $n+1$ instead of $n$) shows that
$f_{n+1}$ is the number of subsets $I$ of $\left\{  1,2,\ldots,n-1\right\}  $
such that no two elements of $I$ are consecutive. We shall refer to such
subsets $I$ as \textit{lacunar sets}.\footnote{We keep $n$ fixed, so a
\textquotedblleft lacunar set\textquotedblright\ will always be a subset of
$\left\{  1,2,\ldots,n-1\right\}  $.} Thus, $f_{n+1}$ is the number of all
lacunar sets.

(For example, if $n=5$, then the lacunar sets are $\varnothing$, $\left\{
1\right\}  $, $\left\{  2\right\}  $, $\left\{  3\right\}  $, $\left\{
4\right\}  $, $\left\{  1,3\right\}  $, $\left\{  1,4\right\}  $, and
$\left\{  2,4\right\}  $. Their number, unsurprisingly, is $8=f_{5+1}$.)

For any $\sigma\in S_{n}$, we define the following terminology:

\begin{itemize}
\item The \textit{excedances} of $\sigma$ are the elements $i\in\left\{
1,2,\ldots,n\right\}  $ satisfying $\sigma\left(  i\right)  >i$. For instance,
the permutation in $S_{7}$ written in one-line notation as $\left(
3,1,2,4,5,7,6\right)  $ has excedances $1$ and $6$.

\item We let $\operatorname*{Exced}\sigma$ denote the set of all excedances of
$\sigma$.

\item A permutation $\sigma\in S_{n}$ is said to be \textit{short-legged} if
every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies $\left\vert
\sigma\left(  i\right)  -i\right\vert \leq1$. For instance, the permutation in
$S_{7}$ written in one-line notation as $\left(  1,2,4,3,5,7,6\right)  $ is short-legged.
\end{itemize}

(Most of the terminology here is my own, tailored for this exercise; only the
notion of \textquotedblleft excedance\textquotedblright\ is standard. I chose
the name \textquotedblleft short-legged\textquotedblright\ because a
permutation $\sigma$ satisfying $\left\vert \sigma\left(  i\right)
-i\right\vert \leq1$ \textquotedblleft does not take $i$ very
far\textquotedblright.)

What does this all have to do with the exercise? Let us write our matrix $A$
in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Thus, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we have%
\begin{equation}
a_{i,j}=%
\begin{cases}
a_{i}, & \text{if }i=j;\\
b_{i}, & \text{if }i=j-1;\\
c_{j}, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
=%
\begin{cases}
1, & \text{if }i=j;\\
1, & \text{if }i=j-1;\\
-1, & \text{if }i=j+1;\\
0, & \text{otherwise}%
\end{cases}
\label{sol.tridiag.fib.sol2.aij}%
\end{equation}
(since $a_{i}=1$, $b_{i}=1$ and $c_{j}=-1$). Notice that, as a consequence of
this equality, we have%
\begin{equation}
a_{i,j}=0\ \ \ \ \ \ \ \ \ \ \text{whenever }\left\vert i-j\right\vert >1.
\label{sol.tridiag.fib.sol2.locality}%
\end{equation}


Now, recall that $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
so that $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. But
Exercise \ref{exe.ps4.4} yields $\det\left(  A^{T}\right)  =\det A$. Hence,%
\begin{equation}
\det A=\det\left(  A^{T}\right)  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}
\label{sol.tridiag.fib.sol2.detA=}%
\end{equation}
(by (\ref{eq.det.eq.2}), applied to $A^{T}$ and $a_{j,i}$ instead of $A$ and
$a_{i,j}$). This is an expression for $\det A$, but in order to get any
mileage out of it we need to simplify the terms $\prod_{i=1}^{n}%
a_{\sigma\left(  i\right)  ,i}$ for $\sigma\in S_{n}$. This turns out to
depend on whether the permutation $\sigma$ is short-legged or not:

\begin{itemize}
\item If a permutation $\sigma\in S_{n}$ is not short-legged, then%
\begin{equation}
\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}=0.
\label{sol.tridiag.fib.sol2.term1}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.fib.sol2.term1}):} Let $\sigma\in S_{n}$ be
not short-legged. Thus, there exists a $k\in\left\{  1,2,\ldots,n\right\}  $
satisfying $\left\vert \sigma\left(  k\right)  -k\right\vert >1$. The factor
$a_{\sigma\left(  k\right)  ,k}$ corresponding to this $k$ must be $0$
(because of (\ref{sol.tridiag.fib.sol2.locality})); this forces the whole
product $\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}$ to become $0$. Thus,
(\ref{sol.tridiag.fib.sol2.term1}) follows.

\item If a permutation $\sigma\in S_{n}$ is short-legged, then%
\begin{equation}
\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}=\left(  -1\right)  ^{\left\vert
\operatorname*{Exced}\sigma\right\vert }. \label{sol.tridiag.fib.sol2.term2}%
\end{equation}


\textit{Proof of (\ref{sol.tridiag.fib.sol2.term2}):} Let $\sigma\in S_{n}$ be
short-legged. Thus, every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies
$\left\vert \sigma\left(  i\right)  -i\right\vert \leq1$. Consequently, for
every $i\in\left\{  1,2,\ldots,n\right\}  $, we have
\begin{align*}
a_{\sigma\left(  i\right)  ,i}  &  =%
\begin{cases}
1, & \text{if }\sigma\left(  i\right)  =i;\\
1, & \text{if }\sigma\left(  i\right)  =i-1;\\
-1, & \text{if }\sigma\left(  i\right)  =i+1;\\
0, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.tridiag.fib.sol2.aij})}\right)
\\
&  =%
\begin{cases}
1, & \text{if }\sigma\left(  i\right)  =i;\\
1, & \text{if }\sigma\left(  i\right)  =i-1;\\
-1, & \text{if }\sigma\left(  i\right)  =i+1
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the inequality }\left\vert \sigma\left(  i\right)  -i\right\vert
\leq1\text{ ensures that one of the}\\
\text{conditions }\sigma\left(  i\right)  =i\text{, }\sigma\left(  i\right)
=i-1\text{ and }\sigma\left(  i\right)  =i+1\text{ must hold}%
\end{array}
\right) \\
&  =%
\begin{cases}
1, & \text{if }\sigma\left(  i\right)  \leq i;\\
-1, & \text{if }\sigma\left(  i\right)  >i
\end{cases}
.
\end{align*}
Thus,%
\begin{align*}
\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}  &  =\prod_{i=1}^{n}%
\begin{cases}
1, & \text{if }\sigma\left(  i\right)  \leq i;\\
-1, & \text{if }\sigma\left(  i\right)  >i
\end{cases}
=\left(  -1\right)  ^{\left(  \text{the number of all }i\in\left\{
1,2,\ldots,n\right\}  \text{ satisfying }\sigma\left(  i\right)  >i\right)
}\\
&  =\left(  -1\right)  ^{\left\vert \left\{  i\in\left\{  1,2,\ldots
,n\right\}  \ \mid\ \sigma\left(  i\right)  >i\right\}  \right\vert }=\left(
-1\right)  ^{\left\vert \operatorname*{Exced}\sigma\right\vert };
\end{align*}
thus, (\ref{sol.tridiag.fib.sol2.term2}) is proven.
\end{itemize}

Now, (\ref{sol.tridiag.fib.sol2.detA=}) becomes%
\begin{align}
\det A  &  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{\sigma\left(  i\right)  ,i}\nonumber\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is not short-legged}%
}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{\sigma\left(
i\right)  ,i}}_{\substack{=0\\\text{(by (\ref{sol.tridiag.fib.sol2.term1}))}%
}}+\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is short-legged}}}\left(
-1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{\sigma\left(  i\right)  ,i}%
}_{\substack{=\left(  -1\right)  ^{\left\vert \operatorname*{Exced}%
\sigma\right\vert }\\\text{(by (\ref{sol.tridiag.fib.sol2.term2}))}%
}}\nonumber\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is not
short-legged}}}\left(  -1\right)  ^{\sigma}0}_{=0}+\sum_{\substack{\sigma\in
S_{n};\\\sigma\text{ is short-legged}}}\left(  -1\right)  ^{\sigma}\left(
-1\right)  ^{\left\vert \operatorname*{Exced}\sigma\right\vert }\nonumber\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is short-legged}}}\left(
-1\right)  ^{\sigma}\left(  -1\right)  ^{\left\vert \operatorname*{Exced}%
\sigma\right\vert }. \label{sol.tridiag.fib.sol2.det1}%
\end{align}
We still don't see how this connects to $f_{n+1}$, though. So let us relate
short-legged permutations to lacunar sets.

For any lacunar set $I$, we can define a permutation $\tau_{I}\in S_{n}$ by
the following rule:%
\[
\tau_{I}\left(  k\right)  =%
\begin{cases}
k+1, & \text{if }k\in I;\\
k-1, & \text{if }k-1\in I;\\
k, & \text{otherwise}%
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots,n\right\}  .
\]
In other words, $\tau_{I}$ is the permutation of $\left\{  1,2,\ldots
,n\right\}  $ which interchanges every element $i$ of $I$ with its successor
$i+1$, while leaving all remaining elements unchanged. Make sure you
understand why $\tau_{I}$ is a well-defined map $\left\{  1,2,\ldots
,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $\ \ \ \ \footnote{Here
are the two things you need to check:
\par
\begin{itemize}
\item The term $%
\begin{cases}
k+1, & \text{if }k\in I;\\
k-1, & \text{if }k-1\in I;\\
k, & \text{otherwise}%
\end{cases}
$ is unambiguous, because no $k$ satisfies both $k\in I$ and $k-1\in I$ at the
same time. (Here is where you use the lacunarity of $I$.)
\par
\item We have $%
\begin{cases}
k+1, & \text{if }k\in I;\\
k-1, & \text{if }k-1\in I;\\
k, & \text{otherwise}%
\end{cases}
\in\left\{  1,2,\ldots,n\right\}  $ for every $k\in\left\{  1,2,\ldots
,n\right\}  $. (Here you use $I\subseteq\left\{  1,2,\ldots,n-1\right\}  $. If
$I$ were only a subset of $\left\{  1,2,\ldots,n\right\}  $, then this would
fall outside of $\left\{  1,2,\ldots,n\right\}  $ for $k=n$.)
\end{itemize}
} and a permutation\footnote{Indeed, $\tau_{I}\circ\tau_{I}=\operatorname*{id}%
$, so that $\tau_{I}$ is its own inverse.}.

(For example, if $n=7$ and $I=\left\{  2,5\right\}  $, then $\tau_{I}=\left(
1,3,2,4,6,5,7\right)  $ in one-line notation.)

It is clear that the permutation $\tau_{I}$ is short-legged. Moreover, it
satisfies%
\begin{equation}
\operatorname*{Exced}\left(  \tau_{I}\right)  =I;
\label{sol.tridiag.fib.sol2.tauI.Exced}%
\end{equation}
as a consequence, it is possible to reconstruct $I$ from $\tau_{I}$. Thus, the
permutations $\tau_{I}$ for distinct $I$ are distinct.

What is the sign $\left(  -1\right)  ^{\tau_{I}}$ ? It is easy to see (from
the construction of $\tau_{I}$) that the only inversions of $\tau_{I}$ are the
pairs $\left(  i,i+1\right)  $ for $i\in I$ (essentially, the short-leggedness
of $\tau_{I}$ prevents $\tau_{I}$ from changing the order of two non-adjacent
integers). Thus, the number of these inversions is $\left\vert I\right\vert $.
Thus, $\ell\left(  \tau_{I}\right)  =\left\vert I\right\vert $. Hence,
\begin{equation}
\left(  -1\right)  ^{\tau_{I}}=\left(  -1\right)  ^{\ell\left(  \tau
_{I}\right)  }=\left(  -1\right)  ^{\left\vert I\right\vert }.
\label{sol.tridiag.fib.sol2.tauI.sign}%
\end{equation}


So there are at least some short-legged permutations that we understand well:
the $\tau_{I}$ for lacunar sets $I$. Are there others?

It turns out that there aren't. Indeed,
\begin{equation}
\text{every short-legged }\sigma\in S_{n}\text{ has the form }\tau_{I}\text{
for some lacunar set }I. \label{sol.tridiag.fib.sol2.tauI.surj}%
\end{equation}
Before we can prove this, we shall prove two auxiliary observations:

\textit{Observation 1:} Let $\sigma\in S_{n}$ be short-legged. If
$i\in\left\{  1,2,\ldots,n\right\}  $ be such that $\sigma\left(  i\right)
=i+1$, then $\sigma\left(  i+1\right)  =i$.

\textit{Observation 2:} Let $\sigma\in S_{n}$ be short-legged. If
$i\in\left\{  1,2,\ldots,n\right\}  $ be such that $\sigma\left(  i\right)
=i-1$, then $\sigma\left(  i-1\right)  =i$.

\textit{Proof of Observation 1.} Assume the contrary. Thus, there exists some
$i\in\left\{  1,2,\ldots,n\right\}  $ such that $\sigma\left(  i\right)  =i+1$
but $\sigma\left(  i+1\right)  \neq i$. We call such $i$'s \textit{evil}. By
our assumption, there exists at least one evil $i$. Consider the highest evil
$i$. Thus, $i+1$ is not evil.

Since $i$ is evil, we have $\sigma\left(  i\right)  =i+1$ but $\sigma\left(
i+1\right)  \neq i$. In particular, $i+1=\sigma\left(  i\right)  \in\left\{
1,2,\ldots,n\right\}  $, so that $\sigma\left(  i+1\right)  $ is well-defined.
Since $\sigma$ is short-legged, we have $\left\vert \sigma\left(  i+1\right)
-\left(  i+1\right)  \right\vert \leq1$. Hence, $\sigma\left(  i+1\right)  $
is either $i$ or $i+1$ or $i+2$. But $\sigma\left(  i+1\right)  $ cannot be
$i$ (since $\sigma\left(  i+1\right)  \neq i$) and cannot be $i+1$ either
(since this would cause $\sigma\left(  i+1\right)  =i+1=\sigma\left(
i\right)  $, which would contradict the injectivity of $\sigma$). Hence,
$\sigma\left(  i+1\right)  $ must be $i+2$. In other words, $\sigma\left(
i+1\right)  =i+2$. Moreover, the injectivity of $\sigma$ shows that
$\sigma\left(  i+2\right)  \neq\sigma\left(  i\right)  =i+1$, so that $i+1$ is
evil. But this contradicts the fact that $i+1$ is not evil. Thus, Observation
1 is proven.

\textit{Proof of Observation 2.} Analogous to Observation 1 (this time, take
the lowest evil $i$), and left to the reader.

\textit{Proof of (\ref{sol.tridiag.fib.sol2.tauI.surj}):} Let $\sigma\in
S_{n}$ be short-legged. We must show that $\sigma=\tau_{I}$ for some lacunar
set $I$.

We set $I=\operatorname*{Exced}\sigma$. (This is the only choice we can make
to have any hope for $\sigma=\tau_{I}$ to be true; indeed,
(\ref{sol.tridiag.fib.sol2.tauI.Exced}) ensures that if $\sigma=\tau_{I}$,
then $\operatorname*{Exced}\sigma=I$.)

We notice that%
\begin{equation}
\sigma\left(  i\right)  =i+1\ \ \ \ \ \ \ \ \ \ \text{for every }i\in I
\label{sol.tridiag.fib.sol2.tauI.surj.pf.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}):} Let
$i\in I$. Thus, $i\in I=\operatorname*{Exced}\sigma$. In other words, $i$ is
an excedance of $\sigma$. Hence, $\sigma\left(  i\right)  >i$. Since
$\left\vert \sigma\left(  i\right)  -i\right\vert \leq1$ (because $\sigma$ is
short-legged), this means that $\sigma\left(  i\right)  =i+1$, qed.}. Thus,
$n$ cannot belong to $I$ (since this would entail $\sigma\left(  n\right)
=n+1$, but $n+1\notin\left\{  1,2,\ldots,n\right\}  $). Hence, $I\subseteq
\left\{  1,2,\ldots,n-1\right\}  $.

Let us first show that $I$ is a lacunar set. Indeed, assume (for the sake of
contradiction) that this is not so. Then, there exists some $i\in I$ such that
$i+1\in I$. Consider such an $i$. We have $\sigma\left(  i\right)  =i+1$ (by
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1})), and thus $\sigma\left(
i+1\right)  =i$ (by Observation 1). But $\sigma\left(  i+1\right)  =i+2$ (by
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}), applied to $i+1$ instead of $i$).
Hence, $i+2=\sigma\left(  i+1\right)  =i$, which is absurd. Hence, we have
found a contradiction. This finishes our proof that $I$ is a lacunar set.

We still need to show that we actually have $\sigma=\tau_{I}$. In other words,
we need to show that $\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ for
every $k\in\left\{  1,2,\ldots,n\right\}  $.

So let us fix $k\in\left\{  1,2,\ldots,n\right\}  $, and let us show that
$\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $. We are in one of the
following three cases:

\textit{Case 1:} We have $k\in I$.

\textit{Case 2:} We have $k-1\in I$.

\textit{Case 3:} Neither $k\in I$ nor $k-1\in I$.

\begin{itemize}
\item Let us first consider Case 1. In this case, $k\in I$. Hence,
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}) (applied to $i=k$) yields
$\sigma\left(  k\right)  =k+1$. On the other hand, the definition of $\tau
_{I}$ shows that $\tau_{I}\left(  k\right)  =k+1$ as well. Thus,
$\sigma\left(  k\right)  =k+1=\tau_{I}\left(  k\right)  $. Hence,
$\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ is proven in Case 1.

\item Let us now consider Case 2. In this case, $k-1\in I$. Hence,
(\ref{sol.tridiag.fib.sol2.tauI.surj.pf.1}) (applied to $i=k-1$) yields
$\sigma\left(  k-1\right)  =k$. Since $\sigma$ is injective, we have
$\sigma\left(  k\right)  \neq\sigma\left(  k-1\right)  =k$. Also, $I$ is
lacunar, so that $k-1\in I$ entails $k\notin I$; thus, $k\notin
I=\operatorname*{Exced}\sigma$, so that $k$ is not an excedance of $\sigma$.
In other words, $\sigma\left(  k\right)  \leq k$. Combined with $\sigma\left(
k\right)  \neq k$, this yields $\sigma\left(  k\right)  <k$. Since $\left\vert
\sigma\left(  k\right)  -k\right\vert \leq1$ (because $\sigma$ is
short-legged), this shows that $\sigma\left(  k\right)  =k-1$. On the other
hand, $\tau_{I}\left(  k\right)  =k-1$ by the definition of $\tau_{I}$. Thus,
$\sigma\left(  k\right)  =k-1=\tau_{I}\left(  k\right)  $. Hence,
$\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ is proven in Case 2.

\item Let us finally consider Case 3. In this case, neither $k\in I$ nor
$k-1\in I$. Let us now show that $\sigma\left(  k\right)  =k$. Indeed, assume
the contrary. Thus, $\sigma\left(  k\right)  \neq k$. As in Case 2, we can use
this (and $k\notin I$) to show that $\sigma\left(  k\right)  =k-1$.
Observation 2 thus shows that $\sigma\left(  k-1\right)  =k>k-1$, so that
$k-1$ is an excedance of $\sigma$. In other words, $k-1\in
\operatorname*{Exced}\sigma=I$. This contradicts the assumption that we do not
have $k-1\in I$. This contradiction concludes our proof of $\sigma\left(
k\right)  =k$. On the other hand, $\tau_{I}\left(  k\right)  =k$ by the
definition of $\tau_{I}$. Thus, $\sigma\left(  k\right)  =k=\tau_{I}\left(
k\right)  $. Hence, $\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $ is
proven in Case 3.
\end{itemize}

We now have shown that $\sigma\left(  k\right)  =\tau_{I}\left(  k\right)  $
in all possible cases. Thus, $\sigma=\tau_{I}$. Since $I$ is a lacunar set,
this proves (\ref{sol.tridiag.fib.sol2.tauI.surj}).

All we now need to do is combine our results. For every lacunar set $I$, we
have defined a short-legged permutation $\tau_{I}\in S_{n}$. Conversely, we
know (from (\ref{sol.tridiag.fib.sol2.tauI.surj})) that every short-legged
$\sigma\in S_{n}$ has the form $\tau_{I}$ for some lacunar set $I$; we also
know that this $I$ is uniquely determined by the $\sigma$ (since the
permutations $\tau_{I}$ for distinct $I$ are distinct). Thus, we have a
bijection between the lacunar sets and the short-legged permutations in
$S_{n}$; the bijection sends every $I$ to $\tau_{I}$. Consequently,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is short-legged}}}\left(
-1\right)  ^{\sigma}\left(  -1\right)  ^{\left\vert \operatorname*{Exced}%
\sigma\right\vert }\\
&  =\sum_{I\text{ is a lacunar set}}\underbrace{\left(  -1\right)  ^{\tau_{I}%
}}_{\substack{=\left(  -1\right)  ^{\left\vert I\right\vert }\\\text{(by
(\ref{sol.tridiag.fib.sol2.tauI.sign}))}}}\underbrace{\left(  -1\right)
^{\left\vert \operatorname*{Exced}\left(  \tau_{I}\right)  \right\vert }%
}_{\substack{=\left(  -1\right)  ^{\left\vert I\right\vert }\\\text{(by
(\ref{sol.tridiag.fib.sol2.tauI.Exced}))}}}\\
&  =\sum_{I\text{ is a lacunar set}}\underbrace{\left(  -1\right)
^{\left\vert I\right\vert }\left(  -1\right)  ^{\left\vert I\right\vert }%
}_{=\left(  \left(  -1\right)  ^{\left\vert I\right\vert }\right)  ^{2}%
=1}=\sum_{I\text{ is a lacunar set}}1\\
&  =\left(  \text{the number of all lacunar sets}\right)  =f_{n+1}.
\end{align*}
Combining this with (\ref{sol.tridiag.fib.sol2.det1}), we conclude that $\det
A=f_{n+1}$.
\end{proof}

\subsection{Solution to Exercise \ref{exe.block2x2.mult}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.mult}.]This is another case where the
solution is really clear with the appropriate amount of waving hands and
pointing fingers, but on paper becomes nearly impossible to convey. I shall
therefore resort to formalism and computation.

Write the matrices $A$, $B$, $C$, $D$, $A^{\prime}$, $B^{\prime}$, $C^{\prime
}$ and $D^{\prime}$ in the forms%
\begin{align*}
A  &  =\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}%
,\ \ \ \ \ \ \ \ \ \ B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}},\\
C  &  =\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq
m},\ \ \ \ \ \ \ \ \ \ D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m^{\prime}},\\
A^{\prime}  &  =\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq
j\leq\ell},\ \ \ \ \ \ \ \ \ \ B^{\prime}=\left(  b_{i,j}^{\prime}\right)
_{1\leq i\leq m,\ 1\leq j\leq\ell^{\prime}},\\
C^{\prime}  &  =\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime
},\ 1\leq j\leq\ell},\ \ \ \ \ \ \ \ \ \ D^{\prime}=\left(  d_{i,j}^{\prime
}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell^{\prime}}.
\end{align*}
The definition of the $\left(  n+n^{\prime}\right)  \times\left(  m+m^{\prime
}\right)  $-matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ shows that%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cccccccc}%
a_{1,1} & a_{1,2} & \cdots & a_{1,m} & b_{1,1} & b_{1,2} & \cdots &
b_{1,m^{\prime}}\\
a_{2,1} & a_{2,2} & \cdots & a_{2,m} & b_{2,1} & b_{2,2} & \cdots &
b_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
a_{n,1} & a_{n,2} & \cdots & a_{n,m} & b_{n,1} & b_{n,2} & \cdots &
b_{n,m^{\prime}}\\
c_{1,1} & c_{1,2} & \cdots & c_{1,m} & d_{1,1} & d_{1,2} & \cdots &
d_{1,m^{\prime}}\\
c_{2,1} & c_{2,2} & \cdots & c_{2,m} & d_{2,1} & d_{2,2} & \cdots &
d_{2,m^{\prime}}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
c_{n^{\prime},1} & c_{n^{\prime},2} & \cdots & c_{n^{\prime},m} &
d_{n^{\prime},1} & d_{n^{\prime},2} & \cdots & d_{n^{\prime},m^{\prime}}%
\end{array}
\right) \\
&  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\end{align*}
Similarly,%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cccccccc}%
a_{1,1}^{\prime} & a_{1,2}^{\prime} & \cdots & a_{1,\ell}^{\prime} &
b_{1,1}^{\prime} & b_{1,2}^{\prime} & \cdots & b_{1,\ell^{\prime}}^{\prime}\\
a_{2,1}^{\prime} & a_{2,2}^{\prime} & \cdots & a_{2,\ell}^{\prime} &
b_{2,1}^{\prime} & b_{2,2}^{\prime} & \cdots & b_{2,\ell^{\prime}}^{\prime}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
a_{m,1}^{\prime} & a_{m,2}^{\prime} & \cdots & a_{m,\ell}^{\prime} &
b_{m,1}^{\prime} & b_{m,2}^{\prime} & \cdots & b_{m,\ell^{\prime}}^{\prime}\\
c_{1,1}^{\prime} & c_{1,2}^{\prime} & \cdots & c_{1,\ell}^{\prime} &
d_{1,1}^{\prime} & d_{1,2}^{\prime} & \cdots & d_{1,\ell^{\prime}}^{\prime}\\
c_{2,1}^{\prime} & c_{2,2}^{\prime} & \cdots & c_{2,\ell}^{\prime} &
d_{2,1}^{\prime} & d_{2,2}^{\prime} & \cdots & d_{2,\ell^{\prime}}^{\prime}\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
c_{m^{\prime},1}^{\prime} & c_{m^{\prime},2}^{\prime} & \cdots & c_{m^{\prime
},\ell}^{\prime} & d_{m^{\prime},1}^{\prime} & d_{m^{\prime},2}^{\prime} &
\cdots & d_{m^{\prime},\ell^{\prime}}^{\prime}%
\end{array}
\right) \\
&  =\left(
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq m\text{ and }j\leq\ell;\\
b_{i,j-\ell}^{\prime}, & \text{if }i\leq m\text{ and }j>\ell;\\
c_{i-m,j}^{\prime}, & \text{if }i>m\text{ and }j\leq\ell;\\
d_{i-m,j-\ell}^{\prime}, & \text{if }i>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\end{align*}
Using these two equalities, we can compute the product $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $: Namely,%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.short.3}%
\end{align}


On the other hand, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$ and $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq
m,\ 1\leq j\leq\ell}$. Hence, the definition of the product of two matrices
shows that%
\begin{equation}
AA^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.short.5a}%
\end{equation}


Also, we have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$ and $C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq
m^{\prime},\ 1\leq j\leq\ell}$. Hence, the definition of the product of two
matrices shows that%
\begin{equation}
BC^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}\right)
_{1\leq i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.short.5b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.short.5a}) and
(\ref{sol.block2x2.mult.short.5b}), we obtain%
\begin{align}
AA^{\prime}+BC^{\prime}  &  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime
}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}+\left(  \sum_{k=1}^{m^{\prime}%
}b_{i,k}c_{k,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}b_{i,k-m}c_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}.
\label{sol.block2x2.mult.short.5c}%
\end{align}


Similarly,%
\begin{equation}
AB^{\prime}+BD^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq
n,\ 1\leq j\leq\ell^{\prime}}; \label{sol.block2x2.mult.short.6c}%
\end{equation}%
\begin{equation}
CA^{\prime}+DC^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq\ell}; \label{sol.block2x2.mult.short.7c}%
\end{equation}%
\begin{equation}
CB^{\prime}+DD^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.short.8c}%
\end{equation}


Now, we have the four equalities (\ref{sol.block2x2.mult.short.5c}),
(\ref{sol.block2x2.mult.short.6c}), (\ref{sol.block2x2.mult.short.7c}) and
(\ref{sol.block2x2.mult.short.8c}). Hence, the definition of the block matrix
$\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  $ (or, more precisely, the equality (\ref{eq.def.block2x2.formal}),
applied to $n$, $n^{\prime}$, $\ell$, $\ell^{\prime}$, $AA^{\prime}%
+BC^{\prime}$, $AB^{\prime}+BD^{\prime}$, $CA^{\prime}+DC^{\prime}$,
$CB^{\prime}+DD^{\prime}$, $\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum
_{k=m+1}^{m+m^{\prime}}b_{i,k-m}c_{k-m,j}^{\prime}$, $\sum_{k=1}^{m}%
a_{i,k}b_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime
}$, $\sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i,k-m}c_{k-m,j}^{\prime}$ and $\sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$,
$c_{i,j}$ and $d_{i,j}$) shows that
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.short.9}%
\end{align}


But our goal is to prove that $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  $. In other words, we want to prove that the left hand sides of the
equalities (\ref{sol.block2x2.mult.short.3}) and
(\ref{sol.block2x2.mult.short.9}) are equal. For this, it clearly suffices to
show that the right hand sides of these equalities are equal. In other words,
it suffices to show that every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots,\ell+\ell^{\prime}\right\}  $
satisfies%
\begin{align}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\nonumber\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
. \label{sol.block2x2.mult.short.entrywise}%
\end{align}


\textit{Proof of (\ref{sol.block2x2.mult.short.entrywise}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{
1,2,\ldots,\ell+\ell^{\prime}\right\}  $. Thus, \newline$i\in\left\{
1,2,\ldots,n+n^{\prime}\right\}  $ and $j\in\left\{  1,2,\ldots,\ell
+\ell^{\prime}\right\}  $. We must be in one of the following four cases:

\textit{Case 1:} We have $i\leq n$ and $j\leq\ell$.

\textit{Case 2:} We have $i\leq n$ and $j>\ell$.

\textit{Case 3:} We have $i>n$ and $j\leq\ell$.

\textit{Case 4:} We have $i>n$ and $j>\ell$.

All four cases are completely analogous; we thus will only show how to deal
with Case 1. In this case, we have $i\leq n$ and $j\leq\ell$. Now, comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=a_{i,k}\\\text{(since }i\leq n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=a_{k,j}^{\prime}\\\text{(since }k\leq m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=b_{i,k-m}\\\text{(since }i\leq n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=c_{k-m,j}^{\prime}\\\text{(since }k>m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq
n\text{ and }j\leq\ell\right)  ,
\end{align*}
we obtain precisely (\ref{sol.block2x2.mult.short.entrywise}). Thus,
(\ref{sol.block2x2.mult.short.entrywise}) is proven in Case 1. As I said, the
other three cases are similar, and so (\ref{sol.block2x2.mult.short.entrywise}%
) is proven.

From (\ref{sol.block2x2.mult.short.entrywise}), we see that the right hand
sides of the equalities (\ref{sol.block2x2.mult.short.3}) and
(\ref{sol.block2x2.mult.short.9}) are equal. Hence, so are their left hand
sides. In other words, $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  $. This solves Exercise \ref{exe.block2x2.mult}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.mult}.]Write the $n\times m$-matrix
$A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.

Write the $n\times m^{\prime}$-matrix $B$ in the form $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$.

Write the $n^{\prime}\times m$-matrix $C$ in the form $C=\left(
c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$.

Write the $n^{\prime}\times m^{\prime}$-matrix $D$ in the form $D=\left(
d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}$.

Write the $m\times\ell$-matrix $A^{\prime}$ in the form $A^{\prime}=\left(
a_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq j\leq\ell}$.

Write the $m\times\ell^{\prime}$-matrix $B^{\prime}$ in the form $B^{\prime
}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq j\leq\ell^{\prime}%
}$.

Write the $m^{\prime}\times\ell$-matrix $C^{\prime}$ in the form $C^{\prime
}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell
}$.

Write the $m^{\prime}\times\ell^{\prime}$-matrix $D^{\prime}$ in the form
$D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime},\ 1\leq
j\leq\ell^{\prime}}$.

We have $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq
j\leq\ell}$, $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq
m,\ 1\leq j\leq\ell^{\prime}}$, $C^{\prime}=\left(  c_{i,j}^{\prime}\right)
_{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell}$ and $D^{\prime}=\left(
d_{i,j}^{\prime}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq\ell^{\prime}}%
$. Thus, (\ref{eq.def.block2x2.formal}) (applied to $m$, $m^{\prime}$, $\ell$,
$\ell^{\prime}$, $A^{\prime}$, $B^{\prime}$, $C^{\prime}$, $D^{\prime}$,
$a_{i,j}^{\prime}$, $b_{i,j}^{\prime}$, $c_{i,j}^{\prime}$ and $d_{i,j}%
^{\prime}$ instead of $n$, $n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$,
$D$, $a_{i,j}$, $b_{i,j}$, $c_{i,j}$ and $d_{i,j}$) shows that
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq m\text{ and }j\leq\ell;\\
b_{i,j-\ell}^{\prime}, & \text{if }i\leq m\text{ and }j>\ell;\\
c_{i-m,j}^{\prime}, & \text{if }i>m\text{ and }j\leq\ell;\\
d_{i-m,j-\ell}^{\prime}, & \text{if }i>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.1}%
\end{equation}


Now, we have (\ref{eq.def.block2x2.formal}) and (\ref{sol.block2x2.mult.1}).
Thus, the definition of the product of two matrices shows that%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.3}%
\end{align}


On the other hand, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$ and $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq
m,\ 1\leq j\leq\ell}$. Hence, the definition of the product of two matrices
shows that%
\begin{equation}
AA^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.5a}%
\end{equation}


Also, we have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$ and $C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq
m^{\prime},\ 1\leq j\leq\ell}$. Hence, the definition of the product of two
matrices shows that%
\begin{equation}
BC^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}\right)
_{1\leq i\leq n,\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.5b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.5a}) and
(\ref{sol.block2x2.mult.5b}), we obtain%
\begin{align}
AA^{\prime}+BC^{\prime}  &  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime
}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}+\left(  \sum_{k=1}^{m^{\prime}%
}b_{i,k}c_{k,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}b_{i,k}c_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}b_{i,k-m}c_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell}.
\label{sol.block2x2.mult.5c}%
\end{align}


Furthermore, we have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$ and $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq m,\ 1\leq
j\leq\ell^{\prime}}$. Hence, the definition of the product of two matrices
shows that%
\begin{equation}
AB^{\prime}=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.6a}%
\end{equation}


Also, we have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$ and $D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq i\leq
m^{\prime},\ 1\leq j\leq\ell^{\prime}}$. Hence, the definition of the product
of two matrices shows that%
\begin{equation}
BD^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}b_{i,k}d_{k,j}^{\prime}\right)
_{1\leq i\leq n,\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.6b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.6a}) and
(\ref{sol.block2x2.mult.6b}), we obtain%
\begin{align}
AB^{\prime}+BD^{\prime}  &  =\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime
}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell^{\prime}}+\left(  \sum
_{k=1}^{m^{\prime}}b_{i,k}d_{k,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}b_{i,k}d_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq\ell
^{\prime}}. \label{sol.block2x2.mult.6c}%
\end{align}


Furthermore, we have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m}$ and $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq
i\leq m,\ 1\leq j\leq\ell}$. Hence, the definition of the product of two
matrices shows that%
\begin{equation}
CA^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}\right)  _{1\leq
i\leq n^{\prime},\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.7a}%
\end{equation}


Also, we have $D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq m^{\prime}}$ and $C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq
i\leq m^{\prime},\ 1\leq j\leq\ell}$. Hence, the definition of the product of
two matrices shows that%
\begin{equation}
DC^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}d_{i,k}c_{k,j}^{\prime}\right)
_{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell}. \label{sol.block2x2.mult.7b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.7a}) and
(\ref{sol.block2x2.mult.7b}), we obtain%
\begin{align}
CA^{\prime}+DC^{\prime}  &  =\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime
}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell}+\left(  \sum
_{k=1}^{m^{\prime}}d_{i,k}c_{k,j}^{\prime}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}d_{i,k}c_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}d_{i,k-m}c_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq\ell}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i,k-m}c_{k-m,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq\ell}. \label{sol.block2x2.mult.7c}%
\end{align}


Furthermore, we have $C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m}$ and $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq
i\leq m,\ 1\leq j\leq\ell^{\prime}}$. Hence, the definition of the product of
two matrices shows that%
\begin{equation}
CB^{\prime}=\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}\right)  _{1\leq
i\leq n^{\prime},\ 1\leq j\leq\ell^{\prime}}. \label{sol.block2x2.mult.8a}%
\end{equation}


Also, we have $D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq m^{\prime}}$ and $D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq
i\leq m^{\prime},\ 1\leq j\leq\ell^{\prime}}$. Hence, the definition of the
product of two matrices shows that%
\begin{equation}
DD^{\prime}=\left(  \sum_{k=1}^{m^{\prime}}d_{i,k}d_{k,j}^{\prime}\right)
_{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell^{\prime}}.
\label{sol.block2x2.mult.8b}%
\end{equation}


Adding the equalities (\ref{sol.block2x2.mult.8a}) and
(\ref{sol.block2x2.mult.8b}), we obtain%
\begin{align}
CB^{\prime}+DD^{\prime}  &  =\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime
}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq\ell^{\prime}}+\left(
\sum_{k=1}^{m^{\prime}}d_{i,k}d_{k,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}+\underbrace{\sum
_{k=1}^{m^{\prime}}d_{i,k}d_{k,j}^{\prime}}_{\substack{=\sum_{k=m+1}%
^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}\\\text{(here, we have substituted
}k-m\text{ for }k\text{ in the sum)}}}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq\ell^{\prime}}\nonumber\\
&  =\left(  \sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i,k-m}d_{k-m,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq\ell^{\prime}}. \label{sol.block2x2.mult.8c}%
\end{align}


Now, we have the four equalities (\ref{sol.block2x2.mult.5c}),
(\ref{sol.block2x2.mult.6c}), (\ref{sol.block2x2.mult.7c}) and
(\ref{sol.block2x2.mult.8c}). Hence, (\ref{eq.def.block2x2.formal}) (applied
to $n$, $n^{\prime}$, $\ell$, $\ell^{\prime}$, $AA^{\prime}+BC^{\prime}$,
$AB^{\prime}+BD^{\prime}$, $CA^{\prime}+DC^{\prime}$, $CB^{\prime}+DD^{\prime
}$, $\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}c_{k-m,j}^{\prime}$, $\sum_{k=1}^{m}a_{i,k}b_{k,j}^{\prime}%
+\sum_{k=m+1}^{m+m^{\prime}}b_{i,k-m}d_{k-m,j}^{\prime}$, $\sum_{k=1}%
^{m}c_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}d_{i,k-m}%
c_{k-m,j}^{\prime}$ and $\sum_{k=1}^{m}c_{i,k}b_{k,j}^{\prime}+\sum
_{k=m+1}^{m+m^{\prime}}d_{i,k-m}d_{k-m,j}^{\prime}$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$,
$c_{i,j}$ and $d_{i,j}$) shows that
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}.
\label{sol.block2x2.mult.9}%
\end{align}


Now, we shall show that every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots,\ell+\ell^{\prime}\right\}  $
satisfies%
\begin{align}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\nonumber\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
. \label{sol.block2x2.mult.entrywise}%
\end{align}


\textit{Proof of (\ref{sol.block2x2.mult.entrywise}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{
1,2,\ldots,\ell+\ell^{\prime}\right\}  $. Thus, $i\in\left\{  1,2,\ldots
,n+n^{\prime}\right\}  $ and $j\in\left\{  1,2,\ldots,\ell+\ell^{\prime
}\right\}  $. We must be in one of the following two cases:

\textit{Case 1:} We have $i\leq n$.

\textit{Case 2:} We have $i>n$.

Let us first consider Case 1. In this case, we have $i\leq n$. Now, we must be
in one of the following two subcases:

\textit{Subcase 1.1:} We have $j\leq\ell$.

\textit{Subcase 1.2:} We have $j>\ell$.

Let us first consider Subcase 1.1. In this Subcase, we have $j\leq\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=a_{i,k}\\\text{(since }i\leq n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=a_{k,j}^{\prime}\\\text{(since }k\leq m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=b_{i,k-m}\\\text{(since }i\leq n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=c_{k-m,j}^{\prime}\\\text{(since }k>m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq
n\text{ and }j\leq\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 1.1.

Let us now consider Subcase 1.2. In this Subcase, we have $j>\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=a_{i,k}\\\text{(since }i\leq n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=b_{k,j-\ell}^{\prime}\\\text{(since }k\leq m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=b_{i,k-m}\\\text{(since }i\leq n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=d_{k-m,j-\ell}^{\prime}\\\text{(since }k>m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i\leq n\text{ and }j>\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 1.2.

We have thus proven (\ref{sol.block2x2.mult.entrywise}) in each of the two
Subcases 1.1 and 1.2. Since these two Subcases cover the whole Case 1, this
shows that (\ref{sol.block2x2.mult.entrywise}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $i>n$. Now, we must be in
one of the following two subcases:

\textit{Subcase 2.1:} We have $j\leq\ell$.

\textit{Subcase 2.2:} We have $j>\ell$.

Let us first consider Subcase 2.1. In this Subcase, we have $j\leq\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=c_{i-n,k}\\\text{(since }i>n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=a_{k,j}^{\prime}\\\text{(since }k\leq m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=d_{i-n,k-m}\\\text{(since }i>n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=c_{k-m,j}^{\prime}\\\text{(since }k>m\text{ and }j\leq
\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}c_{k-m,j}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}c_{k-m,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i>n\text{ and }j\leq\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 2.1.

Let us now consider Subcase 2.2. In this Subcase, we have $j>\ell$. Now,
comparing%
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=c_{i-n,k}\\\text{(since }i>n\text{ and }k\leq m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=b_{k,j-\ell}^{\prime}\\\text{(since }k\leq m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{k=m+1}^{m+m^{\prime}}\underbrace{%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}
}_{\substack{=d_{i-n,k-m}\\\text{(since }i>n\text{ and }k>m\text{)}%
}}\underbrace{%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=d_{k-m,j-\ell}^{\prime}\\\text{(since }k>m\text{ and }%
j>\ell\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq m\leq m+m^{\prime}\right) \\
&  =\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}%
\end{align*}
with%
\begin{align*}
&
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\
&  =\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}i>n\text{ and }j>\ell\right)  ,
\end{align*}
we obtain
\begin{align*}
&  \sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
\\
&  =%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
.
\end{align*}
Thus, (\ref{sol.block2x2.mult.entrywise}) is proven in Subcase 2.2.

We have thus proven (\ref{sol.block2x2.mult.entrywise}) in each of the two
Subcases 2.1 and 2.2. Since these two Subcases cover the whole Case 2, this
shows that (\ref{sol.block2x2.mult.entrywise}) is proven in Case 2.

We have thus proven (\ref{sol.block2x2.mult.entrywise}) in each of the two
Cases 1 and 2. Thus, (\ref{sol.block2x2.mult.entrywise}) always holds. This
completes our proof of (\ref{sol.block2x2.mult.entrywise}).

Now, (\ref{sol.block2x2.mult.3}) becomes%
\begin{align*}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \\
&  =\left(  \underbrace{\sum_{k=1}^{m+m^{\prime}}%
\begin{cases}
a_{i,k}, & \text{if }i\leq n\text{ and }k\leq m;\\
b_{i,k-m}, & \text{if }i\leq n\text{ and }k>m;\\
c_{i-n,k}, & \text{if }i>n\text{ and }k\leq m;\\
d_{i-n,k-m}, & \text{if }i>n\text{ and }k>m
\end{cases}%
\begin{cases}
a_{k,j}^{\prime}, & \text{if }k\leq m\text{ and }j\leq\ell;\\
b_{k,j-\ell}^{\prime}, & \text{if }k\leq m\text{ and }j>\ell;\\
c_{k-m,j}^{\prime}, & \text{if }k>m\text{ and }j\leq\ell;\\
d_{k-m,j-\ell}^{\prime}, & \text{if }k>m\text{ and }j>\ell
\end{cases}
}_{\substack{=%
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\\\text{(by (\ref{sol.block2x2.mult.entrywise}))}}}\right)  _{1\leq i\leq
n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}\\
&  =\left(
\begin{cases}
\sum_{k=1}^{m}a_{i,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
b_{i,k-m}c_{k-m,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}a_{i,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}b_{i,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i\leq n\text{ and }j>\ell;\\
\sum_{k=1}^{m}c_{i-n,k}a_{k,j}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}}%
d_{i-n,k-m}c_{k-m,j}^{\prime}, & \text{if }i>n\text{ and }j\leq\ell;\\
\sum_{k=1}^{m}c_{i-n,k}b_{k,j-\ell}^{\prime}+\sum_{k=m+1}^{m+m^{\prime}%
}d_{i-n,k-m}d_{k-m,j-\ell}^{\prime}, & \text{if }i>n\text{ and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq\ell+\ell^{\prime}}\\
&  =\left(
\begin{array}
[c]{cc}%
AA^{\prime}+BC^{\prime} & AB^{\prime}+BD^{\prime}\\
CA^{\prime}+DC^{\prime} & CB^{\prime}+DD^{\prime}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.block2x2.mult.9}%
)}\right)  .
\end{align*}
This solves Exercise \ref{exe.block2x2.mult}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.block2x2.tridet}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.tridet}.]We shall prove Exercise
\ref{exe.block2x2.tridet} by induction over $m$:

\textit{Induction base:} Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Let $B$ be an $n\times0$-matrix\footnote{Of course, there is only
one such $n\times0$-matrix (namely, the empty matrix).}. Let $D$ be a
$0\times0$-matrix\footnote{Of course, there is only one such $0\times0$-matrix
(namely, the empty matrix).}. Then, all three matrices $B$, $0_{0\times n}$
and $D$ are empty (in the sense that each of them has either $0$ rows or $0$
columns or both), and thus we have $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =A$. Hence, $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A$. Combined with $\det D=1$ (since $D$ is a $0\times
0$-matrix), this yields $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$.

Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have proven
that every $n\in\mathbb{N}$, every $n\times n$-matrix $A$, every $n\times
0$-matrix $B$ and every $0\times0$-matrix $D$ satisfy $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=0$. This completes the induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$ be positive. Assume that
Exercise \ref{exe.block2x2.tridet} holds for $m=M-1$. We need to prove that
Exercise \ref{exe.block2x2.tridet} holds for $m=M$.

Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let $B$ be an $n\times
M$-matrix. Let $D$ be an $M\times M$-matrix.

Write the $M\times M$-matrix $D$ in the form $D=\left(  d_{i,j}\right)
_{1\leq i\leq M,\ 1\leq j\leq M}$. Hence, Theorem \ref{thm.laplace.gen}
\textbf{(a)} (applied to $M$, $D$, $d_{i,j}$ and $M$ instead of $n$, $A$,
$a_{i,j}$ and $p$) shows that%
\begin{equation}
\det D=\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det\left(  D_{\sim
M,\sim q}\right)  \label{sol.block2x2.tridet.short.indstep.detD}%
\end{equation}
(since $M\in\left\{  1,2,\ldots,M\right\}  $ (because $M>0$)).

Write the $\left(  n+M\right)  \times\left(  n+M\right)  $-matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $ in the form $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\left(  u_{i,j}\right)  _{1\leq u\leq n+M,\ 1\leq v\leq n+M}$. Thus,%
\begin{equation}
u_{n+M,q}=0\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{  1,2,\ldots
,n\right\}  , \label{sol.block2x2.tridet.short.indstep.u=0}%
\end{equation}
and%
\begin{equation}
u_{n+M,n+q}=d_{M,q}\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{
1,2,\ldots,M\right\}  . \label{sol.block2x2.tridet.short.indstep.u=d}%
\end{equation}


Furthermore, for every $q\in\left\{  1,2,\ldots,M\right\}  $, we have%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }=\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  , \label{sol.block2x2.tridet.short.indstep.1}%
\end{equation}
where $B_{q}^{\prime}$ is the result of crossing out the $q$-th column in the
matrix $B$. (Draw the matrices and cross out the appropriate rows and columns
to see why this is true.)

But we have assumed that Exercise \ref{exe.block2x2.tridet} holds for $m=M-1$.
Hence, for every $q\in\left\{  1,2,\ldots,M\right\}  $, we can apply Exercise
\ref{exe.block2x2.tridet} to $M-1$, $B_{q}^{\prime}$ and $D_{\sim M,\sim q}$
instead of $m$, $B$ and $D$. As a result, for every $q\in\left\{
1,2,\ldots,M\right\}  $, we obtain%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  .
\]
Now, taking determinants in (\ref{sol.block2x2.tridet.short.indstep.1}), we
obtain%
\begin{align}
\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)   &
=\det\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right) \nonumber\\
&  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  .
\label{sol.block2x2.tridet.short.indstep.2}%
\end{align}


But $n+M>0$ and thus $n+M\in\left\{  1,2,\ldots,n+M\right\}  $. Thus, Theorem
\ref{thm.laplace.gen} \textbf{(a)} (applied to $n+M$, $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $, $u_{i,j}$ and $n+M$ instead of $n$, $A$, $a_{i,j}$ and $p$) shows
that%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right) \\
&  =\sum_{q=1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}u_{n+M,q}%
\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
\underbrace{u_{n+M,q}}_{\substack{=0\\\text{(by
(\ref{sol.block2x2.tridet.short.indstep.u=0}))}}}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq n\leq n+M\right) \\
&  =\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)
+q}0\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right)  }_{=0}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  =\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
u_{n+M,q}\det\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim q}\right) \\
&  =\sum_{q=1}^{M}\underbrace{\left(  -1\right)  ^{\left(  n+M\right)
+\left(  n+q\right)  }}_{\substack{=\left(  -1\right)  ^{M+q}\\\text{(since
}\left(  n+M\right)  +\left(  n+q\right)  \\=2n+M+q\equiv
M+q\operatorname{mod}2\text{)}}}\underbrace{u_{n+M,n+q}}_{\substack{=d_{M,q}%
\\\text{(by (\ref{sol.block2x2.tridet.short.indstep.u=d}))}}}\underbrace{\det
\left(  \left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  _{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)
}_{\substack{=\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  \\\text{(by
(\ref{sol.block2x2.tridet.short.indstep.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n+q\text{ for
}q\text{ in the sum}\right) \\
&  =\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det A\cdot\det\left(
D_{\sim M,\sim q}\right)  =\det A\cdot\underbrace{\sum_{q=1}^{M}\left(
-1\right)  ^{M+q}d_{M,q}\det\left(  D_{\sim M,\sim q}\right)  }%
_{\substack{=\det D\\\text{(by (\ref{sol.block2x2.tridet.short.indstep.detD}%
))}}}\\
&  =\det A\cdot\det D.
\end{align*}


Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have shown
that for every $n\in\mathbb{N}$, for every $n\times n$-matrix $A$, for every
$n\times M$-matrix $B$, and for every $M\times M$-matrix $D$, we have
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=M$. This completes the induction step.
Hence, Exercise \ref{exe.block2x2.tridet} is solved by induction.
\end{proof}
\end{vershort}

\begin{verlong}
Before we prove Exercise \ref{exe.block2x2.tridet}, we state a lemma (which is
just a straightforward formalization of an obvious fact):

\begin{lemma}
\label{lem.block2x2.tridet.last-row-minor} Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$ be an $n\times m$-matrix. Let $q\in\left\{  1,2,\ldots,m\right\}  $. For
every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}_{r}\left(
j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.

\textbf{(a)} We have $\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q}%
,\ldots,m}A=\left(  a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq m-1}$.

\textbf{(b)} If $n$ is positive, then $A_{\sim n,\sim q}=\left(
a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
m-1}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.block2x2.tridet.last-row-minor}.]We have
$q\in\left\{  1,2,\ldots,m\right\}  $, so that $1\leq q\leq m$, so that $1\leq
m$. Thus, $m-1\in\mathbb{N}$.

Define an $\left(  m-1\right)  $-tuple $\left(  u_{1},u_{2},\ldots
,u_{m-1}\right)  $ by $\left(  u_{1},u_{2},\ldots,u_{m-1}\right)  =\left(
1,2,\ldots,\widehat{q},\ldots,m\right)  $. Thus, for every $j\in\left\{
1,2,\ldots,m-1\right\}  $, we have%
\begin{equation}
u_{j}=%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
=\mathbf{d}_{q}\left(  j\right)
\label{pf.lem.block2x2.tridet.last-row-minor.uj}%
\end{equation}
(since $\mathbf{d}_{q}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
$ (by the definition of $\mathbf{d}_{q}\left(  j\right)  $)).

\textbf{(a)} We have%
\begin{align*}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,m}A  &
=\operatorname*{cols}\nolimits_{u_{1},u_{2},\ldots,u_{m-1}}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,m\right)  =\left(  u_{1},u_{2},\ldots,u_{m-1}\right)  \right) \\
&  =\left(  a_{i,u_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq m-1}%
\end{align*}
(by the definition of $\operatorname*{cols}\nolimits_{u_{1},u_{2}%
,\ldots,u_{m-1}}A$, since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$). Thus,%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,m}A=\left(
a_{i,u_{y}}\right)  _{1\leq i\leq n,\ 1\leq y\leq m-1}=\left(  a_{i,u_{j}%
}\right)  _{1\leq i\leq n,\ 1\leq j\leq m-1}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  i,j\right)  $).
Hence,%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,m}A=\left(
\underbrace{a_{i,u_{j}}}_{\substack{=a_{i,\mathbf{d}_{q}\left(  j\right)
}\\\text{(since }u_{j}=\mathbf{d}_{q}\left(  j\right)  \\\text{(by
(\ref{pf.lem.block2x2.tridet.last-row-minor.uj})))}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-1}=\left(  a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}.
\]
This proves Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(a)}.

\textbf{(b)} Assume that $n$ is positive. The definition of $A_{\sim n,\sim
q}$ yields%
\begin{align*}
A_{\sim n,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{n}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,m}A\\
&  =\operatorname*{sub}\nolimits_{1,2,\ldots,n-1}^{1,2,\ldots,\widehat{q}%
,\ldots,m}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots
,\widehat{n},\ldots,n\right)  =\left(  1,2,\ldots,n-1\right)  \right) \\
&  =\operatorname*{sub}\nolimits_{1,2,\ldots,n-1}^{u_{1},u_{2},\ldots,u_{m-1}%
}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,m\right)  =\left(  u_{1},u_{2},\ldots,u_{m-1}\right)  \right) \\
&  =\left(  a_{x,u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq m-1}%
\end{align*}
(by the definition of $\operatorname*{sub}\nolimits_{1,2,\ldots,n-1}%
^{u_{1},u_{2},\ldots,u_{m-1}}A$, since $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$). Thus,%
\[
A_{\sim n,\sim q}=\left(  a_{x,u_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq
m-1}=\left(  a_{i,u_{j}}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m-1}%
\]
(here, we renamed the index $\left(  x,y\right)  $ as $\left(  i,j\right)  $).
Hence,%
\[
A_{\sim n,\sim q}=\left(  \underbrace{a_{i,u_{j}}}_{\substack{=a_{i,\mathbf{d}%
_{q}\left(  j\right)  }\\\text{(since }u_{j}=\mathbf{d}_{q}\left(  j\right)
\\\text{(by (\ref{pf.lem.block2x2.tridet.last-row-minor.uj})))}}}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq m-1}=\left(  a_{i,\mathbf{d}_{q}\left(
j\right)  }\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m-1}.
\]
This proves Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(b)}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.block2x2.tridet}.]We shall prove Exercise
\ref{exe.block2x2.tridet} by induction over $m$:

\textit{Induction base:} Let $n\in\mathbb{N}$. Let $A$ be an $n\times
n$-matrix. Let $B$ be an $n\times0$-matrix\footnote{Of course, there is only
one such $n\times0$-matrix (namely, the empty matrix).}. Let $D$ be a
$0\times0$-matrix\footnote{Of course, there is only one such $0\times0$-matrix
(namely, the empty matrix).}. Then, the matrix $B$ has $0$ columns (since it
is an $n\times0$-matrix), whereas the matrix $0_{0\times n}$ has $0$ rows
(since it is a $0\times n$-matrix), and the matrix $D$ has $0$ rows (since it
is a $0\times0$-matrix). Thus, each of the three matrices $B$, $D$ and
$0_{0\times n}$ has either $0$ rows or $0$ columns. Therefore%
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =A \label{sol.block2x2.tridet.indbase.1}%
\end{equation}
\footnote{Here is a more formal \textit{proof of
(\ref{sol.block2x2.tridet.indbase.1}):} Write the $n\times n$-matrix $A$ in
the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
\par
Write the $n\times0$-matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq0}$.
\par
We have $0_{0\times n}=\left(  0\right)  _{1\leq i\leq0,\ 1\leq j\leq n}$ (by
the definition of $0_{0\times n}$).
\par
Write the $0\times0$-matrix $D$ in the form $D=\left(  d_{i,j}\right)  _{1\leq
i\leq0,\ 1\leq j\leq0}$.
\par
Now, (\ref{eq.def.block2x2.formal}) (applied to $n$, $0$, $0$, $0_{0\times n}$
and $0$ instead of $n^{\prime}$, $m$, $m^{\prime}$, $C$ and $c_{i,j}$) shows
that
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)   &  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+0,\ 1\leq j\leq n+0}\\
&  =\left(  \underbrace{%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
}_{\substack{=a_{i,j}\\\text{(since }i\leq n\text{ and }j\leq n\text{)}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }n+0=n\right) \\
&  =\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=A.
\end{align*}
This proves (\ref{sol.block2x2.tridet.indbase.1}).}. Hence,
\begin{equation}
\det\underbrace{\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  }_{=A}=\det A. \label{sol.block2x2.tridet.indbase.2}%
\end{equation}
But $D$ is a $0\times0$-matrix, and thus has determinant $\det D=1$. Hence,
$\det A\cdot\underbrace{\det D}_{=1}=\det A$. Compared with
(\ref{sol.block2x2.tridet.indbase.2}), this yields $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$.

Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have proven
that every $n\in\mathbb{N}$, every $n\times n$-matrix $A$, every $n\times
0$-matrix $B$ and every $0\times0$-matrix $D$ satisfy $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{0\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=0$. This completes the induction base.

\textit{Induction step:} Let $M\in\mathbb{N}$ be positive. Assume that
Exercise \ref{exe.block2x2.tridet} holds for $m=M-1$. We need to prove that
Exercise \ref{exe.block2x2.tridet} holds for $m=M$.

Let $n\in\mathbb{N}$. Let $A$ be an $n\times n$-matrix. Let $B$ be an $n\times
M$-matrix. Let $D$ be an $M\times M$-matrix. Let $U$ be the $\left(
n+M\right)  \times\left(  n+M\right)  $-matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $. Write the matrix $U$ in the form $U=\left(  u_{i,j}\right)
_{1\leq u\leq n+M,\ 1\leq v\leq n+M}$.

Write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

Write the $n\times M$-matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq M}$.

We have $0_{M\times n}=\left(  0\right)  _{1\leq i\leq M,\ 1\leq j\leq n}$ (by
the definition of $0_{M\times n}$).

Write the $M\times M$-matrix $D$ in the form $D=\left(  d_{i,j}\right)
_{1\leq i\leq M,\ 1\leq j\leq M}$.

We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq M}$, $0_{M\times
n}=\left(  0\right)  _{1\leq i\leq M,\ 1\leq j\leq n}$ and $D=\left(
d_{i,j}\right)  _{1\leq i\leq M,\ 1\leq j\leq M}$. Thus,
(\ref{eq.def.block2x2.formal}) (applied to $n$, $M$, $n$, $M$, $A$, $B$,
$0_{M\times n}$, $D$, $a_{i,j}$, $b_{i,j}$, $0$ and $d_{i,j}$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$,
$c_{i,j}$ and $d_{i,j}$) shows that
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}.
\label{sol.block2x2.tridet.indstep.U0}%
\end{equation}
Thus,%
\begin{align}
U  &  =\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}.
\label{sol.block2x2.tridet.indstep.U}%
\end{align}


Now, for every $q\in\left\{  1,2,\ldots,M\right\}  $, let us denote the
$n\times\left(  M-1\right)  $-matrix \newline$\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{q},\ldots,M}B$ by $B_{q}^{\prime}$. Thus,%
\[
B_{q}^{\prime}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots
,M}B\ \ \ \ \ \ \ \ \ \ \text{for every }q\in\left\{  1,2,\ldots,M\right\}  .
\]


Write the $\left(  n+M\right)  \times\left(  n+M\right)  $-matrix $U$ in the
form $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}$. Thus,%
\[
\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}=U=\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}%
\]
(by (\ref{sol.block2x2.tridet.indstep.U})). In other words,%
\begin{equation}
u_{i,j}=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\label{sol.block2x2.tridet.indstep.minor.pf.1}%
\end{equation}
for all $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+M\right\}  ^{2}$.

Now, it is easy to see that%
\begin{equation}
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }=\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  \label{sol.block2x2.tridet.indstep.minor}%
\end{equation}
for every $q\in\left\{  1,2,\ldots,M\right\}  $\ \ \ \ \footnote{\textit{Proof
of (\ref{sol.block2x2.tridet.indstep.minor}):} Let $q\in\left\{
1,2,\ldots,M\right\}  $. Notice that $M$ is positive, and thus $n+M$ is
positive. Also, $q\in\left\{  1,2,\ldots,M\right\}  $, so that $n+q\in\left\{
n+1,n+2,\ldots,n+M\right\}  \subseteq\left\{  1,2,\ldots,n+M\right\}  $.
\par
For every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}_{r}\left(
j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.
\par
Now, Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(b)} (applied to
$n+M$, $n+M$, $n+q$, $U$ and $u_{i,j}$ instead of $n$, $m$, $q$, $A$ and
$a_{i,j}$) yields%
\begin{equation}
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }=\left(  u_{i,\mathbf{d}%
_{n+q}\left(  j\right)  }\right)  _{1\leq i\leq n+M-1,\ 1\leq j\leq n+M-1}
\label{sol.block2x2.tridet.indstep.minor.pf.2}%
\end{equation}
(since $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq n+M}$).
\par
On the other hand, $D=\left(  d_{i,j}\right)  _{1\leq i\leq M,\ 1\leq j\leq
M}$. Hence, Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(b)}
(applied to $M$, $M$, $D$ and $d_{i,j}$ instead of $n$, $m$, $A$ and $a_{i,j}%
$) yields%
\begin{equation}
D_{\sim M,\sim q}=\left(  d_{i,\mathbf{d}_{q}\left(  j\right)  }\right)
_{1\leq i\leq M-1,\ 1\leq j\leq M-1} .
\label{sol.block2x2.tridet.indstep.minor.pf.3}%
\end{equation}
\par
Also, $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq M}$. Hence,
Lemma \ref{lem.block2x2.tridet.last-row-minor} \textbf{(a)} (applied to $n$,
$M$, $B$ and $b_{i,j}$ instead of $n$, $m$, $A$ and $a_{i,j}$) yields%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,M}B=\left(
b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
M-1}.
\]
Thus,%
\begin{equation}
B_{q}^{\prime}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots
,M}B=\left(  b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq M-1}. \label{sol.block2x2.tridet.indstep.minor.pf.4}%
\end{equation}
\par
Now, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$,
$B_{q}^{\prime}=\left(  b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq M-1}$, $0_{\left(  M-1\right)  \times n}=\left(
0\right)  _{1\leq i\leq M-1,\ 1\leq j\leq n}$ (by the definition of
$0_{\left(  M-1\right)  \times n}$) and $D_{\sim M,\sim q}=\left(
d_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq M-1,\ 1\leq j\leq
M-1}$. Thus, (\ref{eq.def.block2x2.formal}) (applied to $n$, $M-1$, $n$,
$M-1$, $A$, $B_{q}^{\prime}$, $0_{\left(  M-1\right)  \times n}$, $D_{\sim
M,\sim q}$, $a_{i,j}$, $b_{i,\mathbf{d}_{q}\left(  j\right)  }$, $0$ and
$d_{i,\mathbf{d}_{q}\left(  j\right)  }$ instead of $n$, $n^{\prime}$, $m$,
$m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$, $c_{i,j}$ and
$d_{i,j}$) shows that
\begin{equation}
\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M-1,\ 1\leq j\leq n+M-1}.
\label{sol.block2x2.tridet.indstep.minor.pf.6}%
\end{equation}
\par
Now, we shall prove that%
\begin{equation}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\label{sol.block2x2.tridet.indstep.minor.pf.8}%
\end{equation}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+M-1\right\}  ^{2}$.
\par
[\textit{Proof of (\ref{sol.block2x2.tridet.indstep.minor.pf.8}):} Let
$\left(  i,j\right)  \in\left\{  1,2,\ldots,n+M-1\right\}  ^{2}$. Thus,
$i\in\left\{  1,2,\ldots,n+M-1\right\}  $ and $j\in\left\{  1,2,\ldots
,n+M-1\right\}  $.
\par
The definition of $\mathbf{d}_{n+q}\left(  j\right)  $ yields $\mathbf{d}%
_{n+q}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j+1, & \text{if }j\geq n+q
\end{cases}
$. The definition of $\mathbf{d}_{q}\left(  j-n\right)  $ yields
$\mathbf{d}_{q}\left(  j-n\right)  =%
\begin{cases}
j-n, & \text{if }j-n<q;\\
j-n+1, & \text{if }j-n\geq q
\end{cases}
$. The definition of $\mathbf{d}_{q}\left(  j\right)  $ yields $\mathbf{d}%
_{q}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
$.
\par
We have%
\begin{align*}
\mathbf{d}_{n+q}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j+1, & \text{if }j\geq n+q
\end{cases}
=%
\begin{cases}
j, & \text{if }j-n<q;\\
j+1, & \text{if }j-n\geq q
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the condition }j<n+q\text{ is equivalent to }j-n<q\text{,}\\
\text{and since the condition }j\geq n+q\text{ is equivalent to }j-n\geq q
\end{array}
\right)  .
\end{align*}
Subtracting $n$ from both sides of this equality, we obtain
\begin{align*}
\mathbf{d}_{n+q}\left(  j\right)  -n  &  =%
\begin{cases}
j, & \text{if }j-n<q;\\
j+1, & \text{if }j-n\geq q
\end{cases}
-n=%
\begin{cases}
j-n, & \text{if }j-n<q;\\
j+1-n, & \text{if }j-n\geq q
\end{cases}
\\
&  =%
\begin{cases}
j-n, & \text{if }j-n<q;\\
j-n+1, & \text{if }j-n\geq q
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j+1-n=j-n+1\text{ in}\\
\text{the case when }j-n\geq q
\end{array}
\right) \\
&  =\mathbf{d}_{q}\left(  j-n\right)  .
\end{align*}
\par
Notice that%
\begin{align*}
\mathbf{d}_{n+q}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j+1, & \text{if }j\geq n+q
\end{cases}
\\
&  \geq%
\begin{cases}
j, & \text{if }j<n+q;\\
j, & \text{if }j\geq n+q
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1\geq j\text{ in the case when
}j\geq n+q\right) \\
&  =j.
\end{align*}
\par
We must be in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq n$.
\par
\textit{Case 2:} We have $i>n$.
\par
Let us first consider Case 1. In this case, we have $i\leq n$. We are in one
of the following two subcases:
\par
\textit{Subcase 1.1:} We have $j\leq n$.
\par
\textit{Subcase 1.2:} We have $j>n$.
\par
Let us first consider Subcase 1.1. In this Subcase, we have $j\leq n$. Now,
$j\leq n<n+q$ (since $n+\underbrace{q}_{>0}>n$) and $\mathbf{d}_{n+q}\left(
j\right)  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j-1, & \text{if }j\geq n+q
\end{cases}
=j$ (since $j<n+q$). Thus,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =u_{i,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{n+q}\left(  j\right)
=j\right) \\
&  =%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1})}\right) \\
&  =a_{i,j}%
\end{align*}
(since $i\leq n$ and $j\leq n$). Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq n\text{ and }j\leq
n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
1.1.
\par
Let us now consider Subcase 1.2. In this Subcase, we have $j>n$. Hence,
$\mathbf{d}_{n+q}\left(  j\right)  \geq j>n$. Now,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =%
\begin{cases}
a_{i,\mathbf{d}_{n+q}\left(  j\right)  } & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
b_{i,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n;\\
0, & \text{if }i>n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
d_{i-n,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i>n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1}), applied to }\left(
i,\mathbf{d}_{n+q}\left(  j\right)  \right)  \text{ instead of }\left(
i,j\right)  \right) \\
&  =b_{i,\mathbf{d}_{n+q}\left(  j\right)  -n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\leq n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  >n\right) \\
&  =b_{i,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathbf{d}_{n+q}\left(  j\right)  -n=\mathbf{d}_{q}\left(
j-n\right)  \right)  .
\end{align*}
Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=b_{i,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\leq n\text{ and }j>n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
1.2.
\par
We have thus proven (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) in each of
the two Subcases 1.1 and 1.2. Since these two Subcases cover the whole Case 1,
this shows that (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) holds in Case
1.
\par
Let us now consider Case 2. In this case, we have $i>n$. We are in one of the
following two subcases:
\par
\textit{Subcase 2.1:} We have $j\leq n$.
\par
\textit{Subcase 2.2:} We have $j>n$.
\par
Let us first consider Subcase 2.1. In this Subcase, we have $j\leq n$. Now,
$j\leq n<n+q$ (since $n+\underbrace{q}_{>0}>n$) and $\mathbf{d}_{n+q}\left(
j\right)  =%
\begin{cases}
j, & \text{if }j<n+q;\\
j-1, & \text{if }j\geq n+q
\end{cases}
=j$ (since $j<n+q$). Thus,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =u_{i,j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{n+q}\left(  j\right)
=j\right) \\
&  =%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,j-n}, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1})}\right) \\
&  =0
\end{align*}
(since $i>n$ and $j\leq n$). Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i>n\text{ and }j\leq n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
2.1.
\par
Let us now consider Subcase 2.2. In this Subcase, we have $j>n$. Hence,
$\mathbf{d}_{n+q}\left(  j\right)  \geq j>n$. Now,%
\begin{align*}
u_{i,\mathbf{d}_{n+q}\left(  j\right)  }  &  =%
\begin{cases}
a_{i,\mathbf{d}_{n+q}\left(  j\right)  } & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
b_{i,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i\leq n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n;\\
0, & \text{if }i>n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  \leq n;\\
d_{i-n,\mathbf{d}_{n+q}\left(  j\right)  -n}, & \text{if }i>n\text{ and
}\mathbf{d}_{n+q}\left(  j\right)  >n
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.block2x2.tridet.indstep.minor.pf.1}), applied to }\left(
i,\mathbf{d}_{n+q}\left(  j\right)  \right)  \text{ instead of }\left(
i,j\right)  \right) \\
&  =d_{i-n,\mathbf{d}_{n+q}\left(  j\right)  -n}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i>n\text{ and }\mathbf{d}_{n+q}\left(  j\right)  >n\right) \\
&  =d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathbf{d}_{n+q}\left(  j\right)  -n=\mathbf{d}_{q}\left(
j-n\right)  \right)  .
\end{align*}
Compared with%
\[%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
=d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i>n\text{ and }j>n\right)  ,
\]
this shows that $u_{i,\mathbf{d}_{n+q}\left(  j\right)  }=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
$. Hence, (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) is proven in Subcase
2.2.
\par
We have thus proven (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) in each of
the two Subcases 2.1 and 2.2. Since these two Subcases cover the whole Case 2,
this shows that (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) holds in Case
2.
\par
Thus, we have shown that (\ref{sol.block2x2.tridet.indstep.minor.pf.8}) holds
in each of the two Cases 1 and 2. Hence,
(\ref{sol.block2x2.tridet.indstep.minor.pf.8}) always holds. This completes
the proof of (\ref{sol.block2x2.tridet.indstep.minor.pf.8}).]
\par
Now, (\ref{sol.block2x2.tridet.indstep.minor.pf.2}) becomes%
\begin{align*}
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }  &  =\left(
\underbrace{u_{i,\mathbf{d}_{n+q}\left(  j\right)  }}_{\substack{=%
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\\\text{(by (\ref{sol.block2x2.tridet.indstep.minor.pf.8}))}}}\right)  _{1\leq
i\leq n+M-1,\ 1\leq j\leq n+M-1}\\
&  =\left(
\begin{cases}
a_{i,j} & \text{if }i\leq n\text{ and }j\leq n;\\
b_{i,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i\leq n\text{ and
}j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
d_{i-n,\mathbf{d}_{q}\left(  j-n\right)  }, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+M-1,\ 1\leq j\leq n+M-1}\\
&  =\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)
\end{align*}
(by (\ref{sol.block2x2.tridet.indstep.minor.pf.6})). This proves
(\ref{sol.block2x2.tridet.indstep.minor}).}. Hence, for every $q\in\left\{
1,2,\ldots,M\right\}  $, we have%
\begin{equation}
\det\left(  U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)
=\det A\cdot\det\left(  D_{\sim M,\sim q}\right)
\label{sol.block2x2.tridet.indstep.minor-det}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.block2x2.tridet.indstep.minor-det}):} Let
$q\in\left\{  1,2,\ldots,M\right\}  $. We have assumed that Exercise
\ref{exe.block2x2.tridet} holds for $m=M-1$. Hence, we can apply Exercise
\ref{exe.block2x2.tridet} to $M-1$, $B_{q}^{\prime}$ and $D_{\sim M,\sim q}$
instead of $m$, $B$ and $D$. As a result, we obtain%
\[
\det\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  .
\]
Since $\left(
\begin{array}
[c]{cc}%
A & B_{q}^{\prime}\\
0_{\left(  M-1\right)  \times n} & D_{\sim M,\sim q}%
\end{array}
\right)  =U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }$, this
rewrites as $\det\left(  U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)
}\right)  =\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  $. This proves
(\ref{sol.block2x2.tridet.indstep.minor-det}).}.

Furthermore, for every $q\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
u_{n+M,q}=0 \label{sol.block2x2.tridet.indstep.u=0}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.block2x2.tridet.indstep.u=0}):} Let
$q\in\left\{  1,2,\ldots,n\right\}  $. Thus, $q\leq n$. Also,
$n+\underbrace{M}_{>0}>n$. Now, (\ref{sol.block2x2.tridet.indstep.minor.pf.1})
(applied to $\left(  i,j\right)  =\left(  n+M,q\right)  $) yields%
\[
u_{n+M,q}=%
\begin{cases}
a_{n+M,q} & \text{if }n+M\leq n\text{ and }q\leq n;\\
b_{n+M,q-n}, & \text{if }n+M\leq n\text{ and }q>n;\\
0, & \text{if }n+M>n\text{ and }q\leq n;\\
d_{n+M-n,q-n}, & \text{if }n+M>n\text{ and }q>n
\end{cases}
=0
\]
(since $n+M>n$ and $q\leq n$). This proves
(\ref{sol.block2x2.tridet.indstep.u=0}).}. On the other hand, for every
$q\in\left\{  1,2,\ldots,M\right\}  $, we have%
\begin{equation}
u_{n+M,n+q}=d_{M,q} \label{sol.block2x2.tridet.indstep.u=d}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.block2x2.tridet.indstep.u=d}):} Let
$q\in\left\{  1,2,\ldots,M\right\}  $. Thus, $q>0$, so that $n+\underbrace{q}%
_{>0}>n$. Also, $n+\underbrace{M}_{>0}>n$. Now,
(\ref{sol.block2x2.tridet.indstep.minor.pf.1}) (applied to $\left(
i,j\right)  =\left(  n+M,n+q\right)  $) yields%
\begin{align*}
u_{n+M,n+q}  &  =%
\begin{cases}
a_{n+M,n+q} & \text{if }n+M\leq n\text{ and }n+q\leq n;\\
b_{n+M,n+q-n}, & \text{if }n+M\leq n\text{ and }n+q>n;\\
0, & \text{if }n+M>n\text{ and }n+q\leq n;\\
d_{n+M-n,n+q-n}, & \text{if }n+M>n\text{ and }n+q>n
\end{cases}
\\
&  =d_{n+M-n,n+q-n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n+M>n\text{ and
}n+q>n\right) \\
&  =d_{M,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n+M-n=M\text{ and
}n+q-n=q\right)  ,
\end{align*}
This proves (\ref{sol.block2x2.tridet.indstep.u=d}).}.

Now, recall that $U=\left(  u_{i,j}\right)  _{1\leq i\leq n+M,\ 1\leq j\leq
n+M}$ and $n+M\in\left\{  1,2,\ldots,n+M\right\}  $ (since $n+M$ is positive).
Hence, Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to $n+M$, $U$,
$u_{i,j}$ and $n+M$ instead of $n$, $A$, $a_{i,j}$ and $p$) shows that%
\begin{align}
\det U  &  =\sum_{q=1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)
+q}u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
\underbrace{u_{n+M,q}}_{\substack{=0\\\text{(by
(\ref{sol.block2x2.tridet.indstep.u=0}))}}}\det\left(  U_{\sim\left(
n+M\right)  ,\sim q}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right)
\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }0\leq n\leq n+M\right) \nonumber\\
&  =\underbrace{\sum_{q=1}^{n}\left(  -1\right)  ^{\left(  n+M\right)
+q}0\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right)  }_{=0}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(
n+M\right)  +q}u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right)
\nonumber\\
&  =\sum_{q=n+1}^{n+M}\left(  -1\right)  ^{\left(  n+M\right)  +q}%
u_{n+M,q}\det\left(  U_{\sim\left(  n+M\right)  ,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{M}\underbrace{\left(  -1\right)  ^{\left(  n+M\right)
+\left(  n+q\right)  }}_{\substack{=\left(  -1\right)  ^{M+q}\\\text{(since
}\left(  n+M\right)  +\left(  n+q\right)  =2n+M+q\equiv M+q\operatorname{mod}%
2\text{)}}}\underbrace{u_{n+M,n+q}}_{\substack{=d_{M,q}\\\text{(by
(\ref{sol.block2x2.tridet.indstep.u=d}))}}}\underbrace{\det\left(
U_{\sim\left(  n+M\right)  ,\sim\left(  n+q\right)  }\right)  }%
_{\substack{=\det A\cdot\det\left(  D_{\sim M,\sim q}\right)  \\\text{(by
(\ref{sol.block2x2.tridet.indstep.minor-det}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n+q\text{ for
}q\text{ in the sum}\right) \nonumber\\
&  =\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det A\cdot\det\left(
D_{\sim M,\sim q}\right) \nonumber\\
&  =\det A\cdot\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det\left(
D_{\sim M,\sim q}\right)  . \label{sol.block2x2.tridet.indstep.detU}%
\end{align}


On the other hand, $D=\left(  d_{i,j}\right)  _{1\leq i\leq M,\ 1\leq j\leq
M}$ and $M\in\left\{  1,2,\ldots,M\right\}  $ (since $M>0$). Hence, Theorem
\ref{thm.laplace.gen} \textbf{(a)} (applied to $M$, $D$, $d_{i,j}$ and $M$
instead of $n$, $A$, $a_{i,j}$ and $p$) shows that%
\[
\det D=\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}d_{M,q}\det\left(  D_{\sim
M,\sim q}\right)  .
\]
Thus, (\ref{sol.block2x2.tridet.indstep.detU}) becomes%
\[
\det U=\det A\cdot\underbrace{\sum_{q=1}^{M}\left(  -1\right)  ^{M+q}%
d_{M,q}\det\left(  D_{\sim M,\sim q}\right)  }_{=\det D}=\det A\cdot\det D.
\]
Since $U=\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  $, this rewrites as $\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\det A\cdot\det D$.

Now, let us forget that we fixed $n$, $A$, $B$ and $D$. We thus have shown
that for every $n\in\mathbb{N}$, for every $n\times n$-matrix $A$, for every
$n\times M$-matrix $B$, and for every $M\times M$-matrix $D$, we have
$\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{M\times n} & D
\end{array}
\right)  =\det A\cdot\det D$. In other words, Exercise
\ref{exe.block2x2.tridet} holds for $m=M$. This completes the induction step.
Hence, Exercise \ref{exe.block2x2.tridet} is solved by induction.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.block2x2.tridet.transposed}}

There are two ways to solve Exercise \ref{exe.block2x2.tridet.transposed}: One
way is to essentially repeat our above solution to Exercise
\ref{exe.block2x2.tridet} with some straightforward modifications (for
example, we must use Theorem \ref{thm.laplace.gen} \textbf{(b)} instead of
Theorem \ref{thm.laplace.gen} \textbf{(a)}). Another way is to derive Exercise
\ref{exe.block2x2.tridet.transposed} from Exercise \ref{exe.block2x2.tridet}
using transpose matrices. Let me show the latter way. We begin with a simple lemma:

\begin{lemma}
\label{lem.block2x2.transpose}Let $n$, $n^{\prime}$, $m$ and $m^{\prime}$ be
four nonnegative integers. Let $A\in\mathbb{K}^{n\times m}$, $B\in
\mathbb{K}^{n\times m^{\prime}}$, $C\in\mathbb{K}^{n^{\prime}\times m}$ and
$D\in\mathbb{K}^{n^{\prime}\times m^{\prime}}$. Then,%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  ^{T}=\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
B^{T} & D^{T}%
\end{array}
\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.block2x2.transpose}.]Lemma
\ref{lem.block2x2.transpose} results in a straightforward way by recalling the
definitions of $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  ^{T}$ and $\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
B^{T} & D^{T}%
\end{array}
\right)  $ and comparing.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.block2x2.transpose}.]Write the $n\times m$-matrix $A$
in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.
Thus, $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$ (by the
definition of $A^{T}$).

Write the $n\times m^{\prime}$-matrix $B$ in the form $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$. Thus,
$B^{T}=\left(  b_{j,i}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq n}$ (by
the definition of $B^{T}$).

Write the $n^{\prime}\times m$-matrix $C$ in the form $C=\left(
c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$. Thus,
$C^{T}=\left(  c_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n^{\prime}}$ (by
the definition of $C^{T}$).

Write the $n^{\prime}\times m^{\prime}$-matrix $D$ in the form $D=\left(
d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}$. Thus,
$D^{T}=\left(  d_{j,i}\right)  _{1\leq i\leq m^{\prime},\ 1\leq j\leq
n^{\prime}}$ (by the definition of $D^{T}$).

We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$,
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$,
$C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$ and
$D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}%
$. Hence, (\ref{eq.def.block2x2.formal}) yields%
\[
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\]
Hence, the definition of the transpose matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  ^{T}$ yields%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  ^{T}\nonumber\\
&  =\left(
\begin{cases}
a_{j,i}, & \text{if }j\leq n\text{ and }i\leq m;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
c_{j-n,i}, & \text{if }j>n\text{ and }i\leq m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq n+n^{\prime}}.
\label{pf.lem.block2x2.transpose.1}%
\end{align}


On the other hand, $A^{T}=\left(  a_{j,i}\right)  _{1\leq i\leq m,\ 1\leq
j\leq n}$, $C^{T}=\left(  c_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq
n^{\prime}}$, $B^{T}=\left(  b_{j,i}\right)  _{1\leq i\leq m^{\prime},\ 1\leq
j\leq n}$ and $D^{T}=\left(  d_{j,i}\right)  _{1\leq i\leq m^{\prime},\ 1\leq
j\leq n^{\prime}}$. Hence, (\ref{eq.def.block2x2.formal}) (applied to $m$,
$m^{\prime}$, $n$, $n^{\prime}$, $A^{T}$, $C^{T}$, $B^{T}$, $D^{T}$, $a_{j,i}%
$, $c_{j,i}$, $b_{j,i}$ and $d_{j,i}$ instead of $n$, $n^{\prime}$, $m$,
$m^{\prime}$, $A$, $B$, $C$, $D$, $a_{i,j}$, $b_{i,j}$, $c_{i,j}$ and
$d_{i,j}$) yields%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
B^{T} & D^{T}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
a_{j,i}, & \text{if }i\leq m\text{ and }j\leq n;\\
c_{j-n,i}, & \text{if }i\leq m\text{ and }j>n;\\
b_{j,i-m}, & \text{if }i>m\text{ and }j\leq n;\\
d_{j-n,i-m}, & \text{if }i>m\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq n+n^{\prime}}.
\label{pf.lem.block2x2.transpose.2}%
\end{align}


But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,m+m^{\prime}\right\}
\times\left\{  1,2,\ldots,n+n^{\prime}\right\}  $ satisfies%
\begin{align*}
&
\begin{cases}
a_{j,i}, & \text{if }j\leq n\text{ and }i\leq m;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
c_{j-n,i}, & \text{if }j>n\text{ and }i\leq m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\\
&  =%
\begin{cases}
a_{j,i}, & \text{if }j\leq n\text{ and }i\leq m;\\
c_{j-n,i}, & \text{if }j>n\text{ and }i\leq m;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have switched the second and the
third cases}\right) \\
&  =%
\begin{cases}
a_{j,i}, & \text{if }i\leq m\text{ and }j\leq n;\\
c_{j-n,i}, & \text{if }j>n\text{ and }i\leq m;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  j\leq n\text{ and }i\leq
m\right)  \text{ is equivalent to }\left(  i\leq m\text{ and }j\leq n\right)
\right) \\
&  =%
\begin{cases}
a_{j,i}, & \text{if }i\leq m\text{ and }j\leq n;\\
c_{j-n,i}, & \text{if }i\leq m\text{ and }j>n;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  j>n\text{ and }i\leq
m\right)  \text{ is equivalent to }\left(  i\leq m\text{ and }j>n\right)
\right)
\end{align*}%
\begin{align*}
&  =%
\begin{cases}
a_{j,i}, & \text{if }i\leq m\text{ and }j\leq n;\\
c_{j-n,i}, & \text{if }i\leq m\text{ and }j>n;\\
b_{j,i-m}, & \text{if }i>m\text{ and }j\leq n;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  j\leq n\text{ and
}i>m\right)  \text{ is equivalent to }\left(  i>m\text{ and }j\leq n\right)
\right) \\
&  =%
\begin{cases}
a_{j,i}, & \text{if }i\leq m\text{ and }j\leq n;\\
c_{j-n,i}, & \text{if }i\leq m\text{ and }j>n;\\
b_{j,i-m}, & \text{if }i>m\text{ and }j\leq n;\\
d_{j-n,i-m}, & \text{if }i>m\text{ and }j>n
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  j>n\text{ and }i>m\right)
\text{ is equivalent to }\left(  i>m\text{ and }j>n\right)  \right)  .
\end{align*}
In other words,%
\begin{align*}
&  \left(
\begin{cases}
a_{j,i}, & \text{if }j\leq n\text{ and }i\leq m;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
c_{j-n,i}, & \text{if }j>n\text{ and }i\leq m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq n+n^{\prime}}\\
&  =\left(
\begin{cases}
a_{j,i}, & \text{if }i\leq m\text{ and }j\leq n;\\
c_{j-n,i}, & \text{if }i\leq m\text{ and }j>n;\\
b_{j,i-m}, & \text{if }i>m\text{ and }j\leq n;\\
d_{j-n,i-m}, & \text{if }i>m\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq n+n^{\prime}}.
\end{align*}
Comparing this with (\ref{pf.lem.block2x2.transpose.2}), we obtain%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
B^{T} & D^{T}%
\end{array}
\right)   &  =\left(
\begin{cases}
a_{j,i}, & \text{if }j\leq n\text{ and }i\leq m;\\
b_{j,i-m}, & \text{if }j\leq n\text{ and }i>m;\\
c_{j-n,i}, & \text{if }j>n\text{ and }i\leq m;\\
d_{j-n,i-m}, & \text{if }j>n\text{ and }i>m
\end{cases}
\right)  _{1\leq i\leq m+m^{\prime},\ 1\leq j\leq n+n^{\prime}}\\
&  =\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  ^{T}%
\end{align*}
(by (\ref{pf.lem.block2x2.transpose.1})). This proves Lemma
\ref{lem.block2x2.transpose}.
\end{proof}
\end{verlong}

Now, we can comfortably solve Exercise \ref{exe.block2x2.tridet.transposed}:

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.tridet.transposed}.]Exercise
\ref{exe.ps4.4} (applied to $n+m$ and $\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  $ instead of $n$ and $A$) shows that
\[
\det\left(  \left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  ^{T}\right)  =\det\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  .
\]
Hence,%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right) \\
&  =\det\left(  \underbrace{\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  ^{T}}_{\substack{=\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
\left(  0_{n\times m}\right)  ^{T} & D^{T}%
\end{array}
\right)  \\\text{(by Lemma \ref{lem.block2x2.transpose} (applied to }n\text{,
}m\text{, }n\text{, }m\\\text{and }0_{n\times m}\text{ instead of }n\text{,
}n^{\prime}\text{, }m\text{, }m^{\prime}\text{ and }B\text{))}}}\right)
=\det\underbrace{\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
\left(  0_{n\times m}\right)  ^{T} & D^{T}%
\end{array}
\right)  }_{\substack{=\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
0_{m\times n} & D^{T}%
\end{array}
\right)  \\\text{(since }\left(  0_{n\times m}\right)  ^{T}=0_{m\times
n}\text{)}}}\\
&  =\det\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
0_{m\times n} & D^{T}%
\end{array}
\right)  =\underbrace{\det\left(  A^{T}\right)  }_{\substack{=\det
A\\\text{(by Exercise \ref{exe.ps4.4})}}}\cdot\underbrace{\det\left(
D^{T}\right)  }_{\substack{=\det D\\\text{(by Exercise \ref{exe.ps4.4}%
}\\\text{(applied to }m\text{ and }D\text{ instead}\\\text{of }n\text{ and
}A\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.block2x2.tridet}
(applied to }A^{T}\text{, }C^{T}\text{ and }D^{T}\text{ instead of }A\text{,
}B\text{ and }D\text{)}\right) \\
&  =\det A\cdot\det D.
\end{align*}
This solves Exercise \ref{exe.block2x2.tridet.transposed}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.block2x2.tridet.transposed}.]The matrix $A$ is
an $n\times n$-matrix; hence, its transpose $A^{T}$ is an $n\times n$-matrix.
In other words, $A^{T}\in\mathbb{K}^{n\times n}$.

The matrix $C$ is an $m\times n$-matrix; hence, its transpose $C^{T}$ is an
$n\times m$-matrix. In other words, $C^{T}\in\mathbb{K}^{n\times m}$.

The matrix $D$ is an $m\times m$-matrix; hence, its transpose $D^{T}$ is an
$m\times m$-matrix. In other words, $D^{T}\in\mathbb{K}^{m\times m}$.

Clearly, $\left(  0_{n\times m}\right)  ^{T}=0_{m\times n}$.

The matrix $\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  $ is an $\left(  n+m\right)  \times\left(  n+m\right)  $-matrix.
Hence, Exercise \ref{exe.ps4.4} (applied to $n+m$ and $\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  $ insead of $n$ and $A$) shows that
\[
\det\left(  \left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  ^{T}\right)  =\det\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  .
\]
Hence,%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right) \\
&  =\det\left(  \underbrace{\left(
\begin{array}
[c]{cc}%
A & 0_{n\times m}\\
C & D
\end{array}
\right)  ^{T}}_{\substack{=\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
\left(  0_{n\times m}\right)  ^{T} & D^{T}%
\end{array}
\right)  \\\text{(by Lemma \ref{lem.block2x2.transpose} (applied to }n\text{,
}m\text{, }n\text{, }m\\\text{and }0_{n\times m}\text{ instead of }n\text{,
}n^{\prime}\text{, }m\text{, }m^{\prime}\text{ and }B\text{))}}}\right)
=\det\underbrace{\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
\left(  0_{n\times m}\right)  ^{T} & D^{T}%
\end{array}
\right)  }_{\substack{=\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
0_{m\times n} & D^{T}%
\end{array}
\right)  \\\text{(since }\left(  0_{n\times m}\right)  ^{T}=0_{m\times
n}\text{)}}}\\
&  =\det\left(
\begin{array}
[c]{cc}%
A^{T} & C^{T}\\
0_{m\times n} & D^{T}%
\end{array}
\right)  =\underbrace{\det\left(  A^{T}\right)  }_{\substack{=\det
A\\\text{(by Exercise \ref{exe.ps4.4})}}}\cdot\underbrace{\det\left(
D^{T}\right)  }_{\substack{=\det D\\\text{(by Exercise \ref{exe.ps4.4}%
}\\\text{(applied to }m\text{ and }D\text{ instead}\\\text{of }n\text{ and
}A\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Exercise \ref{exe.block2x2.tridet}
(applied to }A^{T}\text{, }C^{T}\text{ and }D^{T}\text{ instead of }A\text{,
}B\text{ and }D\text{)}\right) \\
&  =\det A\cdot\det D.
\end{align*}
This solves Exercise \ref{exe.block2x2.tridet.transposed}.
\end{proof}
\end{verlong}

\subsection{Second solution to Exercise \ref{exe.ps4.5}}

\begin{proof}
[Second solution to Exercise \ref{exe.ps4.5} (sketched).]\textbf{(b)} We have%
\begin{align*}
&  \det\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
f & 0 & 0 & 0 & g\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
\ell & m & n & o & p
\end{array}
\right) \\
&  =-\det\left(
\begin{array}
[c]{ccccc}%
a & b & c & d & e\\
\ell & m & n & o & p\\
h & 0 & 0 & 0 & i\\
j & 0 & 0 & 0 & k\\
f & 0 & 0 & 0 & g
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(a)}, because we have just}\\
\text{switched the }2\text{-nd and the }5\text{-th rows of the matrix}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{ccccc}%
d & b & c & a & e\\
o & m & n & \ell & p\\
0 & 0 & 0 & h & i\\
0 & 0 & 0 & j & k\\
0 & 0 & 0 & f & g
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(b)}, because we have just}\\
\text{switched the }1\text{-st and the }4\text{-th columns of the matrix}%
\end{array}
\right) \\
&  =\underbrace{\det\left(
\begin{array}
[c]{ccc}%
d & b & c\\
o & m & n\\
0 & 0 & 0
\end{array}
\right)  }_{\substack{=0\\\text{(by Exercise \ref{exe.ps4.6} \textbf{(c)})}%
}}\cdot\det\left(
\begin{array}
[c]{cc}%
j & k\\
f & g
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.block2x2.tridet}, applied to }\left(
\begin{array}
[c]{ccc}%
d & b & c\\
o & m & n\\
0 & 0 & 0
\end{array}
\right)  \text{,}\\
\left(
\begin{array}
[c]{cc}%
a & e\\
\ell & p\\
h & i
\end{array}
\right)  \text{, }\left(
\begin{array}
[c]{cc}%
j & k\\
f & g
\end{array}
\right)  \text{, }2\text{ and }3\text{ instead of }A\text{, }B\text{,
}D\text{, }m\text{ and }n
\end{array}
\right) \\
&  =0.
\end{align*}
This solves Exercise \ref{exe.ps4.5} \textbf{(b)}.

\textbf{(a)} We have%
\begin{align*}
&  \det\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
l & 0 & 0 & e\\
k & 0 & 0 & f\\
j & i & h & g
\end{array}
\right) \\
&  =-\det\left(
\begin{array}
[c]{cccc}%
a & b & c & d\\
j & i & h & g\\
k & 0 & 0 & f\\
l & 0 & 0 & e
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(a)}, because we have just}\\
\text{switched the }2\text{-nd and the }4\text{-th rows of the matrix}%
\end{array}
\right) \\
&  =\det\left(
\begin{array}
[c]{cccc}%
c & b & a & d\\
h & i & j & g\\
0 & 0 & k & f\\
0 & 0 & l & e
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.ps4.6} \textbf{(b)}, because we have just}\\
\text{switched the }1\text{-st and the }3\text{-th columns of the matrix}%
\end{array}
\right) \\
&  =\underbrace{\det\left(
\begin{array}
[c]{cc}%
c & b\\
h & i
\end{array}
\right)  }_{=ci-bh}\cdot\underbrace{\det\left(
\begin{array}
[c]{cc}%
k & f\\
l & e
\end{array}
\right)  }_{=ek-lf}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Exercise \ref{exe.block2x2.tridet}, applied to }\left(
\begin{array}
[c]{cc}%
c & b\\
h & i
\end{array}
\right)  \text{,}\\
\left(
\begin{array}
[c]{cc}%
a & d\\
j & g
\end{array}
\right)  \text{, }\left(
\begin{array}
[c]{cc}%
k & f\\
l & e
\end{array}
\right)  \text{, }2\text{ and }2\text{ instead of }A\text{, }B\text{,
}D\text{, }m\text{ and }n
\end{array}
\right) \\
&  =\left(  ci-bh\right)  \left(  ek-lf\right)  =\left(  bh-ci\right)  \left(
lf-ek\right)  .
\end{align*}
This solves Exercise \ref{exe.ps4.5} \textbf{(a)}. (Notice that we have
obtained the result in its factored form!)
\end{proof}

\subsection{Solution to Exercise \ref{exe.adj(AB)}}

\begin{vershort}
Before we start solving this exercise, let us show some lemmas. The first of
them is a (somewhat disguised) particular case of the Cauchy-Binet formula:
\end{vershort}

\begin{verlong}
Before we start solving this exercise, let us show some lemmas. The first of
them formalizes the (intuitively obvious) fact that the elements of a finite
set of integers can be listed in increasing order in exactly one way:

\begin{lemma}
\label{lem.adj(AB).set.increase}Let $n\in\mathbb{N}$. Let $a_{1},a_{2}%
,\ldots,a_{n}$ be $n$ integers such that $a_{1}<a_{2}<\cdots<a_{n}$. Let
$b_{1},b_{2},\ldots,b_{n}$ be $n$ integers such that $b_{1}<b_{2}<\cdots
<b_{n}$. Assume that $\left\{  a_{1},a_{2},\ldots,a_{n}\right\}  =\left\{
b_{1},b_{2},\ldots,b_{n}\right\}  $. Then, $\left(  a_{1},a_{2},\ldots
,a_{n}\right)  =\left(  b_{1},b_{2},\ldots,b_{n}\right)  $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).set.increase}.]We have $b_{1}<b_{2}%
<\cdots<b_{n}$. In other words, if $p$ and $q$ are two elements of $\left\{
1,2,\ldots,n\right\}  $ satisfying $p<q$, then%
\begin{equation}
b_{p}<b_{q}. \label{pf.lem.adj(AB).set.increase.binc}%
\end{equation}
Hence, if $p$ and $q$ are two elements of $\left\{  1,2,\ldots,n\right\}  $
satisfying $b_{p}=b_{q}$, then%
\begin{equation}
p=q \label{pf.lem.adj(AB).set.increase.bdj}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.adj(AB).set.increase.bdj}):} Let $p$
and $q$ be two elements of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$b_{p}=b_{q}$. If we had $p<q$, then we would have $b_{p}<b_{q}$ (by
(\ref{pf.lem.adj(AB).set.increase.binc})), which would contradict $b_{p}%
=b_{q}$. Thus, we cannot have $p<q$. In other words, we must have $p\geq q$.
If we had $q<p$, then we would have $b_{q}<b_{p}$ (by
(\ref{pf.lem.adj(AB).set.increase.binc}) (applied to $q$ and $p$ instead of
$p$ and $q$)), which would contradict $b_{q}=b_{p}$. Thus, we cannot have
$q<p$. In other words, we must have $q\geq p$. Combined with $p\geq q$, this
shows that $p=q$. This proves (\ref{pf.lem.adj(AB).set.increase.bdj}).}.

We define a map $A:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $ as follows:

Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $a_{i}\in\left\{  a_{1}%
,a_{2},\ldots,a_{n}\right\}  =\left\{  b_{1},b_{2},\ldots,b_{n}\right\}  $.
Hence, there exists a $j\in\left\{  1,2,\ldots,n\right\}  $ satisfying
$a_{i}=b_{j}$. Moreover, this $j$ is unique\footnote{\textit{Proof.} Let
$j_{1}$ and $j_{2}$ be two elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$. We shall show that $j_{1}=j_{2}$.
\par
We know that $j_{1}$ is an element $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$. In other words, $j_{1}$ is an element of $\left\{
1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j_{1}}$.
\par
We know that $j_{2}$ is an element $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$. In other words, $j_{2}$ is an element of $\left\{
1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j_{2}}$.
\par
We have $b_{j_{1}}=a_{i}=b_{j_{2}}$. Hence, $j_{1}=j_{2}$ (by
(\ref{pf.lem.adj(AB).set.increase.bdj}), applied to $p=j_{1}$ and $q=j_{2}$).
\par
Now, let us forget that we fixed $j_{1}$ and $j_{2}$. We thus have proven that
if $j_{1}$ and $j_{2}$ are two elements $j\in\left\{  1,2,\ldots,n\right\}  $
satisfying $a_{i}=b_{j}$, then $j_{1}=j_{2}$. In other words, any two elements
$j\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j}$ must be equal.
Hence, the $j\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j}$ is
unique (because we already know that such a $j$ exists). Qed.}. We define
$A\left(  i\right)  $ to be this $j$. Thus, $A\left(  i\right)  $ is the
unique $j\in\left\{  1,2,\ldots,n\right\}  $ satisfying $a_{i}=b_{j}$. In
other words, $A\left(  i\right)  $ is an element of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $a_{i}=b_{A\left(  i\right)  }$.

Thus, we have defined $A\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  $
for every $i\in\left\{  1,2,\ldots,n\right\}  $. In other words, we have
defined a map $A:\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $.

Thus, we have defined a map $A:\left\{  1,2,\ldots,n\right\}  \rightarrow
\left\{  1,2,\ldots,n\right\}  $ which satisfies%
\begin{equation}
\left(  a_{i}=b_{A\left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,\ldots,n\right\}  \right)  .
\label{pf.lem.adj(AB).set.increase.A}%
\end{equation}
An analogous construction (with $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $,
$\left(  b_{1},b_{2},\ldots,b_{n}\right)  $ and $A$ replaced by $\left(
b_{1},b_{2},\ldots,b_{n}\right)  $, $\left(  a_{1},a_{2},\ldots,a_{n}\right)
$ and $B$) allows us to construct a map $B:\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}  $ which satisfies%
\begin{equation}
\left(  b_{i}=a_{B\left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,\ldots,n\right\}  \right)  .
\label{pf.lem.adj(AB).set.increase.B}%
\end{equation}
Consider this map $B$.

We have $A\circ B=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, $a_{B\left(  i\right)
}=b_{A\left(  B\left(  i\right)  \right)  }$ (by
(\ref{pf.lem.adj(AB).set.increase.A}), applied to $B\left(  i\right)  $
instead of $i$). Hence, $b_{A\left(  B\left(  i\right)  \right)  }=a_{B\left(
i\right)  }=b_{i}$ (by (\ref{pf.lem.adj(AB).set.increase.B})). Thus, $A\left(
B\left(  i\right)  \right)  =i$ (by (\ref{pf.lem.adj(AB).set.increase.bdj}),
applied to $p=A\left(  B\left(  i\right)  \right)  $ and $q=i$). Thus,
$\left(  A\circ B\right)  \left(  i\right)  =A\left(  B\left(  i\right)
\right)  =i=\operatorname*{id}\left(  i\right)  $.
\par
Now, let us forget that we fixed $i$. Thus, we have shown that $\left(  A\circ
B\right)  \left(  i\right)  =\operatorname*{id}\left(  i\right)  $ for every
$i\in\left\{  1,2,\ldots,n\right\}  $. In other words, $A\circ
B=\operatorname*{id}$, qed.}. The same argument (but applied to $\left(
b_{1},b_{2},\ldots,b_{n}\right)  $, $\left(  a_{1},a_{2},\ldots,a_{n}\right)
$, $B$ and $A$ instead of $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $,
$\left(  b_{1},b_{2},\ldots,b_{n}\right)  $, $A$ and $B$) shows that $B\circ
A=\operatorname*{id}$.

The maps $A$ and $B$ are mutually inverse (since $A\circ B=\operatorname*{id}$
and $B\circ A=\operatorname*{id}$). Thus, the map $A$ is invertible. In other
words, the map $A$ is a bijection. Hence, $A$ is a bijection $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $. In other
words, $A$ is a permutation of the set $\left\{  1,2,\ldots,n\right\}  $. In
other words, $A\in S_{n}$ (since $S_{n}$ is the set of all permutations of the
set $\left\{  1,2,\ldots,n\right\}  $).

The integers $b_{1},b_{2},\ldots,b_{n}$ are distinct (because of
(\ref{pf.lem.adj(AB).set.increase.bdj})). Proposition \ref{prop.sorting}
\textbf{(c)} (applied to $\left(  b_{1},b_{2},\ldots,b_{n}\right)  $ instead
of $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $) thus shows that there is a
\textbf{unique} permutation $\sigma\in S_{n}$ such that $b_{\sigma\left(
1\right)  }<b_{\sigma\left(  2\right)  }<\cdots<b_{\sigma\left(  n\right)  }$.
Hence, there exists at most one such permutation. In other words, if
$\sigma_{1}$ and $\sigma_{2}$ are two permutations $\sigma\in S_{n}$ such that
$b_{\sigma\left(  1\right)  }<b_{\sigma\left(  2\right)  }<\cdots
<b_{\sigma\left(  n\right)  }$, then%
\begin{equation}
\sigma_{1}=\sigma_{2}. \label{pf.lem.adj(AB).set.increase.uni}%
\end{equation}


The permutation $\operatorname*{id}\in S_{n}$ satisfies $b_{\operatorname*{id}%
\left(  1\right)  }<b_{\operatorname*{id}\left(  2\right)  }<\cdots
<b_{\operatorname*{id}\left(  n\right)  }$\ \ \ \ \footnote{In fact, this is
just a way to rewrite the chain of inequalities $b_{1}<b_{2}<\cdots<b_{n}$
(which is true by assumption).}. In other words, the permutation
$\operatorname*{id}\in S_{n}$ is a permutation $\sigma\in S_{n}$ such that
$b_{\sigma\left(  1\right)  }<b_{\sigma\left(  2\right)  }<\cdots
<b_{\sigma\left(  n\right)  }$.

We have $a_{1}<a_{2}<\cdots<a_{n}$. This rewrites as $b_{A\left(  1\right)
}<b_{A\left(  2\right)  }<\cdots<b_{A\left(  n\right)  }$ (since
$a_{i}=b_{A\left(  i\right)  }$ for every $i\in\left\{  1,2,\ldots,n\right\}
$ (by (\ref{pf.lem.adj(AB).set.increase.A}))). Thus, $A$ is a permutation
$\sigma\in S_{n}$ such that $b_{\sigma\left(  1\right)  }<b_{\sigma\left(
2\right)  }<\cdots<b_{\sigma\left(  n\right)  }$.

Thus, we know that $A$ and $\operatorname*{id}$ are two permutations
$\sigma\in S_{n}$ such that $b_{\sigma\left(  1\right)  }<b_{\sigma\left(
2\right)  }<\cdots<b_{\sigma\left(  n\right)  }$. Hence,
(\ref{pf.lem.adj(AB).set.increase.uni}) (applied to $\sigma_{1}=A$ and
$\sigma_{2}=\operatorname*{id}$) shows that $A=\operatorname*{id}$.

But (\ref{pf.lem.adj(AB).set.increase.A}) shows that%
\begin{align*}
\left(  a_{1},a_{2},\ldots,a_{n}\right)   &  =\left(  b_{A\left(  1\right)
},b_{A\left(  2\right)  },\ldots,b_{A\left(  n\right)  }\right) \\
&  =\left(  b_{\operatorname*{id}\left(  1\right)  },b_{\operatorname*{id}%
\left(  2\right)  },\ldots,b_{\operatorname*{id}\left(  n\right)  }\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\operatorname*{id}\right) \\
&  =\left(  b_{1},b_{2},\ldots,b_{n}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }b_{\operatorname*{id}\left(  i\right)  }=b_{i}\text{ for every
}i\in\left\{  1,2,\ldots,n\right\}  \right)  .
\end{align*}
This proves Lemma \ref{lem.adj(AB).set.increase}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.adj(AB).cauchy-binet}Let $n$ be a positive integer. Let $A$ be an
$\left(  n-1\right)  \times n$-matrix. Let $B$ be an $n\times\left(
n-1\right)  $-matrix. Then,%
\[
\det\left(  AB\right)  =\sum_{k=1}^{n}\det\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}A\right)  \cdot\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}B\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).cauchy-binet}.]Let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $. Define a subset $\mathbf{I}$
of $\left[  n\right]  ^{n-1}$ by%
\[
\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)  \in\left[
n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots<k_{n-1}\right\}  .
\]


Theorem \ref{thm.cauchy-binet} (applied to $n-1$ and $n$ instead of $n$ and
$m$) yields%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n-1}\leq n}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}%
}B\right)  . \label{pf.lem.adj(AB).cauchy-binet.short.1}%
\end{align}
Recall that the summation sign $\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n-1}\leq
n-1}$ is an abbreviation for \newline$\sum_{\substack{\left(  g_{1}%
,g_{2},\ldots,g_{n-1}\right)  \in\left\{  1,2,\ldots,n\right\}  ^{n-1}%
;\\g_{1}<g_{2}<\cdots<g_{n-1}}}$, which can be rewritten as $\sum_{\left(
g_{1},g_{2},\ldots,g_{n-1}\right)  \in\mathbf{I}}$ (because the $\left(
n-1\right)  $-tuples $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left\{
1,2,\ldots,n\right\}  ^{n-1}$ satisfying $g_{1}<g_{2}<\cdots<g_{n-1}$ are
precisely the elements of $\mathbf{I}$). Therefore,
(\ref{pf.lem.adj(AB).cauchy-binet.short.1}) can be rewritten as%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\mathbf{I}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}%
}B\right)  . \label{pf.lem.adj(AB).cauchy-binet.short.2}%
\end{align}


Now, let us take a closer look at $\mathbf{I}$. The set $\mathbf{I}$ consists
of all $\left(  n-1\right)  $-tuples $\left(  k_{1},k_{2},\ldots
,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}$ satisfying $k_{1}<k_{2}%
<\cdots<k_{n-1}$. There are only $n$ such $\left(  n-1\right)  $-tuples:
namely, the $\left(  n-1\right)  $-tuples $\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ for $k\in\left\{  1,2,\ldots,n\right\}  $. This is
intuitively clear: If you want to choose an $\left(  n-1\right)  $-tuple
$\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  \in\mathbf{I}$, you can simply
decide which of the $n$ elements $1,2,\ldots,n$ you do \textbf{not} want to be
an entry of $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $, and then the
$\left(  n-1\right)  $-tuple $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $
will have to be the list of all the remaining $n-1$ elements of $\left\{
1,2,\ldots,n\right\}  $ in increasing order. Let us formalize this argument a
bit more:

For every $k\in\left\{  1,2,\ldots,n\right\}  $, we have $\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  \in\mathbf{I}$ (for obvious reasons).
Hence, we can define a map%
\[
\Phi:\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I}%
\]
by%
\[
\left(  \Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots,n\right\}
\right)  .
\]
Consider this map $\Phi$. This map $\Phi$ is
injective\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ be such that $\Phi\left(  i\right)
=\Phi\left(  j\right)  $. We shall show that $i=j$.
\par
The definition of $\Phi$ yields $\Phi\left(  i\right)  =\left(  1,2,\ldots
,\widehat{i},\ldots,n\right)  $. Hence, $i$ is the only element of $\left\{
1,2,\ldots,n\right\}  $ that does not appear in $\Phi\left(  i\right)  $.
Similarly, $j$ is the only element of $\left\{  1,2,\ldots,n\right\}  $ that
does not appear in $\Phi\left(  j\right)  $. In other words, $j$ is the only
element of $\left\{  1,2,\ldots,n\right\}  $ that does not appear in
$\Phi\left(  i\right)  $ (since $\Phi\left(  i\right)  =\Phi\left(  j\right)
$). Comparing this with the fact that $i$ is the only element of $\left\{
1,2,\ldots,n\right\}  $ that does not appear in $\Phi\left(  i\right)  $, we
conclude that $i=j$.
\par
Now, let us forget that we fixed $i$ and $j$. We thus have proven that if
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}
$ are such that $\Phi\left(  i\right)  =\Phi\left(  j\right)  $, then $i=j$.
In other words, the map $\Phi$ is injective.} and
surjective\footnote{\textit{Proof.} Let $\mathbf{g}\in\mathbf{I}$. We shall
show that $\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
We have $\mathbf{g}\in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots
,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots
<k_{n-1}\right\}  $. In other words, \textbf{$g$} can be written in the form
$\mathbf{g}=\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ for some $\left(
g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left[  n\right]  ^{n-1}$ satisfying
$g_{1}<g_{2}<\cdots<g_{n-1}$. Consider this $\left(  g_{1},g_{2}%
,\ldots,g_{n-1}\right)  $.
\par
The integers $g_{1},g_{2},\ldots,g_{n-1}$ are distinct (since $g_{1}%
<g_{2}<\cdots<g_{n-1}$). Thus, $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  $
is an $\left(  n-1\right)  $-element subset of $\left[  n\right]  $.
Therefore, its complement $\left[  n\right]  \setminus\left\{  g_{1}%
,g_{2},\ldots,g_{n-1}\right\}  $ is a $1$-element subset of $\left[  n\right]
$ (since $n-\left(  n-1\right)  =1$). In other words, $\left[  n\right]
\setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  =\left\{  k\right\}  $
for some $k\in\left[  n\right]  $. Consider this $k$.
\par
We have $k\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $. Since
$\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq\left[  n\right]  $, we
have%
\[
\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  =\left[  n\right]  \setminus
\underbrace{\left(  \left[  n\right]  \setminus\left\{  g_{1},g_{2}%
,\ldots,g_{n-1}\right\}  \right)  }_{=\left\{  k\right\}  }=\left[  n\right]
\setminus\left\{  k\right\}  .
\]
\par
Now, recall that $g_{1}<g_{2}<\cdots<g_{n-1}$. Hence, the $\left(  n-1\right)
$-tuple $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ is the list of all
elements of the set $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  $ in
increasing order. Since $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  =\left[
n\right]  \setminus\left\{  k\right\}  $, this rewrites as follows: The
$\left(  n-1\right)  $-tuple $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ is
the list of all elements of the set $\left[  n\right]  \setminus\left\{
k\right\}  $ in increasing order. But clearly the latter list is $\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  $. Thus, the $\left(  n-1\right)
$-tuple $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $ is the list $\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  $. In other words, $\left(
g_{1},g_{2},\ldots,g_{n-1}\right)  =\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $, so that%
\[
\mathbf{g}=\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  =\Phi\left(  k\right)  \in\Phi\left(  \left\{
1,2,\ldots,n\right\}  \right)  .
\]
\par
Now, let us forget that we fixed $\mathbf{g}$. We thus have proven that
$\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ for every
$\mathbf{g}\in\mathbf{I}$. In other words, $\mathbf{I}\subseteq\Phi\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. In other words, the map $\Phi$ is
surjective.}. Hence, the map $\Phi$ is a bijection. In other words, the map%
\[
\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I}%
,\ \ \ \ \ \ \ \ \ \ k\mapsto\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\]
is a bijection (since this map is precisely $\Phi$).

Now, (\ref{pf.lem.adj(AB).cauchy-binet.short.2}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\mathbf{I}}\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n-1}}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right) \\
&  =\underbrace{\sum_{k\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{k=1}^{n}%
}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}A\right)  \cdot\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  1,2,\ldots,\widehat{k},\ldots
,n\right)  \text{ for}\\
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \text{ in the sum, since the map}\\
\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I},\ k\mapsto\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  \text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{k=1}^{n}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}B\right)  .
\end{align*}
This proves Lemma \ref{lem.adj(AB).cauchy-binet}.
\end{proof}
\end{vershort}

\begin{verlong}
This lemma is a particular case of the Cauchy-Binet formula. Here is how it
can be derived from the latter, in detail:

\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).cauchy-binet}.]Let $\left[  n\right]  $
denote the set $\left\{  1,2,\ldots,n\right\}  $. Define a subset $\mathbf{I}$
of $\left[  n\right]  ^{n-1}$ by%
\[
\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)  \in\left[
n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots<k_{n-1}\right\}  .
\]


Theorem \ref{thm.cauchy-binet} (applied to $n-1$ and $n$ instead of $n$ and
$m$) yields%
\begin{align}
&  \det\left(  AB\right) \nonumber\\
&  =\underbrace{\sum_{1\leq g_{1}<g_{2}<\cdots<g_{n-1}\leq n}}%
_{\substack{=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\left\{  1,2,\ldots,n\right\}  ^{n-1};\\g_{1}<g_{2}<\cdots<g_{n-1}}%
}\\=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left[
n\right]  ^{n-1};\\g_{1}<g_{2}<\cdots<g_{n-1}}}\\\text{(since }\left\{
1,2,\ldots,n-1\right\}  =\left[  n\right]  \text{)}}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right)
\nonumber\\
&  =\underbrace{\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\left[  n\right]  ^{n-1};\\g_{1}<g_{2}<\cdots<g_{n-1}}}}_{\substack{=\sum
_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n-1}\right\}  }\\=\sum_{\left(  g_{1},g_{2},\ldots
,g_{n}\right)  \in\mathbf{I}}\\\text{(since }\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n-1}\right\}  =\mathbf{I}\text{)}}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right)
\nonumber\\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\mathbf{I}}\det\left(
\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}A\right)  \cdot
\det\left(  \operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{n-1}%
}B\right)  . \label{pf.lem.adj(AB).cauchy-binet.1}%
\end{align}


Now, for every $k\in\left\{  1,2,\ldots,n\right\}  $, we have%
\[
\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  \in\mathbf{I}%
\]
\footnote{\textit{Proof.} Let $k\in\left\{  1,2,\ldots,n\right\}  $. Let us
denote the $\left(  n-1\right)  $-tuple $\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ by $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  $. Thus,%
\begin{equation}
t_{i}=%
\begin{cases}
i, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-1\right\}  .
\label{pf.lem.adj(AB).cauchy-binet.wd.pf.1}%
\end{equation}
\par
Now, let $i\in\left\{  1,2,\ldots,n-2\right\}  $. We shall prove that
$t_{i}<t_{i+1}$. Indeed, let us assume the contrary (for the sake of
contradiction). Thus, $t_{i}\geq t_{i+1}$. But%
\begin{align*}
t_{i}  &  =%
\begin{cases}
i, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.adj(AB).cauchy-binet.wd.pf.1})}\right) \\
&  \leq%
\begin{cases}
i+1, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq i+1\text{ in the case when
}i<k\right) \\
&  =i+1,
\end{align*}
so that%
\begin{align*}
i+1  &  \geq t_{i}\geq t_{i+1}=%
\begin{cases}
i+1, & \text{if }i+1<k;\\
\left(  i+1\right)  +1, & \text{if }i+1\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.adj(AB).cauchy-binet.wd.pf.1}), applied to }i+1\text{ instead of
}i\right) \\
&  \geq%
\begin{cases}
i+1, & \text{if }i+1<k;\\
i+1, & \text{if }i+1\geq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  i+1\right)  +1\geq i+1\text{
in the case when }i+1\geq k\right) \\
&  =i+1.
\end{align*}
Combining $i+1\geq t_{i}$ with $t_{i}\geq i+1$, we obtain $i+1=t_{i}$. Hence,
if we had $i<k$, then we would have%
\[
i+1=t_{i}=%
\begin{cases}
i, & \text{if }i<k;\\
i+1, & \text{if }i\geq k
\end{cases}
=i\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i<k\right)  ,
\]
which would contradict $i+1\neq i$. Therefore, we cannot have $i<k$. Thus, we
have $i\geq k$. Thus, $i+1\geq k$. Now,%
\begin{align*}
t_{i+1}  &  =%
\begin{cases}
i+1, & \text{if }i+1<k;\\
\left(  i+1\right)  +1, & \text{if }i+1\geq k
\end{cases}
=\left(  i+1\right)  +1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i+1\geq
k\right) \\
&  >i+1=t_{i}\geq t_{i+1}.
\end{align*}
This is absurd. Thus, we have obtained a contradiction. This contradiction
proves that our assumption was wrong. Hence, $t_{i}<t_{i+1}$.
\par
Now, let us forget that we fixed $i$. We thus have shown that $t_{i}<t_{i+1}$
for every $i\in\left\{  1,2,\ldots,n-2\right\}  $. In other words,
$t_{1}<t_{2}<\cdots<t_{n-1}$.
\par
Also, $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  \in\left[  n\right]  ^{n-1}$. Hence, $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  $ is an element of $\left[  n\right]
^{n-1}$ and satisfies $t_{1}<t_{2}<\cdots<t_{n-1}$. In other words, $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  $ is an element $\left(  k_{1},k_{2}%
,\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}$ satisfying $k_{1}%
<k_{2}<\cdots<k_{n-1}$. In other words,
\[
\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  \in\left\{  \left(  k_{1}%
,k_{2},\ldots,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}%
<k_{2}<\cdots<k_{n-1}\right\}  =\mathbf{I}.
\]
Thus, $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  =\left(  t_{1}%
,t_{2},\ldots,t_{n-1}\right)  \in\mathbf{I}$, qed.}. Hence, we can define a
map%
\[
\Phi:\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I}%
\]
by%
\[
\left(  \Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots,n\right\}
\right)  .
\]
Consider this map $\Phi$. This map $\Phi$ is
injective\footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ be such that $\Phi\left(  i\right)
=\Phi\left(  j\right)  $. We shall show that $i=j$.
\par
The definition of $\Phi$ yields $\Phi\left(  i\right)  =\left(  1,2,\ldots
,\widehat{i},\ldots,n\right)  $. Hence,%
\begin{align*}
&  \left(  \text{the set of all entries of }\underbrace{\Phi\left(  i\right)
}_{=\left(  1,2,\ldots,\widehat{i},\ldots,n\right)  }\right) \\
&  =\left(  \text{the set of all entries of }\left(  1,2,\ldots,\widehat{i}%
,\ldots,n\right)  \right) \\
&  =\left\{  1,2,\ldots,\widehat{i},\ldots,n\right\}  =\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  i\right\}  .
\end{align*}
Thus,%
\begin{align*}
&  \left\{  1,2,\ldots,n\right\}  \setminus\underbrace{\left(  \text{the set
of all entries of }\Phi\left(  i\right)  \right)  }_{=\left\{  1,2,\ldots
,n\right\}  \setminus\left\{  i\right\}  }\\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left(  \left\{  1,2,\ldots
,n\right\}  \setminus\left\{  i\right\}  \right)  =\left\{  i\right\}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left\{  i\right\}  \subseteq\left\{
1,2,\ldots,n\right\}  \text{ (since }i\in\left\{  1,2,\ldots,n\right\}
\text{)}\right)  .
\end{align*}
The same argument (applied to $j$ instead of $i$) shows that%
\[
\left\{  1,2,\ldots,n\right\}  \setminus\left(  \text{the set of all entries
of }\Phi\left(  j\right)  \right)  =\left\{  j\right\}  .
\]
\par
Now,%
\begin{align*}
\left\{  i\right\}   &  =\left\{  1,2,\ldots,n\right\}  \setminus\left(
\text{the set of all entries of }\underbrace{\Phi\left(  i\right)  }%
_{=\Phi\left(  j\right)  }\right) \\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left(  \text{the set of all
entries of }\Phi\left(  j\right)  \right)  =\left\{  j\right\}  .
\end{align*}
Hence, $i\in\left\{  i\right\}  =\left\{  j\right\}  $, so that $i=j$.
\par
Now, let us forget that we fixed $i$ and $j$. We thus have proven that if
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}
$ are such that $\Phi\left(  i\right)  =\Phi\left(  j\right)  $, then $i=j$.
In other words, the map $\Phi$ is injective, qed.} and
surjective\footnote{\textit{Proof.} Let $\mathbf{g}\in\mathbf{I}$. We shall
show that $\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
We have%
\begin{align*}
\mathbf{g}  &  \in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots
,k_{n-1}\right)  \in\left[  n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots
<k_{n-1}\right\} \\
&  =\left\{  \left(  g_{1},g_{2},\ldots,g_{n-1}\right)  \in\left[  n\right]
^{n-1}\ \mid\ g_{1}<g_{2}<\cdots<g_{n-1}\right\}
\end{align*}
(here, we have renamed the index $\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $
as $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $). In other words,
\textbf{$g$} can be written in the form $\mathbf{g}=\left(  g_{1},g_{2}%
,\ldots,g_{n-1}\right)  $ for some $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\left[  n\right]  ^{n-1}$ satisfying $g_{1}<g_{2}<\cdots<g_{n-1}$. Consider
this $\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $.
\par
The integers $g_{1},g_{2},\ldots,g_{n-1}$ are distinct (since $g_{1}%
<g_{2}<\cdots<g_{n-1}$). Hence, they are $n-1$ distinct integers. In other
words, we have $\left\vert \left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}
\right\vert =n-1$. Also, each $i\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies $g_{i}\in\left[  n\right]  $ (since $\left(  g_{1},g_{2}%
,\ldots,g_{n-1}\right)  \in\left[  n\right]  ^{n-1}$). In other words,
$\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq\left[  n\right]  $.
Hence,%
\[
\left\vert \left[  n\right]  \setminus\left\{  g_{1},g_{2},\ldots
,g_{n-1}\right\}  \right\vert =\underbrace{\left\vert \left[  n\right]
\right\vert }_{=n}-\underbrace{\left\vert \left\{  g_{1},g_{2},\ldots
,g_{n-1}\right\}  \right\vert }_{=n-1}=n-\left(  n-1\right)  =1.
\]
In other words, $\left[  n\right]  \setminus\left\{  g_{1},g_{2}%
,\ldots,g_{n-1}\right\}  $ is a one-element set. Hence, the set $\left[
n\right]  \setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  $ has the form
$\left\{  k\right\}  $ for some object $k$. Consider this $k$.
\par
We have $\left[  n\right]  \setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}%
\right\}  =\left\{  k\right\}  $, so that $k\in\left\{  k\right\}  =\left[
n\right]  \setminus\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq
\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $.
\par
Let us denote the $\left(  n-1\right)  $-tuple $\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  $ by $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)
$. Thus,%
\begin{align*}
\left(  t_{1},t_{2},\ldots,t_{n-1}\right)   &  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right) \\
&  \in\mathbf{I}=\left\{  \left(  k_{1},k_{2},\ldots,k_{n-1}\right)
\in\left[  n\right]  ^{n-1}\ \mid\ k_{1}<k_{2}<\cdots<k_{n-1}\right\}  .
\end{align*}
In other words, $\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  $ is an element
$\left(  k_{1},k_{2},\ldots,k_{n-1}\right)  $ of $\left[  n\right]  ^{n-1}$
satisfying $k_{1}<k_{2}<\cdots<k_{n-1}$. In other words, $\left(  t_{1}%
,t_{2},\ldots,t_{n-1}\right)  $ is an element of $\left[  n\right]  ^{n-1}$
and satisfies $t_{1}<t_{2}<\cdots<t_{n-1}$.
\par
Clearly,
\begin{align*}
\left\{  t_{1},t_{2},\ldots,t_{n-1}\right\}   &  =\left(  \text{the set of all
entries of the list }\underbrace{\left(  t_{1},t_{2},\ldots,t_{n-1}\right)
}_{=\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  }\right) \\
&  =\left(  \text{the set of all entries of the list }\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  \right) \\
&  =\left\{  1,2,\ldots,\widehat{k},\ldots,n\right\}  =\underbrace{\left\{
1,2,\ldots,n\right\}  }_{=\left[  n\right]  }\setminus\underbrace{\left\{
k\right\}  }_{=\left[  n\right]  \setminus\left\{  g_{1},g_{2},\ldots
,g_{n-1}\right\}  }\\
&  =\left[  n\right]  \setminus\left(  \left[  n\right]  \setminus\left\{
g_{1},g_{2},\ldots,g_{n-1}\right\}  \right)  =\left\{  g_{1},g_{2}%
,\ldots,g_{n-1}\right\}
\end{align*}
(since $\left\{  g_{1},g_{2},\ldots,g_{n-1}\right\}  \subseteq\left[
n\right]  $).
\par
Now, Lemma \ref{lem.adj(AB).set.increase} (applied to $n-1$, $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  $ and $\left(  g_{1},g_{2},\ldots
,g_{n-1}\right)  $ instead of $n$, $\left(  a_{1},a_{2},\ldots,a_{n}\right)  $
and $\left(  b_{1},b_{2},\ldots,b_{n}\right)  $) shows that $\left(
t_{1},t_{2},\ldots,t_{n-1}\right)  =\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
$. Compared with $\mathbf{g}=\left(  g_{1},g_{2},\ldots,g_{n-1}\right)  $,
this yields%
\[
\mathbf{g}=\left(  t_{1},t_{2},\ldots,t_{n-1}\right)  =\left(  1,2,\ldots
,\widehat{k},\ldots,n\right)  =\Phi\left(  k\right)
\]
(since $\Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
$ (by the definition of $\Phi\left(  k\right)  $)). Thus, $\mathbf{g}%
=\Phi\left(  \underbrace{k}_{\in\left\{  1,2,\ldots,n\right\}  }\right)
\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $.
\par
Now, let us forget that we fixed $\mathbf{g}$. We thus have proven that
$\mathbf{g}\in\Phi\left(  \left\{  1,2,\ldots,n\right\}  \right)  $ for every
$\mathbf{g}\in\mathbf{I}$. In other words, $\mathbf{I}\subseteq\Phi\left(
\left\{  1,2,\ldots,n\right\}  \right)  $. In other words, the map $\Phi$ is
surjective, qed.}. Hence, the map $\Phi$ is bijective. In other words, the map
$\Phi$ is a bijection. In other words, the map%
\begin{align*}
\left\{  1,2,\ldots,n\right\}   &  \rightarrow\mathbf{I},\\
k  &  \mapsto\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\end{align*}
is a bijection (because the map $\Phi$ is the map
\begin{align*}
\left\{  1,2,\ldots,n\right\}   &  \rightarrow\mathbf{I},\\
k  &  \mapsto\left(  1,2,\ldots,\widehat{k},\ldots,n\right)
\end{align*}
(since we have $\Phi\left(  k\right)  =\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ for every $k\in\left\{  1,2,\ldots,n\right\}  $)).

Now, (\ref{pf.lem.adj(AB).cauchy-binet.1}) becomes%
\begin{align*}
\det\left(  AB\right)   &  =\sum_{\left(  g_{1},g_{2},\ldots,g_{n-1}\right)
\in\mathbf{I}}\det\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2}%
,\ldots,g_{n-1}}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{n-1}}B\right) \\
&  =\underbrace{\sum_{k\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{k=1}^{n}%
}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}A\right)  \cdot\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}B\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\left(  1,2,\ldots,\widehat{k},\ldots
,n\right)  \text{ for}\\
\left(  g_{1},g_{2},\ldots,g_{n}\right)  \text{ in the sum, since the map}\\
\left\{  1,2,\ldots,n\right\}  \rightarrow\mathbf{I},\ k\mapsto\left(
1,2,\ldots,\widehat{k},\ldots,n\right)  \text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{k=1}^{n}\det\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  \cdot\det\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}B\right)  .
\end{align*}
This proves Lemma \ref{lem.adj(AB).cauchy-binet}.
\end{proof}
\end{verlong}

Here comes one more simple lemma:

\begin{lemma}
\label{lem.adj(AB).minor-of-AB}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$ and
$p\in\mathbb{N}$. Let $A$ be an $n\times p$-matrix. Let $B$ be a $p\times
m$-matrix. Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of $\left\{
1,2,\ldots,n\right\}  $. Let $j_{1},j_{2},\ldots,j_{v}$ be some elements of
$\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  =\left(  \operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}B\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.adj(AB).minor-of-AB}.]Let us write the $n\times
p$-matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq p}$. Let us write the $p\times m$-matrix $B$ in the form $B=\left(
b_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq m}$.

The definition of the product of two matrices yields $AB=\left(  \sum
_{k=1}^{p}a_{i,k}b_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ (since
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq p}$ and $B=\left(
b_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq m}$). Thus, the definition of
$\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  $ yields%
\begin{equation}
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  =\left(  \sum_{k=1}^{p}a_{i_{x},k}b_{k,j_{y}%
}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}.
\label{pf.lem.adj(AB).minor-of-AB.1}%
\end{equation}


On the other hand, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
p}$. Hence, the definition of $\operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A$ yields%
\[
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(  a_{i_{x}%
,j}\right)  _{1\leq x\leq u,\ 1\leq j\leq p}=\left(  a_{i_{i},j}\right)
_{1\leq i\leq u,\ 1\leq j\leq p}%
\]
\footnote{The double use of the letter \textquotedblleft$i$\textquotedblright%
\ in \textquotedblleft$i_{i}$\textquotedblright\ might appear confusing. The
first \textquotedblleft$i$\textquotedblright\ is part of the notation $i_{k}$
for $k\in\left\{  1,2,\ldots,u\right\}  $; the second \textquotedblleft%
$i$\textquotedblright\ is an element of $\left\{  1,2,\ldots,u\right\}  $.
These two \textquotedblleft$i$\textquotedblright s are unrelated to each
other. I hope the reader can easily tell them apart by the fact that the
\textquotedblleft$i$\textquotedblright\ that is part of the notation $i_{k}$
always appears with a subscript, whereas the second \textquotedblleft%
$i$\textquotedblright\ never does.} (here, we renamed the index $\left(
x,j\right)  $ as $\left(  i,j\right)  $).

Also, $B=\left(  b_{i,j}\right)  _{1\leq i\leq p,\ 1\leq j\leq m}$. Hence, the
definition of $\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B$
yields%
\[
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B=\left(  b_{i,j_{y}%
}\right)  _{1\leq i\leq p,\ 1\leq y\leq v}=\left(  b_{i,j_{j}}\right)  _{1\leq
i\leq p,\ 1\leq j\leq v}%
\]
(here, we renamed the index $\left(  i,y\right)  $ as $\left(  i,j\right)  $).

We have $\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A=\left(
a_{i_{i},j}\right)  _{1\leq i\leq u,\ 1\leq j\leq p}$ and
$\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B=\left(  b_{i,j_{j}%
}\right)  _{1\leq i\leq p,\ 1\leq j\leq v}$. The definition of the product of
two matrices thus yields%
\begin{align*}
\left(  \operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)
\cdot\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{v}}B\right)
&  =\left(  \sum_{k=1}^{p}a_{i_{i},k}b_{k,j_{j}}\right)  _{1\leq i\leq
u,\ 1\leq j\leq v}\\
&  =\left(  \sum_{k=1}^{p}a_{i_{x},k}b_{k,j_{y}}\right)  _{1\leq x\leq
u,\ 1\leq y\leq v}%
\end{align*}
(here, we have renamed the index $\left(  i,j\right)  $ as $\left(
x,y\right)  $). Comparing this with (\ref{pf.lem.adj(AB).minor-of-AB.1}), we
obtain%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  AB\right)  =\left(  \operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{v}}B\right)  .
\]
This proves Lemma \ref{lem.adj(AB).minor-of-AB}.
\end{proof}

We note in passing that Lemma \ref{lem.adj(AB).minor-of-AB} leads to the
following generalization of Theorem \ref{thm.cauchy-binet}:

\begin{corollary}
\label{cor.adj(AB).cauchy-binet-general}Let $n\in\mathbb{N}$, $m\in\mathbb{N}$
and $p\in\mathbb{N}$. Let $A$ be an $n\times p$-matrix. Let $B$ be a $p\times
m$-matrix. Let $u\in\mathbb{N}$. Let $i_{1},i_{2},\ldots,i_{u}$ be some
elements of $\left\{  1,2,\ldots,n\right\}  $. Let $j_{1},j_{2},\ldots,j_{u}$
be some elements of $\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\det\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}%
^{j_{1},j_{2},\ldots,j_{u}}\left(  AB\right)  \right)  =\sum_{1\leq
g_{1}<g_{2}<\cdots<g_{u}\leq p}\det\left(  \operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{g_{1},g_{2},\ldots,g_{u}}A\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}}^{j_{1},j_{2}%
,\ldots,j_{u}}B\right)  .
\]
(Here, the summation sign \textquotedblleft$\sum_{1\leq g_{1}<g_{2}%
<\cdots<g_{u}\leq p}$\textquotedblright\ has to be interpreted as
\textquotedblleft$\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{u}\right)
\in\left\{  1,2,\ldots,p\right\}  ^{u};\\g_{1}<g_{2}<\cdots<g_{u}}%
}$\textquotedblright, in analogy to Remark \ref{rmk.cauchy-binet.sumsign}.)
\end{corollary}

Corollary \ref{cor.adj(AB).cauchy-binet-general} is precisely the formula
\cite[(1.10)]{NoumiYamada}\footnote{We notice that the notation $A_{j_{1}%
,j_{2},\ldots,j_{u}}^{i_{1},i_{2},\ldots,i_{u}}$ in \cite{NoumiYamada} is
equivalent to our notation $\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{u}}A$.}. We shall not use Corollary
\ref{cor.adj(AB).cauchy-binet-general} in what follows, but let us
nevertheless prove it:

\begin{proof}
[Proof of Corollary \ref{cor.adj(AB).cauchy-binet-general}.]Fix any $\left(
g_{1},g_{2},\ldots,g_{u}\right)  \in\left\{  1,2,\ldots,p\right\}  ^{u}$.
Applying Proposition \ref{prop.submatrix.easy} \textbf{(d)} to $p$, $u$ and
$\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ instead of $m$, $v$ and $\left(
j_{1},j_{2},\ldots,j_{v}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{g_{1},g_{2}%
,\ldots,g_{u}}A=\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}%
}\left(  \operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{u}}A\right)
=\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{u}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  .
\]
Applying Proposition \ref{prop.submatrix.easy} \textbf{(d)} to $p$, $B$, $u$
and $\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ instead of $n$, $A$, $v$ and
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}}^{j_{1},j_{2}%
,\ldots,j_{u}}B=\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{u}%
}\left(  \operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)
=\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}\left(
\operatorname*{rows}\nolimits_{g_{1},g_{2},\ldots,g_{u}}B\right)  .
\]


Now, Lemma \ref{lem.adj(AB).minor-of-AB} (applied to $v=u$) shows that%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{u}}\left(  AB\right)  =\left(  \operatorname*{rows}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)  .
\]
Hence,%
\begin{align*}
&  \det\left(  \underbrace{\operatorname*{sub}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{u}}\left(  AB\right)  }_{=\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)  \cdot\left(
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)  }\right) \\
&  =\det\left(  \left(  \operatorname*{rows}\nolimits_{i_{1},i_{2}%
,\ldots,i_{u}}A\right)  \cdot\left(  \operatorname*{cols}\nolimits_{j_{1}%
,j_{2},\ldots,j_{u}}B\right)  \right) \\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{u}\leq p}\det\left(
\underbrace{\operatorname*{cols}\nolimits_{g_{1},g_{2},\ldots,g_{u}}\left(
\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\right)
}_{=\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{g_{1}%
,g_{2},\ldots,g_{u}}A}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \cdot\det\left(  \underbrace{\operatorname*{rows}%
\nolimits_{g_{1},g_{2},\ldots,g_{u}}\left(  \operatorname*{cols}%
\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\right)  }_{=\operatorname*{sub}%
\nolimits_{g_{1},g_{2},\ldots,g_{u}}^{j_{1},j_{2},\ldots,j_{u}}B}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.cauchy-binet} (applied to }u\text{, }p\text{,
}\operatorname*{rows}\nolimits_{i_{1},i_{2},\ldots,i_{u}}A\text{ and}\\
\operatorname*{cols}\nolimits_{j_{1},j_{2},\ldots,j_{u}}B\text{ instead of
}n\text{, }m\text{, }A\text{ and }B\text{)}%
\end{array}
\right) \\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{u}\leq p}\det\left(  \operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{g_{1},g_{2},\ldots,g_{u}}A\right)
\cdot\det\left(  \operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{u}%
}^{j_{1},j_{2},\ldots,j_{u}}B\right)  .
\end{align*}
This proves Corollary \ref{cor.adj(AB).cauchy-binet-general}.
\end{proof}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.adj(AB)}.]Let $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $1\leq u\leq n$, so that $n\geq1$.

For every $k\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
=A_{\sim u,\sim k} \label{sol.adj(AB).short.A}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).short.A}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We can apply Proposition \ref{prop.submatrix.easy}
\textbf{(d)} to $n-1$, $n-1$, $\left(  1,2,\ldots,\widehat{u},\ldots,n\right)
$ and $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  $ instead of $u$, $v$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  $. As a result, we obtain%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{k},\ldots,n}A=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  .
\]
But the definition of $A_{\sim u,\sim k}$ yields%
\[
A_{\sim u,\sim k}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,n}A=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  .
\]
This proves (\ref{sol.adj(AB).short.A}).} and%
\begin{equation}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
=B_{\sim k,\sim v} \label{sol.adj(AB).short.B}%
\end{equation}
\footnote{This holds for similar reasons.}.

We have%
\begin{equation}
\left(  AB\right)  _{\sim u,\sim v}=\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  \cdot\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
\label{sol.adj(AB).short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).short.1}):} The definition of
$\left(  AB\right)  _{\sim u,\sim v}$ yields%
\[
\left(  AB\right)  _{\sim u,\sim v}=\operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}^{1,2,\ldots,\widehat{v},\ldots,n}\left(  AB\right)
=\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,n}A\right)  \cdot\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}B\right)
\]
(by Lemma \ref{lem.adj(AB).minor-of-AB}, applied to $m=n$, $p=n$, $u=n-1$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(  1,2,\ldots,\widehat{u}%
,\ldots,n\right)  $ and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  =\left(
1,2,\ldots,\widehat{v},\ldots,n\right)  $). This proves
(\ref{sol.adj(AB).short.1}).}. Taking determinants on both sides of this
equation, we obtain%
\begin{align}
&  \det\left(  \left(  AB\right)  _{\sim u,\sim v}\right) \nonumber\\
&  =\det\left(  \left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)  \right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  }_{\substack{=A_{\sim
u,\sim k}\\\text{(by (\ref{sol.adj(AB).short.A}))}}}\right)  \cdot\det\left(
\underbrace{\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\right)  }_{\substack{=B_{\sim k,\sim v}\\\text{(by
(\ref{sol.adj(AB).short.B}))}}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Lemma \ref{lem.adj(AB).cauchy-binet}, applied to }%
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\\
\text{and }\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\text{ instead of }A\text{ and }B
\end{array}
\right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  A_{\sim u,\sim k}\right)  \cdot\det\left(
B_{\sim k,\sim v}\right)  . \label{sol.adj(AB).short.5}%
\end{align}


Let us now forget that we fixed $\left(  u,v\right)  $. We thus have proven
(\ref{sol.adj(AB).short.5}) for every $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$.

Now, the definition of $\operatorname*{adj}\left(  AB\right)  $ yields%
\begin{equation}
\operatorname*{adj}\left(  AB\right)  =\left(  \left(  -1\right)  ^{i+j}%
\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{sol.adj(AB).short.L1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \left(  -1\right)  ^{i+j}\underbrace{\det\left(  \left(  AB\right)  _{\sim
j,\sim i}\right)  }_{\substack{=\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right)  \\\text{(by
(\ref{sol.adj(AB).short.5}), applied to }\left(  u,v\right)  =\left(
j,i\right)  \text{)}}}\\
&  =\left(  -1\right)  ^{i+j}\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{i+j}}_{\substack{=\left(
-1\right)  ^{\left(  i+k\right)  +\left(  k+j\right)  }\\\text{(since
}i+j\equiv i+j+2k=\left(  i+k\right)  +\left(  k+j\right)  \operatorname{mod}%
2\text{)}}}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim
k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{\left(  i+k\right)
+\left(  k+j\right)  }}_{=\left(  -1\right)  ^{i+k}\left(  -1\right)  ^{k+j}%
}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim k,\sim
i}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
.
\end{align*}
Thus, (\ref{sol.adj(AB).short.L1}) becomes%
\begin{align}
\operatorname*{adj}\left(  AB\right)   &  =\left(  \underbrace{\left(
-1\right)  ^{i+j}\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)
}_{=\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  =\left(  \sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n} \label{sol.adj(AB).short.L2}%
\end{align}


On the other hand, we have $\operatorname*{adj}B=\left(  \left(  -1\right)
^{i+j}\det\left(  B_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ (by the definition of $\operatorname*{adj}B$) and
$\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the
definition of $\operatorname*{adj}A$). Therefore, the definition of the
product of two matrices shows that%
\[
\operatorname*{adj}B\cdot\operatorname*{adj}A=\left(  \sum_{k=1}^{n}\left(
-1\right)  ^{i+k}\det\left(  B_{\sim k,\sim i}\right)  \cdot\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]
Compared with (\ref{sol.adj(AB).short.L2}), this yields $\operatorname*{adj}%
\left(  AB\right)  =\operatorname*{adj}B\cdot\operatorname*{adj}A$. This
solves Exercise \ref{exe.adj(AB)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Solution to Exercise \ref{exe.adj(AB)}.]Let $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. Thus, $u\in\left\{  1,2,\ldots,n\right\}  $ and
$v\in\left\{  1,2,\ldots,n\right\}  $. From $u\in\left\{  1,2,\ldots
,n\right\}  $, we obtain $1\leq u\leq n$, so that $n\geq1$ and therefore
$n-1\in\mathbb{N}$ and $n>0$.

For every $k\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
=A_{\sim u,\sim k} \label{sol.adj(AB).A}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).A}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We can apply Proposition \ref{prop.submatrix.easy}
\textbf{(d)} to $n-1$, $n-1$, $\left(  1,2,\ldots,\widehat{u},\ldots,n\right)
$ and $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  $ instead of $u$, $v$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  $. As a result, we obtain%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{k},\ldots,n}A=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}A\right)  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  .
\]
But the definition of $A_{\sim u,\sim k}$ yields%
\[
A_{\sim u,\sim k}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,n}A=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  .
\]
This proves (\ref{sol.adj(AB).A}).} and%
\begin{equation}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
=B_{\sim k,\sim v} \label{sol.adj(AB).B}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).B}):} Let $k\in\left\{
1,2,\ldots,n\right\}  $. We can apply Proposition \ref{prop.submatrix.easy}
\textbf{(d)} to $n-1$, $n-1$, $B$, $\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $ and $\left(  1,2,\ldots,\widehat{v},\ldots,n\right)  $,
instead of $u$, $v$, $A$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and
$\left(  j_{1},j_{2},\ldots,j_{v}\right)  $. As a result, we obtain%
\[
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}^{1,2,\ldots
,\widehat{v},\ldots,n}B=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}B\right)  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n}B\right)  .
\]
But the definition of $B_{\sim k,\sim v}$ yields%
\[
B_{\sim k,\sim v}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{k}%
,\ldots,n}^{1,2,\ldots,\widehat{v},\ldots,n}B=\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)  .
\]
This proves (\ref{sol.adj(AB).B}).}.

We have%
\begin{equation}
\left(  AB\right)  _{\sim u,\sim v}=\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  \cdot\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)
\label{sol.adj(AB).1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.adj(AB).1}):} The definition of $\left(
AB\right)  _{\sim u,\sim v}$ yields%
\[
\left(  AB\right)  _{\sim u,\sim v}=\operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}^{1,2,\ldots,\widehat{v},\ldots,n}\left(  AB\right)
=\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,n}A\right)  \cdot\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,n}B\right)
\]
(by Lemma \ref{lem.adj(AB).minor-of-AB}, applied to $m=n$, $p=n$, $u=n-1$,
$\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\left(  1,2,\ldots,\widehat{u}%
,\ldots,n\right)  $ and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  =\left(
1,2,\ldots,\widehat{v},\ldots,n\right)  $). This proves (\ref{sol.adj(AB).1}%
).}. Hence,%
\begin{align}
&  \det\left(  \underbrace{\left(  AB\right)  _{\sim u,\sim v}}_{=\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
\cdot\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\right)  }\right) \nonumber\\
&  =\det\left(  \left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A\right)  \cdot\left(  \operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,n}B\right)  \right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  \underbrace{\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n}\left(  \operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  }_{\substack{=A_{\sim
u,\sim k}\\\text{(by (\ref{sol.adj(AB).A}))}}}\right)  \cdot\det\left(
\underbrace{\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\right)  }_{\substack{=B_{\sim k,\sim v}\\\text{(by (\ref{sol.adj(AB).B}%
))}}}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Lemma \ref{lem.adj(AB).cauchy-binet}, applied to }%
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\\
\text{and }\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,n}B\text{ instead of }A\text{ and }B
\end{array}
\right) \nonumber\\
&  =\sum_{k=1}^{n}\det\left(  A_{\sim u,\sim k}\right)  \cdot\det\left(
B_{\sim k,\sim v}\right)  . \label{sol.adj(AB).5}%
\end{align}


Let us now forget that we fixed $\left(  u,v\right)  $. We thus have proven
(\ref{sol.adj(AB).5}) for every $\left(  u,v\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$.

Now, the definition of $\operatorname*{adj}\left(  AB\right)  $ yields%
\begin{equation}
\operatorname*{adj}\left(  AB\right)  =\left(  \left(  -1\right)  ^{i+j}%
\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)  \right)  _{1\leq i\leq
n,\ 1\leq j\leq n}. \label{sol.adj(AB).L1}%
\end{equation}
But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfies%
\begin{align*}
&  \left(  -1\right)  ^{i+j}\underbrace{\det\left(  \left(  AB\right)  _{\sim
j,\sim i}\right)  }_{\substack{=\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right)  \\\text{(by
(\ref{sol.adj(AB).5}), applied to }\left(  u,v\right)  =\left(  j,i\right)
\text{)}}}\\
&  =\left(  -1\right)  ^{i+j}\sum_{k=1}^{n}\det\left(  A_{\sim j,\sim
k}\right)  \cdot\det\left(  B_{\sim k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{i+j}}_{\substack{=\left(
-1\right)  ^{\left(  i+k\right)  +\left(  k+j\right)  }\\\text{(since
}i+j\equiv i+j+2k=\left(  i+k\right)  +\left(  k+j\right)  \operatorname{mod}%
2\text{)}}}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim
k,\sim i}\right) \\
&  =\sum_{k=1}^{n}\underbrace{\left(  -1\right)  ^{\left(  i+k\right)
+\left(  k+j\right)  }}_{=\left(  -1\right)  ^{i+k}\left(  -1\right)  ^{k+j}%
}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim k,\sim
i}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\underbrace{\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  \cdot\det\left(  B_{\sim k,\sim
i}\right)  }_{=\det\left(  B_{\sim k,\sim i}\right)  \cdot\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  }\\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
.
\end{align*}
Thus, (\ref{sol.adj(AB).L1}) becomes%
\begin{align}
\operatorname*{adj}\left(  AB\right)   &  =\left(  \underbrace{\left(
-1\right)  ^{i+j}\det\left(  \left(  AB\right)  _{\sim j,\sim i}\right)
}_{=\sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\nonumber\\
&  =\left(  \sum_{k=1}^{n}\left(  -1\right)  ^{i+k}\det\left(  B_{\sim k,\sim
i}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  A_{\sim j,\sim k}\right)
\right)  _{1\leq i\leq n,\ 1\leq j\leq n} \label{sol.adj(AB).L2}%
\end{align}


On the other hand, we have $\operatorname*{adj}B=\left(  \left(  -1\right)
^{i+j}\det\left(  B_{\sim j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$ (by the definition of $\operatorname*{adj}B$) and
$\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the
definition of $\operatorname*{adj}A$). Therefore, the definition of the
product of two matrices shows that%
\[
\operatorname*{adj}B\cdot\operatorname*{adj}A=\left(  \sum_{k=1}^{n}\left(
-1\right)  ^{i+k}\det\left(  B_{\sim k,\sim i}\right)  \cdot\left(  -1\right)
^{k+j}\det\left(  A_{\sim j,\sim k}\right)  \right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]
Compared with (\ref{sol.adj(AB).L2}), this yields $\operatorname*{adj}\left(
AB\right)  =\operatorname*{adj}B\cdot\operatorname*{adj}A$. This solves
Exercise \ref{exe.adj(AB)}.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.vander-hook}}

Throughout this section, we shall use the notation $V\left(  y_{1}%
,y_{2},\ldots,y_{n}\right)  $ defined in Exercise \ref{exe.vander-hook}. Let
us now make some preparations for solving Exercise \ref{exe.vander-hook}.

\subsubsection{Lemmas}

\begin{definition}
If $i$ and $j$ are two objects, then $\delta_{i,j}$ is defined to be the
element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.
\end{definition}

Let us now prove four mostly trivial lemmas:

\begin{lemma}
\label{lem.sol.vander-hook.gauss}Let $n\in\mathbb{N}$. Then,%
\begin{equation}
\sum_{r=0}^{n-1}r=\dbinom{n}{2}. \label{sol.vander-hook.gauss}%
\end{equation}

\end{lemma}

\begin{vershort}
\begin{proof}
\item[Proof of Lemma \ref{lem.sol.vander-hook.gauss}.] If $n=0$, then Lemma
\ref{lem.sol.vander-hook.gauss} holds for obvious reasons. Thus, we WLOG
assume that $n\neq0$. Hence, $n\geq1$, so that $n-1\in\mathbb{N}$. Now,%
\begin{align*}
\sum_{r=0}^{n-1}r  &  =\sum_{i=0}^{n-1}i\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we have renamed the summation index }r\text{ as }i\right) \\
&  =\dfrac{\left(  n-1\right)  \left(  \left(  n-1\right)  +1\right)  }%
{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.littlegauss1}) (applied
to }n-1\text{ instead of }n\text{)}\right) \\
&  =\dfrac{n\left(  n-1\right)  }{2}=\dbinom{n}{2}.
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.gauss}.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}


\item[Proof of Lemma \ref{lem.sol.vander-hook.gauss}.] If $n=0$, then%
\begin{align*}
\sum_{r=0}^{n-1}r  &  =\sum_{r=0}^{0-1}r=\left(  \text{empty sum}\right)
=0=\dbinom{0}{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\dbinom{0}{2}%
=\dfrac{0\left(  0-1\right)  }{2}=0\right) \\
&  =\dbinom{n}{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }0=n\right)  .
\end{align*}
Hence, if $n=0$, then Lemma \ref{lem.sol.vander-hook.gauss} holds. Thus, for
the rest of the proof of Lemma \ref{lem.sol.vander-hook.gauss}, we can WLOG
assume that we don't have $n=0$. Assume this.

We have $n\neq0$ (since we don't have $n=0$). Thus, $n\geq1$ (since
$n\in\mathbb{N}$). Therefore, $n-1\in\mathbb{N}$. Now,%
\begin{align*}
\sum_{r=0}^{n-1}r  &  =\sum_{i=0}^{n-1}i\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we have renamed the summation index }r\text{ as }i\right) \\
&  =\dfrac{\left(  n-1\right)  \left(  \left(  n-1\right)  +1\right)  }%
{2}\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{eq.sum.littlegauss1}) (applied
to }n-1\text{ instead of }n\text{)}\right) \\
&  =\dfrac{\left(  n-1\right)  n}{2}=\dfrac{n\left(  n-1\right)  }{2}%
=\dbinom{n}{2}.
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.gauss}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.vander-hook.deltas}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.
Let $k\in\left\{  1,2,\ldots,n\right\}  $. For each $j\in\left\{
1,2,\ldots,n\right\}  $, set $y_{j}=x_{j}+\delta_{j,k}t$. Then,%
\[
\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right)  =\left(  y_{1},y_{2},\ldots,y_{n}\right)  .
\]

\end{lemma}

\begin{vershort}


\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.deltas}.]For each $j\in\left\{
1,2,\ldots,n\right\}  $, we have%
\begin{align*}
&  \left(  \text{the }j\text{-th entry of the list }\left(  y_{1},y_{2}%
,\ldots,y_{n}\right)  \right) \\
&  =y_{j}=x_{j}+\underbrace{\delta_{j,k}}_{=%
\begin{cases}
1, & \text{if }j=k;\\
0, & \text{if }j\neq k
\end{cases}
}t=x_{j}+%
\begin{cases}
1, & \text{if }j=k;\\
0, & \text{if }j\neq k
\end{cases}
t\\
&  =%
\begin{cases}
x_{j}+1t, & \text{if }j=k;\\
x_{j}+0t, & \text{if }j\neq k
\end{cases}
=%
\begin{cases}
x_{j}+t, & \text{if }j=k;\\
x_{j}, & \text{if }j\neq k
\end{cases}
\\
&  =%
\begin{cases}
x_{k}+t, & \text{if }j=k;\\
x_{j}, & \text{if }j\neq k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }x_{j}=x_{k}\text{ in the case when
}j=k\right) \\
&  =\left(  \text{the }j\text{-th entry of the list }\left(  x_{1}%
,x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  \right)  .
\end{align*}
Hence, $\left(  y_{1},y_{2},\ldots,y_{n}\right)  =\left(  x_{1},x_{2}%
,\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  $. This proves
Lemma \ref{lem.sol.vander-hook.deltas}.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.deltas}.]If $\alpha$ and $\beta$ are
two finite lists of elements of $\mathbb{K}$, then we define a new finite list
$\alpha\ast\beta$ of elements of $\mathbb{K}$ by setting%
\[
\alpha\ast\beta=\left(  \alpha_{1},\alpha_{2},\ldots,\alpha_{a},\beta
_{1},\beta_{2},\ldots,\beta_{b}\right)  ,
\]
where $\alpha$ is written in the form $\left(  \alpha_{1},\alpha_{2}%
,\ldots,\alpha_{a}\right)  $, and where $\beta$ is written in the form
$\left(  \beta_{1},\beta_{2},\ldots,\beta_{b}\right)  $. This new list
$\alpha\ast\beta$ is called the \textit{concatenation} of the lists $\alpha$
and $\beta$. If $\alpha$, $\beta$ and $\gamma$ are three finite lists of
elements of $\mathbb{K}$, then the two lists $\left(  \alpha\ast\beta\right)
\ast\gamma$ and $\alpha\ast\left(  \beta\ast\gamma\right)  $ are identical. In
other words, concatenation of finite lists is an associative operation. Thus,
we are able to write $\alpha\ast\beta\ast\gamma$ for any of the two identical
lists $\left(  \alpha\ast\beta\right)  \ast\gamma$ and $\alpha\ast\left(
\beta\ast\gamma\right)  $ whenever $\alpha$, $\beta$ and $\gamma$ are three
finite lists of elements of $\mathbb{K}$.

Now, $x_{j}=y_{j}$ for each $j\in\left\{  1,2,\ldots,k-1\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $j\in\left\{  1,2,\ldots,k-1\right\}
$. Then, $j\leq k-1<k$, hence $j\neq k$. Now, the definition of $y_{j}$ yields
$y_{j}=x_{j}+\underbrace{\delta_{j,k}}_{\substack{=0\\\text{(since }j\neq
k\text{)}}}t=x_{j}+\underbrace{0t}_{=0}=x_{j}$. In other words, $x_{j}=y_{j}$.
Qed.}. In other words, $\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  =\left(
y_{1},y_{2},\ldots,y_{k-1}\right)  $.

The definition of $y_{k}$ yields $y_{k}=x_{k}+\underbrace{\delta_{k,k}%
}_{\substack{=1\\\text{(since }k=k\text{)}}}t=x_{k}+\underbrace{1t}_{=t}%
=x_{k}+t$. In other words, $x_{k}+t=y_{k}$.

Also, $x_{j}=y_{j}$ for each $j\in\left\{  k+1,k+2,\ldots,n\right\}
$\ \ \ \ \footnote{\textit{Proof.} Let $j\in\left\{  k+1,k+2,\ldots,n\right\}
$. Then, $j\geq k+1>k$, hence $j\neq k$. Now, the definition of $y_{j}$ yields
$y_{j}=x_{j}+\underbrace{\delta_{j,k}}_{\substack{=0\\\text{(since }j\neq
k\text{)}}}t=x_{j}+\underbrace{0t}_{=0}=x_{j}$. In other words, $x_{j}=y_{j}$.
Qed.}. In other words, $\left(  x_{k+1},x_{k+2},\ldots,x_{n}\right)  =\left(
y_{k+1},y_{k+2},\ldots,y_{n}\right)  $.

Now,%
\begin{align*}
&  \left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right) \\
&  =\underbrace{\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t\right)  }_{=\left(
x_{1},x_{2},\ldots,x_{k-1}\right)  \ast\left(  x_{k}+t\right)  }\ast\left(
x_{k+1},x_{k+2},\ldots,x_{n}\right) \\
&  =\underbrace{\left(  x_{1},x_{2},\ldots,x_{k-1}\right)  }_{=\left(
y_{1},y_{2},\ldots,y_{k-1}\right)  }\ast\left(  \underbrace{x_{k}+t}_{=y_{k}%
}\right)  \ast\underbrace{\left(  x_{k+1},x_{k+2},\ldots,x_{n}\right)
}_{=\left(  y_{k+1},y_{k+2},\ldots,y_{n}\right)  }\\
&  =\underbrace{\left(  y_{1},y_{2},\ldots,y_{k-1}\right)  \ast\left(
y_{k}\right)  }_{=\left(  y_{1},y_{2},\ldots,y_{k-1},y_{k}\right)  =\left(
y_{1},y_{2},\ldots,y_{k}\right)  }\ast\left(  y_{k+1},y_{k+2},\ldots
,y_{n}\right) \\
&  =\left(  y_{1},y_{2},\ldots,y_{k}\right)  \ast\left(  y_{k+1}%
,y_{k+2},\ldots,y_{n}\right)  =\left(  y_{1},y_{2},\ldots,y_{k},y_{k+1}%
,y_{k+2},\ldots,y_{n}\right) \\
&  =\left(  y_{1},y_{2},\ldots,y_{n}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.deltas}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.vander-hook.V=}Let $n\in\mathbb{N}$. Let $y_{1},y_{2}%
,\ldots,y_{n}$ be $n$ elements of $\mathbb{K}$. Then,%
\[
V\left(  y_{1},y_{2},\ldots,y_{n}\right)  =\det\left(  \left(  y_{i}%
^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.V=}.]Theorem \ref{thm.vander-det}
\textbf{(a)} (applied to $x_{i}=y_{i}$) yields%
\[
\det\left(  \left(  y_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\right)  =\prod_{1\leq i<j\leq n}\left(  y_{i}-y_{j}\right)  .
\]
Comparing this with%
\[
V\left(  y_{1},y_{2},\ldots,y_{n}\right)  =\prod_{1\leq i<j\leq n}\left(
y_{i}-y_{j}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of
}V\left(  y_{1},y_{2},\ldots,y_{n}\right)  \right)  ,
\]
we obtain $V\left(  y_{1},y_{2},\ldots,y_{n}\right)  =\det\left(  \left(
y_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  $. Thus, Lemma
\ref{lem.sol.vander-hook.V=} is proven.
\end{proof}

\begin{lemma}
\label{lem.sol.vander-hook.AvsB}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$k\in\left\{  1,2,\ldots,n\right\}  $. Let $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ be two $n\times m$-matrices. Assume that every
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}
$ satisfying $i\neq k$ satisfy%
\begin{equation}
a_{i,j}=b_{i,j}. \label{eq.lem.sol.vander-hook.AvsB.ass}%
\end{equation}
Then, $B_{\sim k,\sim q}=A_{\sim k,\sim q}$ for every $q\in\left\{
1,2,\ldots,m\right\}  $.
\end{lemma}

\begin{vershort}


\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.AvsB}.]Let $q\in\left\{
1,2,\ldots,m\right\}  $. The condition (\ref{eq.lem.sol.vander-hook.AvsB.ass})
shows that the matrices $A$ and $B$ agree in all their entries except for
those in the respective $k$-th rows of the matrices. Therefore, the matrices
$A_{\sim k,\sim q}$ and $B_{\sim k,\sim q}$ must agree completely (since they
are obtained from the matrices $A$ and $B$ by removing the $k$-th rows and the
$q$-th columns). In other words, $A_{\sim k,\sim q}=B_{\sim k,\sim q}$. This
proves Lemma \ref{lem.sol.vander-hook.AvsB}.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.AvsB}.]Let $q\in\left\{
1,2,\ldots,m\right\}  $.

Denote the list $\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  $ by $\left(
u_{1},u_{2},\ldots,u_{n-1}\right)  $. (This is possible, since the list
$\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  $ has size $n-1$.) Then,
$\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  $. Now, every $x\in\left\{  1,2,\ldots,n-1\right\}  $
satisfies%
\begin{align}
u_{x}  &  \in\left\{  u_{1},u_{2},\ldots,u_{n-1}\right\}  =\left\{
1,2,\ldots,\widehat{k},\ldots,n\right\} \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  u_{1},u_{2},\ldots
,u_{n-1}\right)  =\left(  1,2,\ldots,\widehat{k},\ldots,n\right)  \right)
\nonumber\\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left\{  k\right\}  .
\label{pf.lem.sol.vander-hook.AvsB.uxin}%
\end{align}


Denote the list $\left(  1,2,\ldots,\widehat{q},\ldots,m\right)  $ by $\left(
v_{1},v_{2},\ldots,v_{m-1}\right)  $. (This is possible, since the list
$\left(  1,2,\ldots,\widehat{q},\ldots,m\right)  $ has size $m-1$.) Then,
$\left(  v_{1},v_{2},\ldots,v_{m-1}\right)  =\left(  1,2,\ldots,\widehat{q}%
,\ldots,m\right)  $. Now, every $y\in\left\{  1,2,\ldots,m-1\right\}  $
satisfies%
\begin{align}
v_{y}  &  \in\left\{  v_{1},v_{2},\ldots,v_{m-1}\right\}  =\left\{
1,2,\ldots,\widehat{q},\ldots,m\right\} \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  v_{1},v_{2},\ldots
,v_{m-1}\right)  =\left(  1,2,\ldots,\widehat{q},\ldots,m\right)  \right)
\nonumber\\
&  =\left\{  1,2,\ldots,m\right\}  \setminus\left\{  q\right\}  .
\label{pf.lem.sol.vander-hook.AvsB.vyin}%
\end{align}


Every $\left(  x,y\right)  \in\left\{  1,2,\ldots,n-1\right\}  \times\left\{
1,2,\ldots,m-1\right\}  $ satisfies%
\begin{equation}
a_{u_{x},v_{y}}=b_{u_{x},v_{y}} \label{pf.lem.sol.vander-hook.AvsB.a=b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.vander-hook.AvsB.a=b}):} Let
$\left(  x,y\right)  \in\left\{  1,2,\ldots,n-1\right\}  \times\left\{
1,2,\ldots,m-1\right\}  $. Thus, $x\in\left\{  1,2,\ldots,n-1\right\}  $ and
$y\in\left\{  1,2,\ldots,m-1\right\}  $.
\par
Now, (\ref{pf.lem.sol.vander-hook.AvsB.uxin}) shows that $u_{x}\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  k\right\}  $. In other words,
$u_{x}\in\left\{  1,2,\ldots,n\right\}  $ and $u_{x}\neq k$. Also,
(\ref{pf.lem.sol.vander-hook.AvsB.vyin}) yields $v_{y}\in\left\{
1,2,\ldots,m\right\}  \setminus\left\{  q\right\}  \subseteq\left\{
1,2,\ldots,m\right\}  $. Hence, (\ref{eq.lem.sol.vander-hook.AvsB.ass})
(applied to $i=u_{x}$ and $j=v_{y}$) yields $a_{u_{x},v_{y}}=b_{u_{x},v_{y}}$.
This proves (\ref{pf.lem.sol.vander-hook.AvsB.a=b}).}

The definition of $A_{\sim k,\sim q}$ yields
\begin{align}
A_{\sim k,\sim q}  &  =\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{k}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,m}A=\operatorname*{sub}%
\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{1,2,\ldots,\widehat{q},\ldots
,m}A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{k}%
,\ldots,n\right)  =\left(  u_{1},u_{2},\ldots,u_{n-1}\right)  \right)
\nonumber\\
&  =\operatorname*{sub}\nolimits_{u_{1},u_{2},\ldots,u_{n-1}}^{v_{1}%
,v_{2},\ldots,v_{m-1}}A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{q}%
,\ldots,m\right)  =\left(  v_{1},v_{2},\ldots,v_{m-1}\right)  \right)
\nonumber\\
&  =\left(  a_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq m-1}
\label{pf.lem.sol.vander-hook.AvsB.1A}%
\end{align}
(by the definition of $\operatorname*{sub}\nolimits_{u_{1},u_{2}%
,\ldots,u_{n-1}}^{v_{1},v_{2},\ldots,v_{m-1}}A$, since $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$). The same argument (but
applied to $B$ and $b_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{equation}
B_{\sim k,\sim q}=\left(  b_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq
y\leq m-1}. \label{pf.lem.sol.vander-hook.AvsB.1B}%
\end{equation}
Comparing this with%
\begin{align*}
A_{\sim k,\sim q}  &  =\left(  \underbrace{a_{u_{x},v_{y}}}%
_{\substack{=b_{u_{x},v_{y}}\\\text{(by (\ref{pf.lem.sol.vander-hook.AvsB.a=b}%
))}}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq m-1}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.lem.sol.vander-hook.AvsB.1A})}\right) \\
&  =\left(  b_{u_{x},v_{y}}\right)  _{1\leq x\leq n-1,\ 1\leq y\leq m-1},
\end{align*}
we obtain $B_{\sim k,\sim q}=A_{\sim k,\sim q}$. This proves Lemma
\ref{lem.sol.vander-hook.AvsB}.
\end{proof}
\end{verlong}

Next, let us combine Theorem \ref{thm.laplace.gen} \textbf{(b)} and
Proposition \ref{prop.laplace.0} \textbf{(b)} into a convenient package:

\begin{lemma}
\label{lem.sol.vander-hook.lap0}Let $n\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Let $q\in\left\{  1,2,\ldots,n\right\}  $ and $r\in\left\{  1,2,\ldots
,n\right\}  $. Then,%
\[
\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  =\delta_{q,r}\det A.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.lap0}.]We are in one of the following
two cases:

\textit{Case 1:} We have $q=r$.

\textit{Case 2:} We have $q\neq r$.

Let us first consider Case 1. In this case, we have $q=r$. Hence,
$\delta_{q,r}=1$, so that
\begin{align*}
\underbrace{\delta_{q,r}}_{=1}\det A  &  =\det A=\sum_{p=1}^{n}\left(
-1\right)  ^{p+q}\underbrace{a_{p,q}}_{\substack{=a_{p,r}\\\text{(since
}q=r\text{)}}}\det\left(  A_{\sim p,\sim q}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Theorem \ref{thm.laplace.gen}
\textbf{(b)}}\right) \\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  .
\end{align*}
Hence, Lemma \ref{lem.sol.vander-hook.lap0} is proven in Case 1.

Let us now consider Case 2. In this case, we have $q\neq r$. Hence,
$\delta_{q,r}=0$, so that
\begin{align*}
\underbrace{\delta_{q,r}}_{=0}\det A  &  =0\det A=0\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.laplace.0} \textbf{(b)}}\right)  .
\end{align*}
Hence, Lemma \ref{lem.sol.vander-hook.lap0} is proven in Case 2.

We have now proven Lemma \ref{lem.sol.vander-hook.lap0} in each of the two
Cases 1 and 2. Since these two Cases cover all possibilities, this shows that
Lemma \ref{lem.sol.vander-hook.lap0} always holds.
\end{proof}

Next, we show three crucial lemmas:

\begin{lemma}
\label{lem.sol.vander-hook.V=x}Let $n\in\mathbb{N}$. Let $x_{1},x_{2}%
,\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $A$ be the $n\times
n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Then,%
\begin{equation}
V\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\det A.
\label{eq.lem.sol.vander-hook.V=x.eq}%
\end{equation}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.V=x}.]The definition of $A$ yields
$A=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Lemma
\ref{lem.sol.vander-hook.V=} (applied to $y_{j}=x_{j}$) yields%
\[
V\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\det\left(  \underbrace{\left(
x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}}_{=A}\right)  =\det A.
\]
This proves Lemma \ref{lem.sol.vander-hook.V=x}.
\end{proof}

\begin{lemma}
\label{lem.sol.vander-hook.lap1}Let $n\in\mathbb{N}$. Let $x_{1},x_{2}%
,\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$. Let $A$
be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$. Let $k\in\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} We have%
\[
V\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{q=1}^{n}\left(  -1\right)
^{k+q}x_{k}^{n-q}\det\left(  A_{\sim k,\sim q}\right)  .
\]


\textbf{(b)} We have%
\begin{align*}
&  V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\left(  x_{k}+t\right)  ^{n-q}%
\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}


\textbf{(c)} We have%
\begin{align*}
&  V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right)  -V\left(  x_{1},x_{2},\ldots,x_{n}\right) \\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.lap1}.]The definition of $A$ yields
$A=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

\textbf{(a)} Lemma \ref{lem.sol.vander-hook.V=x} yields%
\[
V\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\det A=\sum_{q=1}^{n}\left(
-1\right)  ^{k+q}x_{k}^{n-q}\det\left(  A_{\sim k,\sim q}\right)
\]
(by Theorem \ref{thm.laplace.gen} \textbf{(a)}, applied to $x_{i}^{n-j}$ and
$k$ instead of $a_{i,j}$ and $p$). This proves Lemma
\ref{lem.sol.vander-hook.lap1} \textbf{(a)}.

\textbf{(b)} For each $j\in\left\{  1,2,\ldots,n\right\}  $, define an element
$y_{j}\in\mathbb{K}$ by $y_{j}=x_{j}+\delta_{j,k}t$. Define an $n\times
n$-matrix $B$ by $B=\left(  y_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$.

Every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,n\right\}  $ satisfying $i\neq k$ satisfy $x_{i}^{n-j}=y_{i}^{n-j}%
$\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left\{  1,2,\ldots,n\right\}  $
and $j\in\left\{  1,2,\ldots,n\right\}  $ be such that $i\neq k$. Then, the
definition of $y_{i}$ yields $y_{i}=x_{i}+\underbrace{\delta_{i,k}%
}_{\substack{=0\\\text{(since }i\neq k\text{)}}}t=x_{i}+\underbrace{0t}%
_{=0}=x_{i}$. Hence, $x_{i}=y_{i}$, and thus $x_{i}^{n-j}=y_{i}^{n-j}$. Qed.}.
Hence, Lemma \ref{lem.sol.vander-hook.AvsB} (applied to $n$, $x_{i}^{n-j}$ and
$y_{i}^{n-j}$ instead of $m$, $a_{i,j}$ and $b_{i,j}$) yields that%
\begin{equation}
B_{\sim k,\sim q}=A_{\sim k,\sim q}\ \ \ \ \ \ \ \ \ \ \text{for every }%
q\in\left\{  1,2,\ldots,n\right\}  . \label{pf.lem.sol.vander-hook.lap1.B.1}%
\end{equation}
Moreover, the definition of $y_{k}$ satisfies $y_{k}=x_{k}+\underbrace{\delta
_{k,k}}_{\substack{=1\\\text{(since }k=k\text{)}}}t=x_{k}+\underbrace{1t}%
_{=t}=x_{k}+t$.

Lemma \ref{lem.sol.vander-hook.deltas} yields%
\[
\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right)  =\left(  y_{1},y_{2},\ldots,y_{n}\right)  .
\]
Hence,%
\begin{align*}
&  V\underbrace{\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right)  }_{=\left(  y_{1},y_{2},\ldots,y_{n}\right)  }\\
&  =V\left(  y_{1},y_{2},\ldots,y_{n}\right)  =\det\left(  \underbrace{\left(
y_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}}_{=B}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Lemma \ref{lem.sol.vander-hook.V=}%
}\right) \\
&  =\det B=\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\underbrace{y_{k}^{n-q}%
}_{\substack{=\left(  x_{k}+t\right)  ^{n-q}\\\text{(since }y_{k}%
=x_{k}+t\text{)}}}\det\left(  \underbrace{B_{\sim k,\sim q}}%
_{\substack{=A_{\sim k,\sim q}\\\text{(by
(\ref{pf.lem.sol.vander-hook.lap1.B.1}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.laplace.gen} \textbf{(a)} (applied to }B\text{,
}y_{i}^{n-j}\text{ and }k\\
\text{instead of }A\text{, }a_{i,j}\text{ and }p\text{)}%
\end{array}
\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\left(  x_{k}+t\right)  ^{n-q}%
\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.lap1} \textbf{(b)}.

\textbf{(c)} For every $m\in\mathbb{N}$, we have%
\begin{equation}
\left(  X+Y\right)  ^{m}=\sum_{\ell=0}^{m}\dbinom{m}{\ell}X^{\ell}Y^{m-\ell}
\label{pf.lem.sol.vander-hook.lap1.c.binom}%
\end{equation}
(an equality between two polynomials in $X$ and $Y$). (Indeed, this is
precisely the statement of Proposition \ref{prop.binom.binomial}, with the
variables $n$ and $k$ renamed as $m$ and $\ell$.) Now, every $q\in\left\{
1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left(  x_{k}+t\right)  ^{n-q}-x_{k}^{n-q}=\sum_{\ell=1}^{n-q}\dbinom
{n-q}{\ell}t^{\ell}x_{k}^{n-q-\ell}
\label{pf.lem.sol.vander-hook.lap1.c.binom-used}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.vander-hook.lap1.c.binom-used}):}
Let $q\in\left\{  1,2,\ldots,n\right\}  $. Thus, $q\leq n$.
\par
Set $m=n-q$. Then, $m=n-q\in\mathbb{N}$ (since $q\leq n$). Substituting $t$
and $x_{k}$ for $X$ and $Y$ in the equality
(\ref{pf.lem.sol.vander-hook.lap1.c.binom}), we obtain%
\begin{align*}
\left(  t+x_{k}\right)  ^{m}  &  =\sum_{\ell=0}^{m}\dbinom{m}{\ell}t^{\ell
}x_{k}^{m-\ell}=\underbrace{\dbinom{m}{0}}_{=1}\underbrace{t^{0}}%
_{=1}\underbrace{x_{k}^{m-0}}_{=x_{k}^{m}}+\sum_{\ell=1}^{m}\dbinom{m}{\ell
}t^{\ell}x_{k}^{m-\ell}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}\ell=0\text{ from the sum}\right) \\
&  =x_{k}^{m}+\sum_{\ell=1}^{m}\dbinom{m}{\ell}t^{\ell}x_{k}^{m-\ell}.
\end{align*}
Subtracting $x_{k}^{m}$ from both sides of this equality, we obtain%
\[
\left(  t+x_{k}\right)  ^{m}-x_{k}^{m}=\sum_{\ell=1}^{m}\dbinom{m}{\ell
}t^{\ell}x_{k}^{m-\ell}.
\]
Since $t+x_{k}=x_{k}+t$, this rewrites as
\[
\left(  x_{k}+t\right)  ^{m}-x_{k}^{m}=\sum_{\ell=1}^{m}\dbinom{m}{\ell
}t^{\ell}x_{k}^{m-\ell}.
\]
Since $m=n-q$, this rewrites as
\[
\left(  x_{k}+t\right)  ^{n-q}-x_{k}^{n-q}=\sum_{\ell=1}^{n-q}\dbinom
{n-q}{\ell}t^{\ell}x_{k}^{n-q-\ell}.
\]
This proves (\ref{pf.lem.sol.vander-hook.lap1.c.binom-used}).}.

But%
\begin{align*}
&  \underbrace{V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right)  }_{\substack{=\sum_{q=1}^{n}\left(  -1\right)
^{k+q}\left(  x_{k}+t\right)  ^{n-q}\det\left(  A_{\sim k,\sim q}\right)
\\\text{(by Lemma \ref{lem.sol.vander-hook.lap1} \textbf{(b)})}}%
}-\underbrace{V\left(  x_{1},x_{2},\ldots,x_{n}\right)  }_{\substack{=\sum
_{q=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q}\det\left(  A_{\sim k,\sim
q}\right)  \\\text{(by Lemma \ref{lem.sol.vander-hook.lap1} \textbf{(a)})}}}\\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\left(  x_{k}+t\right)  ^{n-q}%
\det\left(  A_{\sim k,\sim q}\right)  -\sum_{q=1}^{n}\left(  -1\right)
^{k+q}x_{k}^{n-q}\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\underbrace{\left(  \left(
x_{k}+t\right)  ^{n-q}\det\left(  A_{\sim k,\sim q}\right)  -x_{k}^{n-q}%
\det\left(  A_{\sim k,\sim q}\right)  \right)  }_{=\left(  \left(
x_{k}+t\right)  ^{n-q}-x_{k}^{n-q}\right)  \det\left(  A_{\sim k,\sim
q}\right)  }\\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\underbrace{\left(  \left(
x_{k}+t\right)  ^{n-q}-x_{k}^{n-q}\right)  }_{\substack{=\sum_{\ell=1}%
^{n-q}\dbinom{n-q}{\ell}t^{\ell}x_{k}^{n-q-\ell}\\\text{(by
(\ref{pf.lem.sol.vander-hook.lap1.c.binom-used}))}}}\det\left(  A_{\sim k,\sim
q}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\left(  \sum_{\ell=1}^{n-q}%
\dbinom{n-q}{\ell}t^{\ell}x_{k}^{n-q-\ell}\right)  \det\left(  A_{\sim k,\sim
q}\right) \\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\underbrace{\left(  -1\right)
^{k+q}\dbinom{n-q}{\ell}t^{\ell}}_{=\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.lap1} \textbf{(c)}.
\end{proof}

\begin{lemma}
\label{lem.sol.vander-hook.lap2}Let $n\in\mathbb{N}$. Let $x_{1},x_{2}%
,\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $A$ be the $n\times
n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Let
$q\in\left\{  1,2,\ldots,n\right\}  $. Let $\ell\in\left\{  1,2,\ldots
,n-q\right\}  $. Then,%
\[
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell+1}\det\left(  A_{\sim
k,\sim q}\right)  =\delta_{\ell,1}\det A.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.lap2}.]From $\ell\in\left\{
1,2,\ldots,n-q\right\}  $, we obtain $\ell\geq1$ and $\ell\leq n-q$. Now,
$q+\underbrace{\ell}_{\leq n-q}-1\leq q+\left(  n-q\right)  -1=n-1\leq n$.
Also, $q\in\left\{  1,2,\ldots,n\right\}  $, so that $q\geq1$. Hence,
$\underbrace{q}_{\geq1}+\underbrace{\ell}_{\geq1}-1\geq1+1-1=1$. Combining
this with $q+\ell-1\leq n$, we obtain $q+\ell-1\in\left\{  1,2,\ldots
,n\right\}  $.

\begin{vershort}
It is straightforward to see that $\delta_{q,q+\ell-1}=\delta_{\ell,1}$.
\end{vershort}

\begin{verlong}
We have $\delta_{q,q+\ell-1}=\delta_{\ell,1}$\ \ \ \ \footnote{\textit{Proof.}
We are in one of the following two cases:
\par
\textit{Case 1:} We have $\ell=1$.
\par
\textit{Case 2:} We have $\ell\neq1$.
\par
Let us first consider Case 1. In this case, we have $\ell=1$. Thus,
$\delta_{\ell,1}=1$. But $q+\underbrace{\ell}_{=1}-1=q+1-1=q$, so that
$q=q+\ell-1$. Hence, $\delta_{q,q+\ell-1}=1$. Comparing this with
$\delta_{\ell,1}=1$, we obtain $\delta_{q,q+\ell-1}=\delta_{\ell,1}$. Hence,
$\delta_{q,q+\ell-1}=\delta_{\ell,1}$ is proven in Case 1.
\par
Let us first consider Case 2. In this case, we have $\ell\neq1$. Thus,
$\delta_{\ell,1}=0$. But $q+\ell-1\neq q$ (since $\left(  q+\ell-1\right)
-q=\ell-1\neq0$ (since $\ell\neq1$)). In other words, $q\neq q+\ell-1$. Hence,
$\delta_{q,q+\ell-1}=0$. Comparing this with $\delta_{\ell,1}=0$, we obtain
$\delta_{q,q+\ell-1}=\delta_{\ell,1}$. Hence, $\delta_{q,q+\ell-1}%
=\delta_{\ell,1}$ is proven in Case 2.
\par
We have now proven $\delta_{q,q+\ell-1}=\delta_{\ell,1}$ in each of the two
Cases 1 and 2. Since these two Cases cover all possibilities, this shows that
$\delta_{q,q+\ell-1}=\delta_{\ell,1}$ always holds. Qed.}.
\end{verlong}

We have $A=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by
the definition of $A$) and $q+\ell-1\in\left\{  1,2,\ldots,n\right\}  $.
Hence, Lemma \ref{lem.sol.vander-hook.lap0} (applied to $x_{i}^{n-j}$ and
$q+\ell-1$ instead of $a_{i,j}$ and $r$) yields%
\[
\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-\left(  q+\ell-1\right)  }%
\det\left(  A_{\sim p,\sim q}\right)  =\underbrace{\delta_{q,q+\ell-1}%
}_{=\delta_{\ell,1}}\det A=\delta_{\ell,1}\det A.
\]
Thus,%
\begin{align*}
\delta_{\ell,1}\det A  &  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}%
\underbrace{x_{p}^{n-\left(  q+\ell-1\right)  }}_{\substack{=x_{p}%
^{n-q-\ell+1}\\\text{(since }n-\left(  q+\ell-1\right)  =n-q-\ell+1\text{)}%
}}\det\left(  A_{\sim p,\sim q}\right) \\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-q-\ell+1}\det\left(
A_{\sim p,\sim q}\right)  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}%
x_{k}^{n-q-\ell+1}\det\left(  A_{\sim k,\sim q}\right)
\end{align*}
(here, we have renamed the summation index $p$ as $k$). This proves Lemma
\ref{lem.sol.vander-hook.lap2}.
\end{proof}

\subsubsection{The solution}

\begin{vershort}


\begin{proof}
[Solution to Exercise \ref{exe.vander-hook}.]Let $A$ be the $n\times n$-matrix
$\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

We have%
\begin{align}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -\sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots
,x_{n}\right) \nonumber\\
&  =\sum_{k=1}^{n}x_{k}\underbrace{\left(  V\left(  x_{1},x_{2},\ldots
,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  -V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  \right)  }_{\substack{=\sum_{q=1}^{n}\sum_{\ell
=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell
}\det\left(  A_{\sim k,\sim q}\right)  \\\text{(by Lemma
\ref{lem.sol.vander-hook.lap1} \textbf{(c)})}}}\nonumber\\
&  =\sum_{k=1}^{n}x_{k}\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell
}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim
q}\right) \nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\sum_{k=1}%
^{n}\underbrace{x_{k}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}}_{=\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell+1}}\det\left(  A_{\sim k,\sim q}\right)
\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}%
\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell+1}%
\det\left(  A_{\sim k,\sim q}\right)  }_{\substack{=\delta_{\ell,1}\det
A\\\text{(by Lemma \ref{lem.sol.vander-hook.lap2})}}}\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\delta_{\ell
,1}\det A=\sum_{q=1}^{n}\left(  \sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell
}\delta_{\ell,1}\right)  \det A\nonumber\\
&  =\sum_{r=0}^{n-1}\left(  \sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}%
\delta_{\ell,1}\right)  \det A \label{sol.vander-hook.short.3}%
\end{align}
(here, we have substituted $r$ for $n-q$ in the outer sum).

But every $r\in\left\{  0,1,\ldots,n-1\right\}  $ satisfies%
\begin{equation}
\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}\delta_{\ell,1}=rt
\label{sol.vander-hook.short.suml}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-hook.short.suml}):} Let
$r\in\left\{  0,1,\ldots,n-1\right\}  $. We must prove
(\ref{sol.vander-hook.short.suml}).
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $r\neq0$.
\par
\textit{Case 2:} We have $r=0$.
\par
Let us consider Case 1 first. In this case, we have $r\neq0$. Hence, $r>0$.
Thus, $1\in\left\{  1,2,\ldots,r\right\}  $. Thus, we can split off the addend
for $\ell=1$ from the sum $\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}%
\delta_{\ell,1}$. As a result, we obtain
\[
\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}\delta_{\ell,1}=\underbrace{\dbinom
{r}{1}}_{=r}\underbrace{t^{1}}_{=t}\underbrace{\delta_{1,1}}_{=1}+\sum
_{\ell=2}^{r}\dbinom{r}{\ell}t^{\ell}\underbrace{\delta_{\ell,1}%
}_{\substack{=0\\\text{(since }\ell\neq1\text{)}}}=rt+\underbrace{\sum
_{\ell=2}^{r}\dbinom{r}{\ell}t^{\ell}0}_{=0}=rt.
\]
Thus, (\ref{sol.vander-hook.short.suml}) is proven in Case 1.
\par
Proving (\ref{sol.vander-hook.short.suml}) in Case 2 is straightforward and
left to the reader.
\par
We now have proven (\ref{sol.vander-hook.short.suml}) in each of the two Cases
1 and 2. Thus, (\ref{sol.vander-hook.short.suml}) always holds.}.

Now, (\ref{sol.vander-hook.short.3}) becomes%
\begin{align*}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots
,x_{n}\right) \\
&  =\sum_{r=0}^{n-1}\underbrace{\left(  \sum_{\ell=1}^{r}\dbinom{r}{\ell
}t^{\ell}\delta_{\ell,1}\right)  }_{\substack{=rt\\\text{(by
(\ref{sol.vander-hook.short.suml}))}}}\det A=\sum_{r=0}^{n-1}rt\det A\\
&  =\underbrace{\left(  \sum_{r=0}^{n-1}r\right)  }_{\substack{=\dbinom{n}%
{2}\\\text{(by (\ref{sol.vander-hook.gauss}))}}}t\underbrace{\det
A}_{\substack{=V\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
(\ref{eq.lem.sol.vander-hook.V=x.eq}))}}}\\
&  =\dbinom{n}{2}tV\left(  x_{1},x_{2},\ldots,x_{n}\right)  .
\end{align*}
If we add $\sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ to
both sides of this equality, then we obtain%
\begin{align*}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =\dbinom{n}{2}tV\left(  x_{1},x_{2},\ldots,x_{n}\right)  +\sum_{k=1}%
^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{n}\right) \\
&  =\left(  \dbinom{n}{2}t+\sum_{k=1}^{n}x_{k}\right)  V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  .
\end{align*}
This solves Exercise \ref{exe.vander-hook}.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Solution to Exercise \ref{exe.vander-hook}.]Let $A$ be the $n\times n$-matrix
$\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

We have%
\begin{align}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -\sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots
,x_{n}\right) \nonumber\\
&  =\sum_{k=1}^{n}x_{k}\underbrace{\left(  V\left(  x_{1},x_{2},\ldots
,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  -V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  \right)  }_{\substack{=\sum_{q=1}^{n}\sum_{\ell
=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell
}\det\left(  A_{\sim k,\sim q}\right)  \\\text{(by Lemma
\ref{lem.sol.vander-hook.lap1} \textbf{(c)})}}}\nonumber\\
&  =\sum_{k=1}^{n}\underbrace{x_{k}\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}%
\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }_{=\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}x_{k}%
\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }\nonumber\\
&  =\underbrace{\sum_{k=1}^{n}\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}}_{=\sum
_{q=1}^{n}\sum_{\ell=1}^{n-q}\sum_{k=1}^{n}}\underbrace{x_{k}\dbinom{n-q}%
{\ell}t^{\ell}\left(  -1\right)  ^{k+q}}_{=\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}x_{k}}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)
\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\sum_{k=1}^{n}\dbinom{n-q}{\ell}t^{\ell
}\left(  -1\right)  ^{k+q}\underbrace{x_{k}x_{k}^{n-q-\ell}}_{=x_{k}%
^{n-q-\ell+1}}\det\left(  A_{\sim k,\sim q}\right) \nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\underbrace{\sum_{k=1}^{n}\dbinom
{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell+1}\det\left(
A_{\sim k,\sim q}\right)  }_{=\dbinom{n-q}{\ell}t^{\ell}\sum_{k=1}^{n}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell+1}\det\left(  A_{\sim k,\sim q}\right)
}\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}%
\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell+1}%
\det\left(  A_{\sim k,\sim q}\right)  }_{\substack{=\delta_{\ell,1}\det
A\\\text{(by Lemma \ref{lem.sol.vander-hook.lap2})}}}\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\delta_{\ell
,1}\det A=\sum_{q=1}^{n}\left(  \sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell
}\delta_{\ell,1}\right)  \det A\nonumber\\
&  =\sum_{r=0}^{n-1}\left(  \sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}%
\delta_{\ell,1}\right)  \det A \label{sol.vander-hook.3}%
\end{align}
(here, we have substituted $r$ for $n-q$ in the outer sum).

But every $r\in\left\{  0,1,\ldots,n-1\right\}  $ satisfies%
\begin{equation}
\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}\delta_{\ell,1}=rt
\label{sol.vander-hook.suml}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.vander-hook.suml}):} Let $r\in\left\{
0,1,\ldots,n-1\right\}  $. We must prove (\ref{sol.vander-hook.suml}).
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $r\neq0$.
\par
\textit{Case 2:} We have $r=0$.
\par
Let us consider Case 1 first. In this case, we have $r\neq0$. But $r\geq0$
(since $r\in\left\{  0,1,\ldots,n-1\right\}  $). Hence, $r>0$ (since $r\neq
0$). Thus, $1\in\left\{  1,2,\ldots,r\right\}  $. Thus, we can split off the
addend for $\ell=1$ from the sum $\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell
}\delta_{\ell,1}$. As a result, we obtain
\[
\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}\delta_{\ell,1}=\underbrace{\dbinom
{r}{1}}_{=r}\underbrace{t^{1}}_{=t}\underbrace{\delta_{1,1}}%
_{\substack{=1\\\text{(since }1=1\text{)}}}+\sum_{\ell=2}^{r}\dbinom{r}{\ell
}t^{\ell}\underbrace{\delta_{\ell,1}}_{\substack{=0\\\text{(since }\ell
\neq1\\\text{(since }\ell\geq2>1\text{))}}}=rt+\underbrace{\sum_{\ell=2}%
^{r}\dbinom{r}{\ell}t^{\ell}0}_{=0}=rt.
\]
Thus, (\ref{sol.vander-hook.suml}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $r=0$. Hence,%
\[
\sum_{\ell=1}^{r}\dbinom{r}{\ell}t^{\ell}\delta_{\ell,1}=\sum_{\ell=1}%
^{0}\dbinom{0}{\ell}t^{\ell}\delta_{\ell,1}=\left(  \text{empty sum}\right)
=0=rt
\]
(since $\underbrace{r}_{=0}t=0t=0$). Thus, (\ref{sol.vander-hook.suml}) is
proven in Case 2.
\par
We now have proven (\ref{sol.vander-hook.suml}) in each of the two Cases 1 and
2. Since these two Cases cover all possibilities, this shows that
(\ref{sol.vander-hook.suml}) always holds. Qed.}.

Now, (\ref{sol.vander-hook.3}) becomes%
\begin{align*}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  \ \ \ \ \ \ \ \ \ \ -\sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots
,x_{n}\right) \\
&  =\sum_{r=0}^{n-1}\underbrace{\left(  \sum_{\ell=1}^{r}\dbinom{r}{\ell
}t^{\ell}\delta_{\ell,1}\right)  }_{\substack{=rt\\\text{(by
(\ref{sol.vander-hook.suml}))}}}\det A=\sum_{r=0}^{n-1}rt\det A\\
&  =\underbrace{\left(  \sum_{r=0}^{n-1}r\right)  }_{\substack{=\dbinom{n}%
{2}\\\text{(by (\ref{sol.vander-hook.gauss}))}}}t\underbrace{\det
A}_{\substack{=V\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
(\ref{eq.lem.sol.vander-hook.V=x.eq}))}}}\\
&  =\dbinom{n}{2}tV\left(  x_{1},x_{2},\ldots,x_{n}\right)  .
\end{align*}
If we add $\sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ to
both sides of this equality, then we obtain%
\begin{align*}
&  \sum_{k=1}^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =\dbinom{n}{2}tV\left(  x_{1},x_{2},\ldots,x_{n}\right)  +\sum_{k=1}%
^{n}x_{k}V\left(  x_{1},x_{2},\ldots,x_{n}\right) \\
&  =\left(  \dbinom{n}{2}t+\sum_{k=1}^{n}x_{k}\right)  V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  .
\end{align*}
This solves Exercise \ref{exe.vander-hook}.
\end{proof}
\end{verlong}

\subsubsection{Addendum: a simpler variant}

Exercise \ref{exe.vander-hook} is now solved, but let me discuss one further
fact, which is a variation on it. Namely, the following holds:

\begin{proposition}
\label{prop.sol.vander-hook.variant}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.
Then,%
\[
\sum_{k=1}^{n}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right)  =nV\left(  x_{1},x_{2},\ldots,x_{n}\right)  .
\]

\end{proposition}

We can prove this similarly to how we solved Exercise \ref{exe.vander-hook}.
Instead of Lemma \ref{lem.sol.vander-hook.lap2}, we use the following variant
of this lemma:

\begin{lemma}
\label{lem.sol.vander-hook.lap2.variant}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $A$ be the $n\times
n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Let
$q\in\left\{  1,2,\ldots,n\right\}  $. Let $\ell\in\left\{  1,2,\ldots
,n-q\right\}  $. Then,%
\[
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim
k,\sim q}\right)  =0.
\]

\end{lemma}

\begin{vershort}


\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.lap2.variant}.]Left to the reader.
(Very similar to the above proof of Lemma \ref{lem.sol.vander-hook.lap2}.)
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.lap2.variant}.]From $\ell\in\left\{
1,2,\ldots,n-q\right\}  $, we obtain $\ell\geq1$ and $\ell\leq n-q$. Now,
$q+\underbrace{\ell}_{\leq n-q}\leq q+\left(  n-q\right)  =n$. Also,
$q\in\left\{  1,2,\ldots,n\right\}  $, so that $q\geq1$. Hence,
$\underbrace{q}_{\geq1}+\underbrace{\ell}_{\geq1}\geq1+1\geq1$. Combining this
with $q+\ell\leq n$, we obtain $q+\ell\in\left\{  1,2,\ldots,n\right\}  $.

From $q+\underbrace{\ell}_{\geq1>0}>q$, we obtain $q+\ell\neq q$. Hence,
$q\neq q+\ell$ and thus $\delta_{q,q+\ell}=0$.

We have $A=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by
the definition of $A$) and $q+\ell\in\left\{  1,2,\ldots,n\right\}  $. Hence,
Lemma \ref{lem.sol.vander-hook.lap0} (applied to $x_{i}^{n-j}$ and $q+\ell$
instead of $a_{i,j}$ and $r$) yields%
\[
\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-\left(  q+\ell\right)  }%
\det\left(  A_{\sim p,\sim q}\right)  =\underbrace{\delta_{q,q+\ell}}_{=0}\det
A=0\det A=0.
\]
Thus,%
\begin{align*}
0  &  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}\underbrace{x_{p}^{n-\left(
q+\ell\right)  }}_{\substack{=x_{p}^{n-q-\ell}\\\text{(since }n-\left(
q+\ell\right)  =n-q-\ell\text{)}}}\det\left(  A_{\sim p,\sim q}\right) \\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-q-\ell}\det\left(  A_{\sim
p,\sim q}\right)  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}%
\det\left(  A_{\sim k,\sim q}\right)
\end{align*}
(here, we have renamed the summation index $p$ as $k$). Lemma
\ref{lem.sol.vander-hook.lap2.variant} is therefore proven.
\end{proof}
\end{verlong}

\begin{vershort}


\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.variant}.]This proof is
similar to our solution of Exercise \ref{exe.vander-hook}, but a lot simpler.
Again, the reader can fill in the details.
\end{proof}
\end{vershort}

\begin{verlong}


\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.variant}.]Let $A$ be the
$n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. We have%
\begin{align}
&  \sum_{k=1}^{n}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right)  -\sum_{k=1}^{n}V\left(  x_{1},x_{2},\ldots
,x_{n}\right) \nonumber\\
&  =\sum_{k=1}^{n}\underbrace{\left(  V\left(  x_{1},x_{2},\ldots
,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  -V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  \right)  }_{\substack{=\sum_{q=1}^{n}\sum_{\ell
=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell
}\det\left(  A_{\sim k,\sim q}\right)  \\\text{(by Lemma
\ref{lem.sol.vander-hook.lap1} \textbf{(c)})}}}\nonumber\\
&  =\underbrace{\sum_{k=1}^{n}\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}}_{=\sum
_{q=1}^{n}\sum_{\ell=1}^{n-q}\sum_{k=1}^{n}}\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)
\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\underbrace{\sum_{k=1}^{n}\dbinom
{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }_{=\dbinom{n-q}{\ell}t^{\ell}\sum_{k=1}^{n}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)
}\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}%
\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }_{\substack{=0\\\text{(by Lemma
\ref{lem.sol.vander-hook.lap2.variant})}}}\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=1}^{n-q}\dbinom{n-q}{\ell}t^{\ell}0=0.\nonumber
\end{align}
In other words,%
\begin{align*}
&  \sum_{k=1}^{n}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =\sum_{k=1}^{n}V\left(  x_{1},x_{2},\ldots,x_{n}\right)  =nV\left(
x_{1},x_{2},\ldots,x_{n}\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.vander-hook.variant}.
\end{proof}
\end{verlong}

\subsubsection{Addendum: another sum of Vandermonde determinants}

Here is one more result similar to Exercise \ref{exe.vander-hook} and
Proposition \ref{prop.sol.vander-hook.variant}:

\begin{proposition}
\label{prop.sol.vander-hook.N}Let $n\in\mathbb{N}$. Let $x_{1},x_{2}%
,\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $m\in\left\{
0,1,\ldots,n-1\right\}  $. Let $t\in\mathbb{K}$. Then,%
\[
\sum_{k=1}^{n}x_{k}^{m}V\left(  x_{1},x_{2},\ldots,x_{k-1},t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right)  =t^{m}V\left(  x_{1},x_{2},\ldots,x_{n}\right)
.
\]

\end{proposition}

We have already built all the tools necessary for the proof of this
proposition. We just need to repurpose a few of them:

\begin{lemma}
\label{lem.sol.vander-hook.N.lap1b}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.
Let $A$ be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Let $k\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\[
V\left(  x_{1},x_{2},\ldots,x_{k-1},t,x_{k+1},x_{k+2},\ldots,x_{n}\right)
=\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim
q}\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.N.lap1b}.]Lemma
\ref{lem.sol.vander-hook.lap1} \textbf{(b)} (applied to $t-x_{k}$ instead of
$t$) yields%
\begin{align*}
&  V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+\left(  t-x_{k}\right)
,x_{k+1},x_{k+2},\ldots,x_{n}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\left(  x_{k}+\left(
t-x_{k}\right)  \right)  ^{n-q}\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}
Since $x_{k}+\left(  t-x_{k}\right)  =t$, this rewrites as
\[
V\left(  x_{1},x_{2},\ldots,x_{k-1},t,x_{k+1},x_{k+2},\ldots,x_{n}\right)
=\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim
q}\right)  .
\]
This proves Lemma \ref{lem.sol.vander-hook.N.lap1b}.
\end{proof}

\begin{lemma}
\label{lem.sol.vander-hook.N.lap2}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $A$ be the $n\times
n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Let
$q\in\left\{  1,2,\ldots,n\right\}  $ and $m\in\left\{  0,1,\ldots
,n-1\right\}  $. Then,%
\[
\sum_{k=1}^{n}x_{k}^{m}\left(  -1\right)  ^{k+q}\det\left(  A_{\sim k,\sim
q}\right)  =\delta_{q,n-m}\det A.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.N.lap2}.]From $q\in\left\{
1,2,\ldots,n\right\}  $, we obtain $1\leq q\leq n$. Hence, $1\leq n$, so that
$n\geq1$ and therefore $n\in\left\{  1,2,\ldots,n\right\}  $.

We have $A=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by
the definition of $A$) and $n-m\in\left\{  1,2,\ldots,n\right\}  $ (since
$m\in\left\{  0,1,\ldots,n-1\right\}  $). Hence, Lemma
\ref{lem.sol.vander-hook.lap0} (applied to $x_{i}^{n-j}$ and $n-m$ instead of
$a_{i,j}$ and $r$) yields%
\[
\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-\left(  n-m\right)  }%
\det\left(  A_{\sim p,\sim q}\right)  =\delta_{q,n-m}\det A.
\]
Thus,%
\begin{align*}
\delta_{q,n-m}\det A  &  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}%
\underbrace{x_{p}^{n-\left(  n-m\right)  }}_{\substack{=x_{p}^{m}%
\\\text{(since }n-\left(  n-m\right)  =m\text{)}}}\det\left(  A_{\sim p,\sim
q}\right) \\
&  =\sum_{p=1}^{n}\underbrace{\left(  -1\right)  ^{p+q}x_{p}^{m}}_{=x_{p}%
^{m}\left(  -1\right)  ^{p+q}}\det\left(  A_{\sim p,\sim q}\right) \\
&  =\sum_{p=1}^{n}x_{p}^{m}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  =\sum_{k=1}^{n}x_{k}^{m}\left(  -1\right)  ^{k+q}\det\left(
A_{\sim k,\sim q}\right)
\end{align*}
(here, we have renamed the summation index $p$ as $k$). This proves Lemma
\ref{lem.sol.vander-hook.N.lap2}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.N}.]Let $A$ be the $n\times
n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

From $m\in\left\{  0,1,\ldots,n-1\right\}  $, we obtain $n-m\in\left\{
1,2,\ldots,n\right\}  $. But%
\begin{align*}
&  \sum_{k=1}^{n}x_{k}^{m}\underbrace{V\left(  x_{1},x_{2},\ldots
,x_{k-1},t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  }_{\substack{=\sum_{q=1}%
^{n}\left(  -1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)
\\\text{(by Lemma \ref{lem.sol.vander-hook.N.lap1b})}}}\\
&  =\sum_{k=1}^{n}\underbrace{x_{k}^{m}\sum_{q=1}^{n}\left(  -1\right)
^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  }_{=\sum_{q=1}^{n}x_{k}%
^{m}\left(  -1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  }\\
&  =\underbrace{\sum_{k=1}^{n}\sum_{q=1}^{n}}_{=\sum_{q=1}^{n}\sum_{k=1}^{n}%
}x_{k}^{m}\left(  -1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)
\\
&  =\sum_{q=1}^{n}\underbrace{\sum_{k=1}^{n}x_{k}^{m}\left(  -1\right)
^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  }_{=t^{n-q}\sum_{k=1}%
^{n}x_{k}^{m}\left(  -1\right)  ^{k+q}\det\left(  A_{\sim k,\sim q}\right)
}\\
&  =\underbrace{\sum_{q=1}^{n}}_{=\sum_{q\in\left\{  1,2,\ldots,n\right\}  }%
}t^{n-q}\underbrace{\sum_{k=1}^{n}x_{k}^{m}\left(  -1\right)  ^{k+q}%
\det\left(  A_{\sim k,\sim q}\right)  }_{\substack{=\delta_{q,n-m}\det
A\\\text{(by Lemma \ref{lem.sol.vander-hook.N.lap2})}}}=\sum_{q\in\left\{
1,2,\ldots,n\right\}  }t^{n-q}\delta_{q,n-m}\det A\\
&  =\sum_{\substack{q\in\left\{  1,2,\ldots,n\right\}  ;\\q\neq n-m}%
}t^{n-q}\underbrace{\delta_{q,n-m}}_{\substack{=0\\\text{(since }q\neq
n-m\text{)}}}\det A+\underbrace{t^{n-\left(  n-m\right)  }}_{\substack{=t^{m}%
\\\text{(since }n-\left(  n-m\right)  =m\text{)}}}\underbrace{\delta
_{n-m,n-m}}_{\substack{=1\\\text{(since }n-m=n-m\text{)}}}\det A\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }q=n-m\text{ from the sum}\\
\text{(since }n-m\in\left\{  1,2,\ldots,n\right\}  \text{)}%
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{q\in\left\{  1,2,\ldots,n\right\}  ;\\q\neq
n-m}}t^{n-q}0\det A}_{=0}+t^{m}\det A=t^{m}\underbrace{\det A}%
_{\substack{=V\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
(\ref{eq.lem.sol.vander-hook.V=x.eq}))}}}\\
&  =t^{m}V\left(  x_{1},x_{2},\ldots,x_{n}\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.vander-hook.N}.
\end{proof}

\subsubsection{Addendum: analogues involving products of all but one $x_{j}$}

Let us finally prove a much more complicated analogue of Exercise
\ref{exe.vander-hook} and Proposition \ref{prop.sol.vander-hook.variant}. We
shall use the following notations:

\begin{definition}
\label{def.sol.vander-hook.elsyms}Let $n\in\mathbb{N}$. Let $\left[  n\right]
$ denote the set $\left\{  1,2,\ldots,n\right\}  $. As usual, let
$\mathcal{P}\left(  \left[  n\right]  \right)  $ denote the powerset of
$\left[  n\right]  $. Let $x_{1},x_{2},\ldots,x_{n}$ be $n$ elements of
$\mathbb{K}$.

\textbf{(a)} For every $j\in\mathbb{N}$, define an element $e_{j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  \in\mathbb{K}$ by
\[
e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{I\subseteq
\left[  n\right]  ;\\\left\vert I\right\vert =j}}\prod_{i\in I}x_{i}.
\]


(Here, as usual, the summation sign $\sum_{\substack{I\subseteq\left[
n\right]  ;\\\left\vert I\right\vert =j}}$ means $\sum_{\substack{I\in
\mathcal{P}\left(  \left[  n\right]  \right)  ;\\\left\vert I\right\vert =j}}$.)

\textbf{(b)} For every $t\in\mathbb{K}$, define an element $z_{t}\left(
x_{1},x_{2},\ldots,x_{n}\right)  \in\mathbb{K}$ by%
\[
z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{j=0}^{n-1}e_{n-1-j}%
\left(  x_{1},x_{2},\ldots,x_{n}\right)  t^{j}.
\]

\end{definition}

\begin{proposition}
\label{prop.sol.vander-hook.variant-1}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.
For each $i\in\left\{  1,2,\ldots,n\right\}  $, set $y_{i}=\prod
_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq i}}x_{j}$. Then,%
\begin{align*}
&  \sum_{k=1}^{n}y_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \cdot V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  .
\end{align*}

\end{proposition}

Before we start proving Proposition \ref{prop.sol.vander-hook.variant-1}, let
us explore a few properties of the elements defined in Definition
\ref{def.sol.vander-hook.elsyms}:

\begin{proposition}
\label{prop.sol.vander-hook.viete}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.

\textbf{(a)} We have%
\[
\prod_{i=1}^{n}\left(  x_{i}+t\right)  =\sum_{j=0}^{n}e_{n-j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  t^{j}.
\]


\textbf{(b)} We have%
\[
e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\prod_{i=1}^{n}x_{i}.
\]


\textbf{(c)} We have%
\[
t\cdot z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\prod_{i=1}^{n}\left(
x_{i}+t\right)  -\prod_{i=1}^{n}x_{i}.
\]


\textbf{(d)} Assume that the element $t$ of $\mathbb{K}$ is invertible. Then,%
\[
z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\dfrac{\prod_{i=1}^{n}\left(
x_{i}+t\right)  -\prod_{i=1}^{n}x_{i}}{t}.
\]

\end{proposition}

Proposition \ref{prop.sol.vander-hook.viete} \textbf{(a)} is one of several
interconnected results known as \textit{Vieta's formulas}. Proposition
\ref{prop.sol.vander-hook.viete} \textbf{(d)} gives an alternative description
of $z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ in the case when $t$ is invertible.

\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.viete}.]We shall use the
notations $\left[  n\right]  $, $\mathcal{P}\left(  \left[  n\right]  \right)
$ and $\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}$ as in Definition \ref{def.sol.vander-hook.elsyms}.

\begin{vershort}
We have $\left\vert \left[  n\right]  \right\vert =n$. Hence, every subset $I$
of $\left[  n\right]  $ satisfies $\left\vert I\right\vert \in\left\{
0,1,\ldots,n\right\}  $.
\end{vershort}

\begin{verlong}
Every subset $I$ of $\left[  n\right]  $ satisfies $\left\vert I\right\vert
\in\left\{  0,1,\ldots,n\right\}  $\ \ \ \ \footnote{\textit{Proof.} Let $I$
be a subset of $\left[  n\right]  $. Then, $I\subseteq\left[  n\right]  $, so
that $\left\vert I\right\vert \leq\left\vert \left[  n\right]  \right\vert
=n$. In particular, $\left\vert I\right\vert \in\mathbb{N}$. Thus, $\left\vert
I\right\vert \in\left\{  0,1,\ldots,n\right\}  $ (since $\left\vert
I\right\vert \in\mathbb{N}$ and $\left\vert I\right\vert \leq n$). Qed.}.
\end{verlong}

\begin{vershort}
\textbf{(a)} Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} (applied to
$a_{i}=x_{i}$ and $b_{i}=t$) yields%
\begin{align*}
\prod_{i=1}^{n}\left(  x_{i}+t\right)   &  =\underbrace{\sum_{I\subseteq
\left[  n\right]  }}_{\substack{=\sum_{j\in\left\{  0,1,\ldots,n\right\}
}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}\\\text{(since every subset }I\text{ of }\left[  n\right]
\\\text{satisfies }\left\vert I\right\vert \in\left\{  0,1,\ldots,n\right\}
\text{)}}}\left(  \prod_{i\in I}x_{i}\right)  \underbrace{\left(  \prod
_{i\in\left[  n\right]  \setminus I}t\right)  }_{\substack{=t^{\left\vert
\left[  n\right]  \setminus I\right\vert }=t^{\left\vert \left[  n\right]
\right\vert -\left\vert I\right\vert }\\\text{(since }\left\vert \left[
n\right]  \setminus I\right\vert =\left\vert \left[  n\right]  \right\vert
-\left\vert I\right\vert \\\text{(because }I\subseteq\left[  n\right]
\text{))}}}\\
&  =\underbrace{\sum_{j\in\left\{  0,1,\ldots,n\right\}  }}_{=\sum_{j=0}^{n}%
}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}\left(  \prod_{i\in I}x_{i}\right)  \underbrace{t^{\left\vert \left[
n\right]  \right\vert -\left\vert I\right\vert }}_{\substack{=t^{n-j}%
\\\text{(since }\left\vert \left[  n\right]  \right\vert =n\text{ and
}\left\vert I\right\vert =j\text{)}}}\\
&  =\sum_{j=0}^{n}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert
I\right\vert =j}}\left(  \prod_{i\in I}x_{i}\right)  t^{n-j}.
\end{align*}
Comparing this with%
\begin{align*}
\sum_{j=0}^{n}e_{n-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  t^{j}  &
=\sum_{j=0}^{n}\underbrace{e_{n-\left(  n-j\right)  }\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  }_{\substack{=e_{j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  \\=\sum_{\substack{I\subseteq\left[  n\right]
;\\\left\vert I\right\vert =j}}\prod_{i\in I}x_{i}}}t^{n-j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n-j\text{ for
}j\text{ in the sum}\right) \\
&  =\sum_{j=0}^{n}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert
I\right\vert =j}}\left(  \prod_{i\in I}x_{i}\right)  t^{n-j},
\end{align*}
we obtain%
\[
\prod_{i=1}^{n}\left(  x_{i}+t\right)  =\sum_{j=0}^{n}e_{n-j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  t^{j}.
\]
This proves Proposition \ref{prop.sol.vander-hook.viete} \textbf{(a)}.
\end{vershort}

\begin{verlong}
\textbf{(a)} Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} (applied to
$a_{i}=x_{i}$ and $b_{i}=t$) yields%
\begin{align*}
\prod_{i=1}^{n}\left(  x_{i}+t\right)   &  =\underbrace{\sum_{I\subseteq
\left[  n\right]  }}_{\substack{=\sum_{j\in\left\{  0,1,\ldots,n\right\}
}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}\\\text{(since every subset }I\text{ of }\left[  n\right]
\\\text{satisfies }\left\vert I\right\vert \in\left\{  0,1,\ldots,n\right\}
\text{)}}}\left(  \prod_{i\in I}x_{i}\right)  \underbrace{\left(  \prod
_{i\in\left[  n\right]  \setminus I}t\right)  }_{\substack{=t^{\left\vert
\left[  n\right]  \setminus I\right\vert }=t^{\left\vert \left[  n\right]
\right\vert -\left\vert I\right\vert }\\\text{(since }\left\vert \left[
n\right]  \setminus I\right\vert =\left\vert \left[  n\right]  \right\vert
-\left\vert I\right\vert \\\text{(because }I\subseteq\left[  n\right]
\text{))}}}\\
&  =\underbrace{\sum_{j\in\left\{  0,1,\ldots,n\right\}  }}_{=\sum_{j=0}^{n}%
}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}\left(  \prod_{i\in I}x_{i}\right)  \underbrace{t^{\left\vert \left[
n\right]  \right\vert -\left\vert I\right\vert }}_{\substack{=t^{n-j}%
\\\text{(since }\left\vert \left[  n\right]  \right\vert =n\text{ and
}\left\vert I\right\vert =j\text{)}}}\\
&  =\sum_{j=0}^{n}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert
I\right\vert =j}}\left(  \prod_{i\in I}x_{i}\right)  t^{n-j}.
\end{align*}
Comparing this with%
\begin{align*}
&  \sum_{j=0}^{n}e_{n-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  t^{j}\\
&  =\sum_{j=0}^{n}\underbrace{e_{n-\left(  n-j\right)  }\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  }_{\substack{=e_{j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  \\\text{(since }n-\left(  n-j\right)  =j\text{)}%
}}t^{n-j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n-j\text{ for
}j\text{ in the sum}\right) \\
&  =\sum_{j=0}^{n}\underbrace{e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)
}_{\substack{=\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert
I\right\vert =j}}\prod_{i\in I}x_{i}\\\text{(by the definition of }%
e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \text{)}}}t^{n-j}\\
&  =\sum_{j=0}^{n}\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert
I\right\vert =j}}\left(  \prod_{i\in I}x_{i}\right)  t^{n-j},
\end{align*}
we obtain%
\[
\prod_{i=1}^{n}\left(  x_{i}+t\right)  =\sum_{j=0}^{n}e_{n-j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  t^{j}.
\]
This proves Proposition \ref{prop.sol.vander-hook.viete} \textbf{(a)}.
\end{verlong}

\textbf{(b)} If $I$ is a subset of $\left[  n\right]  $, then we have the
following logical equivalence:%
\begin{equation}
\left(  \left\vert I\right\vert =n\right)  \ \Longleftrightarrow\ \left(
I=\left[  n\right]  \right)  . \label{pf.prop.sol.vander-hook.viete.b.equiv}%
\end{equation}


\begin{vershort}
(The proof of (\ref{pf.prop.sol.vander-hook.viete.b.equiv}) is obvious, since
$\left\vert \left[  n\right]  \right\vert =n$.)
\end{vershort}

\begin{verlong}
[\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete.b.equiv}):} Let $I$ be a
subset of $\left[  n\right]  $.

Assume first that $\left\vert I\right\vert =n$. From $I\subseteq\left[
n\right]  $, we obtain $\left\vert \left[  n\right]  \right\vert =\left\vert
I\right\vert +\left\vert \left[  n\right]  \setminus I\right\vert $ (since the
set $\left[  n\right]  $ is finite). Comparing this with $\left\vert \left[
n\right]  \right\vert =n$, we find $\left\vert I\right\vert +\left\vert
\left[  n\right]  \setminus I\right\vert =n$. Since $\left\vert I\right\vert
=n$, this rewrites as $n+\left\vert \left[  n\right]  \setminus I\right\vert
=n$. Subtracting $n$ from both sides of this equality, we obtain $\left\vert
\left[  n\right]  \setminus I\right\vert =0$. Thus, $\left[  n\right]
\setminus I=\varnothing$. In other words, $\left[  n\right]  \subseteq I$.
Combined with $I\subseteq\left[  n\right]  $, this yields $I=\left[  n\right]
$.

Now, let us forget our assumption that $\left\vert I\right\vert =n$. We thus
have shown that $I=\left[  n\right]  $ under the assumption that $\left\vert
I\right\vert =n$. In other words, we have proven the implication%
\begin{equation}
\left(  \left\vert I\right\vert =n\right)  \ \Longrightarrow\ \left(
I=\left[  n\right]  \right)  .
\label{pf.prop.sol.vander-hook.viete.b.equiv.pf.1}%
\end{equation}


On the other hand, if $I=\left[  n\right]  $, then $\left\vert I\right\vert
=n$ (because if $I=\left[  n\right]  $, then $\left\vert \underbrace{I}%
_{=\left[  n\right]  }\right\vert =\left\vert \left[  n\right]  \right\vert
=n$). In other words, the implication%
\[
\left(  I=\left[  n\right]  \right)  \ \Longrightarrow\ \left(  \left\vert
I\right\vert =n\right)
\]
holds. Combining this implication with
(\ref{pf.prop.sol.vander-hook.viete.b.equiv.pf.1}), we obtain the equivalence
$\left(  \left\vert I\right\vert =n\right)  \ \Longleftrightarrow\ \left(
I=\left[  n\right]  \right)  $. Thus,
(\ref{pf.prop.sol.vander-hook.viete.b.equiv}) is proven.]
\end{verlong}

\begin{vershort}
Now, the definition of $e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ yields%
\begin{align*}
e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)   &  =\underbrace{\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert =n}%
}}_{\substack{=\sum_{\substack{I\subseteq\left[  n\right]  ;\\I=\left[
n\right]  }}\\\text{(by the equivalence
(\ref{pf.prop.sol.vander-hook.viete.b.equiv}))}}}\prod_{i\in I}x_{i}%
=\sum_{\substack{I\subseteq\left[  n\right]  ;\\I=\left[  n\right]  }%
}\prod_{i\in I}x_{i}\\
&  =\underbrace{\prod_{i\in\left[  n\right]  }}_{=\prod_{i=1}^{n}}%
x_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  n\right]  \text{ is a
subset of }\left[  n\right]  \right) \\
&  =\prod_{i=1}^{n}x_{i}.
\end{align*}
This proves Proposition \ref{prop.sol.vander-hook.viete} \textbf{(b)}.
\end{vershort}

\begin{verlong}
Now, the definition of $e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ yields%
\[
e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\underbrace{\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert =n}%
}}_{\substack{=\sum_{\substack{I\subseteq\left[  n\right]  ;\\I=\left[
n\right]  }}\\\text{(because for every subset }I\text{ of }\left[  n\right]
\text{,}\\\text{the condition }\left(  \left\vert I\right\vert =n\right)
\text{ is equivalent}\\\text{to the condition }\left(  I=\left[  n\right]
\right)  \\\text{(by (\ref{pf.prop.sol.vander-hook.viete.b.equiv})))}}%
}\prod_{i\in I}x_{i}=\sum_{\substack{I\subseteq\left[  n\right]  ;\\I=\left[
n\right]  }}\prod_{i\in I}x_{i}.
\]
But $\left[  n\right]  $ is a subset of $\left[  n\right]  $. Hence, the sum
$\sum_{\substack{I\subseteq\left[  n\right]  ;\\I=\left[  n\right]  }%
}\prod_{i\in I}x_{i}$ has exactly one addend: namely, the addend for
$I=\left[  n\right]  $. Therefore, this sum simplifies as follows:%
\[
\sum_{\substack{I\subseteq\left[  n\right]  ;\\I=\left[  n\right]  }%
}\prod_{i\in I}x_{i}=\underbrace{\prod_{i\in\left[  n\right]  }}%
_{\substack{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }\\\text{(since
}\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  \text{)}}}x_{i}%
=\underbrace{\prod_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}%
}x_{i}=\prod_{i=1}^{n}x_{i}.
\]
Thus,%
\[
e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{I\subseteq
\left[  n\right]  ;\\I=\left[  n\right]  }}\prod_{i\in I}x_{i}=\prod_{i=1}%
^{n}x_{i}.
\]
This proves Proposition \ref{prop.sol.vander-hook.viete} \textbf{(b)}.
\end{verlong}

\textbf{(c)} We have $0\in\left\{  0,1,\ldots,n\right\}  $ (since
$n\in\mathbb{N}$). Proposition \ref{prop.sol.vander-hook.viete} \textbf{(a)}
shows that%
\begin{align*}
\prod_{i=1}^{n}\left(  x_{i}+t\right)   &  =\sum_{j=0}^{n}e_{n-j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  t^{j}\\
&  =\underbrace{e_{n-0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  }%
_{\substack{=e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\=\prod_{i=1}%
^{n}x_{i}\\\text{(by Proposition \ref{prop.sol.vander-hook.viete}
\textbf{(b)})}}}\underbrace{t^{0}}_{=1}+\underbrace{\sum_{j=1}^{n}%
e_{n-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  t^{j}}_{\substack{=\sum
_{j=0}^{n-1}e_{n-\left(  j+1\right)  }\left(  x_{1},x_{2},\ldots,x_{n}\right)
t^{j+1}\\\text{(here, we have substituted }j+1\\\text{for }j\text{ in the
sum)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=0\text{ from the sum}\\
\text{(since }0\in\left\{  0,1,\ldots,n\right\}  \text{)}%
\end{array}
\right) \\
&  =\prod_{i=1}^{n}x_{i}+\sum_{j=0}^{n-1}\underbrace{e_{n-\left(  j+1\right)
}\left(  x_{1},x_{2},\ldots,x_{n}\right)  }_{\substack{=e_{n-1-j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(since }n-\left(  j+1\right)
=n-1-j\text{)}}}\underbrace{t^{j+1}}_{=tt^{j}}\\
&  =\prod_{i=1}^{n}x_{i}+\sum_{j=0}^{n-1}e_{n-1-j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  tt^{j}.
\end{align*}
Solving this equality for $\sum_{j=0}^{n-1}e_{n-1-j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  tt^{j}$, we obtain%
\begin{equation}
\sum_{j=0}^{n-1}e_{n-1-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  tt^{j}%
=\prod_{i=1}^{n}\left(  x_{i}+t\right)  -\prod_{i=1}^{n}x_{i}.
\label{pf.prop.sol.vander-hook.viete.c.1}%
\end{equation}


But
\begin{align*}
&  t\cdot\underbrace{z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)
}_{\substack{=\sum_{j=0}^{n-1}e_{n-1-j}\left(  x_{1},x_{2},\ldots
,x_{n}\right)  t^{j}\\\text{(by the definition of }z_{t}\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  \text{)}}}\\
&  =t\cdot\sum_{j=0}^{n-1}e_{n-1-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)
t^{j}=\sum_{j=0}^{n-1}e_{n-1-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)
tt^{j}\\
&  =\prod_{i=1}^{n}\left(  x_{i}+t\right)  -\prod_{i=1}^{n}x_{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.sol.vander-hook.viete.c.1}%
)}\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.vander-hook.viete} \textbf{(c)}.

\textbf{(d)} Proposition \ref{prop.sol.vander-hook.viete} \textbf{(c)} yields%
\[
t\cdot z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\prod_{i=1}^{n}\left(
x_{i}+t\right)  -\prod_{i=1}^{n}x_{i}.
\]
We can divide both sides of this equality by $t$ (since $t$ is invertible). As
a result, we obtain%
\[
z_{t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\dfrac{\prod_{i=1}^{n}\left(
x_{i}+t\right)  -\prod_{i=1}^{n}x_{i}}{t}.
\]
This proves Proposition \ref{prop.sol.vander-hook.viete} \textbf{(d)}.
\end{proof}

Our next proposition is crucial in getting a grip on the elements $y_{k}$ in
Proposition \ref{prop.sol.vander-hook.variant-1}:

\begin{proposition}
\label{prop.sol.vander-hook.viete-y}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. For each $i\in\left\{
1,2,\ldots,n\right\}  $, set $y_{i}=\prod_{\substack{j\in\left\{
1,2,\ldots,n\right\}  ;\\j\neq i}}x_{j}$.

Let $k\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\[
y_{k}=\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}e_{j}\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  x_{k}^{n-1-j}.
\]

\end{proposition}

\begin{vershort}
Our proof of Proposition \ref{prop.sol.vander-hook.viete-y} will rely on a
basic fact about sets:

\begin{proposition}
\label{prop.sol.vander-hook.viete-y.short.Pm.lem}For every set $T$ and every
$j\in\mathbb{N}$, we let $\mathcal{P}_{j}\left(  T\right)  $ denote the set of
all $j$-element subsets of $T$.

Let $S$ be a set. Let $s\in S$. Let $m$ be a positive integer. Then:

\begin{itemize}
\item We have $\mathcal{P}_{m}\left(  S\setminus\left\{  s\right\}  \right)
\subseteq\mathcal{P}_{m}\left(  S\right)  $.

\item The map%
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}
\end{proposition}

Roughly speaking, Proposition \ref{prop.sol.vander-hook.viete-y.short.Pm.lem}
claims that if $s$ is an element of a set $S$, and if $m$ is a positive
integer, then:

\begin{itemize}
\item all $m$-element subsets of $S\setminus\left\{  s\right\}  $ are
$m$-element subsets of $S$ as well;

\item the $m$-element subsets of $S$ which are \textbf{not} $m$-element
subsets of $S\setminus\left\{  s\right\}  $ are in bijection with the $\left(
m-1\right)  $-element subsets of $S\setminus\left\{  s\right\}  $; this
bijection sends an $\left(  m-1\right)  $-element subset $U$ of $S\setminus
\left\{  s\right\}  $ to the $m$-element subset $U\cup\left\{  s\right\}  $ of
$S$.
\end{itemize}

Restated this way, Proposition \ref{prop.sol.vander-hook.viete-y.short.Pm.lem}
should be intuitively clear. A rigorous proof is not hard to
give\footnote{Here is an outline: First, show that the two maps%
\begin{align*}
\mathcal{P}_{m-1}\left(  S\setminus\left\{  s\right\}  \right)   &
\rightarrow\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\cup\left\{  s\right\}
\end{align*}
and%
\begin{align*}
\mathcal{P}_{m}\left(  S\right)  \setminus\mathcal{P}_{m}\left(
S\setminus\left\{  s\right\}  \right)   &  \rightarrow\mathcal{P}_{m-1}\left(
S\setminus\left\{  s\right\}  \right)  ,\\
U  &  \mapsto U\setminus\left\{  s\right\}
\end{align*}
are well-defined. Then, show that these maps are mutually inverse. Conclude
that the first of these two maps is invertible, i.e., is a bijection.}.

\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.viete-y}.]We shall use the
notations $\left[  n\right]  $, $\mathcal{P}\left(  \left[  n\right]  \right)
$ and $\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}$ as in Definition \ref{def.sol.vander-hook.elsyms}. We shall furthermore
use the notation $\mathcal{P}_{j}\left(  T\right)  $ from Proposition
\ref{prop.sol.vander-hook.viete-y.short.Pm.lem}.

Let $K=\left[  n\right]  \setminus\left\{  k\right\}  $. Note that
$k\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $, so that $\left\vert
\left[  n\right]  \setminus\left\{  k\right\}  \right\vert
=\underbrace{\left\vert \left[  n\right]  \right\vert }_{=n}-1=n-1$. From
$K=\left[  n\right]  \setminus\left\{  k\right\}  $, we obtain $\left\vert
K\right\vert =\left\vert \left[  n\right]  \setminus\left\{  k\right\}
\right\vert =n-1$.

For every $j\in\mathbb{N}$, define an element $f_{j}\in\mathbb{K}$ by%
\[
f_{j}=\sum_{\substack{I\subseteq K;\\\left\vert I\right\vert =j}}\prod_{i\in
I}x_{i}.
\]
Then,%
\begin{equation}
f_{0}=1 \label{pf.prop.sol.vander-hook.viete-y.short.f0=}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.short.f0=}):}
There exists exactly one subset $I$ of $K$ satisfying $\left\vert I\right\vert
=0$: namely, the subset $\varnothing$. Hence, the sum $\sum
_{\substack{I\subseteq K;\\\left\vert I\right\vert =0}}\prod_{i\in I}x_{i}$
has only one addend: namely, the addend for $I=\varnothing$. Thus, this sum
simplifies to $\sum_{\substack{I\subseteq K;\\\left\vert I\right\vert
=0}}\prod_{i\in I}x_{i}=\prod_{i\in\varnothing}x_{i}=\left(  \text{empty
product}\right)  =1$.
\par
Now, the definition of $f_{0}$ yields $f_{0}=\sum_{\substack{I\subseteq
K;\\\left\vert I\right\vert =0}}\prod_{i\in I}x_{i}=1$. This proves
(\ref{pf.prop.sol.vander-hook.viete-y.short.f0=}).}.

Next, we shall show that%
\begin{equation}
f_{n-1}=y_{k}. \label{pf.prop.sol.vander-hook.viete-y.short.fn-1=}%
\end{equation}


[\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.short.fn-1=}):} We
have $\left\vert K\right\vert =n-1$. Hence, there exists only one $\left(
n-1\right)  $-element subset $I$ of $K$: namely, the set $K$ itself. In other
words, there exists only one subset $I$ of $K$ satisfying $\left\vert
I\right\vert =n-1$: namely, the set $K$ itself. Hence, the sum $\sum
_{\substack{I\subseteq K;\\\left\vert I\right\vert =n-1}}\prod_{i\in I}x_{i}$
has only one addend: namely, the addend for $I=K$. Thus, this sum simplifies
to%
\begin{align*}
\sum_{\substack{I\subseteq K;\\\left\vert I\right\vert =n-1}}\prod_{i\in
I}x_{i}  &  =\prod_{i\in K}x_{i}=\prod_{j\in K}x_{j}=\underbrace{\prod
_{j\in\left\{  1,2,\ldots,n\right\}  \setminus\left\{  k\right\}  }}%
_{=\prod_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq k}}}x_{j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }K=\underbrace{\left[  n\right]
}_{=\left\{  1,2,\ldots,n\right\}  }\setminus\left\{  k\right\}  =\left\{
1,2,\ldots,n\right\}  \setminus\left\{  k\right\}  \right) \\
&  =\prod_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq k}}x_{j}.
\end{align*}
Now, the definition of $f_{n-1}$ yields%
\[
f_{n-1}=\sum_{\substack{I\subseteq K;\\\left\vert I\right\vert =n-1}%
}\prod_{i\in I}x_{i}=\prod_{\substack{j\in\left\{  1,2,\ldots,n\right\}
;\\j\neq k}}x_{j}.
\]
Comparing this with%
\[
y_{k}=\prod_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq k}%
}x_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }y_{k}\right)  ,
\]
we find $f_{n-1}=y_{k}$. Thus,
(\ref{pf.prop.sol.vander-hook.viete-y.short.fn-1=}) is proven.]

But%
\begin{equation}
e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =1
\label{pf.prop.sol.vander-hook.viete-y.short.e0=}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.short.e0=}):}
There exists exactly one subset $I$ of $\left[  n\right]  $ satisfying
$\left\vert I\right\vert =0$: namely, the subset $\varnothing$. Hence, the sum
$\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=0}}\prod_{i\in I}x_{i}$ has only one addend: namely, the addend for
$I=\varnothing$. Thus, this sum simplifies to $\sum_{\substack{I\subseteq
\left[  n\right]  ;\\\left\vert I\right\vert =0}}\prod_{i\in I}x_{i}%
=\prod_{i\in\varnothing}x_{i}=\left(  \text{empty product}\right)  =1$.
\par
Now, the definition of $e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ yields
$e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{I\subseteq
\left[  n\right]  ;\\\left\vert I\right\vert =0}}\prod_{i\in I}x_{i}=1$. This
proves (\ref{pf.prop.sol.vander-hook.viete-y.short.e0=}).}.

We shall now show that%
\begin{equation}
e_{m}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =f_{m}+x_{k}f_{m-1}
\label{pf.prop.sol.vander-hook.viete-y.short.main}%
\end{equation}
for every positive integer $m$.

[\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.short.main}):} Let $m$
be a positive integer. The definition of $f_{m}$ yields%
\begin{equation}
f_{m}=\underbrace{\sum_{\substack{I\subseteq K;\\\left\vert I\right\vert =m}%
}}_{\substack{=\sum_{\substack{I\text{ is an }m\text{-element}\\\text{subset
of }K}}=\sum_{I\in\mathcal{P}_{m}\left(  K\right)  }\\\text{(since the set of
all }m\text{-element}\\\text{subsets of }K\text{ is }\mathcal{P}_{m}\left(
K\right)  \text{)}}}\prod_{i\in I}x_{i}=\sum_{I\in\mathcal{P}_{m}\left(
K\right)  }\prod_{i\in I}x_{i}.
\label{pf.prop.sol.vander-hook.viete-y.short.main.pf.fm=}%
\end{equation}
The same argument (but applied to $m-1$ instead of $m$) yields%
\begin{equation}
f_{m-1}=\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in I}x_{i}.
\label{pf.prop.sol.vander-hook.viete-y.short.main.pf.fm-1=}%
\end{equation}


Recall that $k\in\left[  n\right]  $. Hence, we can apply Proposition
\ref{prop.sol.vander-hook.viete-y.short.Pm.lem} to $\left[  n\right]  $ and
$k$ instead of $S$ and $s$. As a result, we obtain the following two results:

\begin{itemize}
\item We have $\mathcal{P}_{m}\left(  \left[  n\right]  \setminus\left\{
k\right\}  \right)  \subseteq\mathcal{P}_{m}\left(  \left[  n\right]  \right)
$.

\item The map%
\begin{align*}
\mathcal{P}_{m-1}\left(  \left[  n\right]  \setminus\left\{  k\right\}
\right)   &  \rightarrow\mathcal{P}_{m}\left(  \left[  n\right]  \right)
\setminus\mathcal{P}_{m}\left(  \left[  n\right]  \setminus\left\{  k\right\}
\right)  ,\\
U  &  \mapsto U\cup\left\{  k\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}

Since $\left[  n\right]  \setminus\left\{  k\right\}  =K$, these two results
can be rewritten as follows:

\begin{itemize}
\item We have $\mathcal{P}_{m}\left(  K\right)  \subseteq\mathcal{P}%
_{m}\left(  \left[  n\right]  \right)  $.

\item The map%
\begin{align*}
\mathcal{P}_{m-1}\left(  K\right)   &  \rightarrow\mathcal{P}_{m}\left(
\left[  n\right]  \right)  \setminus\mathcal{P}_{m}\left(  K\right)  ,\\
U  &  \mapsto U\cup\left\{  k\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}

Furthermore, every $I\in\mathcal{P}_{m-1}\left(  K\right)  $ satisfies%
\begin{equation}
\prod_{i\in I\cup\left\{  k\right\}  }x_{i}=x_{k}\prod_{i\in I}x_{i}
\label{pf.prop.sol.vander-hook.viete-y.short.main.pf.2}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.prop.sol.vander-hook.viete-y.short.main.pf.2}):} Let $I\in
\mathcal{P}_{m-1}\left(  K\right)  $. Thus, $I$ is an $\left(  m-1\right)
$-element subset of $K$ (since $\mathcal{P}_{m-1}\left(  K\right)  $ is
defined as the set of all $\left(  m-1\right)  $-element subsets of $K$). In
particular, $I$ is a subset of $K$. Thus, $I\subseteq K=\left[  n\right]
\setminus\left\{  k\right\}  $. Thus, $k\notin I$. Hence, the sets $I$ and
$\left\{  k\right\}  $ are disjoint. Thus,%
\[
\prod_{i\in I\cup\left\{  k\right\}  }x_{i}=\left(  \prod_{i\in I}%
x_{i}\right)  \underbrace{\left(  \prod_{i\in\left\{  k\right\}  }%
x_{i}\right)  }_{=x_{k}}=\left(  \prod_{i\in I}x_{i}\right)  x_{k}=x_{k}%
\prod_{i\in I}x_{i}.
\]
This proves (\ref{pf.prop.sol.vander-hook.viete-y.short.main.pf.2}).}. Now,%
\begin{align}
&  \sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)  \setminus
\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}\nonumber\\
&  =\sum_{U\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in U\cup\left\{
k\right\}  }x_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }U\cup\left\{  k\right\}  \text{ for }I\text{
in the sum, since}\\
\text{the map }\mathcal{P}_{m-1}\left(  K\right)  \rightarrow\mathcal{P}%
_{m}\left(  \left[  n\right]  \right)  \setminus\mathcal{P}_{m}\left(
K\right)  ,\ U\mapsto U\cup\left\{  k\right\} \\
\text{is a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\underbrace{\prod_{i\in
I\cup\left\{  k\right\}  }x_{i}}_{\substack{=x_{k}\prod_{i\in I}%
x_{i}\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.short.main.pf.2}))}%
}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the}\\
\text{summation index }U\text{ as }I
\end{array}
\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }x_{k}\prod_{i\in I}%
x_{i}=x_{k}\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in I}x_{i}.
\label{pf.prop.sol.vander-hook.viete-y.short.main.pf.2b}%
\end{align}


But the definition of $e_{m}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ yields
\begin{align*}
e_{m}\left(  x_{1},x_{2},\ldots,x_{n}\right)   &  =\underbrace{\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert =m}%
}}_{\substack{=\sum_{\substack{I\text{ is an }m\text{-element}\\\text{subset
of }\left[  n\right]  }}=\sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]
\right)  }\\\text{(since the set of all }m\text{-element}\\\text{subsets of
}\left[  n\right]  \text{ is }\mathcal{P}_{m}\left(  \left[  n\right]
\right)  \text{)}}}\prod_{i\in I}x_{i}=\sum_{I\in\mathcal{P}_{m}\left(
\left[  n\right]  \right)  }\prod_{i\in I}x_{i}\\
&  =\underbrace{\sum_{\substack{I\in\mathcal{P}_{m}\left(  \left[  n\right]
\right)  ;\\I\in\mathcal{P}_{m}\left(  K\right)  }}}_{\substack{=\sum
_{I\in\mathcal{P}_{m}\left(  K\right)  }\\\text{(since }\mathcal{P}_{m}\left(
K\right)  \subseteq\mathcal{P}_{m}\left(  \left[  n\right]  \right)  \text{)}%
}}\prod_{i\in I}x_{i}+\underbrace{\sum_{\substack{I\in\mathcal{P}_{m}\left(
\left[  n\right]  \right)  ;\\I\notin\mathcal{P}_{m}\left(  K\right)  }%
}}_{=\sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)
\setminus\mathcal{P}_{m}\left(  K\right)  }}\prod_{i\in I}x_{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)  \text{
satisfies either }I\in\mathcal{P}_{m}\left(  K\right) \\
\text{or }I\notin\mathcal{P}_{m}\left(  K\right)  \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{I\in\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}%
+\underbrace{\sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)
\setminus\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}}%
_{\substack{=x_{k}\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in
I}x_{i}\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.short.main.pf.2b}))}%
}}\\
&  =\underbrace{\sum_{I\in\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in
I}x_{i}}_{\substack{=f_{m}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.short.main.pf.fm=}))}}}+x_{k}%
\underbrace{\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in I}x_{i}%
}_{\substack{=f_{m-1}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.short.main.pf.fm-1=}))}}}\\
&  =f_{m}+x_{k}f_{m-1}.
\end{align*}
This proves (\ref{pf.prop.sol.vander-hook.viete-y.short.main}).]

Recall that $k\in\left\{  1,2,\ldots,n\right\}  $. Hence, $1\leq k\leq n$, so
that $n\geq1$, so that $n-1\in\mathbb{N}$. Hence, $0\in\left\{  0,1,\ldots
,n-1\right\}  $ and $n-1\in\left\{  0,1,\ldots,n-1\right\}  $.

Now,%
\begin{align}
&  \sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}e_{j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  x_{k}^{n-1-j}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{n-1-0}}_{=\left(  -1\right)  ^{n-1}%
}\underbrace{e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  }%
_{\substack{=1\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.short.e0=}))}%
}}\underbrace{x_{k}^{n-1-0}}_{=x_{k}^{n-1}}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}\underbrace{e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)
}_{\substack{=f_{j}+x_{k}f_{j-1}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.short.main}) (applied to }m=j\text{))}%
}}x_{k}^{n-1-j}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=0\text{ from the sum,}\\
\text{since }0\in\left\{  0,1,\ldots,n-1\right\}
\end{array}
\right) \nonumber\\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\underbrace{\sum_{j=1}^{n-1}\left(
-1\right)  ^{n-1-j}\left(  f_{j}+x_{k}f_{j-1}\right)  x_{k}^{n-1-j}}%
_{=\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}+\sum
_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}x_{k}f_{j-1}x_{k}^{n-1-j}}\nonumber\\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j}+\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}%
x_{k}f_{j-1}x_{k}^{n-1-j}. \label{pf.prop.sol.vander-hook.viete-y.short.5}%
\end{align}


But
\begin{align}
&  \sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}\underbrace{x_{k}f_{j-1}%
}_{=f_{j-1}x_{k}}x_{k}^{n-1-j}\nonumber\\
&  =\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j-1}\underbrace{x_{k}%
x_{k}^{n-1-j}}_{=x_{k}^{\left(  n-1-j\right)  +1}=x_{k}^{n-j}}=\sum
_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j-1}x_{k}^{n-j}\nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\underbrace{\left(  -1\right)
^{n-1-\left(  j+1\right)  }}_{\substack{=\left(  -1\right)  ^{n-j}%
\\\text{(since }n-1-\left(  j+1\right)  =n-j-2\equiv n-j\operatorname{mod}%
2\text{)}}}\underbrace{f_{\left(  j+1\right)  -1}}_{=f_{j}}\underbrace{x_{k}%
^{n-\left(  j+1\right)  }}_{\substack{=x_{k}^{n-1-j}\\\text{(since }n-\left(
j+1\right)  =n-1-j\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j+1\text{ for
}j\text{ in the sum}\right) \nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\underbrace{\left(  -1\right)  ^{n-j}%
}_{\substack{=-\left(  -1\right)  ^{n-j-1}=-\left(  -1\right)  ^{n-1-j}%
\\\text{(since }n-j-1=n-1-j\text{)}}}f_{j}x_{k}^{n-1-j}=\sum_{j=0}^{\left(
n-1\right)  -1}\left(  -\left(  -1\right)  ^{n-1-j}\right)  f_{j}x_{k}%
^{n-1-j}\nonumber\\
&  =-\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}. \label{pf.prop.sol.vander-hook.viete-y.short.6a}%
\end{align}
Also,%
\begin{align*}
&  \sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}\\
&  =\underbrace{\left(  -1\right)  ^{n-1-0}}_{=\left(  -1\right)  ^{n-1}%
}\underbrace{f_{0}}_{\substack{=1\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.short.f0=}))}}}\underbrace{x_{k}%
^{n-1-0}}_{=x_{k}^{n-1}}+\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=0\text{ from the sum,}\\
\text{since }0\in\left\{  0,1,\ldots,n-1\right\}
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j},
\end{align*}
so that%
\begin{align}
&  \left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j}\nonumber\\
&  =\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}\nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+\underbrace{\left(  -1\right)  ^{n-1-\left(  n-1\right)  }%
}_{=\left(  -1\right)  ^{0}=1}\underbrace{f_{n-1}}_{\substack{=y_{k}%
\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.short.fn-1=}))}%
}}\underbrace{x_{k}^{n-1-\left(  n-1\right)  }}_{=x_{k}^{0}=1}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=n-1\text{ from the sum,}\\
\text{since }n-1\in\left\{  0,1,\ldots,n-1\right\}
\end{array}
\right) \nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+y_{k}. \label{pf.prop.sol.vander-hook.viete-y.short.6b}%
\end{align}


Now, (\ref{pf.prop.sol.vander-hook.viete-y.short.5}) becomes%
\begin{align*}
&  \sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}e_{j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  x_{k}^{n-1-j}\\
&  =\underbrace{\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(
-1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}}_{\substack{=\sum_{j=0}^{\left(
n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}+y_{k}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.short.6b}))}}}+\underbrace{\sum
_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}x_{k}f_{j-1}x_{k}^{n-1-j}%
}_{\substack{=-\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.short.6a}))}}}\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+y_{k}+\left(  -\sum_{j=0}^{\left(  n-1\right)  -1}\left(
-1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}\right) \\
&  =y_{k}.
\end{align*}
This proves Proposition \ref{prop.sol.vander-hook.viete-y}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.viete-y}.]We shall use the
notations $\left[  n\right]  $, $\mathcal{P}\left(  \left[  n\right]  \right)
$ and $\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}$ as in Definition \ref{def.sol.vander-hook.elsyms}. Let $K=\left[
n\right]  \setminus\left\{  k\right\}  $. Note that $k\in\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  $, so that $\left\vert \left[
n\right]  \setminus\left\{  k\right\}  \right\vert =\underbrace{\left\vert
\left[  n\right]  \right\vert }_{=n}-1=n-1$. From $K=\left[  n\right]
\setminus\left\{  k\right\}  $, we obtain $\left\vert K\right\vert =\left\vert
\left[  n\right]  \setminus\left\{  k\right\}  \right\vert =n-1$. Thus, $K$ is
a finite set.

We shall use the notation introduced in Definition
\ref{def.sol.prop.binom.subsets.Pm}.

For every $j\in\mathbb{N}$, define an element $f_{j}\in\mathbb{K}$ by%
\[
f_{j}=\sum_{I\in\mathcal{P}_{j}\left(  K\right)  }\prod_{i\in I}x_{i}.
\]
Then,%
\begin{equation}
f_{0}=1 \label{pf.prop.sol.vander-hook.viete-y.f0=}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.f0=}):}
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(a)} (applied to
$S=K$) yields $\mathcal{P}_{0}\left(  K\right)  =\left\{  \varnothing\right\}
$. Now, the definition of $f_{0}$ yields%
\begin{align*}
f_{0}  &  =\sum_{I\in\mathcal{P}_{0}\left(  K\right)  }\prod_{i\in I}%
x_{i}=\sum_{I\in\left\{  \varnothing\right\}  }\prod_{i\in I}x_{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathcal{P}_{0}\left(  K\right)
=\left\{  \varnothing\right\}  \right) \\
&  =\prod_{i\in\varnothing}x_{i}=\left(  \text{empty product}\right)  =1.
\end{align*}
This proves (\ref{pf.prop.sol.vander-hook.viete-y.f0=}).}.

Next, we shall show that%
\begin{equation}
f_{n-1}=y_{k}. \label{pf.prop.sol.vander-hook.viete-y.fn-1=}%
\end{equation}


[\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.fn-1=}):} We have
$\left\{  K\right\}  \subseteq\mathcal{P}_{n-1}\left(  K\right)
$\ \ \ \ \footnote{\textit{Proof.} We know that $\mathcal{P}_{n-1}\left(
K\right)  $ is the set of all $\left(  n-1\right)  $-element subsets of $K$
(by the definition of $\mathcal{P}_{n-1}\left(  K\right)  $).
\par
But $K$ is an $\left(  n-1\right)  $-element set (since $\left\vert
K\right\vert =n-1$) and a subset of $K$ (obviously). Hence, $K$ is an $\left(
n-1\right)  $-element subset of $K$. In other words, $K\in\mathcal{P}%
_{n-1}\left(  K\right)  $ (since $\mathcal{P}_{n-1}\left(  K\right)  $ is the
set of all $\left(  n-1\right)  $-element subsets of $K$). Thus, $\left\{
K\right\}  \subseteq\mathcal{P}_{n-1}\left(  K\right)  $. Qed.} and
$\mathcal{P}_{n-1}\left(  K\right)  \subseteq\left\{  K\right\}
$\ \ \ \ \footnote{\textit{Proof.} We know that $\mathcal{P}_{n-1}\left(
K\right)  $ is the set of all $\left(  n-1\right)  $-element subsets of $K$
(by the definition of $\mathcal{P}_{n-1}\left(  K\right)  $).
\par
Let $I\in\mathcal{P}_{n-1}\left(  K\right)  $. Thus, $I$ is an $\left(
n-1\right)  $-element subset of $K$ (since $\mathcal{P}_{n-1}\left(  K\right)
$ is the set of all $\left(  n-1\right)  $-element subsets of $K$). In other
words, $I$ is a subset of $K$ and satisfies $\left\vert I\right\vert =n-1$.
\par
We have $I\subseteq K$ (since $I$ is a subset of $K$) and thus $\left\vert
K\right\vert =\left\vert I\right\vert +\left\vert K\setminus I\right\vert $
(since $K$ is a finite set). Compared with $\left\vert K\right\vert =n-1$,
this yields $n-1=\underbrace{\left\vert I\right\vert }_{=n-1}+\left\vert
K\setminus I\right\vert =n-1+\left\vert K\setminus I\right\vert $. Subtracting
$n-1$ from both sides of this equality, we find $0=\left\vert K\setminus
I\right\vert $. Thus, $\left\vert K\setminus I\right\vert =0$, so that
$K\setminus I=\varnothing$. In other words, $K\subseteq I$. Combined with
$I\subseteq K$, this yields $I=K$. Thus, $I\in\left\{  K\right\}  $.
\par
Now, forget that we fixed $I$. We thus have shown that every $I\in
\mathcal{P}_{n-1}\left(  K\right)  $ satisfies $I\in\left\{  K\right\}  $. In
other words, $\mathcal{P}_{n-1}\left(  K\right)  \subseteq\left\{  K\right\}
$. Qed.}. Combining these two relations, we obtain $\mathcal{P}_{n-1}\left(
K\right)  =\left\{  K\right\}  $. Now, the definition of $f_{n-1}$ yields%
\begin{align*}
f_{n-1}  &  =\sum_{I\in\mathcal{P}_{n-1}\left(  K\right)  }\prod_{i\in I}%
x_{i}=\sum_{I\in\left\{  K\right\}  }\prod_{i\in I}x_{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathcal{P}_{n-1}\left(  K\right)
=\left\{  K\right\}  \right) \\
&  =\underbrace{\prod_{i\in K}}_{\substack{=\prod_{i\in\left[  n\right]
\setminus\left\{  k\right\}  }\\\text{(since }K=\left[  n\right]
\setminus\left\{  k\right\}  \text{)}}}x_{i}=\underbrace{\prod_{i\in\left[
n\right]  \setminus\left\{  k\right\}  }}_{=\prod_{\substack{i\in\left[
n\right]  ;\\i\neq k}}}x_{i}=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
k}}x_{i}\\
&  =\prod_{\substack{j\in\left[  n\right]  ;\\j\neq k}}x_{j}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the index }i\text{ as }j\\
\text{in the product}%
\end{array}
\right) \\
&  =\prod_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq k}%
}x_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \right)  .
\end{align*}
Comparing this with%
\[
y_{k}=\prod_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq k}%
}x_{j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }y_{k}\right)  ,
\]
we find $f_{n-1}=y_{k}$. Thus, (\ref{pf.prop.sol.vander-hook.viete-y.fn-1=})
is proven.]

For every $j\in\mathbb{N}$, we have%
\begin{equation}
e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{I\in\mathcal{P}%
_{j}\left(  \left[  n\right]  \right)  }\prod_{i\in I}x_{i}
\label{pf.prop.sol.vander-hook.viete-y.ej=}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.ej=}):} Let
$j\in\mathbb{N}$. Recall that $\mathcal{P}_{j}\left(  \left[  n\right]
\right)  $ is the set of all $j$-element subsets of $\left[  n\right]  $ (by
the definition of $\mathcal{P}_{j}\left(  \left[  n\right]  \right)  $). In
other words, $\mathcal{P}_{j}\left(  \left[  n\right]  \right)  $ is the set
of all subsets $I$ of $\left[  n\right]  $ satisfying $\left\vert I\right\vert
=j$. Hence, $\sum_{I\in\mathcal{P}_{j}\left(  \left[  n\right]  \right)
}=\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert =j}}$
(an equality of summation signs). Thus,%
\[
\underbrace{\sum_{I\in\mathcal{P}_{j}\left(  \left[  n\right]  \right)  }%
}_{=\sum_{\substack{I\subseteq\left[  n\right]  ;\\\left\vert I\right\vert
=j}}}\prod_{i\in I}x_{i}=\sum_{\substack{I\subseteq\left[  n\right]
;\\\left\vert I\right\vert =j}}\prod_{i\in I}x_{i}.
\]
Comparing this with
\[
e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{\substack{I\subseteq
\left[  n\right]  ;\\\left\vert I\right\vert =j}}\prod_{i\in I}x_{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }e_{j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  \right)  ,
\]
we obtain $e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =\sum_{I\in
\mathcal{P}_{j}\left(  \left[  n\right]  \right)  }\prod_{i\in I}x_{i}$. This
proves (\ref{pf.prop.sol.vander-hook.viete-y.ej=}).}. Also,%
\begin{equation}
e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =1
\label{pf.prop.sol.vander-hook.viete-y.e0=}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.e0=}):}
Proposition \ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(a)} (applied to
$S=\left[  n\right]  $) yields $\mathcal{P}_{0}\left(  \left[  n\right]
\right)  =\left\{  \varnothing\right\}  $. Now,
(\ref{pf.prop.sol.vander-hook.viete-y.ej=}) (applied to $j=0$) yields%
\begin{align*}
e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)   &  =\sum_{I\in\mathcal{P}%
_{0}\left(  \left[  n\right]  \right)  }\prod_{i\in I}x_{i}=\sum_{I\in\left\{
\varnothing\right\}  }\prod_{i\in I}x_{i}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathcal{P}_{0}\left(  \left[  n\right]  \right)  =\left\{
\varnothing\right\}  \right) \\
&  =\prod_{i\in\varnothing}x_{i}=\left(  \text{empty product}\right)  =1.
\end{align*}
This proves (\ref{pf.prop.sol.vander-hook.viete-y.e0=}).}.

We shall now show that%
\begin{equation}
e_{m}\left(  x_{1},x_{2},\ldots,x_{n}\right)  =f_{m}+x_{k}f_{m-1}
\label{pf.prop.sol.vander-hook.viete-y.main}%
\end{equation}
for every positive integer $m$.

[\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.main}):} Let $m$ be a
positive integer. Thus, $m\in\mathbb{N}$ and $m-1\in\mathbb{N}$. The
definition of $f_{m}$ yields%
\begin{equation}
f_{m}=\sum_{I\in\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}.
\label{pf.prop.sol.vander-hook.viete-y.main.pf.fm=}%
\end{equation}
The definition of $f_{m-1}$ yields%
\begin{equation}
f_{m-1}=\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in I}x_{i}.
\label{pf.prop.sol.vander-hook.viete-y.main.pf.fm-1=}%
\end{equation}


Recall that $k\in\left[  n\right]  $. Hence, we can apply Proposition
\ref{prop.sol.prop.binom.subsets.Pm.lem} \textbf{(c)} to $\left[  n\right]  $
and $k$ instead of $S$ and $s$. As a result, we obtain the following two results:

\begin{itemize}
\item We have $\mathcal{P}_{m}\left(  \left[  n\right]  \setminus\left\{
k\right\}  \right)  \subseteq\mathcal{P}_{m}\left(  \left[  n\right]  \right)
$.

\item The map%
\begin{align*}
\mathcal{P}_{m-1}\left(  \left[  n\right]  \setminus\left\{  k\right\}
\right)   &  \rightarrow\mathcal{P}_{m}\left(  \left[  n\right]  \right)
\setminus\mathcal{P}_{m}\left(  \left[  n\right]  \setminus\left\{  k\right\}
\right)  ,\\
U  &  \mapsto U\cup\left\{  k\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}

Since $\left[  n\right]  \setminus\left\{  k\right\}  =K$, these two results
can be rewritten as follows:

\begin{itemize}
\item We have $\mathcal{P}_{m}\left(  K\right)  \subseteq\mathcal{P}%
_{m}\left(  \left[  n\right]  \right)  $.

\item The map%
\begin{align*}
\mathcal{P}_{m-1}\left(  K\right)   &  \rightarrow\mathcal{P}_{m}\left(
\left[  n\right]  \right)  \setminus\mathcal{P}_{m}\left(  K\right)  ,\\
U  &  \mapsto U\cup\left\{  k\right\}
\end{align*}
is well-defined and a bijection.
\end{itemize}

Furthermore, every $I\in\mathcal{P}_{m-1}\left(  K\right)  $ satisfies%
\begin{equation}
\prod_{i\in I\cup\left\{  k\right\}  }x_{i}=x_{k}\prod_{i\in I}x_{i}
\label{pf.prop.sol.vander-hook.viete-y.main.pf.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.vander-hook.viete-y.main.pf.2}):}
Let $I\in\mathcal{P}_{m-1}\left(  K\right)  $. Thus, $I$ is an $\left(
m-1\right)  $-element subset of $K$ (since $\mathcal{P}_{m-1}\left(  K\right)
$ is the set of all $\left(  m-1\right)  $-element subsets of $K$ (by the
definition of $\mathcal{P}_{m-1}\left(  K\right)  $)). In particular, $I$ is a
subset of $K$. Thus, $I\subseteq K=\left[  n\right]  \setminus\left\{
k\right\}  $.
\par
We have $k\in\left\{  k\right\}  $ and thus $k\notin\left[  n\right]
\setminus\left\{  k\right\}  $. If we had $k\in I$, then we would have $k\in
I\subseteq\left[  n\right]  \setminus\left\{  k\right\}  $, which would
contradict $k\notin\left[  n\right]  \setminus\left\{  k\right\}  $. Hence, we
cannot have $k\in I$. Thus, we have $k\notin I$. Hence, the sets $I$ and
$\left\{  k\right\}  $ are disjoint. Thus,%
\[
\prod_{i\in I\cup\left\{  k\right\}  }x_{i}=\left(  \prod_{i\in I}%
x_{i}\right)  \underbrace{\left(  \prod_{i\in\left\{  k\right\}  }%
x_{i}\right)  }_{=x_{k}}=\left(  \prod_{i\in I}x_{i}\right)  x_{k}=x_{k}%
\prod_{i\in I}x_{i}.
\]
This proves (\ref{pf.prop.sol.vander-hook.viete-y.main.pf.2}).}. Now,%
\begin{align}
&  \sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)  \setminus
\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}\nonumber\\
&  =\sum_{U\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in U\cup\left\{
k\right\}  }x_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }U\cup\left\{  k\right\}  \text{ for }I\text{
in the sum, since}\\
\text{the map }\mathcal{P}_{m-1}\left(  K\right)  \rightarrow\mathcal{P}%
_{m}\left(  \left[  n\right]  \right)  \setminus\mathcal{P}_{m}\left(
K\right)  ,\ U\mapsto U\cup\left\{  k\right\} \\
\text{is a bijection}%
\end{array}
\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\underbrace{\prod_{i\in
I\cup\left\{  k\right\}  }x_{i}}_{\substack{=x_{k}\prod_{i\in I}%
x_{i}\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.main.pf.2}))}%
}}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the}\\
\text{summation index }U\text{ as }I
\end{array}
\right) \nonumber\\
&  =\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }x_{k}\prod_{i\in I}%
x_{i}=x_{k}\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in I}x_{i}.
\label{pf.prop.sol.vander-hook.viete-y.main.pf.2b}%
\end{align}


From (\ref{pf.prop.sol.vander-hook.viete-y.ej=}) (applied to $j=m$), we obtain%
\begin{align*}
e_{m}\left(  x_{1},x_{2},\ldots,x_{n}\right)   &  =\sum_{I\in\mathcal{P}%
_{m}\left(  \left[  n\right]  \right)  }\prod_{i\in I}x_{i}=\underbrace{\sum
_{\substack{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)
;\\I\in\mathcal{P}_{m}\left(  K\right)  }}}_{\substack{=\sum_{I\in
\mathcal{P}_{m}\left(  K\right)  }\\\text{(since }\mathcal{P}_{m}\left(
K\right)  \subseteq\mathcal{P}_{m}\left(  \left[  n\right]  \right)  \text{)}%
}}\prod_{i\in I}x_{i}+\underbrace{\sum_{\substack{I\in\mathcal{P}_{m}\left(
\left[  n\right]  \right)  ;\\I\notin\mathcal{P}_{m}\left(  K\right)  }%
}}_{=\sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)
\setminus\mathcal{P}_{m}\left(  K\right)  }}\prod_{i\in I}x_{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)  \text{
satisfies either }I\in\mathcal{P}_{m}\left(  K\right) \\
\text{or }I\notin\mathcal{P}_{m}\left(  K\right)  \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{I\in\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}%
+\underbrace{\sum_{I\in\mathcal{P}_{m}\left(  \left[  n\right]  \right)
\setminus\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in I}x_{i}}%
_{\substack{=x_{k}\sum_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in
I}x_{i}\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.main.pf.2b}))}}}\\
&  =\underbrace{\sum_{I\in\mathcal{P}_{m}\left(  K\right)  }\prod_{i\in
I}x_{i}}_{\substack{=f_{m}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.main.pf.fm=}))}}}+x_{k}\underbrace{\sum
_{I\in\mathcal{P}_{m-1}\left(  K\right)  }\prod_{i\in I}x_{i}}%
_{\substack{=f_{m-1}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.main.pf.fm-1=}))}}}\\
&  =f_{m}+x_{k}f_{m-1}.
\end{align*}
This proves (\ref{pf.prop.sol.vander-hook.viete-y.main}).]

Recall that $k\in\left\{  1,2,\ldots,n\right\}  $. Hence, $1\leq k\leq n$, so
that $1\leq n$. Thus, $n\geq1$, so that $n-1\in\mathbb{N}$. Hence,
$0\in\left\{  0,1,\ldots,n-1\right\}  $ and $n-1\in\left\{  0,1,\ldots
,n-1\right\}  $.

Now,%
\begin{align}
&  \sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}e_{j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  x_{k}^{n-1-j}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{n-1-0}}_{=\left(  -1\right)  ^{n-1}%
}\underbrace{e_{0}\left(  x_{1},x_{2},\ldots,x_{n}\right)  }%
_{\substack{=1\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.e0=}))}%
}}\underbrace{x_{k}^{n-1-0}}_{=x_{k}^{n-1}}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}\underbrace{e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)
}_{\substack{=f_{j}+x_{k}f_{j-1}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.main}) (applied to }m=j\text{))}}%
}x_{k}^{n-1-j}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=0\text{ from the sum,}\\
\text{since }0\in\left\{  0,1,\ldots,n-1\right\}
\end{array}
\right) \nonumber\\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\underbrace{\left(
-1\right)  ^{n-1-j}\left(  f_{j}+x_{k}f_{j-1}\right)  x_{k}^{n-1-j}}_{=\left(
-1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}+\left(  -1\right)  ^{n-1-j}x_{k}%
f_{j-1}x_{k}^{n-1-j}}\nonumber\\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\underbrace{\sum_{j=1}^{n-1}\left(
\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}+\left(  -1\right)  ^{n-1-j}%
x_{k}f_{j-1}x_{k}^{n-1-j}\right)  }_{=\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j}+\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}%
x_{k}f_{j-1}x_{k}^{n-1-j}}\nonumber\\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j}+\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}%
x_{k}f_{j-1}x_{k}^{n-1-j}. \label{pf.prop.sol.vander-hook.viete-y.5}%
\end{align}


But
\begin{align}
&  \sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}\underbrace{x_{k}f_{j-1}%
}_{=f_{j-1}x_{k}}x_{k}^{n-1-j}\nonumber\\
&  =\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j-1}\underbrace{x_{k}%
x_{k}^{n-1-j}}_{\substack{=x_{k}^{\left(  n-1-j\right)  +1}=x_{k}%
^{n-j}\\\text{(since }\left(  n-1-j\right)  +1=n-j\text{)}}}=\sum_{j=1}%
^{n-1}\left(  -1\right)  ^{n-1-j}f_{j-1}x_{k}^{n-j}\nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\underbrace{\left(  -1\right)
^{n-1-\left(  j+1\right)  }}_{\substack{=\left(  -1\right)  ^{n-j}%
\\\text{(since }n-1-\left(  j+1\right)  =n-j-2\equiv n-j\operatorname{mod}%
2\text{)}}}\underbrace{f_{\left(  j+1\right)  -1}}_{\substack{=f_{j}%
\\\text{(since }\left(  j+1\right)  -1=j\text{)}}}\underbrace{x_{k}^{n-\left(
j+1\right)  }}_{\substack{=x_{k}^{n-1-j}\\\text{(since }n-\left(  j+1\right)
=n-1-j\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j+1\text{ for
}j\text{ in the sum}\right) \nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\underbrace{\left(  -1\right)  ^{n-j}%
}_{\substack{=\left(  -1\right)  \left(  -1\right)  ^{n-j-1}\\=-\left(
-1\right)  ^{n-j-1}=-\left(  -1\right)  ^{n-1-j}\\\text{(since }%
n-j-1=n-1-j\text{)}}}f_{j}x_{k}^{n-1-j}=\sum_{j=0}^{\left(  n-1\right)
-1}\left(  -\left(  -1\right)  ^{n-1-j}\right)  f_{j}x_{k}^{n-1-j}\nonumber\\
&  =-\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}. \label{pf.prop.sol.vander-hook.viete-y.6a}%
\end{align}
Also,%
\begin{align*}
&  \sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}\\
&  =\underbrace{\left(  -1\right)  ^{n-1-0}}_{=\left(  -1\right)  ^{n-1}%
}\underbrace{f_{0}}_{\substack{=1\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.f0=}))}}}\underbrace{x_{k}^{n-1-0}%
}_{=x_{k}^{n-1}}+\sum_{j=1}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}%
^{n-1-j}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=0\text{ from the sum,}\\
\text{since }0\in\left\{  0,1,\ldots,n-1\right\}
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j},
\end{align*}
so that%
\begin{align}
&  \left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(  -1\right)
^{n-1-j}f_{j}x_{k}^{n-1-j}\nonumber\\
&  =\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}\nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+\underbrace{\left(  -1\right)  ^{n-1-\left(  n-1\right)  }%
}_{\substack{=\left(  -1\right)  ^{0}\\\text{(since }n-1-\left(  n-1\right)
=0\text{)}}}\underbrace{f_{n-1}}_{\substack{=y_{k}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.fn-1=}))}}}\underbrace{x_{k}%
^{n-1-\left(  n-1\right)  }}_{\substack{=x_{k}^{0}\\\text{(since }n-1-\left(
n-1\right)  =0\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=n-1\text{ from the sum,}\\
\text{since }n-1\in\left\{  0,1,\ldots,n-1\right\}
\end{array}
\right) \nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+\underbrace{\left(  -1\right)  ^{0}}_{=1}y_{k}\underbrace{x_{k}%
^{0}}_{=1}\nonumber\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+y_{k}. \label{pf.prop.sol.vander-hook.viete-y.6b}%
\end{align}


Now, (\ref{pf.prop.sol.vander-hook.viete-y.5}) becomes%
\begin{align*}
&  \sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}e_{j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  x_{k}^{n-1-j}\\
&  =\underbrace{\left(  -1\right)  ^{n-1}x_{k}^{n-1}+\sum_{j=1}^{n-1}\left(
-1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}}_{\substack{=\sum_{j=0}^{\left(
n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}+y_{k}\\\text{(by
(\ref{pf.prop.sol.vander-hook.viete-y.6b}))}}}+\underbrace{\sum_{j=1}%
^{n-1}\left(  -1\right)  ^{n-1-j}x_{k}f_{j-1}x_{k}^{n-1-j}}_{\substack{=-\sum
_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}x_{k}%
^{n-1-j}\\\text{(by (\ref{pf.prop.sol.vander-hook.viete-y.6a}))}}}\\
&  =\sum_{j=0}^{\left(  n-1\right)  -1}\left(  -1\right)  ^{n-1-j}f_{j}%
x_{k}^{n-1-j}+y_{k}+\left(  -\sum_{j=0}^{\left(  n-1\right)  -1}\left(
-1\right)  ^{n-1-j}f_{j}x_{k}^{n-1-j}\right) \\
&  =y_{k}.
\end{align*}
This proves Proposition \ref{prop.sol.vander-hook.viete-y}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.vander-hook.y-sum.1}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. For each $i\in\left\{
1,2,\ldots,n\right\}  $, set $y_{i}=\prod_{\substack{j\in\left\{
1,2,\ldots,n\right\}  ;\\j\neq i}}x_{j}$.

Let $A$ be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Let $q\in\left\{  1,2,\ldots,n\right\}  $. For every
$j\in\mathbb{N}$, define an element $\mathfrak{e}_{j}\in\mathbb{K}$ by
$\mathfrak{e}_{j}=e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $. Then,%
\[
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}\det\left(  A_{\sim k,\sim
q}\right)  =\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.y-sum.1}.]Let $j\in\left\{
0,1,\ldots,n-1\right\}  $. We have $A=\left(  x_{i}^{n-j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}$ (by the definition of $A$) and $j+1\in\left\{
1,2,\ldots,n\right\}  $ (since $j\in\left\{  0,1,\ldots,n-1\right\}  $).
Hence, Lemma \ref{lem.sol.vander-hook.lap0} (applied to $x_{i}^{n-j}$ and
$j+1$ instead of $a_{i,j}$ and $r$) yields%
\[
\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-\left(  j+1\right)  }%
\det\left(  A_{\sim p,\sim q}\right)  =\delta_{q,j+1}\det A.
\]
Hence,%
\begin{align}
\delta_{q,j+1}\det A  &  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}%
\underbrace{x_{p}^{n-\left(  j+1\right)  }}_{\substack{=x_{p}^{n-1-j}%
\\\text{(since }n-\left(  j+1\right)  =n-1-j\text{)}}}\det\left(  A_{\sim
p,\sim q}\right) \nonumber\\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}x_{p}^{n-1-j}\det\left(  A_{\sim
p,\sim q}\right) \nonumber\\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-1-j}\det\left(  A_{\sim
k,\sim q}\right)  \label{pf.lem.sol.vander-hook.y-sum.1.1}%
\end{align}
(here, we have renamed the summation index $p$ as $k$).

Now, forget that we fixed $j$. We thus have proven
(\ref{pf.lem.sol.vander-hook.y-sum.1.1}) for every $j\in\left\{
0,1,\ldots,n-1\right\}  $.

Every $k\in\mathbb{N}$ satisfies%
\begin{align*}
y_{k}  &  =\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}\underbrace{e_{j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  }_{\substack{=\mathfrak{e}_{j}\\\text{(since
}\mathfrak{e}_{j}=e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
the definition of }\mathfrak{e}_{j}\text{))}}}x_{k}^{n-1-j}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition
\ref{prop.sol.vander-hook.viete-y}}\right) \\
&  =\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}\mathfrak{e}_{j}x_{k}^{n-1-j}.
\end{align*}
Thus,%
\begin{align*}
&  \sum_{k=1}^{n}\left(  -1\right)  ^{k+q}\underbrace{y_{k}}_{=\sum
_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}\mathfrak{e}_{j}x_{k}^{n-1-j}}%
\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}\left(  \sum_{j=0}^{n-1}\left(
-1\right)  ^{n-1-j}\mathfrak{e}_{j}x_{k}^{n-1-j}\right)  \det\left(  A_{\sim
k,\sim q}\right) \\
&  =\underbrace{\sum_{k=1}^{n}\sum_{j=0}^{n-1}}_{=\sum_{j=0}^{n-1}\sum
_{k=1}^{n}}\left(  -1\right)  ^{k+q}\left(  -1\right)  ^{n-1-j}\mathfrak{e}%
_{j}x_{k}^{n-1-j}\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{j=0}^{n-1}\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}\left(
-1\right)  ^{n-1-j}\mathfrak{e}_{j}x_{k}^{n-1-j}\det\left(  A_{\sim k,\sim
q}\right)  }_{=\left(  -1\right)  ^{n-1-j}\mathfrak{e}_{j}\sum_{k=1}%
^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-1-j}\det\left(  A_{\sim k,\sim
q}\right)  }\\
&  =\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}\mathfrak{e}_{j}%
\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-1-j}\det\left(
A_{\sim k,\sim q}\right)  }_{\substack{=\delta_{q,j+1}\det A\\\text{(by
(\ref{pf.lem.sol.vander-hook.y-sum.1.1}))}}}\\
&  =\sum_{j=0}^{n-1}\left(  -1\right)  ^{n-1-j}\mathfrak{e}_{j}\delta
_{q,j+1}\det A=\underbrace{\sum_{j=1}^{n}}_{=\sum_{j\in\left\{  1,2,\ldots
,n\right\}  }}\underbrace{\left(  -1\right)  ^{n-1-\left(  j-1\right)  }%
}_{\substack{=\left(  -1\right)  ^{n-j}\\\text{(since }n-1-\left(  j-1\right)
=n-j\text{)}}}\mathfrak{e}_{j-1}\underbrace{\delta_{q,\left(  j-1\right)  +1}%
}_{\substack{=\delta_{q,j}\\\text{(since }\left(  j-1\right)  +1=j\text{)}%
}}\det A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }j-1\text{ for
}j\text{ in the sum}\right) \\
&  =\sum_{j\in\left\{  1,2,\ldots,n\right\}  }\left(  -1\right)
^{n-j}\mathfrak{e}_{j-1}\delta_{q,j}\det A
\end{align*}%
\begin{align*}
&  =\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\underbrace{\delta_{q,q}%
}_{\substack{=1\\\text{(since }q=q\text{)}}}\det A+\sum_{\substack{j\in
\left\{  1,2,\ldots,n\right\}  ;\\j\neq q}}\left(  -1\right)  ^{n-j}%
\mathfrak{e}_{j-1}\underbrace{\delta_{q,j}}_{\substack{=0\\\text{(since }q\neq
j\\\text{(since }j\neq q\text{))}}}\det A\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }j=q\text{ from the sum}\\
\text{(since }q\in\left\{  1,2,\ldots,n\right\}  \text{)}%
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A+\underbrace{\sum
_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq q}}\left(  -1\right)
^{n-j}\mathfrak{e}_{j-1}0\det A}_{=0}=\left(  -1\right)  ^{n-q}\mathfrak{e}%
_{q-1}\det A.
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.y-sum.1}.
\end{proof}

\begin{lemma}
\label{lem.sol.vander-hook.y-sum.2}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. For each $i\in\left\{
1,2,\ldots,n\right\}  $, set $y_{i}=\prod_{\substack{j\in\left\{
1,2,\ldots,n\right\}  ;\\j\neq i}}x_{j}$.

Let $A$ be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Let $q\in\left\{  1,2,\ldots,n\right\}  $ and $\ell
\in\left\{  0,1,\ldots,n-q\right\}  $. For every $j\in\mathbb{N}$, define an
element $\mathfrak{e}_{j}\in\mathbb{K}$ by $\mathfrak{e}_{j}=e_{j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  $. Then,%
\[
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  =\delta_{\ell,n-q}\left(  -1\right)  ^{n-q}%
\mathfrak{e}_{q-1}\det A.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.y-sum.2}.]We are in one of the
following two cases:

\textit{Case 1:} We have $\ell=n-q$.

\textit{Case 2:} We have $\ell\neq n-q$.

Let us first consider Case 1. In this case, we have $\ell=n-q$. Thus,
$n-q-\underbrace{\ell}_{=n-q}=n-q-\left(  n-q\right)  =0$. Now,%
\begin{align*}
&  \sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}\underbrace{x_{k}^{n-q-\ell}%
}_{\substack{=x_{k}^{0}\\\text{(since }n-q-\ell=0\text{)}}}\det\left(  A_{\sim
k,\sim q}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}\underbrace{x_{k}^{0}}%
_{=1}\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}\det\left(  A_{\sim k,\sim
q}\right)  =\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A
\end{align*}
(by Lemma \ref{lem.sol.vander-hook.y-sum.1}). Comparing this with%
\[
\underbrace{\delta_{\ell,n-q}}_{\substack{=1\\\text{(since }\ell=n-q\text{)}%
}}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A=\left(  -1\right)
^{n-q}\mathfrak{e}_{q-1}\det A,
\]
we obtain%
\[
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  =\delta_{\ell,n-q}\left(  -1\right)  ^{n-q}%
\mathfrak{e}_{q-1}\det A.
\]
Hence, Lemma \ref{lem.sol.vander-hook.y-sum.2} is proven in Case 1.

Let us now consider Case 2. In this case, we have $\ell\neq n-q$. Combining
this with $\ell\leq n-q$ (since $\ell\in\left\{  0,1,\ldots,n-q\right\}  $),
we obtain $\ell<n-q$. Hence, $\ell\leq\left(  n-q\right)  -1$ (since $\ell$
and $n-q$ are integers). In other words, $\ell+1\leq n-q$. But $\ell\geq0$
(since $\ell\in\left\{  0,1,\ldots,n-q\right\}  $), so that $\ell+1\geq1$.
Combining this with $\ell+1\leq n-q$, we obtain $\ell+1\in\left\{
1,2,\ldots,n-q\right\}  $. Thus, Lemma \ref{lem.sol.vander-hook.lap2.variant}
(applied to $\ell+1$ instead of $\ell$) yields%
\begin{equation}
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\left(  \ell+1\right)  }%
\det\left(  A_{\sim k,\sim q}\right)  =0.
\label{pf.lem.sol.vander-hook.y-sum.2.c2.1}%
\end{equation}


But every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
y_{k}x_{k}^{n-q-\ell}=\mathfrak{e}_{n}x_{k}^{n-q-\left(  \ell+1\right)  }
\label{pf.lem.sol.vander-hook.y-sum.2.c2.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.vander-hook.y-sum.2.c2.3}):} Let
$k\in\left\{  1,2,\ldots,n\right\}  $. We have $n-q-\underbrace{\ell}%
_{\leq\left(  n-q\right)  -1}\geq n-q-\left(  \left(  n-q\right)  -1\right)
=1$. Thus, $x_{k}^{n-q-\ell}=x_{k}x_{k}^{\left(  n-q-\ell\right)  -1}%
=x_{k}x_{k}^{n-q-\left(  \ell+1\right)  }$ (since $\left(  n-q-\ell\right)
-1=n-q-\left(  \ell+1\right)  $).
\par
The definition of $y_{k}$ yields $y_{k}=\prod_{\substack{j\in\left\{
1,2,\ldots,n\right\}  ;\\j\neq k}}x_{j}$. But the definition of $\mathfrak{e}%
_{n}$ yields $\mathfrak{e}_{n}=e_{n}\left(  x_{1},x_{2},\ldots,x_{n}\right)
=\prod_{i=1}^{n}x_{i}$ (by Proposition \ref{prop.sol.vander-hook.viete}
\textbf{(b)} (applied to $t=0$)). Hence,%
\begin{align}
\mathfrak{e}_{n}  &  =\prod_{i=1}^{n}x_{i}=\underbrace{\prod_{j=1}^{n}%
}_{=\prod_{j\in\left\{  1,2,\ldots,n\right\}  }}x_{j}%
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the index }i\text{ as }j\\
\text{in the product}%
\end{array}
\right) \nonumber\\
&  =\prod_{j\in\left\{  1,2,\ldots,n\right\}  }x_{j}=x_{k}\underbrace{\prod
_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq k}}x_{j}}_{=y_{k}%
}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the factor}\\
\text{for }j=k\text{ from the product}%
\end{array}
\right) \nonumber\\
&  =x_{k}y_{k}=y_{k}x_{k}. \label{pf.lem.sol.vander-hook.y-sum.2.c2.3.pf.2}%
\end{align}
Now,%
\[
y_{k}\underbrace{x_{k}^{n-q-\ell}}_{=x_{k}x_{k}^{n-q-\left(  \ell+1\right)  }%
}=\underbrace{y_{k}x_{k}}_{\substack{=\mathfrak{e}_{n}\\\text{(by
(\ref{pf.lem.sol.vander-hook.y-sum.2.c2.3.pf.2}))}}}x_{k}^{n-q-\left(
\ell+1\right)  }=\mathfrak{e}_{n}x_{k}^{n-q-\left(  \ell+1\right)  }.
\]
This proves (\ref{pf.lem.sol.vander-hook.y-sum.2.c2.3}).}. Now,%
\begin{align*}
&  \sum_{k=1}^{n}\left(  -1\right)  ^{k+q}\underbrace{y_{k}x_{k}^{n-q-\ell}%
}_{\substack{=\mathfrak{e}_{n}x_{k}^{n-q-\left(  \ell+1\right)  }\\\text{(by
(\ref{pf.lem.sol.vander-hook.y-sum.2.c2.3}))}}}\det\left(  A_{\sim k,\sim
q}\right) \\
&  =\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}\mathfrak{e}_{n}x_{k}^{n-q-\left(
\ell+1\right)  }\det\left(  A_{\sim k,\sim q}\right) \\
&  =\mathfrak{e}_{n}\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}%
x_{k}^{n-q-\left(  \ell+1\right)  }\det\left(  A_{\sim k,\sim q}\right)
}_{\substack{=0\\\text{(by (\ref{pf.lem.sol.vander-hook.y-sum.2.c2.1}))}%
}}=\mathfrak{e}_{n}0=0.
\end{align*}
Comparing this with%
\[
\underbrace{\delta_{\ell,n-q}}_{\substack{=0\\\text{(since }\ell\neq
n-q\text{)}}}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A=0\left(
-1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A=0,
\]
we obtain%
\[
\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  =\delta_{\ell,n-q}\left(  -1\right)  ^{n-q}%
\mathfrak{e}_{q-1}\det A.
\]
Hence, Lemma \ref{lem.sol.vander-hook.y-sum.2} is proven in Case 2.

We have now proven Lemma \ref{lem.sol.vander-hook.y-sum.2} in each of the two
Cases 1 and 2. Since these two Cases cover all possibilities, this shows that
Lemma \ref{lem.sol.vander-hook.y-sum.2} always holds.
\end{proof}

\begin{lemma}
\label{lem.sol.vander-hook.lap1-y}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.
Let $A$ be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Let $k\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\begin{align*}
&  V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right) \\
&  =\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.vander-hook.lap1-y}.]The definition of $A$ yields
$A=\left(  x_{i}^{n-j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

For every $m\in\mathbb{N}$, we have%
\begin{equation}
\left(  X+Y\right)  ^{m}=\sum_{\ell=0}^{m}\dbinom{m}{\ell}X^{\ell}Y^{m-\ell}
\label{pf.lem.sol.vander-hook.lap1-y.binom}%
\end{equation}
(an equality between two polynomials in $X$ and $Y$). (Indeed, this is
precisely the statement of Proposition \ref{prop.binom.binomial}, with the
variables $n$ and $k$ renamed as $m$ and $\ell$.) Now, every $q\in\left\{
1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left(  x_{k}+t\right)  ^{n-q}=\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell
}x_{k}^{n-q-\ell} \label{pf.lem.sol.vander-hook.lap1-y.binom-used}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.vander-hook.lap1-y.binom-used}):}
Let $q\in\left\{  1,2,\ldots,n\right\}  $. Thus, $q\leq n$.
\par
Set $m=n-q$. Then, $m=n-q\in\mathbb{N}$ (since $q\leq n$). Substituting $t$
and $x_{k}$ for $X$ and $Y$ in the equality
(\ref{pf.lem.sol.vander-hook.lap1-y.binom}), we obtain%
\[
\left(  t+x_{k}\right)  ^{m}=\sum_{\ell=0}^{m}\dbinom{m}{\ell}t^{\ell}%
x_{k}^{m-\ell}.
\]
Since $t+x_{k}=x_{k}+t$, this rewrites as
\[
\left(  x_{k}+t\right)  ^{m}=\sum_{\ell=0}^{m}\dbinom{m}{\ell}t^{\ell}%
x_{k}^{m-\ell}.
\]
Since $m=n-q$, this rewrites as
\[
\left(  x_{k}+t\right)  ^{n-q}=\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell
}x_{k}^{n-q-\ell}.
\]
This proves (\ref{pf.lem.sol.vander-hook.lap1-y.binom-used}).}.

But%
\begin{align*}
&  V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1},x_{k+2},\ldots
,x_{n}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\underbrace{\left(  x_{k}+t\right)
^{n-q}}_{\substack{=\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell}%
x_{k}^{n-q-\ell}\\\text{(by (\ref{pf.lem.sol.vander-hook.lap1-y.binom-used}%
))}}}\det\left(  A_{\sim k,\sim q}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by Lemma \ref{lem.sol.vander-hook.lap1} \textbf{(b)}}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{k+q}\left(  \sum_{\ell=0}^{n-q}%
\dbinom{n-q}{\ell}t^{\ell}x_{k}^{n-q-\ell}\right)  \det\left(  A_{\sim k,\sim
q}\right) \\
&  =\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}\underbrace{\left(  -1\right)
^{k+q}\dbinom{n-q}{\ell}t^{\ell}}_{=\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.vander-hook.lap1-y}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.variant-1}.]For every
$j\in\mathbb{N}$, define an element $\mathfrak{e}_{j}\in\mathbb{K}$ by
\newline$\mathfrak{e}_{j}=e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $.

Let $A$ be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. We have%
\begin{align}
&  \sum_{k=1}^{n}y_{k}\underbrace{V\left(  x_{1},x_{2},\ldots,x_{k-1}%
,x_{k}+t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  }_{\substack{=\sum_{q=1}^{n}%
\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}%
x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)  \\\text{(by Lemma
\ref{lem.sol.vander-hook.lap1-y})}}}\nonumber\\
&  =\sum_{k=1}^{n}\underbrace{y_{k}\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}%
\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }_{=\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}y_{k}%
\dbinom{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }\nonumber\\
&  =\underbrace{\sum_{k=1}^{n}\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}}_{=\sum
_{q=1}^{n}\sum_{\ell=0}^{n-q}\sum_{k=1}^{n}}\underbrace{y_{k}\dbinom{n-q}%
{\ell}t^{\ell}\left(  -1\right)  ^{k+q}}_{=\dbinom{n-q}{\ell}t^{\ell}\left(
-1\right)  ^{k+q}y_{k}}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)
\nonumber\\
&  =\sum_{q=1}^{n}\sum_{\ell=0}^{n-q}\sum_{k=1}^{n}\dbinom{n-q}{\ell}t^{\ell
}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim
q}\right)  . \label{pf.prop.sol.vander-hook.variant-1.3}%
\end{align}


Now, let $q\in\left\{  1,2,\ldots,n\right\}  $. Thus, $n-q\in\left\{
0,1,\ldots,n-1\right\}  \subseteq\mathbb{N}$. Hence, $n-q\in\left\{
0,1,\ldots,n-q\right\}  $. Furthermore, (\ref{eq.binom.mm}) (applied to
$m=n-q$) yields $\dbinom{n-q}{n-q}=1$. Now,
\begin{align}
&  \sum_{\ell=0}^{n-q}\underbrace{\sum_{k=1}^{n}\dbinom{n-q}{\ell}t^{\ell
}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim
q}\right)  }_{=\dbinom{n-q}{\ell}t^{\ell}\sum_{k=1}^{n}\left(  -1\right)
^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim q}\right)  }\nonumber\\
&  =\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\underbrace{\sum_{k=1}%
^{n}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(  A_{\sim k,\sim
q}\right)  }_{\substack{=\delta_{\ell,n-q}\left(  -1\right)  ^{n-q}%
\mathfrak{e}_{q-1}\det A\\\text{(by Lemma \ref{lem.sol.vander-hook.y-sum.2})}%
}}\nonumber\\
&  =\sum_{\ell=0}^{n-q}\dbinom{n-q}{\ell}t^{\ell}\delta_{\ell,n-q}\left(
-1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A\nonumber\\
&  =\sum_{\ell=0}^{\left(  n-q\right)  -1}\dbinom{n-q}{\ell}t^{\ell
}\underbrace{\delta_{\ell,n-q}}_{\substack{=0\\\text{(because }\ell\neq
n-q\\\text{(since }\ell\leq\left(  n-q\right)  -1<n-q\text{))}}}\left(
-1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\dbinom{n-q}{n-q}}_{=1}t^{n-q}%
\underbrace{\delta_{n-q,n-q}}_{\substack{=1\\\text{(since }n-q=n-q\text{)}%
}}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }\ell=n-q\text{ from the sum}\\
\text{(since }n-q\in\left\{  0,1,\ldots,n-q\right\}  \text{)}%
\end{array}
\right) \nonumber\\
&  =\underbrace{\sum_{\ell=0}^{\left(  n-q\right)  -1}\dbinom{n-q}{\ell
}t^{\ell}0\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A}_{=0}%
+t^{n-q}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A\nonumber\\
&  =t^{n-q}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A.
\label{pf.prop.sol.vander-hook.variant-1.7}%
\end{align}


Now, forget that we fixed $q$. We thus have proven
(\ref{pf.prop.sol.vander-hook.variant-1.7}) for every $q\in\left\{
1,2,\ldots,n\right\}  $.

Now, (\ref{pf.prop.sol.vander-hook.variant-1.3}) becomes%
\begin{align}
&  \sum_{k=1}^{n}y_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \nonumber\\
&  =\sum_{q=1}^{n}\underbrace{\sum_{\ell=0}^{n-q}\sum_{k=1}^{n}\dbinom
{n-q}{\ell}t^{\ell}\left(  -1\right)  ^{k+q}y_{k}x_{k}^{n-q-\ell}\det\left(
A_{\sim k,\sim q}\right)  }_{\substack{=t^{n-q}\left(  -1\right)
^{n-q}\mathfrak{e}_{q-1}\det A\\\text{(by
(\ref{pf.prop.sol.vander-hook.variant-1.7}))}}}\nonumber\\
&  =\sum_{q=1}^{n}t^{n-q}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A.
\label{pf.prop.sol.vander-hook.variant-1.8}%
\end{align}
But%
\begin{align}
&  \sum_{q=1}^{n}\underbrace{t^{n-q}\left(  -1\right)  ^{n-q}}_{=\left(
t\cdot\left(  -1\right)  \right)  ^{n-q}}\mathfrak{e}_{q-1}\det A\nonumber\\
&  =\sum_{q=1}^{n}\left(  \underbrace{t\cdot\left(  -1\right)  }_{=-t}\right)
^{n-q}\mathfrak{e}_{q-1}\det A=\sum_{q=1}^{n}\left(  -t\right)  ^{n-q}%
\mathfrak{e}_{q-1}\det A\nonumber\\
&  =\sum_{j=0}^{n-1}\underbrace{\left(  -t\right)  ^{n-\left(  n-j\right)  }%
}_{\substack{=\left(  -t\right)  ^{j}\\\text{(since }n-\left(  n-j\right)
=j\text{)}}}\underbrace{\mathfrak{e}_{\left(  n-j\right)  -1}}%
_{\substack{=\mathfrak{e}_{n-1-j}\\\text{(since }\left(  n-j\right)
-1=n-1-j\text{)}}}\det A\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n-j\text{ for
}q\text{ in the sum}\right) \nonumber\\
&  =\sum_{j=0}^{n-1}\left(  -t\right)  ^{j}\underbrace{\mathfrak{e}_{n-1-j}%
}_{\substack{=e_{n-1-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
the definition of }\mathfrak{e}_{n-1-j}\text{)}}}\det A=\sum_{j=0}%
^{n-1}\left(  -t\right)  ^{j}e_{n-1-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)
\det A\nonumber\\
&  =\left(  \sum_{j=0}^{n-1}\left(  -t\right)  ^{j}e_{n-1-j}\left(
x_{1},x_{2},\ldots,x_{n}\right)  \right)  \det A.
\label{pf.prop.sol.vander-hook.variant-1.9a}%
\end{align}


But the definition of $z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $ yields%
\begin{align}
z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)   &  =\sum_{j=0}^{n-1}%
e_{n-1-j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \left(  -t\right)
^{j}\nonumber\\
&  =\sum_{j=0}^{n-1}\left(  -t\right)  ^{j}e_{n-1-j}\left(  x_{1},x_{2}%
,\ldots,x_{n}\right)  . \label{pf.prop.sol.vander-hook.variant-1.11}%
\end{align}
Thus, (\ref{pf.prop.sol.vander-hook.variant-1.9a}) becomes%
\begin{align}
&  \sum_{q=1}^{n}t^{n-q}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det
A\nonumber\\
&  =\underbrace{\left(  \sum_{j=0}^{n-1}\left(  -t\right)  ^{j}e_{n-1-j}%
\left(  x_{1},x_{2},\ldots,x_{n}\right)  \right)  }_{\substack{=z_{-t}\left(
x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
(\ref{pf.prop.sol.vander-hook.variant-1.11}))}}}\underbrace{\det
A}_{\substack{=V\left(  x_{1},x_{2},\ldots,x_{n}\right)  \\\text{(by
(\ref{eq.lem.sol.vander-hook.V=x.eq}))}}}\nonumber\\
&  =z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \cdot V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  . \label{pf.prop.sol.vander-hook.variant-1.13}%
\end{align}
Hence, (\ref{pf.prop.sol.vander-hook.variant-1.8}) becomes%
\begin{align*}
&  \sum_{k=1}^{n}y_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},x_{k}+t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =\sum_{q=1}^{n}t^{n-q}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det
A=z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \cdot V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)
\end{align*}
(by (\ref{pf.prop.sol.vander-hook.variant-1.13})). Thus, Proposition
\ref{prop.sol.vander-hook.variant-1} is finally proven.
\end{proof}

Let us finish with one further identity, which is reminiscent of both
Proposition \ref{prop.sol.vander-hook.variant-1} and Proposition
\ref{prop.sol.vander-hook.N}:

\begin{proposition}
\label{prop.sol.vander-hook.variant-2}Let $n\in\mathbb{N}$. Let $x_{1}%
,x_{2},\ldots,x_{n}$ be $n$ elements of $\mathbb{K}$. Let $t\in\mathbb{K}$.
For each $i\in\left\{  1,2,\ldots,n\right\}  $, set $y_{i}=\prod
_{\substack{j\in\left\{  1,2,\ldots,n\right\}  ;\\j\neq i}}x_{j}$. Then,%
\begin{align*}
&  \sum_{k=1}^{n}y_{k}V\left(  x_{1},x_{2},\ldots,x_{k-1},t,x_{k+1}%
,x_{k+2},\ldots,x_{n}\right) \\
&  =z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \cdot V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)  .
\end{align*}

\end{proposition}

The proof proceeds similarly as our proof of Proposition
\ref{prop.sol.vander-hook.N}:

\begin{proof}
[Proof of Proposition \ref{prop.sol.vander-hook.variant-2}.]For every
$j\in\mathbb{N}$, define an element $\mathfrak{e}_{j}\in\mathbb{K}$ by
\newline$\mathfrak{e}_{j}=e_{j}\left(  x_{1},x_{2},\ldots,x_{n}\right)  $.

Let $A$ be the $n\times n$-matrix $\left(  x_{i}^{n-j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Now,%
\begin{align*}
&  \sum_{k=1}^{n}y_{k}\underbrace{V\left(  x_{1},x_{2},\ldots,x_{k-1}%
,t,x_{k+1},x_{k+2},\ldots,x_{n}\right)  }_{\substack{=\sum_{q=1}^{n}\left(
-1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  \\\text{(by
Lemma \ref{lem.sol.vander-hook.N.lap1b})}}}\\
&  =\sum_{k=1}^{n}\underbrace{y_{k}\sum_{q=1}^{n}\left(  -1\right)
^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  }_{=\sum_{q=1}^{n}%
y_{k}\left(  -1\right)  ^{k+q}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  }\\
&  =\underbrace{\sum_{k=1}^{n}\sum_{q=1}^{n}}_{=\sum_{q=1}^{n}\sum_{k=1}^{n}%
}\underbrace{y_{k}\left(  -1\right)  ^{k+q}}_{=\left(  -1\right)  ^{k+q}y_{k}%
}t^{n-q}\det\left(  A_{\sim k,\sim q}\right) \\
&  =\sum_{q=1}^{n}\underbrace{\sum_{k=1}^{n}\left(  -1\right)  ^{k+q}%
y_{k}t^{n-q}\det\left(  A_{\sim k,\sim q}\right)  }_{=t^{n-q}\sum_{k=1}%
^{n}\left(  -1\right)  ^{k+q}y_{k}\det\left(  A_{\sim k,\sim q}\right)  }\\
&  =\sum_{q=1}^{n}t^{n-q}\underbrace{\sum_{k=1}^{n}\left(  -1\right)
^{k+q}y_{k}\det\left(  A_{\sim k,\sim q}\right)  }_{\substack{=\left(
-1\right)  ^{n-q}\mathfrak{e}_{q-1}\det A\\\text{(by Lemma
\ref{lem.sol.vander-hook.y-sum.1})}}}\\
&  =\sum_{q=1}^{n}t^{n-q}\left(  -1\right)  ^{n-q}\mathfrak{e}_{q-1}\det
A=z_{-t}\left(  x_{1},x_{2},\ldots,x_{n}\right)  \cdot V\left(  x_{1}%
,x_{2},\ldots,x_{n}\right)
\end{align*}
(by (\ref{pf.prop.sol.vander-hook.variant-1.13})). This proves Proposition
\ref{prop.sol.vander-hook.variant-2}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.block2x2.VB+WD}}

\begin{proof}
[Solution to Exercise \ref{exe.block2x2.VB+WD}.]Note that $VB$ and $WD$ are
$m\times m$-matrices. Hence, $VB+WD$ is an $m\times m$-matrix. Also, $\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right)  $ and $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ are $\left(  n+m\right)  \times\left(  n+m\right)  $-matrices.

Exercise \ref{exe.block2x2.mult} (applied to $n$, $m$, $n$, $m$, $n$, $m$,
$I_{n}$, $0_{n\times m}$, $V$, $W$, $A$, $B$, $C$ and $D$ instead of $n$,
$n^{\prime}$, $m$, $m^{\prime}$, $\ell$, $\ell^{\prime}$, $A$, $B$, $C$, $D$,
$A^{\prime}$, $B^{\prime}$, $C^{\prime}$ and $D^{\prime}$) yields%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)   &  =\left(
\begin{array}
[c]{cc}%
I_{n}A+0_{n\times m}C & I_{n}B+0_{n\times m}D\\
VA+WC & VB+WD
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
A & B\\
0_{m\times n} & VB+WD
\end{array}
\right)
\end{align*}
(since $I_{n}A+\underbrace{0_{n\times m}C}_{=0_{n\times n}}=I_{n}A=A$,
$I_{n}B+\underbrace{0_{n\times m}D}_{=0_{n\times m}}=I_{n}B=B$ and
$\underbrace{VA}_{=-WC}+WC=-WC+WC=0_{m\times n}$). Taking determinants on both
sides of this equality, we obtain%
\begin{align*}
\det\left(  \left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \right)   &  =\det\left(
\begin{array}
[c]{cc}%
A & B\\
0_{m\times n} & VB+WD
\end{array}
\right) \\
&  =\det A\cdot\det\left(  VB+WD\right)
\end{align*}
(by Exercise \ref{exe.block2x2.tridet} (applied to $VB+WD$ instead of $D$)).
Hence,%
\begin{align*}
&  \det A\cdot\det\left(  VB+WD\right) \\
&  =\det\left(  \left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \right) \\
&  =\underbrace{\det\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right)  }_{\substack{=\det\left(  I_{n}\right)  \cdot\det W\\\text{(by
Exercise \ref{exe.block2x2.tridet.transposed} (applied}\\\text{to }%
I_{n}\text{, }V\text{ and }W\text{ instead of }A\text{, }C\text{ and
}D\text{))}}}\cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Theorem \ref{thm.det(AB)} (applied to }n+m\text{, }\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
V & W
\end{array}
\right) \\
\text{and }\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \text{ instead of }n\text{, }A\text{ and }B\text{)}%
\end{array}
\right) \\
&  =\underbrace{\det\left(  I_{n}\right)  }_{=1}\cdot\det W\cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\det W\cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  .
\end{align*}
This solves Exercise \ref{exe.block2x2.VB+WD}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.block2x2.schur}}

\begin{proof}
[Solution to Exercise \ref{exe.block2x2.schur}.]Recall that $\det\left(
I_{n}\right)  =1$. The same argument (applied to $m$ instead of $n$) yields
$\det\left(  I_{m}\right)  =1$.

The matrix $A$ is invertible; thus, it has an inverse $A^{-1}\in
\mathbb{K}^{n\times n}$. The matrices $I_{m}\in\mathbb{K}^{m\times m}$ and
$-CA^{-1}\in\mathbb{K}^{m\times n}$ satisfy $-\left(  CA^{-1}\right)
A=-I_{m}C$ (since $-\left(  CA^{-1}\right)  A=-C\underbrace{A^{-1}A}_{=I_{n}%
}=-CI_{n}=-\underbrace{C}_{=I_{m}C}=-I_{m}C$). Hence, Exercise
\ref{exe.block2x2.VB+WD} (applied to $W=I_{m}$ and $V=-CA^{-1}$) yields%
\begin{align*}
\det\left(  I_{m}\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)   &  =\det A\cdot\det\left(  -CA^{-1}B+\underbrace{I_{m}D}_{=D}\right)
\\
&  =\det A\cdot\det\left(  \underbrace{-CA^{-1}B+D}_{=D-CA^{-1}B}\right)
=\det A\cdot\det\left(  D-CA^{-1}B\right)  .
\end{align*}
Hence,%
\[
\det A\cdot\det\left(  D-CA^{-1}B\right)  =\underbrace{\det\left(
I_{m}\right)  }_{=1}\cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  .
\]
This solves Exercise \ref{exe.block2x2.schur}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.block2x2.jacobi}}

Before we solve Exercise \ref{exe.block2x2.jacobi}, let us show two really
simple facts:

\begin{lemma}
\label{lem.sol.block2x2.jacobi.InIm}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Then,%
\[
\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
0_{m\times n} & I_{m}%
\end{array}
\right)  =I_{n+m}.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.block2x2.jacobi.InIm}.]Easy, and left to the reader.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.block2x2.jacobi.InIm}.]For any two objects $i$
and $j$, define an element $\delta_{i,j}\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.

We have $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$) and $0_{n\times m}=\left(  0\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$ (by the definition of $0_{n\times m}$) and
$0_{m\times n}=\left(  0\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$ (by the
definition of $0_{m\times n}$) and $I_{m}=\left(  \delta_{i,j}\right)  _{1\leq
i\leq m,\ 1\leq j\leq m}$ (by the definition of $I_{m}$). Hence, the
definition of $\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
0_{m\times n} & I_{m}%
\end{array}
\right)  $ yields%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
0_{m\times n} & I_{m}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\right)  _{1\leq i\leq n+m,\ 1\leq j\leq n+m}.
\label{pf.lem.sol.block2x2.jacobi.InIm.LHS}%
\end{align}


But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+m\right\}  ^{2}$
satisfy%
\begin{equation}%
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
=\delta_{i,j} \label{pf.lem.sol.block2x2.jacobi.InIm.deldel}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}):}
Let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+m\right\}  ^{2}$. We want to
prove (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+m\right\}  ^{2}$. In
other words, $i\in\left\{  1,2,\ldots,n+m\right\}  $ and $j\in\left\{
1,2,\ldots,n+m\right\}  $. We are in one of the following two cases:
\par
\textit{Case 1:} We have $i\leq n$.
\par
\textit{Case 2:} We have $i>n$.
\par
Let us first consider Case 1. In this case, we have $i\leq n$. We are in one
of the following two subcases:
\par
\textit{Subcase 1.1:} We have $j\leq n$.
\par
\textit{Subcase 1.2:} We have $j>n$.
\par
Let us first consider Subcase 1.1. In this case, we have $j\leq n$. Hence,
\[%
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
=\delta_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq n\text{ and }j\leq
n\right)  .
\]
Thus, (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) is proven in Subcase 1.1.
\par
Let us next consider Subcase 1.2. In this case, we have $j>n$. Hence, $n<j$,
so that $i\leq n<j$. Thus, $i\neq j$, so that $\delta_{i,j}=0$. But%
\begin{align*}%
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq n\text{ and }j>n\right) \\
&  =\delta_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{i,j}%
=0\right)  .
\end{align*}
Thus, (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) is proven in Subcase 1.2.
\par
We now have proven (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) in each of
the two Subcases 1.1 and 1.2. Since these two Subcases cover the whole Case 1,
we can thus conclude that (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) holds
in Case 1.
\par
Let us now consider Case 2. In this case, we have $i>n$. We are in one of the
following two subcases:
\par
\textit{Subcase 2.1:} We have $j\leq n$.
\par
\textit{Subcase 2.2:} We have $j>n$.
\par
Let us first consider Subcase 2.1. In this case, we have $j\leq n$. Hence,
$n\geq j$, and thus $i>n\geq j$. Hence, $i\neq j$, and thus $\delta_{i,j}=0$.
But%
\begin{align*}%
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
&  =0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i>n\text{ and }j\leq n\right) \\
&  =\delta_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{i,j}%
=0\right)  .
\end{align*}
Thus, (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) is proven in Subcase 2.1.
\par
Let us next consider Subcase 2.2. In this case, we have $j>n$. But the
definition of $\delta_{i,j}$ yields $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Now,%
\begin{align*}
&
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
\\
&  =\delta_{i-n,j-n}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i>n\text{ and
}j>n\right) \\
&  =%
\begin{cases}
1, & \text{if }i-n=j-n;\\
0, & \text{if }i-n\neq j-n
\end{cases}
\\
&  =%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the statement }i-n=j-n\text{ is equivalent to }i=j\text{,}\\
\text{and since the statement }i-n\neq j-n\text{ is equivalent to }i\neq j
\end{array}
\right) \\
&  =\delta_{i,j}.
\end{align*}
Thus, (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) is proven in Subcase 2.2.
\par
We now have proven (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) in each of
the two Subcases 2.1 and 2.2. Since these two Subcases cover the whole Case 2,
we can thus conclude that (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) holds
in Case 2.
\par
We now have proven (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) in each of
the two Cases 1 and 2. Since these two Cases cover all possibilities, we can
thus conclude that (\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}) always
holds. Qed.}.

Now, (\ref{pf.lem.sol.block2x2.jacobi.InIm.LHS}) becomes%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
0_{m\times n} & I_{m}%
\end{array}
\right)   &  =\left(  \underbrace{%
\begin{cases}
\delta_{i,j}, & \text{if }i\leq n\text{ and }j\leq n;\\
0, & \text{if }i\leq n\text{ and }j>n;\\
0, & \text{if }i>n\text{ and }j\leq n;\\
\delta_{i-n,j-n}, & \text{if }i>n\text{ and }j>n
\end{cases}
}_{\substack{=\delta_{i,j}\\\text{(by
(\ref{pf.lem.sol.block2x2.jacobi.InIm.deldel}))}}}\right)  _{1\leq i\leq
n+m,\ 1\leq j\leq n+m}\\
&  =\left(  \delta_{i,j}\right)  _{1\leq i\leq n+m,\ 1\leq j\leq n+m}.
\end{align*}
Comparing this with%
\[
I_{n+m}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n+m,\ 1\leq j\leq
n+m}\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }I_{n+m}\right)  ,
\]
we obtain $\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
0_{m\times n} & I_{m}%
\end{array}
\right)  =I_{n+m}$. This proves Lemma \ref{lem.sol.block2x2.jacobi.InIm}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.block2x2.jacobi.equal}Let $n$, $n^{\prime}$, $m$ and
$m^{\prime}$ be four nonnegative integers.

Let $A\in\mathbb{K}^{n\times m}$ and $A^{\prime}\in\mathbb{K}^{n\times m}$.
Let $B\in\mathbb{K}^{n\times m^{\prime}}$ and $B^{\prime}\in\mathbb{K}%
^{n\times m^{\prime}}$. Let $C\in\mathbb{K}^{n^{\prime}\times m}$ and
$C^{\prime}\in\mathbb{K}^{n^{\prime}\times m}$. Let $D\in\mathbb{K}%
^{n^{\prime}\times m^{\prime}}$ and $D^{\prime}\in\mathbb{K}^{n^{\prime}\times
m^{\prime}}$. Assume that $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $. Then, $A=A^{\prime}$, $B=B^{\prime}$, $C=C^{\prime}$ and
$D=D^{\prime}$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.block2x2.jacobi.equal}.]Easy, and left to the reader.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.block2x2.jacobi.equal}.]Write the matrix
$A\in\mathbb{K}^{n\times m}$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$.

Write the matrix $A^{\prime}\in\mathbb{K}^{n\times m}$ in the form $A^{\prime
}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$.

Write the matrix $B\in\mathbb{K}^{n\times m^{\prime}}$ in the form $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$.

Write the matrix $B^{\prime}\in\mathbb{K}^{n\times m^{\prime}}$ in the form
$B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m^{\prime}}$.

Write the matrix $C\in\mathbb{K}^{n^{\prime}\times m}$ in the form $C=\left(
c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$.

Write the matrix $C^{\prime}\in\mathbb{K}^{n^{\prime}\times m}$ in the form
$C^{\prime}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq
j\leq m}$.

Write the matrix $D\in\mathbb{K}^{n^{\prime}\times m^{\prime}}$ in the form
$D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}$.

Write the matrix $D^{\prime}\in\mathbb{K}^{n^{\prime}\times m^{\prime}}$ in
the form $D^{\prime}=\left(  d_{i,j}^{\prime}\right)  _{1\leq i\leq n^{\prime
},\ 1\leq j\leq m^{\prime}}$.

We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$,
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}$,
$C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$ and
$D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}%
$. Hence, the definition of $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $ yields%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\label{pf.lem.sol.block2x2.jacobi.equal.1}%
\end{align}


We have $A^{\prime}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m}$, $B^{\prime}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m^{\prime}}$, $C^{\prime}=\left(  c_{i,j}^{\prime}\right)
_{1\leq i\leq n^{\prime},\ 1\leq j\leq m}$ and $D^{\prime}=\left(
d_{i,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}$.
Hence, the definition of $\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $ yields%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}^{\prime}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}^{\prime}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}^{\prime}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}.
\label{pf.lem.sol.block2x2.jacobi.equal.2}%
\end{align}
Now, from (\ref{pf.lem.sol.block2x2.jacobi.equal.1}), we obtain%
\begin{align*}
&  \left(
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}\\
&  =\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right) \\
&  =\left(
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}^{\prime}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}^{\prime}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}^{\prime}, & \text{if }i>n\text{ and }j>m
\end{cases}
\right)  _{1\leq i\leq n+n^{\prime},\ 1\leq j\leq m+m^{\prime}}%
\end{align*}
(by (\ref{pf.lem.sol.block2x2.jacobi.equal.2})). In other words,%
\begin{align}
&
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\nonumber\\
&  =%
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}^{\prime}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}^{\prime}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}^{\prime}, & \text{if }i>n\text{ and }j>m
\end{cases}
\label{pf.lem.sol.block2x2.jacobi.equal.3}%
\end{align}
for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n+n^{\prime}\right\}
\times\left\{  1,2,\ldots,m+m^{\prime}\right\}  $.

Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m\right\}  $ satisfies $a_{i,j}=a_{i,j}^{\prime}$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m\right\}  $. Thus,
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}
$.
\par
We have $i\in\left\{  1,2,\ldots,n\right\}  \subseteq\left\{  1,2,\ldots
,n+n^{\prime}\right\}  $ (since $n\leq n+n^{\prime}$) and $i\leq n$ (since
$i\in\left\{  1,2,\ldots,n\right\}  $). We have $j\in\left\{  1,2,\ldots
,m\right\}  \subseteq\left\{  1,2,\ldots,m+m^{\prime}\right\}  $ (since $m\leq
m+m^{\prime}$) and $j\leq m$ (since $j\in\left\{  1,2,\ldots,m\right\}  $).
\par
From $i\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  $ and $j\in\left\{
1,2,\ldots,m+m^{\prime}\right\}  $, we obtain $\left(  i,j\right)  \in\left\{
1,2,\ldots,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots,m+m^{\prime
}\right\}  $. Now, $%
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
=a_{i,j}$ (since $i\leq n$ and $j\leq m$). Hence,%
\begin{align*}
a_{i,j}  &  =%
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}, & \text{if }i>n\text{ and }j>m
\end{cases}
\\
&  =%
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq n\text{ and }j\leq m;\\
b_{i,j-m}^{\prime}, & \text{if }i\leq n\text{ and }j>m;\\
c_{i-n,j}^{\prime}, & \text{if }i>n\text{ and }j\leq m;\\
d_{i-n,j-m}^{\prime}, & \text{if }i>n\text{ and }j>m
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.lem.sol.block2x2.jacobi.equal.3}%
)}\right) \\
&  =a_{i,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq n\text{ and
}j\leq m\right)  .
\end{align*}
Qed.}. In other words, $\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}=\left(  a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Now,%
\[
A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=\left(
a_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=A^{\prime}.
\]


Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,m^{\prime}\right\}  $ satisfies $b_{i,j}=b_{i,j}^{\prime}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  \times\left\{  1,2,\ldots,m^{\prime}\right\}  $. Thus,
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m^{\prime
}\right\}  $. We have $i\in\left\{  1,2,\ldots,n\right\}  \subseteq\left\{
1,2,\ldots,n+n^{\prime}\right\}  $ (since $n\leq n+n^{\prime}$) and $i\leq n$
(since $i\in\left\{  1,2,\ldots,n\right\}  $). We have $j\in\left\{
1,2,\ldots,m^{\prime}\right\}  $ and thus $j+m\in\left\{  m+1,m+2,\ldots
,m+m^{\prime}\right\}  \subseteq\left\{  1,2,\ldots,m+m^{\prime}\right\}  $
(since $\underbrace{m}_{\geq0}+1\geq1$). Also, $\underbrace{j}_{>0}+m>m$.
\par
From $i\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  $ and $j+m\in\left\{
1,2,\ldots,m+m^{\prime}\right\}  $, we obtain $\left(  i,j+m\right)
\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots
,m+m^{\prime}\right\}  $. Now, $%
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j+m\leq m;\\
b_{i,\left(  j+m\right)  -m}, & \text{if }i\leq n\text{ and }j+m>m;\\
c_{i-n,j+m}, & \text{if }i>n\text{ and }j+m\leq m;\\
d_{i-n,\left(  j+m\right)  -m}, & \text{if }i>n\text{ and }j+m>m
\end{cases}
=b_{i,\left(  j+m\right)  -m}$ (since $i\leq n$ and $j+m>m$). Hence,%
\begin{align*}
b_{i,\left(  j+m\right)  -m}  &  =%
\begin{cases}
a_{i,j}, & \text{if }i\leq n\text{ and }j+m\leq m;\\
b_{i,\left(  j+m\right)  -m}, & \text{if }i\leq n\text{ and }j+m>m;\\
c_{i-n,j+m}, & \text{if }i>n\text{ and }j+m\leq m;\\
d_{i-n,\left(  j+m\right)  -m}, & \text{if }i>n\text{ and }j+m>m
\end{cases}
\\
&  =%
\begin{cases}
a_{i,j}^{\prime}, & \text{if }i\leq n\text{ and }j+m\leq m;\\
b_{i,\left(  j+m\right)  -m}^{\prime}, & \text{if }i\leq n\text{ and }j+m>m;\\
c_{i-n,j+m}^{\prime}, & \text{if }i>n\text{ and }j+m\leq m;\\
d_{i-n,\left(  j+m\right)  -m}^{\prime}, & \text{if }i>n\text{ and }j+m>m
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{pf.lem.sol.block2x2.jacobi.equal.3}) (applied}\\
\text{to }\left(  i,j+m\right)  \text{ instead of }\left(  i,j\right)
\text{)}%
\end{array}
\right) \\
&  =b_{i,\left(  j+m\right)  -m}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i\leq n\text{ and }j+m>m\right)  .
\end{align*}
This rewrites as $b_{i,j}=b_{i,j}^{\prime}$ (since $\left(  j+m\right)
-m=j$). Qed.}. In other words, $\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m^{\prime}}=\left(  b_{i,j}^{\prime}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m^{\prime}}$. Now,%
\[
B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}=\left(
b_{i,j}^{\prime}\right)  _{1\leq i\leq n,\ 1\leq j\leq m^{\prime}}=B^{\prime
}.
\]


Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n^{\prime}\right\}
\times\left\{  1,2,\ldots,m\right\}  $ satisfies $c_{i,j}=c_{i,j}^{\prime}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n^{\prime}\right\}  \times\left\{  1,2,\ldots,m\right\}  $. Thus,
$i\in\left\{  1,2,\ldots,n^{\prime}\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $. We have $i\in\left\{  1,2,\ldots,n^{\prime}\right\}
$ and thus $i+n\in\left\{  n+1,n+2,\ldots,n+n^{\prime}\right\}  \subseteq
\left\{  1,2,\ldots,n+n^{\prime}\right\}  $ (since $\underbrace{n}_{\geq
0}+1\geq n$). Also, $\underbrace{i}_{>0}+n>n$. We have $j\in\left\{
1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots,m+m^{\prime}\right\}  $
(since $m\leq m+m^{\prime}$) and $j\leq m$ (since $j\in\left\{  1,2,\ldots
,m\right\}  $).
\par
From $i+n\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  $ and $j\in\left\{
1,2,\ldots,m+m^{\prime}\right\}  $, we obtain $\left(  i+n,j\right)
\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots
,m+m^{\prime}\right\}  $. Now, $%
\begin{cases}
a_{i+n,j}, & \text{if }i+n\leq n\text{ and }j\leq m;\\
b_{i+n,j-m}, & \text{if }i+n\leq n\text{ and }j>m;\\
c_{\left(  i+n\right)  -n,j}, & \text{if }i+n>n\text{ and }j\leq m;\\
d_{\left(  i+n\right)  -n,j-m}, & \text{if }i+n>n\text{ and }j>m
\end{cases}
=c_{\left(  i+n\right)  -n,j}$ (since $i+n>n$ and $j\leq m$). Hence,%
\begin{align*}
c_{\left(  i+n\right)  -n,j}  &  =%
\begin{cases}
a_{i+n,j}, & \text{if }i+n\leq n\text{ and }j\leq m;\\
b_{i+n,j-m}, & \text{if }i+n\leq n\text{ and }j>m;\\
c_{\left(  i+n\right)  -n,j}, & \text{if }i+n>n\text{ and }j\leq m;\\
d_{\left(  i+n\right)  -n,j-m}, & \text{if }i+n>n\text{ and }j>m
\end{cases}
\\
&  =%
\begin{cases}
a_{i+n,j}^{\prime}, & \text{if }i+n\leq n\text{ and }j\leq m;\\
b_{i+n,j-m}^{\prime}, & \text{if }i+n\leq n\text{ and }j>m;\\
c_{\left(  i+n\right)  -n,j}^{\prime}, & \text{if }i+n>n\text{ and }j\leq m;\\
d_{\left(  i+n\right)  -n,j-m}^{\prime}, & \text{if }i+n>n\text{ and }j>m
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{pf.lem.sol.block2x2.jacobi.equal.3}) (applied}\\
\text{to }\left(  i+n,j\right)  \text{ instead of }\left(  i,j\right)
\text{)}%
\end{array}
\right) \\
&  =c_{\left(  i+n\right)  -n,j}^{\prime}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }i+n>n\text{ and }j\leq m\right)  .
\end{align*}
This rewrites as $c_{i,j}=c_{i,j}^{\prime}$ (since $\left(  i+n\right)
-n=i$). Qed.}. In other words, $\left(  c_{i,j}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq m}=\left(  c_{i,j}^{\prime}\right)  _{1\leq i\leq
n^{\prime},\ 1\leq j\leq m}$. Now,%
\[
C=\left(  c_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}=\left(
c_{i,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m}=C^{\prime
}.
\]


Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n^{\prime}\right\}
\times\left\{  1,2,\ldots,m^{\prime}\right\}  $ satisfies $d_{i,j}%
=d_{i,j}^{\prime}$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)
\in\left\{  1,2,\ldots,n^{\prime}\right\}  \times\left\{  1,2,\ldots
,m^{\prime}\right\}  $. Thus, $i\in\left\{  1,2,\ldots,n^{\prime}\right\}  $
and $j\in\left\{  1,2,\ldots,m^{\prime}\right\}  $. We have $i\in\left\{
1,2,\ldots,n^{\prime}\right\}  $ and thus $i+n\in\left\{  n+1,n+2,\ldots
,n+n^{\prime}\right\}  \subseteq\left\{  1,2,\ldots,n+n^{\prime}\right\}  $
(since $\underbrace{n}_{\geq0}+1\geq n$). Also, $\underbrace{i}_{>0}+n>n$. We
have $j\in\left\{  1,2,\ldots,m^{\prime}\right\}  $ and thus $j+m\in\left\{
m+1,m+2,\ldots,m+m^{\prime}\right\}  \subseteq\left\{  1,2,\ldots,m+m^{\prime
}\right\}  $ (since $\underbrace{m}_{\geq0}+1\geq m$). Also, $\underbrace{j}%
_{>0}+m>m$.
\par
From $i+n\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  $ and $j+m\in\left\{
1,2,\ldots,m+m^{\prime}\right\}  $, we obtain $\left(  i+n,j+m\right)
\in\left\{  1,2,\ldots,n+n^{\prime}\right\}  \times\left\{  1,2,\ldots
,m+m^{\prime}\right\}  $. Now, $%
\begin{cases}
a_{i+n,j+m}, & \text{if }i+n\leq n\text{ and }j+m\leq m;\\
b_{i+n,\left(  j+m\right)  -m}, & \text{if }i+n\leq n\text{ and }j+m>m;\\
c_{\left(  i+n\right)  -n,j+m}, & \text{if }i+n>n\text{ and }j+m\leq m;\\
d_{\left(  i+n\right)  -n,\left(  j+m\right)  -m}, & \text{if }i+n>n\text{ and
}j+m>m
\end{cases}
=d_{\left(  i+n\right)  -n,\left(  j+m\right)  -m}$ (since $i+n>n$ and
$j+m>m$). Hence,%
\begin{align*}
&  d_{\left(  i+n\right)  -n,\left(  j+m\right)  -m}\\
&  =%
\begin{cases}
a_{i+n,j+m}, & \text{if }i+n\leq n\text{ and }j+m\leq m;\\
b_{i+n,\left(  j+m\right)  -m}, & \text{if }i+n\leq n\text{ and }j+m>m;\\
c_{\left(  i+n\right)  -n,j+m}, & \text{if }i+n>n\text{ and }j+m\leq m;\\
d_{\left(  i+n\right)  -n,\left(  j+m\right)  -m}, & \text{if }i+n>n\text{ and
}j+m>m
\end{cases}
\\
&  =%
\begin{cases}
a_{i+n,j+m}^{\prime}, & \text{if }i+n\leq n\text{ and }j+m\leq m;\\
b_{i+n,\left(  j+m\right)  -m}^{\prime}, & \text{if }i+n\leq n\text{ and
}j+m>m;\\
c_{\left(  i+n\right)  -n,j+m}^{\prime}, & \text{if }i+n>n\text{ and }j+m\leq
m;\\
d_{\left(  i+n\right)  -n,\left(  j+m\right)  -m}^{\prime}, & \text{if
}i+n>n\text{ and }j+m>m
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{pf.lem.sol.block2x2.jacobi.equal.3}) (applied}\\
\text{to }\left(  i+n,j+m\right)  \text{ instead of }\left(  i,j\right)
\text{)}%
\end{array}
\right) \\
&  =d_{\left(  i+n\right)  -n,\left(  j+m\right)  -m}^{\prime}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i+n>n\text{ and }j+m>m\right)  .
\end{align*}
This rewrites as $d_{i,j}=d_{i,j}^{\prime}$ (since $\left(  i+n\right)  -n=i$
and $\left(  j+m\right)  -m=j$). Qed.}. In other words, $\left(
d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}=\left(
d_{i,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}}$.
Now,%
\[
D=\left(  d_{i,j}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq m^{\prime}%
}=\left(  d_{i,j}^{\prime}\right)  _{1\leq i\leq n^{\prime},\ 1\leq j\leq
m^{\prime}}=D^{\prime}.
\]


We now have shown that $A=A^{\prime}$, $B=B^{\prime}$, $C=C^{\prime}$ and
$D=D^{\prime}$. This proves Lemma \ref{lem.sol.block2x2.jacobi.equal}.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Exercise \ref{exe.block2x2.jacobi}.]The matrix $\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  $ is the inverse of the matrix $\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  $. In other words, we have%
\begin{align*}
\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)   &  =I_{n+m}\ \ \ \ \ \ \ \ \ \ \text{and}\\
\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)   &  =I_{n+m}.
\end{align*}
But Lemma \ref{lem.sol.block2x2.jacobi.InIm} yields
\[
\left(
\begin{array}
[c]{cc}%
I_{n} & 0_{n\times m}\\
0_{m\times n} & I_{m}%
\end{array}
\right)  =I_{n+m}=\left(
\begin{array}
[c]{cc}%
A^{\prime} & B^{\prime}\\
C^{\prime} & D^{\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
A^{\prime}A+B^{\prime}C & A^{\prime}B+B^{\prime}D\\
C^{\prime}A+D^{\prime}C & C^{\prime}B+D^{\prime}D
\end{array}
\right)
\]
(by Exercise \ref{exe.block2x2.mult} (applied to $n$, $m$, $n$, $m$, $n$, $m$,
$A^{\prime}$, $B^{\prime}$, $C^{\prime}$, $D^{\prime}$, $A$, $B$, $C$ and $D$
instead of $n$, $n^{\prime}$, $m$, $m^{\prime}$, $\ell$, $\ell^{\prime}$, $A$,
$B$, $C$, $D$, $A^{\prime}$, $B^{\prime}$, $C^{\prime}$ and $D^{\prime}$)).
Thus, Lemma \ref{lem.sol.block2x2.jacobi.equal} (applied to $n$, $m$, $n$,
$m$, $I_{n}$, $A^{\prime}A+B^{\prime}C$, $0_{n\times m}$, $A^{\prime
}B+B^{\prime}D$, $0_{m\times n}$, $C^{\prime}A+D^{\prime}C$, $I_{m}$ and
$C^{\prime}B+D^{\prime}D$ instead of $n$, $n^{\prime}$, $m$, $m^{\prime}$,
$A$, $A^{\prime}$, $B$, $B^{\prime}$, $C$, $C^{\prime}$, $D$ and $D^{\prime}$)
yields that $I_{n}=A^{\prime}A+B^{\prime}C$, $0_{n\times m}=A^{\prime
}B+B^{\prime}D$, $0_{m\times n}=C^{\prime}A+D^{\prime}C$ and $I_{m}=C^{\prime
}B+D^{\prime}D$.

Recall that $\det\left(  I_{n}\right)  =1$. The same argument (applied to $m$
instead of $n$) yields $\det\left(  I_{m}\right)  =1$. But from $I_{m}%
=C^{\prime}B+D^{\prime}D$, we obtain $C^{\prime}B+D^{\prime}D=I_{m}$ and thus%
\[
\det\underbrace{\left(  C^{\prime}B+D^{\prime}D\right)  }_{=I_{m}}=\det\left(
I_{m}\right)  =1.
\]


From $0_{m\times n}=C^{\prime}A+D^{\prime}C$, we obtain $C^{\prime
}A=-D^{\prime}C$. Hence, Exercise \ref{exe.block2x2.VB+WD} (applied to
$W=D^{\prime}$ and $V=C^{\prime}$) yields
\[
\det\left(  D^{\prime}\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\det A\cdot\underbrace{\det\left(  C^{\prime}B+D^{\prime}D\right)
}_{=1}=\det A.
\]
Thus,%
\[
\det A=\det\left(  D^{\prime}\right)  \cdot\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  =\det\left(
\begin{array}
[c]{cc}%
A & B\\
C & D
\end{array}
\right)  \cdot\det\left(  D^{\prime}\right)  .
\]
This solves Exercise \ref{exe.block2x2.jacobi}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.block2x2.jacobi.rewr}}

Our solution to Exercise \ref{exe.block2x2.jacobi.rewr} below will use the
following simple lemma:

\begin{lemma}
\label{lem.sol.block2x2.jacobi.rewr.1}Let $n\in\mathbb{N}$ and $m\in
\mathbb{N}$. Let $A\in\mathbb{K}^{n\times m}$. Let $k\in\left\{
0,1,\ldots,n\right\}  $ and $\ell\in\left\{  0,1,\ldots,m\right\}  $. Then,%
\[
A=\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell+1,\ell+2,\ldots,m}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots,m}A
\end{array}
\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.block2x2.jacobi.rewr.1}.]In visual language,
Lemma \ref{lem.sol.block2x2.jacobi.rewr.1} states a triviality: It says that
if we cut the matrix $A$ horizontally (between its $k$-th and $\left(
k+1\right)  $-st rows) and vertically (between its $\ell$-th and $\left(
\ell+1\right)  $-st columns), then we obtain four little matrices (namely,
$\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell}A$,
$\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell+1,\ell+2,\ldots,m}A$,
$\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,\ell}A$ and
$\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots,m}A$)
which can be assembled back to form $A$ (using the block-matrix construction).
Turning this into a formal is straightforward.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.block2x2.jacobi.rewr.1}.]Write the matrix
$A\in\mathbb{K}^{n\times m}$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$.

We have $k\in\left\{  0,1,\ldots,n\right\}  $, thus $k\in\mathbb{N}$ and
$k\leq n$. From $k\leq n$, we obtain $n-k\geq0$ and thus $n-k\in\mathbb{N}$.

We have $\ell\in\left\{  0,1,\ldots,m\right\}  $, thus $\ell\in\mathbb{N}$ and
$\ell\leq m$. From $\ell\leq m$, we obtain $m-\ell\geq0$ and thus $m-\ell
\in\mathbb{N}$.

The definition of $\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots
,\ell}A$ yields $\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell
}A=\left(  a_{i,j}\right)  _{1\leq i\leq k,\ 1\leq j\leq\ell}$ (since
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$).

The definition of $\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell
+1,\ell+2,\ldots,m}A$ yields $\operatorname*{sub}\nolimits_{1,2,\ldots
,k}^{\ell+1,\ell+2,\ldots,m}A=\left(  a_{i,\ell+j}\right)  _{1\leq i\leq
k,\ 1\leq j\leq m-\ell}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$).

The definition of $\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{1,2,\ldots,\ell}A$ yields $\operatorname*{sub}\nolimits_{k+1,k+2,\ldots
,n}^{1,2,\ldots,\ell}A=\left(  a_{k+i,j}\right)  _{1\leq i\leq n-k,\ 1\leq
j\leq\ell}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}$).

The definition of $\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{\ell+1,\ell+2,\ldots,m}A$ yields $\operatorname*{sub}%
\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots,m}A=\left(  a_{k+i,\ell
+j}\right)  _{1\leq i\leq n-k,\ 1\leq j\leq m-\ell}$ (since $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$).

We have $\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell
}A=\left(  a_{i,j}\right)  _{1\leq i\leq k,\ 1\leq j\leq\ell}$,
$\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell+1,\ell+2,\ldots,m}A=\left(
a_{i,\ell+j}\right)  _{1\leq i\leq k,\ 1\leq j\leq m-\ell}$,
$\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,\ell}A=\left(
a_{k+i,j}\right)  _{1\leq i\leq n-k,\ 1\leq j\leq\ell}$ and
$\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots
,m}A=\left(  a_{k+i,\ell+j}\right)  _{1\leq i\leq n-k,\ 1\leq j\leq m-\ell}$.
Hence, the definition of $\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell+1,\ell+2,\ldots,m}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots,m}A
\end{array}
\right)  $ yields%
\begin{align}
&  \left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell+1,\ell+2,\ldots,m}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots,m}A
\end{array}
\right) \nonumber\\
&  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq k\text{ and }j\leq\ell;\\
a_{i,\ell+\left(  j-\ell\right)  }, & \text{if }i\leq k\text{ and }j>\ell;\\
a_{k+\left(  i-k\right)  ,j}, & \text{if }i>k\text{ and }j\leq\ell;\\
a_{k+\left(  i-k\right)  ,\ell+\left(  j-\ell\right)  }, & \text{if }i>k\text{
and }j>\ell
\end{cases}
\right)  _{1\leq i\leq k+\left(  n-k\right)  ,\ 1\leq j\leq\ell+\left(
m-\ell\right)  }\nonumber\\
&  =\left(
\begin{cases}
a_{i,j}, & \text{if }i\leq k\text{ and }j\leq\ell;\\
a_{i,\ell+\left(  j-\ell\right)  }, & \text{if }i\leq k\text{ and }j>\ell;\\
a_{k+\left(  i-k\right)  ,j}, & \text{if }i>k\text{ and }j\leq\ell;\\
a_{k+\left(  i-k\right)  ,\ell+\left(  j-\ell\right)  }, & \text{if }i>k\text{
and }j>\ell
\end{cases}
\right)  _{1\leq i\leq n,\ 1\leq j\leq m}
\label{pf.lem.sol.block2x2.jacobi.rewr.1.1}%
\end{align}
(since $k+\left(  n-k\right)  =n$ and $\ell+\left(  m-\ell\right)  =m$).

But every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}
\times\left\{  1,2,\ldots,m\right\}  $ satisfies%
\begin{align}
&
\begin{cases}
a_{i,j}, & \text{if }i\leq k\text{ and }j\leq\ell;\\
a_{i,\ell+\left(  j-\ell\right)  }, & \text{if }i\leq k\text{ and }j>\ell;\\
a_{k+\left(  i-k\right)  ,j}, & \text{if }i>k\text{ and }j\leq\ell;\\
a_{k+\left(  i-k\right)  ,\ell+\left(  j-\ell\right)  }, & \text{if }i>k\text{
and }j>\ell
\end{cases}
\nonumber\\
&  =%
\begin{cases}
a_{i,j}, & \text{if }i\leq k\text{ and }j\leq\ell;\\
a_{i,j}, & \text{if }i\leq k\text{ and }j>\ell;\\
a_{k+\left(  i-k\right)  ,j}, & \text{if }i>k\text{ and }j\leq\ell;\\
a_{k+\left(  i-k\right)  ,j}, & \text{if }i>k\text{ and }j>\ell
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\ell+\left(  j-\ell\right)  =j\right)
\nonumber\\
&  =%
\begin{cases}
a_{i,j}, & \text{if }i\leq k\text{ and }j\leq\ell;\\
a_{i,j}, & \text{if }i\leq k\text{ and }j>\ell;\\
a_{i,j}, & \text{if }i>k\text{ and }j\leq\ell;\\
a_{i,j}, & \text{if }i>k\text{ and }j>\ell
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k+\left(  i-k\right)  =i\right)
\nonumber\\
&  =a_{i,j}. \label{pf.lem.sol.block2x2.jacobi.rewr.1.2}%
\end{align}
Now, (\ref{pf.lem.sol.block2x2.jacobi.rewr.1.1}) becomes%
\begin{align*}
&  \left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{\ell+1,\ell+2,\ldots,m}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,\ell}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{\ell+1,\ell+2,\ldots,m}A
\end{array}
\right) \\
&  =\left(  \underbrace{%
\begin{cases}
a_{i,j}, & \text{if }i\leq k\text{ and }j\leq\ell;\\
a_{i,\ell+\left(  j-\ell\right)  }, & \text{if }i\leq k\text{ and }j>\ell;\\
a_{k+\left(  i-k\right)  ,j}, & \text{if }i>k\text{ and }j\leq\ell;\\
a_{k+\left(  i-k\right)  ,\ell+\left(  j-\ell\right)  }, & \text{if }i>k\text{
and }j>\ell
\end{cases}
}_{\substack{=a_{i,j}\\\text{(by (\ref{pf.lem.sol.block2x2.jacobi.rewr.1.2}%
))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\\
&  =\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=A.
\end{align*}
This proves Lemma \ref{lem.sol.block2x2.jacobi.rewr.1}.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Exercise \ref{exe.block2x2.jacobi.rewr}.]Lemma
\ref{lem.sol.block2x2.jacobi.rewr.1} (applied to $m=n$ and $\ell=k$) yields%
\begin{equation}
A=\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots,n}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,k}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}A
\end{array}
\right)  . \label{sol.exe.block2x2.jacobi.rewr.A=}%
\end{equation}
But the matrix $A\in\mathbb{K}^{n\times n}$ is invertible. Its inverse is
$A^{-1}\in\mathbb{K}^{n\times n}$. Lemma \ref{lem.sol.block2x2.jacobi.rewr.1}
(applied to $n$, $k$ and $A^{-1}$ instead of $m$, $\ell$ and $A$) yields%
\begin{equation}
A^{-1}=\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}\left(
A^{-1}\right)  & \operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots
,n}\left(  A^{-1}\right) \\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,k}\left(
A^{-1}\right)  & \operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{k+1,k+2,\ldots,n}\left(  A^{-1}\right)
\end{array}
\right)  . \label{sol.exe.block2x2.jacobi.rewr.A-1=}%
\end{equation}


\begin{verlong}
We have $k\in\left\{  0,1,\ldots,n\right\}  $, thus $k\in\mathbb{N}$ and
$k\leq n$. From $k\leq n$, we obtain $n-k\geq0$ and thus $n-k\in\mathbb{N}$.
\end{verlong}

Now, recall that the matrix $A$ is invertible, and its inverse is $A^{-1}$. In
view of the equalities (\ref{sol.exe.block2x2.jacobi.rewr.A=}) and
(\ref{sol.exe.block2x2.jacobi.rewr.A-1=}), this rewrites as follows: The
matrix \newline$\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots,n}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,k}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}A
\end{array}
\right)  $ is invertible, and its inverse is \newline$\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}\left(
A^{-1}\right)  & \operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots
,n}\left(  A^{-1}\right) \\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,k}\left(
A^{-1}\right)  & \operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{k+1,k+2,\ldots,n}\left(  A^{-1}\right)
\end{array}
\right)  $. Hence, Exercise \ref{exe.block2x2.jacobi} (applied to $k$, $n-k$,
$\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}A$,
$\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots,n}A$,
$\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,k}A$,
$\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}A$,
$\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}\left(
A^{-1}\right)  $, $\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots
,n}\left(  A^{-1}\right)  $, $\operatorname*{sub}\nolimits_{k+1,k+2,\ldots
,n}^{1,2,\ldots,k}\left(  A^{-1}\right)  $ and $\operatorname*{sub}%
\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}\left(  A^{-1}\right)  $
instead of $n$, $m$, $A$, $B$, $C$, $D$, $A^{\prime}$, $B^{\prime}$,
$C^{\prime}$ and $D^{\prime}$) yields%
\begin{align*}
&  \det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots
,k}A\right) \\
&  =\det\underbrace{\left(
\begin{array}
[c]{cc}%
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}A &
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{k+1,k+2,\ldots,n}A\\
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{1,2,\ldots,k}A &
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}A
\end{array}
\right)  }_{\substack{=A\\\text{(by (\ref{sol.exe.block2x2.jacobi.rewr.A=}))}%
}}\cdot\det\left(  \operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{k+1,k+2,\ldots,n}\left(  A^{-1}\right)  \right) \\
&  =\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{k+1,k+2,\ldots
,n}^{k+1,k+2,\ldots,n}\left(  A^{-1}\right)  \right)  .
\end{align*}
This solves Exercise \ref{exe.block2x2.jacobi.rewr}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.unrows.basics}}

\begin{vershort}
\begin{proof}
[Solution to Exercise \ref{exe.unrows.basics}.]\textbf{(a)} All claims of
Proposition \ref{prop.unrows.basics} and Proposition
\ref{prop.unrows.basics-I} are \textquotedblleft clear by
inspection\textquotedblright, in the sense that the reader will have no
difficulty convincing themselves of their validity just by studying an example
and seeing \textquotedblleft what is going on\textquotedblright. Let me give
some more detailed proofs. (The following proofs are not very formal; but I
will outline a formal proof of Proposition \ref{prop.unrows.basics}
\textbf{(h)} in a footnote, and I trust that the reader can use the same
methods to formally prove the rest of Proposition \ref{prop.unrows.basics} if
so inclined.)

\begin{proof}
[Proof of Proposition \ref{prop.unrows.basics}.]Proposition
\ref{prop.unrows.basics} \textbf{(a)} follows from the definitions of
$A_{u,\bullet}$ and of $\operatorname*{rows}\nolimits_{u}A$ (indeed, these
definitions show that both $A_{u,\bullet}$ and $\operatorname*{rows}%
\nolimits_{u}A$ are the $u$-th row of $A$). Similarly, Proposition
\ref{prop.unrows.basics} \textbf{(b)} follows from the definitions of
$A_{\bullet,v}$ and $\operatorname*{cols}\nolimits_{v}A$.

Proposition \ref{prop.unrows.basics} \textbf{(c)} is obvious\footnote{In fact:
\par
\begin{itemize}
\item The matrix $\left(  A_{\bullet,\sim v}\right)  _{\sim u,\bullet}$ is the
matrix obtained from $A$ by first removing the $v$-th column and then removing
the $u$-th row.
\par
\item The matrix $\left(  A_{\sim u,\bullet}\right)  _{\bullet,\sim v}$ is the
matrix obtained from $A$ by first removing the $u$-th row and then removing
the $v$-th column.
\par
\item The matrix $A_{\sim u,\sim v}$ is the matrix obtained from $A$ by
removing the $u$-th row and the $v$-th column at the same time.
\end{itemize}
\par
Thus it is clear that all three of these matrices are equal.}.

Proposition \ref{prop.unrows.basics} \textbf{(d)} claims that the $w$-th
column of the matrix $A_{\bullet,\sim v}$ equals the $w$-th column of the
matrix $A$ (whenever $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
1,2,\ldots,v-1\right\}  $). Let us prove this: Let $v\in\left\{
1,2,\ldots,m\right\}  $ and $w\in\left\{  1,2,\ldots,v-1\right\}  $. Thus,
$w<v$ (since $w\in\left\{  1,2,\ldots,v-1\right\}  $). But the matrix
$A_{\bullet,\sim v}$ results from the matrix $A$ by removing the $v$-th
column; clearly, this removal does not change the $w$-th column (because
$w<v$). Thus, the $w$-th column of the matrix $A_{\bullet,\sim v}$ equals the
$w$-th column of the matrix $A$. So Proposition \ref{prop.unrows.basics}
\textbf{(d)} is proven.\footnote{If you found this proof insufficiently
rigorous, let me show a formal proof of Proposition \ref{prop.unrows.basics}
\textbf{(d)}. First, I shall introduce a notation:
\par
\begin{itemize}
\item For every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}%
_{r}\left(  j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.
\end{itemize}
\par
We make the following observation:
\par
\textit{Observation 1:} If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, if
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in\mathbb{K}%
^{n\times m}$ is an $n\times m$-matrix, and if $v$ is an element of $\left\{
1,2,\ldots,m\right\}  $, then $A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}%
_{v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq m-1}$.
\par
\textit{Proof of Observation 1:} Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in
\mathbb{K}^{n\times m}$ be an $n\times m$-matrix. Let $v$ be an element of
$\left\{  1,2,\ldots,m\right\}  $. Recalling how $\mathbf{d}_{v}\left(
j\right)  $ is defined for every $j\in\mathbb{Z}$, we see that%
\begin{align*}
\left(  \mathbf{d}_{v}\left(  1\right)  ,\mathbf{d}_{v}\left(  2\right)
,\ldots,\mathbf{d}_{v}\left(  m-1\right)  \right)   &  =\left(  1,2,\ldots
,v-1,v+1,v+2,\ldots,m\right) \\
&  =\left(  1,2,\ldots,\widehat{v},\ldots,m\right)  .
\end{align*}
But the definition of $A_{\bullet,\sim v}$ yields
\begin{align*}
A_{\bullet,\sim v}  &  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,m}A=\operatorname*{cols}\nolimits_{\mathbf{d}_{v}\left(
1\right)  ,\mathbf{d}_{v}\left(  2\right)  ,\ldots,\mathbf{d}_{v}\left(
m-1\right)  }A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{v}%
,\ldots,m\right)  =\left(  \mathbf{d}_{v}\left(  1\right)  ,\mathbf{d}%
_{v}\left(  2\right)  ,\ldots,\mathbf{d}_{v}\left(  m-1\right)  \right)
\right) \\
&  =\left(  a_{i,\mathbf{d}_{v}\left(  y\right)  }\right)  _{1\leq i\leq
n,\ 1\leq y\leq m-1}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{cols}\nolimits_{\mathbf{d}%
_{v}\left(  1\right)  ,\mathbf{d}_{v}\left(  2\right)  ,\ldots,\mathbf{d}%
_{v}\left(  n-1\right)  }A\\
\text{(since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed
the index }\left(  i,y\right)  \text{ as }\left(  i,j\right)  \right)  .
\end{align*}
This proves Observation 1.
\par
Let us now prove Proposition \ref{prop.unrows.basics} \textbf{(d)}. Indeed,
let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{  1,2,\ldots
,v-1\right\}  $. Then, $w<v$ (since $w\in\left\{  1,2,\ldots,v-1\right\}  $).
Write the $n\times m$-matrix $A$ as $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. The definition of $\mathbf{d}_{v}\left(  w\right)  $ now
yields $\mathbf{d}_{v}\left(  w\right)  =%
\begin{cases}
w, & \text{if }w<v;\\
w+1, & \text{if }w\geq v
\end{cases}
=w$ (since $w<v$). Now, the definition of $\left(  A_{\bullet,\sim v}\right)
_{\bullet,w}$ yields%
\begin{align*}
\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}  &  =\left(  \text{the
}w\text{-th column of the matrix }A_{\bullet,\sim v}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,\mathbf{d}_{v}\left(  w\right)  }\\
a_{2,\mathbf{d}_{v}\left(  w\right)  }\\
\vdots\\
a_{n,\mathbf{d}_{v}\left(  w\right)  }%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\bullet,\sim v}=\left(
a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m-1}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,w}\\
a_{2,w}\\
\vdots\\
a_{n,w}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{v}\left(
w\right)  =w\right) \\
&  =\left(  \text{the }w\text{-th column of the matrix }A\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right) \\
&  =A_{\bullet,w}.
\end{align*}
This proves Proposition \ref{prop.unrows.basics} \textbf{(d)}.}

Proposition \ref{prop.unrows.basics} \textbf{(e)} claims that the $w$-th
column of the matrix $A_{\bullet,\sim v}$ equals the $\left(  w+1\right)  $-th
column of the matrix $A$ (whenever $v\in\left\{  1,2,\ldots,m\right\}  $ and
$w\in\left\{  v,v+1,\ldots,m-1\right\}  $). Let us prove this:

Let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{  v,v+1,\ldots
,m-1\right\}  $. Thus, $w+1>w\geq v$ (since $w\in\left\{  v,v+1,\ldots
,m-1\right\}  $). But the matrix $A_{\bullet,\sim v}$ results from the matrix
$A$ by removing the $v$-th column; this removal moves the $\left(  w+1\right)
$-th column of $A$ one step leftwards (since $w+1>v$). In other words, the
$\left(  w+1\right)  $-th column of $A$ becomes the $w$-th column of the new
matrix $A_{\bullet,\sim v}$. In other words, the $w$-th column of the matrix
$A_{\bullet,\sim v}$ equals the $\left(  w+1\right)  $-th column of the matrix
$A$. So Proposition \ref{prop.unrows.basics} \textbf{(e)} is
proven.\footnote{Again, a more rigorous proof of Proposition
\ref{prop.unrows.basics} \textbf{(e)} can be obtained similarly to our
rigorous proof of Proposition \ref{prop.unrows.basics} \textbf{(d)} in the
previous footnote. (The main difference is that we have $\mathbf{d}_{v}\left(
w\right)  =w+1$ instead of $\mathbf{d}_{v}\left(  w\right)  =w$ this time.)}

Proposition \ref{prop.unrows.basics} \textbf{(f)} is the analogue of
Proposition \ref{prop.unrows.basics} \textbf{(d)} for rows instead of columns
(with $v$ renamed as $u$); its proof is equally analogous.

Proposition \ref{prop.unrows.basics} \textbf{(g)} is the analogue of
Proposition \ref{prop.unrows.basics} \textbf{(e)} for rows instead of columns
(with $v$ renamed as $u$); its proof is equally analogous.

Let us now prove Proposition \ref{prop.unrows.basics} \textbf{(h)}: Let
$v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{  1,2,\ldots
,v-1\right\}  $. We need to show that $\left(  A_{\bullet,\sim v}\right)
_{\bullet,\sim w}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{w}%
,\ldots,\widehat{v},\ldots,m}A$. The matrix $\left(  A_{\bullet,\sim
v}\right)  _{\bullet,\sim w}$ is obtained from the matrix $A$ by first
removing the $v$-th column (thus obtaining an $n\times\left(  m-1\right)
$-matrix) and then removing the $w$-th column (from the resulting
$n\times\left(  m-1\right)  $-matrix). Since $w<v$ (because $w\in\left\{
1,2,\ldots,v-1\right\}  $), the removal of the $v$-th column of $A$ did not
affect the $w$-th column, and therefore the two successive removals could be
replaced by a simultaneous removal of both the $w$-th and the $v$-th columns
from the matrix $A$. In other words, the matrix obtained from the matrix $A$
by first removing the $v$-th column and then removing the $w$-th column is
identical with the matrix obtained from $A$ by simultaneously removing both
the $w$-th and the $v$-th columns. But the former matrix is $\left(
A_{\bullet,\sim v}\right)  _{\bullet,\sim w}$, whereas the latter matrix is
$\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v}%
,\ldots,m}A$. Hence, the identity of these two matrices rewrites as $\left(
A_{\bullet,\sim v}\right)  _{\bullet,\sim w}=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v},\ldots,m}A$. This proves
Proposition \ref{prop.unrows.basics} \textbf{(h)}.\footnote{If you found this
proof insufficiently rigorous, let me show a formal proof of Proposition
\ref{prop.unrows.basics} \textbf{(h)}. First, I shall introduce two notations:
\par
\begin{itemize}
\item For every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}%
_{r}\left(  j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.
\par
\item For every $j\in\mathbb{Z}$, $r\in\mathbb{Z}$ and $s\in\mathbb{Z}$
satisfying $r<s$, we let $\mathbf{d}_{r,s}\left(  j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
$. (This is well-defined, because $r\leq s-1$ (since $r<s$ and since $r$ and
$s$ are integers).)
\end{itemize}
\par
We make the following three observations:
\par
\textit{Observation 1:} If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, if
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in\mathbb{K}%
^{n\times m}$ is an $n\times m$-matrix, and if $v$ is an element of $\left\{
1,2,\ldots,m\right\}  $, then $A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}%
_{v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq m-1}$.
\par
\textit{Observation 2:} If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, if
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in\mathbb{K}%
^{n\times m}$ is an $n\times m$-matrix, and if $w$ and $v$ are two elements of
$\left\{  1,2,\ldots,m\right\}  $ satisfying $w<v$, then $\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v},\ldots,m}A=\left(
a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m-2}$.
\par
\textit{Observation 3:} For every two integers $w$ and $v$ satisfying $w<v$,
we have
\[
\mathbf{d}_{w,v}\left(  j\right)  =\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(
j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for each }j\in\mathbb{Z}.
\]
\par
\textit{Proof of Observation 1:} Observation 1 was already proven in a
previous footnote (namely, the one where we gave a rigorous proof of
Proposition \ref{prop.unrows.basics} \textbf{(d)}).
\par
\textit{Proof of Observation 2:} Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in
\mathbb{K}^{n\times m}$ be an $n\times m$-matrix. Let $w$ and $v$ be two
elements of $\left\{  1,2,\ldots,m\right\}  $ satisfying $w<v$. Recalling how
$\mathbf{d}_{w,v}\left(  j\right)  $ is defined for every $j\in\mathbb{Z}$, we
find that%
\begin{align*}
&  \left(  \mathbf{d}_{w,v}\left(  1\right)  ,\mathbf{d}_{w,v}\left(
2\right)  ,\ldots,\mathbf{d}_{w,v}\left(  m-2\right)  \right) \\
&  =\left(  1,2,\ldots,w-1,w+1,w+2,\ldots,v-1,v+1,v+2,\ldots,m\right) \\
&  =\left(  1,2,\ldots,\widehat{w},\ldots,\widehat{v},\ldots,m\right)  .
\end{align*}
Now,%
\begin{align*}
&  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v}%
,\ldots,m}A\\
&  =\operatorname*{cols}\nolimits_{\mathbf{d}_{w,v}\left(  1\right)
,\mathbf{d}_{w,v}\left(  2\right)  ,\ldots,\mathbf{d}_{w,v}\left(  m-2\right)
}A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{w}%
,\ldots,\widehat{v},\ldots,m\right)  =\left(  \mathbf{d}_{w,v}\left(
1\right)  ,\mathbf{d}_{w,v}\left(  2\right)  ,\ldots,\mathbf{d}_{w,v}\left(
m-2\right)  \right)  \right) \\
&  =\left(  a_{i,\mathbf{d}_{w,v}\left(  y\right)  }\right)  _{1\leq i\leq
n,\ 1\leq y\leq m-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{cols}\nolimits_{\mathbf{d}%
_{w,v}\left(  1\right)  ,\mathbf{d}_{w,v}\left(  2\right)  ,\ldots
,\mathbf{d}_{w,v}\left(  m-2\right)  }A\\
\text{(since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed
the index }\left(  i,y\right)  \text{ as }\left(  i,j\right)  \right)  .
\end{align*}
This proves Observation 2.
\par
\textit{Proof of Observation 3:} Let $w$ and $v$ be two integers satisfying
$w<v$. Then, $w\leq v-1$ (since $w$ and $v$ are integers). For every
$j\in\mathbb{Z}$, we can prove $\mathbf{d}_{w,v}\left(  j\right)
=\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)  $ by an
explicit computation using the definitions of $\mathbf{d}_{w,v}\left(
j\right)  $, of $\mathbf{d}_{w}\left(  j\right)  $ and of $\mathbf{d}%
_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)  $. (The cases when $j<w$,
when $w\leq j<v-1$ and when $v-1\leq j$ need to be treated separately; but
each of these three cases is completely straightforward. For instance, in the
case when $v-1\leq j$, the definition of $\mathbf{d}_{w,v}\left(  j\right)  $
yields%
\[
\mathbf{d}_{w,v}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<w;\\
j+1, & \text{if }w\leq j<v-1;\\
j+2, & \text{if }v-1\leq j
\end{cases}
=j+2\ \ \ \ \ \ \ \ \ \ \left(  \text{since }v-1\leq j\right)  ,
\]
whereas the definition of $\mathbf{d}_{w}\left(  j\right)  $ yields%
\[
\mathbf{d}_{w}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<w;\\
j+1, & \text{if }j\geq w
\end{cases}
=j+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j\geq v-1\geq w\right)
\]
and therefore%
\begin{align*}
\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)   &
=\mathbf{d}_{v}\left(  j+1\right)  =%
\begin{cases}
j+1, & \text{if }j+1<v;\\
\left(  j+1\right)  +1, & \text{if }j+1\geq v
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}_{v}\left(
j+1\right)  \right) \\
&  =\left(  j+1\right)  +1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1\geq
v\text{ (since }j\geq v-1\text{)}\right) \\
&  =j+2=\mathbf{d}_{w,v}\left(  j\right)  ;
\end{align*}
thus, $\mathbf{d}_{w,v}\left(  j\right)  =\mathbf{d}_{v}\left(  \mathbf{d}%
_{w}\left(  j\right)  \right)  $ is proven in this case.) This proves
Observation 3.
\par
Let us now prove Proposition \ref{prop.unrows.basics} \textbf{(h)}. Indeed,
let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{  1,2,\ldots
,v-1\right\}  $. Then, $w\leq v-1$ (since $w\in\left\{  1,2,\ldots
,v-1\right\}  $) and thus $w\leq\underbrace{v}_{\leq m}-1\leq m-1$, so that
$v\in\left\{  1,2,\ldots,m-1\right\}  $.
\par
Write the matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Then, Observation 1 shows that $A_{\bullet,\sim
v}=\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-1}$. Hence, Observation 1 (applied to $m-1$, $A_{\bullet
,\sim v}$, $a_{i,\mathbf{d}_{v}\left(  j\right)  }$ and $w$ instead of $m$,
$A$, $a_{i,j}$ and $v$) yields%
\begin{align*}
\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}  &  =\left(
a_{i,\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq\left(  m-1\right)  -1}=\left(
\underbrace{a_{i,\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)
\right)  }}_{\substack{=a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\\\text{(since
Observation 3 yields}\\\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)
\right)  =\mathbf{d}_{w,v}\left(  j\right)  \text{)}}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}\\
&  =\left(  a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{w}%
,\ldots,\widehat{v},\ldots,m}A
\end{align*}
(by Observation 2). This proves Proposition \ref{prop.unrows.basics}
\textbf{(h)}.}

Similarly, we can prove Proposition \ref{prop.unrows.basics} \textbf{(i)}%
\footnote{We will not actually need Proposition \ref{prop.unrows.basics}
\textbf{(i)} in the following, so you can just as well skip this proof. The
same applies to Proposition \ref{prop.unrows.basics} \textbf{(k)}.}: Let
$v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{  v,v+1,\ldots
,m-1\right\}  $. We need to show that $\left(  A_{\bullet,\sim v}\right)
_{\bullet,\sim w}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v}%
,\ldots,\widehat{w+1},\ldots,m}A$. The matrix $\left(  A_{\bullet,\sim
v}\right)  _{\bullet,\sim w}$ is obtained from the matrix $A$ by first
removing the $v$-th column (thus obtaining an $n\times\left(  m-1\right)
$-matrix) and then removing the $w$-th column (from the resulting
$n\times\left(  m-1\right)  $-matrix). Since $w\geq v$ (because $w\in\left\{
v,v+1,\ldots,m-1\right\}  $), the first of these two removals (i.e., the
removal of the $v$-th column of $A$) has moved the $\left(  w+1\right)  $-th
column of $A$ one step to the left; therefore, the latter column has become
the $w$-th column after this first removal. The second removal then removes
this column. Therefore, the two successive removals could be replaced by a
simultaneous removal of both the $v$-th and the $\left(  w+1\right)  $-th
columns from the matrix $A$. In other words, the matrix obtained from the
matrix $A$ by first removing the $v$-th column and then removing the $w$-th
column is identical with the matrix obtained from $A$ by simultaneously
removing both the $v$-th and the $\left(  w+1\right)  $-th columns. But the
former matrix is $\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}$,
whereas the latter matrix is $\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,\widehat{w+1},\ldots,m}A$. Hence, the identity of these
two matrices rewrites as $\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim
w}=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w+1}%
,\ldots,m}A$. This proves Proposition \ref{prop.unrows.basics} \textbf{(i)}%
.\footnote{Again, a more rigorous proof can be given (if desired) similarly to
our rigorous proof of Proposition \ref{prop.unrows.basics} \textbf{(h)}.}

Proposition \ref{prop.unrows.basics} \textbf{(j)} is the analogue of
Proposition \ref{prop.unrows.basics} \textbf{(h)} for rows instead of columns
(with $v$ renamed as $u$); its proof is equally analogous.

Proposition \ref{prop.unrows.basics} \textbf{(k)} is the analogue of
Proposition \ref{prop.unrows.basics} \textbf{(i)} for rows instead of columns
(with $v$ renamed as $u$); its proof is equally analogous.

Let us now prove Proposition \ref{prop.unrows.basics} \textbf{(l)}: Let
$v\in\left\{  1,2,\ldots,n\right\}  $, $u\in\left\{  1,2,\ldots,n\right\}  $
and $q\in\left\{  1,2,\ldots,m\right\}  $ be such that $u<v$. We have $u<v$
and thus $u\leq v-1$ (since $u$ and $v$ are integers). Combined with $u\geq1$
(since $u\in\left\{  1,2,\ldots,n\right\}  $), this yields $u\in\left\{
1,2,\ldots,v-1\right\}  $.

Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $v$ and $q$
instead of $u$ and $v$) yields $\left(  A_{\bullet,\sim q}\right)  _{\sim
v,\bullet}=\left(  A_{\sim v,\bullet}\right)  _{\bullet,\sim q}=A_{\sim v,\sim
q}$.

Now, $u\leq\underbrace{v}_{\substack{\leq n\\\text{(since }v\in\left\{
1,2,\ldots,n\right\}  \text{)}}}-1\leq n-1$. Combining this with $u\geq1$, we
obtain $u\in\left\{  1,2,\ldots,n-1\right\}  $. Hence, Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to $n-1$, $A_{\sim v,\bullet}$
and $q$ instead of $n$, $A$ and $v$) yields $\left(  \left(  A_{\sim
v,\bullet}\right)  _{\bullet,\sim q}\right)  _{\sim u,\bullet}=\left(  \left(
A_{\sim v,\bullet}\right)  _{\sim u,\bullet}\right)  _{\bullet,\sim q}=\left(
A_{\sim v,\bullet}\right)  _{\sim u,\sim q}$. Hence,%
\begin{align*}
\left(  A_{\sim v,\bullet}\right)  _{\sim u,\sim q}  &  =\left(
\underbrace{\left(  A_{\sim v,\bullet}\right)  _{\bullet,\sim q}}_{=\left(
A_{\bullet,\sim q}\right)  _{\sim v,\bullet}}\right)  _{\sim u,\bullet
}=\left(  \left(  A_{\bullet,\sim q}\right)  _{\sim v,\bullet}\right)  _{\sim
u,\bullet}\\
&  =\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,\widehat{v},\ldots,n}\left(  A_{\bullet,\sim q}\right)
\end{align*}
(by Proposition \ref{prop.unrows.basics} \textbf{(j)}, applied to $m-1$,
$A_{\bullet,\sim q}$, $v$ and $u$ instead of $m$, $A$, $u$ and $w$). This
proves Proposition \ref{prop.unrows.basics} \textbf{(l)}.
\end{proof}

Thus, the proof of Proposition \ref{prop.unrows.basics} is complete. It
remains to prove Proposition \ref{prop.unrows.basics-I}. Let me give an
informal proof:

\begin{proof}
[Proof of Proposition \ref{prop.unrows.basics-I}.]We know that $\left(
I_{n}\right)  _{\bullet,u}$ is the $u$-th column of the identity matrix. All
entries of this column are $0$, except for the $u$-th, which is $1$. In other
words,%
\[
\left(  I_{n}\right)  _{\bullet,u}=\left(  \underbrace{0,0,\ldots
,0}_{u-1\text{ zeroes}},1,\underbrace{0,0,\ldots,0}_{n-u\text{ zeroes}%
}\right)  ^{T}.
\]
Now, $\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}$ is
the result of removing the $v$-th row from this column, i.e., removing the
$v$-th entry from this column\footnote{Indeed, the $v$-th row is the same as
the $v$-th entry in this case, because each row of $\left(  I_{n}\right)
_{\bullet,u}$ consists of one entry only.}. This $v$-th row is below the $1$
in the $u$-th row (since $u<v$); therefore, the result of its removal is the
column vector
\[
\left(  \underbrace{0,0,\ldots,0}_{u-1\text{ zeroes}},1,\underbrace{0,0,\ldots
,0}_{n-u-1\text{ zeroes}}\right)  ^{T}.
\]
Hence, we have shown that%
\begin{align*}
\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}  &
=\left(  \underbrace{0,0,\ldots,0}_{u-1\text{ zeroes}}%
,1,\underbrace{0,0,\ldots,0}_{n-u-1\text{ zeroes}}\right)  ^{T}\\
&  =\left(  \underbrace{0,0,\ldots,0}_{u-1\text{ zeroes}}%
,1,\underbrace{0,0,\ldots,0}_{\left(  n-1\right)  -u\text{ zeroes}}\right)
^{T}\\
&  =\left(  \text{the }u\text{-th column of the matrix }I_{n-1}\right)
=\left(  I_{n-1}\right)  _{\bullet,u}.
\end{align*}
This proves Proposition \ref{prop.unrows.basics-I}\footnote{Again, if this
proof was not rigorous enough for you, here is a more formal proof of
Proposition \ref{prop.unrows.basics-I}:
\par
For every $j\in\mathbb{Z}$ and $r\in\mathbb{Z}$, let $\mathbf{d}_{r}\left(
j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$. For any two objects $i$ and $j$, we define an element $\delta_{i,j}%
\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.
\par
We make the following three observations:
\par
\textit{Observation 1:} If $n\in\mathbb{N}$ and $m\in\mathbb{N}$, if
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in\mathbb{K}%
^{n\times m}$ is an $n\times m$-matrix, and if $v$ is an element of $\left\{
1,2,\ldots,m\right\}  $, then $A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}%
_{v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq m-1}$.
\par
\textit{Observation 2:} If $u\in\mathbb{Z}$, $v\in\mathbb{Z}$ and
$i\in\mathbb{Z}$ satisfy $u<v$, then $\delta_{\mathbf{d}_{v}\left(  i\right)
,u}=\delta_{i,u}$.
\par
\textit{Observation 3:} For every $m\in\mathbb{N}$, we have $I_{m}=\left(
\delta_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}$.
\par
\textit{Proof of Observation 1:} Observation 1 was already proven in a
footnote in our above proof of Proposition \ref{prop.unrows.basics}
\textbf{(h)}.
\par
\textit{Proof of Observation 2:} Let $u\in\mathbb{Z}$, $v\in\mathbb{Z}$ and
$i\in\mathbb{Z}$ satisfy $u<v$. We are in one of the following two Cases:
\par
\textit{Case 1:} We have $i<v$.
\par
\textit{Case 2:} We have $i\geq v$.
\par
Let us first consider Case 1. In this case, we have $i<v$. Now, the definition
of $\mathbf{d}_{v}\left(  i\right)  $ yields $\mathbf{d}_{v}\left(  i\right)
=%
\begin{cases}
i, & \text{if }i<v;\\
i+1, & \text{if }i\geq v
\end{cases}
=i$ (since $i<v$) and thus $\delta_{\mathbf{d}_{v}\left(  i\right)  ,u}%
=\delta_{i,u}$. Hence, Observation 2 is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i\geq v$. Now, the
definition of $\mathbf{d}_{v}\left(  i\right)  $ yields $\mathbf{d}_{v}\left(
i\right)  =%
\begin{cases}
i, & \text{if }i<v;\\
i+1, & \text{if }i\geq v
\end{cases}
=i+1$ (since $i\geq v$) and thus $\mathbf{d}_{v}\left(  i\right)  =i+1>i\geq
v>u$. Hence, $\mathbf{d}_{v}\left(  i\right)  \neq u$. Thus, $\delta
_{\mathbf{d}_{v}\left(  i\right)  ,u}=0$. Also, $i\geq v>u$ and thus $i\neq
u$; hence, $\delta_{i,u}=0$. Now, $\delta_{\mathbf{d}_{v}\left(  i\right)
,u}=0=\delta_{i,u}$. Hence, Observation 2 is proven in Case 2.
\par
Thus, Observation 2 is proven in both Cases 1 and 2; hence, Observation 2 is
always proven.
\par
\textit{Proof of Observation 3:} Observation 3 is just the definition of
$I_{m}$.
\par
Now, let $n$, $u$ and $v$ be as in Proposition \ref{prop.unrows.basics-I}.
Then, Observation 3 (applied to $m=n$) yields $I_{n}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. But Observation 3 (applied
to $m=n-1$) yields $I_{n-1}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq n-1}$.
\par
From $u<v\leq n$, we obtain $u\in\left\{  1,2,\ldots,n-1\right\}  $, so that
$\left(  I_{n-1}\right)  _{\bullet,u}$ is well-defined. Also, $\left(  \left(
I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}$ is well-defined (since
$u$ and $v$ belong to $\left\{  1,2,\ldots,n\right\}  $). Now, the definition
of $\left(  I_{n}\right)  _{\bullet,u}$ yields%
\begin{align*}
\left(  I_{n}\right)  _{\bullet,u}  &  =\left(  \text{the }u\text{-th column
of the matrix }I_{n}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\delta_{1,u}\\
\delta_{2,u}\\
\vdots\\
\delta_{n,u}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I_{n}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \delta_{i,u}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\end{align*}
Hence, Observation 1 (applied to $1$, $\left(  I_{n}\right)  _{\bullet,u}$,
$\delta_{i,u}$ and $v$ instead of $m$, $A$, $a_{i,j}$ and $u$) yields%
\[
\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}=\left(
\underbrace{\delta_{\mathbf{d}_{v}\left(  i\right)  ,u}}_{\substack{=\delta
_{i,u}\\\text{(by Observation 2)}}}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
1}=\left(  \delta_{i,u}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq1}.
\]
Comparing this with%
\begin{align*}
\left(  I_{n-1}\right)  _{\bullet,u}  &  =\left(  \text{the }u\text{-th column
of the matrix }I_{n-1}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the
definition of }\left(  I_{n-1}\right)  _{\bullet,u}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\delta_{1,u}\\
\delta_{2,u}\\
\vdots\\
\delta_{n-1,u}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I_{n-1}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right) \\
&  =\left(  \delta_{i,u}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq1},
\end{align*}
we obtain $\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim
v,\bullet}=\left(  I_{n-1}\right)  _{\bullet,u}$. This proves Proposition
\ref{prop.unrows.basics-I}.}.
\end{proof}

We have now proven both Proposition \ref{prop.unrows.basics} and Proposition
\ref{prop.unrows.basics-I}. Thus, Exercise \ref{exe.unrows.basics}
\textbf{(a)} is solved.

\textbf{(b)} We need to derive Proposition \ref{prop.desnanot.12} and
Proposition \ref{prop.desnanot.1n} from Theorem \ref{thm.desnanot}.

\begin{proof}
[Proof of Proposition \ref{prop.desnanot.12} using Theorem \ref{thm.desnanot}%
.]We have%
\begin{equation}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{2}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n}A=\widetilde{A}
\label{pf.prop.desnanot.12.short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.desnanot.12.short.1}):} The $\left(
n-2\right)  $-tuple $\left(  1,2,\ldots,\widehat{1},\ldots,\widehat{2}%
,\ldots,n\right)  $ is obtained by removing the $1$-st and the $2$-nd entries
from the $n$-tuple $\left(  1,2,\ldots,n\right)  $. Thus,%
\[
\left(  1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n\right)  =\left(
3,4,\ldots,n\right)  =\left(  1+2,2+2,\ldots,\left(  n-2\right)  +2\right)  .
\]
Hence,%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{2}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n}A  &
=\operatorname*{sub}\nolimits_{1+2,2+2,\ldots,\left(  n-2\right)
+2}^{1+2,2+2,\ldots,\left(  n-2\right)  +2}A=\left(  a_{x+2,y+2}\right)
_{1\leq x\leq n-2,\ 1\leq y\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{1+2,2+2,\ldots,\left(  n-2\right)  +2}^{1+2,2+2,\ldots,\left(
n-2\right)  +2}A\right) \\
&  =\left(  a_{i+2,j+2}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right) \\
&  =\widetilde{A}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{A}%
=\left(  a_{i+2,j+2}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\right)  .
\end{align*}
Qed.}. Now, $1<2$. Moreover, $1$ and $2$ are elements of $\left\{
1,2,\ldots,n\right\}  $ (since $n\geq2$). Hence, Theorem \ref{thm.desnanot}
(applied to $p=1$, $q=2$, $u=1$ and $v=2$) yields%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{1},\ldots,\widehat{2},\ldots,n}^{1,2,\ldots,\widehat{1}%
,\ldots,\widehat{2},\ldots,n}A\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim2,\sim
2}\right)  -\det\left(  A_{\sim1,\sim2}\right)  \cdot\det\left(  A_{\sim
2,\sim1}\right)  .
\end{align*}
In view of (\ref{pf.prop.desnanot.12.short.1}), this rewrites as
\begin{align*}
&  \det A\cdot\det\widetilde{A}\\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim2,\sim
2}\right)  -\det\left(  A_{\sim1,\sim2}\right)  \cdot\det\left(  A_{\sim
2,\sim1}\right)  .
\end{align*}
This proves Proposition \ref{prop.desnanot.12}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.desnanot.1n} using Theorem \ref{thm.desnanot}%
.]We have%
\begin{equation}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{n}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n}A=A^{\prime}
\label{pf.prop.desnanot.1n.short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.desnanot.1n.short.1}):} The $\left(
n-2\right)  $-tuple $\left(  1,2,\ldots,\widehat{1},\ldots,\widehat{n}%
,\ldots,n\right)  $ is obtained by removing the $1$-st and the $n$-th entries
from the $n$-tuple $\left(  1,2,\ldots,n\right)  $. Thus,%
\[
\left(  1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n\right)  =\left(
2,3,\ldots,n-1\right)  =\left(  1+1,2+1,\ldots,\left(  n-2\right)  +1\right)
.
\]
Hence,%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{n}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n}A  &
=\operatorname*{sub}\nolimits_{1+1,2+1,\ldots,\left(  n-2\right)
+1}^{1+1,2+1,\ldots,\left(  n-2\right)  +1}A=\left(  a_{x+1,y+1}\right)
_{1\leq x\leq n-2,\ 1\leq y\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{1+1,2+1,\ldots,\left(  n-2\right)  +1}^{1+1,2+1,\ldots,\left(
n-2\right)  +1}A\right) \\
&  =\left(  a_{i+1,j+1}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right) \\
&  =A^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A^{\prime}=\left(
a_{i+1,j+1}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\right)  .
\end{align*}
Qed.}. Now, $1<n$ (since $n\geq2$) and $1<n$. Moreover, $1$ and $n$ are
elements of $\left\{  1,2,\ldots,n\right\}  $ (since $n\geq2\geq1$). Hence,
Theorem \ref{thm.desnanot} (applied to $p=1$, $q=n$, $u=1$ and $v=n$) yields%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{1},\ldots,\widehat{n},\ldots,n}^{1,2,\ldots,\widehat{1}%
,\ldots,\widehat{n},\ldots,n}A\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim n,\sim
n}\right)  -\det\left(  A_{\sim1,\sim n}\right)  \cdot\det\left(  A_{\sim
n,\sim1}\right)  .
\end{align*}
In view of (\ref{pf.prop.desnanot.1n.short.1}), this rewrites as
\begin{align*}
&  \det A\cdot\det\left(  A^{\prime}\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim n,\sim
n}\right)  -\det\left(  A_{\sim1,\sim n}\right)  \cdot\det\left(  A_{\sim
n,\sim1}\right)  .
\end{align*}
This proves Proposition \ref{prop.desnanot.1n}.
\end{proof}

Now, both Proposition \ref{prop.desnanot.12} and Proposition
\ref{prop.desnanot.1n} are proven using Theorem \ref{thm.desnanot}. This
completes the solution of Exercise \ref{exe.unrows.basics} \textbf{(b)}.
\end{proof}
\end{vershort}

\begin{verlong}
In preparation for our solution to Exercise \ref{exe.unrows.basics}, let us
introduce the following notations:

\begin{definition}
\label{def.sol.unrows.d}\textbf{(a)} For every $j\in\mathbb{Z}$ and
$r\in\mathbb{Z}$, let $\mathbf{d}_{r}\left(  j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$.

\textbf{(b)} For every $j\in\mathbb{Z}$, $r\in\mathbb{Z}$ and $s\in\mathbb{Z}$
satisfying $r<s$, we let $\mathbf{d}_{r,s}\left(  j\right)  $ be the integer $%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
$. (This is well-defined, because $r\leq s-1$ (since $r<s$ and since $r$ and
$s$ are integers).)
\end{definition}

Using these notations, we can state three really basic facts:

\begin{proposition}
\label{prop.sol.unirows.r}Let $n\in\mathbb{N}$. Let $r\in\left\{
1,2,\ldots,n\right\}  $. Then, $\left(  \mathbf{d}_{r}\left(  1\right)
,\mathbf{d}_{r}\left(  2\right)  ,\ldots,\mathbf{d}_{r}\left(  n-1\right)
\right)  =\left(  1,2,\ldots,\widehat{r},\ldots,n\right)  $.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sol.unirows.r}.]We have $\mathbf{d}_{r}\left(
j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
$ for each $j\in\left\{  1,2,\ldots,n-1\right\}  $ (by the definition of
$\mathbf{d}_{r}\left(  j\right)  $). From this, we conclude that
\begin{equation}
\left(  \mathbf{d}_{r}\left(  1\right)  ,\mathbf{d}_{r}\left(  2\right)
,\ldots,\mathbf{d}_{r}\left(  n-1\right)  \right)  =\left(  1,2,\ldots
,r-1,r+1,r+2,\ldots,n\right)  \label{pf.prop.sol.unirows.r.1}%
\end{equation}
\footnote{Here is this argument in more detail:
\par
If $\alpha$ and $\beta$ are two finite lists of integers, then we define a new
finite list $\alpha\ast\beta$ of integers by setting%
\[
\alpha\ast\beta=\left(  \alpha_{1},\alpha_{2},\ldots,\alpha_{a},\beta
_{1},\beta_{2},\ldots,\beta_{b}\right)  ,
\]
where $\alpha$ is written in the form $\left(  \alpha_{1},\alpha_{2}%
,\ldots,\alpha_{a}\right)  $, and where $\beta$ is written in the form
$\left(  \beta_{1},\beta_{2},\ldots,\beta_{b}\right)  $. This new list
$\alpha\ast\beta$ is called the \textit{concatenation} of the lists $\alpha$
and $\beta$.
\par
For every $j\in\left\{  1,2,\ldots,r-1\right\}  $, we have%
\begin{align*}
\mathbf{d}_{r}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}_{r}\right)
\\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<r\text{ (because }j\in\left\{
1,2,\ldots,r-1\right\}  \text{)}\right)  .
\end{align*}
In other words, $\left(  \mathbf{d}_{r}\left(  1\right)  ,\mathbf{d}%
_{r}\left(  2\right)  ,\ldots,\mathbf{d}_{r}\left(  r-1\right)  \right)
=\left(  1,2,\ldots,r-1\right)  $.
\par
On the other hand, for every $j\in\left\{  r,r+1,\ldots,n-1\right\}  $, we
have%
\begin{align*}
\mathbf{d}_{r}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}_{r}\right)
\\
&  =j+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j\geq r\text{ (because }%
j\in\left\{  r,r+1,\ldots,n-1\right\}  \text{)}\right)  .
\end{align*}
In other words, $\left(  \mathbf{d}_{r}\left(  r\right)  ,\mathbf{d}%
_{r}\left(  r+1\right)  ,\ldots,\mathbf{d}_{r}\left(  n-1\right)  \right)
=\left(  r+1,\left(  r+1\right)  +1,\ldots,\left(  n-1\right)  +1\right)  $.
Now,%
\begin{align*}
&  \left(  \mathbf{d}_{r}\left(  1\right)  ,\mathbf{d}_{r}\left(  2\right)
,\ldots,\mathbf{d}_{r}\left(  n-1\right)  \right) \\
&  =\left(  \mathbf{d}_{r}\left(  1\right)  ,\mathbf{d}_{r}\left(  2\right)
,\ldots,\mathbf{d}_{r}\left(  r-1\right)  ,\mathbf{d}_{r}\left(  r\right)
,\mathbf{d}_{r}\left(  r+1\right)  ,\ldots,\mathbf{d}_{r}\left(  n-1\right)
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }r\in\left\{  1,2,\ldots,n\right\}
\right) \\
&  =\underbrace{\left(  \mathbf{d}_{r}\left(  1\right)  ,\mathbf{d}_{r}\left(
2\right)  ,\ldots,\mathbf{d}_{r}\left(  r-1\right)  \right)  }_{=\left(
1,2,\ldots,r-1\right)  }\ast\underbrace{\left(  \mathbf{d}_{r}\left(
r\right)  ,\mathbf{d}_{r}\left(  r+1\right)  ,\ldots,\mathbf{d}_{r}\left(
n-1\right)  \right)  }_{\substack{=\left(  r+1,\left(  r+1\right)
+1,\ldots,\left(  n-1\right)  +1\right)  \\=\left(  r+1,r+2,\ldots,n\right)
}}\\
&  =\left(  1,2,\ldots,r-1\right)  \ast\left(  r+1,r+2,\ldots,n\right)
=\left(  1,2,\ldots,r-1,r+1,r+2,\ldots,n\right)
\end{align*}
(since $r\in\left\{  1,2,\ldots,n\right\}  $). This proves
(\ref{pf.prop.sol.unirows.r.1}).}. Now, the definition of $\left(
1,2,\ldots,\widehat{r},\ldots,n\right)  $ yields%
\begin{align*}
\left(  1,2,\ldots,\widehat{r},\ldots,n\right)   &  =\left(  1,2,\ldots
,r-1,r+1,r+2,\ldots,n\right) \\
&  =\left(  \mathbf{d}_{r}\left(  1\right)  ,\mathbf{d}_{r}\left(  2\right)
,\ldots,\mathbf{d}_{r}\left(  n-1\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.prop.sol.unirows.r.1})}\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.unirows.r}.
\end{proof}

\begin{proposition}
\label{prop.sol.unirows.d}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in\mathbb{K}%
^{n\times m}$ be an $n\times m$-matrix.

\textbf{(a)} For every $u\in\left\{  1,2,\ldots,n\right\}  $, we have $A_{\sim
u,\bullet}=\left(  a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)  _{1\leq
i\leq n-1,\ 1\leq j\leq m}$.

\textbf{(b)} For every $v\in\left\{  1,2,\ldots,m\right\}  $, we have
$A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}$.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sol.unirows.d}.]\textbf{(a)} Let $u\in\left\{
1,2,\ldots,n\right\}  $. Then, Proposition \ref{prop.sol.unirows.r} (applied
to $r=u$) yields%
\[
\left(  \mathbf{d}_{u}\left(  1\right)  ,\mathbf{d}_{u}\left(  2\right)
,\ldots,\mathbf{d}_{u}\left(  n-1\right)  \right)  =\left(  1,2,\ldots
,\widehat{u},\ldots,n\right)  .
\]


The definition of $A_{\sim u,\bullet}$ yields
\begin{align*}
A_{\sim u,\bullet}  &  =\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A=\operatorname*{rows}\nolimits_{\mathbf{d}_{u}\left(
1\right)  ,\mathbf{d}_{u}\left(  2\right)  ,\ldots,\mathbf{d}_{u}\left(
n-1\right)  }A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{u}%
,\ldots,n\right)  =\left(  \mathbf{d}_{u}\left(  1\right)  ,\mathbf{d}%
_{u}\left(  2\right)  ,\ldots,\mathbf{d}_{u}\left(  n-1\right)  \right)
\right) \\
&  =\left(  a_{\mathbf{d}_{u}\left(  x\right)  ,j}\right)  _{1\leq x\leq
n-1,\ 1\leq j\leq m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{rows}\nolimits_{\mathbf{d}%
_{u}\left(  1\right)  ,\mathbf{d}_{u}\left(  2\right)  ,\ldots,\mathbf{d}%
_{u}\left(  n-1\right)  }A\\
\text{(since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
x,j\right)  \text{ as }\left(  i,j\right)  \right)  .
\end{align*}
This proves Proposition \ref{prop.sol.unirows.d} \textbf{(a)}.

\textbf{(b)} Let $v\in\left\{  1,2,\ldots,m\right\}  $. Then, Proposition
\ref{prop.sol.unirows.r} (applied to $v$ and $m$ instead of $r$ and $n$)
yields%
\[
\left(  \mathbf{d}_{v}\left(  1\right)  ,\mathbf{d}_{v}\left(  2\right)
,\ldots,\mathbf{d}_{v}\left(  m-1\right)  \right)  =\left(  1,2,\ldots
,\widehat{v},\ldots,m\right)  .
\]


The definition of $A_{\bullet,\sim v}$ yields
\begin{align*}
A_{\bullet,\sim v}  &  =\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,m}A=\operatorname*{cols}\nolimits_{\mathbf{d}_{v}\left(
1\right)  ,\mathbf{d}_{v}\left(  2\right)  ,\ldots,\mathbf{d}_{v}\left(
m-1\right)  }A\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  1,2,\ldots,\widehat{v}%
,\ldots,m\right)  =\left(  \mathbf{d}_{v}\left(  1\right)  ,\mathbf{d}%
_{v}\left(  2\right)  ,\ldots,\mathbf{d}_{v}\left(  m-1\right)  \right)
\right) \\
&  =\left(  a_{i,\mathbf{d}_{v}\left(  y\right)  }\right)  _{1\leq i\leq
n,\ 1\leq y\leq m-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{cols}\nolimits_{\mathbf{d}%
_{v}\left(  1\right)  ,\mathbf{d}_{v}\left(  2\right)  ,\ldots,\mathbf{d}%
_{v}\left(  m-1\right)  }A\\
\text{(since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
i,y\right)  \text{ as }\left(  i,j\right)  \right)  .
\end{align*}
This proves Proposition \ref{prop.sol.unirows.d} \textbf{(b)}.
\end{proof}

\begin{proposition}
\label{prop.sol.unrows.2}Let $n\in\mathbb{N}$. Let $r$ and $s$ be two elements
of $\left\{  1,2,\ldots,n\right\}  $ such that $r<s$.

\textbf{(a)} Then,
\[
\left(  1,2,\ldots,\widehat{r},\ldots,\widehat{s},\ldots,n\right)  =\left(
\mathbf{d}_{r,s}\left(  1\right)  ,\mathbf{d}_{r,s}\left(  2\right)
,\ldots,\mathbf{d}_{r,s}\left(  n-2\right)  \right)  .
\]


\textbf{(b)} We have
\[
\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}_{r}\left(
j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }j\in\mathbb{Z}.
\]


\textbf{(c)} We have%
\[
\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}%
_{s-1}\left(  j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
j\in\mathbb{Z}.
\]

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sol.unrows.2}.]Recall that $r<s$. Thus, $r\leq
s-1$ (since $r$ and $s$ are integers). Hence, $\underbrace{r}_{\leq s-1}%
-1\leq\left(  s-1\right)  -1=s-2$.

\textbf{(a)} If $\alpha$ and $\beta$ are two finite lists of integers, then we
define a new finite list $\alpha\ast\beta$ of integers by setting%
\[
\alpha\ast\beta=\left(  \alpha_{1},\alpha_{2},\ldots,\alpha_{a},\beta
_{1},\beta_{2},\ldots,\beta_{b}\right)  ,
\]
where $\alpha$ is written in the form $\left(  \alpha_{1},\alpha_{2}%
,\ldots,\alpha_{a}\right)  $, and where $\beta$ is written in the form
$\left(  \beta_{1},\beta_{2},\ldots,\beta_{b}\right)  $. This new list
$\alpha\ast\beta$ is called the \textit{concatenation} of the lists $\alpha$
and $\beta$. If $\alpha$, $\beta$ and $\gamma$ are three finite lists of
integers, then the two lists $\left(  \alpha\ast\beta\right)  \ast\gamma$ and
$\alpha\ast\left(  \beta\ast\gamma\right)  $ are identical. In other words,
concatenation of finite lists is an associative operation. Thus, we are able
to write $\alpha\ast\beta\ast\gamma$ for any of the two identical lists
$\left(  \alpha\ast\beta\right)  \ast\gamma$ and $\alpha\ast\left(  \beta
\ast\gamma\right)  $ whenever $\alpha$, $\beta$ and $\gamma$ are three finite
lists of integers.

Now, the definition of $\left(  1,2,\ldots,\widehat{r},\ldots,\widehat{s}%
,\ldots,n\right)  $ yields%
\begin{align}
&  \left(  1,2,\ldots,\widehat{r},\ldots,\widehat{s},\ldots,n\right)
\nonumber\\
&  =\left(  \underbrace{1,2,\ldots,r-1}_{\substack{\text{all integers}%
\\\text{from }1\text{ to }r-1}},\underbrace{r+1,r+2,\ldots,s-1}%
_{\substack{\text{all integers}\\\text{from }r+1\text{ to }s-1}%
},\underbrace{s+1,s+2,\ldots,n}_{\substack{\text{all integers}\\\text{from
}s+1\text{ to }n}}\right) \nonumber\\
&  =\underbrace{\left(  \underbrace{1,2,\ldots,r-1}_{\substack{\text{all
integers}\\\text{from }1\text{ to }r-1}},\underbrace{r+1,r+2,\ldots
,s-1}_{\substack{\text{all integers}\\\text{from }r+1\text{ to }s-1}}\right)
}_{=\left(  1,2,\ldots,r-1\right)  \ast\left(  r+1,r+2,\ldots,s-1\right)
}\ast\left(  s+1,s+2,\ldots,n\right) \nonumber\\
&  =\left(  1,2,\ldots,r-1\right)  \ast\left(  r+1,r+2,\ldots,s-1\right)
\ast\left(  s+1,s+2,\ldots,n\right)  . \label{pf.prop.sol.unrows.2.a.5}%
\end{align}
Now,
\begin{equation}
\mathbf{d}_{r,s}\left(  j\right)  =j\ \ \ \ \ \ \ \ \ \ \text{for every }%
j\in\left\{  1,2,\ldots,r-1\right\}  \label{pf.prop.sol.unrows.2.a.7a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.unrows.2.a.7a}):} Let
$j\in\left\{  1,2,\ldots,r-1\right\}  $. Then, $j\in\mathbb{Z}$ and $j<r$
(since $j\in\left\{  1,2,\ldots,r-1\right\}  $). The definition of
$\mathbf{d}_{r,s}\left(  j\right)  $ yields $\mathbf{d}_{r,s}\left(  j\right)
=%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
=j$ (since $j<r$). This proves (\ref{pf.prop.sol.unrows.2.a.7a}).}. In other
words,%
\begin{equation}
\left(  \mathbf{d}_{r,s}\left(  1\right)  ,\mathbf{d}_{r,s}\left(  2\right)
,\ldots,\mathbf{d}_{r,s}\left(  r-1\right)  \right)  =\left(  1,2,\ldots
,r-1\right)  . \label{pf.prop.sol.unrows.2.a.7b}%
\end{equation}


Also,%
\begin{equation}
\mathbf{d}_{r,s}\left(  j\right)  =j+1\ \ \ \ \ \ \ \ \ \ \text{for every
}j\in\left\{  r,r+1,\ldots,s-2\right\}  \label{pf.prop.sol.unrows.2.a.8a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.unrows.2.a.8a}):} Let
$j\in\left\{  r,r+1,\ldots,s-2\right\}  $. Then, $j\in\mathbb{Z}$. Also,
$j\in\left\{  r,r+1,\ldots,s-2\right\}  $, so that $r\leq j\leq s-2$. From
$j\leq s-2<s-1$ and $r\leq j$, we obtain $r\leq j<s-1$. The definition of
$\mathbf{d}_{r,s}\left(  j\right)  $ yields $\mathbf{d}_{r,s}\left(  j\right)
=%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
=j+1$ (since $r\leq j<s-1$). This proves (\ref{pf.prop.sol.unrows.2.a.8a}).}.
In other words,%
\begin{align}
\left(  \mathbf{d}_{r,s}\left(  r\right)  ,\mathbf{d}_{r,s}\left(  r+1\right)
,\ldots,\mathbf{d}_{r,s}\left(  s-2\right)  \right)   &  =\left(  r+1,\left(
r+1\right)  +1,\ldots,\left(  s-2\right)  +1\right) \nonumber\\
&  =\left(  r+1,r+2,\ldots,s-1\right)  . \label{pf.prop.sol.unrows.2.a.8b}%
\end{align}


Finally,%
\begin{equation}
\mathbf{d}_{r,s}\left(  j\right)  =j+2\ \ \ \ \ \ \ \ \ \ \text{for every
}j\in\left\{  s-1,s,\ldots,n-2\right\}  \label{pf.prop.sol.unrows.2.a.9a}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.unrows.2.a.9a}):} Let
$j\in\left\{  s-1,s,\ldots,n-2\right\}  $. Then, $j\in\mathbb{Z}$. Also,
$j\in\left\{  s-1,s,\ldots,n-2\right\}  $, so that $j\geq s-1$. Thus, $s-1\leq
j$. The definition of $\mathbf{d}_{r,s}\left(  j\right)  $ yields
$\mathbf{d}_{r,s}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
=j+2$ (since $s-1\leq j$). This proves (\ref{pf.prop.sol.unrows.2.a.9a}).}. In
other words,%
\begin{align}
\left(  \mathbf{d}_{r,s}\left(  s-1\right)  ,\mathbf{d}_{r,s}\left(  s\right)
,\ldots,\mathbf{d}_{r,s}\left(  n-2\right)  \right)   &  =\left(  \left(
s-1\right)  +2,s+2,\ldots,\left(  n-2\right)  +2\right) \nonumber\\
&  =\left(  s+1,s+2,\ldots,n\right)  . \label{pf.prop.sol.unrows.2.a.9b}%
\end{align}


Now, recall that $r-1\leq s-2$. Hence,%
\begin{align}
&  \left(  \mathbf{d}_{r,s}\left(  1\right)  ,\mathbf{d}_{r,s}\left(
2\right)  ,\ldots,\mathbf{d}_{r,s}\left(  s-2\right)  \right) \nonumber\\
&  =\underbrace{\left(  \mathbf{d}_{r,s}\left(  1\right)  ,\mathbf{d}%
_{r,s}\left(  2\right)  ,\ldots,\mathbf{d}_{r,s}\left(  r-1\right)  \right)
}_{\substack{=\left(  1,2,\ldots,r-1\right)  \\\text{(by
(\ref{pf.prop.sol.unrows.2.a.7b}))}}}\ast\underbrace{\left(  \mathbf{d}%
_{r,s}\left(  r\right)  ,\mathbf{d}_{r,s}\left(  r+1\right)  ,\ldots
,\mathbf{d}_{r,s}\left(  s-2\right)  \right)  }_{\substack{=\left(
r+1,r+2,\ldots,s-1\right)  \\\text{(by (\ref{pf.prop.sol.unrows.2.a.8b}))}%
}}\nonumber\\
&  =\left(  1,2,\ldots,r-1\right)  \ast\left(  r+1,r+2,\ldots,s-1\right)  .
\label{pf.prop.sol.unrows.2.a.10b}%
\end{align}
Now,%
\begin{align*}
&  \left(  \mathbf{d}_{r,s}\left(  1\right)  ,\mathbf{d}_{r,s}\left(
2\right)  ,\ldots,\mathbf{d}_{r,s}\left(  n-2\right)  \right) \\
&  =\underbrace{\left(  \mathbf{d}_{r,s}\left(  1\right)  ,\mathbf{d}%
_{r,s}\left(  2\right)  ,\ldots,\mathbf{d}_{r,s}\left(  s-2\right)  \right)
}_{\substack{=\left(  1,2,\ldots,r-1\right)  \ast\left(  r+1,r+2,\ldots
,s-1\right)  \\\text{(by (\ref{pf.prop.sol.unrows.2.a.10b}))}}}\ast
\underbrace{\left(  \mathbf{d}_{r,s}\left(  s-1\right)  ,\mathbf{d}%
_{r,s}\left(  s\right)  ,\ldots,\mathbf{d}_{r,s}\left(  n-2\right)  \right)
}_{\substack{=\left(  s+1,s+2,\ldots,n\right)  \\\text{(by
(\ref{pf.prop.sol.unrows.2.a.9b}))}}}\\
&  =\left(  1,2,\ldots,r-1\right)  \ast\left(  r+1,r+2,\ldots,s-1\right)
\ast\left(  s+1,s+2,\ldots,n\right) \\
&  =\left(  1,2,\ldots,\widehat{r},\ldots,\widehat{s},\ldots,n\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{pf.prop.sol.unrows.2.a.5})}\right)
.
\end{align*}
This proves Proposition \ref{prop.sol.unrows.2} \textbf{(a)}.

\textbf{(b)} Let $j\in\mathbb{Z}$. We want to prove that $\mathbf{d}%
_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}_{r}\left(  j\right)
\right)  $.

We have either $j<s-1$ or $j\geq s-1$. In other words, we must be in one of
the following two Cases:

\textit{Case 1:} We have $j<s-1$.

\textit{Case 2:} We have $j\geq s-1$.

Let us first consider Case 1. In this case, we have $j<s-1$. Thus, $j<s-1<s$.
We have either $j<r$ or $j\geq r$. In other words, we must be in one of the
following two Subcases:

\textit{Subcase 1.1:} We have $j<r$.

\textit{Subcase 1.2:} We have $j\geq r$.

Let us first consider Subcase 1.1. In this subcase, we have $j<r$. Now, the
definition of $\mathbf{d}_{r}\left(  j\right)  $ yields $\mathbf{d}_{r}\left(
j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
=j$ (since $j<r$). But $j<r<s$. Now,
\begin{align*}
\mathbf{d}_{s}\left(  \underbrace{\mathbf{d}_{r}\left(  j\right)  }%
_{=j}\right)   &  =\mathbf{d}_{s}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<s;\\
j+1, & \text{if }j\geq s
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{s}\left(  j\right)  \right) \\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<s\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\mathbf{d}_{r,s}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r,s}\left(  j\right)  \right) \\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<r\right)  ,
\end{align*}
we obtain $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(
\mathbf{d}_{r}\left(  j\right)  \right)  $. Thus, $\mathbf{d}_{r,s}\left(
j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}_{r}\left(  j\right)  \right)  $ is
proven in Subcase 1.1.

Let us now consider Subcase 1.2. In this subcase, we have $j\geq r$. In other
words, $r\leq j$. Now, the definition of $\mathbf{d}_{r}\left(  j\right)  $
yields $\mathbf{d}_{r}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
=j+1$ (since $j\geq r$). But $j<s-1$, so that $j+1<s$. Now,
\begin{align*}
\mathbf{d}_{s}\left(  \underbrace{\mathbf{d}_{r}\left(  j\right)  }%
_{=j+1}\right)   &  =\mathbf{d}_{s}\left(  j+1\right)  =%
\begin{cases}
j+1, & \text{if }j+1<s;\\
\left(  j+1\right)  +1, & \text{if }j+1\geq s
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{s}\left(  j+1\right)  \right) \\
&  =j+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1<s\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\mathbf{d}_{r,s}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r,s}\left(  j\right)  \right) \\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }r\leq j<s-1\text{ (because
}r\leq j\text{ and }j<s-1\text{)}\right)  ,
\end{align*}
we obtain $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(
\mathbf{d}_{r}\left(  j\right)  \right)  $. Thus, $\mathbf{d}_{r,s}\left(
j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}_{r}\left(  j\right)  \right)  $ is
proven in Subcase 1.2.

We have now proven $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(
\mathbf{d}_{r}\left(  j\right)  \right)  $ in each of the two Subcases 1.1 and
1.2. Since these two Subcases cover all of Case 1, we thus conclude that
$\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}%
_{r}\left(  j\right)  \right)  $ holds in Case 1.

Let us now consider Case 2. In this case, we have $j\geq s-1$. Now, $r\leq
s-1\leq j$ (since $j\geq s-1$) and thus $j\geq r$. Now, the definition of
$\mathbf{d}_{r}\left(  j\right)  $ yields $\mathbf{d}_{r}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
=j+1$ (since $j\geq r$). But $j\geq s-1$, so that $j+1\geq s$. Now,
\begin{align*}
\mathbf{d}_{s}\left(  \underbrace{\mathbf{d}_{r}\left(  j\right)  }%
_{=j+1}\right)   &  =\mathbf{d}_{s}\left(  j+1\right)  =%
\begin{cases}
j+1, & \text{if }j+1<s;\\
\left(  j+1\right)  +1, & \text{if }j+1\geq s
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{s}\left(  j+1\right)  \right) \\
&  =\left(  j+1\right)  +1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1\geq
s\right) \\
&  =j+2.
\end{align*}
Comparing this with%
\begin{align*}
\mathbf{d}_{r,s}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r,s}\left(  j\right)  \right) \\
&  =j+2\ \ \ \ \ \ \ \ \ \ \left(  \text{since }s-1\leq j\right)  ,
\end{align*}
we obtain $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(
\mathbf{d}_{r}\left(  j\right)  \right)  $. Thus, $\mathbf{d}_{r,s}\left(
j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}_{r}\left(  j\right)  \right)  $ is
proven in Case 2.

We have now proven $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(
\mathbf{d}_{r}\left(  j\right)  \right)  $ in each of the two Cases 1 and 2.
Since these two Cases cover all possibilities, we thus conclude that
$\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{s}\left(  \mathbf{d}%
_{r}\left(  j\right)  \right)  $ always holds. This completes the proof of
Proposition \ref{prop.sol.unrows.2} \textbf{(b)}.

\textbf{(c)} Let $j\in\mathbb{Z}$. We want to prove that $\mathbf{d}%
_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}_{s-1}\left(
j\right)  \right)  $.

We have either $j<s-1$ or $j\geq s-1$. In other words, we must be in one of
the following two Cases:

\textit{Case 1:} We have $j<s-1$.

\textit{Case 2:} We have $j\geq s-1$.

Let us first consider Case 1. In this case, we have $j<s-1$. Thus, $j<s-1<s$.
Now, the definition of $\mathbf{d}_{s-1}\left(  j\right)  $ yields
$\mathbf{d}_{s-1}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<s-1;\\
j+1, & \text{if }j\geq s-1
\end{cases}
=j$ (since $j<s-1$).

We have either $j<r$ or $j\geq r$. In other words, we must be in one of the
following two Subcases:

\textit{Subcase 1.1:} We have $j<r$.

\textit{Subcase 1.2:} We have $j\geq r$.

Let us first consider Subcase 1.1. In this subcase, we have $j<r$. Now,%
\begin{align*}
\mathbf{d}_{r}\left(  \underbrace{\mathbf{d}_{s-1}\left(  j\right)  }%
_{=j}\right)   &  =\mathbf{d}_{r}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}_{r}\left(
j\right)  \right) \\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<r\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\mathbf{d}_{r,s}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r,s}\left(  j\right)  \right) \\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j<r\right)  ,
\end{align*}
we obtain $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(
\mathbf{d}_{s-1}\left(  j\right)  \right)  $. Thus, $\mathbf{d}_{r,s}\left(
j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}_{s-1}\left(  j\right)  \right)  $
is proven in Subcase 1.1.

Let us now consider Subcase 1.2. In this subcase, we have $j\geq r$. In other
words, $r\leq j$. Now,%
\begin{align*}
\mathbf{d}_{r}\left(  \underbrace{\mathbf{d}_{s-1}\left(  j\right)  }%
_{=j}\right)   &  =\mathbf{d}_{r}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }j\geq r
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}_{r}\left(
j\right)  \right) \\
&  =j+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j\geq r\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\mathbf{d}_{r,s}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r,s}\left(  j\right)  \right) \\
&  =j\ \ \ \ \ \ \ \ \ \ \left(  \text{since }r\leq j<s-1\text{ (because
}r\leq j\text{ and }j<s-1\text{)}\right)  ,
\end{align*}
we obtain $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(
\mathbf{d}_{s-1}\left(  j\right)  \right)  $. Thus, $\mathbf{d}_{r,s}\left(
j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}_{s-1}\left(  j\right)  \right)  $
is proven in Subcase 1.2.

We have now proven $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(
\mathbf{d}_{s-1}\left(  j\right)  \right)  $ in each of the two Subcases 1.1
and 1.2. Since these two Subcases cover all of Case 1, we thus conclude that
$\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}%
_{s-1}\left(  j\right)  \right)  $ holds in Case 1.

Let us now consider Case 2. In this case, we have $j\geq s-1$. In other words,
$s-1\leq j$. Now, the definition of $\mathbf{d}_{s-1}\left(  j\right)  $
yields $\mathbf{d}_{s-1}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<s-1;\\
j+1, & \text{if }j\geq s-1
\end{cases}
=j+1$ (since $j\geq s-1$). Also, $j\geq s-1$, so that $j+1\geq s>r$ and thus
$j+1\geq r$. Now,
\begin{align*}
\mathbf{d}_{r}\left(  \underbrace{\mathbf{d}_{s-1}\left(  j\right)  }%
_{=j+1}\right)   &  =\mathbf{d}_{r}\left(  j+1\right)  =%
\begin{cases}
j+1, & \text{if }j+1<r;\\
\left(  j+1\right)  +1, & \text{if }j+1\geq r
\end{cases}
\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r}\left(  j+1\right)  \right) \\
&  =\left(  j+1\right)  +1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j+1\geq
r\right) \\
&  =j+2.
\end{align*}
Comparing this with%
\begin{align*}
\mathbf{d}_{r,s}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<r;\\
j+1, & \text{if }r\leq j<s-1;\\
j+2, & \text{if }s-1\leq j
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}%
_{r,s}\left(  j\right)  \right) \\
&  =j+2\ \ \ \ \ \ \ \ \ \ \left(  \text{since }s-1\leq j\right)  ,
\end{align*}
we obtain $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(
\mathbf{d}_{s-1}\left(  j\right)  \right)  $. Thus, $\mathbf{d}_{r,s}\left(
j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}_{s-1}\left(  j\right)  \right)  $
is proven in Case 2.

We have now proven $\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(
\mathbf{d}_{s-1}\left(  j\right)  \right)  $ in each of the two Cases 1 and 2.
Since these two Cases cover all possibilities, we thus conclude that
$\mathbf{d}_{r,s}\left(  j\right)  =\mathbf{d}_{r}\left(  \mathbf{d}%
_{s-1}\left(  j\right)  \right)  $ always holds. This completes the proof of
Proposition \ref{prop.sol.unrows.2} \textbf{(c)}.
\end{proof}

Now, we can step to the proof of Proposition \ref{prop.unrows.basics}:

\begin{proof}
[Proof of Proposition \ref{prop.unrows.basics}.]Write the matrix
$A\in\mathbb{K}^{n\times m}$ in the form $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$.

\textbf{(a)} Let $u\in\left\{  1,2,\ldots,n\right\}  $. The definition of
$A_{u,\bullet}$ shows that $A_{u,\bullet}$ is the $u$-th row of the matrix
$A$. In other words, $A_{u,\bullet}=\left(  \text{the }u\text{-th row of the
matrix }A\right)  $.

On the other hand, the definition of $\operatorname*{rows}\nolimits_{u}A$
yields $\operatorname*{rows}\nolimits_{u}A=\left(  a_{u,j}\right)  _{1\leq
x\leq1,\ 1\leq j\leq m}$. Now,%
\begin{align*}
A_{u,\bullet}  &  =\left(  \text{the }u\text{-th row of the matrix }A\right)
=\left(  a_{u,1},a_{u,2},\ldots,a_{u,m}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right) \\
&  =\left(  a_{u,j}\right)  _{1\leq x\leq1,\ 1\leq j\leq m}%
=\operatorname*{rows}\nolimits_{u}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\operatorname*{rows}\nolimits_{u}A=\left(  a_{u,j}\right)  _{1\leq
x\leq1,\ 1\leq j\leq m}\right)  .
\end{align*}
This proves Proposition \ref{prop.unrows.basics} \textbf{(a)}.

\textbf{(b)} Let $v\in\left\{  1,2,\ldots,m\right\}  $. The definition of
$A_{\bullet,v}$ shows that $A_{\bullet,v}$ is the $v$-th column of the matrix
$A$. In other words, $A_{\bullet,v}=\left(  \text{the }v\text{-th column of
the matrix }A\right)  $.

On the other hand, the definition of $\operatorname*{cols}\nolimits_{v}A$
yields $\operatorname*{cols}\nolimits_{v}A=\left(  a_{i,v}\right)  _{1\leq
i\leq n,\ 1\leq y\leq1}$. Now,%
\begin{align*}
A_{\bullet,v}  &  =\left(  \text{the }v\text{-th column of the matrix
}A\right)  =\left(
\begin{array}
[c]{c}%
a_{1,v}\\
a_{2,v}\\
\vdots\\
a_{n,v}%
\end{array}
\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right) \\
&  =\left(  a_{i,v}\right)  _{1\leq i\leq n,\ 1\leq y\leq1}%
=\operatorname*{cols}\nolimits_{v}A\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\operatorname*{cols}\nolimits_{v}A=\left(  a_{i,v}\right)  _{1\leq i\leq
n,\ 1\leq y\leq1}\right)  .
\end{align*}
This proves Proposition \ref{prop.unrows.basics} \textbf{(b)}.

\textbf{(c)} Let $u\in\left\{  1,2,\ldots,n\right\}  $ and $v\in\left\{
1,2,\ldots,m\right\}  $. Then, Proposition \ref{prop.submatrix.easy}
\textbf{(d)} (applied to $n-1$, $\left(  1,2,\ldots,\widehat{u},\ldots
,n\right)  $, $m-1$ and $\left(  1,2,\ldots,\widehat{v},\ldots,m\right)  $
instead of $u$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $, $v$ and $\left(
j_{1},j_{2},\ldots,j_{v}\right)  $) yields%
\begin{align}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{v},\ldots,m}A  &  =\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,m}A\right) \label{pf.prop.unrows.basics.c.1}\\
&  =\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,m}\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)  .
\label{pf.prop.unrows.basics.c.2}%
\end{align}
But the definition of $A_{\sim u,\sim v}$ yields $A_{\sim u,\sim
v}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}%
^{1,2,\ldots,\widehat{v},\ldots,m}A$.

Now, $A_{\sim u,\bullet}=\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,n}A$ (by the definition of $A_{\sim u,\bullet}$), and%
\begin{align}
\left(  A_{\sim u,\bullet}\right)  _{\bullet,\sim v}  &  =\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{v},\ldots,m}\left(  \underbrace{A_{\sim
u,\bullet}}_{=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,n}A}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(
A_{\sim u,\bullet}\right)  _{\bullet,\sim v}\right) \nonumber\\
&  =\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,m}\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}A\right)
=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{v},\ldots,m}A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.unrows.basics.c.2})}\right) \nonumber\\
&  =A_{\sim u,\sim v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\sim u,\sim
v}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}%
^{1,2,\ldots,\widehat{v},\ldots,m}A\right)  .
\label{pf.prop.unrows.basics.c.5}%
\end{align}
Also, $A_{\bullet,\sim v}=\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,m}A$ (by the definition of $A_{\bullet,\sim v}$), and%
\begin{align*}
\left(  A_{\bullet,\sim v}\right)  _{\sim u,\bullet}  &  =\operatorname*{rows}%
\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}\left(  \underbrace{A_{\bullet,\sim
v}}_{=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,m}%
A}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(
A_{\bullet,\sim v}\right)  _{\sim u,\bullet}\right) \\
&  =\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}\left(
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,m}A\right)
=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}^{1,2,\ldots
,\widehat{v},\ldots,m}A\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.unrows.basics.c.1})}\right) \\
&  =A_{\sim u,\sim v}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\sim u,\sim
v}=\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{u},\ldots,n}%
^{1,2,\ldots,\widehat{v},\ldots,m}A\right)  .
\end{align*}
Combining this with (\ref{pf.prop.unrows.basics.c.5}), we obtain $\left(
A_{\bullet,\sim v}\right)  _{\sim u,\bullet}=\left(  A_{\sim u,\bullet
}\right)  _{\bullet,\sim v}=A_{\sim u,\sim v}$. This proves Proposition
\ref{prop.unrows.basics} \textbf{(c)}.

\textbf{(d)} Let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
1,2,\ldots,v-1\right\}  $. From $w\in\left\{  1,2,\ldots,v-1\right\}  $, we
obtain $w\geq1$ and $w\leq v-1$. Thus, $w\leq v-1<v$, so that $w\leq v\leq m$
(since $v\in\left\{  1,2,\ldots,m\right\}  $). Also, $w\in\left\{
1,2,\ldots,v-1\right\}  \subseteq\left\{  1,2,\ldots,m-1\right\}  $ (since
$\underbrace{v}_{\leq m}-1\leq m-1$). From $w\geq1$ and $w\leq m$, we obtain
$w\in\left\{  1,2,\ldots,m\right\}  $. The definition of $\mathbf{d}%
_{v}\left(  w\right)  $ yields $\mathbf{d}_{v}\left(  w\right)  =%
\begin{cases}
w, & \text{if }w<v;\\
w+1, & \text{if }w\geq v
\end{cases}
=w$ (since $w<v$).

But $A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}$ (by Proposition \ref{prop.sol.unirows.d}
\textbf{(b)}). But $\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}$ is the
$w$-th column of the matrix $A_{\bullet,\sim v}$ (by the definition of
$\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}$). Thus,%
\begin{align*}
\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}  &  =\left(  \text{the
}w\text{-th column of the matrix }A_{\bullet,\sim v}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,\mathbf{d}_{v}\left(  w\right)  }\\
a_{2,\mathbf{d}_{v}\left(  w\right)  }\\
\vdots\\
a_{n,\mathbf{d}_{v}\left(  w\right)  }%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\bullet,\sim v}=\left(
a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m-1}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,w}\\
a_{2,w}\\
\vdots\\
a_{n,w}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{v}\left(
w\right)  =w\right)  .
\end{align*}
Comparing this with%
\begin{align*}
A_{\bullet,w}  &  =\left(  \text{the }w\text{-th column of the matrix
}A\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{\bullet,w}\text{ is the }w\text{-th column of the matrix }A\\
\text{(by the definition of }A_{\bullet,w}\text{)}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,w}\\
a_{2,w}\\
\vdots\\
a_{n,w}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}\right)  ,
\end{align*}
we obtain $\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}=A_{\bullet,w}$.
This proves Proposition \ref{prop.unrows.basics} \textbf{(d)}.

\textbf{(e)} Let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
v,v+1,\ldots,m-1\right\}  $. From $w\in\left\{  v,v+1,\ldots,m-1\right\}  $,
we obtain $w\geq v$ and $w\leq m-1$. Now, $w+1>w\geq v$, so that $w+1\geq
v\geq1$ (since $v\in\left\{  1,2,\ldots,m\right\}  $). Combining this with
$w+1\leq m$ (since $w\leq m-1$), we obtain $w+1\in\left\{  1,2,\ldots
,m\right\}  $. Also, $w\in\left\{  v,v+1,\ldots,m-1\right\}  \subseteq\left\{
1,2,\ldots,m-1\right\}  $ (since $v\geq1$).

The definition of $\mathbf{d}_{v}\left(  w\right)  $ yields $\mathbf{d}%
_{v}\left(  w\right)  =%
\begin{cases}
w, & \text{if }w<v;\\
w+1, & \text{if }w\geq v
\end{cases}
=w+1$ (since $w\geq v$).

But $A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}$ (by Proposition \ref{prop.sol.unirows.d}
\textbf{(b)}). But $\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}$ is the
$w$-th column of the matrix $A_{\bullet,\sim v}$ (by the definition of
$\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}$). Thus,%
\begin{align*}
\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}  &  =\left(  \text{the
}w\text{-th column of the matrix }A_{\bullet,\sim v}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,\mathbf{d}_{v}\left(  w\right)  }\\
a_{2,\mathbf{d}_{v}\left(  w\right)  }\\
\vdots\\
a_{n,\mathbf{d}_{v}\left(  w\right)  }%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\bullet,\sim v}=\left(
a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m-1}\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,w+1}\\
a_{2,w+1}\\
\vdots\\
a_{n,w+1}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{v}\left(
w\right)  =w+1\right)  .
\end{align*}
Comparing this with%
\begin{align*}
A_{\bullet,w+1}  &  =\left(  \text{the }\left(  w+1\right)  \text{-th column
of the matrix }A\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{\bullet,w+1}\text{ is the }\left(  w+1\right)  \text{-th
column of the matrix }A\\
\text{(by the definition of }A_{\bullet,w+1}\text{)}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,w+1}\\
a_{2,w+1}\\
\vdots\\
a_{n,w+1}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}\right)  ,
\end{align*}
we obtain $\left(  A_{\bullet,\sim v}\right)  _{\bullet,w}=A_{\bullet,w+1}$.
This proves Proposition \ref{prop.unrows.basics} \textbf{(e)}.

\textbf{(f)} Let $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
1,2,\ldots,u-1\right\}  $. From $w\in\left\{  1,2,\ldots,u-1\right\}  $, we
obtain $w\geq1$ and $w\leq u-1$. Thus, $w\leq u-1<u$, so that $w\leq u\leq n$
(since $u\in\left\{  1,2,\ldots,n\right\}  $). From $w\geq1$ and $w\leq n$, we
obtain $w\in\left\{  1,2,\ldots,n\right\}  $. We have $w\in\left\{
1,2,\ldots,u-1\right\}  \subseteq\left\{  1,2,\ldots,n-1\right\}  $ (since
$\underbrace{u}_{\leq n}-1\leq n-1$). The definition of $\mathbf{d}_{u}\left(
w\right)  $ yields $\mathbf{d}_{v}\left(  w\right)  =%
\begin{cases}
w, & \text{if }w<u;\\
w+1, & \text{if }w\geq u
\end{cases}
=w$ (since $w<u$).

But $A_{\sim u,\bullet}=\left(  a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq m}$ (by Proposition \ref{prop.sol.unirows.d}
\textbf{(a)}). But $\left(  A_{\sim u,\bullet}\right)  _{w,\bullet}$ is the
$w$-th row of the matrix $A_{\sim u,\bullet}$ (by the definition of $\left(
A_{\sim u,\bullet}\right)  _{w,\bullet}$). Thus,%
\begin{align*}
\left(  A_{\sim u,\bullet}\right)  _{w,\bullet}  &  =\left(  \text{the
}w\text{-th row of the matrix }A_{\sim u,\bullet}\right) \\
&  =\left(  a_{\mathbf{d}_{u}\left(  w\right)  ,1},a_{\mathbf{d}_{u}\left(
w\right)  ,2},\ldots,a_{\mathbf{d}_{u}\left(  w\right)  ,m}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\sim u,\bullet}=\left(
a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
m}\right) \\
&  =\left(  a_{w,1},a_{w,2},\ldots,a_{w,m}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }\mathbf{d}_{u}\left(  w\right)  =w\right)  .
\end{align*}
Comparing this with%
\begin{align*}
A_{w,\bullet}  &  =\left(  \text{the }w\text{-th row of the matrix }A\right)
\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{w,\bullet}\text{ is the }w\text{-th row of the matrix }A\\
\text{(by the definition of }A_{w,\bullet}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{w,1},a_{w,2},\ldots,a_{w,m}\right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\right)  ,
\end{align*}
we obtain $\left(  A_{\sim u,\bullet}\right)  _{w,\bullet}=A_{w,\bullet}$.
This proves Proposition \ref{prop.unrows.basics} \textbf{(f)}.

\textbf{(g)} Let $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
u,u+1,\ldots,n-1\right\}  $. From $w\in\left\{  u,u+1,\ldots,n-1\right\}  $,
we obtain $w\geq u$ and $w\leq n-1$. Now, $w+1>w\geq u$, so that $w+1\geq
u\geq1$ (since $u\in\left\{  1,2,\ldots,n\right\}  $). Combining this with
$w+1\leq n$ (since $w\leq n-1$), we obtain $w+1\in\left\{  1,2,\ldots
,n\right\}  $. We have $w\in\left\{  u,u+1,\ldots,n-1\right\}  \subseteq
\left\{  1,2,\ldots,n-1\right\}  $ (since $u\geq1$).

The definition of $\mathbf{d}_{u}\left(  w\right)  $ yields $\mathbf{d}%
_{u}\left(  w\right)  =%
\begin{cases}
w, & \text{if }w<u;\\
w+1, & \text{if }w\geq u
\end{cases}
=w+1$ (since $w\geq u$).

But $A_{\sim u,\bullet}=\left(  a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq m}$ (by Proposition \ref{prop.sol.unirows.d}
\textbf{(a)}). But $\left(  A_{\sim u,\bullet}\right)  _{w,\bullet}$ is the
$w$-th row of the matrix $A_{\sim u,\bullet}$ (by the definition of $\left(
A_{\sim u,\bullet}\right)  _{w,\bullet}$). Thus,%
\begin{align*}
\left(  A_{\sim u,\bullet}\right)  _{w,\bullet}  &  =\left(  \text{the
}w\text{-th row of the matrix }A_{\sim u,\bullet}\right) \\
&  =\left(  a_{\mathbf{d}_{u}\left(  w\right)  ,1},a_{\mathbf{d}_{u}\left(
w\right)  ,2},\ldots,a_{\mathbf{d}_{u}\left(  w\right)  ,m}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A_{\sim u,\bullet}=\left(
a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
m}\right) \\
&  =\left(  a_{w+1,1},a_{w+1,2},\ldots,a_{w+1,m}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{d}_{u}\left(  w\right)
=w+1\right)  .
\end{align*}
Comparing this with%
\begin{align*}
A_{w+1,\bullet}  &  =\left(  \text{the }\left(  w+1\right)  \text{-th row of
the matrix }A\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{w+1,\bullet}\text{ is the }\left(  w+1\right)  \text{-th row
of the matrix }A\\
\text{(by the definition of }A_{w+1,\bullet}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{w+1,1},a_{w+1,2},\ldots,a_{w+1,m}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}\right)  ,
\end{align*}
we obtain $\left(  A_{\sim u,\bullet}\right)  _{w,\bullet}=A_{w+1,\bullet}$.
This proves Proposition \ref{prop.unrows.basics} \textbf{(g)}.

\textbf{(h)} Let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
1,2,\ldots,v-1\right\}  $. From $w\in\left\{  1,2,\ldots,v-1\right\}  $, we
obtain $w\geq1$ and $w\leq v-1$. Thus, $w\leq v-1<v$, so that $w\leq v\leq m$
(since $v\in\left\{  1,2,\ldots,m\right\}  $). Also, $w\in\left\{
1,2,\ldots,v-1\right\}  \subseteq\left\{  1,2,\ldots,m-1\right\}  $ (since
$\underbrace{v}_{\leq m}-1\leq m-1$). From $w\geq1$ and $w\leq m$, we obtain
$w\in\left\{  1,2,\ldots,m\right\}  $. Thus, Proposition
\ref{prop.sol.unrows.2} \textbf{(b)} (applied to $m$, $w$ and $v$ instead of
$n$, $r$ and $s$) shows that
\begin{equation}
\mathbf{d}_{w,v}\left(  j\right)  =\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(
j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }j\in\mathbb{Z}.
\label{pf.prop.unrows.basics.h.1}%
\end{equation}


Now, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Hence,
Proposition \ref{prop.sol.unirows.d} \textbf{(b)} yields
\[
A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}.
\]
Thus, Proposition \ref{prop.sol.unirows.d} \textbf{(b)} (applied to $m-1$,
$A_{\bullet,\sim v}$, $a_{i,\mathbf{d}_{v}\left(  j\right)  }$ and $w$ instead
of $m$, $A$, $a_{i,j}$ and $v$) yields
\begin{align}
\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}  &  =\left(
a_{i,\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq\left(  m-1\right)  -1}=\left(
\underbrace{a_{i,\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)
\right)  }}_{\substack{=a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\\\text{(since
}\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)  =\mathbf{d}%
_{w,v}\left(  j\right)  \\\text{(by (\ref{pf.prop.unrows.basics.h.1})))}%
}}\right)  _{1\leq i\leq n,\ 1\leq j\leq m-2}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  m-1\right)  -1=m-2\right)
\nonumber\\
&  =\left(  a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}. \label{pf.prop.unrows.basics.h.3}%
\end{align}
On the other hand, recall that $w$ and $v$ are two elements of $\left\{
1,2,\ldots,m\right\}  $ and satisfy $w<v$. Hence, Proposition
\ref{prop.sol.unrows.2} \textbf{(a)} (applied to $m$, $w$ and $v$ instead of
$n$, $r$ and $s$) yields%
\[
\left(  1,2,\ldots,\widehat{w},\ldots,\widehat{v},\ldots,m\right)  =\left(
\mathbf{d}_{w,v}\left(  1\right)  ,\mathbf{d}_{w,v}\left(  2\right)
,\ldots,\mathbf{d}_{w,v}\left(  m-2\right)  \right)  .
\]
Thus,%
\begin{align*}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v}%
,\ldots,m}A  &  =\operatorname*{cols}\nolimits_{\mathbf{d}_{w,v}\left(
1\right)  ,\mathbf{d}_{w,v}\left(  2\right)  ,\ldots,\mathbf{d}_{w,v}\left(
m-2\right)  }A=\left(  a_{i,\mathbf{d}_{w,v}\left(  y\right)  }\right)
_{1\leq i\leq n,\ 1\leq y\leq m-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{cols}\nolimits_{\mathbf{d}%
_{w,v}\left(  1\right)  ,\mathbf{d}_{w,v}\left(  2\right)  ,\ldots
,\mathbf{d}_{w,v}\left(  m-2\right)  }A\\
\text{(since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{i,\mathbf{d}_{w,v}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}%
\end{align*}
(here, we have renamed the index $\left(  i,y\right)  $ as $\left(
i,j\right)  $). Comparing this with (\ref{pf.prop.unrows.basics.h.3}), we
obtain $\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}%
=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{v}%
,\ldots,m}A$. This proves Proposition \ref{prop.unrows.basics} \textbf{(h)}.

\textbf{(i)} Let $v\in\left\{  1,2,\ldots,m\right\}  $ and $w\in\left\{
v,v+1,\ldots,m-1\right\}  $. From $w\in\left\{  v,v+1,\ldots,m-1\right\}  $,
we obtain $w\geq v$ and $w\leq m-1$. Now, $w+1>w\geq v$, so that $w+1\geq
v\geq1$ (since $v\in\left\{  1,2,\ldots,m\right\}  $). Combining this with
$w+1\leq m$ (since $w\leq m-1$), we obtain $w+1\in\left\{  1,2,\ldots
,m\right\}  $. Also, $w\in\left\{  v,v+1,\ldots,m-1\right\}  \subseteq\left\{
1,2,\ldots,m-1\right\}  $ (since $v\geq1$). Now, $w+1>v$, so that $v<w+1$.
Thus, Proposition \ref{prop.sol.unrows.2} \textbf{(c)} (applied to $m$, $v$
and $w+1$ instead of $n$, $r$ and $s$) shows that%
\[
\mathbf{d}_{v,w+1}\left(  j\right)  =\mathbf{d}_{v}\left(  \mathbf{d}_{\left(
w+1\right)  -1}\left(  j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}j\in\mathbb{Z}.
\]
In other words,
\begin{equation}
\mathbf{d}_{v,w+1}\left(  j\right)  =\mathbf{d}_{v}\left(  \mathbf{d}%
_{w}\left(  j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
j\in\mathbb{Z} \label{pf.prop.unrows.basics.i.1}%
\end{equation}
(since $\left(  w+1\right)  -1=w$).

Now, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Hence,
Proposition \ref{prop.sol.unirows.d} \textbf{(b)} yields
\[
A_{\bullet,\sim v}=\left(  a_{i,\mathbf{d}_{v}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}.
\]
Thus, Proposition \ref{prop.sol.unirows.d} \textbf{(b)} (applied to $m-1$,
$A_{\bullet,\sim v}$, $a_{i,\mathbf{d}_{v}\left(  j\right)  }$ and $w$ instead
of $m$, $A$, $a_{i,j}$ and $v$) yields
\begin{align}
\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}  &  =\left(
a_{i,\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)  \right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq\left(  m-1\right)  -1}=\left(
\underbrace{a_{i,\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)
\right)  }}_{\substack{=a_{i,\mathbf{d}_{v,w+1}\left(  j\right)
}\\\text{(since }\mathbf{d}_{v}\left(  \mathbf{d}_{w}\left(  j\right)
\right)  =\mathbf{d}_{v,w+1}\left(  j\right)  \\\text{(by
(\ref{pf.prop.unrows.basics.i.1})))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m-2}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  m-1\right)  -1=m-2\right)
\nonumber\\
&  =\left(  a_{i,\mathbf{d}_{v,w+1}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}. \label{pf.prop.unrows.basics.i.3}%
\end{align}


But $v$ and $w+1$ are two elements of $\left\{  1,2,\ldots,m\right\}  $ and
satisfy $v<w+1$. Hence, Proposition \ref{prop.sol.unrows.2} \textbf{(a)}
(applied to $m$, $v$ and $w+1$ instead of $n$, $r$ and $s$) yields%
\[
\left(  1,2,\ldots,\widehat{v},\ldots,\widehat{w+1},\ldots,m\right)  =\left(
\mathbf{d}_{v,w+1}\left(  1\right)  ,\mathbf{d}_{v,w+1}\left(  2\right)
,\ldots,\mathbf{d}_{v,w+1}\left(  m-2\right)  \right)  .
\]
Thus,%
\begin{align*}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w+1}%
,\ldots,m}A  &  =\operatorname*{cols}\nolimits_{\mathbf{d}_{v,w+1}\left(
1\right)  ,\mathbf{d}_{v,w+1}\left(  2\right)  ,\ldots,\mathbf{d}%
_{v,w+1}\left(  m-2\right)  }A\\
&  =\left(  a_{i,\mathbf{d}_{v,w+1}\left(  y\right)  }\right)  _{1\leq i\leq
n,\ 1\leq y\leq m-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{cols}\nolimits_{\mathbf{d}%
_{v,w+1}\left(  1\right)  ,\mathbf{d}_{v,w+1}\left(  2\right)  ,\ldots
,\mathbf{d}_{v,w+1}\left(  m-2\right)  }A\\
\text{(since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\text{)}%
\end{array}
\right) \\
&  =\left(  a_{i,\mathbf{d}_{v,w+1}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m-2}%
\end{align*}
(here, we have renamed the index $\left(  i,y\right)  $ as $\left(
i,j\right)  $). Comparing this with (\ref{pf.prop.unrows.basics.i.3}), we
obtain $\left(  A_{\bullet,\sim v}\right)  _{\bullet,\sim w}%
=\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w+1}%
,\ldots,m}A$. This proves Proposition \ref{prop.unrows.basics} \textbf{(i)}.

\textbf{(j)} Let $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
1,2,\ldots,u-1\right\}  $. From $w\in\left\{  1,2,\ldots,u-1\right\}  $, we
obtain $w\geq1$ and $w\leq u-1$. Thus, $w\leq u-1<u$, so that $w\leq u\leq n$
(since $u\in\left\{  1,2,\ldots,n\right\}  $). Also, $w\in\left\{
1,2,\ldots,u-1\right\}  \subseteq\left\{  1,2,\ldots,n-1\right\}  $ (since
$\underbrace{u}_{\leq n}-1\leq n-1$). From $w\geq1$ and $w\leq n$, we obtain
$w\in\left\{  1,2,\ldots,n\right\}  $. Thus, Proposition
\ref{prop.sol.unrows.2} \textbf{(b)} (applied to $m$, $w$ and $u$ instead of
$n$, $r$ and $s$) shows that
\[
\mathbf{d}_{w,u}\left(  j\right)  =\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(
j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }j\in\mathbb{Z}.
\]
Renaming the index $j$ as $i$ in this result, we obtain the following fact:%
\begin{equation}
\mathbf{d}_{w,u}\left(  i\right)  =\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(
i\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }i\in\mathbb{Z}.
\label{pf.prop.unrows.basics.j.1}%
\end{equation}


Now, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Hence,
Proposition \ref{prop.sol.unirows.d} \textbf{(a)} yields
\[
A_{\sim u,\bullet}=\left(  a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq m}.
\]
Thus, Proposition \ref{prop.sol.unirows.d} \textbf{(a)} (applied to $n-1$,
$A_{\sim u,\bullet}$, $a_{\mathbf{d}_{u}\left(  i\right)  ,j}$ and $w$ instead
of $n$, $A$, $a_{i,j}$ and $u$) yields
\begin{align}
\left(  A_{\sim u,\bullet}\right)  _{\sim w,\bullet}  &  =\left(
a_{\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(  i\right)  \right)  ,j}\right)
_{1\leq i\leq\left(  n-1\right)  -1,\ 1\leq j\leq m}=\left(
\underbrace{a_{\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(  i\right)  \right)
,j}}_{\substack{=a_{\mathbf{d}_{w,u}\left(  i\right)  ,j}\\\text{(since
}\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(  i\right)  \right)  =\mathbf{d}%
_{w,u}\left(  i\right)  \\\text{(by (\ref{pf.prop.unrows.basics.j.1})))}%
}}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq m}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  n-1\right)  -1=n-2\right)
\nonumber\\
&  =\left(  a_{\mathbf{d}_{w,u}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n-2,\ 1\leq j\leq m}. \label{pf.prop.unrows.basics.j.3}%
\end{align}
On the other hand, recall that $w$ and $u$ are two elements of $\left\{
1,2,\ldots,n\right\}  $ and satisfy $w<u$. Hence, Proposition
\ref{prop.sol.unrows.2} \textbf{(a)} (applied to $w$ and $u$ instead of $r$
and $s$) yields%
\[
\left(  1,2,\ldots,\widehat{w},\ldots,\widehat{u},\ldots,n\right)  =\left(
\mathbf{d}_{w,u}\left(  1\right)  ,\mathbf{d}_{w,u}\left(  2\right)
,\ldots,\mathbf{d}_{w,u}\left(  n-2\right)  \right)  .
\]
Thus,%
\begin{align*}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{u}%
,\ldots,n}A  &  =\operatorname*{rows}\nolimits_{\mathbf{d}_{w,u}\left(
1\right)  ,\mathbf{d}_{w,u}\left(  2\right)  ,\ldots,\mathbf{d}_{w,u}\left(
n-2\right)  }A=\left(  a_{\mathbf{d}_{w,u}\left(  x\right)  ,j}\right)
_{1\leq x\leq n-2,\ 1\leq j\leq m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\operatorname*{rows}\nolimits_{\mathbf{d}_{w,v}\left(  1\right)
,\mathbf{d}_{w,v}\left(  2\right)  ,\ldots,\mathbf{d}_{w,v}\left(  n-2\right)
}A\right) \\
&  =\left(  a_{\mathbf{d}_{w,u}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n-2,\ 1\leq j\leq m}%
\end{align*}
(here, we have renamed the index $\left(  x,j\right)  $ as $\left(
i,j\right)  $). Comparing this with (\ref{pf.prop.unrows.basics.j.3}), we
obtain $\left(  A_{\sim u,\bullet}\right)  _{\sim w,\bullet}%
=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{w},\ldots,\widehat{u}%
,\ldots,n}A$. This proves Proposition \ref{prop.unrows.basics} \textbf{(j)}.

\textbf{(k)} Let $u\in\left\{  1,2,\ldots,n\right\}  $ and $w\in\left\{
u,u+1,\ldots,n-1\right\}  $. From $w\in\left\{  u,u+1,\ldots,n-1\right\}  $,
we obtain $w\geq u$ and $w\leq n-1$. But $u\geq1$ (since $u\in\left\{
1,2,\ldots,n\right\}  $). Hence, $w\geq u\geq1$. Therefore, $w+1\geq w\geq1$.
Combining this with $w+1\leq n$ (since $w\leq n-1$), we obtain $w+1\in\left\{
1,2,\ldots,n\right\}  $. Also, $w\in\left\{  u,u+1,\ldots,n-1\right\}
\subseteq\left\{  1,2,\ldots,n-1\right\}  $ (since $u\geq1$). Now, $w+1>w\geq
u$, so that $u<w+1$. Thus, Proposition \ref{prop.sol.unrows.2} \textbf{(c)}
(applied to $m$, $u$ and $w+1$ instead of $n$, $r$ and $s$) shows that%
\[
\mathbf{d}_{u,w+1}\left(  j\right)  =\mathbf{d}_{u}\left(  \mathbf{d}_{\left(
w+1\right)  -1}\left(  j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every
}j\in\mathbb{Z}.
\]
In other words,
\[
\mathbf{d}_{u,w+1}\left(  j\right)  =\mathbf{d}_{u}\left(  \mathbf{d}%
_{w}\left(  j\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
j\in\mathbb{Z}%
\]
(since $\left(  w+1\right)  -1=w$). Renaming the index $j$ as $i$ in this
result, we obtain the following fact:%
\begin{equation}
\mathbf{d}_{u,w+1}\left(  i\right)  =\mathbf{d}_{u}\left(  \mathbf{d}%
_{w}\left(  i\right)  \right)  \ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\mathbb{Z}. \label{pf.prop.unrows.basics.k.1}%
\end{equation}


Now, $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Hence,
Proposition \ref{prop.sol.unirows.d} \textbf{(a)} yields
\[
A_{\sim u,\bullet}=\left(  a_{\mathbf{d}_{u}\left(  i\right)  ,j}\right)
_{1\leq i\leq n-1,\ 1\leq j\leq m}.
\]
Thus, Proposition \ref{prop.sol.unirows.d} \textbf{(a)} (applied to $n-1$,
$A_{\sim u,\bullet}$, $a_{\mathbf{d}_{u}\left(  i\right)  ,j}$ and $w$ instead
of $n$, $A$, $a_{i,j}$ and $u$) yields
\begin{align}
\left(  A_{\sim u,\bullet}\right)  _{\sim w,\bullet}  &  =\left(
a_{\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(  i\right)  \right)  ,j}\right)
_{1\leq i\leq\left(  n-1\right)  -1,\ 1\leq j\leq m}=\left(
\underbrace{a_{\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(  i\right)  \right)
,j}}_{\substack{=a_{\mathbf{d}_{u,w+1}\left(  i\right)  ,j}\\\text{(since
}\mathbf{d}_{u}\left(  \mathbf{d}_{w}\left(  i\right)  \right)  =\mathbf{d}%
_{u,w+1}\left(  i\right)  \\\text{(by (\ref{pf.prop.unrows.basics.k.1})))}%
}}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq m}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  n-1\right)  -1=n-2\right)
\nonumber\\
&  =\left(  a_{\mathbf{d}_{u,w+1}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n-2,\ 1\leq j\leq m}. \label{pf.prop.unrows.basics.k.3}%
\end{align}
On the other hand, recall that $u$ and $w+1$ are two elements of $\left\{
1,2,\ldots,n\right\}  $ and satisfy $u<w+1$. Hence, Proposition
\ref{prop.sol.unrows.2} \textbf{(a)} (applied to $u$ and $w+1$ instead of $r$
and $s$) yields%
\[
\left(  1,2,\ldots,\widehat{u},\ldots,\widehat{w+1},\ldots,n\right)  =\left(
\mathbf{d}_{u,w+1}\left(  1\right)  ,\mathbf{d}_{u,w+1}\left(  2\right)
,\ldots,\mathbf{d}_{u,w+1}\left(  n-2\right)  \right)  .
\]
Thus,%
\begin{align*}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{w+1}%
,\ldots,n}A  &  =\operatorname*{rows}\nolimits_{\mathbf{d}_{u,w+1}\left(
1\right)  ,\mathbf{d}_{u,w+1}\left(  2\right)  ,\ldots,\mathbf{d}%
_{u,w+1}\left(  n-2\right)  }A\\
&  =\left(  a_{\mathbf{d}_{u,w+1}\left(  x\right)  ,j}\right)  _{1\leq x\leq
n-2,\ 1\leq j\leq m}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }%
\operatorname*{rows}\nolimits_{\mathbf{d}_{u,w+1}\left(  1\right)
,\mathbf{d}_{u,w+1}\left(  2\right)  ,\ldots,\mathbf{d}_{u,w+1}\left(
n-2\right)  }A\right) \\
&  =\left(  a_{\mathbf{d}_{u,w+1}\left(  i\right)  ,j}\right)  _{1\leq i\leq
n-2,\ 1\leq j\leq m}%
\end{align*}
(here, we have renamed the index $\left(  x,j\right)  $ as $\left(
i,j\right)  $). Comparing this with (\ref{pf.prop.unrows.basics.k.3}), we
obtain $\left(  A_{\sim u,\bullet}\right)  _{\sim w,\bullet}%
=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{w+1}%
,\ldots,n}A$. This proves Proposition \ref{prop.unrows.basics} \textbf{(k)}.

\textbf{(l)} Let $v\in\left\{  1,2,\ldots,n\right\}  $, $u\in\left\{
1,2,\ldots,n\right\}  $ and $q\in\left\{  1,2,\ldots,m\right\}  $ be such that
$u<v$. We have $u<v$ and thus $u\leq v-1$ (since $u$ and $v$ are integers).
Combined with $u\geq1$ (since $u\in\left\{  1,2,\ldots,n\right\}  $), this
shows that $u\in\left\{  1,2,\ldots,v-1\right\}  $.

Proposition \ref{prop.unrows.basics} \textbf{(c)} (applied to $v$ and $q$
instead of $u$ and $v$) yields $\left(  A_{\bullet,\sim q}\right)  _{\sim
v,\bullet}=\left(  A_{\sim v,\bullet}\right)  _{\bullet,\sim q}=A_{\sim v,\sim
q}$.

Now, $u\leq\underbrace{v}_{\substack{\leq n\\\text{(since }v\in\left\{
1,2,\ldots,n\right\}  \text{)}}}-1\leq n-1$. Combining this with $u\geq1$
(since $u\in\left\{  1,2,\ldots,n\right\}  $), we obtain $u\in\left\{
1,2,\ldots,n-1\right\}  $. Hence, Proposition \ref{prop.unrows.basics}
\textbf{(c)} (applied to $n-1$, $A_{\sim v,\bullet}$ and $q$ instead of $n$,
$A$ and $v$) yields $\left(  \left(  A_{\sim v,\bullet}\right)  _{\bullet,\sim
q}\right)  _{\sim u,\bullet}=\left(  \left(  A_{\sim v,\bullet}\right)  _{\sim
u,\bullet}\right)  _{\bullet,\sim q}=\left(  A_{\sim v,\bullet}\right)  _{\sim
u,\sim q}$. Hence,%
\begin{align*}
\left(  A_{\sim v,\bullet}\right)  _{\sim u,\sim q}  &  =\left(
\underbrace{\left(  A_{\sim v,\bullet}\right)  _{\bullet,\sim q}}_{=\left(
A_{\bullet,\sim q}\right)  _{\sim v,\bullet}}\right)  _{\sim u,\bullet
}=\left(  \left(  A_{\bullet,\sim q}\right)  _{\sim v,\bullet}\right)  _{\sim
u,\bullet}\\
&  =\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots
,\widehat{v},\ldots,n}\left(  A_{\bullet,\sim q}\right)
\end{align*}
(by Proposition \ref{prop.unrows.basics} \textbf{(j)}, applied to $m-1$,
$A_{\bullet,\sim q}$, $v$ and $u$ instead of $m$, $A$, $u$ and $w$). This
proves Proposition \ref{prop.unrows.basics} \textbf{(l)}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.unrows.basics-I}.]We have $u<v\leq n$ (since
$v\in\left\{  1,2,\ldots,n\right\}  $) and thus $u\leq n-1$ (since $u$ and $n$
are integers). Combining this with $u\geq1$ (since $u\in\left\{
1,2,\ldots,n\right\}  $), we obtain $u\in\left\{  1,2,\ldots,n-1\right\}  $.
Hence, the column vector $\left(  I_{n-1}\right)  _{\bullet,u}\in
\mathbb{K}^{\left(  n-1\right)  \times1}$ is well-defined. Also, the column
vector $\left(  I_{n}\right)  _{\bullet,u}\in\mathbb{K}^{n\times1}$ is
well-defined (since $u\in\left\{  1,2,\ldots,n\right\}  $), and therefore the
column vector $\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim
v,\bullet}\in\mathbb{K}^{\left(  n-1\right)  \times1}$ is well-defined.

For any two objects $i$ and $j$, we define an element $\delta_{i,j}%
\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, every $m\in\mathbb{N}$ satisfies%
\begin{equation}
I_{m}=\left(  \delta_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}
\label{pf.prop.unrows.basics-I.Im}%
\end{equation}
(by the definition of $I_{m}$). Applying this to $m=n$, we obtain
$I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Now,
$\left(  I_{n}\right)  _{\bullet,u}$ is the $u$-th column of the matrix
$I_{n}$ (by the definition of $\left(  I_{n}\right)  _{\bullet,u}$). Thus,%
\begin{align*}
\left(  I_{n}\right)  _{\bullet,u}  &  =\left(  \text{the }u\text{-th column
of the matrix }I_{n}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\delta_{1,u}\\
\delta_{2,u}\\
\vdots\\
\delta_{n,u}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I_{n}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \delta_{i,u}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\end{align*}
Hence, Proposition \ref{prop.sol.unirows.d} \textbf{(a)} (applied to $1$,
$\left(  I_{n}\right)  _{\bullet,u}$, $\delta_{i,u}$ and $v$ instead of $m$,
$A$, $a_{i,j}$ and $u$) yields%
\begin{equation}
\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}=\left(
\delta_{\mathbf{d}_{v}\left(  i\right)  ,u}\right)  _{1\leq i\leq n-1,\ 1\leq
j\leq1}. \label{pf.prop.unrows.basics-I.1}%
\end{equation}


But every $i\in\mathbb{Z}$ satisfies%
\begin{equation}
\delta_{\mathbf{d}_{v}\left(  i\right)  ,u}=\delta_{i,u}
\label{pf.prop.unrows.basics-I.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.unrows.basics-I.1}):} Let
$i\in\mathbb{Z}$. We must prove the equality (\ref{pf.prop.unrows.basics-I.2}%
).
\par
The definition of $\mathbf{d}_{v}\left(  i\right)  $ yields $\mathbf{d}%
_{v}\left(  i\right)  =%
\begin{cases}
i, & \text{if }i<v;\\
i+1, & \text{if }i\geq v
\end{cases}
$. But $u<v$, and thus $v>u$.
\par
We are in one of the following two Cases:
\par
\textit{Case 1:} We have $i<v$.
\par
\textit{Case 2:} We have $i\geq v$.
\par
Let us first consider Case 1. In this case, we have $i<v$. Now, $\mathbf{d}%
_{v}\left(  i\right)  =%
\begin{cases}
i, & \text{if }i<v;\\
i+1, & \text{if }i\geq v
\end{cases}
=i$ (since $i<v$) and thus $\delta_{\mathbf{d}_{v}\left(  i\right)  ,u}%
=\delta_{i,u}$. Hence, (\ref{pf.prop.unrows.basics-I.2}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i\geq v$. Now,
$\mathbf{d}_{v}\left(  i\right)  =%
\begin{cases}
i, & \text{if }i<v;\\
i+1, & \text{if }i\geq v
\end{cases}
=i+1$ (since $i\geq v$) and thus $\mathbf{d}_{v}\left(  i\right)  =i+1>i\geq
v>u$. Hence, $\mathbf{d}_{v}\left(  i\right)  \neq u$. Thus, $\delta
_{\mathbf{d}_{v}\left(  i\right)  ,u}=0$. Also, $i\geq v>u$ and thus $i\neq
u$; hence, $\delta_{i,u}=0$. Now, $\delta_{\mathbf{d}_{v}\left(  i\right)
,u}=0=\delta_{i,u}$. Hence, (\ref{pf.prop.unrows.basics-I.2}) is proven in
Case 2.
\par
We have now proven (\ref{pf.prop.unrows.basics-I.2}) in each of the two Cases
1 and 2. Since these two Cases cover all possibilities, this shows that
(\ref{pf.prop.unrows.basics-I.2}) always holds.}. Hence,
(\ref{pf.prop.unrows.basics-I.1}) becomes%
\begin{equation}
\left(  \left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}=\left(
\underbrace{\delta_{\mathbf{d}_{v}\left(  i\right)  ,u}}_{\substack{=\delta
_{i,u}\\\text{(by (\ref{pf.prop.unrows.basics-I.2}))}}}\right)  _{1\leq i\leq
n-1,\ 1\leq j\leq1}=\left(  \delta_{i,u}\right)  _{1\leq i\leq n-1,\ 1\leq
j\leq1}. \label{pf.prop.unrows.basics-I.3}%
\end{equation}


But (\ref{pf.prop.unrows.basics-I.Im}) (applied to $m=n-1$) yields
$I_{n-1}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}$.
Now, $\left(  I_{n-1}\right)  _{\bullet,u}$ is the $u$-th column of the matrix
$I_{n-1}$ (by the definition of $\left(  I_{n-1}\right)  _{\bullet,u}$). Thus,%
\begin{align*}
\left(  I_{n-1}\right)  _{\bullet,u}  &  =\left(  \text{the }u\text{-th column
of the matrix }I_{n-1}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\delta_{1,u}\\
\delta_{2,u}\\
\vdots\\
\delta_{n-1,u}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I_{n-1}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq n-1}\right) \\
&  =\left(  \delta_{i,u}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq1}.
\end{align*}
Comparing this with (\ref{pf.prop.unrows.basics-I.3}), we obtain $\left(
\left(  I_{n}\right)  _{\bullet,u}\right)  _{\sim v,\bullet}=\left(
I_{n-1}\right)  _{\bullet,u}$. This proves Proposition
\ref{prop.unrows.basics-I}.
\end{proof}

We have now proven Proposition \ref{prop.unrows.basics} and Proposition
\ref{prop.unrows.basics-I}. Thus, Exercise \ref{exe.unrows.basics}
\textbf{(a)} is solved.

Next, let us derive Proposition \ref{prop.desnanot.12} and Proposition
\ref{prop.desnanot.1n} from Theorem \ref{thm.desnanot}:

\begin{proof}
[Proof of Proposition \ref{prop.desnanot.12} using Theorem \ref{thm.desnanot}%
.]We have%
\begin{equation}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{2}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n}A=\widetilde{A}
\label{pf.prop.desnanot.12.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.desnanot.12.1}):} The definition of
$\left(  1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n\right)  $ yields%
\begin{align*}
&  \left(  1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n\right) \\
&  =\left(  \underbrace{1,2,\ldots,1-1}_{\substack{\text{all integers}%
\\\text{from }1\text{ to }1-1}},\underbrace{1+1,1+2,\ldots,2-1}%
_{\substack{\text{all integers}\\\text{from }1+1\text{ to }2-1}%
},\underbrace{2+1,2+2,\ldots,n}_{\substack{\text{all integers}\\\text{from
}2+1\text{ to }n}}\right) \\
&  =\left(  \underbrace{\underbrace{1,2,\ldots,0}_{\substack{\text{all
integers}\\\text{from }1\text{ to }0}}}_{\text{this is an empty list}%
},\underbrace{\underbrace{2,3,\ldots,1}_{\substack{\text{all integers}%
\\\text{from }2\text{ to }1}}}_{\text{this is an empty list}}%
,\underbrace{3,4,\ldots,n}_{\substack{\text{all integers}\\\text{from }3\text{
to }n}}\right) \\
&  =\left(  \underbrace{3,4,\ldots,n}_{\substack{\text{all integers}%
\\\text{from }3\text{ to }n}}\right)  =\left(  3,4,\ldots,n\right)  =\left(
1+2,2+2,\ldots,\left(  n-2\right)  +2\right)  .
\end{align*}
Hence,%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{2}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{2},\ldots,n}A  &
=\operatorname*{sub}\nolimits_{1+2,2+2,\ldots,\left(  n-2\right)
+2}^{1+2,2+2,\ldots,\left(  n-2\right)  +2}A=\left(  a_{x+2,y+2}\right)
_{1\leq x\leq n-2,\ 1\leq y\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{sub}\nolimits_{1+2,2+2,\ldots
,\left(  n-2\right)  +2}^{1+2,2+2,\ldots,\left(  n-2\right)  +2}A\text{,}\\
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\end{array}
\right) \\
&  =\left(  a_{i+2,j+2}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right) \\
&  =\widetilde{A}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{A}%
=\left(  a_{i+2,j+2}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\right)  .
\end{align*}
Qed.}. Now, $1<2$ and $1<2$. Moreover, $1$ and $2$ are elements of $\left\{
1,2,\ldots,n\right\}  $ (since $n\geq2$). Hence, Theorem \ref{thm.desnanot}
(applied to $p=1$, $q=2$, $u=1$ and $v=2$) yields%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{1},\ldots,\widehat{2},\ldots,n}^{1,2,\ldots,\widehat{1}%
,\ldots,\widehat{2},\ldots,n}A\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim2,\sim
2}\right)  -\det\left(  A_{\sim1,\sim2}\right)  \cdot\det\left(  A_{\sim
2,\sim1}\right)  .
\end{align*}
In view of (\ref{pf.prop.desnanot.12.1}), this rewrites as
\begin{align*}
&  \det A\cdot\det\widetilde{A}\\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim2,\sim
2}\right)  -\det\left(  A_{\sim1,\sim2}\right)  \cdot\det\left(  A_{\sim
2,\sim1}\right)  .
\end{align*}
This proves Proposition \ref{prop.desnanot.12}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.desnanot.1n} using Theorem \ref{thm.desnanot}%
.]We have%
\begin{equation}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{n}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n}A=A^{\prime}
\label{pf.prop.desnanot.1n.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.desnanot.1n.1}):} The definition of
$\left(  1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n\right)  $ yields%
\begin{align*}
&  \left(  1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n\right) \\
&  =\left(  \underbrace{\underbrace{1,2,\ldots,1-1}_{\substack{\text{all
integers}\\\text{from }1\text{ to }1-1}}}_{\text{this is an empty list}%
},\underbrace{1+1,1+2,\ldots,n-1}_{\substack{\text{all integers}\\\text{from
}1+1\text{ to }n-1}},\underbrace{\underbrace{n+1,n+2,\ldots,n}%
_{\substack{\text{all integers}\\\text{from }n+1\text{ to }n}}}_{\text{this is
an empty list}}\right) \\
&  =\left(  \underbrace{1+1,1+2,\ldots,n-1}_{\substack{\text{all
integers}\\\text{from }1+1\text{ to }n-1}}\right)  =\left(  1+1,1+2,\ldots
,n-1\right) \\
&  =\left(  2,3,\ldots,n-1\right)  =\left(  1+1,2+1,\ldots,\left(  n-2\right)
+1\right)  .
\end{align*}
Hence,%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{1},\ldots,\widehat{n}%
,\ldots,n}^{1,2,\ldots,\widehat{1},\ldots,\widehat{n},\ldots,n}A  &
=\operatorname*{sub}\nolimits_{1+1,2+1,\ldots,\left(  n-2\right)
+1}^{1+1,2+1,\ldots,\left(  n-2\right)  +1}A=\left(  a_{x+1,y+1}\right)
_{1\leq x\leq n-2,\ 1\leq y\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{sub}\nolimits_{1+1,2+1,\ldots
,\left(  n-2\right)  +1}^{1+1,2+1,\ldots,\left(  n-2\right)  +1}A\text{,}\\
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\end{array}
\right) \\
&  =\left(  a_{i+1,j+1}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }\left(
x,y\right)  \text{ as }\left(  i,j\right)  \right) \\
&  =A^{\prime}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }A^{\prime}=\left(
a_{i+1,j+1}\right)  _{1\leq i\leq n-2,\ 1\leq j\leq n-2}\right)  .
\end{align*}
Qed.}. Now, $1<n$ (since $n\geq2$) and $1<n$. Moreover, $1$ and $n$ are
elements of $\left\{  1,2,\ldots,n\right\}  $ (since $n\geq2\geq1$). Hence,
Theorem \ref{thm.desnanot} (applied to $p=1$, $q=n$, $u=1$ and $v=n$) yields%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{1},\ldots,\widehat{n},\ldots,n}^{1,2,\ldots,\widehat{1}%
,\ldots,\widehat{n},\ldots,n}A\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim n,\sim
n}\right)  -\det\left(  A_{\sim1,\sim n}\right)  \cdot\det\left(  A_{\sim
n,\sim1}\right)  .
\end{align*}
In view of (\ref{pf.prop.desnanot.1n.1}), this rewrites as
\begin{align*}
&  \det A\cdot\det\left(  A^{\prime}\right) \\
&  =\det\left(  A_{\sim1,\sim1}\right)  \cdot\det\left(  A_{\sim n,\sim
n}\right)  -\det\left(  A_{\sim1,\sim n}\right)  \cdot\det\left(  A_{\sim
n,\sim1}\right)  .
\end{align*}
This proves Proposition \ref{prop.desnanot.1n}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.unrows.basics}.]\textbf{(a)} We proven
Proposition \ref{prop.unrows.basics} and Proposition
\ref{prop.unrows.basics-I} above. Thus, Exercise \ref{exe.unrows.basics}
\textbf{(a)} is solved.

\textbf{(b)} We have derived Proposition \ref{prop.desnanot.12} and
Proposition \ref{prop.desnanot.1n} from Theorem \ref{thm.desnanot} above.
Thus, Exercise \ref{exe.unrows.basics} \textbf{(b)} is solved.
\end{proof}
\end{verlong}

\subsection{Solution to Exercise \ref{exe.prop.addcol.props}}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.addcol.props1}.]The matrix $\left(  A\mid
v\right)  $ is defined as the $n\times\left(  m+1\right)  $-matrix whose $m+1$
columns are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$ (from left to
right). Thus, the first $m$ columns of the matrix $\left(  A\mid v\right)  $
are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m}$, whereas the $\left(
m+1\right)  $-st column of $\left(  A\mid v\right)  $ is $v$.

\textbf{(a)} For every $q\in\left\{  1,2,\ldots,m\right\}  $, we have%
\begin{align*}
\left(  A\mid v\right)  _{\bullet,q}  &  =\left(  \text{the }q\text{-th column
of the matrix }\left(  A\mid v\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  A\mid
v\right)  _{\bullet,q}\right) \\
&  =A_{\bullet,q}%
\end{align*}
(since the first $m$ columns of the matrix $\left(  A\mid v\right)  $ are
$A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m}$). This proves Proposition
\ref{prop.addcol.props1} \textbf{(a)}.

\textbf{(b)} The definition of $\left(  A\mid v\right)  _{\bullet,m+1}$ yields%
\[
\left(  A\mid v\right)  _{\bullet,m+1}=\left(  \text{the }\left(  m+1\right)
\text{-st column of the matrix }\left(  A\mid v\right)  \right)  =v
\]
(since the $\left(  m+1\right)  $-st column of $\left(  A\mid v\right)  $ is
$v$). This proves Proposition \ref{prop.addcol.props1} \textbf{(b)}.

\textbf{(c)} Let $q\in\left\{  1,2,\ldots,m\right\}  $. Recall that $\left(
A\mid v\right)  $ is the $n\times\left(  m+1\right)  $-matrix whose $m+1$
columns are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$. The matrix
$\left(  A\mid v\right)  _{\bullet,\sim q}$ is obtained from this matrix
$\left(  A\mid v\right)  $ by removing its $q$-th column; thus,
\begin{equation}
\text{the columns of this matrix }\left(  A\mid v\right)  _{\bullet,\sim
q}\text{ are }A_{\bullet,1},A_{\bullet,2},\ldots,\widehat{A_{\bullet,q}%
},\ldots,A_{\bullet,m},v \label{pf.prop.addcol.props1.short.c.1}%
\end{equation}
(since the columns of the matrix $\left(  A\mid v\right)  $ are $A_{\bullet
,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$, and since $q\in\left\{
1,2,\ldots,m\right\}  $).

On the other hand, the matrix $A_{\bullet,\sim q}$ is obtained from the matrix
$A$ by removing its $q$-th column; thus, the columns of this matrix
$A_{\bullet,\sim q}$ are $A_{\bullet,1},A_{\bullet,2},\ldots
,\widehat{A_{\bullet,q}},\ldots,A_{\bullet,m}$. Hence,
\begin{equation}
\text{the columns of the matrix }\left(  A_{\bullet,\sim q}\mid v\right)
\text{ are }A_{\bullet,1},A_{\bullet,2},\ldots,\widehat{A_{\bullet,q}}%
,\ldots,A_{\bullet,m},v \label{pf.prop.addcol.props1.short.c.2}%
\end{equation}
(since the matrix $\left(  A_{\bullet,\sim q}\mid v\right)  $ is obtained from
$A_{\bullet,\sim q}$ by attaching the column $v$ at its \textquotedblleft
right edge\textquotedblright).

Comparing (\ref{pf.prop.addcol.props1.short.c.1}) with
(\ref{pf.prop.addcol.props1.short.c.2}), we see that the columns of the matrix
$\left(  A\mid v\right)  _{\bullet,\sim q}$ are precisely the columns of the
matrix $\left(  A_{\bullet,\sim q}\mid v\right)  $. Thus, these two matrices
must be identical. In other words, $\left(  A\mid v\right)  _{\bullet,\sim
q}=\left(  A_{\bullet,\sim q}\mid v\right)  $. This proves Proposition
\ref{prop.addcol.props1} \textbf{(c)}.

\textbf{(d)} We have defined $\left(  A\mid v\right)  $ as the $n\times\left(
m+1\right)  $-matrix whose $m+1$ columns are $A_{\bullet,1},A_{\bullet
,2},\ldots,A_{\bullet,m},v$. The matrix $\left(  A\mid v\right)
_{\bullet,\sim\left(  m+1\right)  }$ is obtained from this matrix $\left(
A\mid v\right)  $ by removing its $\left(  m+1\right)  $-th column; thus,
\begin{equation}
\text{the columns of this matrix }\left(  A\mid v\right)  _{\bullet
,\sim\left(  m+1\right)  }\text{ are }A_{\bullet,1},A_{\bullet,2}%
,\ldots,A_{\bullet,m} \label{pf.prop.addcol.props1.short.d.1}%
\end{equation}
(since the columns of the matrix $\left(  A\mid v\right)  $ are $A_{\bullet
,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$). On the other hand, clearly,%
\begin{equation}
\text{the columns of the matrix }A\text{ are }A_{\bullet,1},A_{\bullet
,2},\ldots,A_{\bullet,m}. \label{pf.prop.addcol.props1.short.d.2}%
\end{equation}
Comparing (\ref{pf.prop.addcol.props1.short.d.1}) with
(\ref{pf.prop.addcol.props1.short.d.2}), we see that the columns of the matrix
$\left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }$ are precisely
the columns of the matrix $A$. Thus, these two matrices must be identical. In
other words, $\left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }=A$.
This proves Proposition \ref{prop.addcol.props1} \textbf{(d)}.

\textbf{(e)} Here is an informal proof: Let $p\in\left\{  1,2,\ldots
,n\right\}  $. The matrix $\left(  A\mid v\right)  $ is obtained from the
matrix $A$ by attaching the column $v$ at its \textquotedblleft right
edge\textquotedblright. The matrix $\left(  A\mid v\right)  _{\sim p,\bullet}$
is obtained from the matrix $\left(  A\mid v\right)  $ by removing its $p$-th
row. Hence, the matrix $\left(  A\mid v\right)  _{\sim p,\bullet}$ is obtained
from the matrix $A$ by first attaching the column $v$ at its \textquotedblleft
right edge\textquotedblright\ and then removing the $p$-th row. On the other
hand, the matrix $\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  $
is obtained from the matrix $A$ by first removing the $p$-th row (this is how
we get $A_{\sim p,\bullet}$), and then attaching the column $v_{\sim
p,\bullet}$ (this is $v$ with its $p$-th row removed) at its \textquotedblleft
right edge\textquotedblright. Obviously, the two procedures result in the same
matrix, i.e., we have $\left(  A\mid v\right)  _{\sim p,\bullet}=\left(
A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  $. Thus, Proposition
\ref{prop.addcol.props1} \textbf{(e)} is proven.\footnote{Here is an outline
of a more formal proof: First, we observe that if $A$ is an $n\times
m$-matrix, if $p\in\left\{  1,2,\ldots,n\right\}  $ and if $q\in\left\{
1,2,\ldots,m\right\}  $, then%
\begin{equation}
\left(  A_{\sim p,\bullet}\right)  _{\bullet,q}=\left(  A_{\bullet,q}\right)
_{\sim p,\bullet}. \label{pf.prop.addcol.props1.short.e.fn1.pf.1}%
\end{equation}
(Indeed, this is a simple fact that is similar to the statements of
Proposition \ref{prop.unrows.basics}. What it says is that if you remove the
$p$-th row of the matrix $A$ and then take the $q$-th column of the resulting
matrix, then you get the same result as when you first take the $q$-th column
of $A$ and then remove the $p$-th row of this column.)
\par
Now, let us prove Proposition \ref{prop.addcol.props1} \textbf{(e)} formally:
Both $\left(  A\mid v\right)  _{\sim p,\bullet}$ and $\left(  A_{\sim
p,\bullet}\mid v_{\sim p,\bullet}\right)  $ are $\left(  n-1\right)
\times\left(  m+1\right)  $-matrices (since they have one fewer row and one
more column than $A$). We shall now show that
\begin{equation}
\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)  _{\bullet,q}=\left(
A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  _{\bullet,q}
\label{pf.prop.addcol.props1.short.e.fn1.pf.8}%
\end{equation}
for every $q\in\left\{  1,2,\ldots,m+1\right\}  $.
\par
\textit{Proof of (\ref{pf.prop.addcol.props1.short.e.fn1.pf.8}):} Let
$q\in\left\{  1,2,\ldots,m+1\right\}  $. We must prove
(\ref{pf.prop.addcol.props1.short.e.fn1.pf.8}).
\par
We can apply (\ref{pf.prop.addcol.props1.short.e.fn1.pf.1}) to $m+1$ and
$\left(  A\mid v\right)  $ instead of $m$ and $A$. As a result, we obtain%
\begin{equation}
\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)  _{\bullet,q}=\left(
\left(  A\mid v\right)  _{\bullet,q}\right)  _{\sim p,\bullet}.
\label{pf.prop.addcol.props1.short.e.fn1.pf.8.pf.1}%
\end{equation}
\par
We are in one of the following two cases:
\par
\textit{Case 1:} We have $q\neq m+1$.
\par
\textit{Case 2:} We have $q=m+1$.
\par
Let us consider Case 1 first. In this case, we have $q\neq m+1$. Hence,
$q\in\left\{  1,2,\ldots,m\right\}  $ (since $q\in\left\{  1,2,\ldots
,m+1\right\}  $). Thus, Proposition \ref{prop.addcol.props1} \textbf{(a)}
yields $\left(  A\mid v\right)  _{\bullet,q}=A_{\bullet,q}$. Now,
(\ref{pf.prop.addcol.props1.short.e.fn1.pf.8.pf.1}) becomes%
\[
\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)  _{\bullet,q}=\left(
\underbrace{\left(  A\mid v\right)  _{\bullet,q}}_{=A_{\bullet,q}}\right)
_{\sim p,\bullet}=\left(  A_{\bullet,q}\right)  _{\sim p,\bullet}.
\]
Comparing this with%
\begin{align*}
\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  _{\bullet,q}  &
=\left(  A_{\sim p,\bullet}\right)  _{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.addcol.props1} \textbf{(a)}, applied to}\\
n-1\text{, }A_{\sim p,\bullet}\text{ and }v_{\sim p,\bullet}\text{ instead of
}n\text{, }A\text{ and }v
\end{array}
\right) \\
&  =\left(  A_{\bullet,q}\right)  _{\sim p,\bullet}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.prop.addcol.props1.short.e.fn1.pf.1})}\right)  ,
\end{align*}
we obtain $\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)
_{\bullet,q}=\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)
_{\bullet,q}$. Thus, (\ref{pf.prop.addcol.props1.short.e.fn1.pf.8}) is proven
in Case 1.
\par
Let us now consider Case 2. In this case, we have $q=m+1$. Hence, $\left(
A\mid v\right)  _{\bullet,q}=\left(  A\mid v\right)  _{\bullet,m+1}=v$ (by
Proposition \ref{prop.addcol.props1} \textbf{(b)}). Now,
(\ref{pf.prop.addcol.props1.short.e.fn1.pf.8.pf.1}) becomes%
\[
\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)  _{\bullet,q}=\left(
\underbrace{\left(  A\mid v\right)  _{\bullet,q}}_{=v}\right)  _{\sim
p,\bullet}=v_{\sim p,\bullet}.
\]
Comparing this with%
\begin{align*}
\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  _{\bullet,q}  &
=\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  _{\bullet
,m+1}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }q=m+1\right) \\
&  =v_{\sim p,\bullet}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.addcol.props1} \textbf{(b)}, applied to}\\
n-1\text{, }A_{\sim p,\bullet}\text{ and }v_{\sim p,\bullet}\text{ instead of
}n\text{, }A\text{ and }v
\end{array}
\right)  ,
\end{align*}
we obtain $\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)
_{\bullet,q}=\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)
_{\bullet,q}$. Thus, (\ref{pf.prop.addcol.props1.short.e.fn1.pf.8}) is proven
in Case 2.
\par
Now we have proven (\ref{pf.prop.addcol.props1.short.e.fn1.pf.8}) in each of
the two Cases 1 and 2. Thus, the proof of
(\ref{pf.prop.addcol.props1.short.e.fn1.pf.8}) is complete.
\par
Now, for every $q\in\left\{  1,2,\ldots,m+1\right\}  $, we have%
\begin{align*}
&  \left(  \text{the }q\text{-th column of the matrix }\left(  A\mid v\right)
_{\sim p,\bullet}\right) \\
&  =\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)  _{\bullet,q}\\
&  =\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  _{\bullet
,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.addcol.props1.short.e.fn1.pf.8})}\right) \\
&  =\left(  \text{the }q\text{-th column of the matrix }\left(  A_{\sim
p,\bullet}\mid v_{\sim p,\bullet}\right)  \right)  .
\end{align*}
This shows that the matrices $\left(  A\mid v\right)  _{\sim p,\bullet}$ and
$\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  $ are identical
(since they are both $\left(  n-1\right)  \times\left(  m+1\right)
$-matrices). Proposition \ref{prop.addcol.props1} \textbf{(e)} is thus
proven.}

\textbf{(f)} Let $p\in\left\{  1,2,\ldots,n\right\}  $. Then, $\left(  A\mid
v\right)  $ is an $n\times\left(  m+1\right)  $-matrix. Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to $m+1$, $\left(  A\mid
v\right)  $, $p$ and $m+1$ instead of $u$ and $v$) yields
\[
\left(  \left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }\right)
_{\sim p,\bullet}=\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)
_{\bullet,\sim\left(  m+1\right)  }=\left(  A\mid v\right)  _{\sim
p,\sim\left(  m+1\right)  }.
\]
Hence,%
\[
\left(  A\mid v\right)  _{\sim p,\sim\left(  m+1\right)  }=\left(
\underbrace{\left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }%
}_{\substack{=A\\\text{(by Proposition \ref{prop.addcol.props1} \textbf{(d)}%
)}}}\right)  _{\sim p,\bullet}=A_{\sim p,\bullet}.
\]
This proves Proposition \ref{prop.addcol.props1} \textbf{(f)}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.addcol.props2}.]\textbf{(a)} Let $v=\left(
v_{1},v_{2},\ldots,v_{n}\right)  ^{T}\in\mathbb{K}^{n\times1}$ be a vector.

Clearly, $\left(  A\mid v\right)  $ is an $n\times n$-matrix (since it is
obtained by attaching the column $v$ to the $n\times\left(  n-1\right)
$-matrix $A$). Write it in the form $\left(  A\mid v\right)  =\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Then, every $p\in\left\{
1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
b_{p,n}=v_{p} \label{pf.prop.addcols.props2.short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.addcols.props2.short.1}):} We have
$\left(  A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$. Thus,%
\[
\left(  \text{the }n\text{-th column of the matrix }\left(  A\mid v\right)
\right)  =\left(
\begin{array}
[c]{c}%
b_{1,n}\\
b_{2,n}\\
\vdots\\
b_{n,n}%
\end{array}
\right)  .
\]
Hence,%
\begin{align*}
\left(
\begin{array}
[c]{c}%
b_{1,n}\\
b_{2,n}\\
\vdots\\
b_{n,n}%
\end{array}
\right)   &  =\left(  \text{the }n\text{-th column of the matrix }\left(
A\mid v\right)  \right)  =\left(  A\mid v\right)  _{\bullet,n}\\
&  =\left(  A\mid v\right)  _{\bullet,\left(  n-1\right)  +1}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=\left(  n-1\right)  +1\right) \\
&  =v\ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.addcol.props1}
\textbf{(b)}, applied to }m=n-1\right) \\
&  =\left(  v_{1},v_{2},\ldots,v_{n}\right)  ^{T}=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{n}%
\end{array}
\right)  .
\end{align*}
In other words, every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfies
$b_{p,n}=v_{p}$. This proves (\ref{pf.prop.addcols.props2.short.1}).} and%
\begin{equation}
\left(  A\mid v\right)  _{\sim p,\sim n}=A_{\sim p,\bullet}
\label{pf.prop.addcols.props2.short.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.addcols.props2.short.2}):} Let
$p\in\left\{  1,2,\ldots,n\right\}  $. Proposition \ref{prop.addcol.props1}
\textbf{(f)} (applied to $m=n-1$) yields $\left(  A\mid v\right)  _{\sim
p,\sim\left(  \left(  n-1\right)  +1\right)  }=A_{\sim p,\bullet}$. This
rewrites as $\left(  A\mid v\right)  _{\sim p,\sim n}=A_{\sim p,\bullet}$
(since $\left(  n-1\right)  +1=n$). This proves
(\ref{pf.prop.addcols.props2.short.2}).}.

We have $n\in\left\{  1,2,\ldots,n\right\}  $ (since $n$ is a positive
integer) and $\left(  A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Hence, Theorem \ref{thm.laplace.gen} \textbf{(b)}
(applied to $\left(  A\mid v\right)  $, $b_{i,j}$ and $n$ instead of $A$,
$a_{i,j}$ and $q$) yields%
\begin{align*}
\det\left(  A\mid v\right)   &  =\sum_{p=1}^{n}\underbrace{\left(  -1\right)
^{p+n}}_{=\left(  -1\right)  ^{n+p}}\underbrace{b_{p,n}}_{\substack{=v_{p}%
\\\text{(by (\ref{pf.prop.addcols.props2.short.1}))}}}\det\left(
\underbrace{\left(  A\mid v\right)  _{\sim p,\sim n}}_{\substack{=A_{\sim
p,\bullet}\\\text{(by (\ref{pf.prop.addcols.props2.short.2}))}}}\right) \\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{n+p}v_{p}\det\left(  A_{\sim p,\bullet
}\right)  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}v_{i}\det\left(  A_{\sim
i,\bullet}\right)
\end{align*}
(here, we have renamed the summation index $p$ as $i$). This proves
Proposition \ref{prop.addcol.props2} \textbf{(a)}.

\textbf{(b)} Let $p\in\left\{  1,2,\ldots,n\right\}  $. For any two objects
$i$ and $j$, we define an element $\delta_{i,j}\in\mathbb{K}$ by $\delta
_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$). Now, $\left(  I_{n}\right)  _{\bullet,p}$ is
the $p$-th column of the matrix $I_{n}$ (by the definition of $\left(
I_{n}\right)  _{\bullet,p}$). Thus,%
\begin{align*}
\left(  I_{n}\right)  _{\bullet,p}  &  =\left(  \text{the }p\text{-th column
of the matrix }I_{n}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\delta_{1,p}\\
\delta_{2,p}\\
\vdots\\
\delta_{n,p}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I_{n}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \delta_{1,p},\delta_{2,p},\ldots,\delta_{n,p}\right)  ^{T}.
\end{align*}
Hence, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to $\left(
I_{n}\right)  _{\bullet,p}$ and $\delta_{i,p}$ instead of $v$ and $v_{i}$)
yields%
\begin{align*}
&  \det\left(  A\mid\left(  I_{n}\right)  _{\bullet,p}\right) \\
&  =\underbrace{\sum_{i=1}^{n}}_{=\sum_{i\in\left\{  1,2,\ldots,n\right\}  }%
}\left(  -1\right)  ^{n+i}\delta_{i,p}\det\left(  A_{\sim i,\bullet}\right)
=\sum_{i\in\left\{  1,2,\ldots,n\right\}  }\left(  -1\right)  ^{n+i}%
\delta_{i,p}\det\left(  A_{\sim i,\bullet}\right) \\
&  =\left(  -1\right)  ^{n+p}\underbrace{\delta_{p,p}}%
_{\substack{=1\\\text{(since }p=p\text{)}}}\det\left(  A_{\sim p,\bullet
}\right)  +\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}\left(  -1\right)  ^{n+i}\underbrace{\delta_{i,p}}%
_{\substack{=0\\\text{(since }i\neq p\text{)}}}\det\left(  A_{\sim i,\bullet
}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }i=p\text{ from the sum,}\\
\text{since }p\in\left\{  1,2,\ldots,n\right\}
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n+p}\det\left(  A_{\sim p,\bullet}\right)
+\underbrace{\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}\left(  -1\right)  ^{n+i}0\det\left(  A_{\sim i,\bullet}\right)  }%
_{=0}=\left(  -1\right)  ^{n+p}\det\left(  A_{\sim p,\bullet}\right)  .
\end{align*}
This proves Proposition \ref{prop.addcol.props2} \textbf{(b)}.
\end{proof}

Before we prove Proposition \ref{prop.addcol.props3}, let us make a simple observation:

\begin{lemma}
\label{lem.sol.prop.addcol.props3.bc.short}Let $n\in\mathbb{N}$. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\in\mathbb{K}%
^{n\times n}$ be an $n\times n$-matrix. Let $r$ and $q$ be two elements of
$\left\{  1,2,\ldots,n\right\}  $. Then,%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =\left(  -1\right)
^{n+q}\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  .\nonumber
\end{equation}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.addcol.props3.bc.short}.]We have
$r\in\left\{  1,2,\ldots,n\right\}  $. Hence, $A_{\bullet,r}$ is an $n\times
1$-matrix (since $A$ is an $n\times n$-matrix). Also, $r\in\left\{
1,2,\ldots,n\right\}  $ shows that $1\leq r\leq n$; hence, $1\leq n$, so that
$n$ is a positive integer.

Also, $q\in\left\{  1,2,\ldots,n\right\}  $. Hence, $A_{\bullet,\sim q}$ is an
$n\times\left(  n-1\right)  $-matrix (since $A$ is an $n\times n$-matrix).

Furthermore, the definition of $A_{\bullet,r}$ yields%
\begin{align*}
A_{\bullet,r}  &  =\left(  \text{the }r\text{-th column of the matrix
}A\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,r}\\
a_{2,r}\\
\vdots\\
a_{n,r}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{1,r},a_{2,r},\ldots,a_{n,r}\right)  ^{T}.
\end{align*}
Thus, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to
$A_{\bullet,\sim q}$, $A_{\bullet,r}$ and $a_{i,r}$ instead of $A$, $v$ and
$v_{i}$) yields%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =\sum_{i=1}%
^{n}\left(  -1\right)  ^{n+i}a_{i,r}\det\left(  \left(  A_{\bullet,\sim
q}\right)  _{\sim i,\bullet}\right)  .
\label{pf.lem.sol.prop.addcol.props3.bc.short.1}%
\end{equation}
But every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet}=A_{\sim i,\sim q}
\label{pf.lem.sol.prop.addcol.props3.bc.short.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.prop.addcol.props3.bc.short.2}):}
Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $i$ and $q$ instead of
$m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet
}=\left(  A_{\sim i,\bullet}\right)  _{\bullet,\sim q}=A_{\sim i,\sim q}$.
This proves (\ref{pf.lem.sol.prop.addcol.props3.bc.short.2}).} and%
\begin{equation}
\left(  -1\right)  ^{n+i}=\left(  -1\right)  ^{n+q}\left(  -1\right)  ^{i+q}
\label{pf.lem.sol.prop.addcol.props3.bc.short.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.prop.addcol.props3.bc.short.3}):}
Let $i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\left(  -1\right)
^{n+q}\left(  -1\right)  ^{i+q}=\left(  -1\right)  ^{\left(  n+q\right)
+\left(  i+q\right)  }=\left(  -1\right)  ^{n+i}$ (since $\left(  n+q\right)
+\left(  i+q\right)  =n+i+2q\equiv n+i\operatorname{mod}2$). This proves
(\ref{pf.lem.sol.prop.addcol.props3.bc.short.3}).}. Now,
(\ref{pf.lem.sol.prop.addcol.props3.bc.short.1}) becomes%
\begin{align*}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)   &  =\sum_{i=1}%
^{n}\underbrace{\left(  -1\right)  ^{n+i}}_{\substack{=\left(  -1\right)
^{n+q}\left(  -1\right)  ^{i+q}\\\text{(by
(\ref{pf.lem.sol.prop.addcol.props3.bc.short.3}))}}}a_{i,r}\det\left(
\underbrace{\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet}%
}_{\substack{=A_{\sim i,\sim q}\\\text{(by
(\ref{pf.lem.sol.prop.addcol.props3.bc.short.2}))}}}\right) \\
&  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+q}\left(  -1\right)  ^{i+q}%
a_{i,r}\det\left(  A_{\sim i,\sim q}\right) \\
&  =\left(  -1\right)  ^{n+q}\sum_{i=1}^{n}\left(  -1\right)  ^{i+q}%
a_{i,r}\det\left(  A_{\sim i,\sim q}\right) \\
&  =\left(  -1\right)  ^{n+q}\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}%
a_{p,r}\det\left(  A_{\sim p,\sim q}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}i\text{ as }p\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.prop.addcol.props3.bc.short}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.addcol.props3}.]\textbf{(a)} Proposition
\ref{prop.addcol.props3} \textbf{(a)} simply says that if we remove the $n$-th
column from the matrix $A$, and then reattach this column back to the matrix
(at its right edge), then we get the original matrix $A$ back. This should be
completely obvious\footnote{Nevertheless, let me give a formal proof of
Proposition \ref{prop.addcol.props3} \textbf{(a)} as well:
\par
Assume that $n>0$. Thus, $n\in\left\{  1,2,\ldots,n\right\}  $. Hence,
$A_{\bullet,\sim n}$ is a well-defined $n\times\left(  n-1\right)  $-matrix,
and $A_{\bullet,n}$ is a well-defined $n\times1$-matrix. Therefore, $\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  $ is an $n\times n$-matrix.
\par
We shall now show that%
\begin{equation}
A_{\bullet,q}=\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q} \label{pf.prop.addcol.props3.short.a.main}%
\end{equation}
for each $q\in\left\{  1,2,\ldots,n\right\}  $.
\par
\textit{Proof of (\ref{pf.prop.addcol.props3.short.a.main}):} Let
$q\in\left\{  1,2,\ldots,n\right\}  $. We must prove the equality
(\ref{pf.prop.addcol.props3.short.a.main}). We are in one of the following two
cases:
\par
\textit{Case 1:} We have $q\neq n$.
\par
\textit{Case 2:} We have $q=n$.
\par
Let us first consider Case 1. In this case, we have $q\neq n$. Combining
$q\in\left\{  1,2,\ldots,n\right\}  $ with $q\neq n$, we obtain $q\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots
,n-1\right\}  $. Thus, Proposition \ref{prop.addcol.props1} \textbf{(a)}
(applied to $n-1$, $A_{\bullet,\sim n}$ and $A_{\bullet,n}$ instead of $m$,
$A$ and $v$) yields $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}=\left(  A_{\bullet,\sim n}\right)  _{\bullet,q}$.
\par
But Proposition \ref{prop.unrows.basics} \textbf{(d)} (applied to $n$, $n$ and
$q$ instead of $m$, $v$ and $w$) yields $\left(  A_{\bullet,\sim n}\right)
_{\bullet,q}=A_{\bullet,q}$. Hence, $A_{\bullet,q}=\left(  A_{\bullet,\sim
n}\right)  _{\bullet,q}=\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}$ (since $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}=\left(  A_{\bullet,\sim n}\right)  _{\bullet,q}$). Hence,
(\ref{pf.prop.addcol.props3.short.a.main}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $q=n$. But Proposition
\ref{prop.addcol.props1} \textbf{(b)} (applied to $n-1$, $A_{\bullet,\sim n}$
and $A_{\bullet,n}$ instead of $m$, $A$ and $v$) yields $\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  _{\bullet,\left(  n-1\right)
+1}=A_{\bullet,n}$. Thus, $A_{\bullet,n}=\left(  A_{\bullet,\sim n}\mid
A_{\bullet,n}\right)  _{\bullet,\left(  n-1\right)  +1}=\left(  A_{\bullet
,\sim n}\mid A_{\bullet,n}\right)  _{\bullet,q}$ (since $\left(  n-1\right)
+1=n=q$). Now, $q=n$, so that $A_{\bullet,q}=A_{\bullet,n}=\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  _{\bullet,q}$. Hence,
(\ref{pf.prop.addcol.props3.short.a.main}) is proven in Case 2.
\par
Now, (\ref{pf.prop.addcol.props3.short.a.main}) is proven in each of the two
Cases 1 and 2. This completes the proof of
(\ref{pf.prop.addcol.props3.short.a.main}).
\par
Now we know that $A$ and $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
$ are two $n\times n$-matrices, and that every $q\in\left\{  1,2,\ldots
,n\right\}  $ satisfies%
\begin{align*}
\left(  \text{the }q\text{-th column of the matrix }A\right)   &
=A_{\bullet,q}=\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.prop.addcol.props3.short.a.main})}\right) \\
&  =\left(  \text{the }q\text{-th column of the matrix }\left(  A_{\bullet
,\sim n}\mid A_{\bullet,n}\right)  \right)  .
\end{align*}
Hence, the matrices $A$ and $\left(  A_{\bullet,\sim n}\mid A_{\bullet
,n}\right)  $ are identical. This proves Proposition \ref{prop.addcol.props3}
\textbf{(a)}.}.

\textbf{(b)} Write the matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. (This is possible since $A$ is an $n\times
n$-matrix.) Lemma \ref{lem.sol.prop.addcol.props3.bc.short} (applied to $r=q$)
yields%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,q}\right)  =\left(  -1\right)
^{n+q}\underbrace{\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=\det A\\\text{(by Theorem
\ref{thm.laplace.gen} \textbf{(b)})}}}=\left(  -1\right)  ^{n+q}\det
A.\nonumber
\end{equation}
This proves Proposition \ref{prop.addcol.props3} \textbf{(b)}.

[\textit{Remark:} Let me outline an alternative proof of Proposition
\ref{prop.addcol.props3} \textbf{(b)}: Let $q\in\left\{  1,2,\ldots,n\right\}
$. The matrix $\left(  A_{\bullet,\sim q}\mid A_{\bullet,q}\right)  $ is
obtained from the matrix $A$ by removing the $q$-th column and then
reattaching this column to the right end of the matrix. This procedure can be
replaced by the following procedure, which clearly leads to the same result:

\begin{itemize}
\item Switch the $q$-th column of $A$ with the $\left(  q+1\right)  $-th column;

\item then switch the $\left(  q+1\right)  $-th column of the resulting matrix
with the $\left(  q+2\right)  $-th column;

\item then switch the $\left(  q+2\right)  $-th column of the resulting matrix
with the $\left(  q+3\right)  $-th column;

\item and so on, finally switching the $\left(  n-1\right)  $-st column of the
matrix with the $n$-th column.
\end{itemize}

But this latter procedure is a sequence of $n-q$ switches of two columns. Each
such switch multiplies the determinant of the matrix by $-1$ (according to
Exercise \ref{exe.ps4.6} \textbf{(b)}). Thus, the whole procedure multiplies
the determinant of the matrix by $\left(  -1\right)  ^{n-q}=\left(  -1\right)
^{n+q}$ (since $n-q\equiv n+q\operatorname{mod}2$). Since this procedure takes
the matrix $A$ to the matrix $\left(  A_{\bullet,\sim q}\mid A_{\bullet
,q}\right)  $, we thus conclude that $\det\left(  A_{\bullet,\sim q}\mid
A_{\bullet,q}\right)  =\left(  -1\right)  ^{n+q}\det A$. This proves
Proposition \ref{prop.addcol.props3} \textbf{(b)} again.]

\textbf{(c)} Let $r$ and $q$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $r\neq q$. We must show that $\det\left(
A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =0$.

Write the matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. (This is possible since $A$ is an $n\times n$-matrix.)
Lemma \ref{lem.sol.prop.addcol.props3.bc.short} yields%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =\left(  -1\right)
^{n+q}\underbrace{\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by Proposition
\ref{prop.laplace.0} \textbf{(b)}}\\\text{(since }q\neq r\text{))}%
}}=0.\nonumber
\end{equation}
This proves Proposition \ref{prop.addcol.props3} \textbf{(c)}.

[\textit{Remark:} Let me outline an alternative proof of Proposition
\ref{prop.addcol.props3} \textbf{(c)}: Let $r$ and $q$ be two elements of
$\left\{  1,2,\ldots,n\right\}  $ satisfying $r\neq q$. Then, $A_{\bullet,r}$
is one of the columns of the matrix $A_{\bullet,\sim q}$ (since the $r$-th
column of the matrix $A$ is not lost when the $q$-th column is removed).
Hence, the column vector $A_{\bullet,r}$ appears twice as a column in the
matrix $\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  $ (namely, once
as one of the columns of $A_{\bullet,\sim q}$, and another time as the
attached column). Therefore, the matrix $\left(  A_{\bullet,\sim q}\mid
A_{\bullet,r}\right)  $ has two equal columns. Exercise \ref{exe.ps4.6}
\textbf{(f)} thus shows that $\det\left(  A_{\bullet,\sim q}\mid A_{\bullet
,r}\right)  =0$. This proves Proposition \ref{prop.addcol.props3} \textbf{(c)} again.]

\textbf{(d)} Let $p\in\left\{  1,2,\ldots,n\right\}  $ and $q\in\left\{
1,2,\ldots,n\right\}  $. Then, Proposition \ref{prop.unrows.basics}
\textbf{(c)} (applied to $u=p$ and $v=q$) yields%
\[
\left(  A_{\bullet,\sim q}\right)  _{\sim p,\bullet}=\left(  A_{\sim
p,\bullet}\right)  _{\bullet,\sim q}=A_{\sim p,\sim q}.
\]


But $A$ is an $n\times n$-matrix. Hence, $A_{\bullet,\sim q}$ is an
$n\times\left(  n-1\right)  $-matrix. Moreover, $p\in\left\{  1,2,\ldots
,n\right\}  $, so that $1\leq p\leq n$ and thus $n\geq1$; hence, $n$ is a
positive integer. Proposition \ref{prop.addcol.props2} \textbf{(b)} (applied
to $A_{\bullet,\sim q}$ instead of $A$) thus yields
\[
\det\left(  A_{\bullet,\sim q}\mid\left(  I_{n}\right)  _{\bullet,p}\right)
=\left(  -1\right)  ^{n+p}\det\left(  \underbrace{\left(  A_{\bullet,\sim
q}\right)  _{\sim p,\bullet}}_{=A_{\sim p,\sim q}}\right)  =\left(  -1\right)
^{n+p}\det\left(  A_{\sim p,\sim q}\right)  .
\]
This proves Proposition \ref{prop.addcol.props3} \textbf{(d)}.

\textbf{(e)} Let $u$ and $v$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $u<v$. Let $r$ be an element of $\left\{
1,2,\ldots,n-1\right\}  $ satisfying $r\neq u$. We must show that $\det\left(
A_{\bullet,\sim u}\mid\left(  A_{\bullet,\sim v}\right)  _{\bullet,r}\right)
=0$.

We are in one of the following two cases:

\textit{Case 1:} We have $r<v$.

\textit{Case 2:} We have $r\geq v$.

Let us first consider Case 1. In this case, we have $r<v$. Thus, $r\in\left\{
1,2,\ldots,v-1\right\}  $. Hence, Proposition \ref{prop.unrows.basics}
\textbf{(d)} (applied to $m=n$ and $w=r$) yields $\left(  A_{\bullet,\sim
v}\right)  _{\bullet,r}=A_{\bullet,r}$. Hence,%
\[
\det\left(  A_{\bullet,\sim u}\mid\underbrace{\left(  A_{\bullet,\sim
v}\right)  _{\bullet,r}}_{=A_{\bullet,r}}\right)  =\det\left(  A_{\bullet,\sim
u}\mid A_{\bullet,r}\right)  =0
\]
(by Proposition \ref{prop.addcol.props3} \textbf{(c)}, applied to $q=u$).
Hence, Proposition \ref{prop.addcol.props3} \textbf{(e)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $r\geq v$. Hence,
$r\in\left\{  v,v+1,\ldots,n-1\right\}  $ (since $r\in\left\{  1,2,\ldots
,n-1\right\}  $), so that $r+1\in\left\{  v+1,v+2,\ldots,n\right\}
\subseteq\left\{  1,2,\ldots,n\right\}  $. Furthermore, $r+1>r\geq v>u$ (since
$u<v$) and thus $r+1\neq u$. Hence, Proposition \ref{prop.addcol.props3}
\textbf{(c)} (applied to $r+1$ and $u$ instead of $r$ and $q$) yields
$\det\left(  A_{\bullet,\sim u}\mid A_{\bullet,r+1}\right)  =0$.

But recall that $r\in\left\{  v,v+1,\ldots,n-1\right\}  $. Thus, Proposition
\ref{prop.unrows.basics} \textbf{(e)} (applied to $m=n$ and $w=r$) yields
$\left(  A_{\bullet,\sim v}\right)  _{\bullet,r}=A_{\bullet,r+1}$. Hence,%
\[
\det\left(  A_{\bullet,\sim u}\mid\underbrace{\left(  A_{\bullet,\sim
v}\right)  _{\bullet,r}}_{=A_{\bullet,r+1}}\right)  =\det\left(
A_{\bullet,\sim u}\mid A_{\bullet,r+1}\right)  =0.
\]
Hence, Proposition \ref{prop.addcol.props3} \textbf{(e)} is proven in Case 2.

We have now proven Proposition \ref{prop.addcol.props3} \textbf{(e)} in each
of the two Cases 1 and 2. Hence, Proposition \ref{prop.addcol.props3}
\textbf{(e)} always holds.

\textbf{(f)} Let $u$ and $v$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $u<v$. Then, $u<v$, so that $u\in\left\{
1,2,\ldots,v-1\right\}  $. Hence, Proposition \ref{prop.unrows.basics}
\textbf{(d)} (applied to $m=n$ and $w=u$) yields $\left(  A_{\bullet,\sim
v}\right)  _{\bullet,u}=A_{\bullet,u}$. Thus,%
\[
\det\left(  A_{\bullet,\sim u}\mid\underbrace{\left(  A_{\bullet,\sim
v}\right)  _{\bullet,u}}_{=A_{\bullet,u}}\right)  =\det\left(  A_{\bullet,\sim
u}\mid A_{\bullet,u}\right)  =\left(  -1\right)  ^{n+u}\det A
\]
(by Proposition \ref{prop.addcol.props3} \textbf{(b)}, applied to $q=u$).
Thus,%
\[
\left(  -1\right)  ^{u}\underbrace{\det\left(  A_{\bullet,\sim u}\mid\left(
A_{\bullet,\sim v}\right)  _{\bullet,u}\right)  }_{=\left(  -1\right)
^{n+u}\det A}=\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)  ^{n+u}%
}_{\substack{=\left(  -1\right)  ^{u+\left(  n+u\right)  }=\left(  -1\right)
^{n}\\\text{(since }u+\left(  n+u\right)  =2u+n\equiv n\operatorname{mod}%
2\text{)}}}\det A=\left(  -1\right)  ^{n}\det A.
\]
This proves Proposition \ref{prop.addcol.props3} \textbf{(f)}.
\end{proof}
\end{vershort}

\begin{verlong}
In preparation for our solution to Exercise \ref{exe.prop.addcol.props}, we
shall prove a lemma:

\begin{lemma}
\label{lem.sol.prop.addcol.props.1}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}\in
\mathbb{K}^{n\times m}$ be an $n\times m$-matrix. Let $v=\left(  v_{1}%
,v_{2},\ldots,v_{n}\right)  ^{T}\in\mathbb{K}^{n\times1}$ be a column vector
with $n$ entries.

\textbf{(a)} Every $q\in\left\{  1,2,\ldots,m\right\}  $ satisfies $\left(
A\mid v\right)  _{\bullet,q}=A_{\bullet,q}$.

\textbf{(b)} We have $\left(  A\mid v\right)  _{\bullet,m+1}=v$.

\textbf{(c)} Write the matrix $\left(  A\mid v\right)  $ in the form $\left(
A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$.
Then, we have%
\begin{equation}
b_{i,j}=a_{i,j} \label{eq.lem.sol.prop.addcol.props.1.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and every $j\in\left\{
1,2,\ldots,m\right\}  $. Moreover,%
\begin{equation}
b_{i,m+1}=v_{i} \label{eq.lem.sol.prop.addcol.props.1.2}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $.
\end{lemma}

\begin{noncompile}
[The following version of the proof of Lemma \ref{lem.sol.prop.addcol.props.1}
was written for the vershort environment, back when I thought that this lemma
would appear in this environment at all:]

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.addcol.props.1}.]The matrix $\left(  A\mid
v\right)  $ is defined as the $n\times\left(  m+1\right)  $-matrix whose $m+1$
columns are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$ (from left to
right). Thus, the first $m$ columns of the matrix $\left(  A\mid v\right)  $
are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m}$, whereas the $\left(
m+1\right)  $-st column of $\left(  A\mid v\right)  $ is $v$.

\textbf{(a)} For every $q\in\left\{  1,2,\ldots,m\right\}  $, we have%
\begin{align*}
\left(  A\mid v\right)  _{\bullet,q}  &  =\left(  \text{the }q\text{-th column
of the matrix }\left(  A\mid v\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  A\mid
v\right)  _{\bullet,q}\right) \\
&  =A_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since the first }m\text{ columns of the matrix }\left(  A\mid v\right)
\\
\text{are }A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m}%
\end{array}
\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(a)}.

\textbf{(b)} The definition of $\left(  A\mid v\right)  _{\bullet,m+1}$ yields%
\[
\left(  A\mid v\right)  _{\bullet,m+1}=\left(  \text{the }\left(  m+1\right)
\text{-st column of the matrix }\left(  A\mid v\right)  \right)  =v
\]
(since the $\left(  m+1\right)  $-st column of $\left(  A\mid v\right)  $ is
$v$). This proves Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(b)}.

\textbf{(c)} \textit{Proof of (\ref{eq.lem.sol.prop.addcol.props.1.1}):} We
have $\left(  A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq m+1}$. Thus, for every $i\in\left\{  1,2,\ldots,n\right\}  $ and
$j\in\left\{  1,2,\ldots,m\right\}  $, we have%
\begin{align*}
b_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }\left(  A\mid v\right)  \right) \\
&  =\left(  \text{the }i\text{-th entry of }\underbrace{\text{the }j\text{-th
column of the matrix }\left(  A\mid v\right)  }_{\substack{=\left(  A\mid
v\right)  _{\bullet,j}=A_{\bullet,j}\\\text{(by Lemma
\ref{lem.sol.prop.addcol.props.1} \textbf{(a)},}\\\text{applied to
}q=j\text{)}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of }\underbrace{A_{\bullet,j}%
}_{\substack{=\left(  \text{the }j\text{-th column of the matrix }A\right)
\\\text{(by the definition of }A_{\bullet,j}\text{)}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of the }j\text{-th column of the
matrix }A\right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A\right)  =a_{i,j}.
\end{align*}
This proves (\ref{eq.lem.sol.prop.addcol.props.1.1}).

\textit{Proof of (\ref{eq.lem.sol.prop.addcol.props.1.2}):} We have $\left(
A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$.
Thus, for every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{align*}
b_{i,m+1}  &  =\left(  \text{the }\left(  i,m+1\right)  \text{-th entry of the
matrix }\left(  A\mid v\right)  \right) \\
&  =\left(  \text{the }i\text{-th entry of }\underbrace{\text{the }\left(
m+1\right)  \text{-th column of the matrix }\left(  A\mid v\right)
}_{\substack{=\left(  A\mid v\right)  _{\bullet,m+1}=v\\\text{(by Lemma
\ref{lem.sol.prop.addcol.props.1} \textbf{(b)})}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of }v\right)  =v_{i}.
\end{align*}
This proves (\ref{eq.lem.sol.prop.addcol.props.1.2}).

Thus, Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(c)} is proven.
\end{proof}
\end{noncompile}

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.addcol.props.1}.]The matrix $\left(  A\mid
v\right)  $ is defined as the $n\times\left(  m+1\right)  $-matrix whose $m+1$
columns are $A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m},v$ (from left to right).

For each $q\in\left\{  1,2,\ldots,m+1\right\}  $, we have%
\begin{align}
&  \left(  \text{the }q\text{-th column of the matrix }\left(  A\mid v\right)
\right) \nonumber\\
&  =\left(  \text{the }q\text{-th entry of }\underbrace{\text{the list of the
columns of the matrix }\left(  A\mid v\right)  }_{\substack{=\left(
A_{\bullet,1},A_{\bullet,2},\ldots,A_{\bullet,m},v\right)  \\\text{(since
}\left(  A\mid v\right)  \text{ is the }n\times\left(  m+1\right)
\text{-matrix whose }m+1\\\text{columns are }A_{\bullet,1},A_{\bullet
,2},\ldots,A_{\bullet,m},v\text{ (from left to right))}}}\right) \nonumber\\
&  =\left(  \text{the }q\text{-th entry of }\left(  A_{\bullet,1}%
,A_{\bullet,2},\ldots,A_{\bullet,m},v\right)  \right) \nonumber\\
&  =%
\begin{cases}
A_{\bullet,q}, & \text{if }q<m+1;\\
v, & \text{if }q=m+1
\end{cases}
. \label{pf.lem.sol.prop.addcol.props.1.long.1}%
\end{align}


Now, for each $q\in\left\{  1,2,\ldots,m+1\right\}  $, we have%
\begin{align}
&  \left(  A\mid v\right)  _{\bullet,q}\nonumber\\
&  =\left(  \text{the }q\text{-th column of the matrix }\left(  A\mid
v\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\left(  A\mid
v\right)  _{\bullet,q}\right) \nonumber\\
&  =%
\begin{cases}
A_{\bullet,q}, & \text{if }q<m+1;\\
v, & \text{if }q=m+1
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.1.long.1})}\right)  .
\label{pf.lem.sol.prop.addcol.props.1.long.2}%
\end{align}


\textbf{(a)} Let $q\in\left\{  1,2,\ldots,m\right\}  $. Then, $q\leq m<m+1$.
Moreover, $q\in\left\{  1,2,\ldots,m\right\}  \subseteq\left\{  1,2,\ldots
,m+1\right\}  $. Hence, (\ref{pf.lem.sol.prop.addcol.props.1.long.2}) yields%
\[
\left(  A\mid v\right)  _{\bullet,q}=%
\begin{cases}
A_{\bullet,q}, & \text{if }q<m+1;\\
v, & \text{if }q=m+1
\end{cases}
=A_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }q<m+1\right)  .
\]
Thus, Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(a)} is proven.

\textbf{(b)} We have $\underbrace{m}_{\geq0}+1\geq1$, and thus $m+1\in\left\{
1,2,\ldots,m+1\right\}  $. Hence, (\ref{pf.lem.sol.prop.addcol.props.1.long.2}%
) (applied to $q=m+1$) yields%
\[
\left(  A\mid v\right)  _{\bullet,m+1}=%
\begin{cases}
A_{\bullet,m+1}, & \text{if }m+1<m+1;\\
v, & \text{if }m+1=m+1
\end{cases}
=v\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m+1=m+1\right)  .
\]
Thus, Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(b)} is proven.

\textbf{(c)} We have $\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m+1}=\left(  A\mid v\right)  $. Thus, for every $i\in\left\{  1,2,\ldots
,n\right\}  $ and $j\in\left\{  1,2,\ldots,m+1\right\}  $, we have%
\begin{align}
b_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }\left(  A\mid v\right)  \right)
\label{pf.lem.sol.prop.addcol.props.1.long.c.0}\\
&  =\left(  \text{the }i\text{-th entry of }\underbrace{\text{the }j\text{-th
column of the matrix }\left(  A\mid v\right)  }_{\substack{=\left(  A\mid
v\right)  _{\bullet,j}\\\text{(since }\left(  A\mid v\right)  _{\bullet
,j}\text{ is the }j\text{-th column of the matrix }\left(  A\mid v\right)
\\\text{(by the definition of }\left(  A\mid v\right)  _{\bullet,j}\text{))}%
}}\right) \nonumber\\
&  =\left(  \text{the }i\text{-th entry of }\left(  A\mid v\right)
_{\bullet,j}\right)  . \label{pf.lem.sol.prop.addcol.props.1.long.c.0b}%
\end{align}


\textit{Proof of (\ref{eq.lem.sol.prop.addcol.props.1.1}):} We have $\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=A$. Hence, for every
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}
$, we have%
\begin{equation}
a_{i,j}=\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A\right)  . \label{pf.lem.sol.prop.addcol.props.1.long.c.1}%
\end{equation}


Now, let $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $. Then, $j\in\left\{  1,2,\ldots,m\right\}
\subseteq\left\{  1,2,\ldots,m+1\right\}  $. Thus,
(\ref{pf.lem.sol.prop.addcol.props.1.long.c.0b}) yields%
\begin{align*}
b_{i,j}  &  =\left(  \text{the }i\text{-th entry of }\underbrace{\left(  A\mid
v\right)  _{\bullet,j}}_{\substack{=A_{\bullet,j}\\\text{(by Lemma
\ref{lem.sol.prop.addcol.props.1} \textbf{(a)},}\\\text{applied to
}q=j\text{)}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of }\underbrace{A_{\bullet,j}%
}_{\substack{=\left(  \text{the }j\text{-th column of the matrix }A\right)
\\\text{(since }A_{\bullet,j}\text{ is defined as the }j\text{-th column of
the matrix }A\text{)}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of the }j\text{-th column of the
matrix }A\right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A\right)  =a_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.1.long.c.1})}\right)  .
\end{align*}
This proves (\ref{eq.lem.sol.prop.addcol.props.1.1}).

\textit{Proof of (\ref{eq.lem.sol.prop.addcol.props.1.2}):} We have $v=\left(
v_{1},v_{2},\ldots,v_{n}\right)  ^{T}=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{n}%
\end{array}
\right)  $. Hence, for every $i\in\left\{  1,2,\ldots,n\right\}  $, we have%
\begin{equation}
\left(  \text{the }i\text{-th entry of }v\right)  =v_{i}.
\label{pf.lem.sol.prop.addcol.props.1.long.c.2}%
\end{equation}


Now, let $i\in\left\{  1,2,\ldots,n\right\}  $. We have $\underbrace{m}%
_{\geq0}+1\geq1$, and thus $m+1\in\left\{  1,2,\ldots,m+1\right\}  $. Thus,
(\ref{pf.lem.sol.prop.addcol.props.1.long.c.0b}) (applied to $j=m+1$) yields%
\begin{align*}
b_{i,m+1}  &  =\left(  \text{the }i\text{-th entry of }\underbrace{\left(
A\mid v\right)  _{\bullet,m+1}}_{\substack{=v\\\text{(by Lemma
\ref{lem.sol.prop.addcol.props.1} \textbf{(b)})}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of }v\right)  =v_{i}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.1.long.c.2})}\right)  .
\end{align*}
This proves (\ref{eq.lem.sol.prop.addcol.props.1.2}).

Now, we have proven both (\ref{eq.lem.sol.prop.addcol.props.1.1}) and
(\ref{eq.lem.sol.prop.addcol.props.1.2}). Thus, Lemma
\ref{lem.sol.prop.addcol.props.1} \textbf{(c)} is proven.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.addcol.props1}.]Write the $n\times m$-matrix
$A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}%
\in\mathbb{K}^{n\times m}$. Write the column vector $v\in\mathbb{K}^{n\times
1}$ in the form $v=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{n}%
\end{array}
\right)  $. Thus, $v=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{n}%
\end{array}
\right)  =\left(  v_{1},v_{2},\ldots,v_{n}\right)  ^{T}$.

We shall use the notation introduced in Definition \ref{def.sol.unrows.d}
\textbf{(a)}.

\textbf{(a)} Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(a)} shows that
every $q\in\left\{  1,2,\ldots,m\right\}  $ satisfies $\left(  A\mid v\right)
_{\bullet,q}=A_{\bullet,q}$. This proves Proposition \ref{prop.addcol.props1}
\textbf{(a)}.

\textbf{(b)} Lemma \ref{lem.sol.prop.addcol.props.1} \textbf{(b)} shows that
$\left(  A\mid v\right)  _{\bullet,m+1}=v$. This proves Proposition
\ref{prop.addcol.props1} \textbf{(b)}.

\textbf{(c)} Write the matrix $\left(  A\mid v\right)  $ in the form $\left(
A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$.
(This is possible since $\left(  A\mid v\right)  $ is an $n\times\left(
m+1\right)  $-matrix.) Then, we have%
\begin{equation}
b_{i,j}=a_{i,j} \label{pf.prop.addcol.props1.c.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and every $j\in\left\{
1,2,\ldots,m\right\}  $ (by (\ref{eq.lem.sol.prop.addcol.props.1.1})).
Moreover,%
\begin{equation}
b_{i,m+1}=v_{i} \label{pf.prop.addcol.props1.c.2}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ (by
(\ref{eq.lem.sol.prop.addcol.props.1.2})).

Let $q\in\left\{  1,2,\ldots,m\right\}  $. The matrix $A_{\bullet,\sim q}$ is
an $n\times\left(  m-1\right)  $-matrix (since $A$ is an $n\times m$-matrix).
Hence, $\left(  A_{\bullet,\sim q}\mid v\right)  $ is an $n\times\left(
\left(  m-1\right)  +1\right)  $-matrix. In other words, $\left(
A_{\bullet,\sim q}\mid v\right)  $ is an $n\times m$-matrix (since $\left(
m-1\right)  +1=m$).

Proposition \ref{prop.sol.unirows.d} \textbf{(b)} (applied to $v=q$) shows
that $A_{\bullet,\sim q}=\left(  a_{i,\mathbf{d}_{q}\left(  j\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq m-1}$. But recall that $\left(  A\mid
v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$. Hence,
Proposition \ref{prop.sol.unirows.d} \textbf{(b)} (applied to $m+1$, $\left(
A\mid v\right)  $, $b_{i,j}$ and $q$ instead of $m$, $A$, $a_{i,j}$ and $v$)
shows that
\begin{equation}
\left(  A\mid v\right)  _{\bullet,\sim q}=\left(  b_{i,\mathbf{d}_{q}\left(
j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq\left(  m+1\right)
-1}=\left(  b_{i,\mathbf{d}_{q}\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m} \label{pf.prop.addcol.props1.c.mat1}%
\end{equation}
(since $\left(  m+1\right)  -1=m$).

On the other hand, write the matrix $\left(  A_{\bullet,\sim q}\mid v\right)
$ in the form
\[
\left(  A_{\bullet,\sim q}\mid v\right)  =\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\left(  m-1\right)  +1}.
\]
(This is possible since $\left(  A_{\bullet,\sim q}\mid v\right)  $ is an
$n\times\left(  \left(  m-1\right)  +1\right)  $-matrix.) Recall that
$A_{\bullet,\sim q}=\left(  a_{i,\mathbf{d}_{q}\left(  j\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq m-1}$. Thus, we have%
\begin{equation}
c_{i,j}=a_{i,\mathbf{d}_{q}\left(  j\right)  }
\label{pf.prop.addcol.props1.c.3}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and every $j\in\left\{
1,2,\ldots,m-1\right\}  $ (by (\ref{eq.lem.sol.prop.addcol.props.1.1}),
applied to $m-1$, $A_{\bullet,\sim q}$, $a_{i,\mathbf{d}_{q}\left(  j\right)
}$ and $c_{i,j}$ instead of $m$, $A$, $a_{i,j}$ and $b_{i,j}$). Moreover,%
\begin{equation}
c_{i,\left(  m-1\right)  +1}=v_{i} \label{pf.prop.addcol.props1.c.4}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ (by
(\ref{eq.lem.sol.prop.addcol.props.1.2}), applied to $m-1$, $A_{\bullet,\sim
q}$, $a_{i,\mathbf{d}_{q}\left(  j\right)  }$ and $c_{i,j}$ instead of $m$,
$A$, $a_{i,j}$ and $b_{i,j}$).

Now,
\begin{equation}
\left(  A_{\bullet,\sim q}\mid v\right)  =\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq\left(  m-1\right)  +1}=\left(  c_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m} \label{pf.prop.addcol.props1.c.mat2}%
\end{equation}
(since $\left(  m-1\right)  +1=m$). We shall now show that%
\begin{equation}
b_{i,\mathbf{d}_{q}\left(  j\right)  }=c_{i,j}
\label{pf.prop.addcol.props1.c.10}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $.

\textit{Proof of (\ref{pf.prop.addcol.props1.c.10}):} Let $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}  $. We must
prove the equality (\ref{pf.prop.addcol.props1.c.10}). We are in one of the
following two cases:

\textit{Case 1:} We have $j\neq m$.

\textit{Case 2:} We have $j=m$.

Let us first consider Case 1. In this case, we have $j\neq m$. Combined with
$j\in\left\{  1,2,\ldots,m\right\}  $, this yields $j\in\left\{
1,2,\ldots,m\right\}  \setminus\left\{  m\right\}  =\left\{  1,2,\ldots
,m-1\right\}  $. Hence, $j+1\in\left\{  2,3,\ldots,m\right\}  $, so that
$j+1\leq m$. Now, the definition of $\mathbf{d}_{q}\left(  j\right)  $ yields%
\begin{align*}
\mathbf{d}_{q}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
\\
&  \leq%
\begin{cases}
j+1, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j\leq j+1\text{ in the case when }j<q\text{,}\\
\text{and since }j+1\leq j+1\text{ in the case when }j\geq q
\end{array}
\right) \\
&  =j+1\leq m.
\end{align*}
Combining this with%
\begin{align*}
\mathbf{d}_{q}\left(  j\right)   &  =%
\begin{cases}
j, & \text{if }j<q;\\
j+1, & \text{if }j\geq q
\end{cases}
\\
&  \geq%
\begin{cases}
j, & \text{if }j<q;\\
j, & \text{if }j\geq q
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }j\geq j\text{ in the case when }j<q\text{,}\\
\text{and since }j+1\geq j\text{ in the case when }j\geq q
\end{array}
\right) \\
&  =j\geq1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }j\in\left\{  1,2,\ldots
,m\right\}  \right)  ,
\end{align*}
we obtain $\mathbf{d}_{q}\left(  j\right)  \in\left\{  1,2,\ldots,m\right\}
$. Hence, (\ref{pf.prop.addcol.props1.c.1}) (applied to $\mathbf{d}_{q}\left(
j\right)  $ instead of $j$) shows that $b_{i,\mathbf{d}_{q}\left(  j\right)
}=a_{i,\mathbf{d}_{q}\left(  j\right)  }$. But
(\ref{pf.prop.addcol.props1.c.3}) yields $c_{i,j}=a_{i,\mathbf{d}_{q}\left(
j\right)  }$. Thus, $b_{i,\mathbf{d}_{q}\left(  j\right)  }=a_{i,\mathbf{d}%
_{q}\left(  j\right)  }=c_{i,j}$. Hence, (\ref{pf.prop.addcol.props1.c.10}) is
proven in Case 1.

Let us now consider Case 2. In this case, we have $j=m$. Hence,
\begin{align*}
\mathbf{d}_{q}\left(  \underbrace{j}_{=m}\right)   &  =\mathbf{d}_{q}\left(
m\right)  =%
\begin{cases}
m, & \text{if }m<q;\\
m+1, & \text{if }m\geq q
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\mathbf{d}_{q}\left(
m\right)  \right) \\
&  =m+1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }m\geq q\text{ (since }q\leq
m\text{ (because }q\in\left\{  1,2,\ldots,m\right\}  \text{))}\right)  .
\end{align*}
Hence, $b_{i,\mathbf{d}_{q}\left(  j\right)  }=b_{i,m+1}=v_{i}$ (by
(\ref{pf.prop.addcol.props1.c.2})). But from $j=m=\left(  m-1\right)  +1$, we
obtain $c_{i,j}=c_{i,\left(  m-1\right)  +1}=v_{i}$ (by
(\ref{pf.prop.addcol.props1.c.4})). Thus, $b_{i,\mathbf{d}_{q}\left(
j\right)  }=v_{i}=c_{i,j}$. Hence, (\ref{pf.prop.addcol.props1.c.10}) is
proven in Case 2.

We have now proven the equality (\ref{pf.prop.addcol.props1.c.10}) in each of
the two Cases 1 and 2. Since these two Cases cover all possibilities, this
shows that (\ref{pf.prop.addcol.props1.c.10}) always holds. Thus,
(\ref{pf.prop.addcol.props1.c.10}) is proven.

Now, (\ref{pf.prop.addcol.props1.c.mat1}) becomes%
\[
\left(  A\mid v\right)  _{\bullet,\sim q}=\left(  \underbrace{b_{i,\mathbf{d}%
_{q}\left(  j\right)  }}_{\substack{=c_{i,j}\\\text{(by
(\ref{pf.prop.addcol.props1.c.10}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}=\left(  c_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=\left(
A_{\bullet,\sim q}\mid v\right)
\]
(by (\ref{pf.prop.addcol.props1.c.mat2})). This proves Proposition
\ref{prop.addcol.props1} \textbf{(c)}.

\textbf{(d)} Write the matrix $\left(  A\mid v\right)  $ in the form $\left(
A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$.
(This is possible since $\left(  A\mid v\right)  $ is an $n\times\left(
m+1\right)  $-matrix.) Then, we have%
\begin{equation}
b_{i,j}=a_{i,j} \label{pf.prop.addcol.props1.d.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and every $j\in\left\{
1,2,\ldots,m\right\}  $ (by (\ref{eq.lem.sol.prop.addcol.props.1.1}))

We have $m+1\in\left\{  1,2,\ldots,m+1\right\}  $ (since $\underbrace{m}%
_{\geq0}+1\geq1$). Thus, the matrix $\left(  A\mid v\right)  _{\bullet
,\sim\left(  m+1\right)  }$ is well-defined (since $\left(  A\mid v\right)  $
is an $n\times\left(  m+1\right)  $-matrix). This matrix $\left(  A\mid
v\right)  _{\bullet,\sim\left(  m+1\right)  }$ is an $n\times\left(  \left(
m+1\right)  -1\right)  $-matrix (since $\left(  A\mid v\right)  $ is an
$n\times\left(  m+1\right)  $-matrix). In other words, $\left(  A\mid
v\right)  _{\bullet,\sim\left(  m+1\right)  }$ is an $n\times m$-matrix (since
$\left(  m+1\right)  -1=m$). Proposition \ref{prop.sol.unirows.d} \textbf{(b)}
(applied to $m+1$, $\left(  A\mid v\right)  $, $b_{i,j}$ and $m+1$ instead of
$m$, $A$, $a_{i,j}$ and $v$) shows that
\begin{equation}
\left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }=\left(
b_{i,\mathbf{d}_{m+1}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq
j\leq\left(  m+1\right)  -1}=\left(  b_{i,\mathbf{d}_{m+1}\left(  j\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq m} \label{pf.prop.addcol.props1.d.2}%
\end{equation}
(since $\left(  m+1\right)  -1=m$).

But every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $ satisfy
\begin{equation}
b_{i,\mathbf{d}_{m+1}\left(  j\right)  }=a_{i,j}
\label{pf.prop.addcol.props1.d.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.addcol.props1.d.3}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,m\right\}
$. From $j\in\left\{  1,2,\ldots,m\right\}  $, we obtain $j\leq m<m+1$. Now,
the definition of $\mathbf{d}_{m+1}\left(  j\right)  $ yields $\mathbf{d}%
_{m+1}\left(  j\right)  =%
\begin{cases}
j, & \text{if }j<m+1;\\
j+1, & \text{if }j\geq m+1
\end{cases}
=j$ (since $j<m+1$). Thus, $b_{i,\mathbf{d}_{m+1}\left(  j\right)  }%
=b_{i,j}=a_{i,j}$ (by (\ref{pf.prop.addcol.props1.d.1})). This proves
(\ref{pf.prop.addcol.props1.d.3}).}. Hence, $\left(  b_{i,\mathbf{d}%
_{m+1}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=A$. Thus,
(\ref{pf.prop.addcol.props1.d.2}) becomes%
\[
\left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }=\left(
b_{i,\mathbf{d}_{m+1}\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}=A.
\]
This proves Proposition \ref{prop.addcol.props1} \textbf{(d)}.

\textbf{(e)} Write the matrix $\left(  A\mid v\right)  $ in the form $\left(
A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$.
(This is possible since $\left(  A\mid v\right)  $ is an $n\times\left(
m+1\right)  $-matrix.) Then, we have%
\begin{equation}
b_{i,j}=a_{i,j} \label{pf.prop.addcol.props1.e.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and every $j\in\left\{
1,2,\ldots,m\right\}  $ (by (\ref{eq.lem.sol.prop.addcol.props.1.1})).
Moreover,%
\begin{equation}
b_{i,m+1}=v_{i} \label{pf.prop.addcol.props1.e.2}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ (by
(\ref{eq.lem.sol.prop.addcol.props.1.2})).

Let $p\in\left\{  1,2,\ldots,n\right\}  $. The matrix $A_{\sim p,\bullet}$ is
an $\left(  n-1\right)  \times m$-matrix (since $A$ is an $n\times m$-matrix).
Also, the matrix $v_{\sim p,\bullet}$ is an $\left(  n-1\right)  \times
1$-matrix (since $v$ is an $n\times1$-matrix). Hence, $\left(  A_{\sim
p,\bullet}\mid v_{\sim p,\bullet}\right)  $ is an $\left(  n-1\right)
\times\left(  m+1\right)  $-matrix.

Proposition \ref{prop.sol.unirows.d} \textbf{(a)} (applied to $u=p$) shows
that $A_{\sim p,\bullet}=\left(  a_{\mathbf{d}_{p}\left(  i\right)
,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m}$. But recall that $\left(
A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m+1}$.
Hence, Proposition \ref{prop.sol.unirows.d} \textbf{(a)} (applied to $m+1$,
$\left(  A\mid v\right)  $, $b_{i,j}$ and $p$ instead of $m$, $A$, $a_{i,j}$
and $u$) shows that
\begin{equation}
\left(  A\mid v\right)  _{\sim p,\bullet}=\left(  b_{\mathbf{d}_{p}\left(
i\right)  ,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m+1}.
\label{pf.prop.addcol.props1.e.3}%
\end{equation}


Also, $v=\left(
\begin{array}
[c]{c}%
v_{1}\\
v_{2}\\
\vdots\\
v_{n}%
\end{array}
\right)  =\left(  v_{i}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}$. Hence,
Proposition \ref{prop.sol.unirows.d} \textbf{(a)} (applied to $1$, $v$,
$v_{i}$ and $p$ instead of $m$, $A$, $a_{i,j}$ and $u$) yields
\begin{equation}
v_{\sim p,\bullet}=\left(  v_{\mathbf{d}_{p}\left(  i\right)  }\right)
_{1\leq i\leq n-1,\ 1\leq j\leq1}=\left(
\begin{array}
[c]{c}%
v_{\mathbf{d}_{p}\left(  1\right)  }\\
v_{\mathbf{d}_{p}\left(  2\right)  }\\
\vdots\\
v_{\mathbf{d}_{p}\left(  n-1\right)  }%
\end{array}
\right)  =\left(  v_{\mathbf{d}_{p}\left(  1\right)  },v_{\mathbf{d}%
_{p}\left(  2\right)  },\ldots,v_{\mathbf{d}_{p}\left(  n-1\right)  }\right)
^{T}. \label{pf.prop.addcol.props1.e.4}%
\end{equation}


On the other hand, write the matrix $\left(  A_{\sim p,\bullet}\mid v_{\sim
p,\bullet}\right)  $ in the form
\begin{equation}
\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)  =\left(
c_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m+1}.
\label{pf.prop.addcol.props1.e.mat2}%
\end{equation}
(This is possible since $\left(  A_{\sim p,\bullet}\mid v_{\sim p,\bullet
}\right)  $ is an $\left(  n-1\right)  \times\left(  m+1\right)  $-matrix.)
Recall that $A_{\sim p,\bullet}=\left(  a_{\mathbf{d}_{p}\left(  i\right)
,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m}$ and $v_{\sim p,\bullet
}=\left(  v_{\mathbf{d}_{p}\left(  1\right)  },v_{\mathbf{d}_{p}\left(
2\right)  },\ldots,v_{\mathbf{d}_{p}\left(  n-1\right)  }\right)  ^{T}$. Thus,
we have%
\begin{equation}
c_{i,j}=a_{\mathbf{d}_{p}\left(  i\right)  ,j}
\label{pf.prop.addcol.props1.e.5}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n-1\right\}  $ and every $j\in\left\{
1,2,\ldots,m\right\}  $ (by (\ref{eq.lem.sol.prop.addcol.props.1.1}), applied
to $n-1$, $A_{\sim p,\bullet}$, $a_{\mathbf{d}_{p}\left(  i\right)  ,j}$,
$v_{\sim p,\bullet}$, $v_{\mathbf{d}_{p}\left(  i\right)  }$ and $c_{i,j}$
instead of $n$, $A$, $a_{i,j}$, $v$, $v_{i}$ and $b_{i,j}$). Moreover,%
\begin{equation}
c_{i,m+1}=v_{\mathbf{d}_{p}\left(  i\right)  }
\label{pf.prop.addcol.props1.e.6}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n-1\right\}  $ (by
(\ref{eq.lem.sol.prop.addcol.props.1.2}), applied to $n-1$, $A_{\sim
p,\bullet}$, $a_{\mathbf{d}_{p}\left(  i\right)  ,j}$, $v_{\sim p,\bullet}$,
$v_{\mathbf{d}_{p}\left(  i\right)  }$ and $c_{i,j}$ instead of $n$, $A$,
$a_{i,j}$, $v$, $v_{i}$ and $b_{i,j}$).

We shall now show that%
\begin{equation}
b_{\mathbf{d}_{p}\left(  i\right)  ,j}=c_{i,j}
\label{pf.prop.addcol.props1.e.10}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n-1\right\}  $ and $j\in\left\{
1,2,\ldots,m+1\right\}  $.

\textit{Proof of (\ref{pf.prop.addcol.props1.e.10}):} Let $i\in\left\{
1,2,\ldots,n-1\right\}  $ and $j\in\left\{  1,2,\ldots,m+1\right\}  $. We must
prove the equality (\ref{pf.prop.addcol.props1.e.10}). We have $\mathbf{d}%
_{p}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} From $i\in\left\{  1,2,\ldots,n-1\right\}
$, we obtain $i\leq n-1$. Now, the definition of $\mathbf{d}_{p}\left(
i\right)  $ yields%
\begin{align*}
\mathbf{d}_{p}\left(  i\right)   &  =%
\begin{cases}
i, & \text{if }i<p;\\
i+1, & \text{if }i\geq p
\end{cases}
\\
&  \leq%
\begin{cases}
i+1, & \text{if }i<p;\\
i+1, & \text{if }i\geq p
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }i\leq i+1\text{ in the case when }i<p\text{,}\\
\text{and since }i+1\leq i+1\text{ in the case when }i\geq p
\end{array}
\right) \\
&  =i+1\leq n\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq n-1\right)  .
\end{align*}
Combined with%
\begin{align*}
\mathbf{d}_{p}\left(  i\right)   &  =%
\begin{cases}
i, & \text{if }i<p;\\
i+1, & \text{if }i\geq p
\end{cases}
\\
&  \geq%
\begin{cases}
i, & \text{if }i<p;\\
i, & \text{if }i\geq p
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }i\geq i\text{ in the case when }i<p\text{,}\\
\text{and since }i+1\geq i\text{ in the case when }i\geq p
\end{array}
\right) \\
&  =i\geq1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\in\left\{  1,2,\ldots
,n-1\right\}  \right)  ,
\end{align*}
this yields $\mathbf{d}_{p}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}
$. Qed.}.

We are in one of the following two cases:

\textit{Case 1:} We have $j\neq m+1$.

\textit{Case 2:} We have $j=m+1$.

Let us first consider Case 1. In this case, we have $j\neq m+1$. Combined with
$j\in\left\{  1,2,\ldots,m+1\right\}  $, this yields $j\in\left\{
1,2,\ldots,m+1\right\}  \setminus\left\{  m+1\right\}  =\left\{
1,2,\ldots,m\right\}  $. Hence, (\ref{pf.prop.addcol.props1.e.5}) yields
$c_{i,j}=a_{\mathbf{d}_{p}\left(  i\right)  ,j}$. Thus, $a_{\mathbf{d}%
_{p}\left(  i\right)  ,j}=c_{i,j}$. But (\ref{pf.prop.addcol.props1.e.1})
(applied to $\mathbf{d}_{p}\left(  i\right)  $ instead of $i$) yields
$b_{\mathbf{d}_{p}\left(  i\right)  ,j}=a_{\mathbf{d}_{p}\left(  i\right)
,j}$ (since $\mathbf{d}_{p}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}
$). Hence, $b_{\mathbf{d}_{p}\left(  i\right)  ,j}=a_{\mathbf{d}_{p}\left(
i\right)  ,j}=c_{i,j}$. Hence, (\ref{pf.prop.addcol.props1.e.10}) is proven in
Case 1.

Let us now consider Case 2. In this case, we have $j=m+1$. Hence,
$c_{i,j}=c_{i,m+1}=v_{\mathbf{d}_{p}\left(  i\right)  }$ (by
(\ref{pf.prop.addcol.props1.e.6})). Thus, $v_{\mathbf{d}_{p}\left(  i\right)
}=c_{i,j}$. But (\ref{pf.prop.addcol.props1.e.2}) (applied to $\mathbf{d}%
_{p}\left(  i\right)  $ instead of $i$) yields $b_{\mathbf{d}_{p}\left(
i\right)  ,m+1}=v_{\mathbf{d}_{p}\left(  i\right)  }$ (since $\mathbf{d}%
_{p}\left(  i\right)  \in\left\{  1,2,\ldots,n\right\}  $). Now, from $j=m+1$,
we obtain $b_{\mathbf{d}_{p}\left(  i\right)  ,j}=b_{\mathbf{d}_{p}\left(
i\right)  ,m+1}=v_{\mathbf{d}_{p}\left(  i\right)  }=c_{i,j}$. Hence,
(\ref{pf.prop.addcol.props1.e.10}) is proven in Case 2.

We have now proven the equality (\ref{pf.prop.addcol.props1.e.10}) in each of
the two Cases 1 and 2. Since these two Cases cover all possibilities, this
shows that (\ref{pf.prop.addcol.props1.e.10}) always holds. Thus,
(\ref{pf.prop.addcol.props1.e.10}) is proven.

Now, (\ref{pf.prop.addcol.props1.e.3}) becomes%
\[
\left(  A\mid v\right)  _{\sim p,\bullet}=\left(  \underbrace{b_{\mathbf{d}%
_{p}\left(  i\right)  ,j}}_{\substack{=c_{i,j}\\\text{(by
(\ref{pf.prop.addcol.props1.e.10}))}}}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq
m+1}=\left(  c_{i,j}\right)  _{1\leq i\leq n-1,\ 1\leq j\leq m+1}=\left(
A_{\sim p,\bullet}\mid v_{\sim p,\bullet}\right)
\]
(by (\ref{pf.prop.addcol.props1.e.mat2})). This proves Proposition
\ref{prop.addcol.props1} \textbf{(e)}.

\textbf{(f)} Let $p\in\left\{  1,2,\ldots,n\right\}  $. Then, $\left(  A\mid
v\right)  $ is an $n\times\left(  m+1\right)  $-matrix. Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to $m+1$, $\left(  A\mid
v\right)  $, $p$ and $m+1$ instead of $u$ and $v$) yields
\[
\left(  \left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }\right)
_{\sim p,\bullet}=\left(  \left(  A\mid v\right)  _{\sim p,\bullet}\right)
_{\bullet,\sim\left(  m+1\right)  }=\left(  A\mid v\right)  _{\sim
p,\sim\left(  m+1\right)  }.
\]
Hence,%
\[
\left(  A\mid v\right)  _{\sim p,\sim\left(  m+1\right)  }=\left(
\underbrace{\left(  A\mid v\right)  _{\bullet,\sim\left(  m+1\right)  }%
}_{\substack{=A\\\text{(by Proposition \ref{prop.addcol.props1} \textbf{(d)}%
)}}}\right)  _{\sim p,\bullet}=A_{\sim p,\bullet}.
\]
This proves Proposition \ref{prop.addcol.props1} \textbf{(f)}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.addcol.props2}.]We have $n-1\in\mathbb{N}$
(since $n$ is a positive integer).

\textbf{(a)} Write the $n\times\left(  n-1\right)  $-matrix $A$ in the form
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n-1}\in
\mathbb{K}^{n\times\left(  n-1\right)  }$. Let $v=\left(  v_{1},v_{2}%
,\ldots,v_{n}\right)  ^{T}\in\mathbb{K}^{n\times1}$ be a vector.

Clearly, $\left(  A\mid v\right)  $ is an $n\times\left(  \left(  n-1\right)
+1\right)  $-matrix (since $A$ is an $n\times\left(  n-1\right)  $-matrix, and
since $v$ is an $n\times1$-matrix). In other words, $\left(  A\mid v\right)  $
is an $n\times n$-matrix (since $\left(  n-1\right)  +1=n$). Thus,
$\det\left(  A\mid v\right)  $ is well-defined.

Write the matrix $\left(  A\mid v\right)  $ in the form $\left(  A\mid
v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. (This is
possible since $\left(  A\mid v\right)  $ is an $n\times n$-matrix.) Thus,
\[
\left(  A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq\left(  n-1\right)
+1}%
\]
(since $n=\left(  n-1\right)  +1$). Hence,
(\ref{eq.lem.sol.prop.addcol.props.1.2}) shows that we have%
\[
b_{i,\left(  n-1\right)  +1}=v_{i}%
\]
for every $i\in\left\{  1,2,\ldots,n\right\}  $. Since $\left(  n-1\right)
+1=n$, this rewrites as follows: We have%
\begin{equation}
b_{i,n}=v_{i} \label{pf.prop.addcols.props2.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $.

But every $p\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left(  A\mid v\right)  _{\sim p,\sim n}=A_{\sim p,\bullet}
\label{pf.prop.addcols.props2.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.addcols.props2.2}):} Let
$p\in\left\{  1,2,\ldots,n\right\}  $. Proposition \ref{prop.addcol.props1}
\textbf{(f)} (applied to $m=n-1$) yields $\left(  A\mid v\right)  _{\sim
p,\sim\left(  \left(  n-1\right)  +1\right)  }=A_{\sim p,\bullet}$. This
rewrites as $\left(  A\mid v\right)  _{\sim p,\sim n}=A_{\sim p,\bullet}$
(since $\left(  n-1\right)  +1=n$). This proves
(\ref{pf.prop.addcols.props2.2}).}.

We have $n\in\left\{  1,2,\ldots,n\right\}  $ (since $n$ is a positive
integer) and $\left(  A\mid v\right)  =\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Hence, Theorem \ref{thm.laplace.gen} \textbf{(b)}
(applied to $\left(  A\mid v\right)  $, $b_{i,j}$ and $n$ instead of $A$,
$a_{i,j}$ and $q$) yields%
\begin{align*}
\det\left(  A\mid v\right)   &  =\sum_{p=1}^{n}\underbrace{\left(  -1\right)
^{p+n}}_{\substack{=\left(  -1\right)  ^{n+p}\\\text{(since }p+n=n+p\text{)}%
}}\underbrace{b_{p,n}}_{\substack{=v_{p}\\\text{(by
(\ref{pf.prop.addcols.props2.1}),}\\\text{applied to }i=p\text{)}}}\det\left(
\underbrace{\left(  A\mid v\right)  _{\sim p,\sim n}}_{\substack{=A_{\sim
p,\bullet}\\\text{(by (\ref{pf.prop.addcols.props2.2}))}}}\right) \\
&  =\sum_{p=1}^{n}\left(  -1\right)  ^{n+p}v_{p}\det\left(  A_{\sim p,\bullet
}\right)  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}v_{i}\det\left(  A_{\sim
i,\bullet}\right)
\end{align*}
(here, we have renamed the summation index $p$ as $i$). This proves
Proposition \ref{prop.addcol.props2} \textbf{(a)}.

\textbf{(b)} Let $p\in\left\{  1,2,\ldots,n\right\}  $. For any two objects
$i$ and $j$, we define an element $\delta_{i,j}\in\mathbb{K}$ by $\delta
_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$). Now, $\left(  I_{n}\right)  _{\bullet,p}$ is
the $p$-th column of the matrix $I_{n}$ (by the definition of $\left(
I_{n}\right)  _{\bullet,p}$). Thus,%
\begin{align*}
\left(  I_{n}\right)  _{\bullet,p}  &  =\left(  \text{the }p\text{-th column
of the matrix }I_{n}\right) \\
&  =\left(
\begin{array}
[c]{c}%
\delta_{1,p}\\
\delta_{2,p}\\
\vdots\\
\delta_{n,p}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I_{n}=\left(  \delta
_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  \delta_{1,p},\delta_{2,p},\ldots,\delta_{n,p}\right)  ^{T}.
\end{align*}
Hence, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to $\left(
I_{n}\right)  _{\bullet,p}$ and $\delta_{i,p}$ instead of $v$ and $v_{i}$)
yields%
\begin{align*}
&  \det\left(  A\mid\left(  I_{n}\right)  _{\bullet,p}\right) \\
&  =\underbrace{\sum_{i=1}^{n}}_{=\sum_{i\in\left\{  1,2,\ldots,n\right\}  }%
}\left(  -1\right)  ^{n+i}\delta_{i,p}\det\left(  A_{\sim i,\bullet}\right)
=\sum_{i\in\left\{  1,2,\ldots,n\right\}  }\left(  -1\right)  ^{n+i}%
\delta_{i,p}\det\left(  A_{\sim i,\bullet}\right) \\
&  =\left(  -1\right)  ^{n+p}\underbrace{\delta_{p,p}}%
_{\substack{=1\\\text{(since }p=p\text{)}}}\det\left(  A_{\sim p,\bullet
}\right)  +\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}\left(  -1\right)  ^{n+i}\underbrace{\delta_{i,p}}%
_{\substack{=0\\\text{(since }i\neq p\text{)}}}\det\left(  A_{\sim i,\bullet
}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }i=p\text{ from the sum,}\\
\text{since }p\in\left\{  1,2,\ldots,n\right\}
\end{array}
\right) \\
&  =\left(  -1\right)  ^{n+p}\det\left(  A_{\sim p,\bullet}\right)
+\underbrace{\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}\left(  -1\right)  ^{n+i}0\det\left(  A_{\sim i,\bullet}\right)  }%
_{=0}=\left(  -1\right)  ^{n+p}\det\left(  A_{\sim p,\bullet}\right)  .
\end{align*}
This proves Proposition \ref{prop.addcol.props2} \textbf{(b)}.
\end{proof}

Before we prove Proposition \ref{prop.addcol.props3}, let us make two more
trivial observations:

\begin{lemma}
\label{lem.sol.prop.addcol.props.cols}Let $n\in\mathbb{N}$ and $m\in
\mathbb{N}$. Let $A$ and $B$ be two $n\times m$-matrices. Assume that
$A_{\bullet,q}=B_{\bullet,q}$ for each $q\in\left\{  1,2,\ldots,m\right\}  $.
Then, $A=B$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.addcol.props.cols}.]We have assumed that%
\begin{equation}
A_{\bullet,q}=B_{\bullet,q} \label{pf.lem.sol.prop.addcol.props.cols.AB}%
\end{equation}
for each $q\in\left\{  1,2,\ldots,m\right\}  $. For every $q\in\left\{
1,2,\ldots,m\right\}  $, we have%
\begin{equation}
A_{\bullet,q}=\left(  \text{the }q\text{-th column of the matrix }A\right)
\label{pf.lem.sol.prop.addcol.props.cols.A}%
\end{equation}
(since $A_{\bullet,q}$ is the $q$-th column of the matrix $A$ (by the
definition of $A_{\bullet,q}$)) and%
\begin{equation}
B_{\bullet,q}=\left(  \text{the }q\text{-th column of the matrix }B\right)
\label{pf.lem.sol.prop.addcol.props.cols.B}%
\end{equation}
(since $B_{\bullet,q}$ is the $q$-th column of the matrix $B$ (by the
definition of $B_{\bullet,q}$)). Hence, for every $q\in\left\{  1,2,\ldots
,m\right\}  $, we have%
\begin{align}
&  \left(  \text{the }q\text{-th column of the matrix }A\right) \nonumber\\
&  =A_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.cols.A})}\right) \nonumber\\
&  =B_{\bullet,q}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.cols.AB})}\right) \nonumber\\
&  =\left(  \text{the }q\text{-th column of the matrix }B\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.cols.B})}\right)  .
\label{pf.lem.sol.prop.addcol.props.cols.q=q}%
\end{align}


Write the matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. (This is possible since $A$ is an $n\times m$-matrix.)

Write the matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. (This is possible since $B$ is an $n\times m$-matrix.)

We have $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Thus,%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}A\right)  =a_{i,j} \label{pf.lem.sol.prop.addcol.props.cols.Aij}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $.

We have $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$. Thus,%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right)  =b_{i,j} \label{pf.lem.sol.prop.addcol.props.cols.Bij}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $.

Now, for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,m\right\}  $, we have%
\begin{align*}
a_{i,j}  &  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the
matrix }A\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.cols.Aij})}\right) \\
&  =\left(  \text{the }i\text{-th entry of }\underbrace{\text{the }j\text{-th
column of the matrix }A}_{\substack{=\left(  \text{the }j\text{-th column of
the matrix }B\right)  \\\text{(by (\ref{pf.lem.sol.prop.addcol.props.cols.q=q}%
), applied to }q=j\text{)}}}\right) \\
&  =\left(  \text{the }i\text{-th entry of the }j\text{-th column of the
matrix }B\right) \\
&  =\left(  \text{the }\left(  i,j\right)  \text{-th entry of the matrix
}B\right) \\
&  =b_{i,j}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.prop.addcol.props.cols.Bij})}\right)  .
\end{align*}
Hence, $\left(  \underbrace{a_{i,j}}_{=b_{i,j}}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=B$
(since $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$). Thus,
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}=B$. This proves
Lemma \ref{lem.sol.prop.addcol.props.cols}.
\end{proof}

\begin{lemma}
\label{lem.sol.prop.addcol.props3.bc}Let $n\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\in\mathbb{K}^{n\times n}$ be
an $n\times n$-matrix. Let $r$ and $q$ be two elements of $\left\{
1,2,\ldots,n\right\}  $. Then,%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =\left(  -1\right)
^{n+q}\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  .\nonumber
\end{equation}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.prop.addcol.props3.bc}.]We have $r\in\left\{
1,2,\ldots,n\right\}  $. Hence, $A_{\bullet,r}$ is an $n\times1$-matrix (since
$A$ is an $n\times n$-matrix). Also, $r\in\left\{  1,2,\ldots,n\right\}  $
shows that $1\leq r\leq n$; hence, $1\leq n$, so that $n$ is a positive integer.

Also, $q\in\left\{  1,2,\ldots,n\right\}  $. Hence, $A_{\bullet,\sim q}$ is an
$n\times\left(  n-1\right)  $-matrix (since $A$ is an $n\times n$-matrix); in
other words, $A_{\bullet,\sim q}\in\mathbb{K}^{n\times\left(  n-1\right)  }$.

Furthermore, $A_{\bullet,r}$ is the $r$-th column of the matrix $A$ (by the
definition of $A_{\bullet,r}$). Thus,%
\begin{align*}
A_{\bullet,r}  &  =\left(  \text{the }r\text{-th column of the matrix
}A\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,r}\\
a_{2,r}\\
\vdots\\
a_{n,r}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}\right) \\
&  =\left(  a_{1,r},a_{2,r},\ldots,a_{n,r}\right)  ^{T}.
\end{align*}
Thus, Proposition \ref{prop.addcol.props2} \textbf{(a)} (applied to
$A_{\bullet,\sim q}$, $A_{\bullet,r}$ and $a_{i,r}$ instead of $A$, $v$ and
$v_{i}$) yields%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =\sum_{i=1}%
^{n}\left(  -1\right)  ^{n+i}a_{i,r}\det\left(  \left(  A_{\bullet,\sim
q}\right)  _{\sim i,\bullet}\right)  .
\label{pf.lem.sol.prop.addcol.props3.bc.1}%
\end{equation}
But every $i\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet}=A_{\sim i,\sim q}
\label{pf.lem.sol.prop.addcol.props3.bc.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.prop.addcol.props3.bc.2}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $i$ and $q$ instead of
$m$, $u$ and $v$) yields $\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet
}=\left(  A_{\sim i,\bullet}\right)  _{\bullet,\sim q}=A_{\sim i,\sim q}$.
Hence, $\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet}=A_{\sim i,\sim
q}$. This proves (\ref{pf.lem.sol.prop.addcol.props3.bc.2}).} and%
\begin{equation}
\left(  -1\right)  ^{n+i}=\left(  -1\right)  ^{n+q}\left(  -1\right)  ^{i+q}
\label{pf.lem.sol.prop.addcol.props3.bc.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.prop.addcol.props3.bc.3}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Then, $\left(  -1\right)  ^{n+q}\left(
-1\right)  ^{i+q}=\left(  -1\right)  ^{\left(  n+q\right)  +\left(
i+q\right)  }=\left(  -1\right)  ^{n+i}$ (since $\left(  n+q\right)  +\left(
i+q\right)  =n+i+2q\equiv n+i\operatorname{mod}2$). This proves
(\ref{pf.lem.sol.prop.addcol.props3.bc.3}).}. Now,
(\ref{pf.lem.sol.prop.addcol.props3.bc.1}) becomes%
\begin{align*}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)   &  =\sum_{i=1}%
^{n}\underbrace{\left(  -1\right)  ^{n+i}}_{\substack{=\left(  -1\right)
^{n+q}\left(  -1\right)  ^{i+q}\\\text{(by
(\ref{pf.lem.sol.prop.addcol.props3.bc.3}))}}}a_{i,r}\det\left(
\underbrace{\left(  A_{\bullet,\sim q}\right)  _{\sim i,\bullet}%
}_{\substack{=A_{\sim i,\sim q}\\\text{(by
(\ref{pf.lem.sol.prop.addcol.props3.bc.2}))}}}\right) \\
&  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+q}\left(  -1\right)  ^{i+q}%
a_{i,r}\det\left(  A_{\sim i,\sim q}\right) \\
&  =\left(  -1\right)  ^{n+q}\sum_{i=1}^{n}\left(  -1\right)  ^{i+q}%
a_{i,r}\det\left(  A_{\sim i,\sim q}\right) \\
&  =\left(  -1\right)  ^{n+q}\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}%
a_{p,r}\det\left(  A_{\sim p,\sim q}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}i\text{ as }p\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.prop.addcol.props3.bc}.
\end{proof}

\begin{proof}
[Proof of Proposition \ref{prop.addcol.props3}.]\textbf{(a)} Assume that
$n>0$. Thus, $n$ is a positive integer, so that $n\in\left\{  1,2,\ldots
,n\right\}  $. Hence, $A_{\bullet,\sim n}$ is a well-defined $n\times\left(
n-1\right)  $-matrix, and $A_{\bullet,n}$ is a well-defined $n\times1$-matrix.
Therefore, $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  $ is an
$n\times\left(  \left(  n-1\right)  +1\right)  $-matrix. In other words,
$\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  $ is an $n\times
n$-matrix (since $\left(  n-1\right)  +1=n$). Also, $n-1\in\mathbb{N}$ (since
$n$ is a positive integer).

We shall now show that%
\begin{equation}
A_{\bullet,q}=\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q} \label{pf.prop.addcol.props3.a.main}%
\end{equation}
for each $q\in\left\{  1,2,\ldots,n\right\}  $.

\textit{Proof of (\ref{pf.prop.addcol.props3.a.main}):} Let $q\in\left\{
1,2,\ldots,n\right\}  $. We must prove the equality
(\ref{pf.prop.addcol.props3.a.main}). We are in one of the following two cases:

\textit{Case 1:} We have $q\neq n$.

\textit{Case 2:} We have $q=n$.

Let us first consider Case 1. In this case, we have $q\neq n$. Combining
$q\in\left\{  1,2,\ldots,n\right\}  $ with $q\neq n$, we obtain $q\in\left\{
1,2,\ldots,n\right\}  \setminus\left\{  n\right\}  =\left\{  1,2,\ldots
,n-1\right\}  $. Thus, Proposition \ref{prop.addcol.props1} \textbf{(a)}
(applied to $n-1$, $A_{\bullet,\sim n}$ and $A_{\bullet,n}$ instead of $m$,
$A$ and $v$) yields $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}=\left(  A_{\bullet,\sim n}\right)  _{\bullet,q}$.

But Proposition \ref{prop.unrows.basics} \textbf{(d)} (applied to $n$, $n$ and
$q$ instead of $m$, $v$ and $w$) yields $\left(  A_{\bullet,\sim n}\right)
_{\bullet,q}=A_{\bullet,q}$. Hence, $A_{\bullet,q}=\left(  A_{\bullet,\sim
n}\right)  _{\bullet,q}=\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}$ (since $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
_{\bullet,q}=\left(  A_{\bullet,\sim n}\right)  _{\bullet,q}$). Hence,
(\ref{pf.prop.addcol.props3.a.main}) is proven in Case 1.

Let us now consider Case 2. In this case, we have $q=n$. But Proposition
\ref{prop.addcol.props1} \textbf{(b)} (applied to $n-1$, $A_{\bullet,\sim n}$
and $A_{\bullet,n}$ instead of $m$, $A$ and $v$) yields $\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  _{\bullet,\left(  n-1\right)
+1}=A_{\bullet,n}$. Thus, $A_{\bullet,n}=\left(  A_{\bullet,\sim n}\mid
A_{\bullet,n}\right)  _{\bullet,\left(  n-1\right)  +1}=\left(  A_{\bullet
,\sim n}\mid A_{\bullet,n}\right)  _{\bullet,q}$ (since $\left(  n-1\right)
+1=n=q$). Now, $q=n$, so that $A_{\bullet,q}=A_{\bullet,n}=\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  _{\bullet,q}$. Hence,
(\ref{pf.prop.addcol.props3.a.main}) is proven in Case 2.

Now, (\ref{pf.prop.addcol.props3.a.main}) is proven in each of the two Cases 1
and 2. Since these two Cases cover all possibilities, this completes the proof
of (\ref{pf.prop.addcol.props3.a.main}).

Now we know that $A$ and $\left(  A_{\bullet,\sim n}\mid A_{\bullet,n}\right)
$ are two $n\times n$-matrices and that (\ref{pf.prop.addcol.props3.a.main})
holds for each $q\in\left\{  1,2,\ldots,n\right\}  $. Hence, Lemma
\ref{lem.sol.prop.addcol.props.cols} (applied to $m=n$ and $B=\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  $) yields $A=\left(
A_{\bullet,\sim n}\mid A_{\bullet,n}\right)  $. This proves Proposition
\ref{prop.addcol.props3} \textbf{(a)}.

\textbf{(b)} Write the matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. (This is possible since $A$ is an $n\times
n$-matrix.)

Lemma \ref{lem.sol.prop.addcol.props3.bc} (applied to $r=q$) yields%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,q}\right)  =\left(  -1\right)
^{n+q}\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim
q}\right)  .\nonumber
\end{equation}
Comparing this with%
\[
\left(  -1\right)  ^{n+q}\underbrace{\det A}_{\substack{=\sum_{p=1}^{n}\left(
-1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim q}\right)  \\\text{(by
Theorem \ref{thm.laplace.gen} \textbf{(b)})}}}=\left(  -1\right)  ^{n+q}%
\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,q}\det\left(  A_{\sim p,\sim
q}\right)  ,
\]
we obtain $\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,q}\right)  =\left(
-1\right)  ^{n+q}\det A$. This proves Proposition \ref{prop.addcol.props3}
\textbf{(b)}.

[\textit{Remark:} Let me outline an alternative proof of Proposition
\ref{prop.addcol.props3} \textbf{(b)}: Let $q\in\left\{  1,2,\ldots,n\right\}
$. The matrix $\left(  A_{\bullet,\sim q}\mid A_{\bullet,q}\right)  $ is
obtained from the matrix $A$ by removing the $q$-th column and then
reattaching this column to the right end of the matrix. This procedure can be
replaced by the following procedure, which clearly leads to the same result:

\begin{itemize}
\item Switch the $q$-th column of $A$ with the $\left(  q+1\right)  $-th column;

\item then switch the $\left(  q+1\right)  $-th column of the resulting matrix
with the $\left(  q+2\right)  $-th column;

\item then switch the $\left(  q+2\right)  $-th column of the resulting matrix
with the $\left(  q+3\right)  $-th column;

\item and so on, finally switching the $\left(  n-1\right)  $-st column of the
matrix with the $n$-th column.
\end{itemize}

But this latter procedure is a sequence of $n-q$ switches of two columns. Each
such switch multiplies the determinant of the matrix by $-1$ (according to
Exercise \ref{exe.ps4.6} \textbf{(b)}). Thus, the whole procedure multiplies
the determinant of the matrix by $\left(  -1\right)  ^{n-q}=\left(  -1\right)
^{n+q}$ (since $n-q\equiv n+q\operatorname{mod}2$). Since this procedure takes
the matrix $A$ to the matrix $\left(  A_{\bullet,\sim q}\mid A_{\bullet
,q}\right)  $, we thus conclude that $\det\left(  A_{\bullet,\sim q}\mid
A_{\bullet,q}\right)  =\left(  -1\right)  ^{n+q}\det A$. This proves
Proposition \ref{prop.addcol.props3} \textbf{(b)} again.]

\textbf{(c)} Let $r$ and $q$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $r\neq q$. We must show that $\det\left(
A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =0$.

Write the matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. (This is possible since $A$ is an $n\times n$-matrix.)

We have $q\neq r$ (since $r\neq q$). Hence, Proposition \ref{prop.laplace.0}
\textbf{(b)} yields%
\begin{equation}
0=\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(  A_{\sim p,\sim
q}\right)  . \label{pf.prop.addcol.props3.c.1}%
\end{equation}


Lemma \ref{lem.sol.prop.addcol.props3.bc} yields%
\begin{equation}
\det\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  =\left(  -1\right)
^{n+q}\underbrace{\sum_{p=1}^{n}\left(  -1\right)  ^{p+q}a_{p,r}\det\left(
A_{\sim p,\sim q}\right)  }_{\substack{=0\\\text{(by
(\ref{pf.prop.addcol.props3.c.1}))}}}=0.\nonumber
\end{equation}
This proves Proposition \ref{prop.addcol.props3} \textbf{(c)}.

[\textit{Remark:} Let me outline an alternative proof of Proposition
\ref{prop.addcol.props3} \textbf{(c)}: Let $r$ and $q$ be two elements of
$\left\{  1,2,\ldots,n\right\}  $ satisfying $r\neq q$. Then, $A_{\bullet,r}$
is one of the columns of the matrix $A_{\bullet,\sim q}$ (since the $r$-th
column of the matrix $A$ is not lost when the $q$-th column is removed).
Hence, the column vector $A_{\bullet,r}$ appears twice as a column in the
matrix $\left(  A_{\bullet,\sim q}\mid A_{\bullet,r}\right)  $ (namely, once
as one of the columns of $A_{\bullet,\sim q}$, and another time as the
attached column). Therefore, the matrix $\left(  A_{\bullet,\sim q}\mid
A_{\bullet,r}\right)  $ has two equal columns. Exercise \ref{exe.ps4.6}
\textbf{(f)} thus shows that $\det\left(  A_{\bullet,\sim q}\mid A_{\bullet
,r}\right)  =0$. This proves Proposition \ref{prop.addcol.props3} \textbf{(c)} again.]

\textbf{(d)} Let $p\in\left\{  1,2,\ldots,n\right\}  $ and $q\in\left\{
1,2,\ldots,n\right\}  $. Then, Proposition \ref{prop.unrows.basics}
\textbf{(c)} (applied to $u=p$ and $v=q$) yields%
\[
\left(  A_{\bullet,\sim q}\right)  _{\sim p,\bullet}=\left(  A_{\sim
p,\bullet}\right)  _{\bullet,\sim q}=A_{\sim p,\sim q}.
\]


But $A$ is an $n\times n$-matrix. Hence, $A_{\bullet,\sim q}$ is an
$n\times\left(  n-1\right)  $-matrix (since $q\in\left\{  1,2,\ldots
,n\right\}  $). Moreover, $p\in\left\{  1,2,\ldots,n\right\}  $, so that
$1\leq p\leq n$ and thus $n\geq1$; hence, $n$ is a positive integer.
Proposition \ref{prop.addcol.props2} \textbf{(b)} (applied to $A_{\bullet,\sim
q}$ instead of $A$) thus yields
\[
\det\left(  A_{\bullet,\sim q}\mid\left(  I_{n}\right)  _{\bullet,p}\right)
=\left(  -1\right)  ^{n+p}\det\left(  \underbrace{\left(  A_{\bullet,\sim
q}\right)  _{\sim p,\bullet}}_{=A_{\sim p,\sim q}}\right)  =\left(  -1\right)
^{n+p}\det\left(  A_{\sim p,\sim q}\right)  .
\]
This proves Proposition \ref{prop.addcol.props3} \textbf{(d)}.

\textbf{(e)} Let $u$ and $v$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $u<v$. Let $r$ be an element of $\left\{
1,2,\ldots,n-1\right\}  $ satisfying $r\neq u$. We must show that $\det\left(
A_{\bullet,\sim u}\mid\left(  A_{\bullet,\sim v}\right)  _{\bullet,r}\right)
=0$.

We are in one of the following two cases:

\textit{Case 1:} We have $r<v$.

\textit{Case 2:} We have $r\geq v$.

Let us first consider Case 1. In this case, we have $r<v$. Thus, $r\leq v-1$
(since $r$ and $v$ are integers). But $r\geq1$ (since $r\in\left\{
1,2,\ldots,n-1\right\}  $). Combining $r\geq1$ with $r\leq v-1$, we obtain
$r\in\left\{  1,2,\ldots,v-1\right\}  $. Hence, Proposition
\ref{prop.unrows.basics} \textbf{(d)} (applied to $m=n$ and $w=r$) yields
$\left(  A_{\bullet,\sim v}\right)  _{\bullet,r}=A_{\bullet,r}$. Hence,%
\[
\det\left(  A_{\bullet,\sim u}\mid\underbrace{\left(  A_{\bullet,\sim
v}\right)  _{\bullet,r}}_{=A_{\bullet,r}}\right)  =\det\left(  A_{\bullet,\sim
u}\mid A_{\bullet,r}\right)  =0
\]
(by Proposition \ref{prop.addcol.props3} \textbf{(c)}, applied to $q=u$).
Hence, Proposition \ref{prop.addcol.props3} \textbf{(e)} is proven in Case 1.

Let us now consider Case 2. In this case, we have $r\geq v$. But $r\leq n-1$
(since $r\in\left\{  1,2,\ldots,n-1\right\}  $), so that $r+1\leq n$.
Combining this with $r+1\geq r\geq1$ (since $r\in\left\{  1,2,\ldots
,n\right\}  $), we obtain $r+1\in\left\{  1,2,\ldots,n\right\}  $.
Furthermore, $r+1>r\geq v>u$ (since $u<v$) and thus $r+1\neq u$. Hence,
Proposition \ref{prop.addcol.props3} \textbf{(c)} (applied to $r+1$ and $u$
instead of $r$ and $q$) yields $\det\left(  A_{\bullet,\sim u}\mid
A_{\bullet,r+1}\right)  =0$.

But combining $r\geq v$ with $r\leq n-1$, we obtain $r\in\left\{
v,v+1,\ldots,n-1\right\}  $. Thus, Proposition \ref{prop.unrows.basics}
\textbf{(e)} (applied to $m=n$ and $w=r$) yields $\left(  A_{\bullet,\sim
v}\right)  _{\bullet,r}=A_{\bullet,r+1}$. Hence,%
\[
\det\left(  A_{\bullet,\sim u}\mid\underbrace{\left(  A_{\bullet,\sim
v}\right)  _{\bullet,r}}_{=A_{\bullet,r+1}}\right)  =\det\left(
A_{\bullet,\sim u}\mid A_{\bullet,r+1}\right)  =0.
\]
Hence, Proposition \ref{prop.addcol.props3} \textbf{(e)} is proven in Case 2.

We have now proven Proposition \ref{prop.addcol.props3} \textbf{(e)} in each
of the two Cases 1 and 2. Since these two Cases cover all possibilities, this
shows that Proposition \ref{prop.addcol.props3} \textbf{(e)} always holds.

\textbf{(f)} Let $u$ and $v$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $u<v$. Then, $u<v$, so that $u\leq v-1$ (since $u$
and $v$ are integers). Combined with $u\geq1$ (since $u\in\left\{
1,2,\ldots,n\right\}  $), this yields $u\in\left\{  1,2,\ldots,v-1\right\}  $.
Hence, Proposition \ref{prop.unrows.basics} \textbf{(d)} (applied to $m=n$ and
$w=u$) yields $\left(  A_{\bullet,\sim v}\right)  _{\bullet,u}=A_{\bullet,u}$.
Thus,%
\[
\det\left(  A_{\bullet,\sim u}\mid\underbrace{\left(  A_{\bullet,\sim
v}\right)  _{\bullet,u}}_{=A_{\bullet,u}}\right)  =\det\left(  A_{\bullet,\sim
u}\mid A_{\bullet,u}\right)  =\left(  -1\right)  ^{n+u}\det A
\]
(by Proposition \ref{prop.addcol.props3} \textbf{(b)}, applied to $q=u$).
Thus,%
\[
\left(  -1\right)  ^{u}\underbrace{\det\left(  A_{\bullet,\sim u}\mid\left(
A_{\bullet,\sim v}\right)  _{\bullet,u}\right)  }_{=\left(  -1\right)
^{n+u}\det A}=\underbrace{\left(  -1\right)  ^{u}\left(  -1\right)  ^{n+u}%
}_{\substack{=\left(  -1\right)  ^{u+\left(  n+u\right)  }=\left(  -1\right)
^{n}\\\text{(since }u+\left(  n+u\right)  =2u+n\equiv n\operatorname{mod}%
2\text{)}}}\det A=\left(  -1\right)  ^{n}\det A.
\]
This proves Proposition \ref{prop.addcol.props3} \textbf{(f)}.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Exercise \ref{exe.prop.addcol.props}.]We have proven Proposition
\ref{prop.addcol.props1}, Proposition \ref{prop.addcol.props2} and Proposition
\ref{prop.addcol.props3}. Thus, Exercise \ref{exe.prop.addcol.props} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.desnanot.jaw}}

\begin{proof}
[Solution to Exercise \ref{exe.desnanot.jaw}.]We have $u<v$ and $u<w$. Define
an $n\times n$-matrix $C\in\mathbb{K}^{n\times n}$ as in Lemma
\ref{lem.desnanot.AB.tech}.

Lemma \ref{lem.desnanot.AB.tech} \textbf{(a)} yields%
\begin{align}
\det\left(  C_{\sim v,\sim q}\right)   &  =-\left(  -1\right)  ^{n+u}%
\underbrace{\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{v},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  }_{\substack{=\beta_{u,v}\\\text{(since }\beta_{u,v}=\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{v}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right)  \\\text{(by the
definition of }\beta_{u,v}\text{))}}}\nonumber\\
&  =-\left(  -1\right)  ^{n+u}\beta_{u,v}. \label{sol.desnanot.jaw.1}%
\end{align}


Lemma \ref{lem.desnanot.AB.tech} \textbf{(a)} (applied to $w$ instead of $v$)
yields%
\begin{align}
\det\left(  C_{\sim w,\sim q}\right)   &  =-\left(  -1\right)  ^{n+u}%
\underbrace{\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{u},\ldots,\widehat{w},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  }_{\substack{=\beta_{u,w}\\\text{(since }\beta_{u,w}=\det\left(
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{u},\ldots,\widehat{w}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \right)  \\\text{(by the
definition of }\beta_{u,w}\text{))}}}\nonumber\\
&  =-\left(  -1\right)  ^{n+u}\beta_{u,w}. \label{sol.desnanot.jaw.2}%
\end{align}


Moreover,
\begin{equation}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,\widehat{n},\ldots,n}%
C=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w}%
,\ldots,n}\left(  B_{\bullet,\sim q}\right)  \label{sol.desnanot.jaw.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.desnanot.jaw.3}):} From $C=\left(
B\mid\left(  I_{n}\right)  _{\bullet,u}\right)  $, we obtain
\begin{align*}
C_{\bullet,\sim n}  &  =\left(  B\mid\left(  I_{n}\right)  _{\bullet
,u}\right)  _{\bullet,\sim n}=\left(  B\mid\left(  I_{n}\right)  _{\bullet
,u}\right)  _{\bullet,\sim\left(  \left(  n-1\right)  +1\right)
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n=\left(  n-1\right)  +1\right) \\
&  =B
\end{align*}
(by Proposition \ref{prop.addcol.props1} \textbf{(d)}, applied to $n-1$, $B$
and $\left(  I_{n}\right)  _{\bullet,u}$ instead of $m$, $A$ and $v$).
\par
Proposition \ref{prop.submatrix.easy} \textbf{(d)} (applied to $n$, $C$,
$n-2$, $\left(  1,2,\ldots,\widehat{v},\ldots,\widehat{w},\ldots,n\right)  $,
$n-2$ and $\left(  1,2,\ldots,\widehat{q},\ldots,\widehat{n},\ldots,n\right)
$ instead of $m$, $A$, $u$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $, $v$
and $\left(  j_{1},j_{2},\ldots,j_{v}\right)  $) yields%
\begin{align}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,\widehat{n},\ldots,n}C  &
=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w}%
,\ldots,n}\left(  \operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q}%
,\ldots,\widehat{n},\ldots,n}C\right) \label{sol.desnanot.jaw.3.pf.1}\\
&  =\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots
,\widehat{n},\ldots,n}\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,\widehat{w},\ldots,n}C\right)  .\nonumber
\end{align}
\par
But $n$ is a positive integer; hence, $n\in\left\{  1,2,\ldots,n\right\}  $.
Also, $q\in\left\{  1,2,\ldots,n-1\right\}  $. Thus, Proposition
\ref{prop.unrows.basics} \textbf{(h)} (applied to $n$, $C$, $n$ and $q$
instead of $m$, $A$, $v$ and $w$) yields%
\[
\left(  C_{\bullet,\sim n}\right)  _{\bullet,\sim q}=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{q},\ldots,\widehat{n},\ldots,n}C.
\]
Hence,%
\[
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{q},\ldots,\widehat{n}%
,\ldots,n}C=\left(  \underbrace{C_{\bullet,\sim n}}_{=B}\right)
_{\bullet,\sim q}=B_{\bullet,\sim q}.
\]
Hence, (\ref{sol.desnanot.jaw.3.pf.1}) becomes%
\begin{align*}
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w}%
,\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,\widehat{n},\ldots,n}C  &
=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{v},\ldots,\widehat{w}%
,\ldots,n}\left(  \underbrace{\operatorname*{cols}\nolimits_{1,2,\ldots
,\widehat{q},\ldots,\widehat{n},\ldots,n}C}_{=B_{\bullet,\sim q}}\right) \\
&  =\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{v},\ldots
,\widehat{w},\ldots,n}\left(  B_{\bullet,\sim q}\right)  .
\end{align*}
This proves (\ref{sol.desnanot.jaw.3}).}. Taking determinants on both sides of
this equality, we obtain%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{v}%
,\ldots,\widehat{w},\ldots,n}^{1,2,\ldots,\widehat{q},\ldots,\widehat{n}%
,\ldots,n}C\right)  =\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,\widehat{w},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  =\beta_{v,w} \label{sol.desnanot.jaw.3b}%
\end{equation}
(since $\beta_{v,w}=\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,\widehat{w},\ldots,n}\left(  B_{\bullet,\sim q}\right)
\right)  $ (by the definition of $\beta_{v,w}$)).

Lemma \ref{lem.desnanot.AB.tech} \textbf{(c)} yields $C_{\sim v,\sim
n}=B_{\sim v,\bullet}$. Thus,%
\begin{equation}
\det\left(  \underbrace{C_{\sim v,\sim n}}_{=B_{\sim v,\bullet}}\right)
=\det\left(  B_{\sim v,\bullet}\right)  =\alpha_{v} \label{sol.desnanot.jaw.4}%
\end{equation}
(since $\alpha_{v}=\det\left(  B_{\sim v,\bullet}\right)  $ (by the definition
of $\alpha_{v}$)).

Lemma \ref{lem.desnanot.AB.tech} \textbf{(c)} (applied to $w$ instead of $v$)
yields $C_{\sim w,\sim n}=B_{\sim w,\bullet}$. Thus,%
\begin{equation}
\det\left(  \underbrace{C_{\sim w,\sim n}}_{=B_{\sim w,\bullet}}\right)
=\det\left(  B_{\sim w,\bullet}\right)  =\alpha_{w} \label{sol.desnanot.jaw.5}%
\end{equation}
(since $\alpha_{w}=\det\left(  B_{\sim w,\bullet}\right)  $ (by the definition
of $\alpha_{w}$)).

Lemma \ref{lem.desnanot.AB.tech} \textbf{(f)} yields%
\begin{equation}
\det C=\left(  -1\right)  ^{n+u}\underbrace{\det\left(  B_{\sim u,\bullet
}\right)  }_{\substack{=\alpha_{u}\\\text{(since }\alpha_{u}=\det\left(
B_{\sim u,\bullet}\right)  \\\text{(by the definition of }\alpha_{u}\text{))}%
}}=\left(  -1\right)  ^{n+u}\alpha_{u}. \label{sol.desnanot.jaw.6}%
\end{equation}


Now, $q\in\left\{  1,2,\ldots,n-1\right\}  $, so that $q\leq n-1<n$. Also,
$n\in\left\{  1,2,\ldots,n\right\}  $ (since $n$ is a positive integer) and
$q\in\left\{  1,2,\ldots,n-1\right\}  \subseteq\left\{  1,2,\ldots,n\right\}
$. Hence, Theorem \ref{thm.desnanot} (applied to $C$, $v$, $w$, $q$ and $n$
instead of $A$, $p$, $q$, $u$ and $v$) yields%
\begin{align*}
&  \det C\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,\widehat{w},\ldots,n}^{1,2,\ldots,\widehat{q}%
,\ldots,\widehat{n},\ldots,n}C\right) \\
&  =\underbrace{\det\left(  C_{\sim v,\sim q}\right)  }_{\substack{=-\left(
-1\right)  ^{n+u}\beta_{u,v}\\\text{(by (\ref{sol.desnanot.jaw.1}))}}%
}\cdot\underbrace{\det\left(  C_{\sim w,\sim n}\right)  }_{\substack{=\alpha
_{w}\\\text{(by (\ref{sol.desnanot.jaw.5}))}}}-\underbrace{\det\left(  C_{\sim
v,\sim n}\right)  }_{\substack{=\alpha_{v}\\\text{(by
(\ref{sol.desnanot.jaw.4}))}}}\cdot\underbrace{\det\left(  C_{\sim w,\sim
q}\right)  }_{\substack{=-\left(  -1\right)  ^{n+u}\beta_{u,w}\\\text{(by
(\ref{sol.desnanot.jaw.2}))}}}\\
&  =\left(  -\left(  -1\right)  ^{n+u}\beta_{u,v}\right)  \cdot\alpha
_{w}-\alpha_{v}\cdot\left(  -\left(  -1\right)  ^{n+u}\beta_{u,w}\right) \\
&  =-\left(  -1\right)  ^{n+u}\beta_{u,v}\cdot\alpha_{w}+\alpha_{v}%
\cdot\left(  -1\right)  ^{n+u}\beta_{u,w}\\
&  =\alpha_{v}\cdot\left(  -1\right)  ^{n+u}\beta_{u,w}-\left(  -1\right)
^{n+u}\beta_{u,v}\cdot\alpha_{w}=\left(  -1\right)  ^{n+u}\left(  \alpha
_{v}\beta_{u,w}-\beta_{u,v}\alpha_{w}\right)  .
\end{align*}
Hence,%
\begin{align*}
\left(  -1\right)  ^{n+u}\left(  \alpha_{v}\beta_{u,w}-\beta_{u,v}\alpha
_{w}\right)   &  =\underbrace{\det C}_{\substack{=\left(  -1\right)
^{n+u}\alpha_{u}\\\text{(by (\ref{sol.desnanot.jaw.6}))}}}\cdot
\underbrace{\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{v},\ldots,\widehat{w},\ldots,n}^{1,2,\ldots,\widehat{q}%
,\ldots,\widehat{n},\ldots,n}C\right)  }_{\substack{=\beta_{v,w}\\\text{(by
(\ref{sol.desnanot.jaw.3b}))}}}\\
&  =\left(  -1\right)  ^{n+u}\alpha_{u}\beta_{v,w}.
\end{align*}
Multiplying both sides of this equality by $\left(  -1\right)  ^{n+u}$, we
find%
\[
\left(  -1\right)  ^{n+u}\left(  -1\right)  ^{n+u}\left(  \alpha_{v}%
\beta_{u,w}-\beta_{u,v}\alpha_{w}\right)  =\underbrace{\left(  -1\right)
^{n+u}\left(  -1\right)  ^{n+u}}_{\substack{=\left(  -1\right)  ^{\left(
n+u\right)  +\left(  n+u\right)  }=1\\\text{(since }\left(  n+u\right)
+\left(  n+u\right)  =2\left(  n+u\right)  \text{ is even)}}}\alpha_{u}%
\beta_{v,w}=\alpha_{u}\beta_{v,w}.
\]
Hence,%
\begin{align*}
&  \alpha_{u}\beta_{v,w}\\
&  =\underbrace{\left(  -1\right)  ^{n+u}\left(  -1\right)  ^{n+u}%
}_{\substack{=\left(  -1\right)  ^{\left(  n+u\right)  +\left(  n+u\right)
}=1\\\text{(since }\left(  n+u\right)  +\left(  n+u\right)  =2\left(
n+u\right)  \text{ is even)}}}\left(  \alpha_{v}\beta_{u,w}-\beta_{u,v}%
\alpha_{w}\right)  =\alpha_{v}\beta_{u,w}-\beta_{u,v}\alpha_{w}\\
&  =\alpha_{v}\beta_{u,w}-\alpha_{w}\beta_{u,v}.
\end{align*}
Adding $\alpha_{w}\beta_{u,v}$ to both sides of this equality, we obtain
$\alpha_{u}\beta_{v,w}+\alpha_{w}\beta_{u,v}=\alpha_{v}\beta_{u,w}$. This
solves Exercise \ref{exe.desnanot.jaw}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.desnanot.skew}}

Before we solve Exercise \ref{exe.desnanot.skew}], let us state a variant of
Lemma \ref{lem.sol.altern.STAS.2} for the case when $A$ is alternating:

\begin{lemma}
\label{lem.sol.desnanot.skew.STAS=}Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$.
Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an
alternating $n\times n$-matrix. Let $S=\left(  s_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ be an $n\times m$-matrix. Then,%
\[
S^{T}AS=\left(  \sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell
,i}s_{k,j}\right)  a_{k,\ell}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.desnanot.skew.STAS=}.]Every $\left(  i,j\right)
\in\left\{  1,2,\ldots,m\right\}  ^{2}$ satisfies%
\begin{equation}
\sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}%
s_{k,i}s_{\ell,j}a_{k,\ell}=\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell
,i}s_{k,j}\right)  a_{k,\ell}. \label{pf.lem.sol.desnanot.skew.STAS=.1}%
\end{equation}


\begin{vershort}
[\textit{Proof of (\ref{pf.lem.sol.desnanot.skew.STAS=.1}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,m\right\}  ^{2}$. Then,%
\begin{align}
\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k=\ell}}s_{k,i}s_{\ell,j}\underbrace{a_{k,\ell}}_{\substack{=a_{\ell
,\ell}\\\text{(since }k=\ell\text{)}}}  &  =\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,j}\underbrace{a_{\ell,\ell}}_{\substack{=0\\\text{(by Lemma
\ref{lem.sol.altern.STAS.3} \textbf{(a)}}\\\text{(applied to }i=\ell\text{))}%
}}\nonumber\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k=\ell}}s_{k,i}s_{\ell,j}0=0.
\label{pf.lem.sol.desnanot.skew.STAS=.1.pf.short.1}%
\end{align}
Also,%
\begin{align}
&  \sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k>\ell}}s_{k,i}s_{\ell,j}\underbrace{a_{k,\ell}}_{\substack{=-a_{\ell
,k}\\\text{(by Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied
to }\left(  i,j\right)  =\left(  k,\ell\right)  \text{))}}}\nonumber\\
&  =-\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k>\ell}}s_{k,i}s_{\ell,j}a_{\ell,k}=-\underbrace{\sum
_{\substack{\left(  \ell,k\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\\ell>k}}}_{\substack{=\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\\ell>k}}=\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k<\ell}}}}s_{\ell
,i}s_{k,j}a_{k,\ell}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}\left(  k,\ell\right)  \text{ as }\left(  \ell,k\right)  \right) \nonumber\\
&  =-\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{\ell,i}s_{k,j}a_{k,\ell}.
\label{pf.lem.sol.desnanot.skew.STAS=.1.pf.short.2}%
\end{align}
Now,%
\begin{align*}
&  \sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}%
s_{k,i}s_{\ell,j}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}+\underbrace{\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,j}a_{k,\ell}}_{\substack{=0\\\text{(by
(\ref{pf.lem.sol.desnanot.skew.STAS=.1.pf.short.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}%
}_{\substack{=-\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{\ell,i}s_{k,j}a_{k,\ell}\\\text{(by
(\ref{pf.lem.sol.desnanot.skew.STAS=.1.pf.short.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}\text{ satisfies exactly one of}\\
\text{the three assertions }\left(  k<\ell\right)  \text{, }\left(
k=\ell\right)  \text{ and }\left(  k>\ell\right)
\end{array}
\right) \\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}-\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{\ell
,i}s_{k,j}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)  a_{k,\ell}.
\end{align*}
This proves (\ref{pf.lem.sol.desnanot.skew.STAS=.1}).]
\end{vershort}

\begin{verlong}
[\textit{Proof of (\ref{pf.lem.sol.desnanot.skew.STAS=.1}):} Let $\left(
i,j\right)  \in\left\{  1,2,\ldots,m\right\}  ^{2}$. Then,%
\begin{align}
\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k=\ell}}s_{k,i}s_{\ell,j}\underbrace{a_{k,\ell}}_{\substack{=a_{\ell
,\ell}\\\text{(since }k=\ell\text{)}}}  &  =\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,j}\underbrace{a_{\ell,\ell}}_{\substack{=0\\\text{(by Lemma
\ref{lem.sol.altern.STAS.3} \textbf{(a)}}\\\text{(applied to }i=\ell\text{))}%
}}\nonumber\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k=\ell}}s_{k,i}s_{\ell,j}0=0.
\label{pf.lem.sol.desnanot.skew.STAS=.1.pf.1}%
\end{align}
Also,%
\begin{align}
&  \sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k>\ell}}s_{k,i}s_{\ell,j}\underbrace{a_{k,\ell}}_{\substack{=-a_{\ell
,k}\\\text{(by Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)}}\\\text{(applied
to }\left(  i,j\right)  =\left(  k,\ell\right)  \text{))}}}\nonumber\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k>\ell}}s_{k,i}s_{\ell,j}\left(  -a_{\ell,k}\right)  =-\sum
_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k>\ell}}s_{k,i}s_{\ell,j}a_{\ell,k}\nonumber\\
&  =-\underbrace{\sum_{\substack{\left(  \ell,k\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\\ell>k}}}_{\substack{=\sum_{\ell\in\left\{
1,2,\ldots,n\right\}  }\sum_{\substack{k\in\left\{  1,2,\ldots,n\right\}
;\\\ell>k}}\\=\sum_{k\in\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\ell
\in\left\{  1,2,\ldots,n\right\}  ;\\\ell>k}}\\=\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\\ell>k}}=\sum
_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\\\text{(because for every }\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}\text{, the}\\\text{statement }\left(
\ell>k\right)  \text{ is equivalent to }\left(  k<\ell\right)  \text{)}%
}}s_{\ell,i}s_{k,j}a_{k,\ell}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}\left(  k,\ell\right)  \text{ as }\left(  \ell,k\right)  \right) \nonumber\\
&  =-\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{\ell,i}s_{k,j}a_{k,\ell}.
\label{pf.lem.sol.desnanot.skew.STAS=.1.pf.2}%
\end{align}
Now,%
\begin{align*}
&  \sum_{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}}%
s_{k,i}s_{\ell,j}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}+\underbrace{\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k=\ell}}s_{k,i}%
s_{\ell,j}a_{k,\ell}}_{\substack{=0\\\text{(by
(\ref{pf.lem.sol.desnanot.skew.STAS=.1.pf.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k>\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}%
}_{\substack{=-\sum_{\substack{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{\ell,i}s_{k,j}a_{k,\ell}\\\text{(by
(\ref{pf.lem.sol.desnanot.skew.STAS=.1.pf.2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each }\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2}\text{ satisfies exactly one of}\\
\text{the three assertions }\left(  k<\ell\right)  \text{, }\left(
k=\ell\right)  \text{ and }\left(  k>\ell\right)
\end{array}
\right) \\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}+0+\left(  -\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{\ell
,i}s_{k,j}a_{k,\ell}\right) \\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}s_{k,i}s_{\ell,j}a_{k,\ell}-\sum_{\substack{\left(
k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2};\\k<\ell}}s_{\ell
,i}s_{k,j}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)  a_{k,\ell}.
\end{align*}
This proves (\ref{pf.lem.sol.desnanot.skew.STAS=.1}).]
\end{verlong}

Lemma \ref{lem.sol.altern.STAS.2} yields%
\begin{align*}
S^{T}AS  &  =\left(  \underbrace{\sum_{\left(  k,\ell\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}}s_{k,i}s_{\ell,j}a_{k,\ell}}_{\substack{=\sum
_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)  a_{k,\ell
}\\\text{(by (\ref{pf.lem.sol.desnanot.skew.STAS=.1}))}}}\right)  _{1\leq
i\leq m,\ 1\leq j\leq m}\\
&  =\left(  \sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)
a_{k,\ell}\right)  _{1\leq i\leq m,\ 1\leq j\leq m}.
\end{align*}
Thus, Lemma \ref{lem.sol.desnanot.skew.STAS=}.
\end{proof}

Another simple lemma that we will use is the following:

\begin{lemma}
\label{lem.sol.desnanot.skew.multi}Let $n\in\mathbb{N}$. Let $B=\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an alternating $n\times
n$-matrix. Let $c\in\mathbb{K}$. Assume that for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$ satisfying $i<j$, the element
$b_{i,j}$ of $\mathbb{K}$ is a multiple of $c$. Then, each entry of $B$ is a
multiple of $c$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.desnanot.skew.multi}.]We have assumed that the
following holds:

\begin{statement}
\textit{Fact 1:} For every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$ satisfying $i<j$, the element $b_{i,j}$ of $\mathbb{K}$ is a
multiple of $c$.
\end{statement}

From this, it is easy to derive the following claim:

\begin{statement}
\textit{Fact 2:} For every $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$, the element $b_{i,j}$ of $\mathbb{K}$ is a multiple of $c$.
\end{statement}

[\textit{Proof of Fact 2:} Let $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$. We must show that the element $b_{i,j}$ of $\mathbb{K}$ is
a multiple of $c$.

We are in one of the following three cases:

\textit{Case 1:} We have $i<j$.

\textit{Case 2:} We have $i=j$.

\textit{Case 3:} We have $i>j$.

Let us first consider Case 1. In this case, we have $i<j$. Hence, the element
$b_{i,j}$ of $\mathbb{K}$ is a multiple of $c$ (by Fact 1). Thus, Fact 2 is
proven in Case 1.

Let us next consider Case 2. In this case, we have $i=j$. Hence, $j=i$.
Therefore, $b_{i,j}=b_{i,i}=0$ (by Lemma \ref{lem.sol.altern.STAS.3}
\textbf{(a)} (applied to $B$ and $b_{i,j}$ instead of $A$ and $a_{i,j}$)).
Thus, the element $b_{i,j}$ of $\mathbb{K}$ is a multiple of $c$ (since the
element $0$ of $\mathbb{K}$ is a multiple of $c$). Hence, Fact 2 is proven in
Case 2.

Let us finally consider Case 3. In this case, we have $i>j$. Thus, $j<i$.
Hence, Fact 1 (applied to $\left(  j,i\right)  $ instead of $\left(
i,j\right)  $) shows that the element $b_{j,i}$ of $\mathbb{K}$ is a multiple
of $c$. In other words, $b_{j,i}=dc$ for some $d\in\mathbb{K}$. Consider this
$d$. But Lemma \ref{lem.sol.altern.STAS.3} \textbf{(b)} (applied to $B$ and
$b_{i,j}$ instead of $A$ and $a_{i,j}$) yields $b_{i,j}=-\underbrace{b_{j,i}%
}_{=dc}=-dc=\left(  -d\right)  c$. Hence, the element $b_{i,j}$ of
$\mathbb{K}$ is a multiple of $c$. Thus, Fact 2 is proven in Case 3.

We have now proven Fact 2 in each of the three Cases 1, 2 and 3. Since these
three Cases cover all possibilities, we thus conclude that Fact 2 always holds.]

Notice that $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Thus, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$,
we have%
\begin{equation}
\left(  \text{the }\left(  i,j\right)  \text{-th entry of }B\right)  =b_{i,j}.
\label{pf.lem.sol.desnanot.skew.multi.1}%
\end{equation}


We now need to prove that each entry of $B$ is a multiple of $c$. In other
words, we need to show that, for every $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$, the $\left(  i,j\right)  $-th entry of $B$ is a
multiple of $c$. So let us fix $\left(  i,j\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2}$. Then, Fact 2 shows that $b_{i,j}$ is a multiple of $c$. In
light of (\ref{pf.lem.sol.desnanot.skew.multi.1}), this rewrites as follows:
The $\left(  i,j\right)  $-th entry of $B$ is a multiple of $c$. This is
precisely what we wanted to prove. Thus, the proof of Lemma
\ref{lem.sol.desnanot.skew.multi} is complete.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.desnanot.skew}.]Theorem \ref{thm.desnanot}
leads to the following fact: If $i$, $j$, $k$ and $\ell$ are four elements of
$\left\{  1,2,\ldots,n\right\}  $ such that $i<j$ and $k<\ell$, then%
\begin{align}
&  \det S\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots
,\widehat{i},\ldots,\widehat{j},\ldots,n}^{1,2,\ldots,\widehat{k}%
,\ldots,\widehat{\ell},\ldots,n}S\right) \nonumber\\
&  =\det\left(  S_{\sim i,\sim k}\right)  \cdot\det\left(  S_{\sim j,\sim\ell
}\right)  -\det\left(  S_{\sim i,\sim\ell}\right)  \cdot\det\left(  S_{\sim
j,\sim k}\right)  \label{sol.desnanot.skew.des}%
\end{align}
\footnote{\textit{Proof of (\ref{sol.desnanot.skew.des}):} Let $i$, $j$, $k$
and $\ell$ be four elements of $\left\{  1,2,\ldots,n\right\}  $ such that
$i<j$ and $k<\ell$. We have $i\geq1$ (since $i\in\left\{  1,2,\ldots
,n\right\}  $) and $j\leq n$ (since $j\in\left\{  1,2,\ldots,n\right\}  $).
But $i<j$ and thus $i\leq j-1$ (since $i$ and $j$ are integers). Hence, $1\leq
i\leq\underbrace{j}_{\leq n}-1\leq n-1$, so that $n-1\geq1$ and thus $n\geq2$.
Hence, Theorem \ref{thm.desnanot} (applied to $S$, $i$, $j$, $k$ and $\ell$
instead of $A$, $p$, $q$, $u$ and $v$) yields%
\begin{equation}
\det S\cdot\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i}%
,\ldots,\widehat{j},\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,\widehat{\ell
},\ldots,n}S\right)  =\det\left(  S_{\sim i,\sim k}\right)  \cdot\det\left(
S_{\sim j,\sim\ell}\right)  -\det\left(  S_{\sim i,\sim\ell}\right)  \cdot
\det\left(  S_{\sim j,\sim k}\right)  .\nonumber
\end{equation}
This proves (\ref{sol.desnanot.skew.des}).}.

For each $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, define
an element $s_{i,j}$ of $\mathbb{K}$ by%
\begin{equation}
s_{i,j}=\left(  -1\right)  ^{i+j}\det\left(  S_{\sim j,\sim i}\right)  .
\label{sol.desnanot.skew.1}%
\end{equation}
(Notice that these elements $s_{i,j}$ are \textbf{not} supposed to be the
entries of $S$, despite the notation!)

The definition of $\operatorname*{adj}S$ yields%
\[
\operatorname*{adj}S=\left(  \underbrace{\left(  -1\right)  ^{i+j}\det\left(
S_{\sim j,\sim i}\right)  }_{\substack{=s_{i,j}\\\text{(by
(\ref{sol.desnanot.skew.1}))}}}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  s_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Thus, Lemma \ref{lem.sol.desnanot.skew.STAS=} (applied to $n$ and
$\operatorname*{adj}S$ instead of $m$ and $S$) yields%
\begin{equation}
\left(  \operatorname*{adj}S\right)  ^{T}A\left(  \operatorname*{adj}S\right)
=\left(  \sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)
a_{k,\ell}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\label{sol.desnanot.skew.3}%
\end{equation}
Moreover, Exercise \ref{exe.altern.STAS} (applied to $n$ and
$\operatorname*{adj}S$ instead of $m$ and $S$) yields that the $n\times
n$-matrix $\left(  \operatorname*{adj}S\right)  ^{T}A\left(
\operatorname*{adj}S\right)  $ is alternating.

But if $i$, $j$, $k$ and $\ell$ are four elements of $\left\{  1,2,\ldots
,n\right\}  $ such that $i<j$ and $k<\ell$, then%
\begin{align}
&  \underbrace{s_{k,i}}_{\substack{=\left(  -1\right)  ^{k+i}\det\left(
S_{\sim i,\sim k}\right)  \\\text{(by the definition of }s_{k,i}\text{)}%
}}\underbrace{s_{\ell,j}}_{\substack{=\left(  -1\right)  ^{\ell+j}\det\left(
S_{\sim j,\sim\ell}\right)  \\\text{(by the definition of }s_{\ell,j}\text{)}%
}}-\underbrace{s_{\ell,i}}_{\substack{=\left(  -1\right)  ^{\ell+i}\det\left(
S_{\sim i,\sim\ell}\right)  \\\text{(by the definition of }s_{\ell,i}\text{)}%
}}\underbrace{s_{k,j}}_{\substack{=\left(  -1\right)  ^{k+j}\det\left(
S_{\sim j,\sim k}\right)  \\\text{(by the definition of }s_{k,j}\text{)}%
}}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{k+i}\det\left(  S_{\sim i,\sim k}\right)
\cdot\left(  -1\right)  ^{\ell+j}\det\left(  S_{\sim j,\sim\ell}\right)
}_{=\left(  -1\right)  ^{k+i}\left(  -1\right)  ^{\ell+j}\det\left(  S_{\sim
i,\sim k}\right)  \cdot\det\left(  S_{\sim j,\sim\ell}\right)  }%
-\underbrace{\left(  -1\right)  ^{\ell+i}\det\left(  S_{\sim i,\sim\ell
}\right)  \cdot\left(  -1\right)  ^{k+j}\det\left(  S_{\sim j,\sim k}\right)
}_{=\left(  -1\right)  ^{\ell+i}\left(  -1\right)  ^{k+j}\det\left(  S_{\sim
i,\sim\ell}\right)  \cdot\det\left(  S_{\sim j,\sim k}\right)  }\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{k+i}\left(  -1\right)  ^{\ell+j}%
}_{\substack{=\left(  -1\right)  ^{\left(  k+i\right)  +\left(  \ell+j\right)
}=\left(  -1\right)  ^{k+\ell+i+j}\\\text{(since }\left(  k+i\right)  +\left(
\ell+j\right)  =k+\ell+i+j\text{)}}}\det\left(  S_{\sim i,\sim k}\right)
\cdot\det\left(  S_{\sim j,\sim\ell}\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ -\underbrace{\left(  -1\right)  ^{\ell+i}\left(
-1\right)  ^{k+j}}_{\substack{=\left(  -1\right)  ^{\left(  \ell+i\right)
+\left(  k+j\right)  }=\left(  -1\right)  ^{k+\ell+i+j}\\\text{(since }\left(
\ell+i\right)  +\left(  k+j\right)  =k+\ell+i+j\text{)}}}\det\left(  S_{\sim
i,\sim\ell}\right)  \cdot\det\left(  S_{\sim j,\sim k}\right) \nonumber\\
&  =\left(  -1\right)  ^{k+\ell+i+j}\det\left(  S_{\sim i,\sim k}\right)
\cdot\det\left(  S_{\sim j,\sim\ell}\right)  -\left(  -1\right)  ^{k+\ell
+i+j}\det\left(  S_{\sim i,\sim\ell}\right)  \cdot\det\left(  S_{\sim j,\sim
k}\right) \nonumber\\
&  =\left(  -1\right)  ^{k+\ell+i+j}\underbrace{\left(  \det\left(  S_{\sim
i,\sim k}\right)  \cdot\det\left(  S_{\sim j,\sim\ell}\right)  -\det\left(
S_{\sim i,\sim\ell}\right)  \cdot\det\left(  S_{\sim j,\sim k}\right)
\right)  }_{\substack{=\det S\cdot\det\left(  \operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{i},\ldots,\widehat{j},\ldots,n}^{1,2,\ldots
,\widehat{k},\ldots,\widehat{\ell},\ldots,n}S\right)  \\\text{(by
(\ref{sol.desnanot.skew.des}))}}}\nonumber\\
&  =\left(  -1\right)  ^{k+\ell+i+j}\det S\cdot\det\left(  \operatorname*{sub}%
\nolimits_{1,2,\ldots,\widehat{i},\ldots,\widehat{j},\ldots,n}^{1,2,\ldots
,\widehat{k},\ldots,\widehat{\ell},\ldots,n}S\right)  .
\label{sol.desnanot.skew.5}%
\end{align}
Hence, for every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$
satisfying $i<j$, the element \newline$\sum_{\substack{\left(  k,\ell\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell
,j}-s_{\ell,i}s_{k,j}\right)  a_{k,\ell}$ of $\mathbb{K}$ is a multiple of
$\det S$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$ be such that $i<j$. From $\left(  i,j\right)
\in\left\{  1,2,\ldots,n\right\}  ^{2}$, we obtain $i\in\left\{
1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots,n\right\}  $. Now,%
\begin{align*}
&  \sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\underbrace{\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)
}_{\substack{=\left(  -1\right)  ^{k+\ell+i+j}\det S\cdot\det\left(
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots,\widehat{j}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,\widehat{\ell},\ldots,n}S\right)
\\\text{(by (\ref{sol.desnanot.skew.5}))}}}a_{k,\ell}\\
&  =\sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\left(  -1\right)  ^{k+\ell+i+j}\det S\cdot\det\left(
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots,\widehat{j}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,\widehat{\ell},\ldots,n}S\right)
a_{k,\ell}\\
&  =\left(  \sum_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots
,n\right\}  ^{2};\\k<\ell}}\left(  -1\right)  ^{k+\ell+i+j}\det\left(
\operatorname*{sub}\nolimits_{1,2,\ldots,\widehat{i},\ldots,\widehat{j}%
,\ldots,n}^{1,2,\ldots,\widehat{k},\ldots,\widehat{\ell},\ldots,n}S\right)
a_{k,\ell}\right)  \det S
\end{align*}
is clearly a multiple of $\det S$. Qed.}. Thus, Lemma
\ref{lem.sol.desnanot.skew.multi} (applied to $B=\left(  \operatorname*{adj}%
S\right)  ^{T}A\left(  \operatorname*{adj}S\right)  $, $b_{i,j}=\sum
_{\substack{\left(  k,\ell\right)  \in\left\{  1,2,\ldots,n\right\}
^{2};\\k<\ell}}\left(  s_{k,i}s_{\ell,j}-s_{\ell,i}s_{k,j}\right)  a_{k,\ell}$
and $c=\det S$) shows that each entry of the matrix $\left(
\operatorname*{adj}S\right)  ^{T}A\left(  \operatorname*{adj}S\right)  $ is a
multiple of $\det S$ (because the matrix $\left(  \operatorname*{adj}S\right)
^{T}A\left(  \operatorname*{adj}S\right)  $ is alternating and satisfies
(\ref{sol.desnanot.skew.3})). This solves Exercise \ref{exe.desnanot.skew}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.pluecker.rederive-AC}}

\begin{proof}
[Second proof of Proposition \ref{prop.desnanot.AC}.]Recall that $I_{m}$
denotes the $m\times m$ identity matrix for each $m\in\mathbb{N}$. Thus,
$I_{n}$ is the $n\times n$ identity matrix. Hence, $\left(  I_{n}\right)
_{\bullet,v}$ is a well-defined $n\times1$-matrix (since $v\in\left\{
1,2,\ldots,n\right\}  $).

We know that $C$ is an $n\times n$-matrix (since $C\in\mathbb{K}^{n\times n}%
$), and that $\left(  I_{n}\right)  _{\bullet,v}$ is an $n\times1$-matrix.
Thus, $\left(  C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  $ is an
$n\times\left(  n+1\right)  $-matrix. In other words, $\left(  C\mid\left(
I_{n}\right)  _{\bullet,v}\right)  \in\mathbb{K}^{n\times\left(  n+1\right)
}$.

Set $B=\left(  C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  $. Thus,
$B=\left(  C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  \in\mathbb{K}%
^{n\times\left(  n+1\right)  }$. We have%
\begin{equation}
B_{\bullet,r}=C_{\bullet,r}\ \ \ \ \ \ \ \ \ \ \text{for every }r\in\left\{
1,2,\ldots,n\right\}  \label{sol.pluecker.rederive-AC.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.pluecker.rederive-AC.1}):} Let
$r\in\left\{  1,2,\ldots,n\right\}  $. Proposition \ref{prop.addcol.props1}
\textbf{(a)} (applied to $n$, $C$, $\left(  I_{n}\right)  _{\bullet,v}$ and
$r$ instead of $m$, $A$, $v$ and $q$) yields $\left(  C\mid\left(
I_{n}\right)  _{\bullet,v}\right)  _{\bullet,r}=C_{\bullet,r}$. Now, from
$B=\left(  C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  $, we obtain
$B_{\bullet,r}=\left(  C\mid\left(  I_{n}\right)  _{\bullet,v}\right)
_{\bullet,r}=C_{\bullet,r}$. This proves (\ref{sol.pluecker.rederive-AC.1}).}.
Furthermore,%
\begin{equation}
\det\left(  A\mid B_{\bullet,n+1}\right)  =\left(  -1\right)  ^{n+v}%
\det\left(  A_{\sim v,\bullet}\right)  \label{sol.pluecker.rederive-AC.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.pluecker.rederive-AC.2}):} Proposition
\ref{prop.addcol.props1} \textbf{(b)} (applied to $n$, $C$ and $\left(
I_{n}\right)  _{\bullet,v}$ instead of $m$, $A$ and $v$) yields $\left(
C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  _{\bullet,n+1}=\left(
I_{n}\right)  _{\bullet,v}$. Now, from $B=\left(  C\mid\left(  I_{n}\right)
_{\bullet,v}\right)  $, we obtain $B_{\bullet,n+1}=\left(  C\mid\left(
I_{n}\right)  _{\bullet,v}\right)  _{\bullet,n+1}=\left(  I_{n}\right)
_{\bullet,v}$. Hence,%
\[
\det\left(  A\mid\underbrace{B_{\bullet,n+1}}_{=\left(  I_{n}\right)
_{\bullet,v}}\right)  =\det\left(  A\mid\left(  I_{n}\right)  _{\bullet
,v}\right)  =\left(  -1\right)  ^{n+v}\det\left(  A_{\sim v,\bullet}\right)
\]
(by Proposition \ref{prop.addcol.props2} \textbf{(b)}, applied to $p=v$). This
proves (\ref{sol.pluecker.rederive-AC.2}).}. Moreover,%
\begin{equation}
\det\left(  B_{\bullet,\sim r}\right)  =\left(  -1\right)  ^{n+v}\det\left(
C_{\sim v,\sim r}\right)  \ \ \ \ \ \ \ \ \ \ \text{for every }r\in\left\{
1,2,\ldots,n\right\}  \label{sol.pluecker.rederive-AC.3}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.pluecker.rederive-AC.3}):} Let
$r\in\left\{  1,2,\ldots,n\right\}  $. From $B=\left(  C\mid\left(
I_{n}\right)  _{\bullet,v}\right)  $, we obtain $B_{\bullet,\sim r}=\left(
C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  _{\bullet,\sim r}=\left(
C_{\bullet,\sim r}\mid\left(  I_{n}\right)  _{\bullet,v}\right)  $ (by
Proposition \ref{prop.addcol.props1} \textbf{(c)}, applied to $n$, $C$,
$\left(  I_{n}\right)  _{\bullet,v}$ and $r$ instead of $m$, $A$, $v$ and
$q$). Hence,%
\begin{equation}
\det\left(  \underbrace{B_{\bullet,\sim r}}_{=\left(  C_{\bullet,\sim r}%
\mid\left(  I_{n}\right)  _{\bullet,v}\right)  }\right)  =\det\left(
C_{\bullet,\sim r}\mid\left(  I_{n}\right)  _{\bullet,v}\right)  =\left(
-1\right)  ^{n+v}\det\left(  \left(  C_{\bullet,\sim r}\right)  _{\sim
v,\bullet}\right)  \label{sol.pluecker.rederive-AC.3.pf.1}%
\end{equation}
(by Proposition \ref{prop.addcol.props2} \textbf{(b)}, applied to $v$ and
$C_{\bullet,\sim r}$ instead of $p$ and $A$). But Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to $n$, $C$, $v$ and $r$
instead of $m$, $A$, $u$ and $v$) yields $\left(  C_{\bullet,\sim r}\right)
_{\sim v,\bullet}=\left(  C_{\sim v,\bullet}\right)  _{\bullet,\sim r}=C_{\sim
v,\sim r}$. Now, (\ref{sol.pluecker.rederive-AC.3.pf.1}) becomes%
\[
\det\left(  B_{\bullet,\sim r}\right)  =\left(  -1\right)  ^{n+v}\det\left(
\underbrace{\left(  C_{\bullet,\sim r}\right)  _{\sim v,\bullet}}_{=C_{\sim
v,\sim r}}\right)  =\left(  -1\right)  ^{n+v}\det\left(  C_{\sim v,\sim
r}\right)  .
\]
This proves (\ref{sol.pluecker.rederive-AC.3}).}. Finally,%
\begin{equation}
B_{\bullet,\sim\left(  n+1\right)  }=C \label{sol.pluecker.rederive-AC.4}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.pluecker.rederive-AC.4}):} Proposition
\ref{prop.addcol.props1} \textbf{(d)} (applied to $n$, $C$ and $\left(
I_{n}\right)  _{\bullet,v}$ instead of $m$, $A$ and $v$) yields $\left(
C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  _{\bullet,\sim\left(
n+1\right)  }=C$. Now, from $B=\left(  C\mid\left(  I_{n}\right)  _{\bullet
,v}\right)  $, we obtain $B_{\bullet,\sim\left(  n+1\right)  }=\left(
C\mid\left(  I_{n}\right)  _{\bullet,v}\right)  _{\bullet,\sim\left(
n+1\right)  }=C$. This proves (\ref{sol.pluecker.rederive-AC.4}).}.

Now, Theorem \ref{thm.pluecker.plu} yields%
\begin{align*}
0  &  =\sum_{r=1}^{n+1}\left(  -1\right)  ^{r}\det\left(  A\mid B_{\bullet
,r}\right)  \det\left(  B_{\bullet,\sim r}\right) \\
&  =\sum_{r=1}^{n}\left(  -1\right)  ^{r}\det\left(  A\mid
\underbrace{B_{\bullet,r}}_{\substack{=C_{\bullet,r}\\\text{(by
(\ref{sol.pluecker.rederive-AC.1}))}}}\right)  \underbrace{\det\left(
B_{\bullet,\sim r}\right)  }_{\substack{=\left(  -1\right)  ^{n+v}\det\left(
C_{\sim v,\sim r}\right)  \\\text{(by (\ref{sol.pluecker.rederive-AC.3}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\left(  -1\right)  ^{n+1}\underbrace{\det\left(  A\mid
B_{\bullet,n+1}\right)  }_{\substack{=\left(  -1\right)  ^{n+v}\det\left(
A_{\sim v,\bullet}\right)  \\\text{(by (\ref{sol.pluecker.rederive-AC.2}))}%
}}\det\left(  \underbrace{B_{\bullet,\sim\left(  n+1\right)  }}%
_{\substack{=C\\\text{(by (\ref{sol.pluecker.rederive-AC.4}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}r=n+1\text{ from the sum}\right) \\
&  =\sum_{r=1}^{n}\left(  -1\right)  ^{r}\underbrace{\det\left(  A\mid
C_{\bullet,r}\right)  \left(  -1\right)  ^{n+v}}_{=\left(  -1\right)
^{n+v}\det\left(  A\mid C_{\bullet,r}\right)  }\det\left(  C_{\sim v,\sim
r}\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  -1\right)  ^{n+1}\left(
-1\right)  ^{n+v}}_{\substack{=\left(  -1\right)  ^{\left(  n+1\right)
+\left(  n+v\right)  }=\left(  -1\right)  ^{v+1}\\\text{(since }\left(
n+1\right)  +\left(  n+v\right)  =2n+\left(  v+1\right)  \equiv
v+1\operatorname{mod}2\text{)}}}\det\left(  A_{\sim v,\bullet}\right)  \det
C\\
&  =\sum_{r=1}^{n}\underbrace{\left(  -1\right)  ^{r}\left(  -1\right)
^{n+v}}_{=\left(  -1\right)  ^{r+\left(  n+v\right)  }}\det\left(  A\mid
C_{\bullet,r}\right)  \det\left(  C_{\sim v,\sim r}\right)
+\underbrace{\left(  -1\right)  ^{v+1}}_{=-\left(  -1\right)  ^{v}}\det\left(
A_{\sim v,\bullet}\right)  \det C\\
&  =\sum_{r=1}^{n}\left(  -1\right)  ^{r+\left(  n+v\right)  }\det\left(
A\mid C_{\bullet,r}\right)  \det\left(  C_{\sim v,\sim r}\right)  -\left(
-1\right)  ^{v}\det\left(  A_{\sim v,\bullet}\right)  \det C.
\end{align*}
Adding $\left(  -1\right)  ^{v}\det\left(  A_{\sim v,\bullet}\right)  \det C$
to both sides of this equality, we obtain%
\[
\left(  -1\right)  ^{v}\det\left(  A_{\sim v,\bullet}\right)  \det
C=\sum_{r=1}^{n}\left(  -1\right)  ^{r+\left(  n+v\right)  }\det\left(  A\mid
C_{\bullet,r}\right)  \det\left(  C_{\sim v,\sim r}\right)  .
\]
Multiplying both sides of this equality by $\left(  -1\right)  ^{v}$, we find%
\begin{align*}
&  \left(  -1\right)  ^{v}\left(  -1\right)  ^{v}\det\left(  A_{\sim
v,\bullet}\right)  \det C\\
&  =\left(  -1\right)  ^{v}\sum_{r=1}^{n}\left(  -1\right)  ^{r+\left(
n+v\right)  }\det\left(  A\mid C_{\bullet,r}\right)  \det\left(  C_{\sim
v,\sim r}\right) \\
&  =\sum_{r=1}^{n}\underbrace{\left(  -1\right)  ^{v}\left(  -1\right)
^{r+\left(  n+v\right)  }}_{\substack{=\left(  -1\right)  ^{v+\left(
r+\left(  n+v\right)  \right)  }=\left(  -1\right)  ^{n+r}\\\text{(since
}v+\left(  r+\left(  n+v\right)  \right)  =2v+n+r\equiv n+r\operatorname{mod}%
2\text{)}}}\det\left(  A\mid C_{\bullet,r}\right)  \det\left(  C_{\sim v,\sim
r}\right) \\
&  =\sum_{r=1}^{n}\left(  -1\right)  ^{n+r}\det\left(  A\mid C_{\bullet
,r}\right)  \det\left(  C_{\sim v,\sim r}\right) \\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{n+q}\det\left(  A\mid C_{\bullet
,q}\right)  \det\left(  C_{\sim v,\sim q}\right)
\end{align*}
(here, we have renamed the summation index $r$ as $q$). Hence,%
\begin{align*}
&  \sum_{q=1}^{n}\left(  -1\right)  ^{n+q}\det\left(  A\mid C_{\bullet
,q}\right)  \det\left(  C_{\sim v,\sim q}\right) \\
&  =\underbrace{\left(  -1\right)  ^{v}\left(  -1\right)  ^{v}}%
_{\substack{=\left(  -1\right)  ^{v+v}=1\\\text{(since }v+v=2v\text{ is
even)}}}\det\left(  A_{\sim v,\bullet}\right)  \det C=\det\left(  A_{\sim
v,\bullet}\right)  \det C.
\end{align*}
This proves Proposition \ref{prop.desnanot.AC} again.
\end{proof}

Thus, Exercise \ref{exe.pluecker.rederive-AC} is solved.

\subsection{Solution to Exercise \ref{exe.det.laplace-multi}}

Throughout this section, we shall use the notations introduced in Definition
\ref{def.submatrix} and in Definition \ref{def.sect.laplace.notations}.

Let us prepare for the proof of Lemma \ref{lem.det.laplace-multi.Apq} by
showing a particular case:

\begin{lemma}
\label{lem.sol.det.laplace-multi.1}Let $n\in\mathbb{N}$. For any subset $I$ of
$\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote the
complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be two $n\times
n$-matrices. Let $Q$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Let
$k=\left\vert Q\right\vert $. Then,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{  1,2,\ldots
,k\right\}  \right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod
_{i\in\left\{  1,2,\ldots,k\right\}  }a_{i,\sigma\left(  i\right)  }\right)
\left(  \prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma\left(
i\right)  }\right) \\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.1}.]We begin with some simple observations.

\begin{vershort}
The definition of $\widetilde{Q}$ yields $\widetilde{Q}=\left\{
1,2,\ldots,n\right\}  \setminus Q$. Since $Q$ is a subset of $\left\{
1,2,\ldots,n\right\}  $, this leads to $\left\vert \widetilde{Q}\right\vert
=\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert }%
_{=n}-\underbrace{\left\vert Q\right\vert }_{=k}=n-k$. Thus, $n-k=\left\vert
\widetilde{Q}\right\vert \geq0$, so that $n\geq k$ and thus $k\in\left\{
0,1,\ldots,n\right\}  $.
\end{vershort}

\begin{verlong}
The definition of $\widetilde{Q}$ yields $\widetilde{Q}=\left\{
1,2,\ldots,n\right\}  \setminus Q\subseteq\left\{  1,2,\ldots,n\right\}  $.
Thus, $\widetilde{Q}$ is a finite set (since $\left\{  1,2,\ldots,n\right\}  $
is a finite set). Hence, $\left\vert \widetilde{Q}\right\vert \in\mathbb{N}$.

Also, $Q\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, $Q$ is a finite set
(since $\left\{  1,2,\ldots,n\right\}  $ is a finite set). Hence, $\left\vert
Q\right\vert \in\mathbb{N}$.

Also,%
\begin{align*}
\left\vert \underbrace{\widetilde{Q}}_{=\left\{  1,2,\ldots,n\right\}
\setminus Q}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus Q\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\underbrace{\left\vert Q\right\vert }_{=k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }Q\subseteq\left\{  1,2,\ldots
,n\right\}  \right) \\
&  =n-k.
\end{align*}
Hence, $n-k=\left\vert \widetilde{Q}\right\vert \in\mathbb{N}$; thus,
$n-k\geq0$. In other words, $k\leq n$. Combined with $k\geq0$ (since
$k=\left\vert Q\right\vert \in\mathbb{N}$), this yields $k\in\left\{
0,1,\ldots,n\right\}  $.
\end{verlong}

Let $\left(  q_{1},q_{2},\ldots,q_{k}\right)  $ be the list of all elements of
$Q$ in increasing order (with no repetitions). (This is well-defined, because
$\left\vert Q\right\vert =k$.)

Let $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ be the list of all elements
of $\widetilde{Q}$ in increasing order (with no repetitions). (This is
well-defined, because $\left\vert \widetilde{Q}\right\vert =n-k$.)

\begin{vershort}
The lists $w\left(  Q\right)  $ and $\left(  q_{1},q_{2},\ldots,q_{k}\right)
$ must be identical (since they are both defined to be the list of all
elements of $Q$ in increasing order (with no repetitions)). In other words, we
have $w\left(  Q\right)  =\left(  q_{1},q_{2},\ldots,q_{k}\right)  $.
Similarly, $w\left(  \widetilde{Q}\right)  =\left(  r_{1},r_{2},\ldots
,r_{n-k}\right)  $.
\end{vershort}

\begin{verlong}
But $w\left(  Q\right)  $ is the list of all elements of $Q$ in increasing
order (with no repetitions) (by the definition of $w\left(  Q\right)  $).
Thus,%
\begin{align*}
w\left(  Q\right)   &  =\left(  \text{the list of all elements of }Q\text{ in
increasing order (with no repetitions)}\right) \\
&  =\left(  q_{1},q_{2},\ldots,q_{k}\right)
\end{align*}
(since $\left(  q_{1},q_{2},\ldots,q_{k}\right)  $ is the list of all elements
of $Q$ in increasing order (with no repetitions)).

Also, $w\left(  \widetilde{Q}\right)  $ is the list of all elements of
$\widetilde{Q}$ in increasing order (with no repetitions) (by the definition
of $w\left(  \widetilde{Q}\right)  $). Thus,%
\begin{align*}
w\left(  \widetilde{Q}\right)   &  =\left(  \text{the list of all elements of
}\widetilde{Q}\text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  r_{1},r_{2},\ldots,r_{n-k}\right)
\end{align*}
(since $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ is the list of all
elements of $\widetilde{Q}$ in increasing order (with no repetitions)).
\end{verlong}

We know that $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ is the list of all
elements of $\widetilde{Q}$ in increasing order (with no repetitions). In
other words, $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ is the list of all
elements of $\left\{  1,2,\ldots,n\right\}  \setminus Q$ in increasing order
(with no repetitions) (since $\widetilde{Q}=\left\{  1,2,\ldots,n\right\}
\setminus Q$). Thus, for every $\alpha\in S_{k}$ and $\beta\in S_{n-k}$, we
can define an element $\sigma_{Q,\alpha,\beta}\in S_{n}$ according to Exercise
\ref{exe.Ialbe} \textbf{(a)} (applied to $Q$, $\left(  q_{1},q_{2}%
,\ldots,q_{k}\right)  $ and $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $
instead of $I$, $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ and $\left(
b_{1},b_{2},\ldots,b_{n-k}\right)  $). Consider this $\sigma_{Q,\alpha,\beta}%
$. Exercise \ref{exe.Ialbe} \textbf{(b)} (applied to $Q$, $\left(  q_{1}%
,q_{2},\ldots,q_{k}\right)  $ and $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)
$ instead of $I$, $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ and $\left(
b_{1},b_{2},\ldots,b_{n-k}\right)  $) shows that for every $\alpha\in S_{k}$
and $\beta\in S_{n-k}$, we have%
\[
\ell\left(  \sigma_{Q,\alpha,\beta}\right)  =\ell\left(  \alpha\right)
+\ell\left(  \beta\right)  +\sum Q-\left(  1+2+\cdots+k\right)
\]
and%
\begin{equation}
\left(  -1\right)  ^{\sigma_{Q,\alpha,\beta}}=\left(  -1\right)  ^{\alpha
}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum Q-\left(
1+2+\cdots+k\right)  }. \label{pf.lem.sol.det.laplace-multi.1.sign}%
\end{equation}


Exercise \ref{exe.Ialbe} \textbf{(c)} (applied to $Q$, $\left(  q_{1}%
,q_{2},\ldots,q_{k}\right)  $ and $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)
$ instead of $I$, $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ and $\left(
b_{1},b_{2},\ldots,b_{n-k}\right)  $) shows that the map%
\begin{align*}
S_{k}\times S_{n-k}  &  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
\left\{  1,2,\ldots,k\right\}  \right)  =Q\right\}  ,\\
\left(  \alpha,\beta\right)   &  \mapsto\sigma_{Q,\alpha,\beta}%
\end{align*}
is well-defined and a bijection.

We have $w\left(  Q\right)  =\left(  q_{1},q_{2},\ldots,q_{k}\right)  $. Thus,%
\begin{align*}
\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }A  &  =\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)
}^{\left(  q_{1},q_{2},\ldots,q_{k}\right)  }A=\operatorname*{sub}%
\nolimits_{1,2,\ldots,k}^{q_{1},q_{2},\ldots,q_{k}}A=\left(  a_{x,q_{y}%
}\right)  _{1\leq x\leq k,\ 1\leq y\leq k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{sub}\nolimits_{1,2,\ldots,k}%
^{q_{1},q_{2},\ldots,q_{k}}A\text{,}\\
\text{since }A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\end{array}
\right) \\
&  =\left(  a_{i,q_{j}}\right)  _{1\leq i\leq k,\ 1\leq j\leq k}%
\end{align*}
(here, we have renamed the index $\left(  x,y\right)  $ as $\left(
i,j\right)  $). Thus,%
\begin{align}
\det\left(  \operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)
}^{w\left(  Q\right)  }A\right)   &  =\sum_{\sigma\in S_{k}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{k}a_{i,q_{\sigma\left(  i\right)  }}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.det.eq.2}), applied to }k\text{, }\operatorname*{sub}%
\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(  Q\right)  }A\text{ and
}a_{i,q_{j}}\\
\text{instead of }n\text{, }A\text{ and }a_{i,j}%
\end{array}
\right) \nonumber\\
&  =\sum_{\alpha\in S_{k}}\left(  -1\right)  ^{\alpha}\prod_{i=1}%
^{k}a_{i,q_{\alpha\left(  i\right)  }}
\label{pf.lem.sol.det.laplace-multi.1.det1}%
\end{align}
(here, we have renamed the summation index $\sigma$ as $\alpha$).

Also, $w\left(  \widetilde{Q}\right)  =\left(  r_{1},r_{2},\ldots
,r_{n-k}\right)  $. Thus,%
\begin{align*}
\operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{w\left(
\widetilde{Q}\right)  }B  &  =\operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{\left(  r_{1},r_{2},\ldots,r_{n-k}\right)
}B=\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{r_{1},r_{2},\ldots
,r_{n-k}}B=\left(  b_{k+x,r_{y}}\right)  _{1\leq x\leq n-k,\ 1\leq y\leq
n-k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by the definition of }\operatorname*{sub}\nolimits_{k+1,k+2,\ldots
,n}^{r_{1},r_{2},\ldots,r_{n-k}}B\text{,}\\
\text{since }B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\end{array}
\right) \\
&  =\left(  b_{k+i,r_{j}}\right)  _{1\leq i\leq n-k,\ 1\leq j\leq n-k}%
\end{align*}
(here, we have renamed the index $\left(  x,y\right)  $ as $\left(
i,j\right)  $). Thus,%
\begin{align}
\det\left(  \operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots,n\right)
}^{w\left(  \widetilde{Q}\right)  }B\right)   &  =\sum_{\sigma\in S_{n-k}%
}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-k}b_{k+i,r_{\sigma\left(
i\right)  }}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.det.eq.2}), applied to }n-k\text{, }\operatorname*{sub}%
\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)
}B\text{ and }b_{k+i,r_{j}}\\
\text{instead of }n\text{, }A\text{ and }a_{i,j}%
\end{array}
\right) \nonumber\\
&  =\sum_{\beta\in S_{n-k}}\left(  -1\right)  ^{\beta}\prod_{i=1}%
^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}
\label{pf.lem.sol.det.laplace-multi.1.det2}%
\end{align}
(here, we have renamed the summation index $\sigma$ as $\beta$).

\begin{vershort}
Now, we claim the following: For any $\alpha\in S_{k}$ and $\beta\in S_{n-k}$,
we have%
\begin{equation}
\prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{i,\sigma_{Q,\alpha,\beta}\left(
i\right)  }=\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}
\label{pf.lem.sol.det.laplace-multi.1.short.factor1}%
\end{equation}
and%
\begin{equation}
\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma_{Q,\alpha,\beta
}\left(  i\right)  }=\prod_{i=1}^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}.
\label{pf.lem.sol.det.laplace-multi.1.short.factor2}%
\end{equation}


\textit{Proof of (\ref{pf.lem.sol.det.laplace-multi.1.short.factor1}) and
(\ref{pf.lem.sol.det.laplace-multi.1.short.factor2}):} Let $\alpha\in S_{k}$
and $\beta\in S_{n-k}$. The permutation $\sigma_{Q,\alpha,\beta}$ was defined
as the unique $\sigma\in S_{n}$ satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(
2\right)  },\ldots,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)
},r_{\beta\left(  2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)  .
\label{pf.lem.sol.det.laplace-multi.1.short.factor1.pf.1}%
\end{equation}
Hence, $\sigma_{Q,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.det.laplace-multi.1.short.factor1.pf.1}). In other words,
$\sigma_{Q,\alpha,\beta}$ is an element of $S_{n}$ and satisfies%
\[
\left(  \sigma_{Q,\alpha,\beta}\left(  1\right)  ,\sigma_{Q,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(  n\right)  \right)
=\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(  2\right)  }%
,\ldots,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)  }%
,r_{\beta\left(  2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)  .
\]
In other words,%
\begin{equation}
\left(  \sigma_{Q,\alpha,\beta}\left(  i\right)  =q_{\alpha\left(  i\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,k\right\}
\right)  \label{pf.lem.sol.det.laplace-multi.1.short.factor1.pf.4}%
\end{equation}
and%
\begin{equation}
\left(  \sigma_{Q,\alpha,\beta}\left(  i\right)  =r_{\beta\left(  i-k\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  k+1,k+2,\ldots,n\right\}
\right)  . \label{pf.lem.sol.det.laplace-multi.1.short.factor2.pf.4}%
\end{equation}
Now,%
\[
\underbrace{\prod_{i\in\left\{  1,2,\ldots,k\right\}  }}_{=\prod_{i=1}^{k}%
}\underbrace{a_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }}%
_{\substack{=a_{i,q_{\alpha\left(  i\right)  }}\\\text{(since }\sigma
_{Q,\alpha,\beta}\left(  i\right)  =q_{\alpha\left(  i\right)  }\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.short.factor1.pf.4})))}}}=\prod_{i=1}%
^{k}a_{i,q_{\alpha\left(  i\right)  }}.
\]
This proves (\ref{pf.lem.sol.det.laplace-multi.1.short.factor1}). Furthermore,%
\[
\underbrace{\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }}_{=\prod
_{i=k+1}^{n}}\underbrace{b_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }%
}_{\substack{=b_{i,r_{\beta\left(  i-k\right)  }}\\\text{(since }%
\sigma_{Q,\alpha,\beta}\left(  i\right)  =r_{\beta\left(  i-k\right)
}\\\text{(by (\ref{pf.lem.sol.det.laplace-multi.1.short.factor2.pf.4})))}%
}}=\prod_{i=k+1}^{n}b_{i,r_{\beta\left(  i-k\right)  }}=\prod_{i=1}%
^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}%
\]
(here, we have substituted $k+i$ for $i$ in the product). This proves
(\ref{pf.lem.sol.det.laplace-multi.1.short.factor2}).

Now,%
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{
1,2,\ldots,k\right\}  \right)  =Q}}}_{=\sum_{\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =Q\right\}
}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in\left\{  1,2,\ldots
,k\right\}  }a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod
_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma\left(  i\right)
}\right) \\
&  =\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =Q\right\}  }\left(  -1\right)  ^{\sigma
}\left(  \prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{i,\sigma\left(
i\right)  }\right)  \left(  \prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}
}b_{i,\sigma\left(  i\right)  }\right) \\
&  =\underbrace{\sum_{\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}}%
}_{=\sum_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}}\left(  -1\right)
^{\sigma_{Q,\alpha,\beta}}\underbrace{\left(  \prod_{i\in\left\{
1,2,\ldots,k\right\}  }a_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }\right)
}_{\substack{=\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.short.factor1}))}}}\underbrace{\left(
\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma_{Q,\alpha,\beta
}\left(  i\right)  }\right)  }_{\substack{=\prod_{i=1}^{n-k}b_{k+i,r_{\beta
\left(  i\right)  }}\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.short.factor2}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma_{Q,\alpha,\beta}\text{ for }%
\sigma\text{ in the sum, since the}\\
\text{map }S_{k}\times S_{n-k}\rightarrow\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =Q\right\}  ,\ \left(
\alpha,\beta\right)  \mapsto\sigma_{Q,\alpha,\beta}\\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}\underbrace{\left(
-1\right)  ^{\sigma_{Q,\alpha,\beta}}}_{\substack{=\left(  -1\right)
^{\alpha}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum
Q-\left(  1+2+\cdots+k\right)  }\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.sign}))}}}\left(  \prod_{i=1}%
^{k}a_{i,q_{\alpha\left(  i\right)  }}\right)  \left(  \prod_{i=1}%
^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}\right) \\
&  =\sum_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}\left(  -1\right)  ^{\alpha
}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum Q-\left(
1+2+\cdots+k\right)  }\left(  \prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)
}}\right)  \left(  \prod_{i=1}^{n-k}b_{k+i,r_{\beta\left(  i\right)  }%
}\right)
\end{align*}%
\begin{align*}
&  =\underbrace{\left(  -1\right)  ^{\sum Q-\left(  1+2+\cdots+k\right)  }%
}_{\substack{=\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum
Q}\\\text{(since }\sum Q-\left(  1+2+\cdots+k\right)  \\\equiv\sum Q+\left(
1+2+\cdots+k\right)  \\=\left(  1+2+\cdots+k\right)  +\sum Q\operatorname{mod}%
2\text{)}}}\underbrace{\left(  \sum_{\alpha\in S_{k}}\left(  -1\right)
^{\alpha}\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}\right)
}_{\substack{=\det\left(  \operatorname*{sub}\nolimits_{\left(  1,2,\ldots
,k\right)  }^{w\left(  Q\right)  }A\right)  \\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.det1}))}}}\underbrace{\left(  \sum
_{\beta\in S_{n-k}}\left(  -1\right)  ^{\beta}\prod_{i=1}^{n-k}b_{k+i,r_{\beta
\left(  i\right)  }}\right)  }_{\substack{=\det\left(  \operatorname*{sub}%
\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)
}B\right)  \\\text{(by (\ref{pf.lem.sol.det.laplace-multi.1.det2}))}}}\\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.det.laplace-multi.1}.
\end{vershort}

\begin{verlong}
For any $\alpha\in S_{k}$ and $\beta\in S_{n-k}$, we have%
\begin{equation}
\prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{i,\sigma_{Q,\alpha,\beta}\left(
i\right)  }=\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}
\label{pf.lem.sol.det.laplace-multi.1.factor1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.det.laplace-multi.1.factor1}):}
Let $\alpha\in S_{k}$ and $\beta\in S_{n-k}$. Now, $\sigma_{Q,\alpha,\beta}$
is the unique $\sigma\in S_{n}$ satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(
2\right)  },\ldots,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)
},r_{\beta\left(  2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)
\label{pf.lem.sol.det.laplace-multi.1.factor1.pf.1}%
\end{equation}
(because this is how $\sigma_{Q,\alpha,\beta}$ is defined). Hence,
$\sigma_{Q,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.det.laplace-multi.1.factor1.pf.1}). In other words,
$\sigma_{Q,\alpha,\beta}$ is an element of $S_{n}$ and satisfies%
\begin{align}
&  \left(  \sigma_{Q,\alpha,\beta}\left(  1\right)  ,\sigma_{Q,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(  n\right)  \right)
\nonumber\\
&  =\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(  2\right)  }%
,\ldots,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)  }%
,r_{\beta\left(  2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)  .
\label{pf.lem.sol.det.laplace-multi.1.factor1.pf.2}%
\end{align}
\par
Now,%
\begin{align*}
&  \left(  \sigma_{Q,\alpha,\beta}\left(  1\right)  ,\sigma_{Q,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(  k\right)  \right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  \sigma_{Q,\alpha,\beta}\left(  1\right)  ,\sigma
_{Q,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(
n\right)  \right)  }_{\substack{=\left(  q_{\alpha\left(  1\right)
},q_{\alpha\left(  2\right)  },\ldots,q_{\alpha\left(  k\right)  }%
,r_{\beta\left(  1\right)  },r_{\beta\left(  2\right)  },\ldots,r_{\beta
\left(  n-k\right)  }\right)  \\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.factor1.pf.2}))}}}\right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
q_{\alpha\left(  1\right)  },q_{\alpha\left(  2\right)  },\ldots
,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)  },r_{\beta\left(
2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)  \right) \\
&  =\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(  2\right)  }%
,\ldots,q_{\alpha\left(  k\right)  }\right)  .
\end{align*}
In other words,%
\begin{equation}
\sigma_{Q,\alpha,\beta}\left(  i\right)  =q_{\alpha\left(  i\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,k\right\}  .
\label{pf.lem.sol.det.laplace-multi.1.factor1.pf.4}%
\end{equation}
Hence,%
\[
\underbrace{\prod_{i\in\left\{  1,2,\ldots,k\right\}  }}_{=\prod_{i=1}^{k}%
}\underbrace{a_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }}%
_{\substack{=a_{i,q_{\alpha\left(  i\right)  }}\\\text{(since }\sigma
_{Q,\alpha,\beta}\left(  i\right)  =q_{\alpha\left(  i\right)  }\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.factor1.pf.4})))}}}=\prod_{i=1}%
^{k}a_{i,q_{\alpha\left(  i\right)  }}.
\]
This proves (\ref{pf.lem.sol.det.laplace-multi.1.factor1}).} and%
\begin{equation}
\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma_{Q,\alpha,\beta
}\left(  i\right)  }=\prod_{i=1}^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}
\label{pf.lem.sol.det.laplace-multi.1.factor2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.det.laplace-multi.1.factor2}):}
Let $\alpha\in S_{k}$ and $\beta\in S_{n-k}$. Now, $\sigma_{Q,\alpha,\beta}$
is the unique $\sigma\in S_{n}$ satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(
2\right)  },\ldots,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)
},r_{\beta\left(  2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)
\label{pf.lem.sol.det.laplace-multi.1.factor2.pf.1}%
\end{equation}
(because this is how $\sigma_{Q,\alpha,\beta}$ is defined). Hence,
$\sigma_{Q,\alpha,\beta}$ is a $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.det.laplace-multi.1.factor2.pf.1}). In other words,
$\sigma_{Q,\alpha,\beta}$ is an element of $S_{n}$ and satisfies%
\begin{align}
&  \left(  \sigma_{Q,\alpha,\beta}\left(  1\right)  ,\sigma_{Q,\alpha,\beta
}\left(  2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(  n\right)  \right)
\nonumber\\
&  =\left(  q_{\alpha\left(  1\right)  },q_{\alpha\left(  2\right)  }%
,\ldots,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)  }%
,r_{\beta\left(  2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)  .
\label{pf.lem.sol.det.laplace-multi.1.factor2.pf.2}%
\end{align}
\par
Now,%
\begin{align*}
&  \left(  \sigma_{Q,\alpha,\beta}\left(  k+1\right)  ,\sigma_{Q,\alpha,\beta
}\left(  k+2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(  n\right)  \right)
\\
&  =\left(  \text{the list of the last }n-k\text{ entries of the list
}\underbrace{\left(  \sigma_{Q,\alpha,\beta}\left(  1\right)  ,\sigma
_{Q,\alpha,\beta}\left(  2\right)  ,\ldots,\sigma_{Q,\alpha,\beta}\left(
n\right)  \right)  }_{\substack{=\left(  q_{\alpha\left(  1\right)
},q_{\alpha\left(  2\right)  },\ldots,q_{\alpha\left(  k\right)  }%
,r_{\beta\left(  1\right)  },r_{\beta\left(  2\right)  },\ldots,r_{\beta
\left(  n-k\right)  }\right)  \\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.factor2.pf.2}))}}}\right) \\
&  =\left(  \text{the list of the last }n-k\text{ entries of the list }\left(
q_{\alpha\left(  1\right)  },q_{\alpha\left(  2\right)  },\ldots
,q_{\alpha\left(  k\right)  },r_{\beta\left(  1\right)  },r_{\beta\left(
2\right)  },\ldots,r_{\beta\left(  n-k\right)  }\right)  \right) \\
&  =\left(  r_{\beta\left(  1\right)  },r_{\beta\left(  2\right)  }%
,\ldots,r_{\beta\left(  n-k\right)  }\right)  .
\end{align*}
In other words,%
\begin{equation}
\sigma_{Q,\alpha,\beta}\left(  k+i\right)  =r_{\beta\left(  i\right)
}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n-k\right\}  .
\label{pf.lem.sol.det.laplace-multi.1.factor2.pf.4}%
\end{equation}
Now,%
\begin{align*}
\underbrace{\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }}_{=\prod
_{i=k+1}^{n}}b_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }  &
=\prod_{i=k+1}^{n}b_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }=\prod
_{i=1}^{n-k}\underbrace{b_{i+k,\sigma_{Q,\alpha,\beta}\left(  k+i\right)  }%
}_{\substack{=b_{k+i,\sigma_{Q,\alpha,\beta}\left(  k+i\right)  }%
\\\text{(since }i+k=k+i\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i+k\text{ for
}i\text{ in the product}\right) \\
&  =\prod_{i=1}^{n-k}\underbrace{b_{k+i,\sigma_{Q,\alpha,\beta}\left(
k+i\right)  }}_{\substack{=b_{k+i,r_{\beta\left(  i\right)  }}\\\text{(since
}\sigma_{Q,\alpha,\beta}\left(  k+i\right)  =r_{\beta\left(  i\right)
}\\\text{(by (\ref{pf.lem.sol.det.laplace-multi.1.factor2.pf.4})))}}%
}=\prod_{i=1}^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}.
\end{align*}
This proves (\ref{pf.lem.sol.det.laplace-multi.1.factor2}).}.

Now,%
\begin{align*}
&  \underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{
1,2,\ldots,k\right\}  \right)  =Q}}}_{=\sum_{\sigma\in\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =Q\right\}
}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in\left\{  1,2,\ldots
,k\right\}  }a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod
_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma\left(  i\right)
}\right) \\
&  =\sum_{\sigma\in\left\{  \tau\in S_{n}\ \mid\ \tau\left(  \left\{
1,2,\ldots,k\right\}  \right)  =Q\right\}  }\left(  -1\right)  ^{\sigma
}\left(  \prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{i,\sigma\left(
i\right)  }\right)  \left(  \prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}
}b_{i,\sigma\left(  i\right)  }\right) \\
&  =\underbrace{\sum_{\left(  \alpha,\beta\right)  \in S_{k}\times S_{n-k}}%
}_{=\sum_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}}\left(  -1\right)
^{\sigma_{Q,\alpha,\beta}}\underbrace{\left(  \prod_{i\in\left\{
1,2,\ldots,k\right\}  }a_{i,\sigma_{Q,\alpha,\beta}\left(  i\right)  }\right)
}_{\substack{=\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.factor1}))}}}\underbrace{\left(
\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{i,\sigma_{Q,\alpha,\beta
}\left(  i\right)  }\right)  }_{\substack{=\prod_{i=1}^{n-k}b_{k+i,r_{\beta
\left(  i\right)  }}\\\text{(by (\ref{pf.lem.sol.det.laplace-multi.1.factor2}%
))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma_{Q,\alpha,\beta}\text{ for }%
\sigma\text{ in the sum, since the}\\
\text{map }S_{k}\times S_{n-k}\rightarrow\left\{  \tau\in S_{n}\ \mid
\ \tau\left(  \left\{  1,2,\ldots,k\right\}  \right)  =Q\right\}  ,\ \left(
\alpha,\beta\right)  \mapsto\sigma_{Q,\alpha,\beta}\\
\text{is a bijection}%
\end{array}
\right) \\
&  =\sum_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}\underbrace{\left(
-1\right)  ^{\sigma_{Q,\alpha,\beta}}}_{\substack{=\left(  -1\right)
^{\alpha}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum
Q-\left(  1+2+\cdots+k\right)  }\\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.sign}))}}}\left(  \prod_{i=1}%
^{k}a_{i,q_{\alpha\left(  i\right)  }}\right)  \left(  \prod_{i=1}%
^{n-k}b_{k+i,r_{\beta\left(  i\right)  }}\right) \\
&  =\sum_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}\left(  -1\right)  ^{\alpha
}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum Q-\left(
1+2+\cdots+k\right)  }\left(  \prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)
}}\right)  \left(  \prod_{i=1}^{n-k}b_{k+i,r_{\beta\left(  i\right)  }%
}\right)
\end{align*}%
\begin{align*}
&  =\left(  -1\right)  ^{\sum Q-\left(  1+2+\cdots+k\right)  }\underbrace{\sum
_{\alpha\in S_{k}}\sum_{\beta\in S_{n-k}}\left(  -1\right)  ^{\alpha}%
\cdot\left(  -1\right)  ^{\beta}\cdot\left(  \prod_{i=1}^{k}a_{i,q_{\alpha
\left(  i\right)  }}\right)  \left(  \prod_{i=1}^{n-k}b_{k+i,r_{\beta\left(
i\right)  }}\right)  }_{=\left(  \sum_{\alpha\in S_{k}}\left(  -1\right)
^{\alpha}\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}\right)  \left(
\sum_{\beta\in S_{n-k}}\left(  -1\right)  ^{\beta}\prod_{i=1}^{n-k}%
b_{k+i,r_{\beta\left(  i\right)  }}\right)  }\\
&  =\underbrace{\left(  -1\right)  ^{\sum Q-\left(  1+2+\cdots+k\right)  }%
}_{\substack{=\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum
Q}\\\text{(since }\sum Q-\left(  1+2+\cdots+k\right)  \\\equiv\sum Q+\left(
1+2+\cdots+k\right)  \\=\left(  1+2+\cdots+k\right)  +\sum Q\operatorname{mod}%
2\text{)}}}\underbrace{\left(  \sum_{\alpha\in S_{k}}\left(  -1\right)
^{\alpha}\prod_{i=1}^{k}a_{i,q_{\alpha\left(  i\right)  }}\right)
}_{\substack{=\det\left(  \operatorname*{sub}\nolimits_{\left(  1,2,\ldots
,k\right)  }^{w\left(  Q\right)  }A\right)  \\\text{(by
(\ref{pf.lem.sol.det.laplace-multi.1.det1}))}}}\underbrace{\left(  \sum
_{\beta\in S_{n-k}}\left(  -1\right)  ^{\beta}\prod_{i=1}^{n-k}b_{k+i,r_{\beta
\left(  i\right)  }}\right)  }_{\substack{=\det\left(  \operatorname*{sub}%
\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)
}B\right)  \\\text{(by (\ref{pf.lem.sol.det.laplace-multi.1.det2}))}}}\\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves Lemma \ref{lem.sol.det.laplace-multi.1}.
\end{verlong}
\end{proof}

Before we move on to the proof of Lemma \ref{lem.det.laplace-multi.Apq}, let
us show two more lemmas. The first one is a really simple fact about symmetric groups:

\begin{lemma}
\label{lem.sol.det.laplace-multi.group-bij}Let $n\in\mathbb{N}$. Let
$\gamma\in S_{n}$. Then, the map $S_{n}\rightarrow S_{n},\ \sigma\mapsto
\sigma\circ\gamma$ is a bijection.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.group-bij}.]It is easy to see
that the maps $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma$ and
$S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma^{-1}$ are mutually
inverse. Thus, the map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma
\circ\gamma$ is invertible, i.e., a bijection. Lemma
\ref{lem.sol.det.laplace-multi.group-bij} is proven.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.group-bij}.]Let $\mathbf{A}$ be
the map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma$. Let
$\mathbf{B}$ be the map $S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma
\circ\gamma^{-1}$. Then, $\mathbf{A}\circ\mathbf{B}=\operatorname*{id}%
\nolimits_{S_{n}}$\ \ \ \ \footnote{\textit{Proof.} Every $\sigma\in S_{n}$
satisfies%
\begin{align*}
\left(  \mathbf{A}\circ\mathbf{B}\right)  \left(  \sigma\right)   &
=\mathbf{A}\left(  \underbrace{\mathbf{B}\left(  \sigma\right)  }%
_{\substack{=\sigma\circ\gamma^{-1}\\\text{(by the definition of }%
\mathbf{B}\text{)}}}\right)  =\mathbf{A}\left(  \sigma\circ\gamma^{-1}\right)
=\left(  \sigma\circ\gamma^{-1}\right)  \circ\gamma\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\mathbf{A}\right) \\
&  =\sigma\circ\underbrace{\gamma^{-1}\circ\gamma}_{=\operatorname*{id}%
}=\sigma\circ\operatorname*{id}=\sigma=\operatorname*{id}\nolimits_{S_{n}%
}\left(  \sigma\right)  .
\end{align*}
In other words, $\mathbf{A}\circ\mathbf{B}=\operatorname*{id}\nolimits_{S_{n}%
}$. Qed.} and $\mathbf{B}\circ\mathbf{A}=\operatorname*{id}\nolimits_{S_{n}}%
$\ \ \ \ \footnote{\textit{Proof.} Every $\sigma\in S_{n}$ satisfies%
\begin{align*}
\left(  \mathbf{B}\circ\mathbf{A}\right)  \left(  \sigma\right)   &
=\mathbf{B}\left(  \underbrace{\mathbf{A}\left(  \sigma\right)  }%
_{\substack{=\sigma\circ\gamma\\\text{(by the definition of }\mathbf{A}%
\text{)}}}\right)  =\mathbf{B}\left(  \sigma\circ\gamma\right)  =\left(
\sigma\circ\gamma\right)  \circ\gamma^{-1}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\mathbf{B}\right) \\
&  =\sigma\circ\underbrace{\gamma\circ\gamma^{-1}}_{=\operatorname*{id}%
}=\sigma\circ\operatorname*{id}=\sigma=\operatorname*{id}\nolimits_{S_{n}%
}\left(  \sigma\right)  .
\end{align*}
In other words, $\mathbf{B}\circ\mathbf{A}=\operatorname*{id}\nolimits_{S_{n}%
}$. Qed.}. These two equalities show that the maps $\mathbf{A}$ and
$\mathbf{B}$ are mutually inverse. Hence, the map $\mathbf{A}$ is invertible.
In other words, the map $\mathbf{A}$ is a bijection. In other words, the map
$S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma$ is a bijection
(since the map $\mathbf{A}$ is the map $S_{n}\rightarrow S_{n},\ \sigma
\mapsto\sigma\circ\gamma$). This proves Lemma
\ref{lem.sol.det.laplace-multi.group-bij}.
\end{proof}
\end{verlong}

Next, we show a lemma which is a distillate of some parts of Exercise
\ref{exe.Ialbe}:

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.Ialbe}Let $n\in\mathbb{N}$. For any
subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote
the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $I$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Let $k=\left\vert
I\right\vert $. Then, there exists a $\sigma\in S_{n}$ satisfying $\left(
\sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
k\right)  \right)  =w\left(  I\right)  $, $\left(  \sigma\left(  k+1\right)
,\sigma\left(  k+2\right)  ,\ldots,\sigma\left(  n\right)  \right)  =w\left(
\widetilde{I}\right)  $ and $\left(  -1\right)  ^{\sigma}=\left(  -1\right)
^{\sum I-\left(  1+2+\cdots+k\right)  }$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Ialbe}.]We begin by
introducing some notations:

\begin{verlong}
For every $h\in\mathbb{N}$, we let $\operatorname*{id}\nolimits_{h}$ denote
the identity permutation $\operatorname*{id}\nolimits_{\left\{  1,2,\ldots
,h\right\}  }\in S_{h}$. For every $h\in\mathbb{N}$, we have%
\begin{equation}
\left(  -1\right)  ^{\operatorname*{id}\nolimits_{h}}=1.
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.1}%
\end{equation}
(Indeed, this is just a restatement of the well-known fact that $\left(
-1\right)  ^{\operatorname*{id}}=1$, where $\operatorname*{id}%
=\operatorname*{id}\nolimits_{h}$ is the identity permutation in $S_{h}$.)
\end{verlong}

\begin{vershort}
The definition of $\widetilde{I}$ yields $\widetilde{I}=\left\{
1,2,\ldots,n\right\}  \setminus I$. Since $I$ is a subset of $\left\{
1,2,\ldots,n\right\}  $, this leads to $\left\vert \widetilde{I}\right\vert
=\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert }%
_{=n}-\underbrace{\left\vert I\right\vert }_{=k}=n-k$. Thus, $n-k=\left\vert
\widetilde{I}\right\vert \geq0$, so that $n\geq k$ and thus $k\in\left\{
0,1,\ldots,n\right\}  $.
\end{vershort}

\begin{verlong}
The definition of $\widetilde{I}$ yields $\widetilde{I}=\left\{
1,2,\ldots,n\right\}  \setminus I\subseteq\left\{  1,2,\ldots,n\right\}  $.
Thus, $\widetilde{I}$ is a finite set (since $\left\{  1,2,\ldots,n\right\}  $
is a finite set). Hence, $\left\vert \widetilde{I}\right\vert \in\mathbb{N}$.

Also, $I\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, $I$ is a finite set
(since $\left\{  1,2,\ldots,n\right\}  $ is a finite set). Hence, $\left\vert
I\right\vert \in\mathbb{N}$. Thus, $k=\left\vert I\right\vert \in\mathbb{N}$.

Also,%
\begin{align*}
\left\vert \underbrace{\widetilde{I}}_{=\left\{  1,2,\ldots,n\right\}
\setminus I}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus I\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\underbrace{\left\vert I\right\vert }_{=k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }I\subseteq\left\{  1,2,\ldots
,n\right\}  \right) \\
&  =n-k.
\end{align*}
Hence, $n-k=\left\vert \widetilde{I}\right\vert \in\mathbb{N}$; thus,
$n-k\geq0$. In other words, $k\leq n$. Combined with $k\geq0$ (since
$k=\left\vert I\right\vert \in\mathbb{N}$), this yields $k\in\left\{
0,1,\ldots,n\right\}  $.
\end{verlong}

We know that $w\left(  I\right)  $ is the list of all elements of $I$ in
increasing order (with no repetitions) (by the definition of $w\left(
I\right)  $). Thus, $w\left(  I\right)  $ is a list of $\left\vert
I\right\vert $ elements. In other words, $w\left(  I\right)  $ is a list of
$k$ elements (since $\left\vert I\right\vert =k$).

Write $w\left(  I\right)  $ in the form $w\left(  I\right)  =\left(
a_{1},a_{2},\ldots,a_{k}\right)  $. (This is possible, since $w\left(
I\right)  $ is a list of $k$ elements.)

We know that $w\left(  I\right)  $ is the list of all elements of $I$ in
increasing order (with no repetitions). In other words, $\left(  a_{1}%
,a_{2},\ldots,a_{k}\right)  $ is the list of all elements of $I$ in increasing
order (with no repetitions) (since $w\left(  I\right)  =\left(  a_{1}%
,a_{2},\ldots,a_{k}\right)  $).

We know that $w\left(  \widetilde{I}\right)  $ is the list of all elements of
$\widetilde{I}$ in increasing order (with no repetitions) (by the definition
of $w\left(  \widetilde{I}\right)  $). Thus, $w\left(  \widetilde{I}\right)  $
is a list of $\left\vert \widetilde{I}\right\vert $ elements. In other words,
$w\left(  \widetilde{I}\right)  $ is a list of $n-k$ elements (since
$\left\vert \widetilde{I}\right\vert =n-k$).

Write $w\left(  \widetilde{I}\right)  $ in the form $w\left(  \widetilde{I}%
\right)  =\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $. (This is possible,
since $w\left(  \widetilde{I}\right)  $ is a list of $n-k$ elements.)

We know that $w\left(  \widetilde{I}\right)  $ is the list of all elements of
$\widetilde{I}$ in increasing order (with no repetitions). In other words,
$\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is the list of all elements of
$\widetilde{I}$ in increasing order (with no repetitions) (since $w\left(
\widetilde{I}\right)  =\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $). In other
words, $\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is the list of all
elements of $\left\{  1,2,\ldots,n\right\}  \setminus I$ in increasing order
(with no repetitions) (since $\widetilde{I}=\left\{  1,2,\ldots,n\right\}
\setminus I$).

Now we know that $\left(  a_{1},a_{2},\ldots,a_{k}\right)  $ is the list of
all elements of $I$ in increasing order (with no repetitions), and that
$\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $ is the list of all elements of
$\left\{  1,2,\ldots,n\right\}  \setminus I$ in increasing order (with no
repetitions). Hence, for every $\alpha\in S_{k}$ and $\beta\in S_{n-k}$, we
can define an element $\sigma_{I,\alpha,\beta}\in S_{n}$ according to Exercise
\ref{exe.Ialbe} \textbf{(a)}. Consider this $\sigma_{I,\alpha,\beta}$.
Exercise \ref{exe.Ialbe} \textbf{(b)} shows that for every $\alpha\in S_{k}$
and $\beta\in S_{n-k}$, we have%
\[
\ell\left(  \sigma_{I,\alpha,\beta}\right)  =\ell\left(  \alpha\right)
+\ell\left(  \beta\right)  +\sum I-\left(  1+2+\cdots+k\right)
\]
and%
\begin{equation}
\left(  -1\right)  ^{\sigma_{I,\alpha,\beta}}=\left(  -1\right)  ^{\alpha
}\cdot\left(  -1\right)  ^{\beta}\cdot\left(  -1\right)  ^{\sum I-\left(
1+2+\cdots+k\right)  }.
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.sign-gen}%
\end{equation}


\begin{vershort}
Now, consider the identity permutations $\operatorname*{id}\in S_{k}$ and
$\operatorname*{id}\in S_{n-k}$. (These are (in general) two different
permutations, although we denote them both by $\operatorname*{id}$.) They give
rise to an element $\sigma_{I,\operatorname*{id},\operatorname*{id}}\in S_{n}$
(obtained by setting $\alpha=\operatorname*{id}\in S_{k}$ and $\beta
=\operatorname*{id}\in S_{n-k}$ in the definition of $\sigma_{I,\alpha,\beta}%
$). Denote this element $\sigma_{I,\operatorname*{id},\operatorname*{id}}$ by
$\gamma$. Thus, $\gamma=\sigma_{I,\operatorname*{id},\operatorname*{id}}$, so
that%
\begin{align}
\left(  -1\right)  ^{\gamma}  &  =\left(  -1\right)  ^{\sigma
_{I,\operatorname*{id},\operatorname*{id}}}=\underbrace{\left(  -1\right)
^{\operatorname*{id}}}_{=1}\cdot\underbrace{\left(  -1\right)
^{\operatorname*{id}}}_{=1}\cdot\left(  -1\right)  ^{\sum I-\left(
1+2+\cdots+k\right)  }\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.sign-gen}) (applied to }%
\alpha=\operatorname*{id}\text{ and }\beta=\operatorname*{id}\text{)}\right)
\nonumber\\
&  =\left(  -1\right)  ^{\sum I-\left(  1+2+\cdots+k\right)  }.
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.sign-gamma}%
\end{align}

\end{vershort}

\begin{verlong}
Now, $\operatorname*{id}\nolimits_{k}\in S_{k}$ and $\operatorname*{id}%
\nolimits_{n-k}\in S_{n-k}$. Hence, an element $\sigma_{I,\operatorname*{id}%
\nolimits_{k},\operatorname*{id}\nolimits_{n-k}}\in S_{n}$ is defined (because
we can set $\alpha=\operatorname*{id}\nolimits_{k}$ and $\beta
=\operatorname*{id}\nolimits_{n-k}$ in the definition of $\sigma
_{I,\alpha,\beta}$). Denote this element $\sigma_{I,\operatorname*{id}%
\nolimits_{k},\operatorname*{id}\nolimits_{n-k}}$ by $\gamma$. Thus,
$\gamma=\sigma_{I,\operatorname*{id}\nolimits_{k},\operatorname*{id}%
\nolimits_{n-k}}$, so that%
\begin{align}
\left(  -1\right)  ^{\gamma}  &  =\left(  -1\right)  ^{\sigma
_{I,\operatorname*{id}\nolimits_{k},\operatorname*{id}\nolimits_{n-k}}%
}=\underbrace{\left(  -1\right)  ^{\operatorname*{id}\nolimits_{k}}%
}_{\substack{=1\\\text{(by (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.1}%
)}\\\text{(applied to }h=k\text{))}}}\cdot\underbrace{\left(  -1\right)
^{\operatorname*{id}\nolimits_{n-k}}}_{\substack{=1\\\text{(by
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.1})}\\\text{(applied to
}h=n-k\text{))}}}\cdot\left(  -1\right)  ^{\sum I-\left(  1+2+\cdots+k\right)
}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.sign-gen}) (applied to }%
\alpha=\operatorname*{id}\nolimits_{k}\text{ and }\beta=\operatorname*{id}%
\nolimits_{n-k}\text{)}\right) \nonumber\\
&  =\left(  -1\right)  ^{\sum I-\left(  1+2+\cdots+k\right)  }.
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.sign-gamma}%
\end{align}

\end{verlong}

Now, we claim that%
\begin{equation}
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =\left(  a_{1},a_{2},\ldots,a_{k}\right)
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}%
\end{equation}
and%
\begin{equation}
\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right)  =\left(  b_{1},b_{2},\ldots,b_{n-k}\right)
. \label{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}%
\end{equation}


\begin{vershort}
\textit{Proof of (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}%
) and (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}):} The
permutation $\gamma$ is $\sigma_{I,\operatorname*{id},\operatorname*{id}}$. In
other words, the permutation $\gamma$ is the unique $\sigma\in S_{n}$
satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\operatorname*{id}\left(  1\right)
},a_{\operatorname*{id}\left(  2\right)  },\ldots,a_{\operatorname*{id}\left(
k\right)  },b_{\operatorname*{id}\left(  1\right)  },b_{\operatorname*{id}%
\left(  2\right)  },\ldots,b_{\operatorname*{id}\left(  n-k\right)  }\right)
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.gamma-tuple.pf.1}%
\end{equation}
(because this is how $\sigma_{I,\operatorname*{id},\operatorname*{id}}$ was
defined). Hence, $\gamma$ is a $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.gamma-tuple.pf.1}). In
other words, $\gamma$ is an element of $S_{n}$ and satisfies%
\[
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
n\right)  \right)  =\left(  a_{\operatorname*{id}\left(  1\right)
},a_{\operatorname*{id}\left(  2\right)  },\ldots,a_{\operatorname*{id}\left(
k\right)  },b_{\operatorname*{id}\left(  1\right)  },b_{\operatorname*{id}%
\left(  2\right)  },\ldots,b_{\operatorname*{id}\left(  n-k\right)  }\right)
.
\]
Thus,%
\begin{align*}
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
n\right)  \right)   &  =\left(  a_{\operatorname*{id}\left(  1\right)
},a_{\operatorname*{id}\left(  2\right)  },\ldots,a_{\operatorname*{id}\left(
k\right)  },b_{\operatorname*{id}\left(  1\right)  },b_{\operatorname*{id}%
\left(  2\right)  },\ldots,b_{\operatorname*{id}\left(  n-k\right)  }\right)
\\
&  =\left(  a_{1},a_{2},\ldots,a_{k},b_{1},b_{2},\ldots,b_{n-k}\right)  .
\end{align*}
Hence,%
\begin{equation}
\left(  \gamma\left(  i\right)  =a_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,\ldots,k\right\}  \right)
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.gamma-tuple.pf.gamma1}%
\end{equation}
and%
\begin{equation}
\left(  \gamma\left(  i\right)  =b_{i-k}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  k+1,k+2,\ldots,n\right\}  \right)  .
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.gamma-tuple.pf.gamma2}%
\end{equation}
Now, (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}) follows
immediately from
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.gamma-tuple.pf.gamma1}).
Furthermore, (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2})
follows immediately from
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.short.gamma-tuple.pf.gamma2}).
Hence, both (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}) and
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}) are proven.
\end{vershort}

\begin{verlong}
\textit{Proof of (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}%
) and (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}):} The
permutation $\sigma_{I,\operatorname*{id}\nolimits_{k},\operatorname*{id}%
\nolimits_{n-k}}$ is the unique $\sigma\in S_{n}$ satisfying%
\begin{equation}
\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(
n\right)  \right)  =\left(  a_{\operatorname*{id}\nolimits_{k}\left(
1\right)  },a_{\operatorname*{id}\nolimits_{k}\left(  2\right)  }%
,\ldots,a_{\operatorname*{id}\nolimits_{k}\left(  k\right)  }%
,b_{\operatorname*{id}\nolimits_{n-k}\left(  1\right)  },b_{\operatorname*{id}%
\nolimits_{n-k}\left(  2\right)  },\ldots,b_{\operatorname*{id}\nolimits_{n-k}%
\left(  n-k\right)  }\right)
\label{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple.pf.1}%
\end{equation}
(because this is how $\sigma_{I,\operatorname*{id}\nolimits_{k}%
,\operatorname*{id}\nolimits_{n-k}}$ is defined). Hence, $\sigma
_{I,\operatorname*{id}\nolimits_{k},\operatorname*{id}\nolimits_{n-k}}$ is a
$\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple.pf.1}). In other
words, $\gamma$ is a $\sigma\in S_{n}$ satisfying
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple.pf.1}) (since
$\gamma=\sigma_{I,\operatorname*{id}\nolimits_{k},\operatorname*{id}%
\nolimits_{n-k}}$). In other words, $\gamma$ is an element of $S_{n}$ and
satisfies%
\begin{align}
&  \left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  n\right)  \right) \nonumber\\
&  =\left(  a_{\operatorname*{id}\nolimits_{k}\left(  1\right)  }%
,a_{\operatorname*{id}\nolimits_{k}\left(  2\right)  },\ldots
,a_{\operatorname*{id}\nolimits_{k}\left(  k\right)  },b_{\operatorname*{id}%
\nolimits_{n-k}\left(  1\right)  },b_{\operatorname*{id}\nolimits_{n-k}\left(
2\right)  },\ldots,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)
}\right)  . \label{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple.pf.2}%
\end{align}
Now, $\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  k\right)  \right)  =\left(  a_{\operatorname*{id}%
\nolimits_{k}\left(  1\right)  },a_{\operatorname*{id}\nolimits_{k}\left(
2\right)  },\ldots,a_{\operatorname*{id}\nolimits_{k}\left(  k\right)
}\right)  $\ \ \ \ \footnote{\textit{Proof.} We have%
\begin{align*}
&  \left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  k\right)  \right) \\
&  =\left(  \text{the list of the first }k\text{ entries of the list
}\underbrace{\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  n\right)  \right)  }_{\substack{=\left(
a_{\operatorname*{id}\nolimits_{k}\left(  1\right)  },a_{\operatorname*{id}%
\nolimits_{k}\left(  2\right)  },\ldots,a_{\operatorname*{id}\nolimits_{k}%
\left(  k\right)  },b_{\operatorname*{id}\nolimits_{n-k}\left(  1\right)
},b_{\operatorname*{id}\nolimits_{n-k}\left(  2\right)  },\ldots
,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)  }\right)  \\\text{(by
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple.pf.2}))}}}\right)
\\
&  =\left(  \text{the list of the first }k\text{ entries of the list }\left(
a_{\operatorname*{id}\nolimits_{k}\left(  1\right)  },a_{\operatorname*{id}%
\nolimits_{k}\left(  2\right)  },\ldots,a_{\operatorname*{id}\nolimits_{k}%
\left(  k\right)  },b_{\operatorname*{id}\nolimits_{n-k}\left(  1\right)
},b_{\operatorname*{id}\nolimits_{n-k}\left(  2\right)  },\ldots
,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)  }\right)  \right) \\
&  =\left(  a_{\operatorname*{id}\nolimits_{k}\left(  1\right)  }%
,a_{\operatorname*{id}\nolimits_{k}\left(  2\right)  },\ldots
,a_{\operatorname*{id}\nolimits_{k}\left(  k\right)  }\right)  ,
\end{align*}
qed.}. In other words, $\gamma\left(  i\right)  =a_{\operatorname*{id}%
\nolimits_{k}\left(  i\right)  }$ for every $i\in\left\{  1,2,\ldots
,k\right\}  $. Hence, for every $i\in\left\{  1,2,\ldots,k\right\}  $, we have
$\gamma\left(  i\right)  =a_{\operatorname*{id}\nolimits_{k}\left(  i\right)
}=a_{i}$ (since $\underbrace{\operatorname*{id}\nolimits_{k}}%
_{=\operatorname*{id}\nolimits_{\left\{  1,2,\ldots,k\right\}  }}\left(
i\right)  =\operatorname*{id}\nolimits_{\left\{  1,2,\ldots,k\right\}
}\left(  i\right)  =i$). In other words, $\left(  \gamma\left(  1\right)
,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  \right)  =\left(
a_{1},a_{2},\ldots,a_{k}\right)  $. This proves
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}).

Furthermore, $\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)
,\ldots,\gamma\left(  n\right)  \right)  =\left(  b_{\operatorname*{id}%
\nolimits_{n-k}\left(  1\right)  },b_{\operatorname*{id}\nolimits_{n-k}\left(
2\right)  },\ldots,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)
}\right)  $\ \ \ \ \footnote{\textit{Proof.} We have%
\begin{align*}
&  \left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right) \\
&  =\left(  \text{the list of the last }n-k\text{ entries of the list
}\underbrace{\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  n\right)  \right)  }_{\substack{=\left(
a_{\operatorname*{id}\nolimits_{k}\left(  1\right)  },a_{\operatorname*{id}%
\nolimits_{k}\left(  2\right)  },\ldots,a_{\operatorname*{id}\nolimits_{k}%
\left(  k\right)  },b_{\operatorname*{id}\nolimits_{n-k}\left(  1\right)
},b_{\operatorname*{id}\nolimits_{n-k}\left(  2\right)  },\ldots
,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)  }\right)  \\\text{(by
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple.pf.2}))}}}\right)
\\
&  =\left(  \text{the list of the last }n-k\text{ entries of the list }\left(
a_{\operatorname*{id}\nolimits_{k}\left(  1\right)  },a_{\operatorname*{id}%
\nolimits_{k}\left(  2\right)  },\ldots,a_{\operatorname*{id}\nolimits_{k}%
\left(  k\right)  },b_{\operatorname*{id}\nolimits_{n-k}\left(  1\right)
},b_{\operatorname*{id}\nolimits_{n-k}\left(  2\right)  },\ldots
,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)  }\right)  \right) \\
&  =\left(  b_{\operatorname*{id}\nolimits_{n-k}\left(  1\right)
},b_{\operatorname*{id}\nolimits_{n-k}\left(  2\right)  },\ldots
,b_{\operatorname*{id}\nolimits_{n-k}\left(  n-k\right)  }\right)  ,
\end{align*}
qed.}. In other words, $\gamma\left(  k+i\right)  =b_{\operatorname*{id}%
\nolimits_{n-k}\left(  i\right)  }$ for every $i\in\left\{  1,2,\ldots
,n-k\right\}  $. Hence, for every $i\in\left\{  1,2,\ldots,n-k\right\}  $, we
have $\gamma\left(  k+i\right)  =b_{\operatorname*{id}\nolimits_{n-k}\left(
i\right)  }=b_{i}$ (since $\underbrace{\operatorname*{id}\nolimits_{n-k}%
}_{=\operatorname*{id}\nolimits_{\left\{  1,2,\ldots,n-k\right\}  }}\left(
i\right)  =\operatorname*{id}\nolimits_{\left\{  1,2,\ldots,n-k\right\}
}\left(  i\right)  =i$). In other words, $\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  \right)  =\left(
b_{1},b_{2},\ldots,b_{n-k}\right)  $. This proves
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}). Hence, both
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}) and
(\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}) are proven.
\end{verlong}

Comparing (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple1}) with
$w\left(  I\right)  =\left(  a_{1},a_{2},\ldots,a_{k}\right)  $, we obtain
\[
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =w\left(  I\right)  .
\]
Comparing (\ref{pf.lem.sol.addexe.jacobi-complement.Ialbe.gamma-tuple2}) with
$w\left(  \widetilde{I}\right)  =\left(  b_{1},b_{2},\ldots,b_{n-k}\right)  $,
we obtain
\[
\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{I}\right)  .
\]
Now, we have shown that the permutation $\gamma\in S_{n}$ satisfies $\left(
\gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =w\left(  I\right)  $, $\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  \right)  =w\left(
\widetilde{I}\right)  $ and $\left(  -1\right)  ^{\gamma}=\left(  -1\right)
^{\sum I-\left(  1+2+\cdots+k\right)  }$. Hence, there exists a $\sigma\in
S_{n}$ satisfying $\left(  \sigma\left(  1\right)  ,\sigma\left(  2\right)
,\ldots,\sigma\left(  k\right)  \right)  =w\left(  I\right)  $, \newline%
$\left(  \sigma\left(  k+1\right)  ,\sigma\left(  k+2\right)  ,\ldots
,\sigma\left(  n\right)  \right)  =w\left(  \widetilde{I}\right)  $ and
$\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\sum I-\left(
1+2+\cdots+k\right)  }$ (namely, $\sigma=\gamma$). This proves Lemma
\ref{lem.sol.addexe.jacobi-complement.Ialbe}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.det.laplace-multi.Apq}.]Let us first study the sets
$P$ and $\widetilde{P}$.

\begin{vershort}
Define $k\in\mathbb{N}$ by $k=\left\vert P\right\vert =\left\vert Q\right\vert
$. (This makes sense, since we assumed that $\left\vert P\right\vert
=\left\vert Q\right\vert $.)

The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P$. Since $P$ is a subset of $\left\{
1,2,\ldots,n\right\}  $, this leads to $\left\vert \widetilde{P}\right\vert
=\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert }%
_{=n}-\underbrace{\left\vert P\right\vert }_{=k}=n-k$. Thus, $n-k=\left\vert
\widetilde{P}\right\vert \geq0$, so that $n\geq k$ and thus $k\in\left\{
0,1,\ldots,n\right\}  $.
\end{vershort}

\begin{verlong}
The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P\subseteq\left\{  1,2,\ldots,n\right\}  $.
Thus, $\widetilde{P}$ is a finite set (since $\left\{  1,2,\ldots,n\right\}  $
is a finite set). Hence, $\left\vert \widetilde{P}\right\vert \in\mathbb{N}$.

Also, $P\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, $P$ is a finite set
(since $\left\{  1,2,\ldots,n\right\}  $ is a finite set). Hence, $\left\vert
P\right\vert \in\mathbb{N}$. Thus, we can define $k\in\mathbb{N}$ by
$k=\left\vert P\right\vert $. Consider this $k$. We have $k=\left\vert
P\right\vert =\left\vert Q\right\vert $.

Also,%
\begin{align*}
\left\vert \underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}
\setminus P}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus P\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\underbrace{\left\vert P\right\vert }_{=k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\subseteq\left\{  1,2,\ldots
,n\right\}  \right) \\
&  =n-k.
\end{align*}
Hence, $n-k=\left\vert \widetilde{P}\right\vert \in\mathbb{N}$; thus,
$n-k\geq0$. In other words, $n\geq k$. In other words, $k\leq n$. Combined
with $k\geq0$ (since $k=\left\vert P\right\vert \in\mathbb{N}$), this yields
$k\in\left\{  0,1,\ldots,n\right\}  $.
\end{verlong}

Let $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ be the list of all elements of
$P$ in increasing order (with no repetitions). (This is well-defined, because
$\left\vert P\right\vert =k$.)

Let $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ be the list of all elements
of $\widetilde{P}$ in increasing order (with no repetitions). (This is
well-defined, because $\left\vert \widetilde{P}\right\vert =n-k$.)

Lemma \ref{lem.sol.addexe.jacobi-complement.Ialbe} (applied to $I=P$) shows
that there exists a $\sigma\in S_{n}$ satisfying $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  k\right)  \right)
=w\left(  P\right)  $, $\left(  \sigma\left(  k+1\right)  ,\sigma\left(
k+2\right)  ,\ldots,\sigma\left(  n\right)  \right)  =w\left(  \widetilde{P}%
\right)  $ and $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\sum
P-\left(  1+2+\cdots+k\right)  }$. Denote this $\sigma$ by $\gamma$. Thus,
$\gamma$ is an element of $S_{n}$ satisfying $\left(  \gamma\left(  1\right)
,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  \right)  =w\left(
P\right)  $, $\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)
,\ldots,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{P}\right)  $ and
$\left(  -1\right)  ^{\gamma}=\left(  -1\right)  ^{\sum P-\left(
1+2+\cdots+k\right)  }$.

Notice that%
\begin{align}
\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)  }\underbrace{\left(
-1\right)  ^{\gamma}}_{=\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots
+k\right)  }}  &  =\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)
}\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)  }\nonumber\\
&  =\left(  \left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)  }\right)
^{2}=\left(  -1\right)  ^{2\left(  \sum P-\left(  1+2+\cdots+k\right)
\right)  }\nonumber\\
&  =1 \label{pf.lem.det.laplace-multi.Apq.sign-gamma-multed}%
\end{align}
(since $2\left(  \sum P-\left(  1+2+\cdots+k\right)  \right)  $ is even).

Now, we claim that%
\begin{equation}
\left(  \gamma\left(  i\right)  =p_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,\ldots,k\right\}  \right)
\label{pf.lem.det.laplace-multi.Apq.gamma-tuple1}%
\end{equation}
and%
\begin{equation}
\left(  \gamma\left(  k+i\right)  =r_{i}\ \ \ \ \ \ \ \ \ \ \text{for every
}i\in\left\{  1,2,\ldots,n-k\right\}  \right)  .
\label{pf.lem.det.laplace-multi.Apq.gamma-tuple2}%
\end{equation}


\begin{vershort}
\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}) and
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2}):} The lists $w\left(
P\right)  $ and $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ must be identical
(since they are both defined to be the list of all elements of $P$ in
increasing order (with no repetitions)). In other words, we have $w\left(
P\right)  =\left(  p_{1},p_{2},\ldots,p_{k}\right)  $. Now,
\[
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =w\left(  P\right)  =\left(  p_{1},p_{2},\ldots
,p_{k}\right)  .
\]
In other words, $\gamma\left(  i\right)  =p_{i}$ for every $i\in\left\{
1,2,\ldots,k\right\}  $. This proves
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}).

The lists $w\left(  \widetilde{P}\right)  $ and $\left(  r_{1},r_{2}%
,\ldots,r_{n-k}\right)  $ must be identical (since they are both defined to be
the list of all elements of $\widetilde{P}$ in increasing order (with no
repetitions)). In other words, we have $w\left(  \widetilde{P}\right)
=\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $. Now,%
\[
\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{P}\right)  =\left(
r_{1},r_{2},\ldots,r_{n-k}\right)  .
\]
In other words, $\gamma\left(  k+i\right)  =r_{i}$ for every $i\in\left\{
1,2,\ldots,n-k\right\}  $. This proves
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2}). Thus, both
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}) and
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2}) are proven.
\end{vershort}

\begin{verlong}
\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}) and
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2}):} Recall that $w\left(
P\right)  $ is the list of all elements of $P$ in increasing order (with no
repetitions) (by the definition of $w\left(  P\right)  $). Thus,%
\begin{align*}
w\left(  P\right)   &  =\left(  \text{the list of all elements of }P\text{ in
increasing order (with no repetitions)}\right) \\
&  =\left(  p_{1},p_{2},\ldots,p_{k}\right)
\end{align*}
(since $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is the list of all elements
of $P$ in increasing order (with no repetitions)). Now,
\[
\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right)  =w\left(  P\right)  =\left(  p_{1},p_{2},\ldots
,p_{k}\right)  .
\]
In other words, $\gamma\left(  i\right)  =p_{i}$ for every $i\in\left\{
1,2,\ldots,k\right\}  $. This proves
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}).

Recall that $w\left(  \widetilde{P}\right)  $ is the list of all elements of
$\widetilde{P}$ in increasing order (with no repetitions) (by the definition
of $w\left(  \widetilde{P}\right)  $). Thus,%
\begin{align*}
w\left(  \widetilde{P}\right)   &  =\left(  \text{the list of all elements of
}\widetilde{P}\text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  r_{1},r_{2},\ldots,r_{n-k}\right)
\end{align*}
(since $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ is the list of all
elements of $\widetilde{P}$ in increasing order (with no repetitions)). Now,%
\[
\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{P}\right)  =\left(
r_{1},r_{2},\ldots,r_{n-k}\right)  .
\]
In other words, $\gamma\left(  k+i\right)  =r_{i}$ for every $i\in\left\{
1,2,\ldots,n-k\right\}  $. This proves
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2}). Thus, both
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}) and
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2}) are proven.
\end{verlong}

\begin{verlong}
Now, recall that $\gamma$ is an element of $S_{n}$. In other words, $\gamma$
is a permutation of $\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the
set of all permutations of $\left\{  1,2,\ldots,n\right\}  $). Hence, $\gamma$
is a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $.
\end{verlong}

Define an $n\times n$-matrix $A^{\prime}$ by $A^{\prime}=\left(
a_{\gamma\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Define an $n\times n$-matrix $B^{\prime}$ by $B^{\prime}=\left(
b_{\gamma\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Then,%
\begin{equation}
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A=\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }\left(  A^{\prime}\right)
\label{pf.lem.det.laplace-multi.Apq.sub1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.sub1}):} Write
the list $w\left(  Q\right)  $ in the form $w\left(  Q\right)  =\left(
q_{1},q_{2},\ldots,q_{\ell}\right)  $ for some $\ell\in\mathbb{N}$. (Actually,
$\ell=k$, but we will not use this.)
\par
Recall that $\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  \right)  =w\left(  P\right)  $, so that
$w\left(  P\right)  =\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  \right)  $.
\par
From $w\left(  P\right)  =\left(  \gamma\left(  1\right)  ,\gamma\left(
2\right)  ,\ldots,\gamma\left(  k\right)  \right)  $ and $w\left(  Q\right)
=\left(  q_{1},q_{2},\ldots,q_{\ell}\right)  $, we obtain%
\begin{align}
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A  &
=\operatorname*{sub}\nolimits_{\left(  \gamma\left(  1\right)  ,\gamma\left(
2\right)  ,\ldots,\gamma\left(  k\right)  \right)  }^{\left(  q_{1}%
,q_{2},\ldots,q_{\ell}\right)  }A=\operatorname*{sub}\nolimits_{\gamma\left(
1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  }%
^{q_{1},q_{2},\ldots,q_{\ell}}A=\left(  a_{\gamma\left(  x\right)  ,q_{y}%
}\right)  _{1\leq x\leq k,\ 1\leq y\leq\ell}%
\label{pf.lem.det.laplace-multi.Apq.sub1.pf.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{\gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots
,\gamma\left(  k\right)  }^{q_{1},q_{2},\ldots,q_{\ell}}A\text{, since
}A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .\nonumber
\end{align}
\par
On the other hand, from $w\left(  Q\right)  =\left(  q_{1},q_{2}%
,\ldots,q_{\ell}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }\left(  A^{\prime}\right)  =\operatorname*{sub}\nolimits_{\left(
1,2,\ldots,k\right)  }^{\left(  q_{1},q_{2},\ldots,q_{\ell}\right)  }\left(
A^{\prime}\right)  =\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{q_{1}%
,q_{2},\ldots,q_{\ell}}\left(  A^{\prime}\right)  =\left(  a_{\gamma\left(
x\right)  ,q_{y}}\right)  _{1\leq x\leq k,\ 1\leq y\leq\ell}%
\]
(by the definition of $\operatorname*{sub}\nolimits_{1,2,\ldots,k}%
^{q_{1},q_{2},\ldots,q_{\ell}}\left(  A^{\prime}\right)  $, since $A^{\prime
}=\left(  a_{\gamma\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$). Comparing this with (\ref{pf.lem.det.laplace-multi.Apq.sub1.pf.1}), we
obtain $\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A=\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }\left(  A^{\prime}\right)  $. This proves
(\ref{pf.lem.det.laplace-multi.Apq.sub1}).} and%
\begin{equation}
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B=\operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots
,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  B^{\prime}\right)
\label{pf.lem.det.laplace-multi.Apq.sub2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.sub2}):} Write
the list $w\left(  \widetilde{Q}\right)  $ in the form $w\left(
\widetilde{Q}\right)  =\left(  q_{1},q_{2},\ldots,q_{\ell}\right)  $ for some
$\ell\in\mathbb{N}$. (Actually, $\ell=n-k$, but we will not use this.)
\par
Recall that $\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)
,\ldots,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{P}\right)  $.
Thus, $w\left(  \widetilde{P}\right)  =\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  \right)  $.
\par
From $w\left(  \widetilde{P}\right)  =\left(  \gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  \right)  $ and
$w\left(  \widetilde{Q}\right)  =\left(  q_{1},q_{2},\ldots,q_{\ell}\right)
$, we obtain%
\begin{align}
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B  &  =\operatorname*{sub}\nolimits_{\left(
\gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(
n\right)  \right)  }^{\left(  q_{1},q_{2},\ldots,q_{\ell}\right)
}B=\operatorname*{sub}\nolimits_{\gamma\left(  k+1\right)  ,\gamma\left(
k+2\right)  ,\ldots,\gamma\left(  n\right)  }^{q_{1},q_{2},\ldots,q_{\ell}%
}B=\left(  b_{\gamma\left(  k+x\right)  ,q_{y}}\right)  _{1\leq x\leq
n-k,\ 1\leq y\leq\ell}\label{pf.lem.det.laplace-multi.Apq.sub2.pf.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{\gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  }^{q_{1},q_{2},\ldots,q_{\ell}}B\text{, since
}B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .\nonumber
\end{align}
\par
On the other hand, from $w\left(  \widetilde{Q}\right)  =\left(  q_{1}%
,q_{2},\ldots,q_{\ell}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{w\left(
\widetilde{Q}\right)  }\left(  B^{\prime}\right)  =\operatorname*{sub}%
\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{\left(  q_{1},q_{2}%
,\ldots,q_{\ell}\right)  }\left(  B^{\prime}\right)  =\operatorname*{sub}%
\nolimits_{k+1,k+2,\ldots,n}^{q_{1},q_{2},\ldots,q_{\ell}}\left(  B^{\prime
}\right)  =\left(  b_{\gamma\left(  k+x\right)  ,q_{y}}\right)  _{1\leq x\leq
n-k,\ 1\leq y\leq\ell}%
\]
(by the definition of $\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{q_{1},q_{2},\ldots,q_{\ell}}\left(  B^{\prime}\right)  $, since $B^{\prime
}=\left(  b_{\gamma\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$). Comparing this with (\ref{pf.lem.det.laplace-multi.Apq.sub2.pf.1}), we
obtain $\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }B=\operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  B^{\prime
}\right)  $. This proves (\ref{pf.lem.det.laplace-multi.Apq.sub2}).}.

Lemma \ref{lem.sol.det.laplace-multi.group-bij} shows that the map
$S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma$ is a bijection.

But $\gamma\left(  \left\{  1,2,\ldots,k\right\}  \right)  =P$%
\ \ \ \ \footnote{\textit{Proof.} We have
\begin{align*}
\gamma\left(  \left\{  1,2,\ldots,k\right\}  \right)   &  =\left\{
\gamma\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(
k\right)  \right\}  =\left\{  \underbrace{\gamma\left(  i\right)
}_{\substack{=p_{i}\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1}))}}}\ \mid\ i\in\left\{
1,2,\ldots,k\right\}  \right\} \\
&  =\left\{  p_{i}\ \mid\ i\in\left\{  1,2,\ldots,k\right\}  \right\}
=\left\{  p_{1},p_{2},\ldots,p_{k}\right\}  =P
\end{align*}
(since $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is a list of all elements
of $P$). Qed.}. Hence, every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\left(  \sigma\circ\gamma\right)  \left(  \left\{  1,2,\ldots,k\right\}
\right)  =\sigma\left(  \underbrace{\gamma\left(  \left\{  1,2,\ldots
,k\right\}  \right)  }_{=P}\right)  =\sigma\left(  P\right)  .
\label{pf.lem.det.laplace-multi.Apq.3a}%
\end{equation}


\begin{vershort}
Furthermore, every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)  ,\left(
\sigma\circ\gamma\right)  \left(  i\right)  }=\prod_{i\in P}a_{i,\sigma\left(
i\right)  } \label{pf.lem.det.laplace-multi.Apq.short.3b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.short.3b}):} Let
$\sigma\in S_{n}$.
\par
We shall use the notation introduced in Definition \ref{def.sol.exe.Ialbe.12n}%
. Thus, $\left[  k\right]  =\left\{  1,2,\ldots,k\right\}  $.
\par
Every $i\in\left\{  1,2,\ldots,k\right\}  $ satisfies $p_{i}=\gamma\left(
i\right)  $ (by (\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1})) and
$\sigma\left(  \underbrace{p_{i}}_{=\gamma\left(  i\right)  }\right)
=\sigma\left(  \gamma\left(  i\right)  \right)  =\left(  \sigma\circ
\gamma\right)  \left(  i\right)  $. Hence, every $i\in\left\{  1,2,\ldots
,k\right\}  $ satisfies%
\begin{equation}
a_{p_{i},\sigma\left(  p_{i}\right)  }=a_{\gamma\left(  i\right)  ,\left(
\sigma\circ\gamma\right)  \left(  i\right)  }.
\label{pf.lem.det.laplace-multi.Apq.short.3b.pf.1c}%
\end{equation}
\par
Recall that $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is the list of all
elements of $P$ in increasing order (with no repetitions). Hence, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $P$, $k$ and $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ instead of $S$, $s$ and $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[  k\right]
\rightarrow P,\ h\mapsto p_{h}$ is well-defined and a bijection. Hence, we can
substitute $p_{h}$ for $i$ in the product $\prod_{i\in P}a_{i,\sigma\left(
i\right)  }$. We thus obtain%
\begin{align*}
\prod_{i\in P}a_{i,\sigma\left(  i\right)  }  &  =\prod_{h\in\left[  k\right]
}a_{p_{h},\sigma\left(  p_{h}\right)  }=\underbrace{\prod_{i\in\left[
k\right]  }}_{\substack{=\prod_{i\in\left\{  1,2,\ldots,k\right\}
}\\\text{(since }\left[  k\right]  =\left\{  1,2,\ldots,k\right\}  \text{)}%
}}a_{p_{i},\sigma\left(  p_{i}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }h\text{
as }i\text{ in the product}\right) \\
&  =\prod_{i\in\left\{  1,2,\ldots,k\right\}  }\underbrace{a_{p_{i}%
,\sigma\left(  p_{i}\right)  }}_{\substack{=a_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.short.3b.pf.1c}))}}}\\
&  =\prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }.
\end{align*}
This proves (\ref{pf.lem.det.laplace-multi.Apq.short.3b}).} and%
\begin{equation}
\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }=\prod_{i\in
\widetilde{P}}b_{i,\sigma\left(  i\right)  }
\label{pf.lem.det.laplace-multi.Apq.short.3c}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.short.3c}):} Let
$\sigma\in S_{n}$.
\par
We shall use the notation introduced in Definition \ref{def.sol.exe.Ialbe.12n}%
. Thus, $\left[  n-k\right]  =\left\{  1,2,\ldots,n-k\right\}  $.
\par
Every $i\in\left\{  1,2,\ldots,n-k\right\}  $ satisfies $r_{i}=\gamma\left(
k+i\right)  $ (by (\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2})) and
$\sigma\left(  \underbrace{r_{i}}_{=\gamma\left(  k+i\right)  }\right)
=\sigma\left(  \gamma\left(  k+i\right)  \right)  =\left(  \sigma\circ
\gamma\right)  \left(  k+i\right)  $. Hence, every $i\in\left\{
1,2,\ldots,n-k\right\}  $ satisfies%
\begin{equation}
b_{r_{i},\sigma\left(  r_{i}\right)  }=b_{\gamma\left(  k+i\right)  ,\left(
\sigma\circ\gamma\right)  \left(  k+i\right)  }.
\label{pf.lem.det.laplace-multi.Apq.short.3c.pf.1c}%
\end{equation}
\par
Recall that $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ is the list of all
elements of $\widetilde{P}$ in increasing order (with no repetitions). Hence,
Lemma \ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $\widetilde{P}%
$, $n-k$ and $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ instead of $S$, $s$
and $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[
n-k\right]  \rightarrow\widetilde{P},\ h\mapsto r_{h}$ is well-defined and a
bijection. Hence, we can substitute $r_{h}$ for $i$ in the product
$\prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)  }$. We thus obtain%
\begin{align*}
\prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)  }  &  =\prod
_{h\in\left[  n-k\right]  }b_{r_{h},\sigma\left(  r_{h}\right)  }%
=\underbrace{\prod_{i\in\left[  n-k\right]  }}_{\substack{=\prod_{i\in\left\{
1,2,\ldots,n-k\right\}  }\\\text{(since }\left[  n-k\right]  =\left\{
1,2,\ldots,n-k\right\}  \text{)}}}b_{r_{i},\sigma\left(  r_{i}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }h\text{
as }i\text{ in the product}\right) \\
&  =\prod_{i\in\left\{  1,2,\ldots,n-k\right\}  }\underbrace{b_{r_{i}%
,\sigma\left(  r_{i}\right)  }}_{\substack{=b_{\gamma\left(  k+i\right)
,\left(  \sigma\circ\gamma\right)  \left(  k+i\right)  }\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.short.3c.pf.1c}))}}}\\
&  =\prod_{i\in\left\{  1,2,\ldots,n-k\right\}  }b_{\gamma\left(  k+i\right)
,\left(  \sigma\circ\gamma\right)  \left(  k+i\right)  }=\prod_{i\in\left\{
k+1,k+2,\ldots,n\right\}  }b_{\gamma\left(  i\right)  ,\left(  \sigma
\circ\gamma\right)  \left(  i\right)  }.\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}k+i\text{ in the product}\right)  .
\end{align*}
This proves (\ref{pf.lem.det.laplace-multi.Apq.short.3c}).}.
\end{vershort}

\begin{verlong}
Furthermore, every $\sigma\in S_{n}$ satisfies%
\begin{equation}
\prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)  ,\left(
\sigma\circ\gamma\right)  \left(  i\right)  }=\prod_{i\in P}a_{i,\sigma\left(
i\right)  } \label{pf.lem.det.laplace-multi.Apq.3b}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.3b}):} Let
$\sigma\in S_{n}$.
\par
We shall use the notation introduced in Definition \ref{def.sol.exe.Ialbe.12n}%
. Thus, $\left[  k\right]  =\left\{  1,2,\ldots,k\right\}  $ (by the
definition of $\left[  k\right]  $).
\par
Every $i\in\left\{  1,2,\ldots,k\right\}  $ satisfies%
\begin{equation}
p_{i}=\gamma\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple1})}\right)
\label{pf.lem.det.laplace-multi.Apq.3b.pf.1a}%
\end{equation}
and%
\begin{equation}
\sigma\left(  \underbrace{p_{i}}_{=\gamma\left(  i\right)  }\right)
=\sigma\left(  \gamma\left(  i\right)  \right)  =\left(  \sigma\circ
\gamma\right)  \left(  i\right)  .
\label{pf.lem.det.laplace-multi.Apq.3b.pf.1b}%
\end{equation}
Hence, every $i\in\left\{  1,2,\ldots,k\right\}  $ satisfies%
\begin{align}
a_{p_{i},\sigma\left(  p_{i}\right)  }  &  =a_{p_{i},\left(  \sigma\circ
\gamma\right)  \left(  i\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\sigma\left(  p_{i}\right)  =\left(  \sigma\circ\gamma\right)  \left(
i\right)  \text{ (by (\ref{pf.lem.det.laplace-multi.Apq.3b.pf.1b}))}\right)
\nonumber\\
&  =a_{\gamma\left(  i\right)  ,\left(  \sigma\circ\gamma\right)  \left(
i\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }p_{i}=\gamma\left(
i\right)  \text{ (by (\ref{pf.lem.det.laplace-multi.Apq.3b.pf.1a}))}\right)  .
\label{pf.lem.det.laplace-multi.Apq.3b.pf.1c}%
\end{align}
\par
Recall that $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is the list of all
elements of $P$ in increasing order (with no repetitions). Hence, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $P$, $k$ and $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ instead of $S$, $s$ and $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[  k\right]
\rightarrow P,\ h\mapsto p_{h}$ is well-defined and a bijection. In
particular, this map is a bijection. Hence, we can substitute $p_{h}$ for $i$
in the product $\prod_{i\in P}a_{i,\sigma\left(  i\right)  }$. We thus obtain%
\begin{align*}
\prod_{i\in P}a_{i,\sigma\left(  i\right)  }  &  =\prod_{h\in\left[  k\right]
}a_{p_{h},\sigma\left(  p_{h}\right)  }=\underbrace{\prod_{i\in\left[
k\right]  }}_{\substack{=\prod_{i\in\left\{  1,2,\ldots,k\right\}
}\\\text{(since }\left[  k\right]  =\left\{  1,2,\ldots,k\right\}  \text{)}%
}}a_{p_{i},\sigma\left(  p_{i}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }h\text{
as }i\text{ in the product}\right) \\
&  =\prod_{i\in\left\{  1,2,\ldots,k\right\}  }\underbrace{a_{p_{i}%
,\sigma\left(  p_{i}\right)  }}_{\substack{=a_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.3b.pf.1c}))}}}\\
&  =\prod_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }.
\end{align*}
This proves (\ref{pf.lem.det.laplace-multi.Apq.3b}).} and%
\begin{equation}
\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }=\prod_{i\in
\widetilde{P}}b_{i,\sigma\left(  i\right)  }
\label{pf.lem.det.laplace-multi.Apq.3c}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.det.laplace-multi.Apq.3c}):} Let
$\sigma\in S_{n}$.
\par
We shall use the notation introduced in Definition \ref{def.sol.exe.Ialbe.12n}%
. Thus, $\left[  n-k\right]  =\left\{  1,2,\ldots,n-k\right\}  $ (by the
definition of $\left[  n-k\right]  $).
\par
Every $i\in\left\{  1,2,\ldots,n-k\right\}  $ satisfies%
\begin{equation}
r_{i}=\gamma\left(  k+i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.det.laplace-multi.Apq.gamma-tuple2})}\right)
\label{pf.lem.det.laplace-multi.Apq.3c.pf.1a}%
\end{equation}
and%
\begin{equation}
\sigma\left(  \underbrace{r_{i}}_{=\gamma\left(  k+i\right)  }\right)
=\sigma\left(  \gamma\left(  k+i\right)  \right)  =\left(  \sigma\circ
\gamma\right)  \left(  k+i\right)  .
\label{pf.lem.det.laplace-multi.Apq.3c.pf.1b}%
\end{equation}
Hence, every $i\in\left\{  1,2,\ldots,n-k\right\}  $ satisfies%
\begin{align}
b_{r_{i},\sigma\left(  r_{i}\right)  }  &  =b_{r_{i},\left(  \sigma\circ
\gamma\right)  \left(  k+i\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}\sigma\left(  r_{i}\right)  =\left(  \sigma\circ\gamma\right)  \left(
k+i\right)  \text{ (by (\ref{pf.lem.det.laplace-multi.Apq.3c.pf.1b}))}\right)
\nonumber\\
&  =b_{\gamma\left(  k+i\right)  ,\left(  \sigma\circ\gamma\right)  \left(
k+i\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }r_{i}=\gamma\left(
k+i\right)  \text{ (by (\ref{pf.lem.det.laplace-multi.Apq.3c.pf.1a}))}\right)
\nonumber\\
&  =b_{\gamma\left(  i+k\right)  ,\left(  \sigma\circ\gamma\right)  \left(
i+k\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }k+i=i+k\right)  .
\label{pf.lem.det.laplace-multi.Apq.3c.pf.1c}%
\end{align}
\par
Recall that $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ is the list of all
elements of $\widetilde{P}$ in increasing order (with no repetitions). Hence,
Lemma \ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $\widetilde{P}%
$, $n-k$ and $\left(  r_{1},r_{2},\ldots,r_{n-k}\right)  $ instead of $S$, $s$
and $\left(  c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[
n-k\right]  \rightarrow\widetilde{P},\ h\mapsto r_{h}$ is well-defined and a
bijection. In particular, this map is a bijection. Hence, we can substitute
$r_{h}$ for $i$ in the product $\prod_{i\in\widetilde{P}}b_{i,\sigma\left(
i\right)  }$. We thus obtain%
\begin{align*}
\prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)  }  &  =\prod
_{h\in\left[  n-k\right]  }b_{r_{h},\sigma\left(  r_{h}\right)  }%
=\underbrace{\prod_{i\in\left[  n-k\right]  }}_{\substack{=\prod_{i\in\left\{
1,2,\ldots,n-k\right\}  }\\\text{(since }\left[  n-k\right]  =\left\{
1,2,\ldots,n-k\right\}  \text{)}}}b_{r_{i},\sigma\left(  r_{i}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the index }h\text{
as }i\text{ in the product}\right) \\
&  =\underbrace{\prod_{i\in\left\{  1,2,\ldots,n-k\right\}  }}_{=\prod
_{i=1}^{n-k}}\underbrace{b_{r_{i},\sigma\left(  r_{i}\right)  }}%
_{\substack{=b_{\gamma\left(  i+k\right)  ,\left(  \sigma\circ\gamma\right)
\left(  i+k\right)  }\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.3c.pf.1c}%
))}}}\\
&  =\prod_{i=1}^{n-k}b_{\gamma\left(  i+k\right)  ,\left(  \sigma\circ
\gamma\right)  \left(  i+k\right)  }=\underbrace{\prod_{i=k+1}^{n}}%
_{=\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }}b_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }i\text{ for
}i+k\text{ in the product}\right) \\
&  =\prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }b_{\gamma\left(  i\right)
,\left(  \sigma\circ\gamma\right)  \left(  i\right)  }.
\end{align*}
This proves (\ref{pf.lem.det.laplace-multi.Apq.3c}).}.
\end{verlong}

\begin{vershort}
Now, Lemma \ref{lem.sol.det.laplace-multi.1} (applied to $A^{\prime}$,
$a_{\gamma\left(  i\right)  ,j}$, $B^{\prime}$ and $b_{\gamma\left(  i\right)
,j}$ instead of $A$, $a_{i,j}$, $B$ and $b_{i,j}$) yields%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{  1,2,\ldots
,k\right\}  \right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod
_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)
,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\left\{  k+1,k+2,\ldots
,n\right\}  }b_{\gamma\left(  i\right)  ,\sigma\left(  i\right)  }\right) \\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det
\underbrace{\left(  \operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)
}^{w\left(  Q\right)  }\left(  A^{\prime}\right)  \right)  }%
_{\substack{=\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.sub1}))}%
}}\underbrace{\det\left(  \operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots
,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  B^{\prime}\right)
\right)  }_{\substack{=\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.sub2}))}}}\\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
Comparing this with%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{  1,2,\ldots
,k\right\}  \right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod
_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)
,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\left\{  k+1,k+2,\ldots
,n\right\}  }b_{\gamma\left(  i\right)  ,\sigma\left(  i\right)  }\right) \\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\left(  \sigma\circ
\gamma\right)  \left(  \left\{  1,2,\ldots,k\right\}  \right)  =Q}%
}}_{\substack{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}\\\text{(since every }\sigma\in S_{n}\\\text{satisfies }\left(
\sigma\circ\gamma\right)  \left(  \left\{  1,2,\ldots,k\right\}  \right)
=\sigma\left(  P\right)  \\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.3a}%
)))}}}\underbrace{\left(  -1\right)  ^{\sigma\circ\gamma}}_{\substack{=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\gamma}\\\text{(by
(\ref{eq.sign.prod}), applied}\\\text{to }\tau=\gamma\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \underbrace{\left(  \prod_{i\in\left\{  1,2,\ldots
,k\right\}  }a_{\gamma\left(  i\right)  ,\left(  \sigma\circ\gamma\right)
\left(  i\right)  }\right)  }_{\substack{=\prod_{i\in P}a_{i,\sigma\left(
i\right)  }\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.short.3b}))}%
}}\underbrace{\left(  \prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }%
b_{\gamma\left(  i\right)  ,\left(  \sigma\circ\gamma\right)  \left(
i\right)  }\right)  }_{\substack{=\prod_{i\in\widetilde{P}}b_{i,\sigma\left(
i\right)  }\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.short.3c}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma\circ\gamma\text{ for }\sigma\text{ in
the sum, since}\\
\text{the map }S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma\text{
is a bijection}%
\end{array}
\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\gamma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right) \\
&  =\left(  -1\right)  ^{\gamma}\cdot\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  ,
\end{align*}
we obtain%
\begin{align*}
&  \left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right) \\
&  =\left(  -1\right)  ^{\gamma}\cdot\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  .
\end{align*}
Multiplying both sides of this equality by $\left(  -1\right)  ^{\sum
P-\left(  1+2+\cdots+k\right)  }$, we obtain%
\begin{align*}
&  \left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)  }\left(
-1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right) \\
&  =\underbrace{\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)
}\left(  -1\right)  ^{\gamma}}_{\substack{=1\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.sign-gamma-multed}))}}}\cdot\sum
_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)
^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)  }\right)  \left(
\prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)  }\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right)  .
\end{align*}
Thus,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right) \\
&  =\underbrace{\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)
}\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}}%
_{\substack{=\left(  -1\right)  ^{\left(  \sum P-\left(  1+2+\cdots+k\right)
\right)  +\left(  \left(  1+2+\cdots+k\right)  +\sum Q\right)  }\\=\left(
-1\right)  ^{\sum P+\sum Q}\\\text{(since }\left(  \sum P-\left(
1+2+\cdots+k\right)  \right)  +\left(  \left(  1+2+\cdots+k\right)  +\sum
Q\right)  =\sum P+\sum Q\text{)}}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves Lemma \ref{lem.det.laplace-multi.Apq}.
\end{vershort}

\begin{verlong}
Now, Lemma \ref{lem.sol.det.laplace-multi.1} (applied to $A^{\prime}$,
$a_{\gamma\left(  i\right)  ,j}$, $B^{\prime}$ and $b_{\gamma\left(  i\right)
,j}$ instead of $A$, $a_{i,j}$, $B$ and $b_{i,j}$) yields%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{  1,2,\ldots
,k\right\}  \right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod
_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)
,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\left\{  k+1,k+2,\ldots
,n\right\}  }b_{\gamma\left(  i\right)  ,\sigma\left(  i\right)  }\right) \\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det
\underbrace{\left(  \operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)
}^{w\left(  Q\right)  }\left(  A^{\prime}\right)  \right)  }%
_{\substack{=\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.sub1}))}%
}}\underbrace{\det\left(  \operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots
,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  B^{\prime}\right)
\right)  }_{\substack{=\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.sub2}))}}}\\
&  =\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
Comparing this with%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  \left\{  1,2,\ldots
,k\right\}  \right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod
_{i\in\left\{  1,2,\ldots,k\right\}  }a_{\gamma\left(  i\right)
,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\left\{  k+1,k+2,\ldots
,n\right\}  }b_{\gamma\left(  i\right)  ,\sigma\left(  i\right)  }\right) \\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\left(  \sigma\circ
\gamma\right)  \left(  \left\{  1,2,\ldots,k\right\}  \right)  =Q}%
}}_{\substack{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}\\\text{(since every }\sigma\in S_{n}\\\text{satisfies }\left(
\sigma\circ\gamma\right)  \left(  \left\{  1,2,\ldots,k\right\}  \right)
=\sigma\left(  P\right)  \\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.3a}%
)))}}}\underbrace{\left(  -1\right)  ^{\sigma\circ\gamma}}_{\substack{=\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\gamma}\\\text{(by
(\ref{eq.sign.prod}), applied}\\\text{to }\tau=\gamma\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \underbrace{\left(  \prod_{i\in\left\{  1,2,\ldots
,k\right\}  }a_{\gamma\left(  i\right)  ,\left(  \sigma\circ\gamma\right)
\left(  i\right)  }\right)  }_{\substack{=\prod_{i\in P}a_{i,\sigma\left(
i\right)  }\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.3b}))}%
}}\underbrace{\left(  \prod_{i\in\left\{  k+1,k+2,\ldots,n\right\}  }%
b_{\gamma\left(  i\right)  ,\left(  \sigma\circ\gamma\right)  \left(
i\right)  }\right)  }_{\substack{=\prod_{i\in\widetilde{P}}b_{i,\sigma\left(
i\right)  }\\\text{(by (\ref{pf.lem.det.laplace-multi.Apq.3c}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\sigma\circ\gamma\text{ for }\sigma\text{ in
the sum, since}\\
\text{the map }S_{n}\rightarrow S_{n},\ \sigma\mapsto\sigma\circ\gamma\text{
is a bijection}%
\end{array}
\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\cdot\left(  -1\right)  ^{\gamma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right) \\
&  =\left(  -1\right)  ^{\gamma}\cdot\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  ,
\end{align*}
we obtain%
\begin{align*}
&  \left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right) \\
&  =\left(  -1\right)  ^{\gamma}\cdot\sum_{\substack{\sigma\in S_{n}%
;\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  .
\end{align*}
Multiplying both sides of this equality by $\left(  -1\right)  ^{\sum
P-\left(  1+2+\cdots+k\right)  }$, we obtain%
\begin{align*}
&  \left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)  }\left(
-1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right) \\
&  =\underbrace{\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)
}\left(  -1\right)  ^{\gamma}}_{\substack{=1\\\text{(by
(\ref{pf.lem.det.laplace-multi.Apq.sign-gamma-multed}))}}}\cdot\sum
_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)
^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)  }\right)  \left(
\prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)  }\right) \\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right)  .
\end{align*}
Thus,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right) \\
&  =\underbrace{\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)
}\left(  -1\right)  ^{\left(  1+2+\cdots+k\right)  +\sum Q}}%
_{\substack{=\left(  -1\right)  ^{\left(  \sum P-\left(  1+2+\cdots+k\right)
\right)  +\left(  \left(  1+2+\cdots+k\right)  +\sum Q\right)  }\\=\left(
-1\right)  ^{\sum P+\sum Q}\\\text{(since }\left(  \sum P-\left(
1+2+\cdots+k\right)  \right)  +\left(  \left(  1+2+\cdots+k\right)  +\sum
Q\right)  =\sum P+\sum Q\text{)}}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves Lemma \ref{lem.det.laplace-multi.Apq}.
\end{verlong}
\end{proof}

For the sake of convenience, let us restate a simplified particular case of
Lemma \ref{lem.det.laplace-multi.Apq} for $A=B$:

\begin{lemma}
\label{lem.sol.det.laplace-multi.2}Let $n\in\mathbb{N}$. For any subset $I$ of
$\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote the
complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an
$n\times n$-matrix. Let $P$ and $Q$ be two subsets of $\left\{  1,2,\ldots
,n\right\}  $ such that $\left\vert P\right\vert =\left\vert Q\right\vert $.
Then,%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\left(
-1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}A\right)  .
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.2}.]Every $\sigma\in S_{n}$
satisfies%
\begin{equation}
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}a_{i,\sigma\left(  i\right)  }\right)
\label{pf.lem.sol.det.laplace-multi.2.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.det.laplace-multi.2.1}):} Let
$\sigma\in S_{n}$. Notice that $\widetilde{P}=\left\{  1,2,\ldots,n\right\}
\setminus P$ (by the definition of $\widetilde{P}$). Now,%
\begin{align*}
\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  }%
}a_{i,\sigma\left(  i\right)  }  &  =\prod_{i\in\left\{  1,2,\ldots,n\right\}
}a_{i,\sigma\left(  i\right)  }=\left(  \underbrace{\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\in P}}}_{\substack{=\prod_{i\in
P}\\\text{(since }P\subseteq\left\{  1,2,\ldots,n\right\}  \text{)}%
}}a_{i,\sigma\left(  i\right)  }\right)  \left(  \underbrace{\prod
_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\notin P}}}%
_{\substack{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}%
=\prod_{i\in\widetilde{P}}\\\text{(since }\left\{  1,2,\ldots,n\right\}
\setminus P=\widetilde{P}\text{)}}}a_{i,\sigma\left(  i\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every }i\in\left\{  1,2,\ldots,n\right\}  \text{ satisfies either
}i\in P\\
\text{or }i\notin P\text{ (but not both)}%
\end{array}
\right) \\
&  =\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)  }\right)  \left(
\prod_{i\in\widetilde{P}}a_{i,\sigma\left(  i\right)  }\right)  .
\end{align*}
This proves (\ref{pf.lem.sol.det.laplace-multi.2.1}).}. Now,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)  }\right)
\left(  \prod_{i\in\widetilde{P}}a_{i,\sigma\left(  i\right)  }\right)
\\\text{(by (\ref{pf.lem.sol.det.laplace-multi.2.1}))}}}\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}a_{i,\sigma\left(  i\right)
}\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }A\right)
\end{align*}
(by Lemma \ref{lem.det.laplace-multi.Apq} (applied to $B=A$ and $b_{i,j}%
=a_{i,j}$)). This proves Lemma \ref{lem.sol.det.laplace-multi.2}.
\end{proof}

Another fact that we will need is very simple:

\begin{lemma}
\label{lem.sol.det.laplace-multi.4}Let $n\in\mathbb{N}$. Let $\sigma\in S_{n}%
$. Let $P$ be a subset of $\left\{  1,2,\ldots,n\right\}  $.

\textbf{(a)} The set $\sigma\left(  P\right)  $ is a subset of $\left\{
1,2,\ldots,n\right\}  $ satisfying $\left\vert \sigma\left(  P\right)
\right\vert =\left\vert P\right\vert $.

\textbf{(b)} Let $Q$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Then,
$\sigma^{-1}\left(  Q\right)  =P$ holds if and only if $\sigma\left(
P\right)  =Q$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.4}.]We have $\sigma\in S_{n}$.
In other words, $\sigma$ is a permutation of the set $\left\{  1,2,\ldots
,n\right\}  $. In other words, $\sigma$ is a bijective map $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $. The map
$\sigma$ is bijective, and therefore injective.

\textbf{(a)} The following fact is well-known: If $X$ and $Y$ are two finite
sets, if $f:X\rightarrow Y$ is an injective map, and if $U$ is a subset of
$X$, then $\left\vert f\left(  U\right)  \right\vert =\left\vert U\right\vert
$. Applying this fact to $X=\left\{  1,2,\ldots,n\right\}  $, $Y=\left\{
1,2,\ldots,n\right\}  $, $f=\sigma$ and $U=P$, we obtain $\left\vert
\sigma\left(  P\right)  \right\vert =\left\vert P\right\vert $ (since the map
$\sigma$ is injective). This proves Lemma \ref{lem.sol.det.laplace-multi.4}
\textbf{(a)}.

\textbf{(b)} We have $\sigma\left(  \sigma^{-1}\left(  Q\right)  \right)  =Q$
(since $\sigma$ is bijective). But if $U$ and $V$ are two subsets of $\left\{
1,2,\ldots,n\right\}  $, then $U=V$ holds if and only if $\sigma\left(
U\right)  =\sigma\left(  V\right)  $ (because $\sigma$ is bijective). Applying
this to $U=\sigma^{-1}\left(  Q\right)  $ and $V=P$, we conclude that
$\sigma^{-1}\left(  Q\right)  =P$ holds if and only if $\sigma\left(
\sigma^{-1}\left(  Q\right)  \right)  =\sigma\left(  P\right)  $. Hence, we
have the following chain of equivalences:%
\[
\left(  \sigma^{-1}\left(  Q\right)  =P\right)  \ \Longleftrightarrow\ \left(
\underbrace{\sigma\left(  \sigma^{-1}\left(  Q\right)  \right)  }_{=Q}%
=\sigma\left(  P\right)  \right)  \ \Longleftrightarrow\ \left(
Q=\sigma\left(  P\right)  \right)  \ \Longleftrightarrow\ \left(
\sigma\left(  P\right)  =Q\right)  .
\]
In other words, $\sigma^{-1}\left(  Q\right)  =P$ holds if and only if
$\sigma\left(  P\right)  =Q$. Lemma \ref{lem.sol.det.laplace-multi.4}
\textbf{(b)} is now proven.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.4}.]We have $\sigma\in S_{n}$.
In other words, $\sigma$ is a permutation of the set $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of the set
$\left\{  1,2,\ldots,n\right\}  $). In other words, $\sigma$ is a bijective
map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}
$. The map $\sigma$ is bijective, and therefore injective.

\textbf{(a)} Clearly, the set $\sigma\left(  P\right)  $ is a subset of
$\left\{  1,2,\ldots,n\right\}  $. It remains to prove that $\left\vert
\sigma\left(  P\right)  \right\vert =\left\vert P\right\vert $.

The following fact is well-known: If $X$ and $Y$ are two finite sets, if
$f:X\rightarrow Y$ is an injective map, and if $U$ is a subset of $X$, then
$\left\vert f\left(  U\right)  \right\vert =\left\vert U\right\vert $.
Applying this fact to $X=\left\{  1,2,\ldots,n\right\}  $, $Y=\left\{
1,2,\ldots,n\right\}  $, $f=\sigma$ and $U=P$, we obtain $\left\vert
\sigma\left(  P\right)  \right\vert =\left\vert P\right\vert $ (since the map
$\sigma$ is injective). This proves Lemma \ref{lem.sol.det.laplace-multi.4}
\textbf{(a)}.

\textbf{(b)} We shall first prove the logical implication%
\begin{equation}
\left(  \sigma^{-1}\left(  Q\right)  =P\right)  \ \Longrightarrow\ \left(
\sigma\left(  P\right)  =Q\right)  .
\label{pf.lem.sol.det.laplace-multi.4.dir1}%
\end{equation}


\textit{Proof of (\ref{pf.lem.sol.det.laplace-multi.4.dir1}):} Assume that
$\sigma^{-1}\left(  Q\right)  =P$. We shall show that $\sigma\left(  P\right)
=Q$.

We have $\sigma\left(  P\right)  \subseteq Q$\ \ \ \ \footnote{\textit{Proof.}
Let $z\in\sigma\left(  P\right)  $. Thus, $z=\sigma\left(  p\right)  $ for
some $p\in P$. Consider this $p$. We have $p\in P=\sigma^{-1}\left(  Q\right)
$, so that $\sigma\left(  p\right)  \in Q$. Thus, $z=\sigma\left(  p\right)
\in Q$.
\par
Now, forget that we fixed $z$. We thus have shown that $z\in Q$ for every
$z\in\sigma\left(  P\right)  $. In other words, $\sigma\left(  P\right)
\subseteq Q$. Qed.} and $Q\subseteq\sigma\left(  P\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $q\in Q$. Now, $\sigma^{-1}\left(
q\right)  $ is a well-defined element of $\left\{  1,2,\ldots,n\right\}  $
(since the map $\sigma$ is bijective). This element $\sigma^{-1}\left(
q\right)  $ belongs to $\sigma^{-1}\left(  Q\right)  $ (since $\sigma\left(
\sigma^{-1}\left(  q\right)  \right)  =q\in Q$). Thus, $\sigma^{-1}\left(
q\right)  \in\sigma^{-1}\left(  Q\right)  =P$. Now, $q=\sigma\left(
\underbrace{\sigma^{-1}\left(  q\right)  }_{\in P}\right)  \in\sigma\left(
P\right)  $.
\par
Now, forget that we fixed $q$. We thus have shown that $q\in\sigma\left(
P\right)  $ for every $q\in Q$. In other words, $Q\subseteq\sigma\left(
P\right)  $. Qed.}. Combining these two relations, we obtain $\sigma\left(
P\right)  =Q$.

Now, forget that we assumed that $\sigma^{-1}\left(  Q\right)  =P$. We thus
have proven that $\sigma\left(  P\right)  =Q$ under the assumption that
$\sigma^{-1}\left(  Q\right)  =P$. In other words, we have proven the
implication (\ref{pf.lem.sol.det.laplace-multi.4.dir1}).

Next, we shall prove the logical implication%
\begin{equation}
\left(  \sigma\left(  P\right)  =Q\right)  \ \Longrightarrow\ \left(
\sigma^{-1}\left(  Q\right)  =P\right)  .
\label{pf.lem.sol.det.laplace-multi.4.dir2}%
\end{equation}


\textit{Proof of (\ref{pf.lem.sol.det.laplace-multi.4.dir2}):} Assume that
$\sigma\left(  P\right)  =Q$. We shall show that $\sigma^{-1}\left(  Q\right)
=P$.

We have $\sigma^{-1}\left(  Q\right)  \subseteq P$%
\ \ \ \ \footnote{\textit{Proof.} Let $z\in\sigma^{-1}\left(  Q\right)  $.
Thus, $\sigma\left(  z\right)  \in Q=\sigma\left(  P\right)  $. In other
words, $\sigma\left(  z\right)  =\sigma\left(  p\right)  $ for some $p\in P$.
Consider this $p$.
\par
The map $\sigma$ is injective. Thus, from $\sigma\left(  z\right)
=\sigma\left(  p\right)  $, we obtain $z=p$. Hence, $z=p\in P$.
\par
Now, forget that we fixed $z$. We thus have shown that $z\in P$ for every
$z\in\sigma^{-1}\left(  Q\right)  $. In other words, $\sigma^{-1}\left(
Q\right)  \subseteq P$. Qed.} and $P\subseteq\sigma^{-1}\left(  Q\right)
$\ \ \ \ \footnote{\textit{Proof.} Let $p\in P$. Then, $\sigma\left(
\underbrace{p}_{\in P}\right)  \in\sigma\left(  P\right)  =Q$, so that
$p\in\sigma^{-1}\left(  Q\right)  $.
\par
Now, forget that we fixed $p$. We thus have shown that $p\in\sigma^{-1}\left(
Q\right)  $ for every $p\in P$. In other words, $P\subseteq\sigma^{-1}\left(
Q\right)  $. Qed.}. Combining these two relations, we obtain $\sigma
^{-1}\left(  Q\right)  =P$.

Now, forget that we assumed that $\sigma\left(  P\right)  =Q$. We thus have
proven that $\left(  \sigma^{-1}\left(  Q\right)  =P\right)  $ under the
assumption that $\sigma\left(  P\right)  =Q$. In other words, we have proven
the implication (\ref{pf.lem.sol.det.laplace-multi.4.dir2}).

Combining the two implications (\ref{pf.lem.sol.det.laplace-multi.4.dir1}) and
(\ref{pf.lem.sol.det.laplace-multi.4.dir2}), we obtain the logical equivalence
$\left(  \sigma^{-1}\left(  Q\right)  =P\right)  \ \Longleftrightarrow
\ \left(  \sigma\left(  P\right)  =Q\right)  $. In other words, $\sigma
^{-1}\left(  Q\right)  =P$ holds if and only if $\sigma\left(  P\right)  =Q$.
This proves Lemma \ref{lem.sol.det.laplace-multi.4} \textbf{(b)}.
\end{proof}
\end{verlong}

We can now step to the proof of Theorem \ref{thm.det.laplace-multi}:

\begin{proof}
[Proof of Theorem \ref{thm.det.laplace-multi}.]Write the $n\times n$-matrix
$A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

If $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert \neq\left\vert P\right\vert $, then%
\begin{equation}
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=0
\label{pf.thm.det.laplace-multi.emptysum}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.det.laplace-multi.emptysum}):} Let $P$
and $Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert \neq\left\vert P\right\vert $.
\par
Let $\sigma\in S_{n}$ be such that $\sigma\left(  P\right)  =Q$. We shall
derive a contradiction.
\par
Indeed, Lemma \ref{lem.sol.det.laplace-multi.4} \textbf{(a)} shows that the
set $\sigma\left(  P\right)  $ is a subset of $\left\{  1,2,\ldots,n\right\}
$ satisfying $\left\vert \sigma\left(  P\right)  \right\vert =\left\vert
P\right\vert $. Hence, $\left\vert P\right\vert =\left\vert \underbrace{\sigma
\left(  P\right)  }_{=Q}\right\vert =\left\vert Q\right\vert \neq\left\vert
P\right\vert $. This is absurd. Hence, we have found a contradiction.
\par
Now, forget that we fixed $\sigma$. We thus have found a contradiction for
every $\sigma\in S_{n}$ satisfying $\sigma\left(  P\right)  =Q$. Thus, there
exists no $\sigma\in S_{n}$ satisfying $\sigma\left(  P\right)  =Q$. Hence,
the sum $\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }$ is an empty
sum. Thus,%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }=\left(
\text{empty sum}\right)  =0.
\]
This proves (\ref{pf.thm.det.laplace-multi.emptysum}).}.

\textbf{(a)} Let $P$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Then,
(\ref{eq.det.eq.2}) yields%
\begin{align*}
\det A  &  =\underbrace{\sum_{\sigma\in S_{n}}}_{\substack{=\sum
_{Q\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\\\text{(because for every }\sigma\in
S_{n}\text{, the set}\\\sigma\left(  P\right)  \text{ is a subset of }\left\{
1,2,\ldots,n\right\}  \text{)}}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }=\sum_{Q\subseteq\left\{  1,2,\ldots
,n\right\}  }\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)
}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\sum_{\substack{Q\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert \neq\left\vert P\right\vert
}}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=0\\\text{(by (\ref{pf.thm.det.laplace-multi.emptysum}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every subset }Q\text{ of }\left\{  1,2,\ldots,n\right\}  \text{
satisfies}\\
\text{either }\left\vert Q\right\vert =\left\vert P\right\vert \text{ or
}\left\vert Q\right\vert \neq\left\vert P\right\vert \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\underbrace{\sum
_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert \neq\left\vert P\right\vert }}0}_{=0}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\left(  -1\right)
^{\sum P+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}A\right)  \\\text{(by Lemma \ref{lem.sol.det.laplace-multi.2}}\\\text{(since
}\left\vert P\right\vert =\left\vert Q\right\vert \text{ (since }\left\vert
Q\right\vert =\left\vert P\right\vert \text{))}}}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\end{align*}
This proves Theorem \ref{thm.det.laplace-multi} \textbf{(a)}.

\textbf{(b)} Let $Q$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Then,
(\ref{eq.det.eq.2}) yields%
\begin{align*}
\det A  &  =\underbrace{\sum_{\sigma\in S_{n}}}_{\substack{=\sum
_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma^{-1}\left(  Q\right)  =P}}\\\text{(because for every }\sigma\in
S_{n}\text{, the set}\\\sigma^{-1}\left(  Q\right)  \text{ is a subset of
}\left\{  1,2,\ldots,n\right\}  \text{)}}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\underbrace{\sum
_{\substack{\sigma\in S_{n};\\\sigma^{-1}\left(  Q\right)  =P}}}%
_{\substack{=\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}\\\text{(because for every }\sigma\in S_{n}\text{,}\\\text{the statement
}\left(  \sigma^{-1}\left(  Q\right)  =P\right)  \\\text{is equivalent
to}\\\text{the statement }\left(  \sigma\left(  P\right)  =Q\right)
\\\text{(by Lemma \ref{lem.sol.det.laplace-multi.4} \textbf{(b)}))}}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\\
&  =\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\sum_{\substack{P\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert P\right\vert \neq\left\vert Q\right\vert
}}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n}a_{i,\sigma\left(  i\right)  }%
}_{\substack{=0\\\text{(by (\ref{pf.thm.det.laplace-multi.emptysum}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every subset }P\text{ of }\left\{  1,2,\ldots,n\right\}  \text{
satisfies}\\
\text{either }\left\vert P\right\vert =\left\vert Q\right\vert \text{ or
}\left\vert P\right\vert \neq\left\vert Q\right\vert \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }+\underbrace{\sum
_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert \neq\left\vert Q\right\vert }}0}_{=0}\\
&  =\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n}a_{i,\sigma\left(  i\right)  }}_{\substack{=\left(  -1\right)
^{\sum P+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}A\right)  \\\text{(by Lemma \ref{lem.sol.det.laplace-multi.2})}}}\\
&  =\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\end{align*}
This proves Theorem \ref{thm.det.laplace-multi} \textbf{(b)}.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.det.laplace-multi}.]We have now proven both
Lemma \ref{lem.det.laplace-multi.Apq} and Theorem \ref{thm.det.laplace-multi}.
Thus, Exercise \ref{exe.det.laplace-multi} is solved.
\end{proof}

\subsection{Solution to Exercise \ref{exe.det.laplace-multi.0}}

Throughout this section, we shall use the notations introduced in Definition
\ref{def.submatrix} and in Definition \ref{def.sect.laplace.notations}.

Before we start solving Exercise \ref{exe.det.laplace-multi.0}, let us show a
trivial lemma:

\begin{lemma}
\label{lem.sol.det.laplace-multi.0.subAT}Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $A$ be an $n\times m$-matrix. Let $\mathbf{i}$ be a
finite list of elements of $\left\{  1,2,\ldots,n\right\}  $. Let $\mathbf{j}$
be a finite list of elements of $\left\{  1,2,\ldots,m\right\}  $. Then,
$\left(  \operatorname*{sub}\nolimits_{\mathbf{i}}^{\mathbf{j}}A\right)
^{T}=\operatorname*{sub}\nolimits_{\mathbf{j}}^{\mathbf{i}}\left(
A^{T}\right)  $.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.0.subAT}.]Write the list
$\mathbf{i}$ as $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $. Write the list
$\mathbf{j}$ as $\left(  j_{1},j_{2},\ldots,j_{v}\right)  $. Then, Lemma
\ref{lem.sol.det.laplace-multi.0.subAT} follows immediately from Proposition
\ref{prop.submatrix.easy} \textbf{(e)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.det.laplace-multi.0.subAT}.]Write the list
$\mathbf{i}$ in the form $\mathbf{i}=\left(  i_{1},i_{2},\ldots,i_{u}\right)
$. Then, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  =\mathbf{i}$ is a list of
elements of $\left\{  1,2,\ldots,n\right\}  $. In other words, $i_{1}%
,i_{2},\ldots,i_{u}$ are some elements of $\left\{  1,2,\ldots,n\right\}  $.

Write the list $\mathbf{j}$ in the form $\mathbf{j}=\left(  j_{1},j_{2}%
,\ldots,j_{v}\right)  $. Then, $\left(  j_{1},j_{2},\ldots,j_{v}\right)
=\mathbf{j}$ is a list of elements of $\left\{  1,2,\ldots,m\right\}  $. In
other words, $j_{1},j_{2},\ldots,j_{v}$ be some elements of $\left\{
1,2,\ldots,m\right\}  $.

From $\mathbf{i}=\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\mathbf{j}%
=\left(  j_{1},j_{2},\ldots,j_{v}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{\mathbf{i}}^{\mathbf{j}}A=\operatorname*{sub}%
\nolimits_{\left(  i_{1},i_{2},\ldots,i_{u}\right)  }^{\left(  j_{1}%
,j_{2},\ldots,j_{v}\right)  }A=\operatorname*{sub}\nolimits_{i_{1}%
,i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A.
\]
Hence,%
\begin{equation}
\left(  \underbrace{\operatorname*{sub}\nolimits_{\mathbf{i}}^{\mathbf{j}}%
A}_{=\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}A}\right)  ^{T}=\left(  \operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}A\right)
^{T}=\operatorname*{sub}\nolimits_{j_{1},j_{2},\ldots,j_{v}}^{i_{1}%
,i_{2},\ldots,i_{u}}\left(  A^{T}\right)
\label{pf.lem.sol.det.laplace-multi.0.subAT.1}%
\end{equation}
(by Proposition \ref{prop.submatrix.easy} \textbf{(e)}). From $\mathbf{j}%
=\left(  j_{1},j_{2},\ldots,j_{v}\right)  $ and $\mathbf{i}=\left(
i_{1},i_{2},\ldots,i_{u}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{\mathbf{j}}^{\mathbf{i}}\left(  A^{T}\right)
=\operatorname*{sub}\nolimits_{\left(  j_{1},j_{2},\ldots,j_{v}\right)
}^{\left(  i_{1},i_{2},\ldots,i_{u}\right)  }\left(  A^{T}\right)
=\operatorname*{sub}\nolimits_{j_{1},j_{2},\ldots,j_{v}}^{i_{1},i_{2}%
,\ldots,i_{u}}\left(  A^{T}\right)  .
\]
Comparing this with (\ref{pf.lem.sol.det.laplace-multi.0.subAT.1}), we obtain
$\left(  \operatorname*{sub}\nolimits_{\mathbf{i}}^{\mathbf{j}}A\right)
^{T}=\operatorname*{sub}\nolimits_{\mathbf{j}}^{\mathbf{i}}\left(
A^{T}\right)  $. This proves Lemma \ref{lem.sol.det.laplace-multi.0.subAT}.
\end{proof}
\end{verlong}

\begin{corollary}
\label{cor.sol.det.laplace-multi.0.detsubAT}Let $n\in\mathbb{N}$. For any
subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote
the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A$ be an $n\times n$-matrix. Let $U$ and $V$ be two subsets of $\left\{
1,2,\ldots,n\right\}  $ satisfying $\left\vert U\right\vert =\left\vert
V\right\vert $. Then,%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  U\right)  }^{w\left(
V\right)  }\left(  A^{T}\right)  \right)  =\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  V\right)  }^{w\left(  U\right)  }A\right)
\label{eq.cor.sol.det.laplace-multi.0.detsubAT.1}%
\end{equation}
and%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{U}\right)
}^{w\left(  \widetilde{V}\right)  }\left(  A^{T}\right)  \right)  =\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{V}\right)  }^{w\left(
\widetilde{U}\right)  }A\right)  .
\label{eq.cor.sol.det.laplace-multi.0.detsubAT.2}%
\end{equation}

\end{corollary}

\begin{vershort}
\begin{proof}
[Proof of Corollary \ref{cor.sol.det.laplace-multi.0.detsubAT}.]Let
$k=\left\vert U\right\vert =\left\vert V\right\vert $. Then, each of the lists
$w\left(  U\right)  $ and $w\left(  V\right)  $ is a list of $k$ elements.
Hence, $\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A$ is a $k\times k$-matrix. Thus, Exercise \ref{exe.ps4.4} (applied to $k$
and $\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A$ instead of $n$ and $A$) yields%
\begin{equation}
\det\left(  \left(  \operatorname*{sub}\nolimits_{w\left(  V\right)
}^{w\left(  U\right)  }A\right)  ^{T}\right)  =\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  V\right)  }^{w\left(  U\right)  }A\right)  .
\label{pf.cor.sol.det.laplace-multi.0.detsubAT.short.1}%
\end{equation}
But each of $w\left(  U\right)  $ and $w\left(  V\right)  $ is a list of
elements of $\left\{  1,2,\ldots,n\right\}  $ (since $U$ and $V$ are subsets
of $\left\{  1,2,\ldots,n\right\}  $). Hence, Proposition
\ref{lem.sol.det.laplace-multi.0.subAT} (applied to $m=n$, $\mathbf{i}%
=w\left(  V\right)  $ and $\mathbf{j}=w\left(  U\right)  $) yields $\left(
\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A\right)  ^{T}=\operatorname*{sub}\nolimits_{w\left(  U\right)  }^{w\left(
V\right)  }\left(  A^{T}\right)  $. Taking determinants on both sides of this
equality, we obtain%
\[
\det\left(  \left(  \operatorname*{sub}\nolimits_{w\left(  V\right)
}^{w\left(  U\right)  }A\right)  ^{T}\right)  =\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  U\right)  }^{w\left(  V\right)  }\left(  A^{T}\right)
\right)  .
\]
Comparing this with (\ref{pf.cor.sol.det.laplace-multi.0.detsubAT.short.1}),
we obtain $\det\left(  \operatorname*{sub}\nolimits_{w\left(  U\right)
}^{w\left(  V\right)  }\left(  A^{T}\right)  \right)  =\det\left(
\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A\right)  $. This proves (\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.1}).

The definition of $\widetilde{U}$ yields $\widetilde{U}=\left\{
1,2,\ldots,n\right\}  \setminus U$. Since $U$ is a subset of $\left\{
1,2,\ldots,n\right\}  $, this yields $\left\vert \widetilde{U}\right\vert
=\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}  \right\vert }%
_{=n}-\underbrace{\left\vert U\right\vert }_{=k}=n-k$. Similarly, $\left\vert
\widetilde{V}\right\vert =n-k$. Thus, $\left\vert \widetilde{U}\right\vert
=n-k=\left\vert \widetilde{V}\right\vert $. Now,
(\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.1}) (applied to $\widetilde{U}$
and $\widetilde{V}$ instead of $U$ and $V$) yields $\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{U}\right)  }^{w\left(
\widetilde{V}\right)  }\left(  A^{T}\right)  \right)  =\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{V}\right)  }^{w\left(
\widetilde{U}\right)  }A\right)  $. This proves
(\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.2}). Thus, the proof of
Corollary \ref{cor.sol.det.laplace-multi.0.detsubAT} is complete.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Corollary \ref{cor.sol.det.laplace-multi.0.detsubAT}.]We have
$U\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, the set $U$ is finite
(since the set $\left\{  1,2,\ldots,n\right\}  $ is finite). Hence,
$\left\vert U\right\vert \in\mathbb{N}$. Define $k\in\mathbb{N}$ by
$k=\left\vert U\right\vert $. Thus, $k=\left\vert U\right\vert =\left\vert
V\right\vert $.

Recall that $w\left(  U\right)  $ is the list of all elements of $U$ in
increasing order (with no repetitions) (by the definition of $w\left(
U\right)  $). Thus, $w\left(  U\right)  $ is a list of size $\left\vert
U\right\vert $. In other words, $w\left(  U\right)  $ is a list of size $k$
(since $\left\vert U\right\vert =k$).

Write the list $w\left(  U\right)  $ in the form $w\left(  U\right)  =\left(
i_{1},i_{2},\ldots,i_{k}\right)  $. (This is possible, since $w\left(
U\right)  $ is a list of size $k$.)

Recall that $w\left(  V\right)  $ is the list of all elements of $V$ in
increasing order (with no repetitions) (by the definition of $w\left(
V\right)  $). Thus, $w\left(  V\right)  $ is a list of size $\left\vert
V\right\vert $. In other words, $w\left(  V\right)  $ is a list of size $k$
(since $\left\vert V\right\vert =k$).

Write the list $w\left(  V\right)  $ in the form $w\left(  V\right)  =\left(
j_{1},j_{2},\ldots,j_{k}\right)  $. (This is possible, since $w\left(
V\right)  $ is a list of size $k$.)

From $w\left(  U\right)  =\left(  i_{1},i_{2},\ldots,i_{k}\right)  $ and
$w\left(  V\right)  =\left(  j_{1},j_{2},\ldots,j_{k}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A=\operatorname*{sub}\nolimits_{\left(  j_{1},j_{2},\ldots,j_{k}\right)
}^{\left(  i_{1},i_{2},\ldots,i_{k}\right)  }A=\operatorname*{sub}%
\nolimits_{j_{1},j_{2},\ldots,j_{k}}^{i_{1},i_{2},\ldots,i_{k}}A.
\]
Hence, $\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A$ is a $k\times k$-matrix (since $\operatorname*{sub}\nolimits_{j_{1}%
,j_{2},\ldots,j_{k}}^{i_{1},i_{2},\ldots,i_{k}}A$ is a $k\times k$-matrix).
Thus, Exercise \ref{exe.ps4.4} (applied to $k$ and $\operatorname*{sub}%
\nolimits_{w\left(  V\right)  }^{w\left(  U\right)  }A$ instead of $n$ and
$A$) yields%
\begin{equation}
\det\left(  \left(  \operatorname*{sub}\nolimits_{w\left(  V\right)
}^{w\left(  U\right)  }A\right)  ^{T}\right)  =\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  V\right)  }^{w\left(  U\right)  }A\right)  .
\label{pf.cor.sol.det.laplace-multi.0.detsubAT.det1}%
\end{equation}


We know that $w\left(  U\right)  $ is the list of all elements of $U$ in
increasing order (with no repetitions). Thus, $w\left(  U\right)  $ is a list
of elements of $U$. In other words, the entries of $w\left(  U\right)  $ are
elements of $U$. Hence, the entries of $w\left(  U\right)  $ are elements of
$\left\{  1,2,\ldots,n\right\}  $ (since every element of $U$ is an element of
$\left\{  1,2,\ldots,n\right\}  $ (since $U\subseteq\left\{  1,2,\ldots
,n\right\}  $)). In other words, $w\left(  U\right)  $ is a list of elements
of $\left\{  1,2,\ldots,n\right\}  $. The same argument (applied to $V$
instead of $U$) shows that $w\left(  V\right)  $ is a list of elements of
$\left\{  1,2,\ldots,n\right\}  $. Hence, Proposition
\ref{lem.sol.det.laplace-multi.0.subAT} (applied to $m=n$, $\mathbf{i}%
=w\left(  V\right)  $ and $\mathbf{j}=w\left(  U\right)  $) yields $\left(
\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A\right)  ^{T}=\operatorname*{sub}\nolimits_{w\left(  U\right)  }^{w\left(
V\right)  }\left(  A^{T}\right)  $. Hence,%
\[
\det\left(  \underbrace{\left(  \operatorname*{sub}\nolimits_{w\left(
V\right)  }^{w\left(  U\right)  }A\right)  ^{T}}_{=\operatorname*{sub}%
\nolimits_{w\left(  U\right)  }^{w\left(  V\right)  }\left(  A^{T}\right)
}\right)  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  U\right)
}^{w\left(  V\right)  }\left(  A^{T}\right)  \right)  .
\]
Comparing this with (\ref{pf.cor.sol.det.laplace-multi.0.detsubAT.det1}), we
obtain $\det\left(  \operatorname*{sub}\nolimits_{w\left(  U\right)
}^{w\left(  V\right)  }\left(  A^{T}\right)  \right)  =\det\left(
\operatorname*{sub}\nolimits_{w\left(  V\right)  }^{w\left(  U\right)
}A\right)  $. This proves (\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.1}).

Now, the definition of $\widetilde{U}$ yields $\widetilde{U}=\left\{
1,2,\ldots,n\right\}  \setminus U\subseteq\left\{  1,2,\ldots,n\right\}  $.
Hence, $\widetilde{U}$ is a subset of $\left\{  1,2,\ldots,n\right\}  $. The
same argument (applied to $V$ instead of $U$) shows that $\widetilde{V}$ is a
subset of $\left\{  1,2,\ldots,n\right\}  $.

Moreover, we have $\widetilde{U}=\left\{  1,2,\ldots,n\right\}  \setminus U$
and thus
\begin{align*}
\left\vert \widetilde{U}\right\vert  &  =\left\vert \left\{  1,2,\ldots
,n\right\}  \setminus U\right\vert =\underbrace{\left\vert \left\{
1,2,\ldots,n\right\}  \right\vert }_{=n}-\underbrace{\left\vert U\right\vert
}_{=k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }U\text{ is a subset of
}\left\{  1,2,\ldots,n\right\}  \right) \\
&  =n-k.
\end{align*}
The same argument (applied to $V$ instead of $U$) shows that $\left\vert
\widetilde{V}\right\vert =n-k$. Hence, $\left\vert \widetilde{U}\right\vert
=n-k=\left\vert \widetilde{V}\right\vert $. Now,
(\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.1}) (applied to $\widetilde{U}$
and $\widetilde{V}$ instead of $U$ and $V$) yields $\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{U}\right)  }^{w\left(
\widetilde{V}\right)  }\left(  A^{T}\right)  \right)  =\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{V}\right)  }^{w\left(
\widetilde{U}\right)  }A\right)  $. This proves
(\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.2}). Thus, the proof of
Corollary \ref{cor.sol.det.laplace-multi.0.detsubAT} is complete.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Exercise \ref{exe.det.laplace-multi.0}.]\textbf{(a)} Let $P$ be a
subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
P\right\vert =\left\vert R\right\vert $ and $P\neq R$.

\begin{vershort}
Let $k=\left\vert P\right\vert $. The definition of $\widetilde{P}$ yields
$\widetilde{P}=\left\{  1,2,\ldots,n\right\}  \setminus P$. Since $P$ is a
subset of $\left\{  1,2,\ldots,n\right\}  $, this leads to $\left\vert
\widetilde{P}\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots
,n\right\}  \right\vert }_{=n}-\underbrace{\left\vert P\right\vert }_{=k}%
=n-k$. Thus, $n-k=\left\vert \widetilde{P}\right\vert \geq0$, so that $n\geq
k$ and thus $k\in\left\{  0,1,\ldots,n\right\}  $.
\end{vershort}

\begin{verlong}
The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P\subseteq\left\{  1,2,\ldots,n\right\}  $.
Thus, $\widetilde{P}$ is a finite set (since $\left\{  1,2,\ldots,n\right\}  $
is a finite set). Hence, $\left\vert \widetilde{P}\right\vert \in\mathbb{N}$.

Also, $P\subseteq\left\{  1,2,\ldots,n\right\}  $. Thus, $P$ is a finite set
(since $\left\{  1,2,\ldots,n\right\}  $ is a finite set). Hence, $\left\vert
P\right\vert \in\mathbb{N}$.

Define $k\in\mathbb{N}$ by $k=\left\vert P\right\vert $. (This is
well-defined, since $\left\vert P\right\vert \in\mathbb{N}$.)

Also,%
\begin{align*}
\left\vert \underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}
\setminus P}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus P\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\underbrace{\left\vert P\right\vert }_{=k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\subseteq\left\{  1,2,\ldots
,n\right\}  \right) \\
&  =n-k.
\end{align*}
Hence, $n-k=\left\vert \widetilde{P}\right\vert \in\mathbb{N}$; thus,
$n-k\geq0$. In other words, $k\leq n$. Combined with $k\geq0$ (since
$k=\left\vert P\right\vert \in\mathbb{N}$), this yields $k\in\left\{
0,1,\ldots,n\right\}  $.

We have $k\in\mathbb{N}$, so that $k\geq0$. Thus, $\underbrace{k}_{\geq
0}+1\geq1$.
\end{verlong}

Let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.
Thus, $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $.

We have $k=\left\vert P\right\vert =\left\vert R\right\vert $, so that
$\left\vert R\right\vert =k$.

From $P\neq R$, we can easily deduce that $\widetilde{P}\cap R\neq\varnothing
$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary (for the sake of
contradiction). Thus, $\widetilde{P}\cap R=\varnothing$.
\par
Any three sets $X$, $Y$ and $Z$ satisfy $\left(  X\cap Y\right)  \setminus
Z=X\cap\left(  Y\setminus Z\right)  $. Applying this to $X=R$, $Y=\left\{
1,2,\ldots,n\right\}  $ and $Z=P$, we obtain%
\[
\left(  R\cap\left\{  1,2,\ldots,n\right\}  \right)  \setminus P=R\cap
\underbrace{\left(  \left\{  1,2,\ldots,n\right\}  \setminus P\right)
}_{=\widetilde{P}}=R\cap\widetilde{P}=\widetilde{P}\cap R=\varnothing.
\]
Comparing this with $\underbrace{\left(  R\cap\left\{  1,2,\ldots,n\right\}
\right)  }_{\substack{=R\\\text{(since }R\subseteq\left\{  1,2,\ldots
,n\right\}  \text{)}}}\setminus P=R\setminus P$, we obtain $R\setminus
P=\varnothing$. In other words, $R\subseteq P$. Combining this with $R\neq P$
(since $P\neq R$), we conclude that $R$ is a proper subset of $P$.
\par
But $P$ is a finite set. Hence, every proper subset of $P$ has a size strictly
smaller than $\left\vert P\right\vert $. In other words, if $Y$ is a proper
subset of $P$, then $\left\vert Y\right\vert <\left\vert P\right\vert $.
Applying this to $Y=R$, we conclude that $\left\vert R\right\vert <\left\vert
P\right\vert $. Hence, $\left\vert R\right\vert <\left\vert P\right\vert
=\left\vert R\right\vert $. This is absurd. This contradiction proves that our
assumption was wrong, qed.}.

Let $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ be the list of all elements of
$R$ in increasing order (with no repetitions). (This is well-defined, because
$\left\vert R\right\vert =k$.)

Let $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ be the list of all elements
of $\widetilde{P}$ in increasing order (with no repetitions). (This is
well-defined, because $\left\vert \widetilde{P}\right\vert =n-k$.)

\begin{vershort}
We know that $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ is a list of elements
of $R$. Hence, the elements $r_{1},r_{2},\ldots,r_{k}$ belong to $R$, and thus
to $\left[  n\right]  $ (since $R\subseteq\left\{  1,2,\ldots,n\right\}
=\left[  n\right]  $).

We know that $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ is a list of
elements of $\widetilde{P}$. Hence, the elements $t_{1},t_{2},\ldots,t_{n-k}$
belong to $\widetilde{P}$, and thus to $\left[  n\right]  $ (since
$\widetilde{P}\subseteq\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $).
\end{vershort}

\begin{verlong}
We know that $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ is a list of elements
of $R$ (since $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ is the list of all
elements of $R$ in increasing order (with no repetitions)). Hence, the $k$
elements $r_{1},r_{2},\ldots,r_{k}$ belong to $R$, and thus belong to $\left[
n\right]  $ as well (since $R\subseteq\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $).

We know that $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ is a list of
elements of $\widetilde{P}$ (since $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)
$ is the list of all elements of $\widetilde{P}$ in increasing order (with no
repetitions)). Hence, the $n-k$ elements $t_{1},t_{2},\ldots,t_{n-k}$ belong
to $\widetilde{P}$, and thus belong to $\left[  n\right]  $ as well (since
$\widetilde{P}\subseteq\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $).
\end{verlong}

\begin{vershort}
The $n$ elements $r_{1},r_{2},\ldots,r_{k},t_{1},t_{2},\ldots,t_{n-k}$ belong
to $\left[  n\right]  $ (since the $k$ elements $r_{1},r_{2},\ldots,r_{k}$
belong to $\left[  n\right]  $, and since the $n-k$ elements $t_{1}%
,t_{2},\ldots,t_{n-k}$ belong to $\left[  n\right]  $). Hence, we can define
an $n$-tuple $\left(  \kappa_{1},\kappa_{2},\ldots,\kappa_{n}\right)
\in\left[  n\right]  ^{n}$ by%
\[
\left(  \kappa_{1},\kappa_{2},\ldots,\kappa_{n}\right)  =\left(  r_{1}%
,r_{2},\ldots,r_{k},t_{1},t_{2},\ldots,t_{n-k}\right)  .
\]
Consider this $n$-tuple. Due to its definition, we have%
\begin{equation}
\left(  \kappa_{i}=r_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
1,2,\ldots,k\right\}  \right)
\label{sol.exe.det.laplace-multi.0.short.a.kappai1}%
\end{equation}
and%
\begin{equation}
\left(  \kappa_{i}=t_{i-k}\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{
k+1,k+2,\ldots,n\right\}  \right)  .
\label{sol.exe.det.laplace-multi.0.short.a.kappai2}%
\end{equation}


Define a map $\kappa:\left[  n\right]  \rightarrow\left[  n\right]  $ by%
\[
\left(  \kappa\left(  i\right)  =\kappa_{i}\text{ for every }i\in\left[
n\right]  \right)  .
\]
(This makes sense, since $\kappa_{i}\in\left[  n\right]  $ for every
$i\in\left[  n\right]  $.) Then, every $i\in\left\{  1,2,\ldots,k\right\}  $
satisfies%
\begin{align}
\kappa\left(  i\right)   &  =\kappa_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
the definition of }\kappa\right) \nonumber\\
&  =r_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.exe.det.laplace-multi.0.short.a.kappai1})}\right)  .
\label{sol.exe.det.laplace-multi.0.short.a.kappa1}%
\end{align}
Also, every $j\in\left\{  1,2,\ldots,n-k\right\}  $ satisfies%
\begin{align}
\kappa\left(  k+j\right)   &  =\kappa_{k+j}\ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\kappa\right) \nonumber\\
&  =t_{\left(  k+j\right)  -k}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{sol.exe.det.laplace-multi.0.short.a.kappai2}), applied to }i=k+j\right)
\nonumber\\
&  =t_{j}. \label{sol.exe.det.laplace-multi.0.short.a.kappa2}%
\end{align}

\end{vershort}

\begin{verlong}
The $k+\left(  n-k\right)  $ elements $r_{1},r_{2},\ldots,r_{k},t_{1}%
,t_{2},\ldots,t_{n-k}$ belong to $\left[  n\right]  $ (since the $k$ elements
$r_{1},r_{2},\ldots,r_{k}$ belong to $\left[  n\right]  $, and since the $n-k$
elements $t_{1},t_{2},\ldots,t_{n-k}$ belong to $\left[  n\right]  $). In
other words, $\left(  r_{1},r_{2},\ldots,r_{k},t_{1},t_{2},\ldots
,t_{n-k}\right)  \in\left[  n\right]  ^{k+\left(  n-k\right)  }$. Thus,%
\[
\left(  r_{1},r_{2},\ldots,r_{k},t_{1},t_{2},\ldots,t_{n-k}\right)  \in\left[
n\right]  ^{k+\left(  n-k\right)  }=\left[  n\right]  ^{n}%
\]
(since $k+\left(  n-k\right)  =n$). In other words, $\left(  r_{1}%
,r_{2},\ldots,r_{k},t_{1},t_{2},\ldots,t_{n-k}\right)  $ is an $n$-tuple of
elements of $\left[  n\right]  $. Write this $n$-tuple $\left(  r_{1}%
,r_{2},\ldots,r_{k},t_{1},t_{2},\ldots,t_{n-k}\right)  $ in the form $\left(
\kappa_{1},\kappa_{2},\ldots,\kappa_{n}\right)  $. Thus, $\left(  \kappa
_{1},\kappa_{2},\ldots,\kappa_{n}\right)  =\left(  r_{1},r_{2},\ldots
,r_{k},t_{1},t_{2},\ldots,t_{n-k}\right)  $. In other words,%
\begin{equation}
\kappa_{i}=%
\begin{cases}
r_{i}, & \text{if }i\leq k;\\
t_{i-k}, & \text{if }i>k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for every }i\in\left\{  1,2,\ldots,n\right\}  .
\label{sol.exe.det.laplace-multi.0.a.def-kappa}%
\end{equation}
Moreover, $\left(  \kappa_{1},\kappa_{2},\ldots,\kappa_{n}\right)  \in\left[
n\right]  ^{n}$ (since $\left(  \kappa_{1},\kappa_{2},\ldots,\kappa
_{n}\right)  $ is an $n$-tuple of elements of $\left[  n\right]  $). Hence,
$\kappa_{i}\in\left[  n\right]  $ for each $i\in\left\{  1,2,\ldots,n\right\}
$. In other words, $\kappa_{i}\in\left[  n\right]  $ for each $i\in\left[
n\right]  $ (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $).
Thus, we can define a map $\kappa:\left[  n\right]  \rightarrow\left[
n\right]  $ by%
\[
\left(  \kappa\left(  i\right)  =\kappa_{i}\text{ for every }i\in\left[
n\right]  \right)  .
\]
Consider this $\kappa$.

We have%
\begin{equation}
\kappa\left(  i\right)  =r_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,\ldots,k\right\}  .
\label{sol.exe.det.laplace-multi.0.a.kappa1}%
\end{equation}


\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.a.kappa1}):} Let
$i\in\left\{  1,2,\ldots,k\right\}  $. Then, $i\in\left\{  1,2,\ldots
,k\right\}  \subseteq\left\{  1,2,\ldots,n\right\}  $ (since $k\leq n$), so
that $i\in\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $. Hence,
$\kappa\left(  i\right)  =\kappa_{i}$ (by the definition of $\kappa$). Thus,%
\begin{align*}
\kappa\left(  i\right)   &  =\kappa_{i}=%
\begin{cases}
r_{i}, & \text{if }i\leq k;\\
t_{i-k}, & \text{if }i>k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\kappa_{i}\right) \\
&  =r_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\leq k\text{ (since }%
i\in\left\{  1,2,\ldots,k\right\}  \text{)}\right)  .
\end{align*}
This proves (\ref{sol.exe.det.laplace-multi.0.a.kappa1}).

Furthermore,%
\begin{equation}
\kappa\left(  k+i\right)  =t_{i}\ \ \ \ \ \ \ \ \ \ \text{for every }%
i\in\left\{  1,2,\ldots,n-k\right\}  .
\label{sol.exe.det.laplace-multi.0.a.kappa2}%
\end{equation}


\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.a.kappa2}):} Let
$i\in\left\{  1,2,\ldots,n-k\right\}  $. Thus, $i\geq1>0$. But $i\in\left\{
1,2,\ldots,n-k\right\}  $, and thus%
\begin{align*}
k+i  &  \in\left\{  k+1,k+2,\ldots,k+\left(  n-k\right)  \right\}  =\left\{
k+1,k+2,\ldots,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k+\left(
n-k\right)  =n\right) \\
&  \subseteq\left\{  1,2,\ldots,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }k+1\geq1\right) \\
&  =\left[  n\right]  .
\end{align*}
Hence, $\kappa\left(  k+i\right)  =\kappa_{k+i}$ (by the definition of
$\kappa$). Hence,%
\begin{align*}
\kappa\left(  k+i\right)   &  =\kappa_{k+i}=%
\begin{cases}
r_{k+i}, & \text{if }k+i\leq k;\\
t_{\left(  k+i\right)  -k}, & \text{if }k+i>k
\end{cases}
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\kappa_{k+i}\right) \\
&  =t_{\left(  k+i\right)  -k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since
}k+\underbrace{i}_{>0}>k\right) \\
&  =t_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left(  k+i\right)
-k=i\right)  .
\end{align*}
This proves (\ref{sol.exe.det.laplace-multi.0.a.kappa2}).
\end{verlong}

\begin{vershort}
We have%
\begin{equation}
w\left(  R\right)  =\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)
,\ldots,\kappa\left(  k\right)  \right)
\label{sol.exe.det.laplace-multi.0.short.a.wR=}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.short.a.wR=}):}
The lists $w\left(  R\right)  $ and $\left(  r_{1},r_{2},\ldots,r_{k}\right)
$ must be identical (since each of them is the list of all elements of $R$ in
increasing order (with no repetitions)). In other words, $w\left(  R\right)
=\left(  r_{1},r_{2},\ldots,r_{k}\right)  $. Now,
(\ref{sol.exe.det.laplace-multi.0.short.a.kappa1}) shows that $\kappa\left(
i\right)  =r_{i}$ for every $i\in\left\{  1,2,\ldots,k\right\}  $. In other
words, $\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)
,\ldots,\kappa\left(  k\right)  \right)  =\left(  r_{1},r_{2},\ldots
,r_{k}\right)  $. Comparing this with $w\left(  R\right)  =\left(  r_{1}%
,r_{2},\ldots,r_{k}\right)  $, we obtain $w\left(  R\right)  =\left(
\kappa\left(  1\right)  ,\kappa\left(  2\right)  ,\ldots,\kappa\left(
k\right)  \right)  $. This proves
(\ref{sol.exe.det.laplace-multi.0.short.a.wR=}).} and%
\begin{equation}
w\left(  \widetilde{P}\right)  =\left(  \kappa\left(  k+1\right)
,\kappa\left(  k+2\right)  ,\ldots,\kappa\left(  n\right)  \right)
\label{sol.exe.det.laplace-multi.0.short.a.wPs=}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.short.a.wPs=}):}
The lists $w\left(  \widetilde{P}\right)  $ and $\left(  t_{1},t_{2}%
,\ldots,t_{n-k}\right)  $ must be identical (since each of them is the list of
all elements of $\widetilde{P}$ in increasing order (with no repetitions)). In
other words, $w\left(  \widetilde{P}\right)  =\left(  t_{1},t_{2}%
,\ldots,t_{n-k}\right)  $. Now,
(\ref{sol.exe.det.laplace-multi.0.short.a.kappa2}) shows that $\kappa\left(
k+i\right)  =t_{i}$ for every $i\in\left\{  1,2,\ldots,n-k\right\}  $. In
other words, $\left(  \kappa\left(  k+1\right)  ,\kappa\left(  k+2\right)
,\ldots,\kappa\left(  n\right)  \right)  =\left(  t_{1},t_{2},\ldots
,t_{n-k}\right)  $. Comparing this with $w\left(  \widetilde{P}\right)
=\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $, we obtain $w\left(
\widetilde{P}\right)  =\left(  \kappa\left(  k+1\right)  ,\kappa\left(
k+2\right)  ,\ldots,\kappa\left(  n\right)  \right)  $. This proves
(\ref{sol.exe.det.laplace-multi.0.short.a.wPs=}).}.
\end{vershort}

\begin{verlong}
We have%
\begin{equation}
w\left(  R\right)  =\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)
,\ldots,\kappa\left(  k\right)  \right)
\label{sol.exe.det.laplace-multi.0.a.wR=}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.a.wR=}):} Recall
that $w\left(  R\right)  $ is the list of all elements of $R$ in increasing
order (with no repetitions) (by the definition of $w\left(  R\right)  $).
Thus,%
\begin{align}
w\left(  R\right)   &  =\left(  \text{the list of all elements of }R\text{ in
increasing order (with no repetitions)}\right) \nonumber\\
&  =\left(  r_{1},r_{2},\ldots,r_{k}\right)
\label{sol.exe.det.laplace-multi.0.a.wR=.pf.1}%
\end{align}
(since $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ is the list of all elements
of $R$ in increasing order (with no repetitions)). Now,
(\ref{sol.exe.det.laplace-multi.0.a.kappa1}) shows that $\kappa\left(
i\right)  =r_{i}$ for every $i\in\left\{  1,2,\ldots,k\right\}  $. In other
words, $\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)
,\ldots,\kappa\left(  k\right)  \right)  =\left(  r_{1},r_{2},\ldots
,r_{k}\right)  $. Comparing this with
(\ref{sol.exe.det.laplace-multi.0.a.wR=.pf.1}), we obtain $w\left(  R\right)
=\left(  \kappa\left(  1\right)  ,\kappa\left(  2\right)  ,\ldots
,\kappa\left(  k\right)  \right)  $. This proves
(\ref{sol.exe.det.laplace-multi.0.a.wR=}).} and%
\begin{equation}
w\left(  \widetilde{P}\right)  =\left(  \kappa\left(  k+1\right)
,\kappa\left(  k+2\right)  ,\ldots,\kappa\left(  n\right)  \right)
\label{sol.exe.det.laplace-multi.0.a.wPs=}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.a.wPs=}):} Recall
that $w\left(  \widetilde{P}\right)  $ is the list of all elements of
$\widetilde{P}$ in increasing order (with no repetitions) (by the definition
of $w\left(  \widetilde{P}\right)  $). Thus,%
\begin{align}
w\left(  \widetilde{P}\right)   &  =\left(  \text{the list of all elements of
}\widetilde{P}\text{ in increasing order (with no repetitions)}\right)
\nonumber\\
&  =\left(  t_{1},t_{2},\ldots,t_{n-k}\right)
\label{sol.exe.det.laplace-multi.0.a.wPs=.pf.1}%
\end{align}
(since $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ is the list of all
elements of $\widetilde{P}$ in increasing order (with no repetitions)). Now,
(\ref{sol.exe.det.laplace-multi.0.a.kappa2}) shows that $\kappa\left(
k+i\right)  =t_{i}$ for every $i\in\left\{  1,2,\ldots,n-k\right\}  $. In
other words, $\left(  \kappa\left(  k+1\right)  ,\kappa\left(  k+2\right)
,\ldots,\kappa\left(  k+\left(  n-k\right)  \right)  \right)  =\left(
t_{1},t_{2},\ldots,t_{n-k}\right)  $. Comparing this with
(\ref{sol.exe.det.laplace-multi.0.a.wR=.pf.1}), we obtain
\begin{align*}
w\left(  \widetilde{P}\right)   &  =\left(  \kappa\left(  k+1\right)
,\kappa\left(  k+2\right)  ,\ldots,\kappa\left(  k+\left(  n-k\right)
\right)  \right) \\
&  =\left(  \kappa\left(  k+1\right)  ,\kappa\left(  k+2\right)
,\ldots,\kappa\left(  n\right)  \right)  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }k+\left(  n-k\right)  =n\right)  .
\end{align*}
This proves (\ref{sol.exe.det.laplace-multi.0.a.wPs=}).}.
\end{verlong}

\begin{vershort}
Using $\widetilde{P}\cap R\neq\varnothing$, we can easily see that
$\kappa\notin S_{n}$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary (for
the sake of contradiction). Thus, $\kappa\in S_{n}$. Hence, the inverse
permutation $\kappa^{-1}\in S_{n}$ is well-defined.
\par
Recall that $\widetilde{P}\cap R\neq\varnothing$. In other words, there exists
a $\rho\in\widetilde{P}\cap R$. Consider this $\rho$.
\par
Recall that $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ is a list of all
elements of $R$. Hence, $\left\{  r_{1},r_{2},\ldots,r_{k}\right\}  =R$. Now,
$\rho\in\widetilde{P}\cap R\subseteq R=\left\{  r_{1},r_{2},\ldots
,r_{k}\right\}  $. In other words, there exists some $i\in\left\{
1,2,\ldots,k\right\}  $ such that $\rho=r_{i}$. Consider this $i$.
\par
Recall that $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ is a list of all
elements of $\widetilde{P}$. Hence, $\left\{  t_{1},t_{2},\ldots
,t_{n-k}\right\}  =\widetilde{P}$. Now, $\rho\in\widetilde{P}\cap
R\subseteq\widetilde{P}=\left\{  t_{1},t_{2},\ldots,t_{n-k}\right\}  $. In
other words, there exists some $j\in\left\{  1,2,\ldots,n-k\right\}  $ such
that $\rho=t_{j}$. Consider this $j$.
\par
The equality (\ref{sol.exe.det.laplace-multi.0.short.a.kappa2}) (applied to
$j$ instead of $i$) yields $\kappa\left(  k+j\right)  =t_{j}=\rho$ (since
$\rho=t_{j}$). But from (\ref{sol.exe.det.laplace-multi.0.short.a.kappa1}), we
obtan $\kappa\left(  i\right)  =r_{i}=\rho$ (since $\rho=r_{i}$). Comparing
this with $\kappa\left(  k+j\right)  =\rho$, we obtain $\kappa\left(
i\right)  =\kappa\left(  k+j\right)  $. Now, $i=\kappa^{-1}\left(
\underbrace{\kappa\left(  i\right)  }_{=\kappa\left(  k+j\right)  }\right)
=\kappa^{-1}\left(  \kappa\left(  k+j\right)  \right)  =k+\underbrace{j}%
_{\substack{>0\\\text{(since }j\in\left\{  1,2,\ldots,n-k\right\}  \text{)}%
}}>k$. But $i\in\left\{  1,2,\ldots,k\right\}  $ and thus $i\leq k$. This
contradicts $i>k$. This contradiction proves that our assumption was wrong.
Qed.}.
\end{vershort}

\begin{verlong}
Furthermore, $\kappa\notin S_{n}$\ \ \ \ \footnote{\textit{Proof.} Assume the
contrary (for the sake of contradiction). Thus, $\kappa\in S_{n}$.
\par
We have $\kappa\in S_{n}$. In other words, $\kappa$ is a permutation of
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  $). In other words, $\kappa$
is a bijective map $\left\{  1,2,\ldots,n\right\}  \rightarrow\left\{
1,2,\ldots,n\right\}  $. Thus, the map $\kappa$ is bijective, and therefore
injective.
\par
Recall that $\widetilde{P}\cap R\neq\varnothing$. Thus, the set $\widetilde{P}%
\cap R$ is nonempty. In other words, there exists a $\rho\in\widetilde{P}\cap
R$. Consider this $\rho$.
\par
Recall that $\left(  r_{1},r_{2},\ldots,r_{k}\right)  $ is the list of all
elements of $R$ in increasing order (with no repetitions). Thus, $\left(
r_{1},r_{2},\ldots,r_{k}\right)  $ is a list of all elements of $R$. Hence,
$\left\{  r_{1},r_{2},\ldots,r_{k}\right\}  =R$. Now, $\rho\in\widetilde{P}%
\cap R\subseteq R=\left\{  r_{1},r_{2},\ldots,r_{k}\right\}  $. In other
words, there exists some $i\in\left\{  1,2,\ldots,k\right\}  $ such that
$\rho=r_{i}$. Consider this $i$.
\par
Recall that $\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ is the list of all
elements of $\widetilde{P}$ in increasing order (with no repetitions). Thus,
$\left(  t_{1},t_{2},\ldots,t_{n-k}\right)  $ is a list of all elements of
$\widetilde{P}$. Hence, $\left\{  t_{1},t_{2},\ldots,t_{n-k}\right\}
=\widetilde{P}$. Now, $\rho\in\widetilde{P}\cap R\subseteq\widetilde{P}%
=\left\{  t_{1},t_{2},\ldots,t_{n-k}\right\}  $. In other words, there exists
some $j\in\left\{  1,2,\ldots,n-k\right\}  $ such that $\rho=t_{j}$. Consider
this $j$.
\par
We have $j\in\left\{  1,2,\ldots,n-k\right\}  $, so that%
\begin{align*}
k+j  &  \in\left\{  k+1,k+2,\ldots,k+\left(  n-k\right)  \right\}  =\left\{
k+1,k+2,\ldots,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k+\left(
n-k\right)  =n\right) \\
&  \subseteq\left\{  1,2,\ldots,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(
\text{since }k+1\geq1\right) \\
&  =\left[  n\right]  .
\end{align*}
Hence, $\kappa\left(  k+j\right)  $ is well-defined. The equality
(\ref{sol.exe.det.laplace-multi.0.a.kappa2}) (applied to $j$ instead of $i$)
yields $\kappa\left(  k+j\right)  =t_{j}=\rho$ (since $\rho=t_{j}$).
\par
But%
\begin{align*}
i  &  \in\left\{  1,2,\ldots,k\right\}  \subseteq\left\{  1,2,\ldots
,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }k\leq n\right) \\
&  =\left[  n\right]  .
\end{align*}
Hence, $\kappa\left(  i\right)  $ is well-defined. From
(\ref{sol.exe.det.laplace-multi.0.a.kappa1}), we obtan $\kappa\left(
i\right)  =r_{i}=\rho$ (since $\rho=r_{i}$). Comparing this with
$\kappa\left(  k+j\right)  =\rho$, we obtain $\kappa\left(  i\right)
=\kappa\left(  k+j\right)  $.
\par
But the map $\kappa$ is injective. In other words, if $u$ and $v$ are two
elements of $\left[  n\right]  $ satisfying $\kappa\left(  u\right)
=\kappa\left(  v\right)  $, then $u=v$. Applying this to $u=i$ and $v=k+j$, we
obtain $i=k+j$ (since $\kappa\left(  i\right)  =\kappa\left(  k+j\right)  $).
But $j\in\left\{  1,2,\ldots,n-k\right\}  $, so that $j\geq1>0$ and thus
$k+\underbrace{j}_{>0}>k$. Now, $i=k+j>k$. But $i\in\left\{  1,2,\ldots
,k\right\}  $ and thus $i\leq k$. This contradicts $i>k$. This contradiction
proves that our assumption was wrong. Qed.}.
\end{verlong}

Write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Define an $n\times n$-matrix $A_{\kappa}$
by $A_{\kappa}=\left(  a_{\kappa\left(  i\right)  ,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Then, Lemma \ref{lem.det.sigma} \textbf{(b)} (applied to
$A$, $a_{i,j}$ and $A_{\kappa}$ instead of $B$, $b_{i,j}$ and $B_{\kappa}$)
yields
\begin{equation}
\det\left(  A_{\kappa}\right)  =0 \label{sol.exe.det.laplace-multi.0.a.detAk}%
\end{equation}
(since $\kappa\notin S_{n}$).

Now, every subset $Q$ of $\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}A=\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }\left(  A_{\kappa}\right)
\label{sol.exe.det.laplace-multi.0.a.sub1}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.a.sub1}):} Let
$Q$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Write the list $w\left(
Q\right)  $ in the form $w\left(  Q\right)  =\left(  q_{1},q_{2}%
,\ldots,q_{\ell}\right)  $ for some $\ell\in\mathbb{N}$.
\par
From $w\left(  R\right)  =\left(  \kappa\left(  1\right)  ,\kappa\left(
2\right)  ,\ldots,\kappa\left(  k\right)  \right)  $ and $w\left(  Q\right)
=\left(  q_{1},q_{2},\ldots,q_{\ell}\right)  $, we obtain%
\begin{align}
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }A  &
=\operatorname*{sub}\nolimits_{\left(  \kappa\left(  1\right)  ,\kappa\left(
2\right)  ,\ldots,\kappa\left(  k\right)  \right)  }^{\left(  q_{1}%
,q_{2},\ldots,q_{\ell}\right)  }A=\operatorname*{sub}\nolimits_{\kappa\left(
1\right)  ,\kappa\left(  2\right)  ,\ldots,\kappa\left(  k\right)  }%
^{q_{1},q_{2},\ldots,q_{\ell}}A=\left(  a_{\kappa\left(  x\right)  ,q_{y}%
}\right)  _{1\leq x\leq k,\ 1\leq y\leq\ell}%
\label{sol.exe.det.laplace-multi.0.a.sub1.pf.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{\kappa\left(  1\right)  ,\kappa\left(  2\right)  ,\ldots
,\kappa\left(  k\right)  }^{q_{1},q_{2},\ldots,q_{\ell}}A\text{, since
}A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .\nonumber
\end{align}
\par
On the other hand, from $w\left(  Q\right)  =\left(  q_{1},q_{2}%
,\ldots,q_{\ell}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }\left(  A_{\kappa}\right)  =\operatorname*{sub}\nolimits_{\left(
1,2,\ldots,k\right)  }^{\left(  q_{1},q_{2},\ldots,q_{\ell}\right)  }\left(
A_{\kappa}\right)  =\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{q_{1}%
,q_{2},\ldots,q_{\ell}}\left(  A_{\kappa}\right)  =\left(  a_{\kappa\left(
x\right)  ,q_{y}}\right)  _{1\leq x\leq k,\ 1\leq y\leq\ell}%
\]
(by the definition of $\operatorname*{sub}\nolimits_{1,2,\ldots,k}%
^{q_{1},q_{2},\ldots,q_{\ell}}\left(  A_{\kappa}\right)  $, since $A_{\kappa
}=\left(  a_{\kappa\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$). Comparing this with (\ref{sol.exe.det.laplace-multi.0.a.sub1.pf.1}), we
obtain $\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}A=\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)  }^{w\left(
Q\right)  }\left(  A_{\kappa}\right)  $. This proves
(\ref{sol.exe.det.laplace-multi.0.a.sub1}).} and%
\begin{equation}
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }A=\operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots
,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  A_{\kappa}\right)
\label{sol.exe.det.laplace-multi.0.a.sub2}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.exe.det.laplace-multi.0.a.sub2}):} Let
$Q$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Write the list $w\left(
\widetilde{Q}\right)  $ in the form $w\left(  \widetilde{Q}\right)  =\left(
q_{1},q_{2},\ldots,q_{\ell}\right)  $ for some $\ell\in\mathbb{N}$.
\par
From $w\left(  \widetilde{P}\right)  =\left(  \kappa\left(  k+1\right)
,\kappa\left(  k+2\right)  ,\ldots,\kappa\left(  n\right)  \right)  $ and
$w\left(  \widetilde{Q}\right)  =\left(  q_{1},q_{2},\ldots,q_{\ell}\right)
$, we obtain%
\begin{align}
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }A  &  =\operatorname*{sub}\nolimits_{\left(
\kappa\left(  k+1\right)  ,\kappa\left(  k+2\right)  ,\ldots,\kappa\left(
n\right)  \right)  }^{\left(  q_{1},q_{2},\ldots,q_{\ell}\right)
}A=\operatorname*{sub}\nolimits_{\kappa\left(  k+1\right)  ,\kappa\left(
k+2\right)  ,\ldots,\kappa\left(  n\right)  }^{q_{1},q_{2},\ldots,q_{\ell}%
}A=\left(  a_{\kappa\left(  k+x\right)  ,q_{y}}\right)  _{1\leq x\leq
n-k,\ 1\leq y\leq\ell}\label{sol.exe.det.laplace-multi.0.a.sub2.pf.1}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\operatorname*{sub}%
\nolimits_{\kappa\left(  k+1\right)  ,\kappa\left(  k+2\right)  ,\ldots
,\kappa\left(  n\right)  }^{q_{1},q_{2},\ldots,q_{\ell}}A\text{, since
}A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}\right)  .\nonumber
\end{align}
\par
On the other hand, from $w\left(  \widetilde{Q}\right)  =\left(  q_{1}%
,q_{2},\ldots,q_{\ell}\right)  $, we obtain%
\[
\operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{w\left(
\widetilde{Q}\right)  }\left(  A_{\kappa}\right)  =\operatorname*{sub}%
\nolimits_{\left(  k+1,k+2,\ldots,n\right)  }^{\left(  q_{1},q_{2}%
,\ldots,q_{\ell}\right)  }\left(  A_{\kappa}\right)  =\operatorname*{sub}%
\nolimits_{k+1,k+2,\ldots,n}^{q_{1},q_{2},\ldots,q_{\ell}}\left(  A_{\kappa
}\right)  =\left(  a_{\kappa\left(  k+x\right)  ,q_{y}}\right)  _{1\leq x\leq
n-k,\ 1\leq y\leq\ell}%
\]
(by the definition of $\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}%
^{q_{1},q_{2},\ldots,q_{\ell}}\left(  A_{\kappa}\right)  $, since $A_{\kappa
}=\left(  a_{\kappa\left(  i\right)  ,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$). Comparing this with (\ref{sol.exe.det.laplace-multi.0.a.sub2.pf.1}), we
obtain $\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }A=\operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  A_{\kappa
}\right)  $. This proves (\ref{sol.exe.det.laplace-multi.0.a.sub2}).}.

\begin{vershort}
Define a subset $P^{\prime}$ of $\left\{  1,2,\ldots,n\right\}  $ by
$P^{\prime}=\left\{  1,2,\ldots,k\right\}  $. Then, $\left\vert P^{\prime
}\right\vert =k=\left\vert P\right\vert $.
\end{vershort}

\begin{verlong}
Now, we have $k\leq n$ and thus $\left\{  1,2,\ldots,k\right\}  \subseteq
\left\{  1,2,\ldots,n\right\}  $. In other words, $\left\{  1,2,\ldots
,k\right\}  $ is a subset of $\left\{  1,2,\ldots,n\right\}  $. Thus, we can
define a subset $P^{\prime}$ of $\left\{  1,2,\ldots,n\right\}  $ by
$P^{\prime}=\left\{  1,2,\ldots,k\right\}  $. Consider this $P^{\prime}$. From
$P^{\prime}=\left\{  1,2,\ldots,k\right\}  $, we obtain $\left\vert P^{\prime
}\right\vert =\left\vert \left\{  1,2,\ldots,k\right\}  \right\vert
=k=\left\vert P\right\vert $.
\end{verlong}

\begin{vershort}
We have $w\left(  P^{\prime}\right)  =\left(  1,2,\ldots,k\right)
$\ \ \ \ \footnote{\textit{Proof.} The definition of $w\left(  P^{\prime
}\right)  $ yields%
\begin{align*}
w\left(  P^{\prime}\right)   &  =\left(  \text{the list of all elements of
}\underbrace{P^{\prime}}_{=\left\{  1,2,\ldots,k\right\}  }\text{ in
increasing order (with no repetitions)}\right) \\
&  =\left(  \text{the list of all elements of }\left\{  1,2,\ldots,k\right\}
\text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  1,2,\ldots,k\right)  .
\end{align*}
Qed.} and $w\left(  \widetilde{P^{\prime}}\right)  =\left(  k+1,k+2,\ldots
,n\right)  $\ \ \ \ \footnote{\textit{Proof.} We have $P^{\prime}=\left\{
1,2,\ldots,k\right\}  $. Now,
\begin{align*}
\widetilde{P^{\prime}}  &  =\left\{  1,2,\ldots,n\right\}  \setminus
\underbrace{P^{\prime}}_{=\left\{  1,2,\ldots,k\right\}  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widetilde{P^{\prime}%
}\right) \\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left\{  1,2,\ldots,k\right\}
=\left\{  k+1,k+2,\ldots,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}k\in\left\{  0,1,\ldots,n\right\}  \right)  .
\end{align*}
\par
Now, the definition of $w\left(  \widetilde{P^{\prime}}\right)  $ yields%
\begin{align*}
&  w\left(  \widetilde{P^{\prime}}\right) \\
&  =\left(  \text{the list of all elements of }%
\underbrace{\widetilde{P^{\prime}}}_{=\left\{  k+1,k+2,\ldots,n\right\}
}\text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  \text{the list of all elements of }\left\{  k+1,k+2,\ldots
,n\right\}  \text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  k+1,k+2,\ldots,n\right)  .
\end{align*}
Qed.}.
\end{vershort}

\begin{verlong}
We have $w\left(  P^{\prime}\right)  =\left(  1,2,\ldots,k\right)
$\ \ \ \ \footnote{\textit{Proof.} Recall that $w\left(  P^{\prime}\right)  $
is the list of all elements of $P^{\prime}$ in increasing order (with no
repetitions) (by the definition of $w\left(  P^{\prime}\right)  $). Thus,%
\begin{align*}
w\left(  P^{\prime}\right)   &  =\left(  \text{the list of all elements of
}\underbrace{P^{\prime}}_{=\left\{  1,2,\ldots,k\right\}  }\text{ in
increasing order (with no repetitions)}\right) \\
&  =\left(  \text{the list of all elements of }\left\{  1,2,\ldots,k\right\}
\text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  1,2,\ldots,k\right)  .
\end{align*}
Qed.} and $w\left(  \widetilde{P^{\prime}}\right)  =\left(  k+1,k+2,\ldots
,n\right)  $\ \ \ \ \footnote{\textit{Proof.} We have $P^{\prime}=\left\{
1,2,\ldots,k\right\}  $. Now,
\begin{align*}
\widetilde{P^{\prime}}  &  =\left\{  1,2,\ldots,n\right\}  \setminus
\underbrace{P^{\prime}}_{=\left\{  1,2,\ldots,k\right\}  }%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by the definition of }\widetilde{P^{\prime}%
}\right) \\
&  =\left\{  1,2,\ldots,n\right\}  \setminus\left\{  1,2,\ldots,k\right\}
=\left\{  k+1,k+2,\ldots,n\right\}  \ \ \ \ \ \ \ \ \ \ \left(  \text{since
}k\in\left\{  0,1,\ldots,n\right\}  \right)  .
\end{align*}
\par
Recall that $w\left(  \widetilde{P^{\prime}}\right)  $ is the list of all
elements of $\widetilde{P^{\prime}}$ in increasing order (with no repetitions)
(by the definition of $w\left(  \widetilde{P^{\prime}}\right)  $). Thus,%
\begin{align*}
&  w\left(  \widetilde{P^{\prime}}\right) \\
&  =\left(  \text{the list of all elements of }%
\underbrace{\widetilde{P^{\prime}}}_{=\left\{  k+1,k+2,\ldots,n\right\}
}\text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  \text{the list of all elements of }\left\{  k+1,k+2,\ldots
,n\right\}  \text{ in increasing order (with no repetitions)}\right) \\
&  =\left(  k+1,k+2,\ldots,n\right)  .
\end{align*}
Qed.}.
\end{verlong}

Now, Theorem \ref{thm.det.laplace-multi} \textbf{(a)} (applied to $A_{\kappa}$
and $P^{\prime}$ instead of $A$ and $P$) yields%
\begin{align*}
&  \det\left(  A_{\kappa}\right) \\
&  =\underbrace{\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert Q\right\vert =\left\vert P^{\prime}\right\vert }}}%
_{\substack{=\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert Q\right\vert =\left\vert P\right\vert }}\\\text{(since
}\left\vert P^{\prime}\right\vert =\left\vert P\right\vert \text{)}}}\left(
-1\right)  ^{\sum P^{\prime}+\sum Q}\det\left(
\underbrace{\operatorname*{sub}\nolimits_{w\left(  P^{\prime}\right)
}^{w\left(  Q\right)  }\left(  A_{\kappa}\right)  }%
_{\substack{=\operatorname*{sub}\nolimits_{\left(  1,2,\ldots,k\right)
}^{w\left(  Q\right)  }\left(  A_{\kappa}\right)  \\\text{(since }w\left(
P^{\prime}\right)  =\left(  1,2,\ldots,k\right)  \text{)}}}\right)
\det\left(  \underbrace{\operatorname*{sub}\nolimits_{w\left(
\widetilde{P^{\prime}}\right)  }^{w\left(  \widetilde{Q}\right)  }\left(
A_{\kappa}\right)  }_{\substack{=\operatorname*{sub}\nolimits_{\left(
k+1,k+2,\ldots,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  A_{\kappa
}\right)  \\\text{(since }w\left(  \widetilde{P^{\prime}}\right)  =\left(
k+1,k+2,\ldots,n\right)  \text{)}}}\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P^{\prime
}+\sum Q}\det\left(  \underbrace{\operatorname*{sub}\nolimits_{\left(
1,2,\ldots,k\right)  }^{w\left(  Q\right)  }\left(  A_{\kappa}\right)
}_{\substack{=\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }A\\\text{(by (\ref{sol.exe.det.laplace-multi.0.a.sub1}))}}}\right)
\det\left(  \underbrace{\operatorname*{sub}\nolimits_{\left(  k+1,k+2,\ldots
,n\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  A_{\kappa}\right)
}_{\substack{=\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }A\\\text{(by
(\ref{sol.exe.det.laplace-multi.0.a.sub2}))}}}\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P^{\prime
}+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}A\right)  .
\end{align*}
Comparing this with (\ref{sol.exe.det.laplace-multi.0.a.detAk}), we obtain%
\[
0=\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P^{\prime
}+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}A\right)  .
\]
Multiplying both sides of this equality by $\left(  -1\right)  ^{\sum P-\sum
P^{\prime}}$, we obtain%
\begin{align*}
0  &  =\left(  -1\right)  ^{\sum P-\sum P^{\prime}}\cdot\sum
_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P^{\prime
}+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}A\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\underbrace{\left(  -1\right)  ^{\sum
P-\sum P^{\prime}}\left(  -1\right)  ^{\sum P^{\prime}+\sum Q}}%
_{\substack{=\left(  -1\right)  ^{\left(  \sum P-\sum P^{\prime}\right)
+\left(  \sum P^{\prime}+\sum Q\right)  }\\=\left(  -1\right)  ^{\sum P+\sum
Q}\\\text{(since }\left(  \sum P-\sum P^{\prime}\right)  +\left(  \sum
P^{\prime}+\sum Q\right)  =\sum P+\sum Q\text{)}}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }A\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)  .
\end{align*}
This solves Exercise \ref{exe.det.laplace-multi.0} \textbf{(a)}.

\textbf{(b)} Let $P$ be a subset of $\left\{  1,2,\ldots,n\right\}  $
satisfying $\left\vert P\right\vert =\left\vert R\right\vert $ and $P\neq R$.
Every subset $Q$ of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
Q\right\vert =\left\vert P\right\vert $ satisfies%
\begin{align}
&  \det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }\left(  A^{T}\right)  \right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}\left(  A^{T}\right)  \right) \nonumber\\
&  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  Q\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)
\label{sol.det.laplace-multi.0.b.1}%
\end{align}
\footnote{\textit{Proof of (\ref{sol.det.laplace-multi.0.b.1}):} Let $Q$ be a
subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
Q\right\vert =\left\vert P\right\vert $. Then, $\left\vert Q\right\vert
=\left\vert P\right\vert =\left\vert R\right\vert $. Thus, $\left\vert
R\right\vert =\left\vert Q\right\vert $. Also, $\left\vert P\right\vert
=\left\vert Q\right\vert $ (since $\left\vert Q\right\vert =\left\vert
P\right\vert $). Hence, (\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.1})
(applied to $U=R$ and $V=Q$) yields
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }\left(  A^{T}\right)  \right)  =\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  Q\right)  }^{w\left(  R\right)  }A\right)  .
\label{sol.det.laplace-multi.0.b.1.pf.1}%
\end{equation}
Also, (\ref{eq.cor.sol.det.laplace-multi.0.detsubAT.2}) (applied to $U=P$ and
$V=Q$) yields
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }\left(  A^{T}\right)  \right)  =\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(
\widetilde{P}\right)  }A\right)  . \label{sol.det.laplace-multi.0.b.1.pf.2}%
\end{equation}
Multiplying the equality (\ref{sol.det.laplace-multi.0.b.1.pf.1}) with the
equality (\ref{sol.det.laplace-multi.0.b.1.pf.2}), we obtain%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }\left(  A^{T}\right)  \right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}\left(  A^{T}\right)  \right)  =\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  Q\right)  }^{w\left(  R\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(
\widetilde{P}\right)  }A\right)  .
\]
Thus, (\ref{sol.det.laplace-multi.0.b.1}) is proven.}.

Exercise \ref{exe.det.laplace-multi.0} \textbf{(a)} (applied to $A^{T}$
instead of $A$) yields%
\begin{align}
0  &  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\underbrace{\left(  -1\right)  ^{\sum
P+\sum Q}}_{\substack{=\left(  -1\right)  ^{\sum Q+\sum P}\\\text{(since }\sum
P+\sum Q=\sum Q+\sum P\text{)}}}\underbrace{\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }\left(  A^{T}\right)
\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }\left(  A^{T}\right)  \right)
}_{\substack{=\det\left(  \operatorname*{sub}\nolimits_{w\left(  Q\right)
}^{w\left(  R\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \\\text{(by (\ref{sol.det.laplace-multi.0.b.1}))}}}\nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum Q+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  Q\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)  }A\right) \nonumber\\
&  =\sum_{\substack{G\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
G\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum G+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  G\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{G}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)
\label{sol.det.laplace-multi.0.b.5}%
\end{align}
(here, we have renamed the summation index $Q$ as $G$).

Now, forget that we fixed $P$. We thus have proven that
(\ref{sol.det.laplace-multi.0.b.5}) holds for every subset $P$ of $\left\{
1,2,\ldots,n\right\}  $ satisfying $\left\vert P\right\vert =\left\vert
R\right\vert $ and $P\neq R$.

Now, let $Q$ be a subset $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert =\left\vert R\right\vert $ and $Q\neq R$. Then, we
can apply (\ref{sol.det.laplace-multi.0.b.5}) to $P=Q$. We thus obtain%
\begin{align*}
0  &  =\sum_{\substack{G\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
G\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum G+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  G\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{G}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right) \\
&  =\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }A\right)
\end{align*}
(here, we have renamed the summation index $G$ as $P$). This solves Exercise
\ref{exe.det.laplace-multi.0} \textbf{(b)}.
\end{proof}

\subsection{Solution to Exercise \ref{exe.det(A+B)}}

\begin{vershort}
\begin{proof}
[Proof of Theorem \ref{thm.det(A+B)}.]Write the $n\times n$-matrix $A$ in the
form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Write the
$n\times n$-matrix $B$ in the form $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$.

Let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.
Then, every subset $I$ of $\left[  n\right]  $ satisfies
\begin{equation}
\left[  n\right]  \setminus I=\widetilde{I} \label{sol.det(A+B).short.compl}%
\end{equation}
(because the definition of $\widetilde{I}$ yields $\widetilde{I}%
=\underbrace{\left\{  1,2,\ldots,n\right\}  }_{=\left[  n\right]  }\setminus
I=\left[  n\right]  \setminus I$).

For every $\sigma\in S_{n}$ and every subset $P$ of $\left\{  1,2,\ldots
,n\right\}  $, define an element $c_{\sigma,P}$ of $\mathbb{K}$ by
\begin{equation}
c_{\sigma,P}=\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma
\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma
\left(  i\right)  }\right)  . \label{sol.det(A+B).short.c=}%
\end{equation}


If $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert \neq\left\vert P\right\vert $, then%
\begin{equation}
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}=0
\label{sol.det(A+B).short.emptysum}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det(A+B).short.emptysum}):} This can be
shown in precisely the same way as (\ref{pf.thm.det.laplace-multi.emptysum})
was shown in our proof of Theorem \ref{thm.det.laplace-multi}.}.

If $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert =\left\vert P\right\vert $, then%
\begin{equation}
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma
,P}=\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  \label{sol.det(A+B).short.nonemptysum}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det(A+B).short.nonemptysum}):} Let $P$
and $Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert =\left\vert P\right\vert $. From $\left\vert
Q\right\vert =\left\vert P\right\vert $, we obtain $\left\vert P\right\vert
=\left\vert Q\right\vert $. Lemma \ref{lem.det.laplace-multi.Apq} thus yields%
\[
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right)  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  .
\]
But this is precisely the equality (\ref{sol.det(A+B).short.nonemptysum})
(because of (\ref{sol.det(A+B).short.c=})). Thus,
(\ref{sol.det(A+B).short.nonemptysum}) is proven.}.

Adding the equalities $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, we
obtain $A+B=\left(  a_{i,j}+b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.
Thus, (\ref{eq.det.eq.2}) (applied to $A+B$ and $a_{i,j}+b_{i,j}$ instead of
$A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  A+B\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}\left(  a_{i,\sigma\left(  i\right)
}+b_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=\sum_{I\subseteq\left[
n\right]  }\left(  \prod_{i\in I}a_{i,\sigma\left(  i\right)  }\right)
\left(  \prod_{i\in\left[  n\right]  \setminus I}b_{i,\sigma\left(  i\right)
}\right)  \\\text{(by Exercise \ref{exe.prod(ai+bi)} \textbf{(a)}%
,}\\\text{applied to }a_{i,\sigma\left(  i\right)  }\text{ and }%
b_{i,\sigma\left(  i\right)  }\text{ instead of }a_{i}\text{ and }%
b_{i}\text{)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\sum
_{I\subseteq\left[  n\right]  }}_{=\sum_{I\subseteq\left\{  1,2,\ldots
,n\right\}  }}\left(  \prod_{i\in I}a_{i,\sigma\left(  i\right)  }\right)
\left(  \underbrace{\prod_{i\in\left[  n\right]  \setminus I}}%
_{\substack{=\prod_{i\in\widetilde{I}}\\\text{(by
(\ref{sol.det(A+B).short.compl}))}}}b_{i,\sigma\left(  i\right)  }\right)
\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\sum
_{I\subseteq\left\{  1,2,\ldots,n\right\}  }\left(  \prod_{i\in I}%
a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{I}%
}b_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=\sum_{P\subseteq\left\{
1,2,\ldots,n\right\}  }\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right)  \\\text{(here, we have renamed the summation index }I\text{ as
}P\text{)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{P\subseteq\left\{
1,2,\ldots,n\right\}  }\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right) \nonumber\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\sigma\in S_{n}%
}\underbrace{\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma
\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma
\left(  i\right)  }\right)  }_{\substack{=c_{\sigma,P}\\\text{(by
(\ref{sol.det(A+B).short.c=}))}}}\nonumber\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\sigma\in S_{n}%
}c_{\sigma,P}. \label{sol.det(A+B).short.1}%
\end{align}


But every subset $P$ of $\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align*}
&  \underbrace{\sum_{\sigma\in S_{n}}}_{\substack{=\sum_{Q\subseteq\left\{
1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
P\right)  =Q}}\\\text{(because for every }\sigma\in S_{n}\text{, the
set}\\\sigma\left(  P\right)  \text{ is a subset of }\left\{  1,2,\ldots
,n\right\}  \text{)}}}c_{\sigma,P}\\
&  =\sum_{Q\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}=\sum_{\substack{Q\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert =\left\vert
P\right\vert }}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}c_{\sigma,P}+\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert Q\right\vert \neq\left\vert P\right\vert }}\underbrace{\sum
_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}%
}_{\substack{=0\\\text{(by (\ref{sol.det(A+B).short.emptysum}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every subset }Q\text{ of }\left\{  1,2,\ldots,n\right\}  \text{
satisfies}\\
\text{either }\left\vert Q\right\vert =\left\vert P\right\vert \text{ or
}\left\vert Q\right\vert \neq\left\vert P\right\vert \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}+\underbrace{\sum
_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert \neq\left\vert P\right\vert }}0}_{=0}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}}_{\substack{=\left(
-1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}B\right)  \\\text{(by (\ref{sol.det(A+B).short.nonemptysum}))}}}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
Hence, (\ref{sol.det(A+B).short.1}) becomes%
\begin{align*}
\det\left(  A+B\right)   &  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}
}\underbrace{\sum_{\sigma\in S_{n}}c_{\sigma,P}}_{=\sum_{\substack{Q\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert =\left\vert
P\right\vert }}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  }\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{Q\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert =\left\vert
P\right\vert }}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves Theorem \ref{thm.det(A+B)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Theorem \ref{thm.det(A+B)}.]Write the $n\times n$-matrix $A$ in the
form $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

Write the $n\times n$-matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$.

Let $\left[  n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.
Then,
\begin{equation}
\left[  n\right]  \setminus I=\widetilde{I}\ \ \ \ \ \ \ \ \ \ \text{for every
subset }I\text{ of }\left[  n\right]  \label{sol.det(A+B).compl}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det(A+B).compl}):} Let $I$ be a subset of
$\left[  n\right]  $. Then, $I\subseteq\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Now, the definition of $\widetilde{I}$ yields
$\widetilde{I}=\underbrace{\left\{  1,2,\ldots,n\right\}  }_{=\left[
n\right]  }\setminus I=\left[  n\right]  \setminus I$. This proves
(\ref{sol.det(A+B).compl}).}.

For every $\sigma\in S_{n}$ and every subset $P$ of $\left\{  1,2,\ldots
,n\right\}  $, define an element $c_{\sigma,P}$ of $\mathbb{K}$ by
\begin{equation}
c_{\sigma,P}=\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma
\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma
\left(  i\right)  }\right)  . \label{sol.det(A+B).c=}%
\end{equation}


If $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert \neq\left\vert P\right\vert $, then%
\begin{equation}
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}=0
\label{sol.det(A+B).emptysum}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det(A+B).emptysum}):} Let $P$ and $Q$ be
two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
Q\right\vert \neq\left\vert P\right\vert $.
\par
Let $\sigma\in S_{n}$ be such that $\sigma\left(  P\right)  =Q$. We shall
derive a contradiction.
\par
Indeed, Lemma \ref{lem.sol.det.laplace-multi.4} \textbf{(a)} shows that the
set $\sigma\left(  P\right)  $ is a subset of $\left\{  1,2,\ldots,n\right\}
$ satisfying $\left\vert \sigma\left(  P\right)  \right\vert =\left\vert
P\right\vert $. Hence, $\left\vert P\right\vert =\left\vert \underbrace{\sigma
\left(  P\right)  }_{=Q}\right\vert =\left\vert Q\right\vert \neq\left\vert
P\right\vert $. This is absurd. Hence, we have found a contradiction.
\par
Now, forget that we fixed $\sigma$. We thus have found a contradiction for
every $\sigma\in S_{n}$ satisfying $\sigma\left(  P\right)  =Q$. Thus, there
exists no $\sigma\in S_{n}$ satisfying $\sigma\left(  P\right)  =Q$. Hence,
the sum $\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)
=Q}}c_{\sigma,P}$ is an empty sum. Thus, $\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}=\left(  \text{empty
sum}\right)  =0$. This proves (\ref{sol.det(A+B).emptysum}).}.

If $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert Q\right\vert =\left\vert P\right\vert $, then%
\begin{equation}
\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma
,P}=\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  \label{sol.det(A+B).nonemptysum}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.det(A+B).nonemptysum}):} Let $P$ and $Q$
be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
Q\right\vert =\left\vert P\right\vert $. From $\left\vert Q\right\vert
=\left\vert P\right\vert $, we obtain $\left\vert P\right\vert =\left\vert
Q\right\vert $. Lemma \ref{lem.det.laplace-multi.Apq} thus yields%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  .
\end{align*}
Now,%
\begin{align*}
&  \sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}%
}\underbrace{c_{\sigma,P}}_{=\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  }\\
&  =\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  P\right)  =Q}}\left(
-1\right)  ^{\sigma}\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(
\widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves (\ref{sol.det(A+B).nonemptysum}).}.

Adding the equalities $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$ and $B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$, we
obtain%
\[
A+B=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}+\left(
b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(  a_{i,j}%
+b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(by the definition of the sum of two matrices). Thus, (\ref{eq.det.eq.2})
(applied to $A+B$ and $a_{i,j}+b_{i,j}$ instead of $A$ and $a_{i,j}$) yields%
\begin{align}
\det\left(  A+B\right)   &  =\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\underbrace{\prod_{i=1}^{n}\left(  a_{i,\sigma\left(  i\right)
}+b_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=\sum_{I\subseteq\left[
n\right]  }\left(  \prod_{i\in I}a_{i,\sigma\left(  i\right)  }\right)
\left(  \prod_{i\in\left[  n\right]  \setminus I}b_{i,\sigma\left(  i\right)
}\right)  \\\text{(by Exercise \ref{exe.prod(ai+bi)} \textbf{(a)}%
}\\\text{(applied to }a_{i,\sigma\left(  i\right)  }\text{ and }%
b_{i,\sigma\left(  i\right)  }\text{ instead of }a_{i}\text{ and }%
b_{i}\text{))}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\sum
_{I\subseteq\left[  n\right]  }}_{\substack{=\sum_{I\subseteq\left\{
1,2,\ldots,n\right\}  }\\\text{(since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \text{)}}}\left(  \prod_{i\in I}a_{i,\sigma\left(
i\right)  }\right)  \left(  \underbrace{\prod_{i\in\left[  n\right]  \setminus
I}}_{\substack{=\prod_{i\in\widetilde{I}}\\\text{(since }\left[  n\right]
\setminus I=\widetilde{I}\\\text{(by (\ref{sol.det(A+B).compl})))}%
}}b_{i,\sigma\left(  i\right)  }\right) \nonumber\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\sum
_{I\subseteq\left\{  1,2,\ldots,n\right\}  }\left(  \prod_{i\in I}%
a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{I}%
}b_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=\sum_{P\subseteq\left\{
1,2,\ldots,n\right\}  }\left(  \prod_{i\in P}a_{i,\sigma\left(  i\right)
}\right)  \left(  \prod_{i\in\widetilde{P}}b_{i,\sigma\left(  i\right)
}\right)  \\\text{(here, we have renamed the summation index }I\text{ as
}P\text{)}}}\nonumber\\
&  =\sum_{\sigma\in S_{n}}\underbrace{\left(  -1\right)  ^{\sigma}%
\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  }_{=\sum_{P\subseteq\left\{
1,2,\ldots,n\right\}  }\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  }\nonumber\\
&  =\underbrace{\sum_{\sigma\in S_{n}}\sum_{P\subseteq\left\{  1,2,\ldots
,n\right\}  }}_{=\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum
_{\sigma\in S_{n}}}\underbrace{\left(  -1\right)  ^{\sigma}\left(  \prod_{i\in
P}a_{i,\sigma\left(  i\right)  }\right)  \left(  \prod_{i\in\widetilde{P}%
}b_{i,\sigma\left(  i\right)  }\right)  }_{\substack{=c_{\sigma,P}\\\text{(by
(\ref{sol.det(A+B).c=}))}}}\nonumber\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\sigma\in S_{n}%
}c_{\sigma,P}. \label{sol.det(A+B).1}%
\end{align}


But every subset $P$ of $\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align*}
&  \underbrace{\sum_{\sigma\in S_{n}}}_{\substack{=\sum_{Q\subseteq\left\{
1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
P\right)  =Q}}\\\text{(because for every }\sigma\in S_{n}\text{, the
set}\\\sigma\left(  P\right)  \text{ is a subset of }\left\{  1,2,\ldots
,n\right\}  \text{)}}}c_{\sigma,P}\\
&  =\sum_{Q\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}+\sum_{\substack{Q\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert \neq\left\vert
P\right\vert }}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(
P\right)  =Q}}c_{\sigma,P}}_{\substack{=0\\\text{(by
(\ref{sol.det(A+B).emptysum}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every subset }Q\text{ of }\left\{  1,2,\ldots,n\right\}  \text{
satisfies}\\
\text{either }\left\vert Q\right\vert =\left\vert P\right\vert \text{ or
}\left\vert Q\right\vert \neq\left\vert P\right\vert \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}+\underbrace{\sum
_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert \neq\left\vert P\right\vert }}0}_{=0}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\underbrace{\sum_{\substack{\sigma\in
S_{n};\\\sigma\left(  P\right)  =Q}}c_{\sigma,P}}_{\substack{=\left(
-1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  Q\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}B\right)  \\\text{(by (\ref{sol.det(A+B).nonemptysum}))}}}\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
Q\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
Hence, (\ref{sol.det(A+B).1}) becomes%
\begin{align*}
\det\left(  A+B\right)   &  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}
}\underbrace{\sum_{\sigma\in S_{n}}c_{\sigma,P}}_{=\sum_{\substack{Q\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert =\left\vert
P\right\vert }}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  }\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\sum_{\substack{Q\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert Q\right\vert =\left\vert
P\right\vert }}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}%
\right)  }^{w\left(  \widetilde{Q}\right)  }B\right)  .
\end{align*}
This proves Theorem \ref{thm.det(A+B)}.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Exercise \ref{exe.det(A+B)}.]Exercise \ref{exe.det(A+B)} is
solved, since Theorem \ref{thm.det(A+B)} is proven.
\end{proof}

\subsection{Solution to Exercise \ref{exe.det(A+B).diag}}

\begin{proof}
[Proof of Lemma \ref{lem.diag.minors}.]The set $P$ is a subset of $\left\{
1,2,\ldots,n\right\}  $, and thus is finite (since $\left\{  1,2,\ldots
,n\right\}  $ is finite). Hence, $\left\vert P\right\vert \in\mathbb{N}$.

Define an element $k\in\mathbb{N}$ by $k=\left\vert P\right\vert $. Thus,
$k=\left\vert P\right\vert =\left\vert Q\right\vert $.

\begin{vershort}
The list $w\left(  P\right)  $ is the list of all elements of $P$ in
increasing order (with no repetitions), and thus has $k$ entries (since
$\left\vert P\right\vert =k$). Thus, write this list $w\left(  P\right)  $ in
the form $w\left(  P\right)  =\left(  p_{1},p_{2},\ldots,p_{k}\right)  $.
Similarly, write the list $w\left(  Q\right)  $ in the form $w\left(
Q\right)  =\left(  q_{1},q_{2},\ldots,q_{k}\right)  $.
\end{vershort}

\begin{verlong}
We know that $w\left(  P\right)  $ is the list of all elements of $P$ in
increasing order (with no repetitions) (by the definition of $w\left(
P\right)  $). Thus, $w\left(  P\right)  $ is a list of $\left\vert
P\right\vert $ elements. In other words, $w\left(  P\right)  $ is a list of
$k$ elements (since $\left\vert P\right\vert =k$).

Write $w\left(  P\right)  $ in the form $w\left(  P\right)  =\left(
p_{1},p_{2},\ldots,p_{k}\right)  $. (This is possible, since $w\left(
P\right)  $ is a list of $k$ elements.)

We know that $w\left(  Q\right)  $ is the list of all elements of $Q$ in
increasing order (with no repetitions) (by the definition of $w\left(
Q\right)  $). Thus, $w\left(  P\right)  $ is a list of $\left\vert
Q\right\vert $ elements. In other words, $w\left(  Q\right)  $ is a list of
$k$ elements (since $\left\vert Q\right\vert =k$).

Write $w\left(  Q\right)  $ in the form $w\left(  Q\right)  =\left(
q_{1},q_{2},\ldots,q_{k}\right)  $. (This is possible, since $w\left(
Q\right)  $ is a list of $k$ elements.)
\end{verlong}

From $w\left(  P\right)  =\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ and
$w\left(  Q\right)  =\left(  q_{1},q_{2},\ldots,q_{k}\right)  $, we obtain%
\begin{equation}
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}D=\operatorname*{sub}\nolimits_{\left(  p_{1},p_{2},\ldots,p_{k}\right)
}^{\left(  q_{1},q_{2},\ldots,q_{k}\right)  }D=\operatorname*{sub}%
\nolimits_{p_{1},p_{2},\ldots,p_{k}}^{q_{1},q_{2},\ldots,q_{k}}D=\left(
d_{p_{x}}\delta_{p_{x},q_{y}}\right)  _{1\leq x\leq k,\ 1\leq y\leq k}
\label{pf.lem.diag.minors.sub=}%
\end{equation}
(by the definition of $\operatorname*{sub}\nolimits_{p_{1},p_{2},\ldots,p_{k}%
}^{q_{1},q_{2},\ldots,q_{k}}D$, since $D=\left(  d_{i}\delta_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$). Thus, in particular, we see that
$\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$ is
a $k\times k$-matrix.

\begin{vershort}
Recall that $\left(  p_{1},p_{2},\ldots,p_{k}\right)  =w\left(  P\right)  $ is
a list of all elements of $P$ (with no repetitions). Hence, $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ is a list with no repetitions, and
satisfies $\left\{  p_{1},p_{2},\ldots,p_{k}\right\}  =P$. Now, every $\left(
x,y\right)  \in\left\{  1,2,\ldots,k\right\}  ^{2}$ satisfies%
\begin{equation}
\delta_{p_{x},p_{y}}=\delta_{x,y} \label{pf.lem.diag.minors.short.deldel}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.diag.minors.short.deldel}):} Let
$\left(  x,y\right)  \in\left\{  1,2,\ldots,k\right\}  ^{2}$. Then, the
integers $p_{1},p_{2},\ldots,p_{k}$ are pairwise distinct (since $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ is a list with no repetitions). Hence,
$p_{x}=p_{y}$ holds if and only if $x=y$. Thus, $%
\begin{cases}
1, & \text{if }p_{x}=p_{y};\\
0, & \text{if }p_{x}\neq p_{y}%
\end{cases}
=%
\begin{cases}
1, & \text{if }x=y;\\
0, & \text{if }x\neq y
\end{cases}
$. In view of $\delta_{p_{x},p_{y}}=%
\begin{cases}
1, & \text{if }p_{x}=p_{y};\\
0, & \text{if }p_{x}\neq p_{y}%
\end{cases}
$ and $\delta_{x,y}=%
\begin{cases}
1, & \text{if }x=y;\\
0, & \text{if }x\neq y
\end{cases}
$, this rewrites as $\delta_{p_{x},p_{y}}=\delta_{x,y}$. This proves
(\ref{pf.lem.diag.minors.short.deldel}).}.
\end{vershort}

\begin{verlong}
Notice that%
\begin{equation}
\delta_{p_{x},p_{y}}=\delta_{x,y}\ \ \ \ \ \ \ \ \ \ \text{for every }\left(
x,y\right)  \in\left\{  1,2,\ldots,k\right\}  ^{2}
\label{pf.lem.diag.minors.deldel}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.diag.minors.deldel}):} Let $\left(
x,y\right)  \in\left\{  1,2,\ldots,k\right\}  ^{2}$. Thus, $x\in\left\{
1,2,\ldots,k\right\}  $ and $y\in\left\{  1,2,\ldots,k\right\}  $. We are in
one of the following two cases:
\par
\textit{Case 1:} We have $x=y$.
\par
\textit{Case 2:} We have $x\neq y$.
\par
Let us consider Case 1 first. In this case, we have $x=y$. Hence, $p_{x}%
=p_{y}$. Thus, $\delta_{p_{x},p_{y}}=1$. Comparing this with $\delta_{x,y}=1$
(since $x=y$), we obtain $\delta_{p_{x},p_{y}}=\delta_{x,y}$. Thus,
(\ref{pf.lem.diag.minors.deldel}) is proven in Case 1.
\par
Now, let us consider Case 2. In this case, we have $x\neq y$. In other words,
the elements $x$ and $y$ are distinct.
\par
But $w\left(  P\right)  $ is the list of all elements of $P$ in increasing
order (with no repetitions). Thus, $w\left(  P\right)  $ is a list with no
repetitions. In other words, $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is a
list with no repetitions (since $w\left(  P\right)  =\left(  p_{1}%
,p_{2},\ldots,p_{k}\right)  $). In other words, the elements $p_{1}%
,p_{2},\ldots,p_{k}$ are pairwise distinct. In other words, if $u$ and $v$ are
any two distinct elements of $\left\{  1,2,\ldots,k\right\}  $, then
$p_{u}\neq p_{v}$. Applying this to $u=x$ and $v=y$, we obtain $p_{x}\neq
p_{y}$. Thus, $\delta_{p_{x},p_{y}}=0$. Comparing this with $\delta_{x,y}=0$
(since $x\neq y$), we obtain $\delta_{p_{x},p_{y}}=\delta_{x,y}$. Thus,
(\ref{pf.lem.diag.minors.deldel}) is proven in Case 2.
\par
We have now proven (\ref{pf.lem.diag.minors.deldel}) in each of the two Cases
1 and 2. Since these two Cases cover all possibilities, we thus conclude that
(\ref{pf.lem.diag.minors.deldel}) always holds.}.
\end{verlong}

\begin{vershort}
Recall again that $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is a list of all
elements of $P$ (with no repetitions). Thus, the elements of $P$ are
$p_{1},p_{2},\ldots,p_{k}$ (with no repetitions). Hence,%
\begin{equation}
\prod_{i\in P}d_{i}=d_{p_{1}}d_{p_{2}}\cdots d_{p_{k}}=\prod_{i=1}^{k}%
d_{p_{i}}. \label{pf.lem.diag.minors.short.prod}%
\end{equation}

\end{vershort}

\begin{verlong}
Also,%
\begin{equation}
\prod_{i=1}^{k}d_{p_{i}}=\prod_{i\in P}d_{i} \label{pf.lem.diag.minors.prod}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.diag.minors.prod}):} Let us use the
notation introduced in Definition \ref{def.sol.exe.Ialbe.12n}. Thus, $\left[
k\right]  =\left\{  1,2,\ldots,k\right\}  $ (by the definition of $\left[
k\right]  $).
\par
We know that $w\left(  P\right)  $ is a list of all elements of $P$ (with no
repetitions). In other words, $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is a
list of all elements of $P$ (with no repetitions) (since $w\left(  P\right)
=\left(  p_{1},p_{2},\ldots,p_{k}\right)  $). Thus, Lemma
\ref{lem.sol.exe.Ialbe.inclist} \textbf{(a)} (applied to $P$, $k$ and $\left(
p_{1},p_{2},\ldots,p_{k}\right)  $ instead of $S$, $s$ and $\left(
c_{1},c_{2},\ldots,c_{s}\right)  $) shows that the map $\left[  k\right]
\rightarrow P,\ h\mapsto p_{h}$ is well-defined and a bijection. In
particular, this map is a bijection. Hence, we can substitute $p_{h}$ for $i$
in the product $\prod_{i\in P}d_{i}$. We thus obtain%
\[
\prod_{i\in P}d_{i}=\underbrace{\prod_{h\in\left[  k\right]  }}%
_{\substack{=\prod_{h\in\left\{  1,2,\ldots,k\right\}  }\\\text{(since
}\left[  k\right]  =\left\{  1,2,\ldots,k\right\}  \text{)}}}d_{p_{h}%
}=\underbrace{\prod_{h\in\left\{  1,2,\ldots,k\right\}  }}_{=\prod_{h=1}^{k}%
}d_{p_{h}}=\prod_{h=1}^{k}d_{p_{h}}=\prod_{i=1}^{k}d_{p_{i}}%
\]
(here, we have renamed the index $h$ as $i$ in the product). This proves
(\ref{pf.lem.diag.minors.prod}).}.
\end{verlong}

We are in one of the following two cases:

\textit{Case 1:} We have $P\neq Q$.

\textit{Case 2:} We have $P=Q$.

\begin{vershort}
Let us consider Case 1 first. In this case, we have $P\neq Q$. Hence,
$\delta_{P,Q}=0$. On the other hand, there exists some $p\in P$ such that
$p\notin Q$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary (for the sake
of contradiction). Hence, every $p\in P$ satisfies $p\in Q$. In other words,
$P\subseteq Q$. But $\left\vert P\right\vert =\left\vert Q\right\vert
\geq\left\vert Q\right\vert $.
\par
But if $X$ is a finite set, then every subset of $X$ that has size
$\geq\left\vert X\right\vert $ must be $X$ itself. In other words, if $X$ is a
finite set, and if $Y$ is a subset of $X$ satisfying $\left\vert Y\right\vert
\geq\left\vert X\right\vert $, then $Y=X$. Applying this to $X=Q$ and $Y=P$,
we conclude that $P=Q$ (since $P$ is a subset of $Q$, and since $Q$ is a
finite set). This contradicts $P\neq Q$. This contradiction shows that our
assumption was wrong; qed.}. Consider this $p$.
\end{vershort}

\begin{verlong}
Let us consider Case 1 first. In this case, we have $P\neq Q$. Hence,
$\delta_{P,Q}=0$. On the other hand, there exists some $p\in P$ such that
$p\notin Q$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary (for the sake
of contradiction). Hence, every $p\in P$ satisfies $p\in Q$. In other words,
$P\subseteq Q$. In other words, $P$ is a subset of $Q$. But $\left\vert
P\right\vert =\left\vert Q\right\vert \geq\left\vert Q\right\vert $.
\par
We know that $Q$ is a subset of $\left\{  1,2,\ldots,n\right\}  $, and thus is
a finite set (since $\left\{  1,2,\ldots,n\right\}  $ is a finite set).
\par
But if $X$ is a finite set, then every subset of $X$ that has size
$\geq\left\vert X\right\vert $ must be $X$ itself. In other words, if $X$ is a
finite set, and if $Y$ is a subset of $X$ satisfying $\left\vert Y\right\vert
\geq\left\vert X\right\vert $, then $Y=X$. Applying this to $X=Q$ and $Y=P$,
we conclude that $P=Q$ (since $P$ is a subset of $Q$). This contradicts $P\neq
Q$. This contradiction shows that our assumption was wrong; qed.}. Consider
this $p$.
\end{verlong}

\begin{vershort}
Now, $p\in P=\left\{  p_{1},p_{2},\ldots,p_{k}\right\}  $. In other words,
$p=p_{u}$ for some $u\in\left\{  1,2,\ldots,k\right\}  $. Consider this $u$.
Every $y\in\left\{  1,2,\ldots,k\right\}  $ satisfies%
\begin{equation}
\delta_{p_{u},q_{y}}=0 \label{pf.lem.diag.minors.short.C1.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.diag.minors.short.C1.1}):} Let
$y\in\left\{  1,2,\ldots,k\right\}  $.
\par
We know that $\left(  q_{1},q_{2},\ldots,q_{k}\right)  =w\left(  Q\right)  $
is a list of all elements of $Q$. Hence, $\left\{  q_{1},q_{2},\ldots
,q_{k}\right\}  =Q$. Now, $y\in\left\{  1,2,\ldots,k\right\}  $, so that
$q_{y}\in\left\{  q_{1},q_{2},\ldots,q_{k}\right\}  =Q$. If we had
$p_{u}=q_{y}$, then we would have $p=p_{u}=q_{y}\in Q$, which would contradict
$p\notin Q$. Thus, we cannot have $p_{u}=q_{y}$. In other words, we have
$p_{u}\neq q_{y}$. Hence, $\delta_{p_{u},q_{y}}=0$. This proves
(\ref{pf.lem.diag.minors.short.C1.1}).}. Now,
\begin{align*}
&  \left(  \text{the }u\text{-th row of the matrix }%
\underbrace{\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }D}_{=\left(  d_{p_{x}}\delta_{p_{x},q_{y}}\right)  _{1\leq x\leq
k,\ 1\leq y\leq k}}\right) \\
&  =\left(  \text{the }u\text{-th row of the matrix }\left(  d_{p_{x}}%
\delta_{p_{x},q_{y}}\right)  _{1\leq x\leq k,\ 1\leq y\leq k}\right) \\
&  =\left(  d_{p_{u}}\underbrace{\delta_{p_{u},q_{y}}}%
_{\substack{=0\\\text{(by (\ref{pf.lem.diag.minors.short.C1.1}))}}}\right)
_{1\leq x\leq1,\ 1\leq y\leq k}=\left(  \underbrace{d_{p_{u}}0}_{=0}\right)
_{1\leq x\leq1,\ 1\leq y\leq k}=\left(  0\right)  _{1\leq x\leq1,\ 1\leq y\leq
k}.
\end{align*}
In other words, the $u$-th row of the matrix $\operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$ consists of zeroes.
Therefore, Exercise \ref{exe.ps4.6} \textbf{(c)} (applied to $k$ and
$\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$
instead of $n$ and $A$) yields $\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D\right)  =0$. Comparing
this with $\underbrace{\delta_{P,Q}}_{=0}\prod_{i\in P}d_{i}=0$, we obtain
$\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }D\right)  =\delta_{P,Q}\prod_{i\in P}d_{i}$. Hence, Lemma
\ref{lem.diag.minors} is proven in Case 1.
\end{vershort}

\begin{verlong}
We know that $w\left(  P\right)  $ is a list of all elements of $P$. In other
words, $\left(  p_{1},p_{2},\ldots,p_{k}\right)  $ is a list of all elements
of $P$ (since $w\left(  P\right)  =\left(  p_{1},p_{2},\ldots,p_{k}\right)
$). Hence, $\left\{  p_{1},p_{2},\ldots,p_{k}\right\}  =P$. Now, $p\in
P=\left\{  p_{1},p_{2},\ldots,p_{k}\right\}  $. In other words, $p=p_{u}$ for
some $u\in\left\{  1,2,\ldots,k\right\}  $. Consider this $u$. Every
$y\in\left\{  1,2,\ldots,k\right\}  $ satisfies%
\begin{equation}
\delta_{p_{u},q_{y}}=0 \label{pf.lem.diag.minors.C1.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.diag.minors.C1.1}):} Let $y\in\left\{
1,2,\ldots,k\right\}  $.
\par
We know that $w\left(  Q\right)  $ is a list of all elements of $Q$. In other
words, $\left(  q_{1},q_{2},\ldots,q_{k}\right)  $ is a list of all elements
of $Q$ (since $w\left(  Q\right)  =\left(  q_{1},q_{2},\ldots,q_{k}\right)
$). Hence, $\left\{  q_{1},q_{2},\ldots,q_{k}\right\}  =Q$. Now, $y\in\left\{
1,2,\ldots,k\right\}  $, so that $q_{y}\in\left\{  q_{1},q_{2},\ldots
,q_{k}\right\}  =Q$. If we had $p_{u}=q_{y}$, then we would have
$p=p_{u}=q_{y}\in Q$, which would contradict $p\notin Q$. Thus, we cannot have
$p_{u}=q_{y}$. In other words, we have $p_{u}\neq q_{y}$. Hence,
$\delta_{p_{u},q_{y}}=0$. This proves (\ref{pf.lem.diag.minors.C1.1}).}. Now,
\begin{align*}
&  \left(  \text{the }u\text{-th row of the matrix }%
\underbrace{\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }D}_{=\left(  d_{p_{x}}\delta_{p_{x},q_{y}}\right)  _{1\leq x\leq
k,\ 1\leq y\leq k}}\right) \\
&  =\left(  \text{the }u\text{-th row of the matrix }\left(  d_{p_{x}}%
\delta_{p_{x},q_{y}}\right)  _{1\leq x\leq k,\ 1\leq y\leq k}\right) \\
&  =\left(  d_{p_{u}}\underbrace{\delta_{p_{u},q_{y}}}%
_{\substack{=0\\\text{(by (\ref{pf.lem.diag.minors.C1.1}))}}}\right)  _{1\leq
x\leq1,\ 1\leq y\leq k}=\left(  \underbrace{d_{p_{u}}0}_{=0}\right)  _{1\leq
x\leq1,\ 1\leq y\leq k}=\left(  0\right)  _{1\leq x\leq1,\ 1\leq y\leq k}.
\end{align*}
In other words, the $u$-th row of the matrix $\operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$ consists of zeroes.
Hence, a row of the matrix $\operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }D$ consists of zeroes (namely, the $u$-th row). Hence,
Exercise \ref{exe.ps4.6} \textbf{(c)} (applied to $k$ and $\operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$ instead of $n$ and
$A$) yields $\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }D\right)  =0$. Comparing this with $\underbrace{\delta
_{P,Q}}_{=0}\prod_{i\in P}d_{i}=0$, we obtain $\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D\right)  =\delta
_{P,Q}\prod_{i\in P}d_{i}$. Hence, Lemma \ref{lem.diag.minors} is proven in
Case 1.
\end{verlong}

\begin{vershort}
Let us now consider Case 2. In this case, we have $P=Q$. Hence, $\delta
_{P,Q}=1$. On the other hand, from $w\left(  Q\right)  =\left(  q_{1}%
,q_{2},\ldots,q_{k}\right)  $, we obtain
\[
\left(  q_{1},q_{2},\ldots,q_{k}\right)  =w\left(  \underbrace{Q}_{=P}\right)
=w\left(  P\right)  =\left(  p_{1},p_{2},\ldots,p_{k}\right)  .
\]
In other words, $q_{y}=p_{y}$ for every $y\in\left\{  1,2,\ldots,k\right\}  $.
Thus, every $y\in\left\{  1,2,\ldots,k\right\}  $ satisfies%
\begin{align}
\delta_{p_{x},q_{y}}  &  =\delta_{p_{x},p_{y}}\ \ \ \ \ \ \ \ \ \ \left(
\text{since }q_{y}=p_{y}\right) \nonumber\\
&  =\delta_{x,y}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.diag.minors.short.deldel})}\right)  .
\label{pf.lem.diag.minors.short.C2.2}%
\end{align}
Now, (\ref{pf.lem.diag.minors.sub=}) becomes%
\[
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}D=\left(  d_{p_{x}}\underbrace{\delta_{p_{x},q_{y}}}_{\substack{=\delta
_{x,y}\\\text{(by (\ref{pf.lem.diag.minors.short.C2.2}))}}}\right)  _{1\leq
x\leq k,\ 1\leq y\leq k}=\left(  d_{p_{x}}\delta_{x,y}\right)  _{1\leq x\leq
k,\ 1\leq y\leq k}=\left(  d_{p_{i}}\delta_{i,j}\right)  _{1\leq i\leq
k,\ 1\leq j\leq k}%
\]
(here, we have renamed the index $\left(  x,y\right)  $ as $\left(
i,j\right)  $).
\end{vershort}

\begin{verlong}
Let us now consider Case 2. In this case, we have $P=Q$. Hence, $\delta
_{P,Q}=1$. On the other hand, from $w\left(  Q\right)  =\left(  q_{1}%
,q_{2},\ldots,q_{k}\right)  $, we obtain
\[
\left(  q_{1},q_{2},\ldots,q_{k}\right)  =w\left(  \underbrace{Q}_{=P}\right)
=w\left(  P\right)  =\left(  p_{1},p_{2},\ldots,p_{k}\right)  .
\]
In other words,%
\begin{equation}
q_{y}=p_{y}\ \ \ \ \ \ \ \ \ \ \text{for every }y\in\left\{  1,2,\ldots
,k\right\}  . \label{pf.lem.diag.minors.C2.1}%
\end{equation}
Now, (\ref{pf.lem.diag.minors.sub=}) becomes%
\begin{align*}
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D  &
=\left(  d_{p_{x}}\underbrace{\delta_{p_{x},q_{y}}}_{\substack{=\delta
_{p_{x},p_{y}}\\\text{(by (\ref{pf.lem.diag.minors.C2.1}))}}}\right)  _{1\leq
x\leq k,\ 1\leq y\leq k}=\left(  d_{p_{x}}\underbrace{\delta_{p_{x},p_{y}}%
}_{\substack{=\delta_{x,y}\\\text{(by (\ref{pf.lem.diag.minors.deldel}))}%
}}\right)  _{1\leq x\leq k,\ 1\leq y\leq k}\\
&  =\left(  d_{p_{x}}\delta_{x,y}\right)  _{1\leq x\leq k,\ 1\leq y\leq
k}=\left(  d_{p_{i}}\delta_{i,j}\right)  _{1\leq i\leq k,\ 1\leq j\leq k}%
\end{align*}
(here, we have renamed the index $\left(  x,y\right)  $ as $\left(
i,j\right)  $).
\end{verlong}

\begin{vershort}
But we have $d_{p_{i}}\delta_{i,j}=0$ for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,k\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,k\right\}  ^{2}$ be such that $i<j$. Thus, $i\neq j$ (since $i<j$),
so that $\delta_{i,j}=0$. Thus, $d_{p_{i}}\underbrace{\delta_{i,j}}_{=0}=0$,
qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to $k$, $\operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$ and $d_{p_{i}}%
\delta_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) yields $\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}D\right)  =\left(  d_{p_{1}}\delta_{1,1}\right)  \left(  d_{p_{2}}%
\delta_{2,2}\right)  \cdots\left(  d_{p_{k}}\delta_{k,k}\right)  $ (since
$\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}D=\left(  d_{p_{i}}\delta_{i,j}\right)  _{1\leq i\leq k,\ 1\leq j\leq k}$).
Thus,%
\begin{align*}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }D\right)   &  =\left(  d_{p_{1}}\delta_{1,1}\right)  \left(
d_{p_{2}}\delta_{2,2}\right)  \cdots\left(  d_{p_{k}}\delta_{k,k}\right)
=\prod_{i=1}^{k}\left(  d_{p_{i}}\underbrace{\delta_{i,i}}%
_{\substack{=1\\\text{(since }i=i\text{)}}}\right) \\
&  =\prod_{i=1}^{k}d_{p_{i}}=\prod_{i\in P}d_{i}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.lem.diag.minors.short.prod})}\right)  .
\end{align*}
Comparing this with $\underbrace{\delta_{P,Q}}_{=1}\prod_{i\in P}d_{i}%
=\prod_{i\in P}d_{i}$, we obtain $\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D\right)  =\delta
_{P,Q}\prod_{i\in P}d_{i}$. Hence, Lemma \ref{lem.diag.minors} is proven in
Case 2.
\end{vershort}

\begin{verlong}
But we have $d_{p_{i}}\delta_{i,j}=0$ for every $\left(  i,j\right)
\in\left\{  1,2,\ldots,k\right\}  ^{2}$ satisfying $i<j$%
\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,k\right\}  ^{2}$ be such that $i<j$. Thus, $i\neq j$ (since $i<j$),
so that $\delta_{i,j}=0$. Thus, $d_{p_{i}}\underbrace{\delta_{i,j}}_{=0}=0$,
qed.}. Hence, Exercise \ref{exe.ps4.3} (applied to $k$, $\operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D$ and $d_{p_{i}}%
\delta_{i,j}$ instead of $n$, $A$ and $a_{i,j}$) yields $\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}D\right)  =\left(  d_{p_{1}}\delta_{1,1}\right)  \left(  d_{p_{2}}%
\delta_{2,2}\right)  \cdots\left(  d_{p_{k}}\delta_{k,k}\right)  $ (since
$\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}D=\left(  d_{p_{i}}\delta_{i,j}\right)  _{1\leq i\leq k,\ 1\leq j\leq k}$).
Thus,%
\begin{align*}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }D\right)   &  =\left(  d_{p_{1}}\delta_{1,1}\right)  \left(
d_{p_{2}}\delta_{2,2}\right)  \cdots\left(  d_{p_{k}}\delta_{k,k}\right)
=\prod_{i=1}^{k}\left(  d_{p_{i}}\underbrace{\delta_{i,i}}%
_{\substack{=1\\\text{(since }i=i\text{)}}}\right) \\
&  =\prod_{i=1}^{k}d_{p_{i}}=\prod_{i\in P}d_{i}\ \ \ \ \ \ \ \ \ \ \left(
\text{by (\ref{pf.lem.diag.minors.prod})}\right)  .
\end{align*}
Comparing this with $\underbrace{\delta_{P,Q}}_{=1}\prod_{i\in P}d_{i}%
=\prod_{i\in P}d_{i}$, we obtain $\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }D\right)  =\delta
_{P,Q}\prod_{i\in P}d_{i}$. Hence, Lemma \ref{lem.diag.minors} is proven in
Case 2.
\end{verlong}

We now have proven Lemma \ref{lem.diag.minors} in each of the two Cases 1 and
2. Since these two Cases cover all possibilities, we thus conclude that Lemma
\ref{lem.diag.minors} always holds.
\end{proof}

\begin{proof}
[Proof of Corollary \ref{cor.det(A+D)}.]We start by making some general observations:

\begin{vershort}
For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}%
$ denote the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.
Then, any two subsets $P$ and $Q$ of $\left\{  1,2,\ldots,n\right\}  $ satisfy%
\begin{equation}
\delta_{\widetilde{P},\widetilde{Q}}=\delta_{P,Q}
\label{pf.cor.det(A+D).short.del=del}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.cor.det(A+D).short.del=del}):} Let $P$ and
$Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $. If $P=Q$, then
$\widetilde{P}=\widetilde{Q}$ and thus $\delta_{\widetilde{P},\widetilde{Q}%
}=1=\delta_{P,Q}$ (since $P=Q$ yields $\delta_{P,Q}=1$). Therefore, if $P=Q$,
then (\ref{pf.cor.det(A+D).short.del=del}) holds. Hence, for the rest of the
proof of (\ref{pf.cor.det(A+D).short.del=del}), we WLOG assume that we don't
have $P=Q$. In other words, we have $P\neq Q$. Thus, $\delta_{P,Q}=0$.
\par
Now, the definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P$. Hence, $\widetilde{P}$ is again a subset
of $\left\{  1,2,\ldots,n\right\}  $. The definition of
$\widetilde{\widetilde{P}}$ yields
\[
\widetilde{\widetilde{P}}=\left\{  1,2,\ldots,n\right\}  \setminus
\underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}  \setminus
P}=\left\{  1,2,\ldots,n\right\}  \setminus\left(  \left\{  1,2,\ldots
,n\right\}  \setminus P\right)  =P
\]
(since $P\subseteq\left\{  1,2,\ldots,n\right\}  $). Similarly,
$\widetilde{\widetilde{Q}}=Q$. If we had $\widetilde{P}=\widetilde{Q}$, then
we would have $\widetilde{\widetilde{P}}=\widetilde{\widetilde{Q}}$, which
would contradict $\widetilde{\widetilde{P}}=P\neq Q=\widetilde{\widetilde{Q}}%
$. Hence, we cannot have $\widetilde{P}=\widetilde{Q}$. Thus, we have
$\widetilde{P}\neq\widetilde{Q}$, so that $\delta_{\widetilde{P}%
,\widetilde{Q}}=0$. Compared with $\delta_{P,Q}=0$, this yields $\delta
_{\widetilde{P},\widetilde{Q}}=\delta_{P,Q}$. This proves
(\ref{pf.cor.det(A+D).short.del=del}).}.
\end{vershort}

\begin{verlong}
For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}%
$ denote the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.
Then, any two subsets $P$ and $Q$ of $\left\{  1,2,\ldots,n\right\}  $ satisfy%
\begin{equation}
\delta_{\widetilde{P},\widetilde{Q}}=\delta_{P,Q}
\label{pf.cor.det(A+D).del=del}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.cor.det(A+D).del=del}):} Let $P$ and $Q$
be two subsets of $\left\{  1,2,\ldots,n\right\}  $.
\par
The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P\subseteq\left\{  1,2,\ldots,n\right\}  $.
Hence, $\widetilde{P}$ is again a subset of $\left\{  1,2,\ldots,n\right\}  $.
Thus, the definition of $\widetilde{\widetilde{P}}$ yields
\[
\widetilde{\widetilde{P}}=\left\{  1,2,\ldots,n\right\}  \setminus
\underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}  \setminus
P}=\left\{  1,2,\ldots,n\right\}  \setminus\left(  \left\{  1,2,\ldots
,n\right\}  \setminus P\right)  =P
\]
(since $P\subseteq\left\{  1,2,\ldots,n\right\}  $). The same argument
(applied to $Q$ instead of $P$) shows that $\widetilde{\widetilde{Q}}=Q$.
\par
If $P=Q$, then%
\begin{align*}
\delta_{\widetilde{P},\widetilde{Q}}  &  =\delta_{\widetilde{Q},\widetilde{Q}%
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P=Q\right) \\
&  =1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{Q}=\widetilde{Q}%
\right) \\
&  =\delta_{P,Q}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\delta_{P,Q}=1\text{
(since }P=Q\text{)}\right)  .
\end{align*}
Hence, if $P=Q$, then (\ref{pf.cor.det(A+D).del=del}) is proven. Thus, for the
rest of our proof of (\ref{pf.cor.det(A+D).del=del}), we can WLOG assume that
we don't have $P=Q$. Assume this.
\par
We don't have $P=Q$. In other words, we have $P\neq Q$. Hence, $\delta
_{P,Q}=0$. If we had $\widetilde{P}=\widetilde{Q}$, then we would have
\begin{align*}
\widetilde{\widetilde{P}}  &  =\widetilde{\widetilde{Q}}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\widetilde{P}=\widetilde{Q}\right) \\
&  =Q,
\end{align*}
which would contradict $\widetilde{\widetilde{P}}=P\neq Q$. Hence, we cannot
have $\widetilde{P}=\widetilde{Q}$. In other words, we have $\widetilde{P}%
\neq\widetilde{Q}$. Hence, $\delta_{\widetilde{P},\widetilde{Q}}=0$. Comparing
this with $\delta_{P,Q}=0$ (since $P\neq Q$), we obtain $\delta_{\widetilde{P}%
,\widetilde{Q}}=\delta_{P,Q}$. This proves (\ref{pf.cor.det(A+D).del=del}).}.
\end{verlong}

\begin{vershort}
Furthermore, if $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}
$ satisfying $\left\vert P\right\vert =\left\vert Q\right\vert $, then%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }D\right)  =\delta_{P,Q}\prod_{i\in\left\{
1,2,\ldots,n\right\}  \setminus P}d_{i} \label{pf.cor.det(A+D).short.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.cor.det(A+D).short.1}):} Let $P$ and $Q$
be two subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
P\right\vert =\left\vert Q\right\vert $.
\par
The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P\subseteq\left\{  1,2,\ldots,n\right\}  $. In
other words, $\widetilde{P}$ is a subset of $\left\{  1,2,\ldots,n\right\}  $.
The same argument (applied to $Q$ instead of $P$) shows that $\widetilde{Q}$
is a subset of $\left\{  1,2,\ldots,n\right\}  $.
\par
We have%
\begin{align*}
\left\vert \underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}
\setminus P}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus P\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\left\vert P\right\vert \ \ \ \ \ \ \ \ \ \ \left(
\text{since }P\subseteq\left\{  1,2,\ldots,n\right\}  \right) \\
&  =n-\left\vert P\right\vert .
\end{align*}
The same argument (applied to $Q$ instead of $P$) shows that $\left\vert
\widetilde{Q}\right\vert =n-\left\vert Q\right\vert $. Now, $\left\vert
\widetilde{P}\right\vert =n-\underbrace{\left\vert P\right\vert }_{=\left\vert
Q\right\vert }=n-\left\vert Q\right\vert =\left\vert \widetilde{Q}\right\vert
$. Thus, Lemma \ref{lem.diag.minors} (applied to $\widetilde{P}$ and
$\widetilde{Q}$ instead of $P$ and $Q$) yields
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }D\right)  =\underbrace{\delta
_{\widetilde{P},\widetilde{Q}}}_{\substack{=\delta_{P,Q}\\\text{(by
(\ref{pf.cor.det(A+D).short.del=del}))}}}\underbrace{\prod_{i\in\widetilde{P}%
}}_{\substack{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}\\\text{(since }\widetilde{P}=\left\{  1,2,\ldots,n\right\}  \setminus
P\text{)}}}d_{i}=\delta_{P,Q}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}.
\]
This proves (\ref{pf.cor.det(A+D).short.1}).}.
\end{vershort}

\begin{verlong}
Furthermore, if $P$ and $Q$ are two subsets of $\left\{  1,2,\ldots,n\right\}
$ satisfying $\left\vert P\right\vert =\left\vert Q\right\vert $, then%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }D\right)  =\delta_{P,Q}\prod_{i\in\left\{
1,2,\ldots,n\right\}  \setminus P}d_{i} \label{pf.cor.det(A+D).1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.cor.det(A+D).1}):} Let $P$ and $Q$ be two
subsets of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
P\right\vert =\left\vert Q\right\vert $.
\par
The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P\subseteq\left\{  1,2,\ldots,n\right\}  $. In
other words, $\widetilde{P}$ is a subset of $\left\{  1,2,\ldots,n\right\}  $.
The same argument (applied to $Q$ instead of $P$) shows that $\widetilde{Q}$
is a subset of $\left\{  1,2,\ldots,n\right\}  $.
\par
We have%
\begin{align*}
\left\vert \underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}
\setminus P}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus P\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\left\vert P\right\vert \ \ \ \ \ \ \ \ \ \ \left(
\text{since }P\subseteq\left\{  1,2,\ldots,n\right\}  \right) \\
&  =n-\left\vert P\right\vert .
\end{align*}
The same argument (applied to $Q$ instead of $P$) shows that $\left\vert
\widetilde{Q}\right\vert =n-\left\vert Q\right\vert $. Now, $\left\vert
\widetilde{P}\right\vert =n-\underbrace{\left\vert P\right\vert }_{=\left\vert
Q\right\vert }=n-\left\vert Q\right\vert =\left\vert \widetilde{Q}\right\vert
$. Thus, Lemma \ref{lem.diag.minors} (applied to $\widetilde{P}$ and
$\widetilde{Q}$ instead of $P$ and $Q$) yields
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{P}\right)
}^{w\left(  \widetilde{Q}\right)  }D\right)  =\underbrace{\delta
_{\widetilde{P},\widetilde{Q}}}_{\substack{=\delta_{P,Q}\\\text{(by
(\ref{pf.cor.det(A+D).del=del}))}}}\underbrace{\prod_{i\in\widetilde{P}}%
}_{\substack{=\prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}\\\text{(since }\widetilde{P}=\left\{  1,2,\ldots,n\right\}  \setminus
P\text{)}}}d_{i}=\delta_{P,Q}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}.
\]
This proves (\ref{pf.cor.det(A+D).1}).}.
\end{verlong}

\begin{vershort}
Now, every subset $P$ of $\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{align}
&  \sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \underbrace{\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}D\right)  }_{\substack{=\delta_{P,Q}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\\\text{(by (\ref{pf.cor.det(A+D).short.1}))}}}\nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \delta_{P,Q}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\nonumber\\
&  =\underbrace{\left(  -1\right)  ^{\sum P+\sum P}}%
_{\substack{=1\\\text{(since }\sum P+\sum P=2\sum P\text{ is even)}}%
}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \underbrace{\delta_{P,P}}_{=1}\prod_{i\in\left\{
1,2,\ldots,n\right\}  \setminus P}d_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{Q\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert P\right\vert =\left\vert Q\right\vert ;\\Q\neq
P}}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)
\underbrace{\delta_{P,Q}}_{\substack{=0\\\text{(since }P\neq Q\\\text{(since
}Q\neq P\text{))}}}\prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}%
d_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have split off the addend for }Q=P\text{ from the sum}\\
\text{(since }P\text{ is a subset }Q\text{ of }\left\{  1,2,\ldots,n\right\}
\text{ satisfying }\left\vert P\right\vert =\left\vert Q\right\vert \text{)}%
\end{array}
\right) \nonumber\\
&  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{Q\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert P\right\vert =\left\vert Q\right\vert
;\\Q\neq P}}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  0\prod
_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}d_{i}}_{=0}\nonumber\\
&  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}. \label{pf.cor.det(A+D).short.6}%
\end{align}

\end{vershort}

\begin{verlong}
Now, let $P$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Then,%
\begin{align}
&  \sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \underbrace{\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)
}D\right)  }_{\substack{=\delta_{P,Q}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\\\text{(by (\ref{pf.cor.det(A+D).1}))}}}\nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \delta_{P,Q}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert ;\\Q=P}}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \underbrace{\delta_{P,Q}}_{\substack{=1\\\text{(since
}P=Q\\\text{(since }Q=P\text{))}}}\prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{Q\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert P\right\vert =\left\vert Q\right\vert ;\\Q\neq
P}}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)
\underbrace{\delta_{P,Q}}_{\substack{=0\\\text{(since }P\neq Q\\\text{(since
}Q\neq P\text{))}}}\prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}%
d_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since every subset }Q\text{ of }\left\{  1,2,\ldots,n\right\}  \text{
satisfying }\left\vert P\right\vert =\left\vert Q\right\vert \\
\text{satisfies either }Q=P\text{ or }Q\neq P\text{ (but not both)}%
\end{array}
\right) \nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert ;\\Q=P}}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{Q\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert P\right\vert =\left\vert Q\right\vert
;\\Q\neq P}}\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  0\prod
_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}d_{i}}_{=0}\nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert ;\\Q=P}}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}. \label{pf.cor.det(A+D).4}%
\end{align}
But $P$ is a subset $Q$ of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert P\right\vert =\left\vert Q\right\vert $ and $Q=P$ (because
$\left\vert P\right\vert =\left\vert P\right\vert $ and $P=P$). Also, $P$ is
clearly the only such subset $Q$ (because any such subset $Q$ must satisfy
$Q=P$). Hence, there exists exactly one subset $Q$ of $\left\{  1,2,\ldots
,n\right\}  $ satisfying $\left\vert P\right\vert =\left\vert Q\right\vert $
and $Q=P$: namely, the subset $P$. Thus, the sum
\[
\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert ;\\Q=P}}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}%
\]
has only one addend: namely, the addend for $Q=P$. Thus, this sum rewrites as
follows:%
\begin{align*}
&  \sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert ;\\Q=P}}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}\\
&  =\underbrace{\left(  -1\right)  ^{\sum P+\sum P}}%
_{\substack{=1\\\text{(since }\sum P+\sum P=2\sum P\text{ is even)}}%
}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}\\
&  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}.
\end{align*}
Now, (\ref{pf.cor.det(A+D).4}) becomes%
\begin{align}
&  \sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }D\right) \nonumber\\
&  =\sum_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert ;\\Q=P}}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}\nonumber\\
&  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus
P}d_{i}. \label{pf.cor.det(A+D).6}%
\end{align}


Now, forget that we fixed $P$. We thus have proven that
(\ref{pf.cor.det(A+D).6}) holds for every subset $P$ of $\left\{
1,2,\ldots,n\right\}  $.
\end{verlong}

\begin{vershort}
Now, Theorem \ref{thm.det(A+B)} (applied to $B=D$) yields%
\begin{align*}
&  \det\left(  A+D\right) \\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\underbrace{\sum
_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }D\right)
}_{\substack{=\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\\\text{(by (\ref{pf.cor.det(A+D).short.6}))}}}\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  P\right)
}A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}d_{i}.
\end{align*}
This proves Corollary \ref{cor.det(A+D)}.
\end{vershort}

\begin{verlong}
Now, Theorem \ref{thm.det(A+B)} (applied to $B=D$) yields%
\begin{align*}
&  \det\left(  A+D\right) \\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\underbrace{\sum
_{\substack{Q\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =\left\vert Q\right\vert }}\left(  -1\right)  ^{\sum P+\sum
Q}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{P}\right)  }^{w\left(  \widetilde{Q}\right)  }D\right)
}_{\substack{=\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  P\right)  }A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}
\setminus P}d_{i}\\\text{(by (\ref{pf.cor.det(A+D).6}))}}}\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  P\right)
}A\right)  \prod_{i\in\left\{  1,2,\ldots,n\right\}  \setminus P}d_{i}.
\end{align*}
This proves Corollary \ref{cor.det(A+D)}.
\end{verlong}
\end{proof}

\begin{proof}
[Proof of Corollary \ref{cor.det(A+X)}.]For every two objects $i$ and $j$,
define $\delta_{i,j}\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$). Now,%
\[
x\underbrace{I_{n}}_{=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}}=x\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  x\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(by the definition of $x\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}$). Hence, Corollary \ref{cor.det(A+D)} (applied to $d_{i}=x$ and
$D=xI_{n}$) yields%
\begin{align*}
\det\left(  A+xI_{n}\right)   &  =\sum_{P\subseteq\left\{  1,2,\ldots
,n\right\}  }\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  P\right)  }A\right)  \underbrace{\prod_{i\in\left\{
1,2,\ldots,n\right\}  \setminus P}x}_{\substack{=x^{\left\vert \left\{
1,2,\ldots,n\right\}  \setminus P\right\vert }=x^{\left\vert \left\{
1,2,\ldots,n\right\}  \right\vert -\left\vert P\right\vert }\\\text{(since
}\left\vert \left\{  1,2,\ldots,n\right\}  \setminus P\right\vert =\left\vert
\left\{  1,2,\ldots,n\right\}  \right\vert -\left\vert P\right\vert
\\\text{(since }P\subseteq\left\{  1,2,\ldots,n\right\}  \text{))}}}\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  P\right)
}A\right)  \underbrace{x^{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert -\left\vert P\right\vert }}_{\substack{=x^{n-\left\vert
P\right\vert }\\\text{(since }\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert =n\text{)}}}\\
&  =\sum_{P\subseteq\left\{  1,2,\ldots,n\right\}  }\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  P\right)
}A\right)  x^{n-\left\vert P\right\vert }.
\end{align*}
This proves (\ref{eq.cor.det(A+X).1}).

Furthermore, every subset $P$ of $\left\{  1,2,\ldots,n\right\}  $ satisfies
$\left\vert P\right\vert \in\left\{  0,1,\ldots,n\right\}  $%
\ \ \ \ \footnote{\textit{Proof.} Let $P$ be a subset of $\left\{
1,2,\ldots,n\right\}  $. Then, $P$ is a finite set (since $\left\{
1,2,\ldots,n\right\}  $ is a finite set), so that $\left\vert P\right\vert
\in\mathbb{N}$. Also, $P$ is a subset of $\left\{  1,2,\ldots,n\right\}  $,
and thus we have $\left\vert P\right\vert \leq\left\vert \left\{
1,2,\ldots,n\right\}  \right\vert =n$. Combined with $\left\vert P\right\vert
\in\mathbb{N}$, this yields $\left\vert P\right\vert \in\left\{
0,1,\ldots,n\right\}  $. Qed.}. Now,
\begin{align*}
\det\left(  A+xI_{n}\right)   &  =\underbrace{\sum_{P\subseteq\left\{
1,2,\ldots,n\right\}  }}_{\substack{=\sum_{k\in\left\{  0,1,\ldots,n\right\}
}\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =k}}\\\text{(since every subset }P\text{ of }\left\{
1,2,\ldots,n\right\}  \\\text{satisfies }\left\vert P\right\vert \in\left\{
0,1,\ldots,n\right\}  \text{)}}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right)  x^{n-\left\vert
P\right\vert }\\
&  =\underbrace{\sum_{k\in\left\{  0,1,\ldots,n\right\}  }}_{=\sum_{k=0}^{n}%
}\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
P\right\vert =k}}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  P\right)  }A\right)  \underbrace{x^{n-\left\vert P\right\vert }%
}_{\substack{=x^{n-k}\\\text{(since }\left\vert P\right\vert =k\text{)}}}\\
&  =\sum_{k=0}^{n}\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert P\right\vert =k}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right)  x^{n-k}\\
&  =\sum_{k=0}^{n}\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert P\right\vert =n-k}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right)
\underbrace{x^{n-\left(  n-k\right)  }}_{\substack{=x^{k}\\\text{(since
}n-\left(  n-k\right)  =k\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }n-k\text{ for
}k\text{ in the outer sum}\right) \\
&  =\sum_{k=0}^{n}\sum_{\substack{P\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert P\right\vert =n-k}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right)  x^{k}\\
&  =\sum_{k=0}^{n}\left(  \sum_{\substack{P\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert P\right\vert =n-k}}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  P\right)  }A\right)  \right)  x^{k}.
\end{align*}
This proves (\ref{eq.cor.det(A+X).2}). The proof of Corollary
\ref{cor.det(A+X)} is now complete.
\end{proof}

\begin{proof}
[Solution to Exercise \ref{exe.det(A+B).diag}.]We have proven Lemma
\ref{lem.diag.minors}, Corollary \ref{cor.det(A+D)} and Corollary
\ref{cor.det(A+X)}. Exercise \ref{exe.det(A+B).diag} is thus solved.
\end{proof}

\subsection{\label{sect.sol.exeadd.noncomm.polarization}Solution to Additional
exercise \ref{exeadd.noncomm.polarization}}

Let us first prepare some auxiliary results and notations. Throughout Section
\ref{sect.sol.exeadd.noncomm.polarization}, we shall be using the following conventions:

\begin{itemize}
\item For every $n\in\mathbb{N}$, let $\left[  n\right]  $ denote the set
$\left\{  1,2,\ldots,n\right\}  $.

\item For every $n\in\mathbb{N}$, the sign $\sum_{I\subseteq\left[  n\right]
}$ shall mean $\sum_{I\in\mathcal{P}\left(  \left[  n\right]  \right)  }$,
where $\mathcal{P}\left(  \left[  n\right]  \right)  $ denotes the powerset of
$\left[  n\right]  $.

\item If $\mathcal{A}$ is any logical statement, then $\left[  \mathcal{A}%
\right]  $ will mean the integer $%
\begin{cases}
1, & \text{if }\mathcal{A}\text{ is true};\\
0, & \text{if }\mathcal{A}\text{ is false}%
\end{cases}
$. For example, $\left[  1+1=2\right]  =1$ (since $1+1=2$ is true), whereas
$\left[  1+1=1\right]  =0$ (since $1+1=1$ is false). (This notation is known
as \href{https://en.wikipedia.org/wiki/Iverson_bracket}{the \textit{Iverson
bracket}}.)

\item If $a_{p},a_{p+1},\ldots,a_{q}$ are some elements of a noncommutative
ring $\mathbb{L}$ (with $p$ and $q$ being integers satisfying $p\leq q+1$),
then $\prod_{i=p}^{q}a_{i}$ is defined to be the product $a_{p}a_{p+1}\cdots
a_{q}\in\mathbb{L}$\ \ \ \ \footnote{Of course, in order for this definition
to be fully rigorous, we need to specify how the product $a_{p}a_{p+1}\cdots
a_{q}$ is defined. There are two possible definitions for this product; both
of them proceed by recursion on $q-p$ (with the recursion base being the case
when $q-p=-1$).
\par
\begin{itemize}
\item The first definition sets $a_{p}a_{p+1}\cdots a_{q}$ to be $1$ (the
unity of $\mathbb{L}$) when $q-p=-1$ (this is the case of an empty product),
and sets $a_{p}a_{p+1}\cdots a_{q}$ to be $\left(  a_{p}a_{p+1}\cdots
a_{q-1}\right)  a_{q}$ when $q-p\geq0$.
\par
\item The second definition sets $a_{p}a_{p+1}\cdots a_{q}$ to be $1$ (the
unity of $\mathbb{L}$) when $q-p=-1$ (this is the case of an empty product),
and sets $a_{p}a_{p+1}\cdots a_{q}$ to be $a_{p}\left(  a_{p+1}a_{p+2}\cdots
a_{q}\right)  $ when $q-p\geq0$.
\end{itemize}
\par
Fortunately, the two definitions define exactly the same notion of product;
this is not hard to prove (it follows from a fact called \textquotedblleft
general associativity\textquotedblright), but it is not entirely obvious.}.
This definition of $\prod_{i=p}^{q}a_{i}$ extends the classical definition of
$\prod_{i=p}^{q}a_{i}$ in the case when $a_{p},a_{p+1},\ldots,a_{q}$ are
elements of a commutative ring.

(Notice that we have thus defined products of the form $\prod_{i=p}^{q}a_{i}$
only. In contrast, products of the form $\prod_{i\in I}a_{i}$ for an arbitrary
finite set $I$ are \textbf{not} defined in a noncommutative ring, because it
is not clear in what order their factors are to be multiplied.)
\end{itemize}

A few elementary remarks about noncommutative rings will be useful:

\begin{itemize}
\item If $p$ and $q$ are integers satisfying $p\leq q$, and if $a_{p}%
,a_{p+1},\ldots,a_{q}$ are some elements of a noncommutative ring $\mathbb{L}%
$, then%
\[
\prod_{i=p}^{q}a_{i}=\left(  \prod_{i=p}^{q-1}a_{i}\right)  a_{q}.
\]
In other words, we can always split off the factor for $i=q$ from a product of
the form $\prod_{i=p}^{q}a_{i}$ as long as we have $p\leq q$.

\item When $\mathbb{L}$ is a noncommutative ring, the following two properties
of the $\sum$ sign hold\footnote{These are analogues of the \textquotedblleft
Factoring out\textquotedblright\ property that we have seen in Section
\ref{sect.sums-repetitorium}.}:

\begin{itemize}
\item \underline{\textbf{Factoring out on the left:}} Let $S$ be a finite set.
For every $s\in S$, let $a_{s}$ be an element of $\mathbb{L}$. Also, let
$\lambda$ be an element of $\mathbb{L}$. Then,%
\begin{equation}
\sum_{s\in S}\lambda a_{s}=\lambda\sum_{s\in S}a_{s}.
\label{eq.noncomm.sum.linear2}%
\end{equation}


\item \underline{\textbf{Factoring out on the right:}} Let $S$ be a finite
set. For every $s\in S$, let $a_{s}$ be an element of $\mathbb{L}$. Also, let
$\lambda$ be an element of $\mathbb{L}$. Then,%
\begin{equation}
\sum_{s\in S}a_{s}\lambda=\left(  \sum_{s\in S}a_{s}\right)  \lambda.
\label{eq.noncomm.sum.linear2r}%
\end{equation}

\end{itemize}
\end{itemize}

Next, we state a generalization of Lemma \ref{lem.prodrule2} to the case of a
noncommutative ring $\mathbb{K}$:

\begin{lemma}
\label{lem.noncomm.prodrule2}Let $\mathbb{K}$ be a noncommutative ring.

Let $n\in\mathbb{N}$ and $m\in\mathbb{N}$. For every $i\in\left[  n\right]  $,
let $p_{i,1},p_{i,2},\ldots,p_{i,m}$ be $m$ elements of $\mathbb{K}$. Then,%
\[
\prod_{i=1}^{n}\sum_{k=1}^{m}p_{i,k}=\sum_{\kappa:\left[  n\right]
\rightarrow\left[  m\right]  }\prod_{i=1}^{n}p_{i,\kappa\left(  i\right)  }.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.noncomm.prodrule2}.]The proof of Lemma
\ref{lem.prodrule2} given above (which includes the proof of Lemma
\ref{lem.prodrule} in the solution to Exercise \ref{exe.prodrule}) can be
reused as a proof of Lemma \ref{lem.noncomm.prodrule2}, provided that we
replace our proof of Lemma \ref{lem.prodrule.S.n=2} (which made use of the
commutativity of $\mathbb{K}$) by the following alternative proof (which does
not require $\mathbb{K}$ to be commutative):

\begin{proof}
[Second proof of Lemma \ref{lem.prodrule.S.n=2}.]The equality
(\ref{eq.sum.fubini}) remains valid if $\mathbb{A}$ is replaced by
$\mathbb{K}$ throughout it. It is in this variant that it will be used in the
following proof.

For every $\lambda\in\mathbb{K}$, we have%
\begin{align}
\sum_{y\in Y}\lambda r_{y}  &  =\sum_{s\in Y}\lambda r_{s}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}y\text{ as }s\right) \nonumber\\
&  =\lambda\sum_{s\in Y}r_{s}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.noncomm.sum.linear2}) (applied to }Y\text{, }r_{s}\text{ and
}\mathbb{K}\\
\text{instead of }S\text{, }a_{s}\text{ and }\mathbb{L}\text{)}%
\end{array}
\right) \nonumber\\
&  =\lambda\sum_{y\in Y}r_{y}\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we
renamed the summation index }s\text{ as }y\right)  .
\label{pf.lem.noncomm.prodrule.S.n=2.1}%
\end{align}
For every $\lambda\in\mathbb{K}$, we have%
\begin{align}
\sum_{x\in X}q_{x}\lambda &  =\sum_{s\in X}q_{s}\lambda
\ \ \ \ \ \ \ \ \ \ \left(  \text{here, we renamed the summation index
}x\text{ as }s\right) \nonumber\\
&  =\left(  \sum_{s\in X}q_{s}\right)  \lambda\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by (\ref{eq.noncomm.sum.linear2r}) (applied to }X\text{, }q_{s}\text{
and }\mathbb{K}\\
\text{instead of }S\text{, }a_{s}\text{ and }\mathbb{L}\text{)}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{x\in X}q_{x}\right)  \lambda\ \ \ \ \ \ \ \ \ \ \left(
\text{here, we renamed the summation index }s\text{ as }x\right)  .
\label{pf.lem.noncomm.prodrule.S.n=2.2}%
\end{align}


From (\ref{eq.sum.fubini}) (applied to $\mathbb{K}$ and $q_{x}r_{y}$ instead
of $\mathbb{A}$ and $a_{\left(  x,y\right)  }$), we obtain%
\[
\sum_{x\in X}\sum_{y\in Y}q_{x}r_{y}=\sum_{\left(  x,y\right)  \in X\times
Y}q_{x}r_{y}=\sum_{y\in Y}\sum_{x\in X}q_{x}r_{y}.
\]
Hence,%
\begin{align*}
\sum_{\left(  x,y\right)  \in X\times Y}q_{x}r_{y}  &  =\sum_{x\in
X}\underbrace{\sum_{y\in Y}q_{x}r_{y}}_{\substack{=q_{x}\sum_{y\in Y}%
r_{y}\\\text{(by (\ref{pf.lem.noncomm.prodrule.S.n=2.1}) (applied to }%
\lambda=q_{x}\text{))}}}=\sum_{x\in X}q_{x}\sum_{y\in Y}r_{y}\\
&  =\left(  \sum_{x\in X}q_{x}\right)  \left(  \sum_{y\in Y}r_{y}\right)
\end{align*}
(by (\ref{pf.lem.noncomm.prodrule.S.n=2.2}) (applied to $\lambda=\sum_{y\in
Y}r_{y}$)). This proves Lemma \ref{lem.prodrule.S.n=2}.
\end{proof}

Thus, Lemma \ref{lem.noncomm.prodrule2} is proven.
\end{proof}

Now, let us prove three further lemmas:

\begin{lemma}
\label{lem.sol.exeadd.noncomm.polarization.1}Let $\mathbb{L}$ be a
noncommutative ring. Let $n\in\mathbb{N}$. Let $v_{1},v_{2},\ldots,v_{n}$ be
$n$ elements of $\mathbb{L}$. Let $I$ be a subset of $\left[  n\right]  $.
Then,%
\[
\left(  \sum_{i\in I}v_{i}\right)  ^{m}=\sum_{\substack{f:\left[  m\right]
\rightarrow\left[  n\right]  ;\\f\left(  \left[  m\right]  \right)  \subseteq
I}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.1}.]First, observe
that%
\begin{align}
&  \sum_{k=1}^{n}\left[  k\in I\right]  v_{k}\nonumber\\
&  =\underbrace{\sum_{i=1}^{n}}_{\substack{=\sum_{i\in\left\{  1,2,\ldots
,n\right\}  }=\sum_{i\in\left[  n\right]  }\\\text{(since }\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  \text{)}}}\left[  i\in I\right]
v_{i}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have renamed the}\\
\text{summation index }i\text{ as }k
\end{array}
\right) \nonumber\\
&  =\sum_{i\in\left[  n\right]  }\left[  i\in I\right]  v_{i}\nonumber\\
&  =\underbrace{\sum_{\substack{i\in\left[  n\right]  ;\\i\in I}%
}}_{\substack{=\sum_{i\in I}\\\text{(since }I\subseteq\left[  n\right]
\text{)}}}\underbrace{\left[  i\in I\right]  }_{\substack{=1\\\text{(since
}i\in I\text{ is true)}}}v_{i}+\sum_{\substack{i\in\left[  n\right]
;\\i\notin I}}\underbrace{\left[  i\in I\right]  }_{\substack{=0\\\text{(since
}i\in I\text{ is false}\\\text{(because }i\notin I\text{))}}}v_{i}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since each }i\in\left[  n\right]  \text{
satisfies either }i\in I\text{ or }i\notin I\text{ (but not both)}\right)
\nonumber\\
&  =\sum_{i\in I}1v_{i}+\underbrace{\sum_{\substack{i\in\left[  n\right]
;\\i\notin I}}0v_{i}}_{=0}=\sum_{i\in I}1v_{i}=\sum_{i\in I}v_{i}.
\label{pf.lem.sol.exeadd.noncomm.polarization.1.2}%
\end{align}


But $p^{m}=\prod_{i=1}^{m}p$ for each $p\in\mathbb{L}$. Applying this to
$p=\sum_{k=1}^{n}\left[  k\in I\right]  v_{k}$, we find%
\[
\left(  \sum_{k=1}^{n}\left[  k\in I\right]  v_{k}\right)  ^{m}=\prod
_{i=1}^{m}\sum_{k=1}^{n}\left[  k\in I\right]  v_{k}=\sum_{\kappa:\left[
m\right]  \rightarrow\left[  n\right]  }\prod_{i=1}^{m}\left(  \left[
\kappa\left(  i\right)  \in I\right]  v_{\kappa\left(  i\right)  }\right)
\]
(by Lemma \ref{lem.noncomm.prodrule2} (applied to $\mathbb{L}$, $m$, $n$ and
$\left[  k\in I\right]  v_{k}$ instead of $\mathbb{K}$, $n$, $m$ and $p_{i,k}%
$)). Now,%
\begin{align}
\left(  \underbrace{\sum_{i\in I}v_{i}}_{\substack{=\sum_{k=1}^{n}\left[  k\in
I\right]  v_{k}\\\text{(by (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.2}%
))}}}\right)  ^{m}  &  =\left(  \sum_{k=1}^{n}\left[  k\in I\right]
v_{k}\right)  ^{m}\nonumber\\
&  =\sum_{\kappa:\left[  m\right]  \rightarrow\left[  n\right]  }\prod
_{i=1}^{m}\left(  \left[  \kappa\left(  i\right)  \in I\right]  v_{\kappa
\left(  i\right)  }\right) \nonumber\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{m}\left(  \left[  f\left(  i\right)  \in I\right]  v_{f\left(  i\right)
}\right)  \label{pf.lem.sol.exeadd.noncomm.polarization.1.4}%
\end{align}
(here, we have renamed the summation index $\kappa$ as $f$).

Now, fix any map $f:\left[  m\right]  \rightarrow\left[  n\right]  $. Then,%
\begin{align}
&  \prod_{i=1}^{m}\left(  \left[  f\left(  i\right)  \in I\right]  v_{f\left(
i\right)  }\right) \nonumber\\
&  =\left(  \left[  f\left(  1\right)  \in I\right]  v_{f\left(  1\right)
}\right)  \left(  \left[  f\left(  2\right)  \in I\right]  v_{f\left(
2\right)  }\right)  \cdots\left(  \left[  f\left(  m\right)  \in I\right]
v_{f\left(  m\right)  }\right)  .
\label{pf.lem.sol.exeadd.noncomm.polarization.1.6}%
\end{align}
The factors $\left[  f\left(  1\right)  \in I\right]  ,\left[  f\left(
2\right)  \in I\right]  ,\ldots,\left[  f\left(  m\right)  \in I\right]  $ on
the right hand side of this equality are integers, and therefore can be freely
moved within the product (even though $\mathbb{L}$ is not necessarily
commutative). In particular, we can move them to the front of the product.
Thus, we find%
\begin{align*}
&  \left(  \left[  f\left(  1\right)  \in I\right]  v_{f\left(  1\right)
}\right)  \left(  \left[  f\left(  2\right)  \in I\right]  v_{f\left(
2\right)  }\right)  \cdots\left(  \left[  f\left(  m\right)  \in I\right]
v_{f\left(  m\right)  }\right) \\
&  =\underbrace{\left(  \left[  f\left(  1\right)  \in I\right]  \left[
f\left(  2\right)  \in I\right]  \cdots\left[  f\left(  m\right)  \in
I\right]  \right)  }_{=\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]
}\left(  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }\right) \\
&  =\left(  \prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  \right)
\left(  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }\right)  .
\end{align*}
Hence, (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.6}) rewrites as%
\begin{equation}
\prod_{i=1}^{m}\left(  \left[  f\left(  i\right)  \in I\right]  v_{f\left(
i\right)  }\right)  =\left(  \prod_{i=1}^{m}\left[  f\left(  i\right)  \in
I\right]  \right)  \left(  v_{f\left(  1\right)  }v_{f\left(  2\right)
}\cdots v_{f\left(  m\right)  }\right)  .
\label{pf.lem.sol.exeadd.noncomm.polarization.1.6b}%
\end{equation}


\begin{vershort}
But
\begin{equation}
\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  =\left[  f\left(
\left[  m\right]  \right)  \subseteq I\right]
\label{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}):} We are in one of
the following two cases:
\par
\textit{Case 1:} We have $f\left(  \left[  m\right]  \right)  \subseteq I$.
\par
\textit{Case 2:} We don't have $f\left(  \left[  m\right]  \right)  \subseteq
I$.
\par
Let us first consider Case 1. In this case, we have $f\left(  \left[
m\right]  \right)  \subseteq I$. Thus, $\left[  f\left(  \left[  m\right]
\right)  \subseteq I\right]  =1$. But each $i\in\left\{  1,2,\ldots,m\right\}
$ satisfies $\left[  f\left(  i\right)  \in I\right]  =1$ (since $f\left(
\underbrace{i}_{\in\left[  m\right]  }\right)  \in f\left(  \left[  m\right]
\right)  \subseteq I$). Hence, $\prod_{i=1}^{m}\underbrace{\left[  f\left(
i\right)  \in I\right]  }_{=1}=\prod_{i=1}^{m}1=1$. Comparing this with
$\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]  =1$, we
obtain $\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  =\left[
f\left(  \left[  m\right]  \right)  \subseteq I\right]  $. Hence,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we don't have $f\left(  \left[
m\right]  \right)  \subseteq I$. In other words, not every $p\in\left[
m\right]  $ satisfies $f\left(  p\right)  \in I$. Thus, there exists some
$p\in\left[  m\right]  $ such that $f\left(  p\right)  \notin I$. Consider
this $p$. We have $\left[  f\left(  p\right)  \in I\right]  =0$ (since
$f\left(  p\right)  \notin I$). Thus, at least one factor of the product
$\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  $ equals $0$ (namely,
the factor for $i=p$ is $\left[  f\left(  p\right)  \in I\right]  =0$).
Consequently, the whole product $\prod_{i=1}^{m}\left[  f\left(  i\right)  \in
I\right]  $ equals $0$ (because if at least one factor of a product equals
$0$, then the whole product equals $0$). In other words, $\prod_{i=1}%
^{m}\left[  f\left(  i\right)  \in I\right]  =0$. Comparing this with $\left[
f\left(  \left[  m\right]  \right)  \subseteq I\right]  =0$ (which holds
because we don't have $f\left(  \left[  m\right]  \right)  \subseteq I$), we
obtain $\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  =\left[
f\left(  \left[  m\right]  \right)  \subseteq I\right]  $. Hence,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}) is proven in Case 2.
\par
We have now proven (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}) in
both Cases 1 and 2. This completes the proof of
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}).}. Thus,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.6b}) becomes%
\begin{align}
\prod_{i=1}^{m}\left(  \left[  f\left(  i\right)  \in I\right]  v_{f\left(
i\right)  }\right)   &  =\underbrace{\left(  \prod_{i=1}^{m}\left[  f\left(
i\right)  \in I\right]  \right)  }_{\substack{=\left[  f\left(  \left[
m\right]  \right)  \subseteq I\right]  \\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.7}))}}}\left(
v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}\right) \nonumber\\
&  =\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]  \left(
v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}\right)  . \label{pf.lem.sol.exeadd.noncomm.polarization.1.short.8}%
\end{align}

\end{vershort}

\begin{verlong}
But
\begin{equation}
\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  =\left[  f\left(
\left[  m\right]  \right)  \subseteq I\right]
\label{pf.lem.sol.exeadd.noncomm.polarization.1.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.7}%
):} We are in one of the following two cases:
\par
\textit{Case 1:} We have $f\left(  \left[  m\right]  \right)  \subseteq I$.
\par
\textit{Case 2:} We don't have $f\left(  \left[  m\right]  \right)  \subseteq
I$.
\par
Let us first consider Case 1. In this case, we have $f\left(  \left[
m\right]  \right)  \subseteq I$. Thus, $\left[  f\left(  \left[  m\right]
\right)  \subseteq I\right]  =1$. But each $i\in\left\{  1,2,\ldots,m\right\}
$ satisfies $\left[  f\left(  i\right)  \in I\right]  =1$ (since $f\left(
\underbrace{i}_{\in\left\{  1,2,\ldots,m\right\}  =\left[  m\right]  }\right)
\in f\left(  \left[  m\right]  \right)  \subseteq I$). Hence, $\prod_{i=1}%
^{m}\underbrace{\left[  f\left(  i\right)  \in I\right]  }_{=1}=\prod
_{i=1}^{m}1=1$. Comparing this with $\left[  f\left(  \left[  m\right]
\right)  \subseteq I\right]  =1$, we obtain $\prod_{i=1}^{m}\left[  f\left(
i\right)  \in I\right]  =\left[  f\left(  \left[  m\right]  \right)  \subseteq
I\right]  $. Hence, (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.7}) is
proven in Case 1.
\par
Let us now consider Case 2. In this case, we don't have $f\left(  \left[
m\right]  \right)  \subseteq I$. Thus, $\left[  f\left(  \left[  m\right]
\right)  \subseteq I\right]  =0$. But we have $f\left(  \left[  m\right]
\right)  \not \subseteq I$ (since we don't have $f\left(  \left[  m\right]
\right)  \subseteq I$). Hence, there exists some $q\in f\left(  \left[
m\right]  \right)  $ such that $q\notin I$. Consider this $q$. We have $q\in
f\left(  \left[  m\right]  \right)  $. Thus, there exists some $p\in\left[
m\right]  $ such that $q=f\left(  p\right)  $. Consider this $p$. We have
$f\left(  p\right)  =q\notin I$. In other words, we don't have $f\left(
p\right)  \in I$. Hence, $\left[  f\left(  p\right)  \in I\right]  =0$. But we
have $p\in\left[  m\right]  =\left\{  1,2,\ldots,m\right\}  $. Hence, $\left[
f\left(  p\right)  \in I\right]  $ is a factor of the product $\prod_{i=1}%
^{m}\left[  f\left(  i\right)  \in I\right]  $ (namely, the factor for $i=p$).
Moreover, this factor equals $0$ (since $\left[  f\left(  p\right)  \in
I\right]  =0$). Hence, at least one factor of the product $\prod_{i=1}%
^{m}\left[  f\left(  i\right)  \in I\right]  $ equals $0$ (namely, the factor
$\left[  f\left(  p\right)  \in I\right]  $). Consequently, the whole product
$\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]  $ equals $0$ (because
if at least one factor of a product equals $0$, then the whole product equals
$0$). In other words, $\prod_{i=1}^{m}\left[  f\left(  i\right)  \in I\right]
=0$. Comparing this with $\left[  f\left(  \left[  m\right]  \right)
\subseteq I\right]  =0$, we obtain $\prod_{i=1}^{m}\left[  f\left(  i\right)
\in I\right]  =\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]
$. Hence, (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.7}) is proven in Case
2.
\par
We have now proven (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.7}) in both
Cases 1 and 2. Since these are the only possible cases, we thus have always
proven (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.7}).}. Thus,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.6b}) becomes%
\begin{align}
\prod_{i=1}^{m}\left(  \left[  f\left(  i\right)  \in I\right]  v_{f\left(
i\right)  }\right)   &  =\underbrace{\left(  \prod_{i=1}^{m}\left[  f\left(
i\right)  \in I\right]  \right)  }_{\substack{=\left[  f\left(  \left[
m\right]  \right)  \subseteq I\right]  \\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.7}))}}}\left(  v_{f\left(
1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\right)
\nonumber\\
&  =\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]  \left(
v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}\right)  . \label{pf.lem.sol.exeadd.noncomm.polarization.1.8}%
\end{align}

\end{verlong}

\begin{vershort}
Now, forget that we fixed $f$. We thus have proven
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.8}) for each map
$f:\left[  m\right]  \rightarrow\left[  n\right]  $.
\end{vershort}

\begin{verlong}
Now, forget that we fixed $f$. We thus have proven
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.8}) for each map $f:\left[
m\right]  \rightarrow\left[  n\right]  $.
\end{verlong}

\begin{vershort}
Now, (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.4}) becomes%
\begin{align*}
\left(  \sum_{i\in I}v_{i}\right)  ^{m}  &  =\sum_{f:\left[  m\right]
\rightarrow\left[  n\right]  }\underbrace{\prod_{i=1}^{m}\left(  \left[
f\left(  i\right)  \in I\right]  v_{f\left(  i\right)  }\right)
}_{\substack{=\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]
\left(  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }\right)  \\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.short.8}))}}}\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\left[  f\left(
\left[  m\right]  \right)  \subseteq I\right]  \left(  v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\right) \\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}\underbrace{\left[
f\left(  \left[  m\right]  \right)  \subseteq I\right]  }%
_{\substack{=1\\\text{(since }f\left(  \left[  m\right]  \right)  \subseteq
I\text{)}}}\left(  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  m\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{f:\left[  m\right]  \rightarrow\left[
n\right]  ;\\\text{not }f\left(  \left[  m\right]  \right)  \subseteq
I}}\underbrace{\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]
}_{\substack{=0\\\text{(since we don't have }f\left(  \left[  m\right]
\right)  \subseteq I\text{)}}}\left(  v_{f\left(  1\right)  }v_{f\left(
2\right)  }\cdots v_{f\left(  m\right)  }\right) \\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }+\underbrace{\sum
_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]  ;\\\text{not
}f\left(  \left[  m\right]  \right)  \subseteq I}}0\left(  v_{f\left(
1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\right)
}_{=0}\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }.
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.1}.
\end{vershort}

\begin{verlong}
Now, (\ref{pf.lem.sol.exeadd.noncomm.polarization.1.4}) becomes%
\begin{align*}
\left(  \sum_{i\in I}v_{i}\right)  ^{m}  &  =\sum_{f:\left[  m\right]
\rightarrow\left[  n\right]  }\underbrace{\prod_{i=1}^{m}\left(  \left[
f\left(  i\right)  \in I\right]  v_{f\left(  i\right)  }\right)
}_{\substack{=\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]
\left(  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }\right)  \\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.1.8}))}}}\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\left[  f\left(
\left[  m\right]  \right)  \subseteq I\right]  \left(  v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\right) \\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}\underbrace{\left[
f\left(  \left[  m\right]  \right)  \subseteq I\right]  }%
_{\substack{=1\\\text{(since }f\left(  \left[  m\right]  \right)  \subseteq
I\text{)}}}\left(  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  m\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{f:\left[  m\right]  \rightarrow\left[
n\right]  ;\\\text{not }f\left(  \left[  m\right]  \right)  \subseteq
I}}\underbrace{\left[  f\left(  \left[  m\right]  \right)  \subseteq I\right]
}_{\substack{=0\\\text{(since we don't have }f\left(  \left[  m\right]
\right)  \subseteq I\text{)}}}\left(  v_{f\left(  1\right)  }v_{f\left(
2\right)  }\cdots v_{f\left(  m\right)  }\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each map }f:\left[  m\right]  \rightarrow\left[  n\right]  \text{
satisfies either }f\left(  \left[  m\right]  \right)  \subseteq I\\
\text{or }\left(  \text{not }f\left(  \left[  m\right]  \right)  \subseteq
I\right)  \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }+\underbrace{\sum
_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]  ;\\\text{not
}f\left(  \left[  m\right]  \right)  \subseteq I}}0\left(  v_{f\left(
1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\right)
}_{=0}\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }.
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.1}.
\end{verlong}
\end{proof}

\begin{lemma}
\label{lem.sol.exeadd.noncomm.polarization.2}Let $n\in\mathbb{N}$. Let $J$ be
a subset of $\left[  n\right]  $. Then,%
\[
\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }=\left[  J=\left[  n\right]  \right]
.
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.2}.]Here is a proof
that makes use of Exercise \ref{exe.prod(ai+bi)}. It is not the simplest
possible proof, but it might be the shortest.\footnote{We are working in the
commutative ring $\mathbb{Z}$ in this proof. Hence, product signs such as
$\prod_{i\in I}$ and $\prod_{i\in\left[  n\right]  \setminus I}$ make sense.}

We first make the following observations:

\begin{itemize}
\item If $I$ is a subset of $\left[  n\right]  $ satisfying $J\subseteq I$,
then%
\begin{equation}
\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  =1
\label{pf.lem.sol.exeadd.noncomm.polarization.2.short.1}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.1}):} Let $I$ be a subset
of $\left[  n\right]  $ satisfying $J\subseteq I$.
\par
From $J\subseteq I$, we obtain $\left[  n\right]  \setminus I\subseteq\left[
n\right]  \setminus J$. Thus, each $i\in\left[  n\right]  \setminus I$
satisfies $i\in\left[  n\right]  \setminus I\subseteq\left[  n\right]
\setminus J$ and therefore $i\notin J$. Hence, each $i\in\left[  n\right]
\setminus I$ satisfies $\left[  i\notin J\right]  =1$. Therefore, $\prod
_{i\in\left[  n\right]  \setminus I}\underbrace{\left[  i\notin J\right]
}_{=1}=\prod_{i\in\left[  n\right]  \setminus I}1=1$. This proves
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.1}).}.

\item If $I$ is a subset of $\left[  n\right]  $ that does \textbf{not}
satisfy $J\subseteq I$, then
\begin{equation}
\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  =0
\label{pf.lem.sol.exeadd.noncomm.polarization.2.short.2}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.2}):} Let $I$ be a subset
of $\left[  n\right]  $ that does \textbf{not} satisfy $J\subseteq I$.
\par
We have $J\not \subseteq I$ (since we do not have $J\subseteq I$). Hence,
there exists some $j\in J$ such that $j\notin I$. Consider this $j$. We have
$\left[  j\notin J\right]  =0$ (since $j\in J$). Combining $j\in
J\subseteq\left[  n\right]  $ with $j\notin I$, we obtain $j\in\left[
n\right]  \setminus I$. Hence, at least one factor of the product $\prod
_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  $ is $0$
(namely, the factor for $i=j$ is $\left[  j\notin J\right]  =0$). Thus, the
whole product $\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin
J\right]  $ must be $0$ (because if a factor of a product is $0$, then the
whole product must be $0$). This proves
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.2}).}.
\end{itemize}

But Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} (applied to $\mathbb{Z}$, $1$
and $-\left[  i\notin J\right]  $ instead of $\mathbb{K}$, $a_{i}$ and $b_{i}%
$) yields%
\begin{align}
&  \prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)
\right) \nonumber\\
&  =\sum_{I\subseteq\left[  n\right]  }\underbrace{\left(  \prod_{i\in
I}1\right)  }_{=1}\underbrace{\left(  \prod_{i\in\left[  n\right]  \setminus
I}\left(  -\left[  i\notin J\right]  \right)  \right)  }_{=\left(  -1\right)
^{\left\vert \left[  n\right]  \setminus I\right\vert }\prod_{i\in\left[
n\right]  \setminus I}\left[  i\notin J\right]  }\nonumber\\
&  =\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{\left\vert \left[
n\right]  \setminus I\right\vert }\prod_{i\in\left[  n\right]  \setminus
I}\left[  i\notin J\right] \nonumber\\
&  =\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}%
}\underbrace{\left(  -1\right)  ^{\left\vert \left[  n\right]  \setminus
I\right\vert }}_{\substack{=\left(  -1\right)  ^{n-\left\vert I\right\vert
}\\\text{(since }\left\vert \left[  n\right]  \setminus I\right\vert
=n-\left\vert I\right\vert \\\text{(since }I\subseteq\left[  n\right]
\text{))}}}\underbrace{\prod_{i\in\left[  n\right]  \setminus I}\left[
i\notin J\right]  }_{\substack{=1\\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.1}))}}}+\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\text{not }J\subseteq I}}\left(
-1\right)  ^{\left\vert \left[  n\right]  \setminus I\right\vert
}\underbrace{\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin
J\right]  }_{\substack{=0\\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.2}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each subset }I\text{ of }\left[  n\right]  \text{ satisfies either
}J\subseteq I\text{ or }\left(  \text{not }J\subseteq I\right)  \text{,}\\
\text{but not both}%
\end{array}
\right) \nonumber\\
&  =\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }+\underbrace{\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\text{not }J\subseteq I}}\left(
-1\right)  ^{\left\vert \left[  n\right]  \setminus I\right\vert }0}_{=0}%
=\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }.
\label{pf.lem.sol.exeadd.noncomm.polarization.2.short.8}%
\end{align}


On the other hand, it is easy to see that%
\begin{equation}
\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)  \right)
=\left[  J=\left[  n\right]  \right]
\label{pf.lem.sol.exeadd.noncomm.polarization.2.short.9}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.9}):} We are in one of
the following two cases:
\par
\textit{Case 1:} We have $J=\left[  n\right]  $.
\par
\textit{Case 2:} We don't have $J=\left[  n\right]  $.
\par
Let us first consider Case 1. In this case, we have $J=\left[  n\right]  $.
Thus, $\left[  J=\left[  n\right]  \right]  =1$. But each $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies $i\in\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  =J$ and therefore $\left[  i\notin J\right]  =0$. Hence,%
\[
\prod_{i=1}^{n}\left(  1+\left(  -\underbrace{\left[  i\notin J\right]  }%
_{=0}\right)  \right)  =\prod_{i=1}^{n}\underbrace{\left(  1+\left(
-0\right)  \right)  }_{=1}=\prod_{i=1}^{n}1=1.
\]
Comparing this with $\left[  J=\left[  n\right]  \right]  =1$, we obtain
$\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)  \right)
=\left[  J=\left[  n\right]  \right]  $. Hence,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.9}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we don't have $J=\left[  n\right]
$. Hence, $J\neq\left[  n\right]  $. Thus, $J$ is a \textbf{proper} subset of
$\left[  n\right]  $ (since $J\subseteq\left[  n\right]  $). Therefore, there
exists some $j\in\left[  n\right]  $ such that $j\notin J$. Consider this $j$.
We have $\left[  j\notin J\right]  =1$ (since $j\notin J$). But $j\in\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $. Thus, at least one factor of the
product $\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)
\right)  $ equals $0$ (namely, the factor for $i=j$ is $1+\left(
-\underbrace{\left[  j\notin J\right]  }_{=1}\right)  =1+\left(  -1\right)
=0$). Consequently, the whole product $\prod_{i=1}^{n}\left(  1+\left(
-\left[  i\notin J\right]  \right)  \right)  $ equals $0$ (because if at least
one factor of a product equals $0$, then the whole product equals $0$). In
other words, $\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]
\right)  \right)  =0$. Comparing this with $\left[  J=\left[  n\right]
\right]  =0$ (which holds because we don't have $J=\left[  n\right]  $), we
obtain $\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)
\right)  =\left[  J=\left[  n\right]  \right]  $. Hence,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.9}) is proven in Case 2.
\par
We have now proven (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.9}) in
both Cases 1 and 2. Thus,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.9}) is proven.}.
Comparing this with (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.short.8}),
we obtain%
\[
\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }=\left[  J=\left[  n\right]  \right]
.
\]
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.2}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.2}.]There are several
ways to prove Lemma \ref{lem.sol.exeadd.noncomm.polarization.2} (in
particular, there exist some very simple combinatorial proofs). Let us give
one proof (which might not be the easiest one):\footnote{We are working in the
commutative ring $\mathbb{Z}$ in this proof. Hence, product signs such as
$\prod_{i\in I}$ and $\prod_{i\in\left[  n\right]  \setminus I}$ make sense.}

We first make the following observations:

\begin{itemize}
\item If $I$ is a subset of $\left[  n\right]  $ satisfying $J\subseteq I$,
then%
\begin{equation}
\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  =1
\label{pf.lem.sol.exeadd.noncomm.polarization.2.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.1}%
):} Let $I$ be a subset of $\left[  n\right]  $ satisfying $J\subseteq I$.
\par
From $J\subseteq I$, we obtain $\left[  n\right]  \setminus J\supseteq\left[
n\right]  \setminus I$. In other words, $\left[  n\right]  \setminus
I\subseteq\left[  n\right]  \setminus J$. Thus, each $i\in\left[  n\right]
\setminus I$ satisfies $i\in\left[  n\right]  \setminus I\subseteq\left[
n\right]  \setminus J$ and therefore $i\notin J$. Hence, each $i\in\left[
n\right]  \setminus I$ satisfies $\left[  i\notin J\right]  =1$ (since
$i\notin J$). Therefore, $\prod_{i\in\left[  n\right]  \setminus
I}\underbrace{\left[  i\notin J\right]  }_{=1}=\prod_{i\in\left[  n\right]
\setminus I}1=1$. This proves
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.1}).}.

\item If $I$ is a subset of $\left[  n\right]  $ that does \textbf{not}
satisfy $J\subseteq I$, then
\begin{equation}
\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  =0
\label{pf.lem.sol.exeadd.noncomm.polarization.2.2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.2}%
):} Let $I$ be a subset of $\left[  n\right]  $ that does \textbf{not} satisfy
$J\subseteq I$.
\par
We have $J\not \subseteq I$ (since we do not have $J\subseteq I$). Hence,
there exists some $j\in J$ such that $j\notin I$. Consider this $j$. We have
$\left[  j\notin J\right]  =0$ (since $j\notin J$ is false (since $j\in J$)).
Combining $j\in J\subseteq\left[  n\right]  $ with $j\notin I$, we obtain
$j\in\left[  n\right]  \setminus I$. Hence, $\left[  j\notin J\right]  $ is a
factor of the product $\prod_{i\in\left[  n\right]  \setminus I}\left[
i\notin J\right]  $ (namely, the factor for $i=j$). This factor is $0$ (since
$\left[  j\notin J\right]  =0$). Therefore, at least one factor of the product
$\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  $ is $0$
(namely, the factor $\left[  j\notin J\right]  $). Thus, the whole product
$\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin J\right]  $ must be
$0$ (because if a factor of a product is $0$, then the whole product must be
$0$). In other words, $\prod_{i\in\left[  n\right]  \setminus I}\left[
i\notin J\right]  =0$. This proves
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.2}).}.

\item If $I$ is a subset of $\left[  n\right]  $, then%
\begin{align*}
\left\vert \left[  n\right]  \setminus I\right\vert  &
=\underbrace{\left\vert \left[  n\right]  \right\vert }_{=n}-\left\vert
I\right\vert \ \ \ \ \ \ \ \ \ \ \left(  \text{since }I\subseteq\left[
n\right]  \right) \\
&  =n-\left\vert I\right\vert
\end{align*}
and%
\begin{align}
\prod_{i\in\left[  n\right]  \setminus I}\left(  -\left[  i\notin J\right]
\right)   &  =\underbrace{\left(  -1\right)  ^{\left\vert \left[  n\right]
\setminus I\right\vert }}_{\substack{=\left(  -1\right)  ^{n-\left\vert
I\right\vert }\\\text{(since }\left\vert \left[  n\right]  \setminus
I\right\vert =n-\left\vert I\right\vert \text{)}}}\prod_{i\in\left[  n\right]
\setminus I}\left[  i\notin J\right] \nonumber\\
&  =\left(  -1\right)  ^{n-\left\vert I\right\vert }\prod_{i\in\left[
n\right]  \setminus I}\left[  i\notin J\right]  .
\label{pf.lem.sol.exeadd.noncomm.polarization.2.3}%
\end{align}

\end{itemize}

But Exercise \ref{exe.prod(ai+bi)} \textbf{(a)} (applied to $\mathbb{Z}$, $1$
and $-\left[  i\notin J\right]  $ instead of $\mathbb{K}$, $a_{i}$ and $b_{i}%
$) yields%
\begin{align}
&  \prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)
\right) \nonumber\\
&  =\sum_{I\subseteq\left[  n\right]  }\underbrace{\left(  \prod_{i\in
I}1\right)  }_{=1}\underbrace{\left(  \prod_{i\in\left[  n\right]  \setminus
I}\left(  -\left[  i\notin J\right]  \right)  \right)  }_{\substack{=\left(
-1\right)  ^{n-\left\vert I\right\vert }\prod_{i\in\left[  n\right]  \setminus
I}\left[  i\notin J\right]  \\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.3}))}}}\nonumber\\
&  =\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\prod_{i\in\left[  n\right]  \setminus I}\left[  i\notin
J\right] \nonumber\\
&  =\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }\underbrace{\prod_{i\in\left[
n\right]  \setminus I}\left[  i\notin J\right]  }_{\substack{=1\\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.1}))}}}+\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\text{not }J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }\underbrace{\prod_{i\in\left[
n\right]  \setminus I}\left[  i\notin J\right]  }_{\substack{=0\\\text{(by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.2}))}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each subset }I\text{ of }\left[  n\right]  \text{ satisfies either
}J\subseteq I\text{ or }\left(  \text{not }J\subseteq I\right)  \text{,}\\
\text{but not both}%
\end{array}
\right) \nonumber\\
&  =\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }1+\underbrace{\sum
_{\substack{I\subseteq\left[  n\right]  ;\\\text{not }J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }0}_{=0}=\sum_{\substack{I\subseteq
\left[  n\right]  ;\\J\subseteq I}}\left(  -1\right)  ^{n-\left\vert
I\right\vert }1\nonumber\\
&  =\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }.
\label{pf.lem.sol.exeadd.noncomm.polarization.2.8}%
\end{align}


On the other hand, it is easy to see that%
\begin{equation}
\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)  \right)
=\left[  J=\left[  n\right]  \right]
\label{pf.lem.sol.exeadd.noncomm.polarization.2.9}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.9}%
):} We are in one of the following two cases:
\par
\textit{Case 1:} We have $J=\left[  n\right]  $.
\par
\textit{Case 2:} We don't have $J=\left[  n\right]  $.
\par
Let us first consider Case 1. In this case, we have $J=\left[  n\right]  $.
Thus, $\left[  J=\left[  n\right]  \right]  =1$. But each $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies $i\in\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  =J$ and therefore $\left[  i\notin J\right]  =0$ (since $i\notin J$
is false (since we have $i\in J$)). Hence,%
\[
\prod_{i=1}^{n}\left(  1+\left(  -\underbrace{\left[  i\notin J\right]  }%
_{=0}\right)  \right)  =\prod_{i=1}^{n}\underbrace{\left(  1+\left(
-0\right)  \right)  }_{=1}=\prod_{i=1}^{n}1=1.
\]
Comparing this with $\left[  J=\left[  n\right]  \right]  =1$, we obtain
$\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)  \right)
=\left[  J=\left[  n\right]  \right]  $. Hence,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.9}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we don't have $J=\left[  n\right]
$. Hence, $J\neq\left[  n\right]  $. Thus, $J$ is a \textbf{proper} subset of
$\left[  n\right]  $ (since $J\subseteq\left[  n\right]  $). Therefore, there
exists some $j\in\left[  n\right]  $ such that $j\notin J$. Consider this $j$.
We have $\left[  j\notin J\right]  =1$ (since $j\notin J$). But $j\in\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $. Thus, $1+\left(  -\left[  j\notin
J\right]  \right)  $ is a factor of the product $\prod_{i=1}^{n}\left(
1+\left(  -\left[  i\notin J\right]  \right)  \right)  $ (namely, the factor
for $i=j$). Moreover, this factor equals $0$ (since $1+\left(
-\underbrace{\left[  j\notin J\right]  }_{=1}\right)  =1+\left(  -1\right)
=0$). Hence, at least one factor of the product $\prod_{i=1}^{n}\left(
1+\left(  -\left[  i\notin J\right]  \right)  \right)  $ equals $0$ (namely,
the factor $1+\left(  -\left[  j\notin J\right]  \right)  $). Consequently,
the whole product $\prod_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]
\right)  \right)  $ equals $0$ (because if at least one factor of a product
equals $0$, then the whole product equals $0$). In other words, $\prod
_{i=1}^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)  \right)  =0$.
Comparing this with $\left[  J=\left[  n\right]  \right]  =0$ (which holds
because we don't have $J=\left[  n\right]  $), we obtain $\prod_{i=1}%
^{n}\left(  1+\left(  -\left[  i\notin J\right]  \right)  \right)  =\left[
J=\left[  n\right]  \right]  $. Hence,
(\ref{pf.lem.sol.exeadd.noncomm.polarization.2.9}) is proven in Case 2.
\par
We have now proven (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.9}) in both
Cases 1 and 2. Since these are the only possible cases, we thus have always
proven (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.9}).}. Comparing this
with (\ref{pf.lem.sol.exeadd.noncomm.polarization.2.8}), we obtain%
\[
\sum_{\substack{I\subseteq\left[  n\right]  ;\\J\subseteq I}}\left(
-1\right)  ^{n-\left\vert I\right\vert }=\left[  J=\left[  n\right]  \right]
.
\]
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.2}.
\end{proof}
\end{verlong}

The following lemma is a well-known and basic property of finite sets:

\begin{lemma}
\label{lem.sol.exeadd.noncomm.polarization.pigeon-inj}Let $U$ and $V$ be two
finite sets. Let $f:U\rightarrow V$ be a map.

\textbf{(a)} We have $\left\vert f\left(  S\right)  \right\vert \leq\left\vert
S\right\vert $ for each subset $S$ of $U$.

\textbf{(b)} Assume that $\left\vert f\left(  U\right)  \right\vert
\geq\left\vert U\right\vert $. Then, the map $f$ is injective.

\textbf{(c)} If $f$ is injective, then $\left\vert f\left(  S\right)
\right\vert =\left\vert S\right\vert $ for each subset $S$ of $U$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}%
.]\textbf{(a)} Let $S$ be a subset of $U$. We must prove that $\left\vert
f\left(  S\right)  \right\vert \leq\left\vert S\right\vert $.

Let $\left(  s_{1},s_{2},\ldots,s_{k}\right)  $ be a list of all elements of
$S$ (with no repetitions).\footnote{Such a list exists, since the set $S$ is
finite.} Thus, $\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  =S$ and
$k=\left\vert S\right\vert $. Now,%
\[
f\left(  \underbrace{S}_{=\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  }\right)
=f\left(  \left\{  s_{1},s_{2},\ldots,s_{k}\right\}  \right)  =\left\{
f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)
\right\}  .
\]
Hence,%
\begin{align*}
\left\vert f\left(  S\right)  \right\vert  &  =\left\vert \left\{  f\left(
s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}
\right\vert \\
&  \leq k\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since there are at most }k\text{ distinct elements}\\
\text{among }f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(
s_{k}\right)
\end{array}
\right) \\
&  =\left\vert S\right\vert .
\end{align*}


This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}
\textbf{(a)}.

\textbf{(b)} Let $u$ and $v$ be two elements of $U$ satisfying $f\left(
u\right)  =f\left(  v\right)  $. We shall prove that $u=v$.

Indeed, assume the contrary. Thus, $u\neq v$. Hence, $u\in U\setminus\left\{
v\right\}  $. But $v\in U$ and therefore $\left\vert U\setminus\left\{
v\right\}  \right\vert =\left\vert U\right\vert -1$.

Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(a)}
(applied to $S=U\setminus\left\{  v\right\}  $) shows that $\left\vert
f\left(  U\setminus\left\{  v\right\}  \right)  \right\vert \leq\left\vert
U\setminus\left\{  v\right\}  \right\vert =\left\vert U\right\vert -1$.

Next, I claim that $q\in f\left(  U\setminus\left\{  v\right\}  \right)  $ for
each $q\in f\left(  U\right)  $.

Indeed, let $q\in f\left(  U\right)  $ be arbitrary. We want to show that
$q\in f\left(  U\setminus\left\{  v\right\}  \right)  $.

We know that $q\in f\left(  U\right)  $. Hence, there exists some $p\in U$
satisfying $q=f\left(  p\right)  $. Consider this $p$. If $p=v$, then%
\[
q=f\left(  \underbrace{p}_{=v}\right)  =f\left(  v\right)  =f\left(
\underbrace{u}_{\in U\setminus\left\{  v\right\}  }\right)  \in f\left(
U\setminus\left\{  v\right\}  \right)  .
\]
Hence, if $p=v$, then $q\in f\left(  U\setminus\left\{  v\right\}  \right)  $
is proven. Thus, for the rest of the proof of $q\in f\left(  U\setminus
\left\{  v\right\}  \right)  $, we WLOG assume that $p\neq v$.

Hence, $p\in U\setminus\left\{  v\right\}  $. Now, $q=f\left(  \underbrace{p}%
_{\in U\setminus\left\{  v\right\}  }\right)  \in f\left(  U\setminus\left\{
v\right\}  \right)  $. This completes the proof of $q\in f\left(
U\setminus\left\{  v\right\}  \right)  $.

Now, forget that we fixed $q$. We thus have shown that $q\in f\left(
U\setminus\left\{  v\right\}  \right)  $ for each $q\in f\left(  U\right)  $.
In other words, $f\left(  U\right)  \subseteq f\left(  U\setminus\left\{
v\right\}  \right)  $. Hence, $\left\vert f\left(  U\right)  \right\vert
\leq\left\vert f\left(  U\setminus\left\{  v\right\}  \right)  \right\vert
\leq\left\vert U\right\vert -1<\left\vert U\right\vert $. Thus, $\left\vert
U\right\vert >\left\vert f\left(  U\right)  \right\vert \geq\left\vert
U\right\vert $. This is absurd. This contradiction shows that our assumption
was false. Thus, $u=v$ is proven.

Now, forget that we fixed $u$ and $v$. We thus have shown that if $u$ and $v$
are two elements of $U$ satisfying $f\left(  u\right)  =f\left(  v\right)  $,
then $u=v$. In other words, the map $f$ is injective. This proves Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(b)}.

\textbf{(c)} Assume that $f$ is injective. Let $S$ be a subset of $U$. We must
prove that $\left\vert f\left(  S\right)  \right\vert =\left\vert S\right\vert
$.

Let $\left(  s_{1},s_{2},\ldots,s_{k}\right)  $ be a list of all elements of
$S$ (with no repetitions).\footnote{Such a list exists, since the set $S$ is
finite.} Thus, $\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  =S$ and
$k=\left\vert S\right\vert $. Furthermore, the elements $s_{1},s_{2}%
,\ldots,s_{k}$ are pairwise distinct (since $\left(  s_{1},s_{2},\ldots
,s_{k}\right)  $ is a list with no repetitions). In other words,%
\[
s_{i}\neq s_{j}\ \ \ \ \ \ \ \ \ \ \text{for any two distinct elements
}i\text{ and }j\text{ of }\left\{  1,2,\ldots,k\right\}  .
\]
Thus,%
\[
f\left(  s_{i}\right)  \neq f\left(  s_{j}\right)
\ \ \ \ \ \ \ \ \ \ \text{for any two distinct elements }i\text{ and }j\text{
of }\left\{  1,2,\ldots,k\right\}
\]
(since the map $f$ is injective, and thus $f\left(  s_{i}\right)  \neq
f\left(  s_{j}\right)  $ follows from $s_{i}\neq s_{j}$). In other words, the
$k$ elements $f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(
s_{k}\right)  $ are pairwise distinct. Hence, $\left\vert \left\{  f\left(
s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}
\right\vert =k$.

Now,%
\[
f\left(  \underbrace{S}_{=\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  }\right)
=f\left(  \left\{  s_{1},s_{2},\ldots,s_{k}\right\}  \right)  =\left\{
f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)
\right\}  .
\]
Hence,%
\[
\left\vert \underbrace{f\left(  S\right)  }_{=\left\{  f\left(  s_{1}\right)
,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}  }\right\vert
=\left\vert \left\{  f\left(  s_{1}\right)  ,f\left(  s_{2}\right)
,\ldots,f\left(  s_{k}\right)  \right\}  \right\vert =k=\left\vert
S\right\vert .
\]
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}
\textbf{(c)}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}%
.]\textbf{(a)} Let $S$ be a subset of $U$. We must prove that $\left\vert
f\left(  S\right)  \right\vert \leq\left\vert S\right\vert $.

The set $S$ is finite (since it is a subset of the finite set $U$). Let
$\left(  s_{1},s_{2},\ldots,s_{k}\right)  $ be a list of all elements of $S$
(with no repetitions).\footnote{Such a list exists, since the set $S$ is
finite.} Thus, $\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  =S$ and
$k=\left\vert S\right\vert $. Now,%
\begin{equation}
f\left(  \underbrace{S}_{=\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  }\right)
=f\left(  \left\{  s_{1},s_{2},\ldots,s_{k}\right\}  \right)  =\left\{
f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)
\right\}  . \label{pf.lem.sol.exeadd.noncomm.polarization.pigeon-inj.a.1}%
\end{equation}


But the elements of the set $\left\{  f\left(  s_{1}\right)  ,f\left(
s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}  $ are the $k$ elements
\newline$f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(
s_{k}\right)  $, which may and may not be distinct; in either case, there are
at most $k$ distinct elements among them. Hence, the set $\left\{  f\left(
s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}
$ has at most $k$ elements. In other words, $\left\vert \left\{  f\left(
s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}
\right\vert \leq k$. In light of
(\ref{pf.lem.sol.exeadd.noncomm.polarization.pigeon-inj.a.1}), this rewrites
as $\left\vert f\left(  S\right)  \right\vert \leq k$. Thus, $\left\vert
f\left(  S\right)  \right\vert \leq k=\left\vert S\right\vert $. This proves
Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(a)}.

\textbf{(b)} Let $u$ and $v$ be two elements of $U$ satisfying $f\left(
u\right)  =f\left(  v\right)  $. We shall prove that $u=v$.

Indeed, assume the contrary. Thus, $u\neq v$. Hence, $u\in U\setminus\left\{
v\right\}  $ (since $u\in U$ and $u\neq v$). But $v\in U$ and therefore
$\left\vert U\setminus\left\{  v\right\}  \right\vert =\left\vert U\right\vert
-1$.

Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(a)}
(applied to $S=U\setminus\left\{  v\right\}  $) shows that $\left\vert
f\left(  U\setminus\left\{  v\right\}  \right)  \right\vert \leq\left\vert
U\setminus\left\{  v\right\}  \right\vert =\left\vert U\right\vert -1$.

Next, I claim that $q\in f\left(  U\setminus\left\{  v\right\}  \right)  $ for
each $q\in f\left(  U\right)  $.

Indeed, let $q\in f\left(  U\right)  $ be arbitrary. We want to show that
$q\in f\left(  U\setminus\left\{  v\right\}  \right)  $.

We know that $q\in f\left(  U\right)  $. Hence, there exists some $p\in U$
satisfying $q=f\left(  p\right)  $. Consider this $p$. If $p=v$, then%
\[
q=f\left(  \underbrace{p}_{=v}\right)  =f\left(  v\right)  =f\left(
\underbrace{u}_{\in U\setminus\left\{  v\right\}  }\right)  \in f\left(
U\setminus\left\{  v\right\}  \right)  .
\]
Hence, if $p=v$, then $q\in f\left(  U\setminus\left\{  v\right\}  \right)  $
is proven. Thus, for the rest of the proof of $q\in f\left(  U\setminus
\left\{  v\right\}  \right)  $, we can WLOG assume that we don't have $p=v$.
Assume this.

We have $p\neq v$ (since we don't have $p=v$). Combining this with $p\in U$,
we obtain $p\in U\setminus\left\{  v\right\}  $. Now, $q=f\left(
\underbrace{p}_{\in U\setminus\left\{  v\right\}  }\right)  \in f\left(
U\setminus\left\{  v\right\}  \right)  $. This completes the proof of $q\in
f\left(  U\setminus\left\{  v\right\}  \right)  $.

Now, forget that we fixed $q$. We thus have shown that $q\in f\left(
U\setminus\left\{  v\right\}  \right)  $ for each $q\in f\left(  U\right)  $.
In other words, $f\left(  U\right)  \subseteq f\left(  U\setminus\left\{
v\right\}  \right)  $. Hence, $\left\vert f\left(  U\right)  \right\vert
\leq\left\vert f\left(  U\setminus\left\{  v\right\}  \right)  \right\vert
\leq\left\vert U\right\vert -1<\left\vert U\right\vert $. Thus, $\left\vert
U\right\vert >\left\vert f\left(  U\right)  \right\vert \geq\left\vert
U\right\vert $. This is absurd. Hence, we have found a contradiction. This
contradiction shows that our assumption was false. Thus, $u=v$ is proven.

Now, forget that we fixed $u$ and $v$. We thus have shown that if $u$ and $v$
are two elements of $U$ satisfying $f\left(  u\right)  =f\left(  v\right)  $,
then $u=v$. In other words, the map $f$ is injective. This proves Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(b)}.

\textbf{(c)} Assume that $f$ is injective. Let $S$ be a subset of $U$. We must
prove that $\left\vert f\left(  S\right)  \right\vert =\left\vert S\right\vert
$.

The set $S$ is finite (since it is a subset of the finite set $U$). Let
$\left(  s_{1},s_{2},\ldots,s_{k}\right)  $ be a list of all elements of $S$
(with no repetitions).\footnote{Such a list exists, since the set $S$ is
finite.} Thus, $\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  =S$ and
$k=\left\vert S\right\vert $. Furthermore, the elements $s_{1},s_{2}%
,\ldots,s_{k}$ are pairwise distinct (since $\left(  s_{1},s_{2},\ldots
,s_{k}\right)  $ is a list with no repetitions). In other words,%
\begin{equation}
s_{i}\neq s_{j}\ \ \ \ \ \ \ \ \ \ \text{for any two distinct elements
}i\text{ and }j\text{ of }\left\{  1,2,\ldots,k\right\}  .
\label{pf.lem.sol.exeadd.noncomm.polarization.pigeon-inj.c.1}%
\end{equation}


Now, we have%
\[
f\left(  s_{i}\right)  \neq f\left(  s_{j}\right)
\ \ \ \ \ \ \ \ \ \ \text{for any two distinct elements }i\text{ and }j\text{
of }\left\{  1,2,\ldots,k\right\}
\]
\footnote{\textit{Proof.} Let $i$ and $j$ be two distinct elements of
$\left\{  1,2,\ldots,k\right\}  $. Then, $s_{i}\neq s_{j}$ (by
(\ref{pf.lem.sol.exeadd.noncomm.polarization.pigeon-inj.c.1})). Assume (for
the sake of contradiction) that $f\left(  s_{i}\right)  =f\left(
s_{j}\right)  $.
\par
Recall that the map $f$ is injective. In other words, any two elements $u$ and
$v$ of $U$ satisfying $f\left(  u\right)  =f\left(  v\right)  $ must satisfy
$u=v$. Applying this to $u=s_{i}$ and $v=s_{j}$, we obtain $s_{i}=s_{j}$
(since $f\left(  s_{i}\right)  =f\left(  s_{j}\right)  $). This contradicts
$s_{i}\neq s_{j}$.
\par
This contradiction shows that our assumption (that $f\left(  s_{i}\right)
=f\left(  s_{j}\right)  $) was wrong. Hence, we cannot have $f\left(
s_{i}\right)  =f\left(  s_{j}\right)  $. In other words, we have $f\left(
s_{i}\right)  \neq f\left(  s_{j}\right)  $. Qed.}. In other words, the $k$
elements $f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(
s_{k}\right)  $ are pairwise distinct. Hence, $\left\vert \left\{  f\left(
s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}
\right\vert =k$.

Now,%
\[
f\left(  \underbrace{S}_{=\left\{  s_{1},s_{2},\ldots,s_{k}\right\}  }\right)
=f\left(  \left\{  s_{1},s_{2},\ldots,s_{k}\right\}  \right)  =\left\{
f\left(  s_{1}\right)  ,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)
\right\}  .
\]
Hence,%
\[
\left\vert \underbrace{f\left(  S\right)  }_{=\left\{  f\left(  s_{1}\right)
,f\left(  s_{2}\right)  ,\ldots,f\left(  s_{k}\right)  \right\}  }\right\vert
=\left\vert \left\{  f\left(  s_{1}\right)  ,f\left(  s_{2}\right)
,\ldots,f\left(  s_{k}\right)  \right\}  \right\vert =k=\left\vert
S\right\vert .
\]
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}
\textbf{(c)}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exeadd.noncomm.polarization.pigeon}Let $U$ and $V$ be two
finite sets such that $\left\vert U\right\vert =\left\vert V\right\vert $. Let
$f:U\rightarrow V$ be a map. Then, we have the following logical equivalence:%
\[
\left(  f\text{ is surjective}\right)  \ \Longleftrightarrow\ \left(  f\text{
is bijective}\right)  .
\]

\end{lemma}

Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon} is well-known; let us
nevertheless give its proof.

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon}.]Assume that
$f$ is surjective. Thus, $f\left(  U\right)  =V$. Hence, $\left\vert f\left(
U\right)  \right\vert =\left\vert V\right\vert =\left\vert U\right\vert $
(since $\left\vert U\right\vert =\left\vert V\right\vert $), so that
$\left\vert f\left(  U\right)  \right\vert =\left\vert U\right\vert
\geq\left\vert U\right\vert $. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(b)} shows that
the map $f$ is injective. Since $f$ is both surjective and injective, we see
that $f$ is bijective.

Now, forget that we have assumed that $f$ is surjective. We thus have shown
that if $f$ is surjective, then $f$ is bijective. Of course, the converse also
holds: If $f$ is bijective, then $f$ is surjective. Hence, $f$ is surjective
if and only if $f$ is bijective. This proves Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon}.]Assume that
$f$ is surjective. Thus, $f\left(  U\right)  =V$. Hence, $\left\vert f\left(
U\right)  \right\vert =\left\vert V\right\vert =\left\vert U\right\vert $
(since $\left\vert U\right\vert =\left\vert V\right\vert $), so that
$\left\vert f\left(  U\right)  \right\vert =\left\vert U\right\vert
\geq\left\vert U\right\vert $. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(b)} shows that
the map $f$ is injective. Since $f$ is both surjective and injective, we see
that $f$ is bijective.

Now, forget that we have assumed that $f$ is surjective. We thus have shown
that if $f$ is surjective, then $f$ is bijective. In other words, we have
proven the following implication:%
\begin{equation}
\left(  f\text{ is surjective}\right)  \ \Longrightarrow\ \left(  f\text{ is
bijective}\right)  . \label{sol.exeadd.noncomm.polarization.c.fact1.1}%
\end{equation}
On the other hand, each bijective map is surjective. Hence, we have the
following implication:%
\[
\left(  f\text{ is bijective}\right)  \ \Longrightarrow\ \left(  f\text{ is
surjective}\right)  .
\]
Combining this implication with
(\ref{sol.exeadd.noncomm.polarization.c.fact1.1}), we obtain the logical
equivalence%
\[
\left(  f\text{ is surjective}\right)  \ \Longleftrightarrow\ \left(  f\text{
is bijective}\right)  .
\]
This proves Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon}.
\end{proof}
\end{verlong}

\begin{proof}
[Solution to Additional exercise \ref{exeadd.noncomm.polarization}%
.]\textbf{(a)} Let $m\in\mathbb{N}$.

\begin{vershort}
We have%
\begin{align*}
&  \sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\underbrace{\left(  \sum_{i\in I}v_{i}\right)  ^{m}%
}_{\substack{=\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\\text{(by Lemma
\ref{lem.sol.exeadd.noncomm.polarization.1})}}}\\
&  =\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  =\underbrace{\sum_{I\subseteq\left[  n\right]  }\sum_{\substack{f:\left[
m\right]  \rightarrow\left[  n\right]  ;\\f\left(  \left[  m\right]  \right)
\subseteq I}}}_{=\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }%
\sum_{\substack{I\subseteq\left[  n\right]  ;\\f\left(  \left[  m\right]
\right)  \subseteq I}}}\left(  -1\right)  ^{n-\left\vert I\right\vert
}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\sum
_{\substack{I\subseteq\left[  n\right]  ;\\f\left(  \left[  m\right]  \right)
\subseteq I}}\left(  -1\right)  ^{n-\left\vert I\right\vert }v_{f\left(
1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }%
\underbrace{\left(  \sum_{\substack{I\subseteq\left[  n\right]  ;\\f\left(
\left[  m\right]  \right)  \subseteq I}}\left(  -1\right)  ^{n-\left\vert
I\right\vert }\right)  }_{\substack{=\left[  f\left(  \left[  m\right]
\right)  =\left[  n\right]  \right]  \\\text{(by Lemma
\ref{lem.sol.exeadd.noncomm.polarization.2}}\\\text{(applied to }J=f\left(
\left[  m\right]  \right)  \text{))}}}v_{f\left(  1\right)  }v_{f\left(
2\right)  }\cdots v_{f\left(  m\right)  }%
\end{align*}%
\begin{align*}
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\left[
\underbrace{f\left(  \left[  m\right]  \right)  =\left[  n\right]
}_{\Longleftrightarrow\ \left(  f\text{ is surjective}\right)  }\right]
v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\left[  f\text{ is
surjective}\right]  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  m\right)  }\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\text{ is surjective}}}\underbrace{\left[  f\text{ is surjective}\right]
}_{\substack{=1\\\text{(since }f\text{ is surjective)}}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{f:\left[  m\right]  \rightarrow\left[
n\right]  ;\\f\text{ is not surjective}}}\underbrace{\left[  f\text{ is
surjective}\right]  }_{\substack{=0\\\text{(since }f\text{ is not
surjective)}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\text{ is surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)
}\cdots v_{f\left(  m\right)  }+\underbrace{\sum_{\substack{f:\left[
m\right]  \rightarrow\left[  n\right]  ;\\f\text{ is not surjective}%
}}0v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}}_{=0}\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\text{ is surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)
}\cdots v_{f\left(  m\right)  }.
\end{align*}
This solves Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(a)}.
\end{vershort}

\begin{verlong}
We have%
\begin{align*}
&  \sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\underbrace{\left(  \sum_{i\in I}v_{i}\right)  ^{m}%
}_{\substack{=\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\\text{(by Lemma
\ref{lem.sol.exeadd.noncomm.polarization.1})}}}\\
&  =\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\left(  \left[  m\right]  \right)  \subseteq I}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  =\underbrace{\sum_{I\subseteq\left[  n\right]  }\sum_{\substack{f:\left[
m\right]  \rightarrow\left[  n\right]  ;\\f\left(  \left[  m\right]  \right)
\subseteq I}}}_{=\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }%
\sum_{\substack{I\subseteq\left[  n\right]  ;\\f\left(  \left[  m\right]
\right)  \subseteq I}}}\left(  -1\right)  ^{n-\left\vert I\right\vert
}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\sum
_{\substack{I\subseteq\left[  n\right]  ;\\f\left(  \left[  m\right]  \right)
\subseteq I}}\left(  -1\right)  ^{n-\left\vert I\right\vert }v_{f\left(
1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }%
\underbrace{\left(  \sum_{\substack{I\subseteq\left[  n\right]  ;\\f\left(
\left[  m\right]  \right)  \subseteq I}}\left(  -1\right)  ^{n-\left\vert
I\right\vert }\right)  }_{\substack{=\left[  f\left(  \left[  m\right]
\right)  =\left[  n\right]  \right]  \\\text{(by Lemma
\ref{lem.sol.exeadd.noncomm.polarization.2}}\\\text{(applied to }J=f\left(
\left[  m\right]  \right)  \text{))}}}v_{f\left(  1\right)  }v_{f\left(
2\right)  }\cdots v_{f\left(  m\right)  }%
\end{align*}%
\begin{align*}
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\left[
\underbrace{f\left(  \left[  m\right]  \right)  =\left[  n\right]
}_{\substack{\Longleftrightarrow\ \left(  f\text{ is surjective}\right)
\\\text{(because }f\text{ is surjective if and only if }f\left(  \left[
m\right]  \right)  =\left[  n\right]  \text{)}}}\right]  v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  =\sum_{f:\left[  m\right]  \rightarrow\left[  n\right]  }\left[  f\text{ is
surjective}\right]  v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  m\right)  }\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\text{ is surjective}}}\underbrace{\left[  f\text{ is surjective}\right]
}_{\substack{=1\\\text{(since }f\text{ is surjective)}}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{f:\left[  m\right]  \rightarrow\left[
n\right]  ;\\f\text{ is not surjective}}}\underbrace{\left[  f\text{ is
surjective}\right]  }_{\substack{=0\\\text{(since }f\text{ is not
surjective)}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{because for each }f:\left[  m\right]  \rightarrow\left[  n\right]
\text{, either }\left(  f\text{ is surjective}\right) \\
\text{or }\left(  f\text{ is not surjective}\right)  \text{ (but not both)}%
\end{array}
\right) \\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\text{ is surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)
}\cdots v_{f\left(  m\right)  }+\underbrace{\sum_{\substack{f:\left[
m\right]  \rightarrow\left[  n\right]  ;\\f\text{ is not surjective}%
}}0v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  m\right)
}}_{=0}\\
&  =\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]
;\\f\text{ is surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)
}\cdots v_{f\left(  m\right)  }.
\end{align*}
This solves Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(a)}.
\end{verlong}

\textbf{(b)} Let $m\in\left\{  0,1,\ldots,n-1\right\}  $. Then, there exists
no map $f:\left[  m\right]  \rightarrow\left[  n\right]  $ such that $f$ is
surjective\footnote{\textit{Proof.} Let $f:\left[  m\right]  \rightarrow
\left[  n\right]  $ be a map such that $f$ is surjective. Thus, $f\left(
\left[  m\right]  \right)  =\left[  n\right]  $ (since $f$ is surjective). But
clearly, $\left\vert f\left(  \left[  m\right]  \right)  \right\vert
\leq\left\vert \left[  m\right]  \right\vert =m$. Since $f\left(  \left[
m\right]  \right)  =\left[  n\right]  $, this rewrites as $\left\vert \left[
n\right]  \right\vert \leq m$. Since $\left\vert \left[  n\right]  \right\vert
=n$, this rewrites as $n\leq m$. But $m\leq n-1$ (since $m\in\left\{
0,1,\ldots,n-1\right\}  $). Thus, $n\leq m\leq n-1<n$. This is absurd.
\par
Now, forget that we fixed $f$. We thus have found a contradiction for each map
$f:\left[  m\right]  \rightarrow\left[  n\right]  $ such that $f$ is
surjective. Hence, there exists no map $f:\left[  m\right]  \rightarrow\left[
n\right]  $ such that $f$ is surjective. Qed.}. Hence, the sum $\sum
_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }$ is an empty sum. Therefore, this sum equals $0$. In other words,
$\sum_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]  ;\\f\text{
is surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  m\right)  }=0$.

Now, Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(a)} yields%
\[
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{m}=\sum
_{\substack{f:\left[  m\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
surjective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(
m\right)  }=0.
\]
This solves Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(b)}.

\textbf{(c)} Clearly, $\left[  n\right]  $ and $\left[  n\right]  $ are two
finite sets such that $\left\vert \left[  n\right]  \right\vert =\left\vert
\left[  n\right]  \right\vert $. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon} (applied to $U=\left\vert
\left[  n\right]  \right\vert $ and $V=\left\vert \left[  n\right]
\right\vert $) shows that if $f:\left[  n\right]  \rightarrow\left[  n\right]
$ is a map, then we have the following logical equivalence:%
\begin{equation}
\left(  f\text{ is surjective}\right)  \ \Longleftrightarrow\ \left(  f\text{
is bijective}\right)  . \label{sol.exeadd.noncomm.polarization.c.3}%
\end{equation}


\begin{vershort}
Hence, we have the following equality of summation signs:%
\[
\sum_{\substack{f:\left[  n\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
surjective}}}=\sum_{\substack{f:\left[  n\right]  \rightarrow\left[  n\right]
;\\f\text{ is bijective}}}=\sum_{f\text{ is a permutation of }\left[
n\right]  }=\sum_{f\in S_{n}}%
\]
(since $S_{n}$ is the set of all permutations of $\left[  n\right]  $). Now,
Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(a)} (applied to
$m=n$) yields%
\begin{align*}
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{n}  &  =\underbrace{\sum
_{\substack{f:\left[  n\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
surjective}}}}_{\substack{=\sum_{f\in S_{n}}\\\text{(by
(\ref{sol.exeadd.noncomm.polarization.c.3}))}}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  n\right)  }\\
&  =\sum_{f\in S_{n}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  n\right)  }=\sum_{\sigma\in S_{n}}v_{\sigma\left(  1\right)
}v_{\sigma\left(  2\right)  }\cdots v_{\sigma\left(  n\right)  }%
\end{align*}
(here, we have renamed the summation index $f$ as $\sigma$). This solves
Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(c)}.
\end{vershort}

\begin{verlong}
Now, Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(a)}
(applied to $m=n$) yields%
\begin{align}
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{n}  &  =\underbrace{\sum
_{\substack{f:\left[  n\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
surjective}}}}_{\substack{=\sum_{\substack{f:\left[  n\right]  \rightarrow
\left[  n\right]  ;\\f\text{ is bijective}}}\\\text{(because for a map
}f:\left[  n\right]  \rightarrow\left[  n\right]  \text{,}\\\text{the
condition }\left(  f\text{ is surjective}\right)  \text{ is equivalent}%
\\\text{to the condition }\left(  f\text{ is bijective}\right)
\\\text{(because of (\ref{sol.exeadd.noncomm.polarization.c.3})))}%
}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  n\right)
}\nonumber\\
&  =\sum_{\substack{f:\left[  n\right]  \rightarrow\left[  n\right]
;\\f\text{ is bijective}}}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots
v_{f\left(  n\right)  }. \label{sol.exeadd.noncomm.polarization.c.5}%
\end{align}


But $S_{n}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $. In other words, $S_{n}$ is the set of all permutations of the
set $\left[  n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[
n\right]  $). In other words, $S_{n}$ is the set of all bijective maps
$\left[  n\right]  \rightarrow\left[  n\right]  $ (since the permutations of
the set $\left[  n\right]  $ are exactly the bijective maps $\left[  n\right]
\rightarrow\left[  n\right]  $). Hence,%
\begin{equation}
\sum_{f\in S_{n}}=\sum_{f\text{ is a bijective map }\left[  n\right]
\rightarrow\left[  n\right]  }=\sum_{\substack{f:\left[  n\right]
\rightarrow\left[  n\right]  ;\\f\text{ is bijective}}}
\label{sol.exeadd.noncomm.polarization.c.7}%
\end{equation}
(an equality of summation signs). Now,
(\ref{sol.exeadd.noncomm.polarization.c.5}) becomes%
\begin{align*}
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{n}  &  =\underbrace{\sum
_{\substack{f:\left[  n\right]  \rightarrow\left[  n\right]  ;\\f\text{ is
bijective}}}}_{\substack{=\sum_{f\in S_{n}}\\\text{(by
(\ref{sol.exeadd.noncomm.polarization.c.7}))}}}v_{f\left(  1\right)
}v_{f\left(  2\right)  }\cdots v_{f\left(  n\right)  }=\sum_{f\in S_{n}%
}v_{f\left(  1\right)  }v_{f\left(  2\right)  }\cdots v_{f\left(  n\right)
}\\
&  =\sum_{\sigma\in S_{n}}v_{\sigma\left(  1\right)  }v_{\sigma\left(
2\right)  }\cdots v_{\sigma\left(  n\right)  }%
\end{align*}
(here, we have renamed the summation index $f$ as $\sigma$). This solves
Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(c)}.
\end{verlong}

\textbf{(d)} We have%
\begin{equation}
v_{\sigma\left(  1\right)  }v_{\sigma\left(  2\right)  }\cdots v_{\sigma
\left(  n\right)  }=v_{1}v_{2}\cdots v_{n}
\label{sol.exeadd.noncomm.polarization.d.1}%
\end{equation}
for each $\sigma\in S_{n}$\ \ \ \ \footnote{\textit{Proof of
(\ref{sol.exeadd.noncomm.polarization.d.1}):} Let $\sigma\in S_{n}$. Thus,
$\sigma$ is a permutation of the set $\left\{  1,2,\ldots,n\right\}  $ (since
$S_{n}$ is the set of all permutations of the set $\left\{  1,2,\ldots
,n\right\}  $). In other words, $\sigma$ is a bijection $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $.
\par
But%
\begin{align*}
v_{\sigma\left(  1\right)  }v_{\sigma\left(  2\right)  }\cdots v_{\sigma
\left(  n\right)  }  &  =\underbrace{\prod_{i=1}^{n}}_{=\prod_{i\in\left\{
1,2,\ldots,n\right\}  }}v_{\sigma\left(  i\right)  }=\prod_{i\in\left\{
1,2,\ldots,n\right\}  }v_{\sigma\left(  i\right)  }=\underbrace{\prod
_{i\in\left\{  1,2,\ldots,n\right\}  }}_{=\prod_{i=1}^{n}}v_{i}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }i\text{ for }\sigma\left(  i\right)  \text{
in the product,}\\
\text{since }\sigma\text{ is a bijection }\left\{  1,2,\ldots,n\right\}
\rightarrow\left\{  1,2,\ldots,n\right\}
\end{array}
\right) \\
&  =\prod_{i=1}^{n}v_{i}=v_{1}v_{2}\cdots v_{n}.
\end{align*}
This proves (\ref{sol.exeadd.noncomm.polarization.d.1}).}. Now, Additional
exercise \ref{exeadd.noncomm.polarization} \textbf{(c)} yields%
\begin{align*}
\sum_{I\subseteq\left[  n\right]  }\left(  -1\right)  ^{n-\left\vert
I\right\vert }\left(  \sum_{i\in I}v_{i}\right)  ^{n}  &  =\sum_{\sigma\in
S_{n}}\underbrace{v_{\sigma\left(  1\right)  }v_{\sigma\left(  2\right)
}\cdots v_{\sigma\left(  n\right)  }}_{\substack{=v_{1}v_{2}\cdots
v_{n}\\\text{(by (\ref{sol.exeadd.noncomm.polarization.d.1}))}}}=\sum
_{\sigma\in S_{n}}v_{1}v_{2}\cdots v_{n}\\
&  =\underbrace{\left\vert S_{n}\right\vert }_{=n!}v_{1}v_{2}\cdots
v_{n}=n!\cdot v_{1}v_{2}\cdots v_{n}.
\end{align*}
This solves Additional exercise \ref{exeadd.noncomm.polarization} \textbf{(d)}.
\end{proof}

\subsection{Solution to Additional exercise \ref{exeadd.powerdet.gen}}

We shall next prove some identities in preparation for the solution to
Additional exercise \ref{exeadd.powerdet.gen}.

First, we introduce a notation: For every $n\in\mathbb{N}$, let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $. We shall also
use the notation introduced in Definition \ref{def.submatrix.minor} throughout
this section.

Let us now state a simple corollary of Lemma \ref{lem.prodrule2}:

\begin{corollary}
\label{cor.sol.exeadd.powerdet.gen.prodrule}Let $n\in\mathbb{N}$. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times
n$-matrix. Let $\sigma\in S_{n}$. Then,%
\[
\left(  \sum_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{k}=\sum
_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{k}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}%
\]
for every $k\in\mathbb{N}$.
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.sol.exeadd.powerdet.gen.prodrule}.]Let
$g\in\mathbb{N}$. Then, Lemma \ref{lem.prodrule2} (applied to $g$, $n$ and
$a_{k,\sigma\left(  k\right)  }$ instead of $n$, $m$ and $p_{i,k}$) yields%
\[
\prod_{i=1}^{g}\sum_{k=1}^{n}a_{k,\sigma\left(  k\right)  }=\sum
_{\kappa:\left[  g\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{g}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}.
\]
Thus,%
\begin{align}
\sum_{\kappa:\left[  g\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{g}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}  &  =\prod_{i=1}^{g}\sum_{k=1}^{n}a_{k,\sigma\left(  k\right)  }=\left(
\sum_{k=1}^{n}a_{k,\sigma\left(  k\right)  }\right)  ^{g}\nonumber\\
&  =\left(  \sum_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{g}
\label{pf.cor.sol.exeadd.powerdet.gen.prodrule.1}%
\end{align}
(here, we have renamed the summation index $k$ as $i$ in the sum).

Now, forget that we fixed $g$. We thus have proven the identity
(\ref{pf.cor.sol.exeadd.powerdet.gen.prodrule.1}) for each $g\in\mathbb{N}$.

Now, fix $k\in\mathbb{N}$. Then,
(\ref{pf.cor.sol.exeadd.powerdet.gen.prodrule.1}) (applied to $g=k$) yields
$\sum_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{k}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}=\left(  \sum_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{k}$. This
proves Corollary \ref{cor.sol.exeadd.powerdet.gen.prodrule}.
\end{proof}

Next, we state a simple lemma:

\begin{lemma}
\label{lem.sol.exeadd.powerdet.gen.toofew}Let $n\in\mathbb{N}$ and
$k\in\mathbb{N}$. Let $\kappa:\left[  k\right]  \rightarrow\left[  n\right]  $
be a map such that $\left\vert \kappa\left(  \left[  k\right]  \right)
\right\vert <n-1$. Then,%
\[
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{k}%
a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}=0.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.toofew}.]We have $\left\vert
\left[  n\right]  \setminus\kappa\left(  \left[  k\right]  \right)
\right\vert \geq2$\ \ \ \ \footnote{\textit{Proof.} We have $\left\vert
\kappa\left(  \left[  k\right]  \right)  \right\vert <n-1$. Thus, $\left\vert
\kappa\left(  \left[  k\right]  \right)  \right\vert \leq\left(  n-1\right)
-1$ (since both $\left\vert \kappa\left(  \left[  k\right]  \right)
\right\vert $ and $n-1$ are integers). Hence, $\left\vert \kappa\left(
\left[  k\right]  \right)  \right\vert \leq\left(  n-1\right)  -1=n-2$.
\par
But $\kappa\left(  \left[  k\right]  \right)  \subseteq\left[  n\right]  $.
Hence, $\left\vert \left[  n\right]  \setminus\kappa\left(  \left[  k\right]
\right)  \right\vert =\underbrace{\left\vert \left[  n\right]  \right\vert
}_{=n}-\underbrace{\left\vert \kappa\left(  \left[  k\right]  \right)
\right\vert }_{\leq n-2}\geq n-\left(  n-2\right)  =2$. Qed.}. Hence, the set
$\left[  n\right]  \setminus\kappa\left(  \left[  k\right]  \right)  $ has at
least $2$ elements. In other words, the set $\left[  n\right]  \setminus
\kappa\left(  \left[  k\right]  \right)  $ contains two distinct elements.
Choose two such elements, and denote them by $a$ and $b$. Thus, $a$ and $b$
are two distinct elements of the set $\left[  n\right]  \setminus\kappa\left(
\left[  k\right]  \right)  $.

The elements $a$ and $b$ both belong to $\left[  n\right]  $ (since
$a\in\left[  n\right]  \setminus\kappa\left(  \left[  k\right]  \right)
\subseteq\left[  n\right]  $ and $b\in\left[  n\right]  \setminus\kappa\left(
\left[  k\right]  \right)  \subseteq\left[  n\right]  $).

\begin{vershort}
Thus, $a$ and $b$ are two distinct elements of $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Hence, a transposition $t_{a,b}\in S_{n}$ is defined
(according to Definition \ref{def.transpos}). This transposition satisfies
$t_{a,b}\circ\kappa=\kappa$\ \ \ \ \footnote{\textit{Proof.} We are going to
show that every $i\in\left[  k\right]  $ satisfies $\left(  t_{a,b}\circ
\kappa\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
So let $i\in\left[  k\right]  $. We shall show that $\left(  t_{a,b}%
\circ\kappa\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
We have $t_{a,b}\left(  j\right)  =j$ for every $j\in\left[  n\right]
\setminus\left\{  a,b\right\}  $ (by the definition of $t_{a,b}$).
\par
But $a$ and $b$ are two elements of the set $\left[  n\right]  \setminus
\kappa\left(  \left[  k\right]  \right)  $. Hence, $\left\{  a,b\right\}
\subseteq\left[  n\right]  \setminus\kappa\left(  \left[  k\right]  \right)
$. If we had $\kappa\left(  i\right)  \in\left\{  a,b\right\}  $, then we
would thus have $\kappa\left(  i\right)  \in\left\{  a,b\right\}
\subseteq\left[  n\right]  \setminus\kappa\left(  \left[  k\right]  \right)
$, which would contradict $\kappa\left(  i\right)  \notin\kappa\left(  \left[
k\right]  \right)  $. Hence, we cannot have $\kappa\left(  i\right)
\in\left\{  a,b\right\}  $. Thus, we have $\kappa\left(  i\right)
\notin\left\{  a,b\right\}  $.
\par
Combining $\kappa\left(  i\right)  \in\left[  n\right]  $ with $\kappa\left(
i\right)  \in\left\{  a,b\right\}  $, we obtain $\kappa\left(  i\right)
\in\left[  n\right]  \setminus\left\{  a,b\right\}  $. But recall that
$t_{a,b}\left(  j\right)  =j$ for every $j\in\left[  n\right]  \setminus
\left\{  a,b\right\}  $. Applying this to $j=\kappa\left(  i\right)  $, we
obtain $t_{a,b}\left(  \kappa\left(  i\right)  \right)  =\kappa\left(
i\right)  $ (since $\kappa\left(  i\right)  \in\left[  n\right]
\setminus\left\{  a,b\right\}  $). Hence, $\left(  t_{a,b}\circ\kappa\right)
\left(  i\right)  =t_{a,b}\left(  \kappa\left(  i\right)  \right)
=\kappa\left(  i\right)  $.
\par
Now, let us forget that we fixed $i$. We thus have shown that $\left(
t_{a,b}\circ\kappa\right)  \left(  i\right)  =\kappa\left(  i\right)  $ for
every $i\in\left[  k\right]  $. In other words, $t_{a,b}\circ\kappa=\kappa$,
qed.}.
\end{vershort}

\begin{verlong}
Thus, $a$ and $b$ are two distinct elements of $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $. Hence, a transposition $t_{a,b}\in S_{n}$ is defined
(according to Definition \ref{def.transpos}). This transposition satisfies
$t_{a,b}\circ\kappa=\kappa$\ \ \ \ \footnote{\textit{Proof.} We are going to
show that every $i\in\left[  k\right]  $ satisfies $\left(  t_{a,b}\circ
\kappa\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
So let $i\in\left[  k\right]  $. We shall show that $\left(  t_{a,b}%
\circ\kappa\right)  \left(  i\right)  =\kappa\left(  i\right)  $.
\par
The definition of $t_{a,b}$ shows that $t_{a,b}$ is the permutation in $S_{n}$
which switches $a$ with $b$ while leaving all other elements of $\left\{
1,2,\ldots,n\right\}  $ unchanged. In other words, we have $t_{a,b}\left(
a\right)  =b$, and $t_{a,b}\left(  b\right)  =a$, and $t_{a,b}\left(
j\right)  =j$ for every $j\in\left[  n\right]  \setminus\left\{  a,b\right\}
$.
\par
Let us first assume (for the sake of contradiction) that $\kappa\left(
i\right)  \in\left\{  a,b\right\}  $.
\par
But $a$ and $b$ are two elements of the set $\left[  n\right]  \setminus
\kappa\left(  \left[  k\right]  \right)  $. Hence, $\left\{  a,b\right\}
\subseteq\left[  n\right]  \setminus\kappa\left(  \left[  k\right]  \right)
$. Thus, $\kappa\left(  i\right)  \in\left\{  a,b\right\}  \subseteq\left[
n\right]  \setminus\kappa\left(  \left[  k\right]  \right)  $. In other words,
$\kappa\left(  i\right)  \in\left[  n\right]  $ and $\kappa\left(  i\right)
\notin\kappa\left(  \left[  k\right]  \right)  $. But $\kappa\left(  i\right)
\notin\kappa\left(  \left[  k\right]  \right)  $ clearly contradicts
$\kappa\left(  \underbrace{i}_{\in\left[  k\right]  }\right)  \in\kappa\left(
\left[  k\right]  \right)  $. Thus, we have obtained a contradiction. Hence,
our assumption (that $\kappa\left(  i\right)  \in\left\{  a,b\right\}  $) was
false. Thus, we have $\kappa\left(  i\right)  \notin\left\{  a,b\right\}  $.
\par
Combining $\kappa\left(  i\right)  \in\left[  n\right]  $ with $\kappa\left(
i\right)  \in\left\{  a,b\right\}  $, we obtain $\kappa\left(  i\right)
\in\left[  n\right]  \setminus\left\{  a,b\right\}  $. But recall that
$t_{a,b}\left(  j\right)  =j$ for every $j\in\left[  n\right]  \setminus
\left\{  a,b\right\}  $. Applying this to $j=\kappa\left(  i\right)  $, we
obtain $t_{a,b}\left(  \kappa\left(  i\right)  \right)  =\kappa\left(
i\right)  $ (since $\kappa\left(  i\right)  \in\left[  n\right]
\setminus\left\{  a,b\right\}  $). Hence, $\left(  t_{a,b}\circ\kappa\right)
\left(  i\right)  =t_{a,b}\left(  \kappa\left(  i\right)  \right)
=\kappa\left(  i\right)  $.
\par
Now, let us forget that we fixed $i$. We thus have shown that $\left(
t_{a,b}\circ\kappa\right)  \left(  i\right)  =\kappa\left(  i\right)  $ for
every $i\in\left[  k\right]  $. In other words, $t_{a,b}\circ\kappa=\kappa$,
qed.}.
\end{verlong}

Let $A_{n}$ be the set of all even permutations in $S_{n}$. Let $C_{n}$ be the
set of all odd permutations in $S_{n}$.

We have $\sigma\circ t_{a,b}\in C_{n}$ for every $\sigma\in A_{n}%
$\ \ \ \ \footnote{We have already proven this during our proof of Lemma
\ref{lem.det.sigma} \textbf{(b)}.}. Hence, we can define a map $\Phi
:A_{n}\rightarrow C_{n}$ by%
\[
\Phi\left(  \sigma\right)  =\sigma\circ t_{a,b}\ \ \ \ \ \ \ \ \ \ \text{for
every }\sigma\in A_{n}.
\]
Consider this map $\Phi$. Furthermore, we have $\sigma\circ\left(
t_{a,b}\right)  ^{-1}\in A_{n}$ for every $\sigma\in C_{n}$%
\ \ \ \ \footnote{We have already proven this during our proof of Lemma
\ref{lem.det.sigma} \textbf{(b)}.}. Thus, we can define a map $\Psi
:C_{n}\rightarrow A_{n}$ by%
\[
\Psi\left(  \sigma\right)  =\sigma\circ\left(  t_{a,b}\right)  ^{-1}%
\ \ \ \ \ \ \ \ \ \ \text{for every }\sigma\in C_{n}.
\]
Consider this map $\Psi$.

The maps $\Phi$ and $\Psi$ are mutually inverse\footnote{We have already
proven this during our proof of Lemma \ref{lem.det.sigma} \textbf{(b)}.}.
Hence, the map $\Phi$ is a bijection. Moreover, every $\sigma\in A_{n}$ and
$i\in\left\{  1,2,\ldots,k\right\}  $ satisfy%
\begin{equation}
a_{\kappa\left(  i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)
\left(  \kappa\left(  i\right)  \right)  }=a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }
\label{pf.lem.sol.exeadd.powerdet.gen.toofew.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.powerdet.gen.toofew.1}):}
Let $\sigma\in A_{n}$ and $i\in\left\{  1,2,\ldots,k\right\}  $. Thus,
$i\in\left\{  1,2,\ldots,k\right\}  =\left[  k\right]  $. Now,%
\[
\left(  \underbrace{\Phi\left(  \sigma\right)  }_{=\sigma\circ t_{a,b}%
}\right)  \left(  \kappa\left(  i\right)  \right)  =\left(  \sigma\circ
t_{a,b}\right)  \left(  \kappa\left(  i\right)  \right)  =\sigma\left(
t_{a,b}\left(  \kappa\left(  i\right)  \right)  \right)  =\sigma\left(
\underbrace{\left(  t_{a,b}\circ\kappa\right)  }_{=\kappa}\left(  i\right)
\right)  =\sigma\left(  \kappa\left(  i\right)  \right)  .
\]
Hence, $a_{\kappa\left(  i\right)  ,\left(  \Phi\left(  \sigma\right)
\right)  \left(  \kappa\left(  i\right)  \right)  }=a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }$. This proves
(\ref{pf.lem.sol.exeadd.powerdet.gen.toofew.1}).}.

Now,%
\begin{align*}
&  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{k}%
a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }\\
&  =\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\text{ is even}}%
}}_{\substack{=\sum_{\sigma\in A_{n}}\\\text{(since }A_{n}\text{ is
the}\\\text{set of all even}\\\text{permutations}\\\text{in }S_{n}\text{)}%
}}\underbrace{\left(  -1\right)  ^{\sigma}}_{\substack{=1\\\text{(since
}\sigma\text{ is even)}}}\prod_{i=1}^{k}a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }+\underbrace{\sum
_{\substack{\sigma\in S_{n};\\\sigma\text{ is odd}}}}_{\substack{=\sum
_{\sigma\in C_{n}}\\\text{(since }C_{n}\text{ is the}\\\text{set of all
odd}\\\text{permutations}\\\text{in }S_{n}\text{)}}}\underbrace{\left(
-1\right)  ^{\sigma}}_{\substack{=-1\\\text{(since }\sigma\text{ is odd)}%
}}\prod_{i=1}^{k}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(
i\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{since every permutation }\sigma\in
S_{n}\text{ is either even or odd, but not both}\right) \\
&  =\sum_{\sigma\in A_{n}}\prod_{i=1}^{k}a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }+\sum_{\sigma\in C_{n}%
}\left(  -1\right)  \prod_{i=1}^{k}a_{\kappa\left(  i\right)  ,\sigma\left(
\kappa\left(  i\right)  \right)  }\\
&  =\sum_{\sigma\in A_{n}}\prod_{i=1}^{k}a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }-\sum_{\sigma\in C_{n}}%
\prod_{i=1}^{k}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(
i\right)  \right)  }=0,
\end{align*}
since%
\begin{align*}
&  \sum_{\sigma\in C_{n}}\prod_{i=1}^{k}a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }\\
&  =\sum_{\sigma\in A_{n}}\prod_{i=1}^{k}\underbrace{a_{\kappa\left(
i\right)  ,\left(  \Phi\left(  \sigma\right)  \right)  \left(  \kappa\left(
i\right)  \right)  }}_{\substack{=a_{\kappa\left(  i\right)  ,\sigma\left(
\kappa\left(  i\right)  \right)  }\\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.toofew.1}))}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have substituted }\Phi\left(
\sigma\right)  \text{ for }\sigma\text{, since the map }\Phi\text{ is a
bijection}\right) \\
&  =\sum_{\sigma\in A_{n}}\prod_{i=1}^{k}a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }.
\end{align*}
This proves Lemma \ref{lem.sol.exeadd.powerdet.gen.toofew}.
\end{proof}

We can now solve part \textbf{(a)} of Additional exercise
\ref{exeadd.powerdet.gen}:

\begin{proposition}
\label{prop.sol.exeadd.powerdet.gen.a}Let $n\in\mathbb{N}$. Let $A=\left(
a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times n$-matrix.
Then,%
\[
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \sum_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{k}=0
\]
for each $k\in\left\{  0,1,\ldots,n-2\right\}  $.
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sol.exeadd.powerdet.gen.a}.]Let $k\in\left\{
0,1,\ldots,n-2\right\}  $. Then, each each map $\kappa:\left[  k\right]
\rightarrow\left[  n\right]  $ satisfies%
\begin{equation}
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{k}%
a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }=0
\label{pf.prop.sol.exeadd.powerdet.gen.a.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.sol.exeadd.powerdet.gen.a.1}):} Let
$\kappa:\left[  k\right]  \rightarrow\left[  n\right]  $ be a map. Then, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(a)} (applied to
$U=\left[  k\right]  $, $V=\left[  n\right]  $, $f=\kappa$ and $S=\left[
k\right]  $) yields $\left\vert \kappa\left(  \left[  k\right]  \right)
\right\vert \leq\left\vert \left[  k\right]  \right\vert =k\leq n-2$ (since
$k\in\left\{  0,1,\ldots,n-2\right\}  $), so that $\left\vert \kappa\left(
\left[  k\right]  \right)  \right\vert \leq n-2<n-1$. Hence, Lemma
\ref{lem.sol.exeadd.powerdet.gen.toofew} yields $\sum_{\sigma\in S_{n}}\left(
-1\right)  ^{\sigma}\prod_{i=1}^{k}a_{\kappa\left(  i\right)  ,\sigma\left(
\kappa\left(  i\right)  \right)  }=0$. This proves
(\ref{pf.prop.sol.exeadd.powerdet.gen.a.1}).}.

Now,%
\begin{align*}
&  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\left(
\sum_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{k}}_{\substack{=\sum
_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{k}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}\\\text{(by Corollary \ref{cor.sol.exeadd.powerdet.gen.prodrule})}}}\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\kappa:\left[
k\right]  \rightarrow\left[  n\right]  }\prod_{i=1}^{k}a_{\kappa\left(
i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }=\underbrace{\sum
_{\sigma\in S_{n}}\sum_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]
}}_{=\sum_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]  }\sum
_{\sigma\in S_{n}}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{k}a_{\kappa\left(
i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }\\
&  =\sum_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]  }%
\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}%
^{k}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)
}}_{\substack{=0\\\text{(by (\ref{pf.prop.sol.exeadd.powerdet.gen.a.1}))}%
}}=\sum_{\kappa:\left[  k\right]  \rightarrow\left[  n\right]  }0=0.
\end{align*}
This proves Proposition \ref{prop.sol.exeadd.powerdet.gen.a}.
\end{proof}

\begin{remark}
Corollary \ref{cor.sol.exeadd.powerdet.gen.prodrule}, Lemma
\ref{lem.sol.exeadd.powerdet.gen.toofew} and Proposition
\ref{prop.sol.exeadd.powerdet.gen.a} all remain valid if the commutative ring
$\mathbb{K}$ is replaced by a noncommutative ring $\mathbb{L}$, as long as we
use the conventions made in Section \ref{sect.sol.exeadd.noncomm.polarization}%
. In fact, the proofs given above still work when $\mathbb{K}$ is replaced by
$\mathbb{L}$, provided that we replace the reference to Lemma
\ref{lem.prodrule2} by a reference to Lemma \ref{lem.noncomm.prodrule2}.
\end{remark}

Part \textbf{(b)} of Additional exercise \ref{exeadd.powerdet.gen} is
noticeably harder. We prepare to it by studying permutations in $S_{n}$:

\begin{lemma}
\label{lem.sol.exeadd.powerdet.gen.allbut1}Let $n\in\mathbb{N}$ and
$p\in\left[  n\right]  $. Let $\alpha\in S_{n}$ and $\beta\in S_{n}$. Assume
that%
\begin{equation}
\alpha\left(  i\right)  =\beta\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \text{for
each }i\in\left[  n\right]  \setminus\left\{  p\right\}  .
\label{eq.lem.sol.exeadd.powerdet.gen.allbut1.ass}%
\end{equation}
Then, $\alpha=\beta$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.allbut1}.]We have $\alpha\in
S_{n}$. In other words, $\alpha$ is a permutation of $\left[  n\right]  $
(since $S_{n}$ is the set of all permutations of $\left\{  1,2,\ldots
,n\right\}  =\left[  n\right]  $). In other words, $\alpha$ is a bijective map
$\left[  n\right]  \rightarrow\left[  n\right]  $. The map $\alpha$ is
bijective and thus injective. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(c)} (applied to
$U=\left[  n\right]  $, $V=\left[  n\right]  $, $f=\alpha$ and $S=\left[
n\right]  \setminus\left\{  p\right\}  $) shows that
\[
\left\vert \alpha\left(  \left[  n\right]  \setminus\left\{  p\right\}
\right)  \right\vert =\left\vert \left[  n\right]  \setminus\left\{
p\right\}  \right\vert =\left\vert \left[  n\right]  \right\vert
-1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }p\in\left[  n\right]  \right)  .
\]


Let $G$ denote the subset $\alpha\left(  \left[  n\right]  \setminus\left\{
p\right\}  \right)  $ of $\left[  n\right]  $. Then, $G=\alpha\left(  \left[
n\right]  \setminus\left\{  p\right\}  \right)  $, and thus $\left\vert
G\right\vert =\left\vert \alpha\left(  \left[  n\right]  \setminus\left\{
p\right\}  \right)  \right\vert =\left\vert \left[  n\right]  \right\vert -1$.

But $G$ is a subset of $\left[  n\right]  $; hence,%
\[
\left\vert \left[  n\right]  \setminus G\right\vert =\left\vert \left[
n\right]  \right\vert -\underbrace{\left\vert G\right\vert }_{=\left\vert
\left[  n\right]  \right\vert -1}=\left\vert \left[  n\right]  \right\vert
-\left(  \left\vert \left[  n\right]  \right\vert -1\right)  =1.
\]
In other words, $\left[  n\right]  \setminus G$ is a $1$-element set. Hence,
$\left[  n\right]  \setminus G=\left\{  q\right\}  $ for some element $q$.
Consider this $q$.

We have $q\in\left\{  q\right\}  =\left[  n\right]  \setminus G\subseteq
\left[  n\right]  $.

Now, $\alpha\left(  p\right)  \notin G$\ \ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $\alpha\left(  p\right)  \in G$. Hence,
$\alpha\left(  p\right)  \in G=\alpha\left(  \left[  n\right]  \setminus
\left\{  p\right\}  \right)  $. In other words, there exists an $i\in\left[
n\right]  \setminus\left\{  p\right\}  $ satisfying $\alpha\left(  p\right)
=\alpha\left(  i\right)  $. Consider this $i$.
\par
From $\alpha\left(  p\right)  =\alpha\left(  i\right)  $, we obtain $p=i$
(since $\alpha$ is injective). But $i\in\left[  n\right]  \setminus\left\{
p\right\}  $ shows that $i\notin\left\{  p\right\}  $, so that $i\neq p=i$.
This is clearly a contradiction. This contradiction proves that our assumption
was wrong. Qed.}. Combining this with $\alpha\left(  p\right)  \in\left[
n\right]  $, we obtain $\alpha\left(  p\right)  \in\left[  n\right]  \setminus
G=\left\{  q\right\}  $. Thus, $\alpha\left(  p\right)  =q$.

Also, $\beta\left(  p\right)  \notin G$\ \ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $\beta\left(  p\right)  \in G$. Hence,
$\beta\left(  p\right)  \in G=\alpha\left(  \left[  n\right]  \setminus
\left\{  p\right\}  \right)  $. In other words, there exists an $i\in\left[
n\right]  \setminus\left\{  p\right\}  $ satisfying $\beta\left(  p\right)
=\alpha\left(  i\right)  $. Consider this $i$.
\par
From (\ref{eq.lem.sol.exeadd.powerdet.gen.allbut1.ass}), we obtain
$\alpha\left(  i\right)  =\beta\left(  i\right)  $. Thus, $\beta\left(
p\right)  =\alpha\left(  i\right)  =\beta\left(  i\right)  $.
\par
But recall that the map $\alpha$ is injective. Similarly, the map $\beta$ is
injective. Thus, from $\beta\left(  p\right)  =\beta\left(  i\right)  $, we
obtain $p=i$. But $i\in\left[  n\right]  \setminus\left\{  p\right\}  $ shows
that $i\notin\left\{  p\right\}  $, so that $i\neq p=i$. This is a
contradiction. This contradiction proves that our assumption was wrong. Qed.}.
Combining this with $\beta\left(  p\right)  \in\left[  n\right]  $, we obtain
$\beta\left(  p\right)  \in\left[  n\right]  \setminus G=\left\{  q\right\}
$. Thus, $\beta\left(  p\right)  =q$. Comparing this with $\alpha\left(
p\right)  =q$, we obtain $\alpha\left(  p\right)  =\beta\left(  p\right)  $.

Now, each $i\in\left[  n\right]  $ satisfies $\alpha\left(  i\right)
=\beta\left(  i\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left[
n\right]  $. We must show that $\alpha\left(  i\right)  =\beta\left(
i\right)  $.
\par
If $i=p$, then this is clearly true (because $\alpha\left(  p\right)
=\beta\left(  p\right)  $). Hence, for the rest of this proof, we WLOG assume
that $i\neq p$. Hence, $i\in\left[  n\right]  \setminus\left\{  p\right\}  $.
Thus, (\ref{eq.lem.sol.exeadd.powerdet.gen.allbut1.ass}) yields $\alpha\left(
i\right)  =\beta\left(  i\right)  $. Qed.}. In other words, $\alpha=\beta$.
This proves Lemma \ref{lem.sol.exeadd.powerdet.gen.allbut1}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.allbut1}.]We have $\alpha\in
S_{n}$. In other words, $\alpha$ is a permutation of $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of $\left\{
1,2,\ldots,n\right\}  $). In other words, $\alpha$ is a permutation of
$\left[  n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]
$). In other words, $\alpha$ is a bijective map $\left[  n\right]
\rightarrow\left[  n\right]  $. The map $\alpha$ is bijective and thus
injective. Hence, Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}
\textbf{(c)} (applied to $U=\left[  n\right]  $, $V=\left[  n\right]  $,
$f=\alpha$ and $S=\left[  n\right]  \setminus\left\{  p\right\}  $) shows
that
\[
\left\vert \alpha\left(  \left[  n\right]  \setminus\left\{  p\right\}
\right)  \right\vert =\left\vert \left[  n\right]  \setminus\left\{
p\right\}  \right\vert =\left\vert \left[  n\right]  \right\vert
-1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }p\in\left[  n\right]  \right)  .
\]


Let $G$ denote the subset $\alpha\left(  \left[  n\right]  \setminus\left\{
p\right\}  \right)  $ of $\left[  n\right]  $. Then, $G=\alpha\left(  \left[
n\right]  \setminus\left\{  p\right\}  \right)  $, and thus $\left\vert
G\right\vert =\left\vert \alpha\left(  \left[  n\right]  \setminus\left\{
p\right\}  \right)  \right\vert =\left\vert \left[  n\right]  \right\vert -1$.

But $G$ is a subset of $\left[  n\right]  $; hence,%
\[
\left\vert \left[  n\right]  \setminus G\right\vert =\left\vert \left[
n\right]  \right\vert -\underbrace{\left\vert G\right\vert }_{=\left\vert
\left[  n\right]  \right\vert -1}=\left\vert \left[  n\right]  \right\vert
-\left(  \left\vert \left[  n\right]  \right\vert -1\right)  =1.
\]
In other words, $\left[  n\right]  \setminus G$ is a $1$-element set. Hence,
$\left[  n\right]  \setminus G$ has the form $\left[  n\right]  \setminus
G=\left\{  q\right\}  $ for some element $q$. Consider this $q$.

We have $q\in\left\{  q\right\}  =\left[  n\right]  \setminus G\subseteq
\left[  n\right]  $.

Now, $\alpha\left(  p\right)  \notin G$\ \ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $\alpha\left(  p\right)  \in G$. Hence,
$\alpha\left(  p\right)  \in G=\alpha\left(  \left[  n\right]  \setminus
\left\{  p\right\}  \right)  $. In other words, there exists an $i\in\left[
n\right]  \setminus\left\{  p\right\}  $ satisfying $\alpha\left(  p\right)
=\alpha\left(  i\right)  $. Consider this $i$.
\par
From $i\in\left[  n\right]  \setminus\left\{  p\right\}  $, we obtain
$i\in\left[  n\right]  $ and $i\notin\left\{  p\right\}  $. From
$i\notin\left\{  p\right\}  $, we obtain $i\neq p$ and thus $p\neq i$.
\par
We have $p\in\left[  n\right]  $ and $i\in\left[  n\right]  \setminus\left\{
p\right\}  \subseteq\left[  n\right]  $. But the map $\alpha$ is injective. In
other words, if $u\in\left[  n\right]  $ and $v\in\left[  n\right]  $ satisfy
$\alpha\left(  u\right)  =\alpha\left(  v\right)  $, then $u=v$. Applying this
to $u=p$ and $v=i$, we obtain $p=i$ (since $\alpha\left(  p\right)
=\alpha\left(  i\right)  $). This contradicts $p\neq i$. This contradiction
proves that our assumption was wrong. Qed.}. Combining this with
$\alpha\left(  p\right)  \in\left[  n\right]  $, we obtain $\alpha\left(
p\right)  \in\left[  n\right]  \setminus G=\left\{  q\right\}  $. Thus,
$\alpha\left(  p\right)  =q$.

Also, $\beta\left(  p\right)  \notin G$\ \ \ \ \footnote{\textit{Proof.}
Assume the contrary. Thus, $\beta\left(  p\right)  \in G$. Hence,
$\beta\left(  p\right)  \in G=\alpha\left(  \left[  n\right]  \setminus
\left\{  p\right\}  \right)  $. In other words, there exists an $i\in\left[
n\right]  \setminus\left\{  p\right\}  $ satisfying $\beta\left(  p\right)
=\alpha\left(  i\right)  $. Consider this $i$.
\par
From $i\in\left[  n\right]  \setminus\left\{  p\right\}  $, we obtain
$i\in\left[  n\right]  $ and $i\notin\left\{  p\right\}  $. From
$i\notin\left\{  p\right\}  $, we obtain $i\neq p$ and thus $p\neq i$.
\par
From (\ref{eq.lem.sol.exeadd.powerdet.gen.allbut1.ass}), we obtain
$\alpha\left(  i\right)  =\beta\left(  i\right)  $. Thus, $\beta\left(
p\right)  =\alpha\left(  i\right)  =\beta\left(  i\right)  $.
\par
But recall that $\alpha$ is an injective map $\left[  n\right]  \rightarrow
\left[  n\right]  $. The same argument (applied to $\beta$ instead of $\alpha
$) shows that $\beta$ is an injective map $\left[  n\right]  \rightarrow
\left[  n\right]  $. In particular, $\beta$ is injective. In other words, if
$u\in\left[  n\right]  $ and $v\in\left[  n\right]  $ satisfy $\beta\left(
u\right)  =\beta\left(  v\right)  $, then $u=v$. Applying this to $u=p$ and
$v=i$, we obtain $p=i$ (since $\beta\left(  p\right)  =\beta\left(  i\right)
$). This contradicts $p\neq i$. This contradiction proves that our assumption
was wrong. Qed.}. Combining this with $\beta\left(  p\right)  \in\left[
n\right]  $, we obtain $\beta\left(  p\right)  \in\left[  n\right]  \setminus
G=\left\{  q\right\}  $. Thus, $\beta\left(  p\right)  =q$. Comparing this
with $\alpha\left(  p\right)  =q$, we obtain $\alpha\left(  p\right)
=\beta\left(  p\right)  $.

Now, each $i\in\left[  n\right]  $ satisfies $\alpha\left(  i\right)
=\beta\left(  i\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $i\in\left[
n\right]  $. We must show that $\alpha\left(  i\right)  =\beta\left(
i\right)  $.
\par
If $i=p$, then this is clearly true (because $\alpha\left(  p\right)
=\beta\left(  p\right)  $). Hence, for the rest of this proof, we can WLOG
assume that we don't have $i=p$. Assume this.
\par
We have $i\neq p$ (since we don't have $i=p$). Combining this with
$i\in\left[  n\right]  $, we obtain $i\in\left[  n\right]  \setminus\left\{
p\right\}  $. Thus, (\ref{eq.lem.sol.exeadd.powerdet.gen.allbut1.ass}) yields
$\alpha\left(  i\right)  =\beta\left(  i\right)  $. Qed.}. In other words,
$\alpha=\beta$. This proves Lemma \ref{lem.sol.exeadd.powerdet.gen.allbut1}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exeadd.powerdet.gen.kappas}Let $n\geq1$ be an integer. Then,
the map%
\begin{align*}
S_{n}  &  \rightarrow\left\{  f:\left[  n-1\right]  \rightarrow\left[
n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1\right\}  ,\\
\tau &  \mapsto\tau\mid_{\left[  n-1\right]  }%
\end{align*}
is well-defined and bijective.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.kappas}.]We have
$n-1\in\mathbb{N}$ (since $n\geq1$) and thus $\left\vert \left[  n-1\right]
\right\vert =n-1$.

\begin{vershort}
Also, $n\in\left[  n\right]  $ (since $n\geq1$) and $\left[  n-1\right]
=\left[  n\right]  \setminus\left\{  n\right\}  \subseteq\left[  n\right]  $.
\end{vershort}

\begin{verlong}
Also, $n\in\left[  n\right]  $ (since $n\geq1$) and%
\[
\underbrace{\left[  n\right]  }_{=\left\{  1,2,\ldots,n\right\}  }%
\setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n\right\}  \setminus\left\{
n\right\}  =\left\{  1,2,\ldots,n-1\right\}  .
\]
Thus, $\left[  n-1\right]  =\left\{  1,2,\ldots,n-1\right\}  =\left[
n\right]  \setminus\left\{  n\right\}  \subseteq\left[  n\right]  $. In other
words, $\left[  n-1\right]  $ is a subset of $\left[  n\right]  $.
\end{verlong}

Define a set $Y$ by%
\begin{equation}
Y=\left\{  f:\left[  n-1\right]  \rightarrow\left[  n\right]  \ \mid
\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert \geq
n-1\right\}  . \label{pf.lem.sol.exeadd.powerdet.gen.kappas.Y=}%
\end{equation}


\begin{vershort}
Each $\tau\in S_{n}$ satisfies $\tau\mid_{\left[  n-1\right]  }\in
Y$\ \ \ \ \footnote{\textit{Proof.} Let $\tau\in S_{n}$.
\par
We have $\tau\in S_{n}$. In other words, $\tau$ is a permutation of $\left[
n\right]  $ (since $S_{n}$ is the set of all permutations of $\left\{
1,2,\ldots,n\right\}  =\left[  n\right]  $). In other words, $\tau$ is a
bijective map $\left[  n\right]  \rightarrow\left[  n\right]  $. The map
$\tau$ is bijective and thus injective. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(c)} (applied to
$U=\left[  n\right]  $, $V=\left[  n\right]  $, $f=\tau$ and $S=\left[
n-1\right]  $) shows that $\left\vert \tau\left(  \left[  n-1\right]  \right)
\right\vert =\left\vert \left[  n-1\right]  \right\vert =n-1$.
\par
Now, $\tau\mid_{\left[  n-1\right]  }$ is a map $\left[  n-1\right]
\rightarrow\left[  n\right]  $ and satisfies $\left\vert \underbrace{\left(
\tau\mid_{\left[  n-1\right]  }\right)  \left(  \left[  n-1\right]  \right)
}_{=\tau\left(  \left[  n-1\right]  \right)  }\right\vert =\left\vert
\tau\left(  \left[  n-1\right]  \right)  \right\vert =n-1\geq n-1$. Hence,
$\tau\mid_{\left[  n-1\right]  }$ is a map $f:\left[  n-1\right]
\rightarrow\left[  n\right]  $ satisfying $\left\vert f\left(  \left[
n-1\right]  \right)  \right\vert \geq n-1$. In other words,%
\[
\tau\mid_{\left[  n-1\right]  }\in\left\{  f:\left[  n-1\right]
\rightarrow\left[  n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]
\right)  \right\vert \geq n-1\right\}  .
\]
In light of (\ref{pf.lem.sol.exeadd.powerdet.gen.kappas.Y=}), this rewrites as
$\tau\mid_{\left[  n-1\right]  }\in Y$. Qed.}. Hence, we can define a map
$T:S_{n}\rightarrow Y$ by%
\[
\left(  T\left(  \tau\right)  =\tau\mid_{\left[  n-1\right]  }%
\ \ \ \ \ \ \ \ \ \ \text{for all }\tau\in S_{n}\right)  .
\]
Consider this map $T$.
\end{vershort}

\begin{verlong}
Each $\tau\in S_{n}$ satisfies $\tau\mid_{\left[  n-1\right]  }\in
Y$\ \ \ \ \footnote{\textit{Proof.} Let $\tau\in S_{n}$.
\par
We have $\tau\in S_{n}$. In other words, $\tau$ is a permutation of $\left\{
1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all permutations of
$\left\{  1,2,\ldots,n\right\}  $). In other words, $\tau$ is a permutation of
$\left[  n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]
$). In other words, $\tau$ is a bijective map $\left[  n\right]
\rightarrow\left[  n\right]  $. The map $\tau$ is bijective and thus
injective. Hence, Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj}
\textbf{(c)} (applied to $U=\left[  n\right]  $, $V=\left[  n\right]  $,
$f=\tau$ and $S=\left[  n-1\right]  $) shows that $\left\vert \tau\left(
\left[  n-1\right]  \right)  \right\vert =\left\vert \left[  n-1\right]
\right\vert =n-1$.
\par
Now, $\tau\mid_{\left[  n-1\right]  }$ is a map $\left[  n-1\right]
\rightarrow\left[  n\right]  $ and satisfies $\left\vert \underbrace{\left(
\tau\mid_{\left[  n-1\right]  }\right)  \left(  \left[  n-1\right]  \right)
}_{=\tau\left(  \left[  n-1\right]  \right)  }\right\vert =\left\vert
\tau\left(  \left[  n-1\right]  \right)  \right\vert =n-1\geq n-1$. Hence,
$\tau\mid_{\left[  n-1\right]  }$ is a map $f:\left[  n-1\right]
\rightarrow\left[  n\right]  $ satisfying $\left\vert f\left(  \left[
n-1\right]  \right)  \right\vert \geq n-1$. In other words,%
\[
\tau\mid_{\left[  n-1\right]  }\in\left\{  f:\left[  n-1\right]
\rightarrow\left[  n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]
\right)  \right\vert \geq n-1\right\}  .
\]
In light of (\ref{pf.lem.sol.exeadd.powerdet.gen.kappas.Y=}), this rewrites as
$\tau\mid_{\left[  n-1\right]  }\in Y$. Qed.}. Hence, we can define a map
$T:S_{n}\rightarrow Y$ by%
\[
\left(  T\left(  \tau\right)  =\tau\mid_{\left[  n-1\right]  }%
\ \ \ \ \ \ \ \ \ \ \text{for all }\tau\in S_{n}\right)  .
\]
Consider this map $T$.
\end{verlong}

\begin{vershort}
Each $g\in Y$ satisfies $g\in T\left(  S_{n}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $g\in Y$. Thus, $g\in Y=\left\{
f:\left[  n-1\right]  \rightarrow\left[  n\right]  \ \mid\ \left\vert f\left(
\left[  n-1\right]  \right)  \right\vert \geq n-1\right\}  $. In other words,
$g$ is a map $\left[  n-1\right]  \rightarrow\left[  n\right]  $ and satisfies
$\left\vert g\left(  \left[  n-1\right]  \right)  \right\vert \geq n-1$.
\par
Thus, $\left\vert g\left(  \left[  n-1\right]  \right)  \right\vert \geq
n-1=\left\vert \left[  n-1\right]  \right\vert $. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(b)} (applied to
$U=\left[  n-1\right]  $, $V=\left[  n\right]  $ and $f=g$) shows that the map
$g$ is injective. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(c)} (applied to
$U=\left[  n-1\right]  $, $V=\left[  n\right]  $, $f=g$ and $S=\left[
n-1\right]  $) shows that $\left\vert g\left(  \left[  n-1\right]  \right)
\right\vert =\left\vert \left[  n-1\right]  \right\vert =n-1$. Since $g\left(
\left[  n-1\right]  \right)  $ is a subset of $\left[  n\right]  $, we have%
\[
\left\vert \left[  n\right]  \setminus g\left(  \left[  n-1\right]  \right)
\right\vert =\underbrace{\left\vert \left[  n\right]  \right\vert }%
_{=n}-\underbrace{\left\vert g\left(  \left[  n-1\right]  \right)  \right\vert
}_{=n-1}=n-\left(  n-1\right)  =1.
\]
In other words, $\left[  n\right]  \setminus g\left(  \left[  n-1\right]
\right)  $ is a $1$-element set. In other words, $\left[  n\right]  \setminus
g\left(  \left[  n-1\right]  \right)  =\left\{  p\right\}  $ for some element
$p$. Consider this $p$.
\par
We have $p\in\left\{  p\right\}  =\left[  n\right]  \setminus g\left(  \left[
n-1\right]  \right)  \subseteq\left[  n\right]  $.
\par
Now, define a map $\sigma:\left[  n\right]  \rightarrow\left[  n\right]  $ by%
\[
\left(  \sigma\left(  i\right)  =%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  n\right]  \right)  .
\]
(This is well-defined, because each $i\in\left[  n\right]  $ satisfies $%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
\in\left[  n\right]  $.)
\par
We shall now show that the map $\sigma$ is surjective.
\par
Indeed, $n\notin\left\{  1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $ but
$n\in\left[  n\right]  $. Hence, the definition of $\sigma$ yields%
\[
\sigma\left(  n\right)  =%
\begin{cases}
g\left(  n\right)  , & \text{if }n\in\left[  n-1\right]  ;\\
p, & \text{if }n\notin\left[  n-1\right]
\end{cases}
=p\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n\notin\left[  n-1\right]  \right)
.
\]
But $\sigma\left(  \underbrace{n}_{\in\left[  n\right]  }\right)  \in
\sigma\left(  \left[  n\right]  \right)  $, so that $p=\sigma\left(  n\right)
\in\sigma\left(  \left[  n\right]  \right)  $.
\par
Also, each $i\in\left[  n-1\right]  $ satisfies%
\begin{equation}
\sigma\left(  i\right)  =%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
=g\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\in\left[
n-1\right]  \right)  .
\label{pf.lem.sol.exeadd.powerdet.gen.kappas.short.sur.24}%
\end{equation}
\par
Now,%
\[
g\left(  \left[  n-1\right]  \right)  =\left\{  \underbrace{g\left(  i\right)
}_{\substack{=\sigma\left(  i\right)  \\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.kappas.short.sur.24}))}}}\ \mid
\ i\in\left[  n-1\right]  \right\}  =\left\{  \sigma\left(  i\right)
\ \mid\ i\in\left[  n-1\right]  \right\}  =\sigma\left(  \underbrace{\left[
n-1\right]  }_{\subseteq\left[  n\right]  }\right)  \subseteq\sigma\left(
\left[  n\right]  \right)  .
\]
\par
If $X$ is a set, and if $Y$ is a subset of $X$, then $X=Y\cup\left(
X\setminus Y\right)  $. Applying this to $X=\left[  n\right]  $ and
$Y=g\left(  \left[  n-1\right]  \right)  $, we obtain%
\[
\left[  n\right]  =\underbrace{g\left(  \left[  n-1\right]  \right)
}_{\subseteq\sigma\left(  \left[  n\right]  \right)  }\cup\underbrace{\left(
\left[  n\right]  \setminus g\left(  \left[  n-1\right]  \right)  \right)
}_{\substack{=\left\{  p\right\}  \subseteq\sigma\left(  \left[  n\right]
\right)  \\\text{(since }p\in\sigma\left(  \left[  n\right]  \right)
\text{)}}}\subseteq\sigma\left(  \left[  n\right]  \right)  \cup\sigma\left(
\left[  n\right]  \right)  =\sigma\left(  \left[  n\right]  \right)  .
\]
Combining this with the obvious relation $\sigma\left(  \left[  n\right]
\right)  \subseteq\left[  n\right]  $, we obtain $\left[  n\right]
=\sigma\left(  \left[  n\right]  \right)  $. In other words, the map $\sigma$
is surjective.
\par
Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon} (applied to $U=\left[
n\right]  $, $V=\left[  n\right]  $ and $f=\sigma$) shows that we have the
following logical equivalence:%
\[
\left(  \sigma\text{ is surjective}\right)  \ \Longleftrightarrow\ \left(
\sigma\text{ is bijective}\right)
\]
(since $\left\vert \left[  n\right]  \right\vert =\left\vert \left[  n\right]
\right\vert $). Thus, $\sigma$ is bijective (since $\sigma$ is surjective).
Hence, $\sigma$ is a bijective map $\left[  n\right]  \rightarrow\left[
n\right]  $. In other words, $\sigma$ is a permutation of $\left[  n\right]
$. In other words, $\sigma\in S_{n}$ (since $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $). Hence,
$T\left(  \sigma\right)  $ is well-defined.
\par
The definition of $T$ yields $T\left(  \sigma\right)  =\sigma\mid_{\left[
n-1\right]  }$. Thus, each $i\in\left[  n-1\right]  $ satisfies%
\[
\left(  T\left(  \sigma\right)  \right)  \left(  i\right)  =\left(  \sigma
\mid_{\left[  n-1\right]  }\right)  \left(  i\right)  =\sigma\left(  i\right)
=g\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exeadd.powerdet.gen.kappas.short.sur.24})}\right)  .
\]
In other words, $T\left(  \sigma\right)  =g$. Thus, $g=T\left(
\underbrace{\sigma}_{\in S_{n}}\right)  \in T\left(  S_{n}\right)  $. Qed.}.
In other words, $Y\subseteq T\left(  S_{n}\right)  $. Combining this with
$T\left(  S_{n}\right)  \subseteq Y$ (which is obvious), we obtain $Y=T\left(
S_{n}\right)  $. In other words, the map $T$ is surjective.
\end{vershort}

\begin{verlong}
Each $g\in Y$ satisfies $g\in T\left(  S_{n}\right)  $%
\ \ \ \ \footnote{\textit{Proof.} Let $g\in Y$. Thus, $g\in Y=\left\{
f:\left[  n-1\right]  \rightarrow\left[  n\right]  \ \mid\ \left\vert f\left(
\left[  n-1\right]  \right)  \right\vert \geq n-1\right\}  $. In other words,
$g$ is a map $f:\left[  n-1\right]  \rightarrow\left[  n\right]  $ satisfying
$\left\vert f\left(  \left[  n-1\right]  \right)  \right\vert \geq n-1$. In
other words, $g$ is a map $\left[  n-1\right]  \rightarrow\left[  n\right]  $
and satisfies $\left\vert g\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1$.
\par
Thus, $\left\vert g\left(  \left[  n-1\right]  \right)  \right\vert \geq
n-1=\left\vert \left[  n-1\right]  \right\vert $. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(b)} (applied to
$U=\left[  n-1\right]  $, $V=\left[  n\right]  $ and $f=g$) shows that the map
$g$ is injective. Hence, Lemma
\ref{lem.sol.exeadd.noncomm.polarization.pigeon-inj} \textbf{(c)} (applied to
$U=\left[  n-1\right]  $, $V=\left[  n\right]  $, $f=g$ and $S=\left[
n-1\right]  $) shows that $\left\vert g\left(  \left[  n-1\right]  \right)
\right\vert =\left\vert \left[  n-1\right]  \right\vert =n-1$. Since $g\left(
\left[  n-1\right]  \right)  $ is a subset of $\left[  n\right]  $, we have%
\[
\left\vert \left[  n\right]  \setminus g\left(  \left[  n-1\right]  \right)
\right\vert =\underbrace{\left\vert \left[  n\right]  \right\vert }%
_{=n}-\underbrace{\left\vert g\left(  \left[  n-1\right]  \right)  \right\vert
}_{=n-1}=n-\left(  n-1\right)  =1.
\]
In other words, $\left[  n\right]  \setminus g\left(  \left[  n-1\right]
\right)  $ is a $1$-element set. In other words, $\left[  n\right]  \setminus
g\left(  \left[  n-1\right]  \right)  =\left\{  p\right\}  $ for some element
$p$. Consider this $p$.
\par
We have $p\in\left\{  p\right\}  =\left[  n\right]  \setminus g\left(  \left[
n-1\right]  \right)  \subseteq\left[  n\right]  $.
\par
Now, each $i\in\left[  n\right]  $ satisfies $%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
\in\left[  n\right]  $ (because we have $g\left(  i\right)  \in\left[
n\right]  $ whenever $i\in\left[  n-1\right]  $ (since $g$ is a map $\left[
n-1\right]  \rightarrow\left[  n\right]  $), and because we have $p\in\left[
n\right]  $ whenever $i\notin\left[  n-1\right]  $ (since $p\in\left[
n\right]  $)).
\par
Now, define a map $\sigma:\left[  n\right]  \rightarrow\left[  n\right]  $ by%
\[
\left(  \sigma\left(  i\right)  =%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
\ \ \ \ \ \ \ \ \ \ \text{for each }i\in\left[  n\right]  \right)  .
\]
(This is well-defined, because each $i\in\left[  n\right]  $ satisfies $%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
\in\left[  n\right]  $.)
\par
We shall now show that the map $\sigma$ is surjective.
\par
Indeed, $n\notin\left\{  1,2,\ldots,n-1\right\}  =\left[  n-1\right]  $ but
$n\in\left[  n\right]  $. Hence, the definition of $\sigma$ yields%
\[
\sigma\left(  n\right)  =%
\begin{cases}
g\left(  n\right)  , & \text{if }n\in\left[  n-1\right]  ;\\
p, & \text{if }n\notin\left[  n-1\right]
\end{cases}
=p\ \ \ \ \ \ \ \ \ \ \left(  \text{since }n\notin\left[  n-1\right]  \right)
.
\]
But $\sigma\left(  \underbrace{n}_{\in\left[  n\right]  }\right)  \in
\sigma\left(  \left[  n\right]  \right)  $, so that $p=\sigma\left(  n\right)
\in\sigma\left(  \left[  n\right]  \right)  $.
\par
Also, each $i\in\left[  n-1\right]  $ satisfies $i\in\left[  n-1\right]
\subseteq\left[  n\right]  $. Therefore, for each $i\in\left[  n-1\right]  $,
the value $\sigma\left(  i\right)  $ is well-defined. Furthermore, each
$i\in\left[  n-1\right]  $ satisfies%
\begin{equation}
\sigma\left(  i\right)  =%
\begin{cases}
g\left(  i\right)  , & \text{if }i\in\left[  n-1\right]  ;\\
p, & \text{if }i\notin\left[  n-1\right]
\end{cases}
=g\left(  i\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }i\in\left[
n-1\right]  \right)  . \label{pf.lem.sol.exeadd.powerdet.gen.kappas.sur.24}%
\end{equation}
\par
Now,%
\[
g\left(  \left[  n-1\right]  \right)  =\left\{  \underbrace{g\left(  i\right)
}_{\substack{=\sigma\left(  i\right)  \\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.kappas.sur.24}))}}}\ \mid\ i\in\left[
n-1\right]  \right\}  =\left\{  \sigma\left(  i\right)  \ \mid\ i\in\left[
n-1\right]  \right\}  =\sigma\left(  \underbrace{\left[  n-1\right]
}_{\subseteq\left[  n\right]  }\right)  \subseteq\sigma\left(  \left[
n\right]  \right)  .
\]
\par
If $X$ is a set, and if $Y$ is a subset of $X$, then $X=Y\cup\left(
X\setminus Y\right)  $. Applying this to $X=\left[  n\right]  $ and
$Y=g\left(  \left[  n-1\right]  \right)  $, we obtain%
\[
\left[  n\right]  =\underbrace{g\left(  \left[  n-1\right]  \right)
}_{\subseteq\sigma\left(  \left[  n\right]  \right)  }\cup\underbrace{\left(
\left[  n\right]  \setminus g\left(  \left[  n-1\right]  \right)  \right)
}_{\substack{=\left\{  p\right\}  \subseteq\sigma\left(  \left[  n\right]
\right)  \\\text{(since }p\in\sigma\left(  \left[  n\right]  \right)
\text{)}}}\subseteq\sigma\left(  \left[  n\right]  \right)  \cup\sigma\left(
\left[  n\right]  \right)  =\sigma\left(  \left[  n\right]  \right)  .
\]
Combining this with the obvious relation $\sigma\left(  \left[  n\right]
\right)  \subseteq\left[  n\right]  $, we obtain $\left[  n\right]
=\sigma\left(  \left[  n\right]  \right)  $. In other words, the map $\sigma$
is surjective.
\par
Lemma \ref{lem.sol.exeadd.noncomm.polarization.pigeon} (applied to $U=\left[
n\right]  $, $V=\left[  n\right]  $ and $f=\sigma$) shows that we have the
following logical equivalence:%
\[
\left(  \sigma\text{ is surjective}\right)  \ \Longleftrightarrow\ \left(
\sigma\text{ is bijective}\right)
\]
(since $\left\vert \left[  n\right]  \right\vert =\left\vert \left[  n\right]
\right\vert $). Thus, $\sigma$ is bijective (since $\sigma$ is surjective).
Hence, $\sigma$ is a bijective map $\left[  n\right]  \rightarrow\left[
n\right]  $. In other words, $\sigma$ is a permutation of $\left[  n\right]
$. In other words, $\sigma$ is a permutation of $\left\{  1,2,\ldots
,n\right\}  $ (since $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $). In
other words, $\sigma\in S_{n}$ (since $S_{n}$ is the set of all permutations
of $\left\{  1,2,\ldots,n\right\}  $). Hence, $T\left(  \sigma\right)  $ is
well-defined.
\par
The definition of $T$ yields $T\left(  \sigma\right)  =\sigma\mid_{\left[
n-1\right]  }$. Thus, each $i\in\left[  n-1\right]  $ satisfies%
\[
\underbrace{\left(  T\left(  \sigma\right)  \right)  }_{=\sigma\mid_{\left[
n-1\right]  }}\left(  i\right)  =\left(  \sigma\mid_{\left[  n-1\right]
}\right)  \left(  i\right)  =\sigma\left(  i\right)  =g\left(  i\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exeadd.powerdet.gen.kappas.sur.24})}\right)  .
\]
In other words, $T\left(  \sigma\right)  =g$. Thus, $g=T\left(
\underbrace{\sigma}_{\in S_{n}}\right)  \in T\left(  S_{n}\right)  $. Qed.}.
In other words, $Y\subseteq T\left(  S_{n}\right)  $. Combining this with
$T\left(  S_{n}\right)  \subseteq Y$ (which is obvious), we obtain $Y=T\left(
S_{n}\right)  $. In other words, the map $T$ is surjective.
\end{verlong}

If $\alpha\in S_{n}$ and $\beta\in S_{n}$ satisfy $T\left(  \alpha\right)
=T\left(  \beta\right)  $, then $\alpha=\beta$%
\ \ \ \ \footnote{\textit{Proof.} Let $\alpha\in S_{n}$ and $\beta\in S_{n}$
be such that $T\left(  \alpha\right)  =T\left(  \beta\right)  $. We must show
that $\alpha=\beta$.
\par
The definition of $T$ yields $T\left(  \alpha\right)  =\alpha\mid_{\left[
n-1\right]  }$ and $T\left(  \beta\right)  =\beta\mid_{\left[  n-1\right]  }$.
Hence, $\alpha\mid_{\left[  n-1\right]  }=T\left(  \alpha\right)  =T\left(
\beta\right)  =\beta\mid_{\left[  n-1\right]  }$.
\par
Now, each $i\in\left[  n-1\right]  $ satisfies $\alpha\left(  i\right)
=\underbrace{\left(  \alpha\mid_{\left[  n-1\right]  }\right)  }_{=\beta
\mid_{\left[  n-1\right]  }}\left(  i\right)  =\left(  \beta\mid_{\left[
n-1\right]  }\right)  \left(  i\right)  =\beta\left(  i\right)  $. Thus,
$\alpha\left(  i\right)  =\beta\left(  i\right)  $ for each $i\in\left[
n-1\right]  $. In other words, $\alpha\left(  i\right)  =\beta\left(
i\right)  $ for each $i\in\left[  n\right]  \setminus\left\{  n\right\}  $
(since $\left[  n-1\right]  =\left[  n\right]  \setminus\left\{  n\right\}
$). Thus, Lemma \ref{lem.sol.exeadd.powerdet.gen.allbut1} (applied to $p=n$)
yields $\alpha=\beta$. Qed.}. In other words, the map $T$ is injective.

The map $T$ is surjective and injective. In other words, the map $T$ is bijective.

Recall that $T$ is a map from $S_{n}$ to $Y$. In other words, $T$ is a map
from $S_{n}$ to $\left\{  f:\left[  n-1\right]  \rightarrow\left[  n\right]
\ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert \geq
n-1\right\}  $ (since \newline$Y=\left\{  f:\left[  n-1\right]  \rightarrow
\left[  n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)
\right\vert \geq n-1\right\}  $). Hence, the map
\begin{align*}
S_{n}  &  \rightarrow\left\{  f:\left[  n-1\right]  \rightarrow\left[
n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1\right\}  ,\\
\tau &  \mapsto\tau\mid_{\left[  n-1\right]  }%
\end{align*}
is precisely the map $T$ (since $T\left(  \tau\right)  =\tau\mid_{\left[
n-1\right]  }$ for all $\tau\in S_{n}$). Hence, this map is well-defined and
bijective (since $T$ is bijective). This proves Lemma
\ref{lem.sol.exeadd.powerdet.gen.kappas}.
\end{proof}

\begin{lemma}
\label{lem.sol.exeadd.powerdet.gen.taures}Let $n\geq1$ be an integer. Let
$\tau\in S_{n}$. Then,%
\[
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{\left(
\tau\mid_{\left[  n-1\right]  }\right)  \left(  i\right)  ,\sigma\left(
\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(  i\right)  \right)
}=\sum_{q=1}^{n}\left(  -1\right)  ^{\tau\left(  n\right)  +q}\det\left(
A_{\sim\left(  \tau\left(  n\right)  \right)  ,\sim q}\right)  .
\]

\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.taures}.]We have $\tau\in
S_{n}$. In other words, $\tau$ is a permutation of $\left[  n\right]  $ (since
$S_{n}$ is the set of all permutations of $\left\{  1,2,\ldots,n\right\}
=\left[  n\right]  $). In other words, $\tau$ is a bijective map $\left[
n\right]  \rightarrow\left[  n\right]  $. Thus, $\tau:\left[  n\right]
\rightarrow\left[  n\right]  $ is a bijection.

Also, $n\in\left[  n\right]  $ (since $n\geq1$) and $\left[  n-1\right]
=\left[  n\right]  \setminus\left\{  n\right\}  \subseteq\left[  n\right]  $.

Let $p=\tau\left(  n\right)  $. Then, $p=\tau\left(  n\right)  \in\left[
n\right]  =\left\{  1,2,\ldots,n\right\}  $. From $p=\tau\left(  n\right)  $,
we obtain $n=\tau^{-1}\left(  p\right)  $.

If $b_{1},b_{2},\ldots,b_{n}$ are $n$ elements of $\mathbb{K}$, then%
\begin{equation}
\prod_{i=1}^{n-1}b_{\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(
i\right)  }=\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}b_{i} \label{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod}):} Let $b_{1}%
,b_{2},\ldots,b_{n}$ be $n$ elements of $\mathbb{K}$.
\par
For each $i\in\left[  n\right]  $, we have the following chain of logical
equivalences:%
\[
\left(  i\neq\underbrace{n}_{=\tau^{-1}\left(  p\right)  }\right)
\ \Longleftrightarrow\ \left(  i\neq\tau^{-1}\left(  p\right)  \right)
\ \Longleftrightarrow\ \left(  \tau\left(  i\right)  \neq p\right)
\]
(since $\tau$ is a bijection). In other words, for each $i\in\left[  n\right]
$, the condition $\left(  i\neq n\right)  $ holds if and only if the condition
$\left(  \tau\left(  i\right)  \neq p\right)  $ holds. Thus, we have the
following equality of product signs:%
\[
\prod_{\substack{i\in\left[  n\right]  ;\\i\neq n}}=\prod_{\substack{i\in
\left[  n\right]  ;\\\tau\left(  i\right)  \neq p}}.
\]
Hence,%
\begin{align}
\prod_{\substack{i\in\left[  n\right]  ;\\\tau\left(  i\right)  \neq p}}  &
=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq n}}=\prod_{i\in\left[
n\right]  \setminus\left\{  n\right\}  }=\prod_{i\in\left[  n-1\right]
}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  n\right]  \setminus\left\{
n\right\}  =\left[  n-1\right]  \right) \nonumber\\
&  =\prod_{i=1}^{n-1}.
\label{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod.preq}%
\end{align}
\par
Now, the map $\tau:\left[  n\right]  \rightarrow\left[  n\right]  $ is a
bijection. Thus, we can substitute $\tau\left(  i\right)  $ for $i$ in the
product $\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}b_{i}$. We thus
obtain%
\begin{equation}
\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}b_{i}=\underbrace{\prod
_{\substack{i\in\left[  n\right]  ;\\\tau\left(  i\right)  \neq p}%
}}_{\substack{=\prod_{i=1}^{n-1}\\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod.preq}))}}%
}b_{\tau\left(  i\right)  }=\prod_{i=1}^{n-1}b_{\tau\left(  i\right)  }.
\label{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod.3}%
\end{equation}
Now,%
\begin{align*}
\prod_{i=1}^{n-1}\underbrace{b_{\left(  \tau\mid_{\left[  n-1\right]
}\right)  \left(  i\right)  }}_{\substack{=b_{\tau\left(  i\right)
}\\\text{(since }\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(
i\right)  =\tau\left(  i\right)  \text{)}}}  &  =\prod_{i=1}^{n-1}%
b_{\tau\left(  i\right)  }=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
p}}b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod.3})}\right) \\
&  =\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}%
}b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \right)  .
\end{align*}
This proves (\ref{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod}).}.

We have%
\begin{align*}
&  \underbrace{\sum_{\sigma\in S_{n}}}_{\substack{=\sum_{q\in\left[  n\right]
}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}%
}\\\text{(because for each }\sigma\in S_{n}\text{, there}\\\text{exists a
unique }q\in\left[  n\right]  \\\text{such that }\sigma\left(  p\right)
=q\text{)}}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n-1}%
a_{\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(  i\right)
,\sigma\left(  \left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(
i\right)  \right)  }}_{\substack{=\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.short.bprod}), applied to }%
b_{k}=a_{k,\sigma\left(  k\right)  }\text{)}}}\\
&  =\underbrace{\sum_{q\in\left[  n\right]  }}_{=\sum_{q=1}^{n}}%
\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}_{\substack{=\left(
-1\right)  ^{p+q}\det\left(  A_{\sim p,\sim q}\right)  \\\text{(by Lemma
\ref{lem.laplace.Apq})}}}\\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  =\sum_{q=1}^{n}\left(  -1\right)  ^{\tau\left(  n\right)  +q}%
\det\left(  A_{\sim\left(  \tau\left(  n\right)  \right)  ,\sim q}\right)
\end{align*}
(since $p=\tau\left(  n\right)  $). This proves Lemma
\ref{lem.sol.exeadd.powerdet.gen.taures}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.taures}.]We have $\tau\in
S_{n}$. In other words, $\tau$ is a permutation of $\left\{  1,2,\ldots
,n\right\}  $ (since $S_{n}$ is the set of all permutations of $\left\{
1,2,\ldots,n\right\}  $). In other words, $\tau$ is a permutation of $\left[
n\right]  $ (since $\left\{  1,2,\ldots,n\right\}  =\left[  n\right]  $). In
other words, $\tau$ is a bijective map $\left[  n\right]  \rightarrow\left[
n\right]  $.

Also, $n\in\left[  n\right]  $ (since $n\geq1$) and%
\[
\underbrace{\left[  n\right]  }_{=\left\{  1,2,\ldots,n\right\}  }%
\setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n\right\}  \setminus\left\{
n\right\}  =\left\{  1,2,\ldots,n-1\right\}  .
\]
Thus, $\left[  n-1\right]  =\left\{  1,2,\ldots,n-1\right\}  =\left[
n\right]  \setminus\left\{  n\right\}  \subseteq\left[  n\right]  $.

The value $\tau\left(  n\right)  $ is well-defined (since $n\in\left[
n\right]  $). Let $p=\tau\left(  n\right)  $. Then, $p=\tau\left(  n\right)
\in\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $. Also, $\tau$ is a
bijection (since $\tau$ is a bijective map). Therefore, from $p=\tau\left(
n\right)  $, we obtain $n=\tau^{-1}\left(  p\right)  $.

If $b_{1},b_{2},\ldots,b_{n}$ are $n$ elements of $\mathbb{K}$, then%
\begin{equation}
\prod_{i=1}^{n-1}b_{\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(
i\right)  }=\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq
p}}b_{i} \label{pf.lem.sol.exeadd.powerdet.gen.taures.bprod}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.exeadd.powerdet.gen.taures.bprod}%
):} Let $b_{1},b_{2},\ldots,b_{n}$ be $n$ elements of $\mathbb{K}$.
\par
For each $i\in\left[  n\right]  $, we have the following chain of logical
equivalences:%
\begin{equation}
\left(  i=\underbrace{n}_{=\tau^{-1}\left(  p\right)  }\right)
\ \Longleftrightarrow\ \left(  i=\tau^{-1}\left(  p\right)  \right)
\ \Longleftrightarrow\ \left(  \tau\left(  i\right)  =p\right)  .
\label{pf.lem.sol.exeadd.powerdet.gen.taures.bprod.eq1}%
\end{equation}
Thus, for each $i\in\left[  n\right]  $, we have the following chain of
logical equivalences:%
\[
\left(  i\neq n\right)  \ \Longleftrightarrow\ \left(  \text{not
}\underbrace{i=n}_{\substack{\Longleftrightarrow\ \left(  \tau\left(
i\right)  =p\right)  \\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.bprod.eq1}))}}}\right)
\ \Longleftrightarrow\ \left(  \text{not }\tau\left(  i\right)  =p\right)
\ \Longleftrightarrow\ \left(  \tau\left(  i\right)  \neq p\right)  .
\]
In other words, for each $i\in\left[  n\right]  $, the condition $\left(
i\neq n\right)  $ holds if and only if the condition $\left(  \tau\left(
i\right)  \neq p\right)  $ holds. Thus, we have the following equality of
product signs:%
\[
\prod_{\substack{i\in\left[  n\right]  ;\\i\neq n}}=\prod_{\substack{i\in
\left[  n\right]  ;\\\tau\left(  i\right)  \neq p}}.
\]
Hence,%
\begin{align}
\prod_{\substack{i\in\left[  n\right]  ;\\\tau\left(  i\right)  \neq p}}  &
=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq n}}=\prod_{i\in\left[
n\right]  \setminus\left\{  n\right\}  }=\prod_{i\in\left\{  1,2,\ldots
,n-1\right\}  }\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  n\right]
\setminus\left\{  n\right\}  =\left\{  1,2,\ldots,n-1\right\}  \right)
\nonumber\\
&  =\prod_{i=1}^{n-1}.
\label{pf.lem.sol.exeadd.powerdet.gen.taures.bprod.preq}%
\end{align}
\par
Now, the map $\tau:\left[  n\right]  \rightarrow\left[  n\right]  $ is a
bijection (since $\tau$ is a bijective map $\left[  n\right]  \rightarrow
\left[  n\right]  $). Thus, we can substitute $\tau\left(  i\right)  $ for $i$
in the product $\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}b_{i}$. We
thus obtain%
\begin{equation}
\prod_{\substack{i\in\left[  n\right]  ;\\i\neq p}}b_{i}=\underbrace{\prod
_{\substack{i\in\left[  n\right]  ;\\\tau\left(  i\right)  \neq p}%
}}_{\substack{=\prod_{i=1}^{n-1}\\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.bprod.preq}))}}}b_{\tau\left(
i\right)  }=\prod_{i=1}^{n-1}b_{\tau\left(  i\right)  }.
\label{pf.lem.sol.exeadd.powerdet.gen.taures.bprod.3}%
\end{equation}
Now,%
\begin{align*}
\prod_{i=1}^{n-1}\underbrace{b_{\left(  \tau\mid_{\left[  n-1\right]
}\right)  \left(  i\right)  }}_{\substack{=b_{\tau\left(  i\right)
}\\\text{(since }\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(
i\right)  =\tau\left(  i\right)  \text{)}}}  &  =\prod_{i=1}^{n-1}%
b_{\tau\left(  i\right)  }=\prod_{\substack{i\in\left[  n\right]  ;\\i\neq
p}}b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.bprod.3})}\right) \\
&  =\prod_{\substack{i\in\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}%
}b_{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \right)  .
\end{align*}
This proves (\ref{pf.lem.sol.exeadd.powerdet.gen.taures.bprod}).}.

We have%
\begin{align*}
&  \underbrace{\sum_{\sigma\in S_{n}}}_{\substack{=\sum_{q\in\left[  n\right]
}\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)  =q}%
}\\\text{(because for each }\sigma\in S_{n}\text{, there}\\\text{exists a
unique }q\in\left[  n\right]  \\\text{such that }\sigma\left(  p\right)
=q\text{)}}}\left(  -1\right)  ^{\sigma}\underbrace{\prod_{i=1}^{n-1}%
a_{\left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(  i\right)
,\sigma\left(  \left(  \tau\mid_{\left[  n-1\right]  }\right)  \left(
i\right)  \right)  }}_{\substack{=\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\\text{(by
(\ref{pf.lem.sol.exeadd.powerdet.gen.taures.bprod}), applied to }%
b_{k}=a_{k,\sigma\left(  k\right)  }\text{)}}}\\
&  =\underbrace{\sum_{q\in\left[  n\right]  }}_{\substack{=\sum_{q\in\left\{
1,2,\ldots,n\right\}  }\\\text{(since }\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  \text{)}}}\sum_{\substack{\sigma\in S_{n};\\\sigma
\left(  p\right)  =q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in
\left\{  1,2,\ldots,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }\\
&  =\underbrace{\sum_{q\in\left\{  1,2,\ldots,n\right\}  }}_{=\sum_{q=1}^{n}%
}\underbrace{\sum_{\substack{\sigma\in S_{n};\\\sigma\left(  p\right)
=q}}\left(  -1\right)  ^{\sigma}\prod_{\substack{i\in\left\{  1,2,\ldots
,n\right\}  ;\\i\neq p}}a_{i,\sigma\left(  i\right)  }}_{\substack{=\left(
-1\right)  ^{p+q}\det\left(  A_{\sim p,\sim q}\right)  \\\text{(by Lemma
\ref{lem.laplace.Apq})}}}\\
&  =\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim
q}\right)  =\sum_{q=1}^{n}\left(  -1\right)  ^{\tau\left(  n\right)  +q}%
\det\left(  A_{\sim\left(  \tau\left(  n\right)  \right)  ,\sim q}\right)
\end{align*}
(since $p=\tau\left(  n\right)  $). This proves Lemma
\ref{lem.sol.exeadd.powerdet.gen.taures}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.exeadd.powerdet.gen.n-1}Let $n\geq1$ be an integer.

\textbf{(a)} We have $\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \right\vert =\left(  n-1\right)  !$.

\textbf{(b)} Let $p\in\left[  n\right]  $ and $q\in\left[  n\right]  $. Then,%
\[
\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\right\vert =\left(  n-1\right)  !.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1}.]We have $n\geq1$. Thus,
$n-1\in\mathbb{N}$.

Recall that $\left\vert S_{n}\right\vert =n!$. The same argument (applied to
$n-1$ instead of $n$) yields $\left\vert S_{n-1}\right\vert =\left(
n-1\right)  !$ (since $n-1\in\mathbb{N}$).

\textbf{(a)} Clearly, $n$ is a positive integer (since $n$ is an integer
satisfying $n\geq1$).

Define a subset $T$ of $S_{n}$ by $T=\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  $. Define a map $\Phi:S_{n-1}\rightarrow T$ as in the
proof of Lemma \ref{lem.laplace.lem}. Then, the map $\Phi$ is a
bijection\footnote{This was shown in the proof of Lemma \ref{lem.laplace.lem}%
.}. Hence, there exists a bijection $S_{n-1}\rightarrow T$ (namely, $\Phi$).
Thus, $\left\vert T\right\vert =\left\vert S_{n-1}\right\vert =\left(
n-1\right)  !$. Since $T=\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  $, this rewrites as $\left\vert \left\{  \tau\in S_{n}\ \mid
\ \tau\left(  n\right)  =n\right\}  \right\vert =\left(  n-1\right)  !$. This
proves Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1} \textbf{(a)}.

\begin{vershort}
\textbf{(b)} Define a map $T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $ as in Lemma \ref{lem.laplace.gpshort} \textbf{(d)}.
Then, Lemma \ref{lem.laplace.gpshort} \textbf{(d)} shows that this map $T$ is
well-defined and bijective. Thus, there exists a bijection from $\left\{
\tau\in S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $ to $\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  $ (namely, $T$). Hence,%
\[
\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\right\vert =\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  \right\vert =\left(  n-1\right)  !
\]
(by Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1} \textbf{(a)}). This proves
Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1} \textbf{(b)}.
\end{vershort}

\begin{verlong}
\textbf{(b)} Define a map $T:\left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =n\right\}  \rightarrow\left\{  \tau\in S_{n}\ \mid\ \tau\left(
p\right)  =q\right\}  $ as in Lemma \ref{lem.laplace.gp} \textbf{(g)}. Then,
Lemma \ref{lem.laplace.gp} \textbf{(g)} shows that this map $T$ is
well-defined and bijective. Thus, $T$ is a bijection from $\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =n\right\}  $ to $\left\{  \tau\in
S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}  $ (namely, $T$). Hence, there
exists a bijection from $\left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  $ to $\left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)
=q\right\}  $. Hence,%
\[
\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(  p\right)  =q\right\}
\right\vert =\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(  n\right)
=n\right\}  \right\vert =\left(  n-1\right)  !
\]
(by Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1} \textbf{(a)}). This proves
Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1} \textbf{(b)}.
\end{verlong}
\end{proof}

Finally, we are approaching part \textbf{(b)} of Additional exercise
\ref{exeadd.powerdet.gen}:

\begin{proposition}
\label{prop.sol.exeadd.powerdet.gen.b}Let $n\geq1$ be an integer. Let
$A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ be an $n\times
n$-matrix. Then,%
\[
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\left(  \sum_{i=1}%
^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{n-1}=\left(  n-1\right)
!\cdot\sum_{p=1}^{n}\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim
p,\sim q}\right)  .
\]

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.sol.exeadd.powerdet.gen.b}.]From $n\geq1$, we
obtain $n-1\in\mathbb{N}$. Hence, for every $\sigma\in S_{n}$, we have%
\begin{equation}
\left(  \sum_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{n-1}%
=\sum_{\kappa:\left[  n-1\right]  \rightarrow\left[  n\right]  }\prod
_{i=1}^{n-1}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)
\right)  } \label{pf.prop.sol.exeadd.powerdet.gen.b.prodrule}%
\end{equation}
(by Corollary \ref{cor.sol.exeadd.powerdet.gen.prodrule}, applied to $k=n-1$).

Lemma \ref{lem.sol.exeadd.powerdet.gen.kappas} shows that the map%
\begin{align*}
S_{n}  &  \rightarrow\left\{  f:\left[  n-1\right]  \rightarrow\left[
n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1\right\}  ,\\
\tau &  \mapsto\tau\mid_{\left[  n-1\right]  }%
\end{align*}
is well-defined and bijective. Thus, this map is a bijection.

\begin{verlong}
Notice that $n\in\left[  n\right]  $ (since $n\geq1$).
\end{verlong}

We have $\left[  n\right]  =\left\{  1,2,\ldots,n\right\}  $. Hence, we have
the following equality of summation signs:%
\begin{equation}
\sum_{p\in\left[  n\right]  }=\sum_{p\in\left\{  1,2,\ldots,n\right\}  }%
=\sum_{p=1}^{n}. \label{pf.prop.sol.exeadd.powerdet.gen.b.sumeqp}%
\end{equation}


Now,%
\begin{align*}
&  \sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\underbrace{\left(
\sum_{i=1}^{n}a_{i,\sigma\left(  i\right)  }\right)  ^{n-1}}_{\substack{=\sum
_{\kappa:\left[  n-1\right]  \rightarrow\left[  n\right]  }\prod_{i=1}%
^{n-1}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)
\right)  }\\\text{(by (\ref{pf.prop.sol.exeadd.powerdet.gen.b.prodrule}))}}}\\
&  =\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\sum_{\kappa:\left[
n-1\right]  \rightarrow\left[  n\right]  }\prod_{i=1}^{n-1}a_{\kappa\left(
i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }=\underbrace{\sum
_{\sigma\in S_{n}}\sum_{\kappa:\left[  n-1\right]  \rightarrow\left[
n\right]  }}_{=\sum_{\kappa:\left[  n-1\right]  \rightarrow\left[  n\right]
}\sum_{\sigma\in S_{n}}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }\\
&  =\sum_{\kappa:\left[  n-1\right]  \rightarrow\left[  n\right]  }%
\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}%
a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)  \right)  }\\
&  =\sum_{\substack{\kappa:\left[  n-1\right]  \rightarrow\left[  n\right]
;\\\left\vert \kappa\left(  \left[  n-1\right]  \right)  \right\vert
<n-1}}\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n-1}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(
i\right)  \right)  }}_{\substack{=0\\\text{(by Lemma
\ref{lem.sol.exeadd.powerdet.gen.toofew} (applied to }k=n-1\text{))}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{\kappa:\left[  n-1\right]
\rightarrow\left[  n\right]  ;\\\left\vert \kappa\left(  \left[  n-1\right]
\right)  \right\vert \geq n-1}}}_{=\sum_{\kappa\in\left\{  f:\left[
n-1\right]  \rightarrow\left[  n\right]  \ \mid\ \left\vert f\left(  \left[
n-1\right]  \right)  \right\vert \geq n-1\right\}  }}\sum_{\sigma\in S_{n}%
}\left(  -1\right)  ^{\sigma}\prod_{i=1}^{n-1}a_{\kappa\left(  i\right)
,\sigma\left(  \kappa\left(  i\right)  \right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since each map }\kappa:\left[  n-1\right]  \rightarrow\left[  n\right]
\text{ satisfies either }\left\vert \kappa\left(  \left[  n-1\right]  \right)
\right\vert <n-1\\
\text{or }\left\vert \kappa\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1\text{ (but not both)}%
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{\kappa:\left[  n-1\right]  \rightarrow\left[
n\right]  ;\\\left\vert \kappa\left(  \left[  n-1\right]  \right)  \right\vert
<n-1}}0}_{=0}+\sum_{\kappa\in\left\{  f:\left[  n-1\right]  \rightarrow\left[
n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1\right\}  }\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}%
\prod_{i=1}^{n-1}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(
i\right)  \right)  }\\
&  =\sum_{\kappa\in\left\{  f:\left[  n-1\right]  \rightarrow\left[  n\right]
\ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert \geq
n-1\right\}  }\sum_{\sigma\in S_{n}}\left(  -1\right)  ^{\sigma}\prod
_{i=1}^{n-1}a_{\kappa\left(  i\right)  ,\sigma\left(  \kappa\left(  i\right)
\right)  }%
\end{align*}%
\begin{align*}
&  =\sum_{\tau\in S_{n}}\underbrace{\sum_{\sigma\in S_{n}}\left(  -1\right)
^{\sigma}\prod_{i=1}^{n-1}a_{\left(  \tau\mid_{\left[  n-1\right]  }\right)
\left(  i\right)  ,\sigma\left(  \left(  \tau\mid_{\left[  n-1\right]
}\right)  \left(  i\right)  \right)  }}_{\substack{=\sum_{q=1}^{n}\left(
-1\right)  ^{\tau\left(  n\right)  +q}\det\left(  A_{\sim\left(  \tau\left(
n\right)  \right)  ,\sim q}\right)  \\\text{(by Lemma
\ref{lem.sol.exeadd.powerdet.gen.taures})}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }\tau\mid_{\left[  n-1\right]  }\text{ for
}\kappa\text{ in the outer sum,}\\
\text{since the}\\
\text{map }S_{n}\rightarrow\left\{  f:\left[  n-1\right]  \rightarrow\left[
n\right]  \ \mid\ \left\vert f\left(  \left[  n-1\right]  \right)  \right\vert
\geq n-1\right\}  ,\ \tau\mapsto\tau\mid_{\left[  n-1\right]  }\\
\text{is a bijection}%
\end{array}
\right) \\
&  =\underbrace{\sum_{\tau\in S_{n}}}_{\substack{=\sum_{p\in\left[  n\right]
}\sum_{\substack{\tau\in S_{n};\\\tau\left(  n\right)  =p}}\\\text{(because
for each }\tau\in S_{n}\text{, there}\\\text{exists a unique }p\in\left[
n\right]  \\\text{such that }\tau\left(  n\right)  =p\text{)}}}\sum_{q=1}%
^{n}\left(  -1\right)  ^{\tau\left(  n\right)  +q}\det\left(  A_{\sim\left(
\tau\left(  n\right)  \right)  ,\sim q}\right) \\
&  =\sum_{p\in\left[  n\right]  }\underbrace{\sum_{\substack{\tau\in
S_{n};\\\tau\left(  n\right)  =p}}\sum_{q=1}^{n}}_{=\sum_{q=1}^{n}%
\sum_{\substack{\tau\in S_{n};\\\tau\left(  n\right)  =p}}}\underbrace{\left(
-1\right)  ^{\tau\left(  n\right)  +q}\det\left(  A_{\sim\left(  \tau\left(
n\right)  \right)  ,\sim q}\right)  }_{\substack{=\left(  -1\right)
^{p+q}\det\left(  A_{\sim p,\sim q}\right)  \\\text{(since }\tau\left(
n\right)  =p\text{)}}}\\
&  =\sum_{p\in\left[  n\right]  }\sum_{q=1}^{n}\underbrace{\sum
_{\substack{\tau\in S_{n};\\\tau\left(  n\right)  =p}}\left(  -1\right)
^{p+q}\det\left(  A_{\sim p,\sim q}\right)  }_{=\left\vert \left\{  \tau\in
S_{n}\ \mid\ \tau\left(  n\right)  =p\right\}  \right\vert \left(  -1\right)
^{p+q}\det\left(  A_{\sim p,\sim q}\right)  }\\
&  =\underbrace{\sum_{p\in\left[  n\right]  }}_{\substack{=\sum_{p=1}%
^{n}\\\text{(by (\ref{pf.prop.sol.exeadd.powerdet.gen.b.sumeqp}))}}}\sum
_{q=1}^{n}\underbrace{\left\vert \left\{  \tau\in S_{n}\ \mid\ \tau\left(
n\right)  =p\right\}  \right\vert }_{\substack{=\left(  n-1\right)
!\\\text{(by Lemma \ref{lem.sol.exeadd.powerdet.gen.n-1} \textbf{(b)}%
}\\\text{(applied to }n\text{ and }p\text{ instead of }p\text{ and }%
q\text{))}}}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim p,\sim q}\right) \\
&  =\sum_{p=1}^{n}\sum_{q=1}^{n}\left(  n-1\right)  !\left(  -1\right)
^{p+q}\det\left(  A_{\sim p,\sim q}\right)  =\left(  n-1\right)  !\cdot
\sum_{p=1}^{n}\sum_{q=1}^{n}\left(  -1\right)  ^{p+q}\det\left(  A_{\sim
p,\sim q}\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.exeadd.powerdet.gen.b}.
\end{proof}

\begin{proof}
[Solution to Additional exercise \ref{exeadd.powerdet.gen}.]Additional
exercise \ref{exeadd.powerdet.gen} \textbf{(a)} follows from Proposition
\ref{prop.sol.exeadd.powerdet.gen.a}.

Additional exercise \ref{exeadd.powerdet.gen} \textbf{(b)} follows from
Proposition \ref{prop.sol.exeadd.powerdet.gen.b}.
\end{proof}

\subsection{\label{sect.sol.jacobi-complement}Solution to Additional exercise
\ref{addexe.jacobi-complement}}

\subsubsection{First solution}

Our first solution to Additional exercise \ref{addexe.jacobi-complement}
(inspired by \cite[Lemma 9.2.10]{BruRys91}) shall follow the hint given. We
are going to prepare for it by stating several simple lemmas.

First, let us agree on some notations. We are going to use the notations
introduced in Definition \ref{def.submatrix} and in Definition
\ref{def.sect.laplace.notations} throughout Section
\ref{sect.sol.jacobi-complement}. We are also going to use the following notation:

\begin{definition}
\label{def.sol.addexe.jacobi-complement.Agd}Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\in\mathbb{K}^{n\times m}$. Let $\gamma\in S_{n}$ and $\delta\in S_{m}$.
Then, $A_{\left[  \gamma,\delta\right]  }$ denotes the matrix $\left(
a_{\gamma\left(  i\right)  ,\delta\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}\in\mathbb{K}^{n\times m}$.
\end{definition}

\begin{remark}
Let $n$, $m$, $A$, $\gamma$ and $\delta$ be as in Definition
\ref{def.sol.addexe.jacobi-complement.Agd}. Then, it is easy to see that
$A_{\left[  \gamma,\delta\right]  }=\operatorname*{sub}\nolimits_{\gamma
\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(  n\right)
}^{\delta\left(  1\right)  ,\delta\left(  2\right)  ,\ldots,\delta\left(
m\right)  }A$. Visually speaking, $A_{\left[  \gamma,\delta\right]  }$ is the
matrix obtained from $A$ by permuting the rows (using the permutation $\gamma
$) and permuting the columns (using the permutation $\delta$).
\end{remark}

\begin{remark}
Let $n\in\mathbb{N}$, and let $B\in\mathbb{K}^{n\times n}$. Let $\kappa\in
S_{n}$. Then, Definition \ref{def.sol.addexe.jacobi-complement.Agd} gives rise
to an $n\times n$-matrix $B_{\left[  \kappa,\operatorname*{id}\right]  }$
(where $\operatorname*{id}$ denotes the identity permutation
$\operatorname*{id}\nolimits_{\left\{  1,2,\ldots,n\right\}  }\in S_{n}$).
This matrix $B_{\left[  \kappa,\operatorname*{id}\right]  }$ is precisely the
matrix $B_{\kappa}$ from Lemma \ref{lem.det.sigma}.
\end{remark}

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.Agd.det}Let $n\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times n}$. Let $\gamma\in S_{n}$ and $\delta\in S_{n}$.
Then,%
\[
\det\left(  A_{\left[  \gamma,\delta\right]  }\right)  =\left(  -1\right)
^{\gamma}\left(  -1\right)  ^{\delta}\det A.
\]

\end{lemma}

Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.det} simply says that if we
permute the rows and permute the columns of a square matrix, then the
determinant of this matrix gets multiplied by the product of the signs of the
two permutations.

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.det}.]Let $\left[
n\right]  $ denote the set $\left\{  1,2,\ldots,n\right\}  $.

\begin{verlong}
We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  $). In other words, $\gamma$
is a permutation of $\left[  n\right]  $ (since $\left[  n\right]  =\left\{
1,2,\ldots,n\right\}  $). In other words, $\gamma$ is a bijective map $\left[
n\right]  \rightarrow\left[  n\right]  $. The same argument (but applied to
$\delta$ instead of $\gamma$) shows that $\delta$ is a bijective map $\left[
n\right]  \rightarrow\left[  n\right]  $.
\end{verlong}

Write the $n\times n$-matrix $A$ in the form $A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Then, \newline$A^{T}=\left(  a_{j,i}%
\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of $A^{T}$).

Define a new matrix $C$ by $C=\left(  a_{j,\delta\left(  i\right)  }\right)
_{1\leq i\leq n,\ 1\leq j\leq n}$. Then, Lemma \ref{lem.det.sigma}
\textbf{(a)} (applied to $\delta$, $A^{T}$, $a_{j,i}$ and $C$ instead of
$\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$) yields $\det C=\left(  -1\right)
^{\delta}\cdot\det\left(  A^{T}\right)  $ (because $A^{T}=\left(
a_{j,i}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ and $C=\left(
a_{j,\delta\left(  i\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$).
Thus,%
\[
\det C=\left(  -1\right)  ^{\delta}\cdot\underbrace{\det\left(  A^{T}\right)
}_{\substack{=\det A\\\text{(by Exercise \ref{exe.ps4.4})}}}=\left(
-1\right)  ^{\delta}\cdot\det A.
\]


Clearly, $C$ is an $n\times n$-matrix. Hence, Exercise \ref{exe.ps4.4}
(applied to $C$ instead of $A$) yields
\[
\det\left(  C^{T}\right)  =\det C=\left(  -1\right)  ^{\delta}\cdot\det A.
\]


But we have $C=\left(  a_{j,\delta\left(  i\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$. Hence, $C^{T}=\left(  a_{i,\delta\left(  j\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of $C^{T}$). We
have $C^{T}=\left(  a_{i,\delta\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ and $A_{\left[  \gamma,\delta\right]  }=\left(
a_{\gamma\left(  i\right)  ,\delta\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$ (by the definition of $A_{\left[  \gamma,\delta\right]  }%
$). Thus, Lemma \ref{lem.det.sigma} \textbf{(a)} (applied to $\gamma$, $C^{T}%
$, $a_{i,\delta\left(  j\right)  }$ and $A_{\left[  \gamma,\delta\right]  }$
instead of $\kappa$, $B$, $b_{i,j}$ and $B_{\kappa}$) yields%
\[
\det\left(  A_{\left[  \gamma,\delta\right]  }\right)  =\left(  -1\right)
^{\gamma}\cdot\underbrace{\det\left(  C^{T}\right)  }_{=\left(  -1\right)
^{\delta}\cdot\det A}=\left(  -1\right)  ^{\gamma}\cdot\left(  -1\right)
^{\delta}\cdot\det A=\left(  -1\right)  ^{\gamma}\left(  -1\right)  ^{\delta
}\det A.
\]
This proves Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.det}.
\end{proof}

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.Agd.sub}Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $A\in\mathbb{K}^{n\times m}$. Let $\gamma\in S_{n}$ and
$\delta\in S_{m}$. Let $i_{1},i_{2},\ldots,i_{u}$ be some elements of
$\left\{  1,2,\ldots,n\right\}  $; let $j_{1},j_{2},\ldots,j_{v}$ be some
elements of $\left\{  1,2,\ldots,m\right\}  $. Then,%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  A_{\left[  \gamma,\delta\right]  }\right)
=\operatorname*{sub}\nolimits_{\gamma\left(  i_{1}\right)  ,\gamma\left(
i_{2}\right)  ,\ldots,\gamma\left(  i_{u}\right)  }^{\delta\left(
j_{1}\right)  ,\delta\left(  j_{2}\right)  ,\ldots,\delta\left(  j_{v}\right)
}A.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.sub}.]Write the
$n\times m$-matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Then,
\begin{equation}
\operatorname*{sub}\nolimits_{\gamma\left(  i_{1}\right)  ,\gamma\left(
i_{2}\right)  ,\ldots,\gamma\left(  i_{u}\right)  }^{\delta\left(
j_{1}\right)  ,\delta\left(  j_{2}\right)  ,\ldots,\delta\left(  j_{v}\right)
}A=\left(  a_{\gamma\left(  i_{x}\right)  ,\delta\left(  j_{y}\right)
}\right)  _{1\leq x\leq u,\ 1\leq y\leq v}
\label{pf.lem.sol.addexe.jacobi-complement.Agd.sub.1}%
\end{equation}
(by the definition of $\operatorname*{sub}\nolimits_{\gamma\left(
i_{1}\right)  ,\gamma\left(  i_{2}\right)  ,\ldots,\gamma\left(  i_{u}\right)
}^{\delta\left(  j_{1}\right)  ,\delta\left(  j_{2}\right)  ,\ldots
,\delta\left(  j_{v}\right)  }A$, since $A=\left(  a_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$). On the other hand, the definition of $A_{\left[
\gamma,\delta\right]  }$ yields $A_{\left[  \gamma,\delta\right]  }=\left(
a_{\gamma\left(  i\right)  ,\delta\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Hence, the definition of $\operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2},\ldots,j_{v}}\left(
A_{\left[  \gamma,\delta\right]  }\right)  $ yields%
\[
\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1},j_{2}%
,\ldots,j_{v}}\left(  A_{\left[  \gamma,\delta\right]  }\right)  =\left(
a_{\gamma\left(  i_{x}\right)  ,\delta\left(  j_{y}\right)  }\right)  _{1\leq
x\leq u,\ 1\leq y\leq v}.
\]
Comparing this with (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.sub.1}), we
obtain $\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{u}}^{j_{1}%
,j_{2},\ldots,j_{v}}\left(  A_{\left[  \gamma,\delta\right]  }\right)
=\operatorname*{sub}\nolimits_{\gamma\left(  i_{1}\right)  ,\gamma\left(
i_{2}\right)  ,\ldots,\gamma\left(  i_{u}\right)  }^{\delta\left(
j_{1}\right)  ,\delta\left(  j_{2}\right)  ,\ldots,\delta\left(  j_{v}\right)
}A$. This proves Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.sub}.
\end{proof}

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.Agd.AB}Let $n\in\mathbb{N}$,
$m\in\mathbb{N}$ and $p\in\mathbb{N}$. Let $\gamma\in S_{n}$, $\delta\in
S_{m}$ and $\varepsilon\in S_{p}$. Let $A\in\mathbb{K}^{n\times m}$ and
$B\in\mathbb{K}^{m\times p}$. Then,%
\[
\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]  }=A_{\left[
\gamma,\delta\right]  }B_{\left[  \delta,\varepsilon\right]  }.
\]

\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.AB}.]Write the
$n\times m$-matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$. Then, $A_{\left[  \gamma,\delta\right]  }=\left(
a_{\gamma\left(  i\right)  ,\delta\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$ (by the definition of $A_{\left[  \gamma,\delta\right]  }$).

Write the $m\times p$-matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq m,\ 1\leq j\leq p}$. Then, $B_{\left[  \delta,\varepsilon
\right]  }=\left(  b_{\delta\left(  i\right)  ,\varepsilon\left(  j\right)
}\right)  _{1\leq i\leq m,\ 1\leq j\leq p}$ (by the definition of $B_{\left[
\delta,\varepsilon\right]  }$).

\begin{verlong}
We have $\delta\in S_{m}$. In other words, $\delta$ is a permutation of
$\left\{  1,2,\ldots,m\right\}  $ (since $S_{m}$ is the set of all
permutations of $\left\{  1,2,\ldots,m\right\}  $ (by the definition of
$S_{m}$)). In other words, $\delta$ is a bijective map $\left\{
1,2,\ldots,m\right\}  \rightarrow\left\{  1,2,\ldots,m\right\}  $. In other
words, $\delta:\left\{  1,2,\ldots,m\right\}  \rightarrow\left\{
1,2,\ldots,m\right\}  $ is a bijection.
\end{verlong}

\begin{vershort}
Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,p\right\}  $ satisfies%
\[
\sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,\delta\left(  k\right)  }%
b_{\delta\left(  k\right)  ,\varepsilon\left(  j\right)  }=\sum_{k=1}%
^{m}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon\left(  j\right)  }%
\]
(here, we have substituted $k$ for $\delta\left(  k\right)  $ in the sum,
since the map $\delta:\left\{  1,2,\ldots,m\right\}  \rightarrow\left\{
1,2,\ldots,m\right\}  $ is a bijection). In other words, we have%
\begin{equation}
\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,\delta\left(  k\right)
}b_{\delta\left(  k\right)  ,\varepsilon\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq p}=\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)
,k}b_{k,\varepsilon\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
p}. \label{pf.lem.sol.addexe.jacobi-complement.Agd.AB.short.1}%
\end{equation}


The definition of the product of two matrices yields%
\begin{align}
A_{\left[  \gamma,\delta\right]  }B_{\left[  \delta,\varepsilon\right]  }  &
=\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,\delta\left(  k\right)
}b_{\delta\left(  k\right)  ,\varepsilon\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq p}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{\left[  \gamma,\delta\right]  }=\left(  a_{\gamma\left(
i\right)  ,\delta\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\\
\text{and }B_{\left[  \delta,\varepsilon\right]  }=\left(  b_{\delta\left(
i\right)  ,\varepsilon\left(  j\right)  }\right)  _{1\leq i\leq m,\ 1\leq
j\leq p}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon
\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq p}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.addexe.jacobi-complement.Agd.AB.short.1})}\right)  .
\label{pf.lem.sol.addexe.jacobi-complement.Agd.AB.short.2}%
\end{align}


On the other hand, the definition of the product of two matrices yields
$AB=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
p}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ and
$B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq p}$). Hence, the
definition of $\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]  }$
yields%
\[
\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]  }=\left(  \sum
_{k=1}^{m}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon\left(  j\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq p}.
\]
Comparing this with (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.AB.short.2}%
), we obtain $\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]
}=A_{\left[  \gamma,\delta\right]  }B_{\left[  \delta,\varepsilon\right]  }$.
Thus, Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.AB} is proven.
\end{vershort}

\begin{verlong}
Every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  \times\left\{
1,2,\ldots,p\right\}  $ satisfies%
\begin{align*}
&  \underbrace{\sum_{k=1}^{m}}_{=\sum_{k\in\left\{  1,2,\ldots,m\right\}  }%
}a_{\gamma\left(  i\right)  ,\delta\left(  k\right)  }b_{\delta\left(
k\right)  ,\varepsilon\left(  j\right)  }\\
&  =\sum_{k\in\left\{  1,2,\ldots,m\right\}  }a_{\gamma\left(  i\right)
,\delta\left(  k\right)  }b_{\delta\left(  k\right)  ,\varepsilon\left(
j\right)  }=\underbrace{\sum_{k\in\left\{  1,2,\ldots,m\right\}  }}%
_{=\sum_{k=1}^{m}}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon\left(
j\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }k\text{ for }\delta\left(  k\right)  \text{
in the sum, since}\\
\text{the map }\delta:\left\{  1,2,\ldots,m\right\}  \rightarrow\left\{
1,2,\ldots,m\right\}  \text{ is a bijection}%
\end{array}
\right) \\
&  =\sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon\left(
j\right)  }.
\end{align*}
In other words, we have%
\begin{equation}
\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,\delta\left(  k\right)
}b_{\delta\left(  k\right)  ,\varepsilon\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq p}=\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)
,k}b_{k,\varepsilon\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
p}. \label{pf.lem.sol.addexe.jacobi-complement.Agd.AB.1}%
\end{equation}


The definition of the product of two matrices yields%
\begin{align}
A_{\left[  \gamma,\delta\right]  }B_{\left[  \delta,\varepsilon\right]  }  &
=\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,\delta\left(  k\right)
}b_{\delta\left(  k\right)  ,\varepsilon\left(  j\right)  }\right)  _{1\leq
i\leq n,\ 1\leq j\leq p}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{\left[  \gamma,\delta\right]  }=\left(  a_{\gamma\left(
i\right)  ,\delta\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq
m}\\
\text{and }B_{\left[  \delta,\varepsilon\right]  }=\left(  b_{\delta\left(
i\right)  ,\varepsilon\left(  j\right)  }\right)  _{1\leq i\leq m,\ 1\leq
j\leq p}%
\end{array}
\right) \nonumber\\
&  =\left(  \sum_{k=1}^{m}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon
\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq p}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.addexe.jacobi-complement.Agd.AB.1})}\right)  .
\label{pf.lem.sol.addexe.jacobi-complement.Agd.AB.2}%
\end{align}


On the other hand, the definition of the product of two matrices yields
$AB=\left(  \sum_{k=1}^{m}a_{i,k}b_{k,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
p}$ (since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ and
$B=\left(  b_{i,j}\right)  _{1\leq i\leq m,\ 1\leq j\leq p}$). Hence, the
definition of $\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]  }$
yields%
\[
\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]  }=\left(  \sum
_{k=1}^{m}a_{\gamma\left(  i\right)  ,k}b_{k,\varepsilon\left(  j\right)
}\right)  _{1\leq i\leq n,\ 1\leq j\leq p}.
\]
Comparing this with (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.AB.2}), we
obtain $\left(  AB\right)  _{\left[  \gamma,\varepsilon\right]  }=A_{\left[
\gamma,\delta\right]  }B_{\left[  \delta,\varepsilon\right]  }$. Thus, Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.AB} is proven.
\end{verlong}
\end{proof}

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.Agd.I}Let $n\in\mathbb{N}$ and
$\gamma\in S_{n}$. Then, $\left(  I_{n}\right)  _{\left[  \gamma
,\gamma\right]  }=I_{n}$.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.I}.]For every two
objects $i$ and $j$, define an element $\delta_{i,j}\in\mathbb{K}$ by
$\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$). Hence, $\left(  I_{n}\right)  _{\left[
\gamma,\gamma\right]  }=\left(  \delta_{\gamma\left(  i\right)  ,\gamma\left(
j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of
$\left(  I_{n}\right)  _{\left[  \gamma,\gamma\right]  }$).

The map $\gamma$ is a permutation (since $\gamma\in S_{n}$), thus bijective,
thus injective.

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, we
have $\delta_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }=\delta_{i,j}%
$\ \ \ \ \footnote{\textit{Proof.} Let $\left(  i,j\right)  \in\left\{
1,2,\ldots,n\right\}  ^{2}$. We must prove $\delta_{\gamma\left(  i\right)
,\gamma\left(  j\right)  }=\delta_{i,j}$.
\par
If $i=j$, then both $\delta_{\gamma\left(  i\right)  ,\gamma\left(  j\right)
}$ and $\delta_{i,j}$ equal $1$ (because $\gamma\left(  \underbrace{i}%
_{=j}\right)  =\gamma\left(  j\right)  $ shows that $\delta_{\gamma\left(
i\right)  ,\gamma\left(  j\right)  }=1$, whereas $i=j$ shows that
$\delta_{i,j}=1$). Hence, if $i=j$, then $\delta_{\gamma\left(  i\right)
,\gamma\left(  j\right)  }=\delta_{i,j}$ holds. Thus, for the rest of our
proof of $\delta_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }%
=\delta_{i,j}$, we WLOG assume that $i\neq j$. Hence, $\delta_{i,j}=0$.
\par
But the map $\gamma$ is injective. Thus, from $i\neq j$, we obtain
$\gamma\left(  i\right)  \neq\gamma\left(  j\right)  $. Hence, $\delta
_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }=0$. Comparing this with
$\delta_{i,j}=0$, we obtain $\delta_{\gamma\left(  i\right)  ,\gamma\left(
j\right)  }=\delta_{i,j}$, qed.}. In other words, $\left(  \delta
_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}$. Hence,
\[
\left(  I_{n}\right)  _{\left[  \gamma,\gamma\right]  }=\left(  \delta
_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Comparing this with $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, we obtain $\left(  I_{n}\right)  _{\left[  \gamma
,\gamma\right]  }=I_{n}$. This proves Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.I}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.I}.]For every two
objects $i$ and $j$, define an element $\delta_{i,j}\in\mathbb{K}$ by
$\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$). Hence, $\left(  I_{n}\right)  _{\left[
\gamma,\gamma\right]  }=\left(  \delta_{\gamma\left(  i\right)  ,\gamma\left(
j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of
$\left(  I_{n}\right)  _{\left[  \gamma,\gamma\right]  }$).

We have $\gamma\in S_{n}$. In other words, $\gamma$ is a permutation of
$\left\{  1,2,\ldots,n\right\}  $ (since $S_{n}$ is the set of all
permutations of $\left\{  1,2,\ldots,n\right\}  $ (by the definition of
$S_{n}$)). In other words, $\gamma$ is a bijective map $\left\{
1,2,\ldots,n\right\}  \rightarrow\left\{  1,2,\ldots,n\right\}  $. The map
$\gamma$ is bijective, and thus injective.

For every $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$, we
have%
\begin{equation}
\delta_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }=\delta_{i,j}
\label{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}%
):} Let $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. We must
prove (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}).
\par
We have $\left(  i,j\right)  \in\left\{  1,2,\ldots,n\right\}  ^{2}$. In other
words, $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{  1,2,\ldots
,n\right\}  $. We are in one of the following two cases:
\par
\textit{Case 1:} We have $i=j$.
\par
\textit{Case 2:} We have $i\neq j$.
\par
Let us first consider Case 1. In this case, we have $i=j$. Thus, $\delta
_{i,j}=1$. But $\gamma\left(  \underbrace{i}_{=j}\right)  =\gamma\left(
j\right)  $, and thus $\delta_{\gamma\left(  i\right)  ,\gamma\left(
j\right)  }=1$. Comparing this with $\delta_{i,j}=1$, we obtain $\delta
_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }=\delta_{i,j}$. Thus,
(\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}) is proven in Case 1.
\par
Let us now consider Case 2. In this case, we have $i\neq j$. In other words,
the elements $i$ and $j$ are distinct. But the map $\gamma$ is injective.
Hence, if $u$ and $v$ are two distinct elements of $\left\{  1,2,\ldots
,n\right\}  $, then $\gamma\left(  u\right)  \neq\gamma\left(  v\right)  $.
Applying this to $u=i$ and $v=j$, we conclude that $\gamma\left(  i\right)
\neq\gamma\left(  j\right)  $ (since $i$ and $j$ are distinct). Hence,
$\delta_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }=0$. Comparing this
with $\delta_{i,j}=0$ (since $i\neq j$), we obtain $\delta_{\gamma\left(
i\right)  ,\gamma\left(  j\right)  }=\delta_{i,j}$. Thus,
(\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}) is proven in Case 2.
\par
We have now proven (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}) in each
of the two Cases 1 and 2. Since these two Cases cover all possibilities, we
can thus conclude that (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1})
always holds. Thus, (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.I.1}) is
proven.}. In other words, $\left(  \delta_{\gamma\left(  i\right)
,\gamma\left(  j\right)  }\right)  _{1\leq i\leq n,\ 1\leq j\leq n}=\left(
\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$. Hence,
\[
\left(  I_{n}\right)  _{\left[  \gamma,\gamma\right]  }=\left(  \delta
_{\gamma\left(  i\right)  ,\gamma\left(  j\right)  }\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}.
\]
Comparing this with $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}$, we obtain $\left(  I_{n}\right)  _{\left[  \gamma
,\gamma\right]  }=I_{n}$. This proves Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.I}.
\end{proof}
\end{verlong}

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.Agd.inv}Let $n\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times n}$ be an invertible matrix. Let $\gamma\in S_{n}$
and $\delta\in S_{n}$. Then, the matrix $A_{\left[  \gamma,\delta\right]  }%
\in\mathbb{K}^{n\times n}$ is invertible, and its inverse is $\left(
A_{\left[  \gamma,\delta\right]  }\right)  ^{-1}=\left(  A^{-1}\right)
_{\left[  \delta,\gamma\right]  }$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.inv}.]Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.AB} (applied to $n$, $n$, $\gamma$
and $A^{-1}$ instead of $m$, $p$, $\varepsilon$ and $B$) yields $\left(
AA^{-1}\right)  _{\left[  \gamma,\gamma\right]  }=A_{\left[  \gamma
,\delta\right]  }\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }$.
Thus,%
\begin{equation}
A_{\left[  \gamma,\delta\right]  }\left(  A^{-1}\right)  _{\left[
\delta,\gamma\right]  }=\left(  \underbrace{AA^{-1}}_{=I_{n}}\right)
_{\left[  \gamma,\gamma\right]  }=\left(  I_{n}\right)  _{\left[
\gamma,\gamma\right]  }=I_{n}
\label{pf.lem.sol.addexe.jacobi-complement.Agd.inv.1}%
\end{equation}
(by Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.I}). Also, Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.AB} (applied to $n$, $n$, $\delta$,
$\gamma$, $\delta$, $A^{-1}$ and $A$ instead of $m$, $p$, $\gamma$, $\delta$,
$\varepsilon$, $A$ and $B$) yields $\left(  A^{-1}A\right)  _{\left[
\delta,\delta\right]  }=\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]
}A_{\left[  \gamma,\delta\right]  }$. Thus,%
\begin{equation}
\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }A_{\left[
\gamma,\delta\right]  }=\left(  \underbrace{A^{-1}A}_{=I_{n}}\right)
_{\left[  \delta,\delta\right]  }=\left(  I_{n}\right)  _{\left[
\delta,\delta\right]  }=I_{n}
\label{pf.lem.sol.addexe.jacobi-complement.Agd.inv.2}%
\end{equation}
(by Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.I} (applied to $\delta$
instead of $\gamma$)).

\begin{vershort}
The two equalities (\ref{pf.lem.sol.addexe.jacobi-complement.Agd.inv.1}) and
(\ref{pf.lem.sol.addexe.jacobi-complement.Agd.inv.2}) (combined) show that the
matrix $\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }$ is an
inverse of the matrix $A_{\left[  \gamma,\delta\right]  }$. Thus, the matrix
$A_{\left[  \gamma,\delta\right]  }\in\mathbb{K}^{n\times n}$ is invertible,
and its inverse is $\left(  A_{\left[  \gamma,\delta\right]  }\right)
^{-1}=\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }$. Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.inv} is proven.
\end{vershort}

\begin{verlong}
So we have shown that the $n\times n$-matrix $\left(  A^{-1}\right)  _{\left[
\delta,\gamma\right]  }$ satisfies $\left(  A^{-1}\right)  _{\left[
\delta,\gamma\right]  }A_{\left[  \gamma,\delta\right]  }=I_{n}$ and
$A_{\left[  \gamma,\delta\right]  }\left(  A^{-1}\right)  _{\left[
\delta,\gamma\right]  }=I_{n}$. In other words, $\left(  A^{-1}\right)
_{\left[  \delta,\gamma\right]  }$ is an $n\times n$-matrix $B$ such that
$BA_{\left[  \gamma,\delta\right]  }=I_{n}$ and $A_{\left[  \gamma
,\delta\right]  }B=I_{n}$. In other words, $\left(  A^{-1}\right)  _{\left[
\delta,\gamma\right]  }$ is an inverse of $A_{\left[  \gamma,\delta\right]  }$
(because an inverse of $A_{\left[  \gamma,\delta\right]  }$ means an $n\times
n$-matrix $B$ such that $BA_{\left[  \gamma,\delta\right]  }=I_{n}$ and
$A_{\left[  \gamma,\delta\right]  }B=I_{n}$ (by the definition of an
\textquotedblleft inverse of $A_{\left[  \gamma,\delta\right]  }%
$\textquotedblright)). Thus, the matrix $A_{\left[  \gamma,\delta\right]  }$
has an inverse; in other words, the matrix $A_{\left[  \gamma,\delta\right]
}$ is invertible. Also, clearly, the inverse of $A_{\left[  \gamma
,\delta\right]  }$ is $\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]
}$ (since we have proven that $\left(  A^{-1}\right)  _{\left[  \delta
,\gamma\right]  }$ is an inverse of $A_{\left[  \gamma,\delta\right]  }$). In
other words, $\left(  A_{\left[  \gamma,\delta\right]  }\right)  ^{-1}=\left(
A^{-1}\right)  _{\left[  \delta,\gamma\right]  }$. Thus, the inverse of
$A_{\left[  \gamma,\delta\right]  }$ is $\left(  A_{\left[  \gamma
,\delta\right]  }\right)  ^{-1}=\left(  A^{-1}\right)  _{\left[  \delta
,\gamma\right]  }$. This completes the proof of Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.inv}.
\end{verlong}
\end{proof}

We can finally step to the solution of Additional exercise
\ref{addexe.jacobi-complement}:

\begin{proof}
[First solution to Additional exercise \ref{addexe.jacobi-complement}.]Let
$k=\left\vert P\right\vert $. Thus, $k=\left\vert P\right\vert =\left\vert
Q\right\vert $.

\begin{vershort}
Clearly, $k\in\left\{  0,1,\ldots,n\right\}  $ (since $k=\left\vert
P\right\vert $ for a subset $P$ of $\left\{  1,2,\ldots,n\right\}  $).
\end{vershort}

\begin{verlong}
We know that $P$ is a subset of $\left\{  1,2,\ldots,n\right\}  $. Thus, $P$
is a finite set (since $\left\{  1,2,\ldots,n\right\}  $ is a finite set).
Hence, $\left\vert P\right\vert \in\mathbb{N}$, so that $k=\left\vert
P\right\vert \in\mathbb{N}$.

The definition of $\widetilde{P}$ yields $\widetilde{P}=\left\{
1,2,\ldots,n\right\}  \setminus P\subseteq\left\{  1,2,\ldots,n\right\}  $.
Thus, $\widetilde{P}$ is a finite set (since $\left\{  1,2,\ldots,n\right\}  $
is a finite set). Hence, $\left\vert \widetilde{P}\right\vert \in\mathbb{N}$.

Also,%
\begin{align*}
\left\vert \underbrace{\widetilde{P}}_{=\left\{  1,2,\ldots,n\right\}
\setminus P}\right\vert  &  =\left\vert \left\{  1,2,\ldots,n\right\}
\setminus P\right\vert =\underbrace{\left\vert \left\{  1,2,\ldots,n\right\}
\right\vert }_{=n}-\underbrace{\left\vert P\right\vert }_{=k}%
\ \ \ \ \ \ \ \ \ \ \left(  \text{since }P\subseteq\left\{  1,2,\ldots
,n\right\}  \right) \\
&  =n-k.
\end{align*}
Hence, $n-k=\left\vert \widetilde{P}\right\vert \in\mathbb{N}$; thus,
$n-k\geq0$. In other words, $k\leq n$. Combined with $k\geq0$ (since
$k=\left\vert P\right\vert \in\mathbb{N}$), this yields $k\in\left\{
0,1,\ldots,n\right\}  $.
\end{verlong}

Lemma \ref{lem.sol.addexe.jacobi-complement.Ialbe} (applied to $I=P$) yields
that there exists a $\sigma\in S_{n}$ satisfying $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  k\right)  \right)
=w\left(  P\right)  $, $\left(  \sigma\left(  k+1\right)  ,\sigma\left(
k+2\right)  ,\ldots,\sigma\left(  n\right)  \right)  =w\left(  \widetilde{P}%
\right)  $ and $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\sum
P-\left(  1+2+\cdots+k\right)  }$. Denote this $\sigma$ by $\gamma$. Thus,
$\gamma$ is an element of $S_{n}$ satisfying $\left(  \gamma\left(  1\right)
,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  \right)  =w\left(
P\right)  $, $\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)
,\ldots,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{P}\right)  $ and
$\left(  -1\right)  ^{\gamma}=\left(  -1\right)  ^{\sum P-\left(
1+2+\cdots+k\right)  }$.

Lemma \ref{lem.sol.addexe.jacobi-complement.Ialbe} (applied to $I=Q$) yields
that there exists a $\sigma\in S_{n}$ satisfying $\left(  \sigma\left(
1\right)  ,\sigma\left(  2\right)  ,\ldots,\sigma\left(  k\right)  \right)
=w\left(  Q\right)  $, $\left(  \sigma\left(  k+1\right)  ,\sigma\left(
k+2\right)  ,\ldots,\sigma\left(  n\right)  \right)  =w\left(  \widetilde{Q}%
\right)  $ and $\left(  -1\right)  ^{\sigma}=\left(  -1\right)  ^{\sum
Q-\left(  1+2+\cdots+k\right)  }$. Denote this $\sigma$ by $\delta$. Thus,
$\delta$ is an element of $S_{n}$ satisfying $\left(  \delta\left(  1\right)
,\delta\left(  2\right)  ,\ldots,\delta\left(  k\right)  \right)  =w\left(
Q\right)  $, $\left(  \delta\left(  k+1\right)  ,\delta\left(  k+2\right)
,\ldots,\delta\left(  n\right)  \right)  =w\left(  \widetilde{Q}\right)  $ and
$\left(  -1\right)  ^{\delta}=\left(  -1\right)  ^{\sum Q-\left(
1+2+\cdots+k\right)  }$.

Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.inv} shows that the matrix
$A_{\left[  \gamma,\delta\right]  }\in\mathbb{K}^{n\times n}$ is invertible,
and its inverse is $\left(  A_{\left[  \gamma,\delta\right]  }\right)
^{-1}=\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }$. Hence,
Exercise \ref{exe.block2x2.jacobi.rewr} (applied to $A_{\left[  \gamma
,\delta\right]  }$ instead of $A$) yields that%
\begin{align}
&  \det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots
,k}\left(  A_{\left[  \gamma,\delta\right]  }\right)  \right) \nonumber\\
&  =\det\left(  A_{\left[  \gamma,\delta\right]  }\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}\left(
\underbrace{\left(  A_{\left[  \gamma,\delta\right]  }\right)  ^{-1}%
}_{=\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }}\right)  \right)
\nonumber\\
&  =\det\left(  A_{\left[  \gamma,\delta\right]  }\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}\left(
\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }\right)  \right)  .
\label{sol.addexe.jacobi-complement.1}%
\end{align}
But Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.sub} (applied to $n$, $k$,
$k$, $\left(  1,2,\ldots,k\right)  $ and $\left(  1,2,\ldots,k\right)  $
instead of $m$, $u$, $v$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and
$\left(  j_{1},j_{2},\ldots,j_{v}\right)  $) yields%
\begin{align}
\operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots,k}\left(  A_{\left[
\gamma,\delta\right]  }\right)   &  =\operatorname*{sub}\nolimits_{\gamma
\left(  1\right)  ,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)
}^{\delta\left(  1\right)  ,\delta\left(  2\right)  ,\ldots,\delta\left(
k\right)  }A=\operatorname*{sub}\nolimits_{\left(  \gamma\left(  1\right)
,\gamma\left(  2\right)  ,\ldots,\gamma\left(  k\right)  \right)  }^{\left(
\delta\left(  1\right)  ,\delta\left(  2\right)  ,\ldots,\delta\left(
k\right)  \right)  }A\nonumber\\
&  =\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A
\label{sol.addexe.jacobi-complement.3a}%
\end{align}
(since $\left(  \gamma\left(  1\right)  ,\gamma\left(  2\right)
,\ldots,\gamma\left(  k\right)  \right)  =w\left(  P\right)  $ and $\left(
\delta\left(  1\right)  ,\delta\left(  2\right)  ,\ldots,\delta\left(
k\right)  \right)  =w\left(  Q\right)  $).

Furthermore, we have $A^{-1}\in\mathbb{K}^{n\times n}$ (since $A\in
\mathbb{K}^{n\times n}$). Hence, Lemma
\ref{lem.sol.addexe.jacobi-complement.Agd.sub} (applied to $n$, $A^{-1}$,
$\delta$, $\gamma$, $n-k$, $n-k$, $\left(  k+1,k+2,\ldots,n\right)  $ and
$\left(  k+1,k+2,\ldots,n\right)  $ instead of $m$, $A$, $\gamma$, $\delta$,
$u$, $v$, $\left(  i_{1},i_{2},\ldots,i_{u}\right)  $ and $\left(  j_{1}%
,j_{2},\ldots,j_{v}\right)  $) yields%
\begin{align}
\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots,n}\left(
\left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }\right)   &
=\operatorname*{sub}\nolimits_{\delta\left(  k+1\right)  ,\delta\left(
k+2\right)  ,\ldots,\delta\left(  n\right)  }^{\gamma\left(  k+1\right)
,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(  n\right)  }\left(
A^{-1}\right) \nonumber\\
&  =\operatorname*{sub}\nolimits_{\left(  \delta\left(  k+1\right)
,\delta\left(  k+2\right)  ,\ldots,\delta\left(  n\right)  \right)  }^{\left(
\gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots,\gamma\left(
n\right)  \right)  }\left(  A^{-1}\right) \nonumber\\
&  =\operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(
\widetilde{P}\right)  }\left(  A^{-1}\right)
\label{sol.addexe.jacobi-complement.3b}%
\end{align}
(since $\left(  \delta\left(  k+1\right)  ,\delta\left(  k+2\right)
,\ldots,\delta\left(  n\right)  \right)  =w\left(  \widetilde{Q}\right)  $ and
$\left(  \gamma\left(  k+1\right)  ,\gamma\left(  k+2\right)  ,\ldots
,\gamma\left(  n\right)  \right)  =w\left(  \widetilde{P}\right)  $).

On the other hand,%
\begin{align}
&  \underbrace{\left(  -1\right)  ^{\gamma}}_{=\left(  -1\right)  ^{\sum
P-\left(  1+2+\cdots+k\right)  }}\underbrace{\left(  -1\right)  ^{\delta}%
}_{=\left(  -1\right)  ^{\sum Q-\left(  1+2+\cdots+k\right)  }}\nonumber\\
&  =\left(  -1\right)  ^{\sum P-\left(  1+2+\cdots+k\right)  }\left(
-1\right)  ^{\sum Q-\left(  1+2+\cdots+k\right)  }=\left(  -1\right)
^{\left(  \sum P-\left(  1+2+\cdots+k\right)  \right)  +\left(  \sum Q-\left(
1+2+\cdots+k\right)  \right)  }\nonumber\\
&  =\left(  -1\right)  ^{\sum P+\sum Q} \label{sol.addexe.jacobi-complement.2}%
\end{align}
(since
\begin{align*}
&  \left(  \sum P-\left(  1+2+\cdots+k\right)  \right)  +\left(  \sum
Q-\left(  1+2+\cdots+k\right)  \right) \\
&  =\left(  \sum P+\sum Q\right)  -2\left(  1+2+\cdots+k\right)  \equiv\sum
P+\sum Q\operatorname{mod}2
\end{align*}
).

Lemma \ref{lem.sol.addexe.jacobi-complement.Agd.det} yields%
\begin{equation}
\det\left(  A_{\left[  \gamma,\delta\right]  }\right)  =\underbrace{\left(
-1\right)  ^{\gamma}\left(  -1\right)  ^{\delta}}_{\substack{=\left(
-1\right)  ^{\sum P+\sum Q}\\\text{(by (\ref{sol.addexe.jacobi-complement.2}%
))}}}\det A=\left(  -1\right)  ^{\sum P+\sum Q}\det A.
\label{sol.addexe.jacobi-complement.3c}%
\end{equation}


Now,%
\begin{align*}
&  \det\left(  \underbrace{\operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }A}_{\substack{=\operatorname*{sub}\nolimits_{1,2,\ldots
,k}^{1,2,\ldots,k}\left(  A_{\left[  \gamma,\delta\right]  }\right)
\\\text{(by (\ref{sol.addexe.jacobi-complement.3a}))}}}\right) \\
&  =\det\left(  \operatorname*{sub}\nolimits_{1,2,\ldots,k}^{1,2,\ldots
,k}\left(  A_{\left[  \gamma,\delta\right]  }\right)  \right) \\
&  =\underbrace{\det\left(  A_{\left[  \gamma,\delta\right]  }\right)
}_{\substack{=\left(  -1\right)  ^{\sum P+\sum Q}\det A\\\text{(by
(\ref{sol.addexe.jacobi-complement.3c}))}}}\cdot\det\left(
\underbrace{\operatorname*{sub}\nolimits_{k+1,k+2,\ldots,n}^{k+1,k+2,\ldots
,n}\left(  \left(  A^{-1}\right)  _{\left[  \delta,\gamma\right]  }\right)
}_{\substack{=\operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }\left(  A^{-1}\right)  \\\text{(by
(\ref{sol.addexe.jacobi-complement.3b}))}}}\right)
\ \ \ \ \ \ \ \ \ \ \left(  \text{by (\ref{sol.addexe.jacobi-complement.1}%
)}\right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det A\cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(
\widetilde{P}\right)  }\left(  A^{-1}\right)  \right)  .
\end{align*}
This solves Additional exercise \ref{addexe.jacobi-complement}.
\end{proof}

\subsubsection{Second solution}

Now, let us give a second solution to Additional exercise
\ref{addexe.jacobi-complement}, following an idea from \cite[Chapter SCHUR,
proof of (1.9)]{LLPT95}.

Throughout this section, we shall use the notations introduced in Definition
\ref{def.submatrix} and in Definition \ref{def.sect.laplace.notations}.

Let us first prove a simple combinatorial fact:

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.increase-bij}Let $S$ be a set of
integers. Let $u\in\mathbb{N}$. Let $\mathcal{P}_{u}\left(  S\right)  $ denote
the set of all $u$-element subsets of $S$. Let $\mathbf{I}$ denote the set
\[
\left\{  \left(  g_{1},g_{2},\ldots,g_{u}\right)  \in S^{u}\ \mid\ g_{1}%
<g_{2}<\cdots<g_{u}\right\}  .
\]
Then, the map%
\begin{align*}
\mathcal{P}_{u}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is well-defined and a bijection.
\end{lemma}

\begin{vershort}
\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.increase-bij}.]Recall
that $\mathcal{P}_{u}\left(  S\right)  $ is the set of all $u$-element subsets
of $S$, whereas $\mathbf{I}$ is the set of all strictly increasing
lists\footnote{A list $\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ is said to
be \textit{strictly increasing} if it satisfies $g_{1}<g_{2}<\cdots<g_{u}$.}
of $u$ elements of $S$.

For every $R\in\mathcal{P}_{u}\left(  S\right)  $, the list $w\left(
R\right)  $ is the list of all elements of $R$ in increasing order (with no
repetitions). Thus, this list $w\left(  R\right)  $ is a strictly increasing
list with $\left\vert R\right\vert =u$ entries (which all belong to $R$ and
therefore to $S$); hence, this list $w\left(  R\right)  $ belongs to
$\mathbf{I}$. Thus, the map
\begin{align*}
\mathcal{P}_{u}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is well-defined. Also, the map%
\begin{align*}
\mathbf{I}  &  \rightarrow\mathcal{P}_{u}\left(  S\right)  ,\\
\left(  g_{1},g_{2},\ldots,g_{u}\right)   &  \mapsto\left\{  g_{1}%
,g_{2},\ldots,g_{u}\right\}
\end{align*}
is well-defined (because if $\left(  g_{1},g_{2},\ldots,g_{u}\right)
\in\mathbf{I}$, then we have $g_{1}<g_{2}<\cdots<g_{u}$, and thus the $u$
elements $g_{1},g_{2},\ldots,g_{u}$ are pairwise distinct; therefore,
$\left\{  g_{1},g_{2},\ldots,g_{u}\right\}  $ is a $u$-element set). These two
maps are mutually inverse\footnote{\textit{Proof.} The former map takes a
$u$-element subset $R$ of $S$ and lists its elements in increasing order; the
latter map takes a strictly increasing list (of $u$ elements of $S$) and
outputs the set of its entries. These two operations clearly revert one
another. Thus, the two maps are mutually inverse.}. Hence, the map%
\begin{align*}
\mathcal{P}_{u}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is invertible, i.e., is a bijection.
\end{proof}
\end{vershort}

\begin{verlong}
The proof of this lemma relies on the following yet simpler facts:

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.strinc}Let $u\in\mathbb{N}$. Let
$\varphi:\left\{  1,2,\ldots,u\right\}  \rightarrow\left\{  1,2,\ldots
,u\right\}  $ be a map such that $\varphi\left(  1\right)  <\varphi\left(
2\right)  <\cdots<\varphi\left(  u\right)  $. Then, $\varphi
=\operatorname*{id}$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.strinc}.]We have
$\varphi\left(  1\right)  <\varphi\left(  2\right)  <\cdots<\varphi\left(
u\right)  $. In other words, if $g$ and $h$ are two elements of $\left\{
1,2,\ldots,u\right\}  $ such that $g<h$, then%
\begin{equation}
\varphi\left(  g\right)  <\varphi\left(  h\right)  .
\label{pf.lem.sol.addexe.jacobi-complement.strinc.1}%
\end{equation}


Now, if $g$ and $h$ are two elements of $\left\{  1,2,\ldots,u\right\}  $
satisfying $\varphi\left(  g\right)  =\varphi\left(  h\right)  $, then
$g=h$\ \ \ \ \footnote{\textit{Proof.} Let $g$ and $h$ be two elements of
$\left\{  1,2,\ldots,u\right\}  $ satisfying $\varphi\left(  g\right)
=\varphi\left(  h\right)  $. We must prove that $g=h$.
\par
Indeed, assume the contrary (for the sake of contradiction). Thus, $g\neq h$.
Assume WLOG that $g\leq h$ (otherwise, we can simply switch $g$ with $h$).
Then, $g<h$ (since $g\leq h$ and $g\neq h$). Thus, $\varphi\left(  g\right)
<\varphi\left(  h\right)  $ (by
(\ref{pf.lem.sol.addexe.jacobi-complement.strinc.1})). This contradicts
$\varphi\left(  g\right)  =\varphi\left(  h\right)  $. This contradiction
proves that our assumption was wrong. Hence, $g=h$ is proven. Qed.}. In other
words, the map $\varphi$ is injective.

But it is well-known that any injective map from a finite set to itself is
bijective. In other words, if $S$ is a finite set, and if $f:S\rightarrow S$
is an injective map, then $f$ is bijective. Applying this to $S=\left\{
1,2,\ldots,u\right\}  $ and $f=\varphi$, we conclude that $\varphi$ is
bijective (since $\left\{  1,2,\ldots,u\right\}  $ is a finite set, and since
$\varphi:\left\{  1,2,\ldots,u\right\}  \rightarrow\left\{  1,2,\ldots
,u\right\}  $ is an injective map). Thus, $\varphi$ is a bijective map
$\left\{  1,2,\ldots,u\right\}  \rightarrow\left\{  1,2,\ldots,u\right\}  $.
In other words, $\varphi$ is a permutation of $\left\{  1,2,\ldots,u\right\}
$. In other words, $\varphi\in S_{u}$ (because $S_{u}$ is the set of all
permutations of $\left\{  1,2,\ldots,u\right\}  $ (by the definition of
$S_{u}$)).

Now, recall that $\varphi\left(  1\right)  <\varphi\left(  2\right)
<\cdots<\varphi\left(  u\right)  $. In other words, $\varphi\left(  k\right)
<\varphi\left(  k+1\right)  $ for each $k\in\left\{  1,2,\ldots,u-1\right\}
$. Hence, $\varphi\left(  k\right)  \leq\varphi\left(  k+1\right)  $ for each
$k\in\left\{  1,2,\ldots,u-1\right\}  $. In other words, $\varphi\left(
1\right)  \leq\varphi\left(  2\right)  \leq\cdots\leq\varphi\left(  u\right)
$. Hence, Proposition \ref{prop.sol.exe.ps2.2.5.d} (applied to $n=u$ and
$\sigma=\varphi$) yields that $\varphi=\operatorname*{id}$. This proves Lemma
\ref{lem.sol.addexe.jacobi-complement.strinc}.
\end{proof}

\begin{lemma}
\label{lem.sol.addexe.jacobi-complement.sets-eq}Let $u\in\mathbb{N}$. Let
$\left(  p_{1},p_{2},\ldots,p_{u}\right)  $ be a list of integers such that
$p_{1}<p_{2}<\cdots<p_{u}$. Let $\left(  q_{1},q_{2},\ldots,q_{u}\right)  $ be
a list of integers such that $q_{1}<q_{2}<\cdots<q_{u}$. Assume that $\left\{
p_{1},p_{2},\ldots,p_{u}\right\}  =\left\{  q_{1},q_{2},\ldots,q_{u}\right\}
$. Then, $\left(  p_{1},p_{2},\ldots,p_{u}\right)  =\left(  q_{1},q_{2}%
,\ldots,q_{u}\right)  $.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.sets-eq}.]We have
$p_{1}<p_{2}<\cdots<p_{u}$. In other words,%
\begin{equation}
p_{k}<p_{k+1}\ \ \ \ \ \ \ \ \ \ \text{for every }k\in\left\{  1,2,\ldots
,u-1\right\}  . \label{pf.lem.sol.addexe.jacobi-complement.sets-eq.p}%
\end{equation}


We have $q_{1}<q_{2}<\cdots<q_{u}$. Hence, if $g$ and $h$ are two elements of
$\left\{  1,2,\ldots,u\right\}  $ satisfying $g\leq h$, then%
\begin{equation}
q_{g}\leq q_{h}. \label{pf.lem.sol.addexe.jacobi-complement.sets-eq.q}%
\end{equation}


Define a map $\varphi:\left\{  1,2,\ldots,u\right\}  \rightarrow\left\{
1,2,\ldots,u\right\}  $ as follows:

Let $k\in\left\{  1,2,\ldots,u\right\}  $. Then, $p_{k}\in\left\{  p_{1}%
,p_{2},\ldots,p_{u}\right\}  =\left\{  q_{1},q_{2},\ldots,q_{u}\right\}  $.
Hence, there exists an $\ell\in\left\{  1,2,\ldots,u\right\}  $ such that
$p_{k}=q_{\ell}$. Consider this $\ell$. Define $\varphi\left(  k\right)  $ by
$\varphi\left(  k\right)  =\ell$. Note that
\begin{equation}
p_{k}=q_{\ell}=q_{\varphi\left(  k\right)  }
\label{pf.lem.sol.addexe.jacobi-complement.sets-eq.1}%
\end{equation}
(since $\ell=\varphi\left(  k\right)  $).

Now, forget that we fixed $k$. Thus, for each $k\in\left\{  1,2,\ldots
,u\right\}  $, we have defined an element $\varphi\left(  k\right)
\in\left\{  1,2,\ldots,u\right\}  $. In other words, we have defined a map
$\varphi:\left\{  1,2,\ldots,u\right\}  \rightarrow\left\{  1,2,\ldots
,u\right\}  $. This map furthermore satisfies
(\ref{pf.lem.sol.addexe.jacobi-complement.sets-eq.1}) for each $k\in\left\{
1,2,\ldots,u\right\}  $.

Now, every $k\in\left\{  1,2,\ldots,u-1\right\}  $ satisfies $\varphi\left(
k\right)  <\varphi\left(  k+1\right)  $\ \ \ \ \footnote{\textit{Proof.} Let
$k\in\left\{  1,2,\ldots,u-1\right\}  $. Then, $k\in\left\{  1,2,\ldots
,u-1\right\}  \subseteq\left\{  1,2,\ldots,u\right\}  $; thus, $\varphi\left(
k\right)  $ is well-defined.
\par
Also, from $k\in\left\{  1,2,\ldots,u-1\right\}  $, we obtain $k+1\in\left\{
2,3,\ldots,u\right\}  \subseteq\left\{  1,2,\ldots,u\right\}  $; thus,
$\varphi\left(  k+1\right)  $ is well-defined.
\par
From (\ref{pf.lem.sol.addexe.jacobi-complement.sets-eq.p}), we obtain
$p_{k}<p_{k+1}$. But (\ref{pf.lem.sol.addexe.jacobi-complement.sets-eq.1})
yields $p_{k}=q_{\varphi\left(  k\right)  }$ (since $k\in\left\{
1,2,\ldots,u\right\}  $). Thus, $q_{\varphi\left(  k\right)  }=p_{k}$. Also,
(\ref{pf.lem.sol.addexe.jacobi-complement.sets-eq.1}) (applied to $k+1$
instead of $k$) yields $p_{k+1}=q_{\varphi\left(  k+1\right)  }$ (since
$k+1\in\left\{  1,2,\ldots,u\right\}  $). Hence, $q_{\varphi\left(  k\right)
}=p_{k}<p_{k+1}=q_{\varphi\left(  k+1\right)  }$.
\par
Now, assume (for the sake of contradiction) that $\varphi\left(  k\right)
\geq\varphi\left(  k+1\right)  $. Thus, $\varphi\left(  k+1\right)
\leq\varphi\left(  k\right)  $. Hence,
(\ref{pf.lem.sol.addexe.jacobi-complement.sets-eq.q}) (applied to
$g=\varphi\left(  k+1\right)  $ and $h=\varphi\left(  k\right)  $) yields
$q_{\varphi\left(  k+1\right)  }\leq q_{\varphi\left(  k\right)  }%
<q_{\varphi\left(  k+1\right)  }$. This is absurd. Thus, we have found a
contradiction. Hence, our assumption (that $\varphi\left(  k\right)
\geq\varphi\left(  k+1\right)  $) must have been false. Therefore, we cannot
have $\varphi\left(  k\right)  \geq\varphi\left(  k+1\right)  $. Hence, we
must have $\varphi\left(  k\right)  <\varphi\left(  k+1\right)  $. Qed.}. In
other words, we have $\varphi\left(  1\right)  <\varphi\left(  2\right)
<\cdots<\varphi\left(  u\right)  $. Hence, Lemma
\ref{lem.sol.addexe.jacobi-complement.strinc} shows that $\varphi
=\operatorname*{id}$. Now, every $k\in\left\{  1,2,\ldots,u\right\}  $
satisfies%
\begin{align*}
p_{k}  &  =q_{\varphi\left(  k\right)  }\ \ \ \ \ \ \ \ \ \ \left(  \text{by
(\ref{pf.lem.sol.addexe.jacobi-complement.sets-eq.1})}\right) \\
&  =q_{k}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\underbrace{\varphi
}_{=\operatorname*{id}}\left(  k\right)  =\operatorname*{id}\left(  k\right)
=k\right)  .
\end{align*}
In other words, $\left(  p_{1},p_{2},\ldots,p_{u}\right)  =\left(  q_{1}%
,q_{2},\ldots,q_{u}\right)  $. This proves Lemma
\ref{lem.sol.addexe.jacobi-complement.sets-eq}.
\end{proof}

\begin{proof}
[Proof of Lemma \ref{lem.sol.addexe.jacobi-complement.increase-bij}.]We have
$S\subseteq\mathbb{Z}$ (since $S$ is a set of integers).

The definition of $\mathbf{I}$ yields%
\[
\mathbf{I}=\left\{  \left(  g_{1},g_{2},\ldots,g_{u}\right)  \in S^{u}%
\ \mid\ g_{1}<g_{2}<\cdots<g_{u}\right\}  .
\]
Hence, every element of $\mathbf{I}$ has the form $\left(  g_{1},g_{2}%
,\ldots,g_{u}\right)  $ for some $\left(  g_{1},g_{2},\ldots,g_{u}\right)  \in
S^{u}$.

For every $R\in\mathcal{P}_{u}\left(  S\right)  $, the list $w\left(
R\right)  $ is well-defined\footnote{\textit{Proof.} Let $R\in\mathcal{P}%
_{u}\left(  S\right)  $. We must show that the list $w\left(  R\right)  $ is
well-defined.
\par
We have $R\in\mathcal{P}_{u}\left(  S\right)  $. In other words, $R$ is a
$u$-element subset of $S$ (since $\mathcal{P}_{u}\left(  S\right)  $ is the
set of all $u$-element subsets of $S$). Now, $R$ is a subset of $S$; in other
words, $R\subseteq S$. Hence, $R\subseteq S\subseteq\mathbb{Z}$ (since $S$ is
a set of integers). In other words, $R$ is a set of integers. Also, $R$ is
finite (since $R$ is a $u$-element set). Hence, $R$ is a finite set of
integers.
\par
But the list $w\left(  I\right)  $ is well-defined whenever $I$ is a finite
set of integers. Applying this to $I=R$, we conclude that the list $w\left(
R\right)  $ is well-defined. Qed.} and belongs to $\mathbf{I}$%
\ \ \ \ \footnote{\textit{Proof.} Let $R\in\mathcal{P}_{u}\left(  S\right)  $.
We must show that the list $w\left(  R\right)  $ belongs to $\mathbf{I}$.
\par
We have $R\in\mathcal{P}_{u}\left(  S\right)  $. In other words, $R$ is a
$u$-element subset of $S$ (since $\mathcal{P}_{u}\left(  S\right)  $ is the
set of all $u$-element subsets of $S$). Now, $R$ is a subset of $S$; in other
words, $R\subseteq S$. Hence, $R\subseteq S\subseteq\mathbb{Z}$ (since $S$ is
a set of integers). In other words, $R$ is a set of integers. Also, $R$ is
finite (since $R$ is a $u$-element set). Hence, $R$ is a finite set of
integers. Also, $\left\vert R\right\vert =u$ (since $R$ is a $u$-element set).
\par
The definition of $w\left(  R\right)  $ shows that $w\left(  R\right)  $ is
the list of all elements of $R$ in increasing order (with no repetitions).
Hence, this list $w\left(  R\right)  $ has $\left\vert R\right\vert $ entries.
In other words, the list $w\left(  R\right)  $ has $u$ entries (since
$\left\vert R\right\vert =u$).
\par
Write the list $w\left(  R\right)  $ in the form $w\left(  R\right)  =\left(
r_{1},r_{2},\ldots,r_{u}\right)  $. (This is possible, since the list
$w\left(  R\right)  $ has $u$ entries.)
\par
The list $w\left(  R\right)  $ is the list of all elements of $R$ in
increasing order (with no repetitions). Hence, $w\left(  R\right)  $ is a list
of all elements of $R$. In other words, $\left(  r_{1},r_{2},\ldots
,r_{u}\right)  $ is a list of all elements of $R$ (since $w\left(  R\right)
=\left(  r_{1},r_{2},\ldots,r_{u}\right)  $). Hence, $\left\{  r_{1}%
,r_{2},\ldots,r_{u}\right\}  =R$. Now, each $i\in\left\{  1,2,\ldots
,u\right\}  $ satisfies $r_{i}\in\left\{  r_{1},r_{2},\ldots,r_{u}\right\}
=R\subseteq S$. Thus, $\left(  r_{1},r_{2},\ldots,r_{u}\right)  \in S^{u}$.
\par
Moreover, the list $w\left(  R\right)  $ is the list of all elements of $R$ in
increasing order (with no repetitions). Hence, the list $w\left(  R\right)  $
is strictly increasing. In other words, the list $\left(  r_{1},r_{2}%
,\ldots,r_{u}\right)  $ is strictly increasing (since $w\left(  R\right)
=\left(  r_{1},r_{2},\ldots,r_{u}\right)  $). In other words, $r_{1}%
<r_{2}<\cdots<r_{u}$.
\par
Now, we know that the list $\left(  r_{1},r_{2},\ldots,r_{u}\right)  \in
S^{u}$ satisfies $r_{1}<r_{2}<\cdots<r_{u}$ and $w\left(  R\right)  =\left(
r_{1},r_{2},\ldots,r_{u}\right)  $. Hence, there exists some $\left(
g_{1},g_{2},\ldots,g_{u}\right)  \in S^{u}$ satisfying $g_{1}<g_{2}%
<\cdots<g_{u}$ and $w\left(  R\right)  =\left(  g_{1},g_{2},\ldots
,g_{u}\right)  $ (namely, $\left(  g_{1},g_{2},\ldots,g_{u}\right)  =\left(
r_{1},r_{2},\ldots,r_{u}\right)  $). In other words,%
\[
w\left(  R\right)  \in\left\{  \left(  g_{1},g_{2},\ldots,g_{u}\right)  \in
S^{u}\ \mid\ g_{1}<g_{2}<\cdots<g_{u}\right\}  .
\]
This rewrites as $w\left(  R\right)  \in\mathbf{I}$ (since $\mathbf{I}%
=\left\{  \left(  g_{1},g_{2},\ldots,g_{u}\right)  \in S^{u}\ \mid
\ g_{1}<g_{2}<\cdots<g_{u}\right\}  $). In other words, $w\left(  R\right)  $
belongs to $\mathbf{I}$. Qed.}. Hence, the map%
\begin{align*}
\mathcal{P}_{u}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is well-defined. Denote this map by $\alpha$.

Now, every $\left(  r_{1},r_{2},\ldots,r_{u}\right)  \in\mathbf{I}$ satisfies
$\left\{  r_{1},r_{2},\ldots,r_{u}\right\}  \in\mathcal{P}_{u}\left(
S\right)  $\ \ \ \ \footnote{\textit{Proof.} Let $\left(  r_{1},r_{2}%
,\ldots,r_{u}\right)  \in\mathbf{I}$. Then,%
\[
\left(  r_{1},r_{2},\ldots,r_{u}\right)  \in\mathbf{I}=\left\{  \left(
g_{1},g_{2},\ldots,g_{u}\right)  \in S^{u}\ \mid\ g_{1}<g_{2}<\cdots
<g_{u}\right\}  .
\]
In other words, $\left(  r_{1},r_{2},\ldots,r_{u}\right)  $ is an element
$\left(  g_{1},g_{2},\ldots,g_{u}\right)  \in S^{u}$ satisfying $g_{1}%
<g_{2}<\cdots<g_{u}$. In other words, $\left(  r_{1},r_{2},\ldots
,r_{u}\right)  $ is an element of $S^{u}$ and satisfies $r_{1}<r_{2}%
<\cdots<r_{u}$.
\par
We have $\left(  r_{1},r_{2},\ldots,r_{u}\right)  \in S^{u}$. In other words,
$r_{i}\in S$ for each $i\in\left\{  1,2,\ldots,u\right\}  $. Hence, $\left\{
r_{1},r_{2},\ldots,r_{u}\right\}  \subseteq S$. In other words, $\left\{
r_{1},r_{2},\ldots,r_{u}\right\}  $ is a subset of $S$.
\par
The $u$ elements $r_{1},r_{2},\ldots,r_{u}$ are pairwise distinct (since
$r_{1}<r_{2}<\cdots<r_{u}$). Hence, $\left\vert \left\{  r_{1},r_{2}%
,\ldots,r_{u}\right\}  \right\vert =u$. Thus, $\left\{  r_{1},r_{2}%
,\ldots,r_{u}\right\}  $ is a $u$-element set. Now, we know that $\left\{
r_{1},r_{2},\ldots,r_{u}\right\}  $ is a $u$-element subset of $S$ (since
$\left\{  r_{1},r_{2},\ldots,r_{u}\right\}  $ is a $u$-element set and a
subset of $S$). In other words, $\left\{  r_{1},r_{2},\ldots,r_{u}\right\}
\in\mathcal{P}_{u}\left(  S\right)  $ (since $\mathcal{P}_{u}\left(  S\right)
$ is the set of all $u$-element subsets of $S$). Qed.}.

Recall that every element of $\mathbf{I}$ has the form $\left(  g_{1}%
,g_{2},\ldots,g_{u}\right)  $ for some $\left(  g_{1},g_{2},\ldots
,g_{u}\right)  \in S^{u}$. In other words, every element of $\mathbf{I}$ has
the form $\left(  r_{1},r_{2},\ldots,r_{u}\right)  $ for some $\left(
r_{1},r_{2},\ldots,r_{u}\right)  \in S^{u}$ (here, we have renamed the index
$\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ as $\left(  r_{1},r_{2}%
,\ldots,r_{u}\right)  $). Hence, we can define a map $\beta:\mathbf{I}%
\rightarrow\mathcal{P}_{u}\left(  S\right)  $ by%
\[
\left(  \beta\left(  r_{1},r_{2},\ldots,r_{u}\right)  =\left\{  r_{1}%
,r_{2},\ldots,r_{u}\right\}  \ \ \ \ \ \ \ \ \ \ \text{for every }\left(
r_{1},r_{2},\ldots,r_{u}\right)  \in\mathbf{I}\right)
\]
(since every $\left(  r_{1},r_{2},\ldots,r_{u}\right)  \in\mathbf{I}$
satisfies $\left\{  r_{1},r_{2},\ldots,r_{u}\right\}  \in\mathcal{P}%
_{u}\left(  S\right)  $). Consider this $\beta$.

We have $\alpha\circ\beta=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.}
Let $\mathbf{i}\in\mathbf{I}$. Then,%
\[
\mathbf{i}\in\mathbf{I}=\left\{  \left(  g_{1},g_{2},\ldots,g_{u}\right)  \in
S^{u}\ \mid\ g_{1}<g_{2}<\cdots<g_{u}\right\}  .
\]
In other words, $\mathbf{i}$ has the form $\mathbf{i}=\left(  g_{1}%
,g_{2},\ldots,g_{u}\right)  $ for some $\left(  g_{1},g_{2},\ldots
,g_{u}\right)  \in S^{u}$ satisfying $g_{1}<g_{2}<\cdots<g_{u}$. Consider this
$\left(  g_{1},g_{2},\ldots,g_{u}\right)  $.
\par
Set $I=\beta\left(  \mathbf{i}\right)  $. Notice that $\beta\left(
\mathbf{i}\right)  \in\mathcal{P}_{u}\left(  S\right)  $ (because $\beta$ is a
map $\mathbf{I}\rightarrow\mathcal{P}_{u}\left(  S\right)  $). In other words,
$I\in\mathcal{P}_{u}\left(  S\right)  $ (since $I=\beta\left(  \mathbf{i}%
\right)  $). In other words, $I$ is a $u$-element subset of $S$ (since
$\mathcal{P}_{u}\left(  S\right)  $ is the set of all $u$-element subsets of
$S$). Thus, $I$ is a $u$-element set. In other words, $\left\vert I\right\vert
=u$.
\par
Applying the map $\beta$ to the equality $\mathbf{i}=\left(  g_{1}%
,g_{2},\ldots,g_{u}\right)  $, we obtain%
\[
\beta\left(  \mathbf{i}\right)  =\beta\left(  g_{1},g_{2},\ldots,g_{u}\right)
=\left\{  g_{1},g_{2},\ldots,g_{u}\right\}
\]
(by the definition of $\beta$). Thus, $I=\beta\left(  \mathbf{i}\right)
=\left\{  g_{1},g_{2},\ldots,g_{u}\right\}  $.
\par
Now, $\alpha\left(  \underbrace{\beta\left(  \mathbf{i}\right)  }_{=I}\right)
=\alpha\left(  I\right)  =w\left(  I\right)  $ (by the definition of the map
$\alpha$). But $w\left(  I\right)  $ is the list of all elements of $I$ in
increasing order (with no repetitions) (by the definition of $w\left(
I\right)  $). Hence, $w\left(  I\right)  $ is a list of size $\left\vert
I\right\vert =u$.
\par
Write the list $w\left(  I\right)  $ in the form $w\left(  I\right)  =\left(
p_{1},p_{2},\ldots,p_{u}\right)  $. (This is possible, since $w\left(
I\right)  $ is a list of size $u$.)
\par
Recall that $w\left(  I\right)  $ is the list of all elements of $I$ in
increasing order (with no repetitions). In other words, $\left(  p_{1}%
,p_{2},\ldots,p_{u}\right)  $ is the list of all elements of $I$ in increasing
order (with no repetitions) (since $w\left(  I\right)  =\left(  p_{1}%
,p_{2},\ldots,p_{u}\right)  $). Thus, $\left(  p_{1},p_{2},\ldots
,p_{u}\right)  $ is a strictly increasing list. In other words, $p_{1}%
<p_{2}<\cdots<p_{u}$.
\par
But $\left(  p_{1},p_{2},\ldots,p_{u}\right)  $ is the list of all elements of
$I$ in increasing order (with no repetitions). Thus, $\left(  p_{1}%
,p_{2},\ldots,p_{u}\right)  $ is a list of all elements of $I$. Hence,
$\left\{  p_{1},p_{2},\ldots,p_{u}\right\}  =I$. Thus, $\left\{  p_{1}%
,p_{2},\ldots,p_{u}\right\}  =I=\left\{  g_{1},g_{2},\ldots,g_{u}\right\}  $.
\par
Note that $I$ is a subset of $S$. Thus, $I\subseteq S\subseteq\mathbb{Z}$.
Each $k\in\left\{  1,2,\ldots,u\right\}  $ satisfies $p_{k}\in\left\{
p_{1},p_{2},\ldots,p_{u}\right\}  =I\subseteq\mathbb{Z}$. In other words, for
each $k\in\left\{  1,2,\ldots,u\right\}  $, the element $p_{k}$ is an integer.
Hence, $\left(  p_{1},p_{2},\ldots,p_{u}\right)  $ is a list of integers.
Also, each $k\in\left\{  1,2,\ldots,u\right\}  $ satisfies $g_{k}\in\left\{
g_{1},g_{2},\ldots,g_{u}\right\}  =I\subseteq\mathbb{Z}$. In other words, for
each $k\in\left\{  1,2,\ldots,u\right\}  $, the element $g_{k}$ is an integer.
Hence, $\left(  g_{1},g_{2},\ldots,g_{u}\right)  $ is a list of integers.
\par
Now, Lemma \ref{lem.sol.addexe.jacobi-complement.sets-eq} (applied to $\left(
g_{1},g_{2},\ldots,g_{u}\right)  $ instead of $\left(  q_{1},q_{2}%
,\ldots,q_{u}\right)  $) yields that $\left(  p_{1},p_{2},\ldots,p_{u}\right)
=\left(  g_{1},g_{2},\ldots,g_{u}\right)  $.
\par
But%
\begin{align*}
\left(  \alpha\circ\beta\right)  \left(  \mathbf{i}\right)   &  =\alpha\left(
\beta\left(  \mathbf{i}\right)  \right)  =w\left(  I\right)  =\left(
p_{1},p_{2},\ldots,p_{u}\right)  =\left(  g_{1},g_{2},\ldots,g_{u}\right) \\
&  =\mathbf{i}\ \ \ \ \ \ \ \ \ \ \left(  \text{since }\mathbf{i}=\left(
g_{1},g_{2},\ldots,g_{u}\right)  \right) \\
&  =\operatorname*{id}\left(  \mathbf{i}\right)  .
\end{align*}
\par
Now, forget that we fixed $\mathbf{i}$. We thus have shown that $\left(
\alpha\circ\beta\right)  \left(  \mathbf{i}\right)  =\operatorname*{id}\left(
\mathbf{i}\right)  $ for each $\mathbf{i}\in\mathbf{I}$. In other words,
$\alpha\circ\beta=\operatorname*{id}$. Qed.} and $\beta\circ\alpha
=\operatorname*{id}$\ \ \ \ \footnote{\textit{Proof.} Let $I\in\mathcal{P}%
_{u}\left(  S\right)  $. Then, $\alpha\left(  I\right)  =w\left(  I\right)  $
(by the definition of $\alpha$). Clearly, $\alpha\left(  I\right)
\in\mathbf{I}$ (since $\alpha$ is a map $\mathcal{P}_{u}\left(  S\right)
\rightarrow\mathbf{I}$).
\par
We have $I\in\mathcal{P}_{u}\left(  S\right)  $. In other words, $I$ is a
$u$-element subset of $S$ (since $\mathcal{P}_{u}\left(  S\right)  $ is the
set of all $u$-element subsets of $S$). Thus, $I$ is a $u$-element set. In
other words, $\left\vert I\right\vert =u$.
\par
But $w\left(  I\right)  $ is the list of all elements of $I$ in increasing
order (with no repetitions) (by the definition of $w\left(  I\right)  $).
Hence, $w\left(  I\right)  $ is a list of size $\left\vert I\right\vert =u$.
In other words, $\alpha\left(  I\right)  $ is a list of size $u$ (since
$\alpha\left(  I\right)  =w\left(  I\right)  $).
\par
Write the list $\alpha\left(  I\right)  $ in the form $\alpha\left(  I\right)
=\left(  p_{1},p_{2},\ldots,p_{u}\right)  $. (This is possible, since
$\alpha\left(  I\right)  $ is a list of size $u$.) Hence, $w\left(  I\right)
=\alpha\left(  I\right)  =\left(  p_{1},p_{2},\ldots,p_{u}\right)  $.
\par
Recall that $w\left(  I\right)  $ is the list of all elements of $I$ in
increasing order (with no repetitions). Thus, $w\left(  I\right)  $ is a list
of all elements of $I$. In other words, $\left(  p_{1},p_{2},\ldots
,p_{u}\right)  $ is a list of all elements of $I$ (since $w\left(  I\right)
=\left(  p_{1},p_{2},\ldots,p_{u}\right)  $). Thus, $\left\{  p_{1}%
,p_{2},\ldots,p_{u}\right\}  =I$.
\par
But%
\begin{align*}
\left(  \beta\circ\alpha\right)  \left(  I\right)   &  =\beta\left(
\underbrace{\alpha\left(  I\right)  }_{=\left(  p_{1},p_{2},\ldots
,p_{u}\right)  }\right)  =\beta\left(  p_{1},p_{2},\ldots,p_{u}\right) \\
&  =\left\{  p_{1},p_{2},\ldots,p_{u}\right\}  \ \ \ \ \ \ \ \ \ \ \left(
\text{by the definition of }\beta\right) \\
&  =I=\operatorname*{id}\left(  I\right)  .
\end{align*}
\par
Let us now forget that we fixed $I$. We thus have shown that $\left(
\beta\circ\alpha\right)  \left(  I\right)  =\operatorname*{id}\left(
I\right)  $ for each $I\in\mathcal{P}_{u}\left(  S\right)  $. In other words,
$\beta\circ\alpha=\operatorname*{id}$. Qed.}. Combining these two equalities,
we conclude that the maps $\alpha$ and $\beta$ are mutually inverse. Hence,
the map $\alpha$ is invertible. In other words, the map $\alpha$ is a
bijection. In other words, the map%
\begin{align*}
\mathcal{P}_{u}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is a bijection (since the map $\alpha$ is the map%
\begin{align*}
\mathcal{P}_{u}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
(by the definition of $\alpha$)). This completes the proof of Lemma
\ref{lem.sol.addexe.jacobi-complement.increase-bij}.
\end{proof}
\end{verlong}

We can use Lemma \ref{lem.sol.addexe.jacobi-complement.increase-bij} to obtain
the following (slightly weaker) form of Corollary
\ref{cor.adj(AB).cauchy-binet-general}:

\begin{corollary}
\label{cor.sol.addexe.jacobi-complement.CB}Let $n\in\mathbb{N}$,
$m\in\mathbb{N}$ and $p\in\mathbb{N}$. Let $A$ be an $n\times p$-matrix. Let
$B$ be a $p\times m$-matrix. Let $k\in\mathbb{N}$. Let $P$ be a subset of
$\left\{  1,2,\ldots,n\right\}  $ such that $\left\vert P\right\vert =k$. Let
$Q$ be a subset of $\left\{  1,2,\ldots,m\right\}  $ such that $\left\vert
Q\right\vert =k$. Then,%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\sum_{\substack{R\subseteq\left\{
1,2,\ldots,p\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  .
\]

\end{corollary}

\begin{vershort}
\begin{proof}
[Proof of Corollary \ref{cor.sol.addexe.jacobi-complement.CB}.]For every set
$S$, we let $\mathcal{P}_{k}\left(  S\right)  $ denote the set of all
$k$-element subsets of $S$.

Recall that $w\left(  P\right)  $ was defined as the list of all elements of
$P$ in increasing order (with no repetitions). Thus, $w\left(  P\right)  $ is
a list of $\left\vert P\right\vert $ elements. In other words, $w\left(
P\right)  $ is a list of $k$ elements (since $\left\vert P\right\vert =k$).
Similarly, $w\left(  Q\right)  $ is a list of $k$ elements.

Write the list $w\left(  P\right)  $ in the form $w\left(  P\right)  =\left(
i_{1},i_{2},\ldots,i_{k}\right)  $. (This is possible, since $w\left(
P\right)  $ is a list of $k$ elements.)

Write the list $w\left(  Q\right)  $ in the form $w\left(  Q\right)  =\left(
j_{1},j_{2},\ldots,j_{k}\right)  $. (This is possible, since $w\left(
Q\right)  $ is a list of $k$ elements.)

We have $\left(  i_{1},i_{2},\ldots,i_{k}\right)  =w\left(  P\right)  $. Thus,
the elements $i_{1},i_{2},\ldots,i_{k}$ are elements of $P$, and hence also
elements of $\left\{  1,2,\ldots,n\right\}  $ (since $P\subseteq\left\{
1,2,\ldots,n\right\}  $). The same argument (but applied to $m$, $Q$ and
$\left(  j_{1},j_{2},\ldots,j_{k}\right)  $ instead of $n$, $P$ and $\left(
i_{1},i_{2},\ldots,i_{k}\right)  $) yields that $j_{1},j_{2},\ldots,j_{k}$ are
elements of $\left\{  1,2,\ldots,m\right\}  $. Hence, Corollary
\ref{cor.adj(AB).cauchy-binet-general} (applied to $u=k$) yields%
\begin{align}
&  \det\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
^{j_{1},j_{2},\ldots,j_{k}}\left(  AB\right)  \right) \nonumber\\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}\det\left(  \operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}^{g_{1},g_{2},\ldots,g_{k}}A\right)
\cdot\det\left(  \operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{k}%
}^{j_{1},j_{2},\ldots,j_{k}}B\right)  .
\label{pf.cor.sol.addexe.jacobi-complement.CB.short.1}%
\end{align}
(Here, the summation sign \textquotedblleft$\sum_{1\leq g_{1}<g_{2}%
<\cdots<g_{k}\leq p}$\textquotedblright\ has to be interpreted as
\textquotedblleft$\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{k}\right)
\in\left\{  1,2,\ldots,p\right\}  ^{k};\\g_{1}<g_{2}<\cdots<g_{k}}%
}$\textquotedblright, in analogy to Remark \ref{rmk.cauchy-binet.sumsign}.)

Now, let $S$ denote the set $\left\{  1,2,\ldots,p\right\}  $. Recall that
$\mathcal{P}_{k}\left(  S\right)  $ denotes the set of all $k$-element subsets
of $S$. In other words,
\[
\mathcal{P}_{k}\left(  S\right)  =\left\{  T\subseteq S\ \mid\ \left\vert
T\right\vert =k\right\}  .
\]


Let $\mathbf{I}$ denote the set
\[
\left\{  \left(  g_{1},g_{2},\ldots,g_{k}\right)  \in S^{k}\ \mid\ g_{1}%
<g_{2}<\cdots<g_{k}\right\}  .
\]
Lemma \ref{lem.sol.addexe.jacobi-complement.increase-bij} (applied to $u=k$)
shows that the map%
\begin{align*}
\mathcal{P}_{k}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is well-defined and a bijection.

The definition of $\mathbf{I}$ yields $\mathbf{I}=\left\{  \left(  g_{1}%
,g_{2},\ldots,g_{k}\right)  \in S^{k}\ \mid\ g_{1}<g_{2}<\cdots<g_{k}\right\}
$. Hence, we have the following equality of summation signs:%
\[
\sum_{\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in\mathbf{I}}=\sum
_{\substack{\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in S^{k};\\g_{1}%
<g_{2}<\cdots<g_{k}}}=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{k}\right)
\in\left\{  1,2,\ldots,p\right\}  ^{k};\\g_{1}<g_{2}<\cdots<g_{k}}}
\]
(since $S=\left\{  1,2,\ldots,p\right\}  $). Comparing this with%
\[
\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}=\sum_{\substack{\left(
g_{1},g_{2},\ldots,g_{k}\right)  \in\left\{  1,2,\ldots,p\right\}
^{k};\\g_{1}<g_{2}<\cdots<g_{k}}},
\]
we obtain%
\[
\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}=\sum_{\left(  g_{1},g_{2}%
,\ldots,g_{k}\right)  \in\mathbf{I}}.
\]
Now, (\ref{pf.cor.sol.addexe.jacobi-complement.CB.short.1}) becomes%
\begin{align*}
&  \det\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
^{j_{1},j_{2},\ldots,j_{k}}\left(  AB\right)  \right) \\
&  =\underbrace{\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}}_{=\sum_{\left(
g_{1},g_{2},\ldots,g_{k}\right)  \in\mathbf{I}}}\det\left(
\underbrace{\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
^{g_{1},g_{2},\ldots,g_{k}}A}_{=\operatorname*{sub}\nolimits_{\left(
i_{1},i_{2},\ldots,i_{k}\right)  }^{\left(  g_{1},g_{2},\ldots,g_{k}\right)
}A}\right)  \cdot\det\left(  \underbrace{\operatorname*{sub}\nolimits_{g_{1}%
,g_{2},\ldots,g_{k}}^{j_{1},j_{2},\ldots,j_{k}}B}_{=\operatorname*{sub}%
\nolimits_{\left(  g_{1},g_{2},\ldots,g_{k}\right)  }^{\left(  j_{1}%
,j_{2},\ldots,j_{k}\right)  }B}\right) \\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in\mathbf{I}}\det\left(
\operatorname*{sub}\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)
}^{\left(  g_{1},g_{2},\ldots,g_{k}\right)  }A\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{\left(  g_{1},g_{2},\ldots,g_{k}\right)
}^{\left(  j_{1},j_{2},\ldots,j_{k}\right)  }B\right) \\
&  =\underbrace{\sum_{R\in\mathcal{P}_{k}\left(  S\right)  }}_{\substack{=\sum
_{\substack{R\subseteq S;\\\left\vert R\right\vert =k}}\\\text{(since
}\mathcal{P}_{k}\left(  S\right)  =\left\{  T\subseteq S\ \mid\ \left\vert
T\right\vert =k\right\}  \text{)}}}\det\left(  \underbrace{\operatorname*{sub}%
\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)  }^{w\left(  R\right)  }%
A}_{\substack{=\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
R\right)  }A\\\text{(since }\left(  i_{1},i_{2},\ldots,i_{k}\right)  =w\left(
P\right)  \text{)}}}\right)  \cdot\det\left(  \underbrace{\operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{\left(  j_{1},j_{2},\ldots,j_{k}\right)  }%
B}_{\substack{=\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }B\\\text{(since }\left(  j_{1},j_{2},\ldots,j_{k}\right)  =w\left(
Q\right)  \text{)}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }w\left(  R\right)  \text{ for }\left(
g_{1},g_{2},\ldots,g_{k}\right)  \text{, since}\\
\text{the map }\mathcal{P}_{k}\left(  S\right)  \rightarrow\mathbf{I}%
,\ R\mapsto w\left(  R\right)  \text{ is a bijection}%
\end{array}
\right) \\
&  =\underbrace{\sum_{\substack{R\subseteq S;\\\left\vert R\right\vert =k}%
}}_{\substack{=\sum_{\substack{R\subseteq\left\{  1,2,\ldots,p\right\}
;\\\left\vert R\right\vert =k}}\\\text{(since }S=\left\{  1,2,\ldots
,p\right\}  \text{)}}}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  R\right)  }A\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}B\right) \\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,p\right\}  ;\\\left\vert
R\right\vert =k}}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  R\right)  }A\right)  \cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\det\left(  \underbrace{\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots
,i_{k}}^{j_{1},j_{2},\ldots,j_{k}}\left(  AB\right)  }_{=\operatorname*{sub}%
\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)  }^{\left(  j_{1}%
,j_{2},\ldots,j_{k}\right)  }\left(  AB\right)  }\right)   &  =\det\left(
\operatorname*{sub}\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)
}^{\left(  j_{1},j_{2},\ldots,j_{k}\right)  }\left(  AB\right)  \right)
=\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  i_{1},i_{2},\ldots,i_{k}\right)  =w\left(  P\right) \\
\text{and }\left(  j_{1},j_{2},\ldots,j_{k}\right)  =w\left(  Q\right)
\end{array}
\right)  ,
\end{align*}
we obtain%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\sum_{\substack{R\subseteq\left\{
1,2,\ldots,p\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  .
\]
This proves Corollary \ref{cor.adj(AB).cauchy-binet-general}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Corollary \ref{cor.sol.addexe.jacobi-complement.CB}.]For every set
$S$, we let $\mathcal{P}_{k}\left(  S\right)  $ denote the set of all
$k$-element subsets of $S$.

We know that $w\left(  P\right)  $ is the list of all elements of $P$ in
increasing order (with no repetitions) (by the definition of $w\left(
P\right)  $). Thus, $w\left(  P\right)  $ is a list of $\left\vert
P\right\vert $ elements. In other words, $w\left(  P\right)  $ is a list of
$k$ elements (since $\left\vert P\right\vert =k$).

Write the list $w\left(  P\right)  $ in the form $w\left(  P\right)  =\left(
i_{1},i_{2},\ldots,i_{k}\right)  $. (This is possible, since $w\left(
P\right)  $ is a list of $k$ elements.)

We know that $w\left(  Q\right)  $ is the list of all elements of $Q$ in
increasing order (with no repetitions) (by the definition of $w\left(
Q\right)  $). Thus, $w\left(  Q\right)  $ is a list of $\left\vert
Q\right\vert $ elements. In other words, $w\left(  Q\right)  $ is a list of
$k$ elements (since $\left\vert Q\right\vert =k$).

Write the list $w\left(  Q\right)  $ in the form $w\left(  Q\right)  =\left(
j_{1},j_{2},\ldots,j_{k}\right)  $. (This is possible, since $w\left(
Q\right)  $ is a list of $k$ elements.)

We know that $w\left(  P\right)  $ is the list of all elements of $P$ in
increasing order (with no repetitions). Hence, $w\left(  P\right)  $ is a list
of all elements of $P$. In other words, $\left(  i_{1},i_{2},\ldots
,i_{k}\right)  $ is a list of all elements of $P$ (since $w\left(  P\right)
=\left(  i_{1},i_{2},\ldots,i_{k}\right)  $). Thus, $\left\{  i_{1}%
,i_{2},\ldots,i_{k}\right\}  =P$. Now, every $\ell\in\left\{  1,2,\ldots
,k\right\}  $ satisfies $i_{\ell}\in\left\{  i_{1},i_{2},\ldots,i_{k}\right\}
=P\subseteq\left\{  1,2,\ldots,n\right\}  $ (since $P$ is a subset of
$\left\{  1,2,\ldots,n\right\}  $). In other words, $i_{1},i_{2},\ldots,i_{k}$
are elements of $\left\{  1,2,\ldots,n\right\}  $. The same argument (but
applied to $m$, $Q$ and $\left(  j_{1},j_{2},\ldots,j_{k}\right)  $ instead of
$n$, $P$ and $\left(  i_{1},i_{2},\ldots,i_{k}\right)  $) yields that
$j_{1},j_{2},\ldots,j_{k}$ are elements of $\left\{  1,2,\ldots,m\right\}  $.
Hence, Corollary \ref{cor.adj(AB).cauchy-binet-general} (applied to $u=k$)
yields%
\begin{align}
&  \det\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
^{j_{1},j_{2},\ldots,j_{k}}\left(  AB\right)  \right) \nonumber\\
&  =\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}\det\left(  \operatorname*{sub}%
\nolimits_{i_{1},i_{2},\ldots,i_{k}}^{g_{1},g_{2},\ldots,g_{k}}A\right)
\cdot\det\left(  \operatorname*{sub}\nolimits_{g_{1},g_{2},\ldots,g_{k}%
}^{j_{1},j_{2},\ldots,j_{k}}B\right)  .
\label{pf.cor.sol.addexe.jacobi-complement.CB.1}%
\end{align}
(Here, the summation sign \textquotedblleft$\sum_{1\leq g_{1}<g_{2}%
<\cdots<g_{k}\leq p}$\textquotedblright\ has to be interpreted as
\textquotedblleft$\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{k}\right)
\in\left\{  1,2,\ldots,p\right\}  ^{k};\\g_{1}<g_{2}<\cdots<g_{k}}%
}$\textquotedblright, in analogy to Remark \ref{rmk.cauchy-binet.sumsign}.)

Now, let $S$ denote the set $\left\{  1,2,\ldots,p\right\}  $. Clearly, $S$ is
a set of integers. Recall that $\mathcal{P}_{k}\left(  S\right)  $ denotes the
set of all $k$-element subsets of $S$. In other words,
\[
\mathcal{P}_{k}\left(  S\right)  =\left\{  T\subseteq S\ \mid\ \left\vert
T\right\vert =k\right\}  .
\]


Let $\mathbf{I}$ denote the set
\[
\left\{  \left(  g_{1},g_{2},\ldots,g_{k}\right)  \in S^{k}\ \mid\ g_{1}%
<g_{2}<\cdots<g_{k}\right\}  .
\]
Thus, every element of $\mathbf{I}$ has the form $\left(  g_{1},g_{2}%
,\ldots,g_{k}\right)  $ for some $\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in
S^{k}$.

Lemma \ref{lem.sol.addexe.jacobi-complement.increase-bij} (applied to $u=k$)
shows that the map%
\begin{align*}
\mathcal{P}_{k}\left(  S\right)   &  \rightarrow\mathbf{I},\\
R  &  \mapsto w\left(  R\right)
\end{align*}
is well-defined and a bijection.

The definition of $\mathbf{I}$ yields
\[
\mathbf{I}=\left\{  \left(  g_{1},g_{2},\ldots,g_{k}\right)  \in S^{k}%
\ \mid\ g_{1}<g_{2}<\cdots<g_{k}\right\}  .
\]
Hence, we have the following equality of summation signs:%
\[
\sum_{\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in\mathbf{I}}=\sum
_{\substack{\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in S^{k};\\g_{1}%
<g_{2}<\cdots<g_{k}}}=\sum_{\substack{\left(  g_{1},g_{2},\ldots,g_{k}\right)
\in\left\{  1,2,\ldots,p\right\}  ^{k};\\g_{1}<g_{2}<\cdots<g_{k}}}
\]
(since $S=\left\{  1,2,\ldots,p\right\}  $). Comparing this with%
\[
\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}=\sum_{\substack{\left(
g_{1},g_{2},\ldots,g_{k}\right)  \in\left\{  1,2,\ldots,p\right\}
^{k};\\g_{1}<g_{2}<\cdots<g_{k}}},
\]
we obtain%
\[
\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}=\sum_{\left(  g_{1},g_{2}%
,\ldots,g_{k}\right)  \in\mathbf{I}}.
\]
Now, (\ref{pf.cor.sol.addexe.jacobi-complement.CB.1}) becomes%
\begin{align*}
&  \det\left(  \operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
^{j_{1},j_{2},\ldots,j_{k}}\left(  AB\right)  \right) \\
&  =\underbrace{\sum_{1\leq g_{1}<g_{2}<\cdots<g_{k}\leq p}}_{=\sum_{\left(
g_{1},g_{2},\ldots,g_{k}\right)  \in\mathbf{I}}}\det\left(
\underbrace{\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots,i_{k}}%
^{g_{1},g_{2},\ldots,g_{k}}A}_{=\operatorname*{sub}\nolimits_{\left(
i_{1},i_{2},\ldots,i_{k}\right)  }^{\left(  g_{1},g_{2},\ldots,g_{k}\right)
}A}\right)  \cdot\det\left(  \underbrace{\operatorname*{sub}\nolimits_{g_{1}%
,g_{2},\ldots,g_{k}}^{j_{1},j_{2},\ldots,j_{k}}B}_{=\operatorname*{sub}%
\nolimits_{\left(  g_{1},g_{2},\ldots,g_{k}\right)  }^{\left(  j_{1}%
,j_{2},\ldots,j_{k}\right)  }B}\right) \\
&  =\sum_{\left(  g_{1},g_{2},\ldots,g_{k}\right)  \in\mathbf{I}}\det\left(
\operatorname*{sub}\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)
}^{\left(  g_{1},g_{2},\ldots,g_{k}\right)  }A\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{\left(  g_{1},g_{2},\ldots,g_{k}\right)
}^{\left(  j_{1},j_{2},\ldots,j_{k}\right)  }B\right) \\
&  =\underbrace{\sum_{R\in\mathcal{P}_{k}\left(  S\right)  }}_{\substack{=\sum
_{R\in\left\{  T\subseteq S\ \mid\ \left\vert T\right\vert =k\right\}
}\\\text{(since }\mathcal{P}_{k}\left(  S\right)  =\left\{  T\subseteq
S\ \mid\ \left\vert T\right\vert =k\right\}  \text{)}}}\det\left(
\underbrace{\operatorname*{sub}\nolimits_{\left(  i_{1},i_{2},\ldots
,i_{k}\right)  }^{w\left(  R\right)  }A}_{\substack{=\operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  R\right)  }A\\\text{(since }\left(
i_{1},i_{2},\ldots,i_{k}\right)  =w\left(  P\right)  \text{)}}}\right)
\cdot\det\left(  \underbrace{\operatorname*{sub}\nolimits_{w\left(  R\right)
}^{\left(  j_{1},j_{2},\ldots,j_{k}\right)  }B}%
_{\substack{=\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }B\\\text{(since }\left(  j_{1},j_{2},\ldots,j_{k}\right)  =w\left(
Q\right)  \text{)}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{here, we have substituted }w\left(  R\right)  \text{ for }\left(
g_{1},g_{2},\ldots,g_{k}\right)  \text{, since}\\
\text{the map }\mathcal{P}_{k}\left(  S\right)  \rightarrow\mathbf{I}%
,\ R\mapsto w\left(  R\right)  \text{ is a bijection}%
\end{array}
\right) \\
&  =\underbrace{\sum_{R\in\left\{  T\subseteq S\ \mid\ \left\vert T\right\vert
=k\right\}  }}_{\substack{=\sum_{\substack{R\subseteq S;\\\left\vert
R\right\vert =k}}=\sum_{\substack{R\subseteq\left\{  1,2,\ldots,p\right\}
;\\\left\vert R\right\vert =k}}\\\text{(since }S=\left\{  1,2,\ldots
,p\right\}  \text{)}}}\det\left(  \operatorname*{sub}\nolimits_{w\left(
P\right)  }^{w\left(  R\right)  }A\right)  \cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}B\right) \\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,p\right\}  ;\\\left\vert
R\right\vert =k}}\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  R\right)  }A\right)  \cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  .
\end{align*}
Comparing this with%
\begin{align*}
\det\left(  \underbrace{\operatorname*{sub}\nolimits_{i_{1},i_{2},\ldots
,i_{k}}^{j_{1},j_{2},\ldots,j_{k}}\left(  AB\right)  }_{=\operatorname*{sub}%
\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)  }^{\left(  j_{1}%
,j_{2},\ldots,j_{k}\right)  }\left(  AB\right)  }\right)   &  =\det\left(
\operatorname*{sub}\nolimits_{\left(  i_{1},i_{2},\ldots,i_{k}\right)
}^{\left(  j_{1},j_{2},\ldots,j_{k}\right)  }\left(  AB\right)  \right)
=\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }\left(  i_{1},i_{2},\ldots,i_{k}\right)  =w\left(  P\right) \\
\text{and }\left(  j_{1},j_{2},\ldots,j_{k}\right)  =w\left(  Q\right)
\end{array}
\right)  ,
\end{align*}
we obtain%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\sum_{\substack{R\subseteq\left\{
1,2,\ldots,p\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  .
\]
This proves Corollary \ref{cor.adj(AB).cauchy-binet-general}.
\end{proof}
\end{verlong}

Our next step towards solving Additional exercise
\ref{addexe.jacobi-complement} again is the following fact:

\begin{proposition}
\label{prop.sol.addexe.jacobi-complement.genform}Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we
let $\widetilde{I}$ denote the complement $\left\{  1,2,\ldots,n\right\}
\setminus I$ of $I$.

Let $A$ be an $n\times n$-matrix. Let $B$ be an $n\times m$-matrix. Let $P$ be
a subset of $\left\{  1,2,\ldots,n\right\}  $. Let $Q$ be a subset of
$\left\{  1,2,\ldots,m\right\}  $ such that $\left\vert P\right\vert
=\left\vert Q\right\vert $. Then,%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right) \\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)  }\left(  AB\right)
\right)  .
\end{align*}

\end{proposition}

\begin{vershort}
\begin{proof}
[Proof of Proposition \ref{prop.sol.addexe.jacobi-complement.genform}.]Set
$k=\left\vert P\right\vert $. Clearly, $k=\left\vert P\right\vert =\left\vert
Q\right\vert $.

If $K$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
K\right\vert =\left\vert P\right\vert $, then%
\begin{align}
&  \det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)
}^{w\left(  R\right)  }A\right)  \cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.1}%
\end{align}
\footnote{\textit{Proof of
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.1}):} Let $K$ be a
subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
K\right\vert =\left\vert P\right\vert $. Then, $\left\vert K\right\vert
=\left\vert P\right\vert =k$ (since $k=\left\vert P\right\vert $). Also,
$\left\vert Q\right\vert =k$ (since $k=\left\vert Q\right\vert $). Hence,
Corollary \ref{cor.sol.addexe.jacobi-complement.CB} (applied to $n$ and $K$
instead of $p$ and $P$) yields
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\sum_{\substack{R\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  .
\]
This proves (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.1}).}.

Now,%
\begin{align}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \underbrace{\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)
}\left(  AB\right)  \right)  }_{\substack{=\sum_{\substack{R\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  \\\text{(by
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.1}))}}}\nonumber\\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \sum_{\substack{R\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  \right) \nonumber\\
&  =\underbrace{\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert K\right\vert =\left\vert P\right\vert }}\sum
_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}}_{=\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert R\right\vert =k}}\sum_{\substack{K\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert }%
}}\underbrace{\left(  -1\right)  ^{\sum P+\sum K}}_{\substack{=\left(
-1\right)  ^{\sum K+\sum P}\\\text{(since }\sum P+\sum K=\sum K+\sum
P\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \underbrace{\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)
}^{w\left(  R\right)  }A\right)  }_{=\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  R\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(
\widetilde{P}\right)  }A\right)  }\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert K\right\vert =\left\vert P\right\vert }}\left(  -1\right)
^{\sum K+\sum P}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \det\left(  \operatorname*{sub}\nolimits_{w\left(
K\right)  }^{w\left(  R\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\left(  \sum_{\substack{K\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert }}\left(
-1\right)  ^{\sum K+\sum P}\det\left(  \operatorname*{sub}\nolimits_{w\left(
K\right)  }^{w\left(  R\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(
R\right)  }^{w\left(  Q\right)  }B\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.3}%
\end{align}


On the other hand, for every subset $G$ of $\left\{  1,2,\ldots,n\right\}  $,
we have%
\begin{equation}
\det A=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert G\right\vert }}\left(  -1\right)  ^{\sum K+\sum
G}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
G\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{G}\right)  }A\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.6}%
\end{equation}
(Indeed, this is precisely the claim of Theorem \ref{thm.det.laplace-multi}
\textbf{(b)}, with the variables $P$ and $Q$ renamed as $K$ and $G$.) Applying
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.6}) to $G=P$, we
obtain%
\begin{equation}
\det A=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
P\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.6b}%
\end{equation}


For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

If $R$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
R\right\vert =k$, then%
\begin{align}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right) \nonumber\\
&  =\delta_{R,P}\det A
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.5}%
\end{align}
\footnote{\textit{Proof of
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.5}):} Let $R$ be a
subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
R\right\vert =k$. We must prove
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.5}).
\par
We have
\[
\underbrace{\delta_{P,P}}_{\substack{=1\\\text{(since }P=P\text{)}}}\det
A=\det A=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert K\right\vert =\left\vert P\right\vert }}\left(  -1\right)
^{\sum K+\sum P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)
}^{w\left(  P\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)
\]
(by (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.6})). In other
words, (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.5}) holds if
$R=P$. Hence, for the rest of our proof of
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.5}), we WLOG assume
that $R\neq P$. Thus, $\delta_{R,P}=0$.
\par
We have $R\neq P$ and thus $P\neq R$. Also, $\left\vert P\right\vert
=k=\left\vert R\right\vert $.
\par
For every subset $G$ of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert G\right\vert =\left\vert R\right\vert $ and $G\neq R$, we have%
\begin{equation}
0=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert G\right\vert }}\left(  -1\right)  ^{\sum K+\sum
G}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{G}\right)  }A\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.7}%
\end{equation}
(Indeed, this is precisely the claim of Exercise \ref{exe.det.laplace-multi.0}
\textbf{(b)}, with the variables $Q$ and $P$ renamed as $G$ and $K$.) Applying
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.7}) to $G=P$, we
obtain%
\[
0=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)
\]
(since $\left\vert P\right\vert =\left\vert R\right\vert $ and $P\neq R$).
Hence,%
\begin{align*}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right) \\
&  =0=\underbrace{0}_{=\delta_{R,P}}\det A=\delta_{R,P}\det A.
\end{align*}
This proves (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.5}).}.

Now, (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.3}) becomes%
\begin{align}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)  }\left(  AB\right)
\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\underbrace{\left(  \sum_{\substack{K\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert
}}\left(  -1\right)  ^{\sum K+\sum P}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  R\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(
\widetilde{P}\right)  }A\right)  \right)  }_{\substack{=\delta_{R,P}\det
A\\\text{(by (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.5}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(
R\right)  }^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.short.11}%
\end{align}


But $P$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ and satisfies
$\left\vert P\right\vert =k$. In other words, $P$ is a subset $R$ of $\left\{
1,2,\ldots,n\right\}  $ satisfying $\left\vert R\right\vert =k$. Thus, the sum
$\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  $ has an addend
for $R=P$. If we split off this addend from this sum, then we obtain%
\begin{align*}
&  \sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right) \\
&  =\underbrace{\delta_{P,P}}_{\substack{=1\\\text{(since }P=P\text{)}}}\det
A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }B\right)  +\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert R\right\vert =k;\\R\neq P}}\underbrace{\delta_{R,P}%
}_{\substack{=0\\\text{(since }R\neq P\text{)}}}\det A\cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}B\right) \\
&  =\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)  +\underbrace{\sum_{\substack{R\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k;\\R\neq P}}0\det
A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }B\right)  }_{=0}\\
&  =\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)  .
\end{align*}
Hence, (\ref{pf.prop.sol.addexe.jacobi-complement.genform.short.11}) becomes%
\begin{align*}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)  }\left(  AB\right)
\right) \\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  =\det
A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }B\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.addexe.jacobi-complement.genform}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Proposition \ref{prop.sol.addexe.jacobi-complement.genform}.]The set
$P$ is a subset of $\left\{  1,2,\ldots,n\right\}  $, and thus is finite
(since $\left\{  1,2,\ldots,n\right\}  $ is finite). Thus, $\left\vert
P\right\vert \in\mathbb{N}$. Hence, we can define $k\in\mathbb{N}$ by
$k=\left\vert P\right\vert $. Consider this $k$. Clearly, $k=\left\vert
P\right\vert =\left\vert Q\right\vert $.

If $K$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
K\right\vert =\left\vert P\right\vert $, then%
\begin{align}
&  \det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)
}^{w\left(  R\right)  }A\right)  \cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)
\label{pf.prop.sol.addexe.jacobi-complement.genform.1}%
\end{align}
\footnote{\textit{Proof of
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.1}):} Let $K$ be a subset
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert K\right\vert
=\left\vert P\right\vert $. Then, $\left\vert K\right\vert =\left\vert
P\right\vert =k$ (since $k=\left\vert P\right\vert $). Also, $\left\vert
Q\right\vert =k$ (since $k=\left\vert Q\right\vert $). Hence, Corollary
\ref{cor.sol.addexe.jacobi-complement.CB} (applied to $n$ and $K$ instead of
$p$ and $P$) yields
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\sum_{\substack{R\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  .
\]
This proves (\ref{pf.prop.sol.addexe.jacobi-complement.genform.1}).}.

Now,%
\begin{align}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \underbrace{\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)
}\left(  AB\right)  \right)  }_{\substack{=\sum_{\substack{R\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  \\\text{(by
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.1}))}}}\nonumber\\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \sum_{\substack{R\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k}}\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  R\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right)  \right) \nonumber\\
&  =\underbrace{\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert K\right\vert =\left\vert P\right\vert }}\sum
_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}}_{=\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert R\right\vert =k}}\sum_{\substack{K\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert }%
}}\underbrace{\left(  -1\right)  ^{\sum P+\sum K}}_{\substack{=\left(
-1\right)  ^{\sum K+\sum P}\\\text{(since }\sum P+\sum K=\sum K+\sum
P\text{)}}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \underbrace{\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)
}^{w\left(  R\right)  }A\right)  }_{=\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  R\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(
\widetilde{P}\right)  }A\right)  }\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert K\right\vert =\left\vert P\right\vert }}\left(  -1\right)
^{\sum K+\sum P}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \det\left(  \operatorname*{sub}\nolimits_{w\left(
K\right)  }^{w\left(  R\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)
}^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\left(  \sum_{\substack{K\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert }}\left(
-1\right)  ^{\sum K+\sum P}\det\left(  \operatorname*{sub}\nolimits_{w\left(
K\right)  }^{w\left(  R\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \right) \nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(
R\right)  }^{w\left(  Q\right)  }B\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.3}%
\end{align}


On the other hand, for every subset $G$ of $\left\{  1,2,\ldots,n\right\}  $,
we have%
\begin{equation}
\det A=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert G\right\vert }}\left(  -1\right)  ^{\sum K+\sum
G}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
G\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{G}\right)  }A\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.6}%
\end{equation}
(Indeed, this is precisely the claim of Theorem \ref{thm.det.laplace-multi}
\textbf{(b)}, with the variables $P$ and $Q$ renamed as $K$ and $G$.) Applying
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.6}) to $G=P$, we obtain%
\begin{equation}
\det A=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
P\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.6b}%
\end{equation}


For any two objects $i$ and $j$, we define $\delta_{i,j}$ to be the element $%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$ of $\mathbb{K}$.

If $R$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
R\right\vert =k$, then%
\begin{align}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right) \nonumber\\
&  =\delta_{R,P}\det A \label{pf.prop.sol.addexe.jacobi-complement.genform.5}%
\end{align}
\footnote{\textit{Proof of
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.5}):} Let $R$ be a subset
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert R\right\vert =k$.
We must prove (\ref{pf.prop.sol.addexe.jacobi-complement.genform.5}).
\par
We have
\begin{align*}
\underbrace{\delta_{P,P}}_{\substack{=1\\\text{(since }P=P\text{)}}}\det A  &
=\det A\\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
P\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)
\end{align*}
(by (\ref{pf.prop.sol.addexe.jacobi-complement.genform.6})). In other words,
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.5}) holds if $R=P$. Hence,
for the rest of our proof of
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.5}), we can WLOG assume
that we don't have $R=P$. Assume this.
\par
We have $R\neq P$ (since we don't have $R=P$). Thus, $\delta_{R,P}=0$.
\par
We have $R\neq P$ and thus $P\neq R$. Also, $\left\vert P\right\vert
=k=\left\vert R\right\vert $.
\par
For every subset $G$ of $\left\{  1,2,\ldots,n\right\}  $ satisfying
$\left\vert G\right\vert =\left\vert R\right\vert $ and $G\neq R$, we have%
\begin{equation}
0=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert G\right\vert }}\left(  -1\right)  ^{\sum K+\sum
G}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{G}\right)  }A\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.7}%
\end{equation}
(Indeed, this is precisely the claim of Exercise \ref{exe.det.laplace-multi.0}
\textbf{(b)}, with the variables $Q$ and $P$ renamed as $G$ and $K$.) Applying
(\ref{pf.prop.sol.addexe.jacobi-complement.genform.7}) to $G=P$, we obtain%
\[
0=\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right)
\]
(since $\left\vert P\right\vert =\left\vert R\right\vert $ and $P\neq R$).
Hence,%
\begin{align*}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum K+\sum
P}\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
R\right)  }A\right)  \det\left(  \operatorname*{sub}\nolimits_{w\left(
\widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)  }A\right) \\
&  =0=\underbrace{0}_{=\delta_{R,P}}\det A=\delta_{R,P}\det A.
\end{align*}
This proves (\ref{pf.prop.sol.addexe.jacobi-complement.genform.5}).}.

Now, (\ref{pf.prop.sol.addexe.jacobi-complement.genform.3}) becomes%
\begin{align}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)  }\left(  AB\right)
\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\underbrace{\left(  \sum_{\substack{K\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert
}}\left(  -1\right)  ^{\sum K+\sum P}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  R\right)  }A\right)  \det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(
\widetilde{P}\right)  }A\right)  \right)  }_{\substack{=\delta_{R,P}\det
A\\\text{(by (\ref{pf.prop.sol.addexe.jacobi-complement.genform.5}))}%
}}\nonumber\\
&  \ \ \ \ \ \ \ \ \ \ \cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(
R\right)  }^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  .
\label{pf.prop.sol.addexe.jacobi-complement.genform.11}%
\end{align}


But $P$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ and satisfies
$\left\vert P\right\vert =k$. In other words, $P$ is a subset $R$ of $\left\{
1,2,\ldots,n\right\}  $ satisfying $\left\vert R\right\vert =k$. Thus, the sum
$\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  $ has an addend
for $R=P$. If we split off this addend from this sum, then we obtain%
\begin{align*}
&  \sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right) \\
&  =\underbrace{\delta_{P,P}}_{\substack{=1\\\text{(since }P=P\text{)}}}\det
A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }B\right)  +\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}
;\\\left\vert R\right\vert =k;\\R\neq P}}\underbrace{\delta_{R,P}%
}_{\substack{=0\\\text{(since }R\neq P\text{)}}}\det A\cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)
}B\right) \\
&  =\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)  +\underbrace{\sum_{\substack{R\subseteq
\left\{  1,2,\ldots,n\right\}  ;\\\left\vert R\right\vert =k;\\R\neq P}}0\det
A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  R\right)  }^{w\left(
Q\right)  }B\right)  }_{=0}\\
&  =\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)  .
\end{align*}
Hence, (\ref{pf.prop.sol.addexe.jacobi-complement.genform.11}) becomes%
\begin{align*}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \det\left(  \operatorname*{sub}%
\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)  }\left(  AB\right)
\right) \\
&  =\sum_{\substack{R\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
R\right\vert =k}}\delta_{R,P}\det A\cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  R\right)  }^{w\left(  Q\right)  }B\right)  =\det
A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }B\right)  .
\end{align*}
This proves Proposition \ref{prop.sol.addexe.jacobi-complement.genform}.
\end{proof}
\end{verlong}

Proposition \ref{prop.sol.addexe.jacobi-complement.genform} becomes
particularly simple when the matrix $AB$ is diagonal (i.e., has all entries
outside of its diagonal equal to $0$):

\begin{corollary}
\label{cor.sol.addexe.jacobi-complement.diagform}Let $n\in\mathbb{N}$. For
every two objects $i$ and $j$, define $\delta_{i,j}\in\mathbb{K}$ by
$\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$.

For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}%
$ denote the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $d_{1},d_{2},\ldots,d_{n}$ be $n$ elements of $\mathbb{K}$. Let $A$ and
$B$ be two $n\times n$-matrices such that $AB=\left(  d_{i}\delta
_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$.

Let $P$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Let $Q$ be a subset
of $\left\{  1,2,\ldots,n\right\}  $ such that $\left\vert P\right\vert
=\left\vert Q\right\vert $. Then,%
\[
\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)  =\left(  -1\right)  ^{\sum P+\sum Q}%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \prod_{i\in Q}d_{i}.
\]

\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.sol.addexe.jacobi-complement.diagform}.]If $K$ is
a subset of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert
K\right\vert =\left\vert P\right\vert $, then%
\begin{equation}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\delta_{K,Q}\prod_{i\in K}d_{i}
\label{pf.cor.sol.addexe.jacobi-complement.diagform.2}%
\end{equation}
\footnote{\textit{Proof of
(\ref{pf.cor.sol.addexe.jacobi-complement.diagform.2}):} Let $K$ be a subset
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert K\right\vert
=\left\vert P\right\vert $. Then, $\left\vert K\right\vert =\left\vert
P\right\vert =\left\vert Q\right\vert $. Also, $AB$ is the $n\times n$-matrix
$\left(  d_{i}\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (since
$AB=\left(  d_{i}\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$).
Hence, Lemma \ref{lem.diag.minors} (applied to $AB$ and $K$ instead of $D$ and
$P$) yields%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(
Q\right)  }\left(  AB\right)  \right)  =\delta_{K,Q}\prod_{i\in K}d_{i}.
\]
This proves (\ref{pf.cor.sol.addexe.jacobi-complement.diagform.2}).}.

Proposition \ref{prop.sol.addexe.jacobi-complement.genform} (applied to $m=n$)
yields%
\begin{align}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right) \nonumber\\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \underbrace{\det\left(
\operatorname*{sub}\nolimits_{w\left(  K\right)  }^{w\left(  Q\right)
}\left(  AB\right)  \right)  }_{\substack{=\delta_{K,Q}\prod_{i\in K}%
d_{i}\\\text{(by (\ref{pf.cor.sol.addexe.jacobi-complement.diagform.2}))}%
}}\nonumber\\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \delta_{K,Q}\prod_{i\in K}d_{i}.
\label{pf.cor.sol.addexe.jacobi-complement.diagform.1}%
\end{align}


But $Q$ is a subset of $\left\{  1,2,\ldots,n\right\}  $ and satisfies
$\left\vert Q\right\vert =\left\vert P\right\vert $ (since $\left\vert
P\right\vert =\left\vert Q\right\vert $). In other words, $Q$ is a subset $K$
of $\left\{  1,2,\ldots,n\right\}  $ satisfying $\left\vert K\right\vert
=\left\vert P\right\vert $. Thus, the sum
\[
\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \delta_{K,Q}\prod_{i\in K}d_{i}%
\]
has an addend for $K=Q$. If we split off this addend from this sum, then we
obtain%
\begin{align*}
&  \sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \delta_{K,Q}\prod_{i\in K}d_{i}\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \underbrace{\delta_{Q,Q}}_{\substack{=1\\\text{(since }Q=Q\text{)}%
}}\prod_{i\in Q}d_{i}\\
&  \ \ \ \ \ \ \ \ \ \ +\sum_{\substack{K\subseteq\left\{  1,2,\ldots
,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert ;\\K\neq
Q}}\left(  -1\right)  ^{\sum P+\sum K}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \underbrace{\delta_{K,Q}}_{\substack{=0\\\text{(since }K\neq
Q\text{)}}}\prod_{i\in K}d_{i}\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \prod_{i\in Q}d_{i}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\sum_{\substack{K\subseteq\left\{
1,2,\ldots,n\right\}  ;\\\left\vert K\right\vert =\left\vert P\right\vert
;\\K\neq Q}}\left(  -1\right)  ^{\sum P+\sum K}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{K}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  0\prod_{i\in K}d_{i}}_{=0}\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \prod_{i\in Q}d_{i}.
\end{align*}
Hence, (\ref{pf.cor.sol.addexe.jacobi-complement.diagform.1}) becomes%
\begin{align*}
&  \det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right) \\
&  =\sum_{\substack{K\subseteq\left\{  1,2,\ldots,n\right\}  ;\\\left\vert
K\right\vert =\left\vert P\right\vert }}\left(  -1\right)  ^{\sum P+\sum
K}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{K}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \delta_{K,Q}\prod_{i\in K}d_{i}\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \prod_{i\in Q}d_{i}.
\end{align*}


This proves Corollary \ref{cor.sol.addexe.jacobi-complement.diagform}.
\end{proof}

Specializing Corollary \ref{cor.sol.addexe.jacobi-complement.diagform} a bit
further, we obtain the following:

\begin{corollary}
\label{cor.sol.addexe.jacobi-complement.lidform}Let $n\in\mathbb{N}$. Let
$\lambda\in\mathbb{K}$.

For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}%
$ denote the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A$ and $B$ be two $n\times n$-matrices such that $AB=\lambda I_{n}$.

Let $P$ be a subset of $\left\{  1,2,\ldots,n\right\}  $. Let $Q$ be a subset
of $\left\{  1,2,\ldots,n\right\}  $ such that $\left\vert P\right\vert
=\left\vert Q\right\vert $. Then,%
\[
\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)  =\left(  -1\right)  ^{\sum P+\sum Q}%
\lambda^{\left\vert Q\right\vert }\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  .
\]

\end{corollary}

\begin{vershort}
\begin{proof}
[Proof of Corollary \ref{cor.sol.addexe.jacobi-complement.lidform}.]For every
two objects $i$ and $j$, define $\delta_{i,j}\in\mathbb{K}$ as in Corollary
\ref{cor.sol.addexe.jacobi-complement.diagform}. We have $I_{n}=\left(
\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$ (by the definition of
$I_{n}$). Now,%
\[
AB=\lambda\underbrace{I_{n}}_{\substack{=\left(  \delta_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}}}=\lambda\left(  \delta_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \lambda\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}.
\]
Hence, Corollary \ref{cor.sol.addexe.jacobi-complement.diagform} (applied to
$d_{k}=\lambda$) yields%
\begin{align*}
\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)   &  =\left(  -1\right)  ^{\sum P+\sum Q}%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \underbrace{\prod_{i\in Q}%
\lambda}_{=\lambda^{\left\vert Q\right\vert }}\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\lambda^{\left\vert Q\right\vert }%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  .
\end{align*}
This proves Corollary \ref{cor.sol.addexe.jacobi-complement.lidform}.
\end{proof}
\end{vershort}

\begin{verlong}
\begin{proof}
[Proof of Corollary \ref{cor.sol.addexe.jacobi-complement.lidform}.]For every
two objects $i$ and $j$, define $\delta_{i,j}\in\mathbb{K}$ by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. We have%
\[
AB=\lambda\underbrace{I_{n}}_{\substack{=\left(  \delta_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq n}\\\text{(by the definition}\\\text{of }I_{n}\text{)}%
}}=\lambda\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}=\left(  \lambda\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Hence, Corollary \ref{cor.sol.addexe.jacobi-complement.diagform} (applied to
$d_{k}=\lambda$) yields%
\begin{align*}
\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }B\right)   &  =\left(  -1\right)  ^{\sum P+\sum Q}%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  \underbrace{\prod_{i\in Q}%
\lambda}_{=\lambda^{\left\vert Q\right\vert }}\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}A\right)  \lambda^{\left\vert Q\right\vert }\\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\lambda^{\left\vert Q\right\vert }%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  .
\end{align*}
This proves Corollary \ref{cor.sol.addexe.jacobi-complement.lidform}.
\end{proof}
\end{verlong}

Finally, we obtain Additional exercise \ref{addexe.jacobi-complement} easily
by setting $\lambda=1$ in Corollary
\ref{cor.sol.addexe.jacobi-complement.lidform}:

\begin{proof}
[Second solution to Additional exercise \ref{addexe.jacobi-complement}.]The
matrix $A\in\mathbb{K}^{n\times n}$ is invertible. Hence, the matrix
$A^{-1}\in\mathbb{K}^{n\times n}$ is well-defined. Theorem \ref{thm.det(AB)}
(applied to $B=A^{-1}$) yields $\det\left(  AA^{-1}\right)  =\det A\cdot
\det\left(  A^{-1}\right)  $. Thus, $\det A\cdot\det\left(  A^{-1}\right)
=\det\left(  \underbrace{AA^{-1}}_{=I_{n}}\right)  =\det\left(  I_{n}\right)
=1$.

We have $A^{-1}A=I_{n}=1\cdot I_{n}$. Hence, Corollary
\ref{cor.sol.addexe.jacobi-complement.lidform} (applied to $A^{-1}$, $A$ and
$1$ instead of $A$, $B$ and $\lambda$) yields%
\begin{align*}
\det\left(  A^{-1}\right)  \cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)   &  =\left(
-1\right)  ^{\sum P+\sum Q}\underbrace{1^{\left\vert Q\right\vert }}_{=1}%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }\left(  A^{-1}\right)  \right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}\left(  A^{-1}\right)  \right)  .
\end{align*}
Multiplying both sides of this equality by $\det A$, we obtain%
\[
\det A\cdot\det\left(  A^{-1}\right)  \cdot\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)  }A\right)  =\det
A\cdot\left(  -1\right)  ^{\sum P+\sum Q}\det\left(  \operatorname*{sub}%
\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(  \widetilde{P}\right)
}\left(  A^{-1}\right)  \right)  .
\]
Comparing this with%
\[
\underbrace{\det A\cdot\det\left(  A^{-1}\right)  }_{=1}\cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(  Q\right)
}A\right)  =\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }A\right)  ,
\]
we obtain%
\begin{align*}
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }A\right)   &  =\det A\cdot\left(  -1\right)  ^{\sum P+\sum Q}%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }\left(  A^{-1}\right)  \right) \\
&  =\left(  -1\right)  ^{\sum P+\sum Q}\det A\cdot\det\left(
\operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)  }^{w\left(
\widetilde{P}\right)  }\left(  A^{-1}\right)  \right)  .
\end{align*}
Thus, Additional exercise \ref{addexe.jacobi-complement} is solved again.
\end{proof}

\subsubsection{Addendum}

As an easy consequence of our Second solution to Additional exercise
\ref{addexe.jacobi-complement}, we can obtain the following fact:

\begin{corollary}
\label{cor.sol.addexe.jacobi-complement.adjform}Let $n\in\mathbb{N}$. For any
subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}$ denote
the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A$ be an $n\times n$-matrix.

Let $P$ and $Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ such that
$\left\vert P\right\vert =\left\vert Q\right\vert $. Then,%
\[
\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }\left(  \operatorname*{adj}A\right)  \right)  =\left(
-1\right)  ^{\sum P+\sum Q}\left(  \det A\right)  ^{\left\vert Q\right\vert
}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  .
\]

\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.sol.addexe.jacobi-complement.adjform}.]Theorem
\ref{thm.adj.inverse} yields $A\cdot\operatorname*{adj}A=\operatorname*{adj}%
A\cdot A=\det A\cdot I_{n}$. Hence, Corollary
\ref{cor.sol.addexe.jacobi-complement.lidform} (applied to
$B=\operatorname*{adj}A$ and $\lambda=\det A$) yields%
\[
\det A\cdot\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)
}^{w\left(  Q\right)  }\left(  \operatorname*{adj}A\right)  \right)  =\left(
-1\right)  ^{\sum P+\sum Q}\left(  \det A\right)  ^{\left\vert Q\right\vert
}\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  .
\]
This proves Corollary \ref{cor.sol.addexe.jacobi-complement.adjform}.
\end{proof}

Note that we could have also deduced Corollary
\ref{cor.sol.addexe.jacobi-complement.adjform} from the First solution to
Additional exercise \ref{addexe.jacobi-complement} (but this would have been
more difficult, since we would have to slightly generalize our argument).

It is possible to strengthen Corollary
\ref{cor.sol.addexe.jacobi-complement.adjform} in the case when $\left\vert
P\right\vert =\left\vert Q\right\vert \geq1$ as follows:

\begin{corollary}
\label{cor.sol.addexe.jacobi-complement.adjform.stronger}Let $n\in\mathbb{N}$.
For any subset $I$ of $\left\{  1,2,\ldots,n\right\}  $, we let $\widetilde{I}%
$ denote the complement $\left\{  1,2,\ldots,n\right\}  \setminus I$ of $I$.

Let $A$ be an $n\times n$-matrix.

Let $P$ and $Q$ be two subsets of $\left\{  1,2,\ldots,n\right\}  $ such that
$\left\vert P\right\vert =\left\vert Q\right\vert \geq1$. Then,%
\[
\det\left(  \operatorname*{sub}\nolimits_{w\left(  P\right)  }^{w\left(
Q\right)  }\left(  \operatorname*{adj}A\right)  \right)  =\left(  -1\right)
^{\sum P+\sum Q}\left(  \det A\right)  ^{\left\vert Q\right\vert -1}%
\det\left(  \operatorname*{sub}\nolimits_{w\left(  \widetilde{Q}\right)
}^{w\left(  \widetilde{P}\right)  }A\right)  .
\]

\end{corollary}

Loosely speaking, the claim of Corollary
\ref{cor.sol.addexe.jacobi-complement.adjform.stronger} is obtained from that
of Corollary \ref{cor.sol.addexe.jacobi-complement.adjform} by cancelling
$\det A$. However, it is not immediately clear that this cancellation is
allowed (for instance, $\det A$ could be $0$, or could be a nonzero
non-cancellable element). There are ways to justify this cancellation in full
generality; however, these are not in the scope of these notes.

\subsection{Solution to Additional exercise \ref{exeadd.det.rk1upd}}

In this section, we shall use the following notation:

\begin{definition}
\label{def.exeadd.det.rk1upd.scalar}If $B$ is any $1\times1$-matrix, then
$\operatorname*{ent}B$ will denote the $\left(  1,1\right)  $-th entry of $B$.
(This entry is, of course, the only entry of $B$. Thus, the $1\times1$-matrix
$B$ satisfies $B=\left(
\begin{array}
[c]{c}%
\operatorname*{ent}B
\end{array}
\right)  $.)
\end{definition}

We can now restate Additional exercise \ref{exeadd.det.rk1upd} in a form that
uses no abuse of notation (such as identifying $1\times1$-matrices with
elements of $\mathbb{K}$):

\begin{theorem}
\label{thm.exeadd.det.rk1upd.claim}Let $n\in\mathbb{N}$. Let $u$ be a column
vector with $n$ entries, and let $v$ be a row vector with $n$ entries. (Thus,
$uv$ is an $n\times n$-matrix, whereas $vu$ is a $1\times1$-matrix.) Let $A$
be an $n\times n$-matrix. Then,%
\[
\det\left(  A+uv\right)  =\det A+\operatorname*{ent}\left(  v\left(
\operatorname*{adj}A\right)  u\right)  .
\]

\end{theorem}

Before we prove this theorem, let us make some preparations. First comes a
simple fact:

\begin{proposition}
\label{prop.exeadd.det.rk1upd.claim.ent(vBu)}Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $u=\left(  u_{1},u_{2},\ldots,u_{m}\right)  ^{T}%
\in\mathbb{K}^{m\times1}$ and $v=\left(  v_{1},v_{2},\ldots,v_{n}\right)
\in\mathbb{K}^{1\times n}$. Let $B=\left(  b_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}\in\mathbb{K}^{n\times m}$. Then,%
\[
\operatorname*{ent}\left(  vBu\right)  =\sum_{i=1}^{n}\sum_{j=1}^{m}u_{j}%
v_{i}b_{i,j}.
\]

\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.exeadd.det.rk1upd.claim.ent(vBu)}.]We have
\[
u=\left(  u_{1},u_{2},\ldots,u_{m}\right)  ^{T}=\left(
\begin{array}
[c]{c}%
u_{1}\\
u_{2}\\
\vdots\\
u_{m}%
\end{array}
\right)  =\left(  u_{i}\right)  _{1\leq i\leq m,\ 1\leq j\leq1}%
\]
and%
\[
v=\left(  v_{1},v_{2},\ldots,v_{n}\right)  =\left(  v_{j}\right)  _{1\leq
i\leq1,\ 1\leq j\leq n}.
\]
The definition of the product of two matrices yields%
\[
vB=\left(  \sum_{k=1}^{n}v_{k}b_{k,j}\right)  _{1\leq i\leq1,\ 1\leq j\leq m}%
\]
(since $v=\left(  v_{j}\right)  _{1\leq i\leq1,\ 1\leq j\leq n}$ and
$B=\left(  b_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$). Thus,%
\[
vB=\left(  \underbrace{\sum_{k=1}^{n}v_{k}b_{k,j}}_{\substack{=\sum_{p=1}%
^{n}v_{p}b_{p,j}\\\text{(here, we have renamed the}\\\text{summation index
}k\text{ as }p\text{)}}}\right)  _{1\leq i\leq1,\ 1\leq j\leq m}=\left(
\sum_{p=1}^{n}v_{p}b_{p,j}\right)  _{1\leq i\leq1,\ 1\leq j\leq m}.
\]


Now, the definition of the product of two matrices yields%
\begin{equation}
\left(  vB\right)  u=\left(  \sum_{k=1}^{m}\left(  \sum_{p=1}^{n}v_{p}%
b_{p,k}\right)  u_{k}\right)  _{1\leq i\leq1,\ 1\leq j\leq1}
\label{pf.prop.exeadd.det.rk1upd.claim.ent(vBu).1}%
\end{equation}
(since $vB=\left(  \sum_{p=1}^{n}v_{p}b_{p,j}\right)  _{1\leq i\leq1,\ 1\leq
j\leq m}$ and $u=\left(  u_{i}\right)  _{1\leq i\leq m,\ 1\leq j\leq1}$). In
particular, $\left(  vB\right)  u$ is a $1\times1$-matrix. The definition of
$\operatorname*{ent}\left(  \left(  vB\right)  u\right)  $ shows that
$\operatorname*{ent}\left(  \left(  vB\right)  u\right)  $ is the $\left(
1,1\right)  $-th entry of this matrix $\left(  vB\right)  u$. Thus,%
\begin{align*}
\operatorname*{ent}\left(  \left(  vB\right)  u\right)   &  =\left(  \text{the
}\left(  1,1\right)  \text{-th entry of the matrix }\underbrace{\left(
vB\right)  u}_{\substack{=\left(  \sum_{k=1}^{m}\left(  \sum_{p=1}^{n}%
v_{p}b_{p,k}\right)  u_{k}\right)  _{1\leq i\leq1,\ 1\leq j\leq1}\\\text{(by
(\ref{pf.prop.exeadd.det.rk1upd.claim.ent(vBu).1}))}}}\right) \\
&  =\left(  \text{the }\left(  1,1\right)  \text{-th entry of the matrix
}\left(  \sum_{k=1}^{m}\left(  \sum_{p=1}^{n}v_{p}b_{p,k}\right)
u_{k}\right)  _{1\leq i\leq1,\ 1\leq j\leq1}\right) \\
&  =\sum_{k=1}^{m}\left(  \sum_{p=1}^{n}v_{p}b_{p,k}\right)  u_{k}%
=\underbrace{\sum_{k=1}^{m}\sum_{p=1}^{n}}_{=\sum_{p=1}^{n}\sum_{k=1}^{m}%
}\underbrace{v_{p}b_{p,k}u_{k}}_{=u_{k}v_{p}b_{p,k}}\\
&  =\sum_{p=1}^{n}\sum_{k=1}^{m}u_{k}v_{p}b_{p,k}=\sum_{i=1}^{n}\sum_{k=1}%
^{m}u_{k}v_{i}b_{i,k}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}p\text{ as }i\right) \\
&  =\sum_{i=1}^{n}\sum_{j=1}^{m}u_{j}v_{i}b_{i,j}%
\end{align*}
(here, we have renamed the summation index $k$ as $j$). Since $\left(
vB\right)  u=vBu$, this rewrites as%
\[
\operatorname*{ent}\left(  vBu\right)  =\sum_{i=1}^{n}\sum_{j=1}^{m}u_{j}%
v_{i}b_{i,j}.
\]
This proves Proposition \ref{prop.exeadd.det.rk1upd.claim.ent(vBu)}.
\end{proof}

From Proposition \ref{prop.exeadd.det.rk1upd.claim.ent(vBu)}, we can easily
obtain the following:

\begin{corollary}
\label{cor.exeadd.det.rk1upd.claim.ent(v(adjA)u)}Let $n\in\mathbb{N}$. Let
$u=\left(  u_{1},u_{2},\ldots,u_{n}\right)  ^{T}\in\mathbb{K}^{n\times1}$ and
$v=\left(  v_{1},v_{2},\ldots,v_{n}\right)  \in\mathbb{K}^{1\times n}$. Let
$A$ be an $n\times n$-matrix. Then,%
\[
\operatorname*{ent}\left(  v\left(  \operatorname*{adj}A\right)  u\right)
=\sum_{i=1}^{n}\sum_{j=1}^{n}\left(  -1\right)  ^{i+j}u_{j}v_{i}\det\left(
A_{\sim j,\sim i}\right)  .
\]
(Here, we are using the notations introduced in Definition
\ref{def.submatrix.minor}.)
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.exeadd.det.rk1upd.claim.ent(v(adjA)u)}.]The
definition of $\operatorname*{adj}A$ yields%
\[
\operatorname*{adj}A=\left(  \left(  -1\right)  ^{i+j}\det\left(  A_{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
Hence, Proposition \ref{prop.exeadd.det.rk1upd.claim.ent(vBu)} (applied to
$m=n$, $B=\operatorname*{adj}A$ and $b_{i,j}=\left(  -1\right)  ^{i+j}%
\det\left(  A_{\sim j,\sim i}\right)  $) yields%
\begin{align*}
\operatorname*{ent}\left(  v\left(  \operatorname*{adj}A\right)  u\right)   &
=\sum_{i=1}^{n}\sum_{j=1}^{n}\underbrace{u_{j}v_{i}\left(  -1\right)  ^{i+j}%
}_{=\left(  -1\right)  ^{i+j}u_{j}v_{i}}\det\left(  A_{\sim j,\sim i}\right)
\\
&  =\sum_{i=1}^{n}\sum_{j=1}^{n}\left(  -1\right)  ^{i+j}u_{j}v_{i}\det\left(
A_{\sim j,\sim i}\right)  .
\end{align*}
This proves Corollary \ref{cor.exeadd.det.rk1upd.claim.ent(v(adjA)u)}.
\end{proof}

Next, we state something slightly more interesting:

\begin{proposition}
\label{prop.exeadd.det.rk1upd.claim.AB+uv}Let $n\in\mathbb{N}$ and
$m\in\mathbb{N}$. Let $A\in\mathbb{K}^{n\times m}$ and $B\in\mathbb{K}%
^{n\times m}$. Let $u\in\mathbb{K}^{n\times1}$ and $v\in\mathbb{K}^{n\times1}%
$. Then,%
\[
\left(  A\mid u\right)  \left(  B\mid v\right)  ^{T}=AB^{T}+uv^{T}.
\]
(Here, we are using the notations introduced in Definition \ref{def.addcol}.)
\end{proposition}

\begin{example}
If we set $n=2$, $m=3$, $A=\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}%
\end{array}
\right)  $, $B=\left(
\begin{array}
[c]{ccc}%
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  $, $u=\left(
\begin{array}
[c]{c}%
x\\
y
\end{array}
\right)  $ and $v=\left(
\begin{array}
[c]{c}%
z\\
w
\end{array}
\right)  $, then Proposition \ref{prop.exeadd.det.rk1upd.claim.AB+uv} says
that%
\begin{align*}
&  \left(
\begin{array}
[c]{cccc}%
a & a^{\prime} & a^{\prime\prime} & x\\
b & b^{\prime} & b^{\prime\prime} & y
\end{array}
\right)  \left(
\begin{array}
[c]{cccc}%
c & c^{\prime} & c^{\prime\prime} & z\\
d & d^{\prime} & d^{\prime\prime} & w
\end{array}
\right)  ^{T}\\
&  =\left(
\begin{array}
[c]{ccc}%
a & a^{\prime} & a^{\prime\prime}\\
b & b^{\prime} & b^{\prime\prime}%
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}%
c & c^{\prime} & c^{\prime\prime}\\
d & d^{\prime} & d^{\prime\prime}%
\end{array}
\right)  ^{T}+\left(
\begin{array}
[c]{c}%
x\\
y
\end{array}
\right)  \left(
\begin{array}
[c]{c}%
z\\
w
\end{array}
\right)  ^{T}.
\end{align*}

\end{example}

\begin{proof}
[First proof of Proposition \ref{prop.exeadd.det.rk1upd.claim.AB+uv}
(sketched).]We can apply Exercise \ref{exe.block2x2.mult} to $n$, $0$, $m$,
$1$, $n$, $0$, $A$, $u$, $0_{0\times m}$, $0_{0\times1}$, $B^{T}$,
$0_{m\times0}$, $v^{T}$ and $0_{1\times0}$ instead of $n$, $n^{\prime}$, $m$,
$m^{\prime}$, $\ell$, $\ell^{\prime}$, $A$, $B$, $C$, $D$, $A^{\prime}$,
$B^{\prime}$, $C^{\prime}$ and $D^{\prime}$. As a result, we obtain%
\[
\left(
\begin{array}
[c]{cc}%
A & u\\
0_{0\times m} & 0_{0\times1}%
\end{array}
\right)  \left(
\begin{array}
[c]{cc}%
B^{T} & 0_{m\times0}\\
v^{T} & 0_{1\times0}%
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}%
AB^{T}+uv^{T} & A0_{m\times0}+u0_{1\times0}\\
0_{0\times m}B^{T}+0_{0\times1}v^{T} & 0_{0\times m}0_{m\times0}+0_{0\times
1}0_{1\times0}%
\end{array}
\right)
\]
(where we are using the notations introduced in Definition \ref{def.block2x2}%
). In view of the equalities%
\[
\left(
\begin{array}
[c]{cc}%
A & u\\
0_{0\times m} & 0_{0\times1}%
\end{array}
\right)  =\left(  A\mid u\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{this is
rather obvious}\right)  ,
\]%
\[
\left(
\begin{array}
[c]{cc}%
B^{T} & 0_{m\times0}\\
v^{T} & 0_{1\times0}%
\end{array}
\right)  =\left(  B\mid v\right)  ^{T}\ \ \ \ \ \ \ \ \ \ \left(  \text{this
is easy to see}\right)
\]
and%
\begin{align*}
&  \left(
\begin{array}
[c]{cc}%
AB^{T}+uv^{T} & A0_{m\times0}+u0_{1\times0}\\
0_{0\times m}B^{T}+0_{0\times1}v^{T} & 0_{0\times m}0_{m\times0}+0_{0\times
1}0_{1\times0}%
\end{array}
\right) \\
&  =\left(
\begin{array}
[c]{cc}%
AB^{T}+uv^{T} & 0_{n\times0}\\
0_{0\times n} & 0_{0\times0}%
\end{array}
\right)  =AB^{T}+uv^{T},
\end{align*}
this rewrites as $\left(  A\mid u\right)  \left(  B\mid v\right)  ^{T}%
=AB^{T}+uv^{T}$. This proves Proposition
\ref{prop.exeadd.det.rk1upd.claim.AB+uv}.
\end{proof}

We shall now give another, self-contained proof of Proposition
\ref{prop.exeadd.det.rk1upd.claim.AB+uv}, based upon the following simple fact:

\begin{proposition}
\label{prop.exeadd.det.rk1upd.claim.AB}Let $n\in\mathbb{N}$ and $m\in
\mathbb{N}$. Let $A\in\mathbb{K}^{n\times m}$ and $B\in\mathbb{K}^{n\times m}%
$. Then,%
\[
AB^{T}=\sum_{k=1}^{m}A_{\bullet,k}\left(  B_{\bullet,k}\right)  ^{T}.
\]
(Here, we are using the notations introduced in Definition \ref{def.unrows}.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.exeadd.det.rk1upd.claim.AB}.]The matrix $A$ is
an $n\times m$-matrix (since $A\in\mathbb{K}^{n\times m}$). Write this
$n\times m$-matrix $A$ in the form $A=\left(  a_{i,j}\right)  _{1\leq i\leq
n,\ 1\leq j\leq m}$.

The matrix $B$ is an $n\times m$-matrix (since $B\in\mathbb{K}^{n\times m}$).
Write this $n\times m$-matrix $B$ in the form $B=\left(  b_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}$. From $B=\left(  b_{i,j}\right)  _{1\leq
i\leq n,\ 1\leq j\leq m}$, we obtain $B^{T}=\left(  b_{j,i}\right)  _{1\leq
i\leq m,\ 1\leq j\leq n}$ (by the definition of $B^{T}$).

The definition of the product of two matrices yields%
\begin{equation}
AB^{T}=\left(  \sum_{k=1}^{m}a_{i,k}b_{j,k}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n} \label{pf.prop.exeadd.det.rk1upd.claim.AB.0}%
\end{equation}
(since $A=\left(  a_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq m}$ and
$B^{T}=\left(  b_{j,i}\right)  _{1\leq i\leq m,\ 1\leq j\leq n}$).

Let us now fix $p\in\left\{  1,2,\ldots,m\right\}  $. Then, $A_{\bullet,p}$ is
the $p$-th column of the matrix $A$ (by the definition of $A_{\bullet,p}$).
Thus,%
\begin{align*}
A_{\bullet,p}  &  =\left(  \text{the }p\text{-th column of the matrix
}A\right) \\
&  =\left(
\begin{array}
[c]{c}%
a_{1,p}\\
a_{2,p}\\
\vdots\\
a_{n,p}%
\end{array}
\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{since }A=\left(  a_{i,j}\right)
_{1\leq i\leq n,\ 1\leq j\leq m}\right) \\
&  =\left(  a_{i,p}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}.
\end{align*}
The same argument (applied to $B$ and $b_{i,j}$ instead of $A$ and $a_{i,j}$)
yields $B_{\bullet,p}=\left(  b_{i,p}\right)  _{1\leq i\leq n,\ 1\leq j\leq1}%
$. Hence, $\left(  B_{\bullet,p}\right)  ^{T}=\left(  b_{j,p}\right)  _{1\leq
i\leq1,\ 1\leq j\leq n}$ (by the definition of $\left(  B_{\bullet,p}\right)
^{T}$).

Now, the definition of the product of two matrices yields%
\begin{align}
A_{\bullet,p}\left(  B_{\bullet,p}\right)  ^{T}  &  =\left(  \underbrace{\sum
_{k=1}^{1}a_{i,p}b_{j,p}}_{=a_{i,p}b_{j,p}}\right)  _{1\leq i\leq n,\ 1\leq
j\leq n}\ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{since }A_{\bullet,p}=\left(  a_{i,p}\right)  _{1\leq i\leq n,\ 1\leq
j\leq1}\\
\text{and }\left(  B_{\bullet,p}\right)  ^{T}=\left(  b_{j,p}\right)  _{1\leq
i\leq1,\ 1\leq j\leq n}%
\end{array}
\right) \nonumber\\
&  =\left(  a_{i,p}b_{j,p}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\label{pf.prop.exeadd.det.rk1upd.claim.AB.1}%
\end{align}


Now, let us forget that we fixed $p$. We thus have proven
(\ref{pf.prop.exeadd.det.rk1upd.claim.AB.1}) for each $p\in\left\{
1,2,\ldots,m\right\}  $. Now,%
\[
\sum_{k=1}^{m}\underbrace{A_{\bullet,k}\left(  B_{\bullet,k}\right)  ^{T}%
}_{\substack{=\left(  a_{i,k}b_{j,k}\right)  _{1\leq i\leq n,\ 1\leq j\leq
n}\\\text{(by (\ref{pf.prop.exeadd.det.rk1upd.claim.AB.1}), applied to
}p=k\text{)}}}=\sum_{k=1}^{m}\left(  a_{i,k}b_{j,k}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}=\left(  \sum_{k=1}^{m}a_{i,k}b_{j,k}\right)  _{1\leq i\leq
n,\ 1\leq j\leq n}.
\]
Comparing this with (\ref{pf.prop.exeadd.det.rk1upd.claim.AB.0}), we obtain
$AB^{T}=\sum_{k=1}^{m}A_{\bullet,k}\left(  B_{\bullet,k}\right)  ^{T}$. This
proves Proposition \ref{prop.exeadd.det.rk1upd.claim.AB}.
\end{proof}

\begin{proof}
[Second proof of Proposition \ref{prop.exeadd.det.rk1upd.claim.AB+uv}.]We have
$\left(  A\mid u\right)  \in\mathbb{K}^{n\times\left(  m+1\right)  }$ (since
$A\in\mathbb{K}^{n\times m}$ and $u\in\mathbb{K}^{n\times1}$) and $\left(
B\mid v\right)  \in\mathbb{K}^{n\times\left(  m+1\right)  }$ (since
$B\in\mathbb{K}^{n\times m}$ and $v\in\mathbb{K}^{n\times1}$). Thus,
Proposition \ref{prop.exeadd.det.rk1upd.claim.AB} (applied to $m+1$, $\left(
A\mid u\right)  $ and $\left(  B\mid v\right)  $ instead of $m$, $A$ and $B$)
yields%
\begin{align*}
&  \left(  A\mid u\right)  \left(  B\mid v\right)  ^{T}\\
&  =\sum_{k=1}^{m+1}\left(  A\mid u\right)  _{\bullet,k}\left(  \left(  B\mid
v\right)  _{\bullet,k}\right)  ^{T}\\
&  =\sum_{k=1}^{m}\underbrace{\left(  A\mid u\right)  _{\bullet,k}%
}_{\substack{=A_{\bullet,k}\\\text{(by Proposition \ref{prop.addcol.props1}
\textbf{(a)},}\\\text{applied to }u\text{ and }k\text{ instead of }v\text{ and
}q\text{)}}}\left(  \underbrace{\left(  B\mid v\right)  _{\bullet,k}%
}_{\substack{=B_{\bullet,k}\\\text{(by Proposition \ref{prop.addcol.props1}
\textbf{(a)},}\\\text{applied to }B\text{ and }k\text{ instead of }A\text{ and
}q\text{)}}}\right)  ^{T}\\
&  \ \ \ \ \ \ \ \ \ \ +\underbrace{\left(  A\mid u\right)  _{\bullet,m+1}%
}_{\substack{=u\\\text{(by Proposition \ref{prop.addcol.props1} \textbf{(b)}%
,}\\\text{applied to }u\text{ instead of }v\text{)}}}\left(
\underbrace{\left(  B\mid v\right)  _{\bullet,m+1}}_{\substack{=v\\\text{(by
Proposition \ref{prop.addcol.props1} \textbf{(b)},}\\\text{applied to }B\text{
instead of }A\text{)}}}\right)  ^{T}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=m+1\text{ from the sum}\right) \\
&  =\sum_{k=1}^{m}A_{\bullet,k}\left(  B_{\bullet,k}\right)  ^{T}+uv^{T}.
\end{align*}
Comparing this with%
\[
\underbrace{AB^{T}}_{\substack{=\sum_{k=1}^{m}A_{\bullet,k}\left(
B_{\bullet,k}\right)  ^{T}\\\text{(by Proposition
\ref{prop.exeadd.det.rk1upd.claim.AB})}}}+uv^{T}=\sum_{k=1}^{m}A_{\bullet
,k}\left(  B_{\bullet,k}\right)  ^{T}+uv^{T},
\]
we obtain $\left(  A\mid u\right)  \left(  B\mid v\right)  ^{T}=AB^{T}+uv^{T}%
$. Thus, Proposition \ref{prop.exeadd.det.rk1upd.claim.AB+uv} is proven again.
\end{proof}

\begin{corollary}
\label{cor.exeadd.det.rk1upd.claim.A+uv}Let $n\in\mathbb{N}$. Let
$u\in\mathbb{K}^{1\times n}$ and $v\in\mathbb{K}^{n\times1}$. Let $A$ be an
$n\times n$-matrix. Then,%
\[
A+uv=\left(  A\mid u\right)  \left(  I_{n}\mid v^{T}\right)  ^{T}.
\]
(Here, we are using the notations introduced in Definition \ref{def.addcol}.)
\end{corollary}

\begin{proof}
[Proof of Corollary \ref{cor.exeadd.det.rk1upd.claim.A+uv}.]We have
$v\in\mathbb{K}^{n\times1}$ and thus $v^{T}\in\mathbb{K}^{1\times n}$. Also,
$I_{n}\in\mathbb{K}^{n\times n}$. Thus, Proposition
\ref{prop.exeadd.det.rk1upd.claim.AB+uv} (applied to $n$, $I_{n}$ and $v^{T}$
instead of $m$, $B$ and $v$) yields%
\[
\left(  A\mid u\right)  \left(  I_{n}\mid v^{T}\right)  ^{T}%
=A\underbrace{\left(  I_{n}\right)  ^{T}}_{=I_{n}}+u\underbrace{\left(
v^{T}\right)  ^{T}}_{\substack{=v\\\text{(since }\left(  C^{T}\right)
^{T}=C\text{ for any}\\\text{matrix }C\text{)}}}=\underbrace{AI_{n}}%
_{=A}+uv=A+uv.
\]
This proves Corollary \ref{cor.exeadd.det.rk1upd.claim.A+uv}.
\end{proof}

Next, we state some simple facts about determinants:

\begin{lemma}
\label{lem.exeadd.det.rk1upd.adjI}Let $n\in\mathbb{N}$. Then:

\textbf{(a)} Every $u\in\left\{  1,2,\ldots,n\right\}  $ satisfies
$\det\left(  \left(  I_{n}\right)  _{\sim u,\sim u}\right)  =1$.

\textbf{(b)} If $u$ and $v$ are two elements of $\left\{  1,2,\ldots
,n\right\}  $ such that $u\neq v$, then $\det\left(  \left(  I_{n}\right)
_{\sim u,\sim v}\right)  =0$.
\end{lemma}

\begin{proof}
[Proof of Lemma \ref{lem.exeadd.det.rk1upd.adjI}.]There are myriad ways to
prove Lemma \ref{lem.exeadd.det.rk1upd.adjI} (for example, one can prove part
\textbf{(a)} by showing that $\left(  I_{n}\right)  _{\sim u,\sim u}=I_{n-1}$,
and prove part \textbf{(b)} by arguing that the matrix $\left(  I_{n}\right)
_{\sim u,\sim v}$ has a row consisting of zeroes\footnote{or, what also
suffices, a column consisting of zeroes}). The proof we will now give is not
the simplest one, but the shortest one (using what we have proven so far):

For any two objects $i$ and $j$, define an element $\delta_{i,j}\in\mathbb{K}$
by $\delta_{i,j}=%
\begin{cases}
1, & \text{if }i=j;\\
0, & \text{if }i\neq j
\end{cases}
$. Then, $I_{n}=\left(  \delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}$
(by the definition of $I_{n}$).

Now, Theorem \ref{thm.adj.inverse} (applied to $A=I_{n}$) yields $I_{n}%
\cdot\operatorname*{adj}\left(  I_{n}\right)  =\operatorname*{adj}\left(
I_{n}\right)  \cdot I_{n}=\det\left(  I_{n}\right)  \cdot I_{n}$. Thus,
$\operatorname*{adj}\left(  I_{n}\right)  \cdot I_{n}=\underbrace{\det\left(
I_{n}\right)  }_{=1}\cdot I_{n}=I_{n}$, so that $I_{n}=\operatorname*{adj}%
\left(  I_{n}\right)  \cdot I_{n}=\operatorname*{adj}\left(  I_{n}\right)  $
(since $BI_{n}=I_{n}$ for any $m\in\mathbb{N}$ and any $m\times n$-matrix
$B$). Thus,%
\[
I_{n}=\operatorname*{adj}\left(  I_{n}\right)  =\left(  \left(  -1\right)
^{i+j}\det\left(  \left(  I_{n}\right)  _{\sim j,\sim i}\right)  \right)
_{1\leq i\leq n,\ 1\leq j\leq n}%
\]
(by the definition of $\operatorname*{adj}\left(  I_{n}\right)  $). Hence,%
\[
\left(  \left(  -1\right)  ^{i+j}\det\left(  \left(  I_{n}\right)  _{\sim
j,\sim i}\right)  \right)  _{1\leq i\leq n,\ 1\leq j\leq n}=I_{n}=\left(
\delta_{i,j}\right)  _{1\leq i\leq n,\ 1\leq j\leq n}.
\]
In other words,%
\begin{equation}
\left(  -1\right)  ^{i+j}\det\left(  \left(  I_{n}\right)  _{\sim j,\sim
i}\right)  =\delta_{i,j} \label{pf.lem.exeadd.det.rk1upd.adjI.1}%
\end{equation}
for every $i\in\left\{  1,2,\ldots,n\right\}  $ and $j\in\left\{
1,2,\ldots,n\right\}  $.

\textbf{(a)} Let $u\in\left\{  1,2,\ldots,n\right\}  $. Then,
(\ref{pf.lem.exeadd.det.rk1upd.adjI.1}) (applied to $i=u$ and $j=u$) yields%
\[
\left(  -1\right)  ^{u+u}\det\left(  \left(  I_{n}\right)  _{\sim u,\sim
u}\right)  =\delta_{u,u}=1\ \ \ \ \ \ \ \ \ \ \left(  \text{since }u=u\right)
.
\]
Comparing this with $\underbrace{\left(  -1\right)  ^{u+u}}%
_{\substack{=1\\\text{(since }u+u=2u\text{ is even)}}}\det\left(  \left(
I_{n}\right)  _{\sim u,\sim u}\right)  =\det\left(  \left(  I_{n}\right)
_{\sim u,\sim u}\right)  $, we obtain $\det\left(  \left(  I_{n}\right)
_{\sim u,\sim u}\right)  =1$. This proves Lemma
\ref{lem.exeadd.det.rk1upd.adjI} \textbf{(a)}.

\textbf{(b)} Let $u$ and $v$ be two elements of $\left\{  1,2,\ldots
,n\right\}  $ such that $u\neq v$. Then,
(\ref{pf.lem.exeadd.det.rk1upd.adjI.1}) (applied to $i=v$ and $j=u$) yields%
\[
\left(  -1\right)  ^{v+u}\det\left(  \left(  I_{n}\right)  _{\sim u,\sim
v}\right)  =\delta_{v,u}=0\ \ \ \ \ \ \ \ \ \ \left(  \text{since }v\neq
u\text{ (since }u\neq v\text{)}\right)  .
\]
Multiplying both sides of this equality by $\left(  -1\right)  ^{v+u}$, we
obtain%
\[
\left(  -1\right)  ^{v+u}\left(  -1\right)  ^{v+u}\det\left(  \left(
I_{n}\right)  _{\sim u,\sim v}\right)  =0.
\]
Comparing this with
\[
\underbrace{\left(  -1\right)  ^{v+u}\left(  -1\right)  ^{v+u}}%
_{\substack{=\left(  -1\right)  ^{\left(  v+u\right)  +\left(  v+u\right)
}=1\\\text{(since }\left(  v+u\right)  +\left(  v+u\right)  =2\left(
v+u\right)  \text{ is even)}}}\det\left(  \left(  I_{n}\right)  _{\sim u,\sim
v}\right)  =\det\left(  \left(  I_{n}\right)  _{\sim u,\sim v}\right)  ,
\]
we obtain $\det\left(  \left(  I_{n}\right)  _{\sim u,\sim v}\right)  =0$.
Lemma \ref{lem.exeadd.det.rk1upd.adjI} \textbf{(b)} is thus proven.
\end{proof}

\begin{proposition}
\label{prop.exeadd.det.rk1upd.Iv}Let $n\in\mathbb{N}$. Let $v=\left(
v_{1},v_{2},\ldots,v_{n}\right)  ^{T}\in\mathbb{K}^{n\times1}$ and
$k\in\left\{  1,2,\ldots,n\right\}  $. Then,%
\[
\det\left(  \left(  I_{n}\mid v\right)  _{\bullet,\sim k}\right)  =\left(
-1\right)  ^{n+k}v_{k}.
\]
(Here, we are using the notations introduced in Definition \ref{def.unrows}.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.exeadd.det.rk1upd.Iv}.]We have $I_{n}%
\in\mathbb{K}^{n\times n}$ and thus $\left(  I_{n}\right)  _{\bullet,\sim
k}\in\mathbb{K}^{n\times\left(  n-1\right)  }$. Every $i\in\left\{
1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\left(  \left(  I_{n}\right)  _{\bullet,\sim k}\right)  _{\sim i,\bullet
}=\left(  I_{n}\right)  _{\sim i,\sim k}
\label{pf.prop.exeadd.det.rk1upd.Iv.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.exeadd.det.rk1upd.Iv.1}):} Let
$i\in\left\{  1,2,\ldots,n\right\}  $. Proposition \ref{prop.unrows.basics}
\textbf{(c)} (applied to $n$, $I_{n}$, $i$ and $k$ instead of $m$, $A$, $u$
and $v$) shows that $\left(  \left(  I_{n}\right)  _{\bullet,\sim k}\right)
_{\sim i,\bullet}=\left(  \left(  I_{n}\right)  _{\sim i,\bullet}\right)
_{\bullet,\sim k}=\left(  I_{n}\right)  _{\sim i,\sim k}$. This proves
(\ref{pf.prop.exeadd.det.rk1upd.Iv.1}).}.

Proposition \ref{prop.addcol.props1} \textbf{(d)} (applied to $m=n$, $A=I_{n}$
and $q=k$) yields $\left(  I_{n}\mid v\right)  _{\bullet,\sim k}=\left(
\left(  I_{n}\right)  _{\bullet,\sim k}\mid v\right)  $. Hence,%
\begin{align*}
&  \det\underbrace{\left(  \left(  I_{n}\mid v\right)  _{\bullet,\sim
k}\right)  }_{=\left(  \left(  I_{n}\right)  _{\bullet,\sim k}\mid v\right)
}\\
&  =\det\left(  \left(  I_{n}\right)  _{\bullet,\sim k}\mid v\right) \\
&  =\underbrace{\sum_{i=1}^{n}}_{=\sum_{i\in\left\{  1,2,\ldots,n\right\}  }%
}\left(  -1\right)  ^{n+i}v_{i}\det\left(  \underbrace{\left(  \left(
I_{n}\right)  _{\bullet,\sim k}\right)  _{\sim i,\bullet}}_{\substack{=\left(
I_{n}\right)  _{\sim i,\sim k}\\\text{(by
(\ref{pf.prop.exeadd.det.rk1upd.Iv.1}))}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Proposition \ref{prop.addcol.props2}
\textbf{(a)}, applied to }A=\left(  I_{n}\right)  _{\bullet,\sim k}\right) \\
&  =\sum_{i\in\left\{  1,2,\ldots,n\right\}  }\left(  -1\right)  ^{n+i}%
v_{i}\det\left(  \left(  I_{n}\right)  _{\sim i,\sim k}\right) \\
&  =\left(  -1\right)  ^{n+k}v_{k}\underbrace{\det\left(  \left(
I_{n}\right)  _{\sim k,\sim k}\right)  }_{\substack{=1\\\text{(by Lemma
\ref{lem.exeadd.det.rk1upd.adjI} \textbf{(a)},}\\\text{applied to }k\text{
instead of }u\text{)}}}+\sum_{\substack{i\in\left\{  1,2,\ldots,n\right\}
;\\i\neq k}}\left(  -1\right)  ^{n+i}v_{i}\underbrace{\det\left(  \left(
I_{n}\right)  _{\sim i,\sim k}\right)  }_{\substack{=0\\\text{(by Lemma
\ref{lem.exeadd.det.rk1upd.adjI} \textbf{(b)},}\\\text{applied to }i\text{ and
}k\\\text{instead of }u\text{ and }v\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}i=k\text{ from the sum}\right) \\
&  =\left(  -1\right)  ^{n+k}v_{k}+\underbrace{\sum_{\substack{i\in\left\{
1,2,\ldots,n\right\}  ;\\i\neq k}}\left(  -1\right)  ^{n+i}v_{i}0}%
_{=0}=\left(  -1\right)  ^{n+k}v_{k}.
\end{align*}
This proves Proposition \ref{prop.exeadd.det.rk1upd.Iv}.
\end{proof}

\begin{proposition}
\label{prop.exeadd.det.rk1upd.det(ABT)}Let $n\in\mathbb{N}$. Let
$A\in\mathbb{K}^{n\times\left(  n+1\right)  }$ and $B\in\mathbb{K}%
^{n\times\left(  n+1\right)  }$. Then,%
\[
\det\left(  AB^{T}\right)  =\sum_{k=1}^{n+1}\det\left(  A_{\bullet,\sim
k}\right)  \det\left(  B_{\bullet,\sim k}\right)  .
\]
(Here, we are using the notations introduced in Definition \ref{def.unrows}.)
\end{proposition}

\begin{proof}
[Proof of Proposition \ref{prop.exeadd.det.rk1upd.det(ABT)}.]Let us use the
notations introduced in Definition \ref{def.rowscols}. Clearly, $n+1$ is a
positive integer (since $n\in\mathbb{N}$).

Also, $A$ is an $n\times\left(  n+1\right)  $-matrix (since $A\in
\mathbb{K}^{n\times\left(  n+1\right)  }$). In other words, $A$ is an $\left(
\left(  n+1\right)  -1\right)  \times\left(  n+1\right)  $-matrix (since
$n=\left(  n+1\right)  -1$).

Also, $B$ is an $n\times\left(  n+1\right)  $-matrix (since $B\in
\mathbb{K}^{n\times\left(  n+1\right)  }$). Hence, $B^{T}$ is an $\left(
n+1\right)  \times n$-matrix. In other words, $B^{T}$ is an $\left(
n+1\right)  \times\left(  \left(  n+1\right)  -1\right)  $-matrix (since
$n=\left(  n+1\right)  -1$).

Now, every $k\in\left\{  1,2,\ldots,n+1\right\}  $ satisfies%
\begin{equation}
\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n+1}%
A=A_{\bullet,\sim k} \label{pf.prop.exeadd.det.rk1upd.det(ABT).1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.exeadd.det.rk1upd.det(ABT).1}):} Let
$k\in\left\{  1,2,\ldots,n+1\right\}  $. Then, the definition of
$A_{\bullet,\sim k}$ yields $A_{\bullet,\sim k}=\operatorname*{cols}%
\nolimits_{1,2,\ldots,\widehat{k},\ldots,n+1}A$ (since $A\in\mathbb{K}%
^{n\times\left(  n+1\right)  }$). This proves
(\ref{pf.prop.exeadd.det.rk1upd.det(ABT).1}).} and%
\begin{equation}
\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n+1}\left(
B^{T}\right)  =\left(  B_{\bullet,\sim k}\right)  ^{T}
\label{pf.prop.exeadd.det.rk1upd.det(ABT).2}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.exeadd.det.rk1upd.det(ABT).2}):} Let
$k\in\left\{  1,2,\ldots,n+1\right\}  $. Then, the definition of $\left(
B^{T}\right)  _{\sim k,\bullet}$ yields $\left(  B^{T}\right)  _{\sim
k,\bullet}=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n+1}\left(  B^{T}\right)  $ (since $B^{T}\in\mathbb{K}^{\left(  n+1\right)
\times n}$ (since $B^{T}$ is an $\left(  n+1\right)  \times n$-matrix)). But
Lemma \ref{lem.unrows.transpose.1} (applied to $m=n+1$ and $r=k$) yields
$\left(  B^{T}\right)  _{\sim k,\bullet}=\left(  B_{\bullet,\sim k}\right)
^{T}$. Comparing this with $\left(  B^{T}\right)  _{\sim k,\bullet
}=\operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots,n+1}\left(
B^{T}\right)  $, we obtain $\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n+1}\left(  B^{T}\right)  =\left(  B_{\bullet,\sim
k}\right)  ^{T}$. This proves (\ref{pf.prop.exeadd.det.rk1upd.det(ABT).2}).}
and%
\begin{equation}
\det\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n+1}\left(  B^{T}\right)  \right)  =\det\left(  B_{\bullet,\sim k}\right)
\label{pf.prop.exeadd.det.rk1upd.det(ABT).3}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.prop.exeadd.det.rk1upd.det(ABT).3}):} Let
$k\in\left\{  1,2,\ldots,n+1\right\}  $. From $B\in\mathbb{K}^{n\times\left(
n+1\right)  }$, we obtain $B_{\bullet,\sim k}\in\mathbb{K}^{n\times n}$. In
other words, $B_{\bullet,\sim k}$ is an $n\times n$-matrix. Hence, Exercise
\ref{exe.ps4.4} (applied to $B_{\bullet,\sim k}$ instead of $A$) yields
$\det\left(  \left(  B_{\bullet,\sim k}\right)  ^{T}\right)  =\det\left(
B_{\bullet,\sim k}\right)  $. Now,%
\[
\det\left(  \underbrace{\operatorname*{rows}\nolimits_{1,2,\ldots
,\widehat{k},\ldots,n+1}\left(  B^{T}\right)  }_{\substack{=\left(
B_{\bullet,\sim k}\right)  ^{T}\\\text{(by
(\ref{pf.prop.exeadd.det.rk1upd.det(ABT).2}))}}}\right)  =\det\left(  \left(
B_{\bullet,\sim k}\right)  ^{T}\right)  =\det\left(  B_{\bullet,\sim
k}\right)  .
\]
This proves (\ref{pf.prop.exeadd.det.rk1upd.det(ABT).3}).}.

Now, Lemma \ref{lem.adj(AB).cauchy-binet} (applied to $n+1$ and $B^{T}$
instead of $n$ and $B$) yields%
\begin{align*}
\det\left(  AB^{T}\right)   &  =\sum_{k=1}^{n+1}\det\left(
\underbrace{\operatorname*{cols}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n+1}A}_{\substack{=A_{\bullet,\sim k}\\\text{(by
(\ref{pf.prop.exeadd.det.rk1upd.det(ABT).1}))}}}\right)  \cdot\underbrace{\det
\left(  \operatorname*{rows}\nolimits_{1,2,\ldots,\widehat{k},\ldots
,n+1}\left(  B^{T}\right)  \right)  }_{\substack{=\det\left(  B_{\bullet,\sim
k}\right)  \\\text{(by (\ref{pf.prop.exeadd.det.rk1upd.det(ABT).3}))}}}\\
&  =\sum_{k=1}^{n+1}\det\left(  A_{\bullet,\sim k}\right)  \det\left(
B_{\bullet,\sim k}\right)  .
\end{align*}
This proves Proposition \ref{prop.exeadd.det.rk1upd.det(ABT)}.
\end{proof}

Now we are more than ready to easily prove Theorem
\ref{thm.exeadd.det.rk1upd.claim}:

\begin{proof}
[Proof of Theorem \ref{thm.exeadd.det.rk1upd.claim}.]We shall use the
notations introduced in Definition \ref{def.submatrix.minor}, in Definition
\ref{def.addcol} and in Definition \ref{def.unrows}.

We know that $u$ is a column vector with $n$ entries; in other words,
$u\in\mathbb{K}^{n\times1}$. Also, $v$ is a row vector with $n$ entries; in
other words, $v\in\mathbb{K}^{1\times n}$. Hence, $v^{T}\in\mathbb{K}%
^{n\times1}$.

From $A\in\mathbb{K}^{n\times n}$ and $u\in\mathbb{K}^{n\times1}$, we obtain
$\left(  A\mid u\right)  \in\mathbb{K}^{n\times\left(  n+1\right)  }$. From
$I_{n}\in\mathbb{K}^{n\times n}$ and $v^{T}\in\mathbb{K}^{n\times1}$, we
obtain $\left(  I_{n}\mid v^{T}\right)  \in\mathbb{K}^{n\times\left(
n+1\right)  }$.

Write the vector $v$ in the form $v=\left(  v_{1},v_{2},\ldots,v_{n}\right)
$. (This is possible, since $v$ is a row vector with $n$ entries.) From
$v=\left(  v_{1},v_{2},\ldots,v_{n}\right)  $, we obtain $v^{T}=\left(
v_{1},v_{2},\ldots,v_{n}\right)  ^{T}$.

Write the vector $u$ in the form $u=\left(  u_{1},u_{2},\ldots,u_{n}\right)
^{T}$. (This is possible, since $u$ is a column vector with $n$ entries.)
Then, every $k\in\left\{  1,2,\ldots,n\right\}  $ satisfies%
\begin{equation}
\det\left(  \left(  A\mid u\right)  _{\bullet,\sim k}\right)  =\sum_{j=1}%
^{n}\left(  -1\right)  ^{n+j}u_{j}\det\left(  A_{\sim j,\sim k}\right)
\label{pf.thm.exeadd.det.rk1upd.claim.1}%
\end{equation}
\footnote{\textit{Proof of (\ref{pf.thm.exeadd.det.rk1upd.claim.1}):} Let
$k\in\left\{  1,2,\ldots,n\right\}  $. Then, $A_{\bullet,\sim k}\in
\mathbb{K}^{n\times\left(  n-1\right)  }$ (since $A\in\mathbb{K}^{n\times n}%
$). Proposition \ref{prop.addcol.props1} \textbf{(c)} (applied to $n$, $u$ and
$k$ instead of $m$, $v$ and $q$) shows that $\left(  A\mid u\right)
_{\bullet,\sim k}=\left(  A_{\bullet,\sim k}\mid u\right)  $. Hence,%
\begin{align*}
\det\left(  \underbrace{\left(  A\mid u\right)  _{\bullet,\sim k}}_{=\left(
A_{\bullet,\sim k}\mid u\right)  }\right)   &  =\det\left(  A_{\bullet,\sim
k}\mid u\right) \\
&  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}u_{i}\det\left(
\underbrace{\left(  A_{\bullet,\sim k}\right)  _{\sim i,\bullet}%
}_{\substack{=A_{\sim i,\sim k}\\\text{(since Proposition
\ref{prop.unrows.basics} \textbf{(c)} (applied to}\\n\text{, }i\text{ and
}k\text{ instead of }m\text{, }u\text{ and }v\text{) shows that}\\\left(
A_{\bullet,\sim k}\right)  _{\sim i,\bullet}=\left(  A_{\sim i,\bullet
}\right)  _{\bullet,\sim k}=A_{\sim i,\sim k}\text{)}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.addcol.props2} \textbf{(a)}, applied to}\\
A_{\bullet,\sim k}\text{, }u\text{ and }u_{i}\text{ instead of }A\text{,
}v\text{ and }v_{i}%
\end{array}
\right) \\
&  =\sum_{i=1}^{n}\left(  -1\right)  ^{n+i}u_{i}\det\left(  A_{\sim i,\sim
k}\right)  =\sum_{j=1}^{n}\left(  -1\right)  ^{n+j}u_{j}\det\left(  A_{\sim
j,\sim k}\right)
\end{align*}
(here, we have renamed the summation index $i$ as $j$). This proves
(\ref{pf.thm.exeadd.det.rk1upd.claim.1}).}.

Now,%
\begin{align*}
&  \det\left(  \underbrace{A+uv}_{\substack{=\left(  A\mid u\right)  \left(
I_{n}\mid v^{T}\right)  ^{T}\\\text{(by Corollary
\ref{cor.exeadd.det.rk1upd.claim.A+uv})}}}\right) \\
&  =\det\left(  \left(  A\mid u\right)  \left(  I_{n}\mid v^{T}\right)
^{T}\right)  =\sum_{k=1}^{n+1}\det\left(  \left(  A\mid u\right)
_{\bullet,\sim k}\right)  \det\left(  \left(  I_{n}\mid v^{T}\right)
_{\bullet,\sim k}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(
\begin{array}
[c]{c}%
\text{by Proposition \ref{prop.exeadd.det.rk1upd.det(ABT)}, applied to
}\left(  A\mid u\right)  \text{ and }\left(  I_{n}\mid v^{T}\right) \\
\text{instead of }A\text{ and }B
\end{array}
\right) \\
&  =\sum_{k=1}^{n}\underbrace{\det\left(  \left(  A\mid u\right)
_{\bullet,\sim k}\right)  }_{\substack{=\sum_{j=1}^{n}\left(  -1\right)
^{n+j}u_{j}\det\left(  A_{\sim j,\sim k}\right)  \\\text{(by
(\ref{pf.thm.exeadd.det.rk1upd.claim.1}))}}}\underbrace{\det\left(  \left(
I_{n}\mid v^{T}\right)  _{\bullet,\sim k}\right)  }_{\substack{=\left(
-1\right)  ^{n+k}v_{k}\\\text{(by Proposition \ref{prop.exeadd.det.rk1upd.Iv}%
,}\\\text{applied to }v^{T}\text{ instead of }v\text{)}}}\\
&  \ \ \ \ \ \ \ \ \ \ +\det\left(  \underbrace{\left(  A\mid u\right)
_{\bullet,\sim\left(  n+1\right)  }}_{\substack{=A\\\text{(by Proposition
\ref{prop.addcol.props1} \textbf{(d)}, applied}\\\text{to }n\text{ and
}u\text{ instead of }m\text{ and }v\text{)}}}\right)  \det\left(
\underbrace{\left(  I_{n}\mid v^{T}\right)  _{\bullet,\sim\left(  n+1\right)
}}_{\substack{=I_{n}\\\text{(by Proposition \ref{prop.addcol.props1}
\textbf{(d)}, applied}\\\text{to }n\text{, }I_{n}\text{ and }v^{T}\text{
instead of }m\text{, }A\text{ and }v\text{)}}}\right) \\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have split off the addend for
}k=n+1\text{ from the sum}\right) \\
&  =\underbrace{\sum_{k=1}^{n}\left(  \sum_{j=1}^{n}\left(  -1\right)
^{n+j}u_{j}\det\left(  A_{\sim j,\sim k}\right)  \right)  \cdot\left(
-1\right)  ^{n+k}v_{k}}_{=\sum_{k=1}^{n}\sum_{j=1}^{n}\left(  -1\right)
^{n+j}u_{j}\det\left(  A_{\sim j,\sim k}\right)  \cdot\left(  -1\right)
^{n+k}v_{k}}+\det A\cdot\underbrace{\det\left(  I_{n}\right)  }_{=1}\\
&  =\sum_{k=1}^{n}\sum_{j=1}^{n}\left(  -1\right)  ^{n+j}u_{j}\det\left(
A_{\sim j,\sim k}\right)  \cdot\left(  -1\right)  ^{n+k}v_{k}+\det A.
\end{align*}
Subtracting $\det A$ from both sides of this equality, we obtain%
\begin{align*}
&  \det\left(  A+uv\right)  -\det A\\
&  =\sum_{k=1}^{n}\sum_{j=1}^{n}\left(  -1\right)  ^{n+j}u_{j}\det\left(
A_{\sim j,\sim k}\right)  \cdot\left(  -1\right)  ^{n+k}v_{k}\\
&  =\sum_{i=1}^{n}\sum_{j=1}^{n}\underbrace{\left(  -1\right)  ^{n+j}u_{j}%
\det\left(  A_{\sim j,\sim i}\right)  \cdot\left(  -1\right)  ^{n+i}v_{i}%
}_{=\left(  -1\right)  ^{n+i}\left(  -1\right)  ^{n+j}u_{j}v_{i}\det\left(
A_{\sim j,\sim i}\right)  }\\
&  \ \ \ \ \ \ \ \ \ \ \left(  \text{here, we have renamed the summation index
}k\text{ as }i\text{ in the first sum}\right) \\
&  =\sum_{i=1}^{n}\sum_{j=1}^{n}\underbrace{\left(  -1\right)  ^{n+i}\left(
-1\right)  ^{n+j}}_{\substack{=\left(  -1\right)  ^{\left(  n+i\right)
+\left(  n+j\right)  }=\left(  -1\right)  ^{i+j}\\\text{(since }\left(
n+i\right)  +\left(  n+j\right)  =2n+i+j\equiv i+j\operatorname{mod}2\text{)}%
}}u_{j}v_{i}\det\left(  A_{\sim j,\sim i}\right) \\
&  =\sum_{i=1}^{n}\sum_{j=1}^{n}\left(  -1\right)  ^{i+j}u_{j}v_{i}\det\left(
A_{\sim j,\sim i}\right)  .
\end{align*}
Comparing this with%
\[
\operatorname*{ent}\left(  v\left(  \operatorname*{adj}A\right)  u\right)
=\sum_{i=1}^{n}\sum_{j=1}^{n}\left(  -1\right)  ^{i+j}u_{j}v_{i}\det\left(
A_{\sim j,\sim i}\right)  \ \ \ \ \ \ \ \ \ \ \left(  \text{by Corollary
\ref{cor.exeadd.det.rk1upd.claim.ent(v(adjA)u)}}\right)  ,
\]
we obtain $\det\left(  A+uv\right)  -\det A=\operatorname*{ent}\left(
v\left(  \operatorname*{adj}A\right)  u\right)  $. In other words,
$\det\left(  A+uv\right)  =\det A+\operatorname*{ent}\left(  v\left(
\operatorname*{adj}A\right)  u\right)  $. This proves Theorem
\ref{thm.exeadd.det.rk1upd.claim}.
\end{proof}

Now, Theorem \ref{thm.exeadd.det.rk1upd.claim} is proven; in other words,
Additional exercise \ref{exeadd.det.rk1upd} is solved (since Theorem
\ref{thm.exeadd.det.rk1upd.claim} is just a restatement of the claim of this exercise).

\subsection{Solution to Exercise \ref{exe.ps1.1.1}}

\begin{proof}
[Solution to Exercise \ref{exe.ps1.1.1}.]\textbf{(a)} We prove the claim by
induction over $\left\vert B\right\vert $.

\textit{Induction base:} Assume that $\left\vert B\right\vert =0$. Thus,
$B=\varnothing$, and thus there are no arrows of $Q$ which start at a vertex
in $A$ and at a vertex in $B$. Hence, $\operatorname*{mut}\nolimits_{A,B}Q=Q$,
and this can clearly be obtained from $Q$ by a sequence of mutations at sinks
(namely, by the empty sequence). Thus, Exercise \ref{exe.ps1.1.1} \textbf{(a)}
holds if $\left\vert B\right\vert =0$. This completes the induction
base.\footnote{Yes, this was a completely honest induction base. You don't
need to start at $\left\vert B\right\vert =1$ unless you want to use something
like $\left\vert B\right\vert >1$ in the induction step (but even then, you
should also handle the case $\left\vert B\right\vert =0$ case).}

\textit{Induction step:} Let $N\in\mathbb{N}$. Assume that Exercise
\ref{exe.ps1.1.1} \textbf{(a)} holds whenever $\left\vert B\right\vert =N$. We
now need to prove that Exercise \ref{exe.ps1.1.1} \textbf{(a)} holds whenever
$\left\vert B\right\vert =N+1$.

So let $A$ and $B$ be two subsets of $Q_{0}$ such that $A\cap B=\varnothing$
and $A\cup B=Q_{0}$. Assume that there exists no arrow of $Q$ that starts at a
vertex in $B$ and ends at a vertex in $A$. Assume further that $\left\vert
B\right\vert =N+1$. We need to prove that $\operatorname*{mut}\nolimits_{A,B}%
Q$ can be obtained from $Q$ by a sequence of mutations at sinks.

Notice that $B=Q_{0}\setminus A$ (since $A\cap B=\varnothing$ and $A\cup
B=Q_{0}$).

It is easy to see that there exist some $b\in B$ such that%
\begin{equation}
\text{there is no }e\in Q_{1}\text{ satisfying }t\left(  e\right)  =b\text{
and }s\left(  e\right)  \in B \label{sol.ps1.exe.1.1.a.3}%
\end{equation}
\footnote{\textit{Proof.} Assume the contrary. Thus, for every $b\in B$, there
is an $e\in Q_{1}$ satisfying $t\left(  e\right)  =b$ and $s\left(  e\right)
\in B$. Let us fix such an $e$ (for each $b\in B$), and denote it by $e_{b}$.
\par
Thus, for every $b\in B$, we have $e_{b}\in Q_{1}$ and $t\left(  e_{b}\right)
=b$ and $s\left(  e_{b}\right)  \in B$. We can thus define a sequence $\left(
b_{0},b_{1},b_{2},\ldots\right)  $ of vertices in $B$ recursively as follows:
Set $b_{0}=b$, and set $b_{i+1}=s\left(  e_{b_{i}}\right)  $ for every
$i\in\mathbb{N}$. Thus, $\left(  b_{0},b_{1},b_{2},\ldots\right)  $ is an
infinite sequence of elements of $B$. Since $B$ is a finite set, this sequence
must thus pass through an element twice (to say the least). In other words,
there are two positive integers $u$ and $v$ such that $u<v$ and $b_{u}=b_{v}$.
Consider these $u$ and $v$.
\par
Now, for every $i\in\mathbb{N}$, we have $t\left(  e_{b_{i}}\right)  =b_{i}$
(by the definition of $e_{b_{i}}$) and $s\left(  e_{b_{i}}\right)  =b_{i+1}$.
Thus, for every $i\in\mathbb{N}$, the arrow $e_{b_{i}}$ is an arrow from
$b_{i+1}$ to $b_{i}$. Thus, there is an arrow from $b_{i+1}$ to $b_{i}$ for
every $i\in\mathbb{N}$. In particular, we have an arrow from $b_{v}$ to
$b_{v-1}$, an arrow from $b_{v-1}$ to $b_{v-2}$, etc., and an arrow from
$b_{u+1}$ to $b_{u}$. Since $b_{u}=b_{v}$, these arrows form a cycle in $Q$,
which contradicts the hypothesis that the quiver $Q$ is acyclic. This
contradiction proves that our assumption was wrong, qed.}. Fix such a $b$.
Clearly, $b\notin A$ (since $b\in B=Q_{0}\setminus A$).

Now, $A\cup\left\{  b\right\}  $ and $B\setminus\left\{  b\right\}  $ are two
subsets of $Q_{0}$ such that $\left(  A\cup\left\{  b\right\}  \right)
\cap\left(  B\setminus\left\{  b\right\}  \right)  =\varnothing$ and $\left(
A\cup\left\{  b\right\}  \right)  \cup\left(  B\setminus\left\{  b\right\}
\right)  =Q_{0}$\ \ \ \ \footnote{\textit{Proof.} These are easy exercises in
set theory. Use $A\cap B=\varnothing$ and $A\cup B=Q_{0}$ and $b\in B$.}.
Furthermore, there exists no arrow of $Q$ that starts at a vertex in
$B\setminus\left\{  b\right\}  $ and ends at a vertex in $A\cup\left\{
b\right\}  $\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus, there
exists an arrow of $Q$ that starts at a vertex in $B\setminus\left\{
b\right\}  $ and ends at a vertex in $A\cup\left\{  b\right\}  $. Let $e$ be
such an arrow. Then, $s\left(  e\right)  \in B\setminus\left\{  b\right\}  $
and $t\left(  e\right)  \in A\cup\left\{  b\right\}  $.
\par
We have $s\left(  e\right)  \in B\setminus\left\{  b\right\}  \subseteq B$.
Thus, $t\left(  e\right)  \neq b$ (because having $t\left(  e\right)  =b$
would contradict (\ref{sol.ps1.exe.1.1.a.3})). Combined with $t\left(
e\right)  \in A\cup\left\{  b\right\}  $, this yields $t\left(  e\right)
\in\left(  A\cup\left\{  b\right\}  \right)  \setminus\left\{  b\right\}
\subseteq A$. Thus, $e$ is an arrow of $Q$ that starts at a vertex in $B$
(since $s\left(  e\right)  \in B$) and ends at a vertex in $A$ (since
$t\left(  e\right)  \in A$). This contradicts our hypothesis that there exists
no arrow of $Q$ that starts at a vertex in $B$ and ends at a vertex in $A$.
This is the desired contradiction, and so we are done.}. Hence,
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ is a well-defined acyclic quiver. Moreover, since $b\in B$, we
have $\left\vert B\setminus\left\{  b\right\}  \right\vert
=\underbrace{\left\vert B\right\vert }_{=N+1}-1=N+1-1=N$. Thus, Exercise
\ref{exe.ps1.1.1} \textbf{(a)} can be applied to $A\cup\left\{  b\right\}  $
and $B\setminus\left\{  b\right\}  $ instead of $A$ and $B$ (by the induction
hypothesis). As a consequence, we conclude that $\operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$ can be
obtained from $Q$ by a sequence of mutations at sinks.

We shall now prove that $\operatorname*{mut}\nolimits_{A,B}Q$ can be obtained
from $\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus
\left\{  b\right\}  }Q$ by a mutation at a sink. In fact, $b$ is a sink of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Thus,
there exists an arrow $e$ of $\operatorname*{mut}\nolimits_{A\cup\left\{
b\right\}  ,B\setminus\left\{  b\right\}  }Q$ which starts at $b$. Consider
this $e$.
\par
Recall that $\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q$ was obtained from $Q$ by turning all arrows
of $Q$ which start at a vertex in $A\cup\left\{  b\right\}  $ and end at a
vertex in $B\setminus\left\{  b\right\}  $. Thus, every arrow of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ which starts at a vertex in $B\setminus\left\{  b\right\}  $
and ends at a vertex in $A\cup\left\{  b\right\}  $ has originally been going
in the opposite direction in $Q$ (because there exists no arrow of $Q$ that
starts at a vertex in $B\setminus\left\{  b\right\}  $ and ends at a vertex in
$A\cup\left\{  b\right\}  $), while all the other arrows of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ have been copied over unchanged from $Q$. The arrow $e$ of
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ starts at $b$ (which is not an element of $B\setminus\left\{
b\right\}  $), so it does \textbf{not} start at a vertex in $B\setminus
\left\{  b\right\}  $ and end at a vertex in $A\cup\left\{  b\right\}  $;
therefore, the preceding sentence shows that this arrow $e$ has been copied
over unchanged from $Q$. In other words, the arrow $e$ starts at $b$ when
considered as an arrow of $Q$ as well. In other words, $s\left(  e\right)
=b$. (Recall that the functions $s$ and $t$ are part of the quiver $Q$; thus,
they map every arrow of $Q$ to its starting point and its terminal point,
respectively. The same arrows might have different starting points and
terminal points when regarded as arrows of $\operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$.)
\par
Recall that there exists no arrow of $Q$ that starts at a vertex in $B$ and
ends at a vertex in $A$. Thus, an arrow of $Q$ which starts at a vertex in $B$
must not end at a vertex in $A$. In particular, the arrow $e$ of $Q$ must not
end at a vertex in $A$ (because it starts at $b\in B$). Hence, the arrow $e$
of $Q$ ends at a vertex in $Q_{0}\setminus A=B$. In other words, $t\left(
e\right)  \in B$.
\par
We cannot have $t\left(  e\right)  =s\left(  e\right)  $ (because otherwise,
the arrow $e$ would form a cycle, but the quiver $Q$ is acyclic). Hence,
$t\left(  e\right)  \neq s\left(  e\right)  =b$ (since $e$ starts at $b$).
Combined with $t\left(  e\right)  \in B$, this yields $t\left(  e\right)  \in
B\setminus\left\{  b\right\}  $.
\par
Thus, the arrow $e$ of $Q$ starts at a vertex in $A\cup\left\{  b\right\}  $
(since $s\left(  e\right)  =b\in A\cup\left\{  b\right\}  $) and ends at a
vertex in $B\setminus\left\{  b\right\}  $ (since $t\left(  e\right)  \in
B\setminus\left\{  b\right\}  $). As we know, $\operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$ was
obtained from $Q$ by turning all such arrows. Hence, the arrow $e$ must have
been turned when it became an arrow of $\operatorname*{mut}\nolimits_{A\cup
\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$. But this contradicts
the fact that the arrow $e$ has been copied over unchanged from $Q$. This
contradiction proves that our assumption was wrong, qed.}. Hence, the mutation
$\mu_{b}\left(  \operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q\right)  $ is well-defined. We now have%
\begin{equation}
\operatorname*{mut}\nolimits_{A,B}Q=\mu_{b}\left(  \operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q\right)
\label{sol.ps1.exe.1.1.a.7}%
\end{equation}
\footnote{\textit{Proof of (\ref{sol.ps1.exe.1.1.a.7}):} We have $Q_{0}%
=A\cup\underbrace{B}_{=\left\{  b\right\}  \cup\left(  B\setminus\left\{
b\right\}  \right)  }=A\cup\left\{  b\right\}  \cup\left(  B\setminus\left\{
b\right\}  \right)  $.
\par
Recall that the quiver $\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q$ was obtained from $Q$ by turning all arrows
of $Q$ which start at a vertex in $A\cup\left\{  b\right\}  $ and end at a
vertex in $B\setminus\left\{  b\right\}  $. Furthermore, the quiver $\mu
_{b}\left(  \operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q\right)  $ was obtained from
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ by turning all arrows ending at $b$. Thus, $\mu_{b}\left(
\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q\right)  $ can be obtained from $Q$ by a two-step process, where
\par
\begin{itemize}
\item in the first step, we turn all arrows of $Q$ which start at a vertex in
$A\cup\left\{  b\right\}  $ and end at a vertex in $B\setminus\left\{
b\right\}  $;
\par
\item in the second step, we turn all arrows ending at $b$.
\end{itemize}
\par
Now, let us analyze what this two-step process does to an arrow of $Q$,
depending on where the arrow starts and ends:
\par
\begin{enumerate}
\item If $e$ is an arrow of $Q$ which ends at a vertex in $A$, then this arrow
never gets turned during our process. Indeed, let $e$ be such an arrow. Then,
$e$ ends at a vertex in $A$, and thus does not end at a vertex in $B$ (since
$A\cap B=\varnothing$); therefore, it does not end at a vertex in
$B\setminus\left\{  b\right\}  $ either. Hence, the first step does not turn
it. Therefore, after the first step, it still does not end at a vertex in $B$
(since it did not end at a vertex in $B$ originally). In particular, it does
not end at $b$ (since $b\in B$). Hence, it does not get turned at the second
step either. So, $e$ never turns, and thus retains its original direction
throughout the process.
\par
\item If $e$ is an arrow of $Q$ which ends at $b$, then this arrow gets turned
once (namely, at the second step). Thus, its direction is reversed at the end
of the process.
\par
\item If $e$ is an arrow of $Q$ which starts at a vertex in $A$ and ends at a
vertex in $B\setminus\left\{  b\right\}  $, then this arrow gets turned once
(namely, at the first step). Here is why: Let $e$ be an arrow of $Q$ which
starts at a vertex in $A$ and ends at a vertex in $B\setminus\left\{
b\right\}  $. Then, $e$ starts at a vertex in $A\cup\left\{  b\right\}  $ and
ends at a vertex in $B\setminus\left\{  b\right\}  $ (since $A\subseteq
A\cup\left\{  b\right\}  $). Thus, it gets turned at the first step. After
this, it becomes an arrow which ends at a vertex in $A$ (because originally it
started at a vertex in $A$), and so it does not end at $b$ (because $b\notin
A$). Therefore, it does not turn at the second step; hence, it has turned
exactly once altogether. Its direction is therefore reversed at the end of the
process.
\par
\item If $e$ is an arrow of $Q$ which starts at $b$ and ends at a vertex in
$B\setminus\left\{  b\right\}  $, then this arrow gets turned twice (once at
each step). Indeed, let $e$ be such an arrow. Then, $e$ starts at a vertex in
$A\cup\left\{  b\right\}  $ (namely, at $b$) and ends at a vertex in
$B\setminus\left\{  b\right\}  $. Hence, it gets turned at the first step.
After that, it ends at $b$ (because it used to start at $b$ before it was
turned), and therefore it gets turned again at the second step. Hence, the
direction of $e$ at the end of the two-step process is again the same as it
was in $Q$.
\par
\item If $e$ is an arrow of $Q$ which starts at a vertex in $B\setminus
\left\{  b\right\}  $ and ends at a vertex in $B\setminus\left\{  b\right\}
$, then this arrow never gets turned. Indeed, it starts at a vertex in
$B\setminus\left\{  b\right\}  $; thus, it does \textbf{not} start at a vertex
in $A\cup\left\{  b\right\}  $ (since $\underbrace{B}_{=Q_{0}\setminus
A}\setminus\left\{  b\right\}  =\left(  Q_{0}\setminus A\right)
\setminus\left\{  b\right\}  =Q_{0}\setminus\left(  A\cup\left\{  b\right\}
\right)  $). Hence, it does not get turned at the first step. Moreover, in
$Q$, this arrow $e$ does not end at $b$ (because it ends at a vertex in
$B\setminus\left\{  b\right\}  $); thus it does not end at $b$ after the first
step either (since it does not get turned at the first step). Hence, it does
not get turned at the second step either. Therefore, $e$ never gets turned,
and thus retains its original direction from $Q$ after the two-step process.
\end{enumerate}
\par
The five cases we have just considered cover all possibilities (because every
arrow $e$ either ends at a vertex in $A$ or ends at $b$ or ends at a vertex in
$B\setminus\left\{  b\right\}  $; and in the latter case, it either starts at
a vertex in $A$, or starts at $b$, or starts at a vertex in $B\setminus
\left\{  b\right\}  $ (since $Q_{0}=A\cup\left\{  b\right\}  \cup\left(
B\setminus\left\{  b\right\}  \right)  $)). From our case analysis, we can
draw the following conclusions:
\par
\begin{itemize}
\item If $e$ is an arrow of $Q$ which starts at a vertex in $A$ and ends at a
vertex in $B$, then the arrow $e$ has reversed its orientation at the end of
the two-step process. (This follows from our Cases 2 and 3 above.)
\par
\item If $e$ is an arrow of $Q$ which starts at a vertex in $B$ or ends at a
vertex in $A$, then this arrow $e$ has the same orientation at the end of the
two-step process as it did in $Q$. (Indeed, let us prove this. Let $e$ be an
arrow of $Q$ which starts at a vertex in $B$ or ends at a vertex in $A$. We
need to show that $e$ has the same orientation at the end of the two-step
process as it did in $Q$. If $e$ ends at a vertex in $A$, then this follows
from our analysis of Case 1. So let us assume that $e$ does not end at a
vertex in $A$. Hence, $e$ must start at a vertex in $B$ (since $e$ starts at a
vertex in $B$ or ends at a vertex in $A$). In other words, $s\left(  e\right)
\in B$. Hence, $t\left(  e\right)  \neq b$ (because if we had $t\left(
e\right)  =b$, then $e$ would contradict (\ref{sol.ps1.exe.1.1.a.3})). But
also $t\left(  e\right)  \notin A$ (since $e$ does not end at a vertex in
$A$), so that $t\left(  e\right)  \in Q_{0}\setminus A=B$ and thus $t\left(
e\right)  \in B\setminus\left\{  b\right\}  $ (since $t\left(  e\right)  \neq
b$). Hence, the arrow $e$ ends at a vertex in $B\setminus\left\{  b\right\}
$. It also starts at a vertex in $B$; thus, it either starts at $b$ or it
starts at a vertex in $B\setminus\left\{  b\right\}  $. Our claim now follows
from our analysis of Case 4 (in the case when $e$ starts at $b$) and from our
analysis of Case 5 (in the case when $e$ starts at a vertex in $B\setminus
\left\{  b\right\}  $). In either case, our claim is proven.)
\end{itemize}
\par
To summarize, the outcome of our two-step process is that every arrow $e$ of
$Q$ which starts at a vertex in $A$ and ends at a vertex in $B$ reverses its
orientation, while all other arrows preserve their orientation. In other
words, the outcome of our two-step process is the same as the outcome of
turning all arrows of $Q$ which start at a vertex in $A$ and end at a vertex
in $B$. But the latter outcome is $\operatorname*{mut}\nolimits_{A,B}Q$
(because this is how $\operatorname*{mut}\nolimits_{A,B}Q$ was defined), while
the former outcome is $\mu_{b}\left(  \operatorname*{mut}\nolimits_{A\cup
\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q\right)  $ (since we know
that $\mu_{b}\left(  \operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}
,B\setminus\left\{  b\right\}  }Q\right)  $ can be obtained from $Q$ by our
two-step process). Thus, we have obtained $\mu_{b}\left(  \operatorname*{mut}%
\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q\right)
=\operatorname*{mut}\nolimits_{A,B}Q$. This proves (\ref{sol.ps1.exe.1.1.a.7}%
).}. Therefore, $\operatorname*{mut}\nolimits_{A,B}Q$ can be obtained from
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ by a single mutation at a sink (namely, at the sink $b$). Since
$\operatorname*{mut}\nolimits_{A\cup\left\{  b\right\}  ,B\setminus\left\{
b\right\}  }Q$ (in turn) can be obtained from $Q$ by a sequence of mutations
at sinks, this shows that $\operatorname*{mut}\nolimits_{A,B}Q$ can be
obtained from $Q$ by a sequence of mutations at sinks (namely, we first need
to mutate at the sinks that give us $\operatorname*{mut}\nolimits_{A\cup
\left\{  b\right\}  ,B\setminus\left\{  b\right\}  }Q$, and then we have to
mutate at $b$). This proves that Exercise \ref{exe.ps1.1.1} \textbf{(a)} holds
whenever $\left\vert B\right\vert =N+1$. The induction step is complete, and
thus Exercise \ref{exe.ps1.1.1} \textbf{(a)} is solved.

\textbf{(b)} Let $i\in Q_{0}$ be a source in $Q$. Let $A=\left\{  i\right\}  $
and $B=Q_{0}\setminus A$. Then, $A$ and $B$ are two subsets of $Q_{0}$ such
that $A\cap B=\varnothing$ and $A\cup B=Q_{0}$. There exists no arrow of $Q$
that starts at a vertex in $B$ and ends at a vertex in $A$%
\ \ \ \ \footnote{\textit{Proof.} Assume the contrary. Then, there exists an
arrow of $Q$ which starts at a vertex in $B$ and ends at a vertex in $A$. Let
$e$ be such an arrow. Then, $e$ ends at a vertex in $A$. In other words,
$t\left(  e\right)  \in A=\left\{  i\right\}  $, so that $t\left(  e\right)
=i$. In other words, $e$ ends at $i$. But this is impossible, since $i$ is a
source. This contradiction proves that our assumption was wrong, qed.}. Hence,
the quiver $\operatorname*{mut}\nolimits_{A,B}Q$ is well-defined. Moreover,
this quiver $\operatorname*{mut}\nolimits_{A,B}Q$ is obtained by turning all
arrows of $Q$ which start at a vertex in $A$ and end at a vertex in $B$. But
these arrows are precisely the arrows of $Q$ starting at $i$%
\ \ \ \ \footnote{\textit{Proof.} Each arrow of $Q$ which starts at a vertex
in $A$ and ends at a vertex in $B$ must be an arrow starting at $i$ (because
it starts at a vertex in $A=\left\{  i\right\}  $, but the only vertex in
$\left\{  i\right\}  $ is $i$). It thus remains to prove the converse -- i.e.,
to prove that each arrow of $Q$ starting at $i$ is an arrow of $Q$ which
starts at a vertex in $A$ and ends at a vertex in $B$. So let $e$ be an arrow
of $Q$ starting at $i$. Then, $e$ clearly starts at a vertex in $A$ (since
$i\in\left\{  i\right\}  =A$). It remains to prove that $e$ ends at a vertex
in $B$. But $Q$ is acyclic, and thus we cannot have $s\left(  e\right)
=t\left(  e\right)  $ (since otherwise, the arrow $e$ would form a trivial
cycle). Hence, $s\left(  e\right)  \neq t\left(  e\right)  $. But $s\left(
e\right)  =i$ (since $e$ starts at $i$), so that $t\left(  e\right)  \neq
s\left(  e\right)  =i$ and thus $t\left(  e\right)  \in Q_{0}\setminus
\underbrace{\left\{  i\right\}  }_{=A}=Q_{0}\setminus A=B$. Hence, $e$ ends at
a vertex in $B$. This completes our proof.}. Hence, $\operatorname*{mut}%
\nolimits_{A,B}Q$ is obtained by turning all arrows of $Q$ starting at $i$.
But this is exactly how we defined $\mu_{i}\left(  Q\right)  $. Therefore,
$\operatorname*{mut}\nolimits_{A,B}Q=\mu_{i}\left(  Q\right)  $. Now, Exercise
\ref{exe.ps1.1.1} \textbf{(a)} shows that $\operatorname*{mut}\nolimits_{A,B}%
Q$ can be obtained from $Q$ by a sequence of mutations at sinks. Hence,
$\mu_{i}\left(  Q\right)  $ can be obtained from $Q$ by a sequence of
mutations at sinks (since $\operatorname*{mut}\nolimits_{A,B}Q=\mu_{i}\left(
Q\right)  $). Exercise \ref{exe.ps1.1.1} \textbf{(b)} is proven.

\textbf{(c)} Let $Q^{\prime}$ be any acyclic quiver which can be obtained from
$Q$ by turning some of its arrows. We need to prove that $Q^{\prime}$ can also
be obtained from $Q$ by a sequence of mutations at sinks. But \cite[proof of
Proposition 2.2.8]{Lampe} shows that $Q^{\prime}$ can be obtained from $Q$ by
a sequence of mutations at sinks and sources. Since every mutation at a source
can be simulated by a sequence of mutations at sinks (by Exercise
\ref{exe.ps1.1.1} \textbf{(b)}), this yields that $Q^{\prime}$ can be obtained
from $Q$ by a sequence of mutations at sinks. This solves Exercise
\ref{exe.ps1.1.1} \textbf{(c)}.
\end{proof}

\begin{thebibliography}{999999999}                                                                                        %


\bibitem[Abeles14]{Abeles}%
\href{http://www.sciencedirect.com/science/article/pii/S0024379514002249}{Francine
F. Abeles, \textit{Chi\`{o}'s and Dodgson's determinantal identities}, Linear
Algebra and its Applications, Volume 454, 1 August 2014, pp. 130--137}.

\bibitem[Aigner07]{Aigner07}%
\href{http://doi.org/10.1007/978-3-540-39035-0}{Martin Aigner, \textit{A
Course in Enumeration}, Graduate Texts in Mathematics \#238, Springer 2007.}

\bibitem[AigZie]{AigZie}Martin Aigner, G\"{u}nter M. Ziegler, \textit{Proofs
from the Book}, 4th edition, Springer 2010.

\bibitem[AmaEsc05]{AmaEsc05}%
\href{http://www.springer.com/us/book/9783764377557}{Herbert Amann, Joachim
Escher, \textit{Analysis I}, translated from the German by Gary Brookfield,
Birkh\"{a}user 2005}.

\bibitem[AndDos]{AndDos}Titu Andreescu, Gabriel Dospinescu, \textit{Problems
from the Book}, XYZ Press 2008.

\bibitem[Artin]{Artin}Michael Artin, \textit{Algebra}, 2nd edition, Pearson 2010.

\bibitem[Axler]{Axler}\href{http://linear.axler.net/}{Sheldon Axler,
\textit{Linear Algebra Done Right}, 3rd edition, Springer 2015.}

\bibitem[BenDre07]{BenDre-Vand}%
\href{http://home.wlu.edu/~dresdeng/papers/VDM.pdf}{Arthur T. Benjamin and
Gregory P. Dresden, \textit{A Combinatorial Proof of Vandermonde's
Determinant}, The American Mathematical Monthly, Vol. 114, No. 4 (Apr., 2007),
pp. 338--341.}

\bibitem[BenQui04]{BenQui-fib}%
\href{https://www.math.hmc.edu/~benjamin/papers/bama.pdf}{Arthur T. Benjamin
and Jennifer J. Quinn, \textit{Proofs that Really Count: The Magic of
Fibonacci Numbers and More}, Mathematical Adventures for Students and
Amateurs, (David F. Hayes and Tatiana Shubin, editors), Spectrum Series of
MAA, pp. 83--98, 2004.}

\bibitem[BerBru08]{BerBru08}%
\href{http://www.math.ualberta.ca/ijiss/SS-Volume-4-2008/No-1-08/SS-08-01-01.pdf}{Adam
Berliner and Richard A. Brualdi, \textit{A combinatorial proof of the
Dodgson/Muir determinantal identity}, International Journal of Information and
Systems Sciences, Volume 4 (2008), Number 1, pp. 1--7.}

\bibitem[Bergma15]{Bergman-Lang}George M. Bergman, \textit{A Companion to
Lang's Algebra}, website (2015).\newline%
\href{https://math.berkeley.edu/~gbergman/.C.to.L/}{https://math.berkeley.edu/\symbol{126}%
gbergman/.C.to.L/}

\bibitem[BirMac99]{BirkMac}Saunders Mac Lane, Garrett Birkhoff,
\textit{Algebra}, 3rd edition, AMS Chelsea Publishing 1999.

\bibitem[BjoBre05]{BjoBre05}\href{http://doi.org/10.1007/3-540-27596-7}{Anders
Bj\"{o}rner, Francesco Brenti, \textit{Combinatorics of Coxeter Groups},
Graduate Texts in Mathematics \#231, Springer 2005.}

\bibitem[Bona12]{Bona12}Miklos Bona, \textit{Combinatorics of Permutations},
2nd edition, CRC Press 2012.

\bibitem[Bresso99]{Bresso99}David M. Bressoud, \textit{Proofs and
Confirmations: The Story of the Alternating Sign Matrix Conjecture}, Cambridge
University Press 1999.

\bibitem[BruRys91]{BruRys91}Richard A. Brualdi, Herbert J. Ryser,
\textit{Combinatorial Matrix Theory}, Cambridge University Press 1991.

\bibitem[CaSoSp12]{CaSoSp12}Sergio Caracciolo, Alan D. Sokal, Andrea
Sportiello, \textit{Algebraic/combinatorial proofs of Cayley-type identities
for derivatives of determinants and pfaffians}, Advances in Applied
Mathematics 50, pp. 474--594 (2013). Also appears as arXiv preprint
\texttt{\href{http://arxiv.org/abs/1105.6270v2}{arXiv:1105.6270v2}}.

\bibitem[Conrad]{Conrad}Keith Conrad, \textit{Sign of permutations},
\newline\url{http://www.math.uconn.edu/~kconrad/blurbs/grouptheory/sign.pdf} .

\bibitem[Conrad2]{Conrad-Pf}Keith Conrad, \textit{Bilinear forms},\newline\url{http://www.math.uconn.edu/~kconrad/blurbs/linmultialg/bilinearform.pdf}

\bibitem[Day16]{Day-proofs}Martin V. Day, \textit{An Introduction to Proofs
and the Mathematical Vernacular}, 7 December 2016. \newline%
\url{https://www.math.vt.edu/people/day/ProofsBook/IPaMV.pdf} .

\bibitem[EdeStra04]{EdelStrang}%
\href{https://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Edelman189-197.pdf}{Alan
Edelman and Gilbert Strang, \textit{Pascal Matrices}, American Mathematical
Monthly, Vol. 111, No. 3 (March 2004), pp. 189--197.}

\bibitem[Eisenk]{Eisenk-planepartits}%
\href{http://arxiv.org/abs/math/9712261v2}{Theresia Eisenk\"{o}lbl,
\textit{Rhombus Tilings of a Hexagon with Three Fixed Border Tiles},
arXiv:math/9712261v2.} Published in: J. Combin. Theory Ser. A 88 (1999), pp. 368--378.

\bibitem[Fulton97]{Fulton-Young}William Fulton, \textit{Young Tableaux: With
Applications to Representation Theory and Geometry}, Cambridge University
Press 1997.

\bibitem[GalKar71]{GalKar71}%
\href{http://www.sciencedirect.com/science/article/pii/S0022000072800163}{David
Gale, Richard M. Karp, \textit{A phenomenon in the theory of sorting}, Journal
of Computer and System Sciences, Volume 6, Issue 2, April 1972, pp. 103--115}.

\bibitem[Gessel79]{Gessel-Vand}Ira Gessel, \textit{Tournaments and
Vandermonde's Determinant}, Journal of Graph Theory, Vol. 3 (1979), pp. 305--307.

\bibitem[Gill12]{Gill}%
\href{http://cseweb.ucsd.edu/~gill/CILASite/Resources/LinearAlgebra.pdf}{Stanley
Gill Williamson, \textit{Matrix Canonical Forms: notational skills and proof
techniques}, 15 July 2015.}

\bibitem[Gill15]{Gill15}\href{http://arxiv.org/abs/1505.05486v1}{Stanley Gill
Williamson, \textit{The common-submatrix Laplace expansion},
arXiv:1505.05486v1}.

\bibitem[GohKol]{GohKol}Israel Gohberg, Israel Koltracht, \textit{Triangular
factors of Cauchy and Vandermonde matrices}, Integr. Equat. Oper. Th. Vol. 26
(1996), pp. 46--59.

\bibitem[Goodman]{Goodman}Frederick M. Goodman, \textit{Algebra: Abstract and
Concrete}, edition 2.6, 1 May 2015.\newline%
\url{http://homepage.math.uiowa.edu/~goodman/algebrabook.dir/book.2.6.pdf} .

\bibitem[Gould10]{Gould-I}\href{http://www.math.wvu.edu/~gould/Vol.4.PDF}{H.
W. Gould, \textit{Combinatorial Identities: Table I: Intermediate Techniques
for Summing Finite Series}, Edited and Compiled by Jocelyn Quaintance, May 3,
2010}.

\bibitem[Gri10]{GriHyp}Darij Grinberg, \textit{A hyperfactorial divisibility},
version of 27 July 2015.\newline\url{http://www.cip.ifi.lmu.de/~grinberg/}

\bibitem[Gri11]{Gri-zeck}Darij Grinberg, \textit{Zeckendorf family identities
generalized}, arXiv preprint
\texttt{\href{http://arxiv.org/abs/1103.4507v1}{arXiv:1103.4507v1}}.

\bibitem[Gri09]{Gri-19.9}%
\href{http://www.cip.ifi.lmu.de/~grinberg/19-9ML.pdf}{Darij Grinberg,
\textit{Solution to Problem 19.9 from \textquotedblleft Problems from the
Book\textquotedblright}}.\newline\url{http://www.cip.ifi.lmu.de/~grinberg/solutions.html}

\bibitem[GriRei15]{Reiner}Darij Grinberg, Victor Reiner, \textit{Hopf algebras
in Combinatorics}, August 22, 2016. arXiv preprint
\href{http://www.arxiv.org/abs/1409.8356v4}{\texttt{arXiv:1409.8356v4}}.
\newline A version which is more often updated can be found at
\url{http://www.math.umn.edu/~reiner/Classes/HopfComb.pdf} .

\bibitem[GrKnPa]{GKP}Ronald L. Graham, Donald E. Knuth, Oren Patashnik,
\textit{Concrete Mathematics, Second Edition}, Addison-Wesley 1994.

\bibitem[Hefferon]{Hefferon}Jim Hefferon, \textit{Linear Algebra}, version of
1 January 2017,\newline\url{http://joshua.smcvt.edu/linearalgebra/} .

\bibitem[Heinig11]{Heinig}\href{http://arxiv.org/abs/1103.2717v3}{Peter
Christian Heinig, \textit{Chio Condensation and Random Sign Matrices},
arXiv:1103.2717v3}.

\bibitem[Herstein]{Herstein}I. N. Herstein, \textit{Topics in Algebra}, 2nd
edition, John Wiley \& Sons, 1975.

\bibitem[HoffKun]{HoffmanKunze}Kenneth Hoffman, Ray Kunze, \textit{Linear
algebra}, 2nd edition, Prentice-Hall 1971.

\bibitem[Hunger03]{Hungerford-03}%
\href{http://link.springer.com/book/10.1007/978-1-4612-6101-8}{Thomas W.
Hungerford, \textit{Algebra}, 12th printing, Springer 2003.}

\bibitem[Hunger14]{Hungerford}Thomas W. Hungerford, \textit{Abstract Algebra:
An Introduction}, 3rd edition, Brooks/Cole 2014.

\bibitem[Jacobs10]{Jacobs10}Nathan Jacobson, \textit{Finite-Dimensional
Division Algebras Over Fields}, corrected 2nd printing, Springer 2010.

\bibitem[KarZha16]{KarZha16}%
\href{http://www.cip.ifi.lmu.de/~grinberg/primes2015/kazh-exp.pdf}{Karthik
Karnik, Anya Zhang, \textit{Combinatorial proof of Chio Pivotal Condensation},
25 May 2016.}

\bibitem[KenWil14]{KenWil14}\href{http://arxiv.org/abs/1411.7425v1}{Richard W.
Kenyon, David B. Wilson, \textit{The space of circular planar electrical
networks}, arXiv:1411.7425v1.}

\bibitem[KleLak72]{KleLak72}S. L. Kleiman and Dan Laksov, \textit{Schubert
Calculus}, The American Mathematical Monthly, Vol. 79, No. 10, (Dec., 1972),
pp. 1061--1082.

\bibitem[Knuth88]{Knuth-fib}%
\href{http://dx.doi.org/10.1016/0893-9659(89)90131-6}{Donald E. Knuth,
\textit{Fibonacci Multiplication}, Appl. Math. Lett., Vol. 1, No. 1, pp.
57--60, 1988}.

\bibitem[Knutson]{Knutson}Allen Knutson,
\textit{\href{http://www.math.cornell.edu/~allenk/schubnotes.pdf}{Schubert
polynomials and Symmetric Functions}}, lecture notes, July 28, 2012.

\bibitem[Kratt]{Krattenthaler}Christian Krattenthaler, \textit{Advanced
Determinant Calculus}, S\'{e}minaire Lotharingien Combin. 42 (1999) (The
Andrews Festschrift), paper B42q, 67 pp.,
\href{http://arxiv.org/abs/math/9902004v3}{arXiv:math/9902004v3}.

\bibitem[LLPT95]{LLPT95}D. Laksov, A. Lascoux, P. Pragacz, and A. Thorup,
\textit{The LLPT Notes}, edited by A. Thorup, 1995,\newline%
\url{http://www.math.ku.dk/~thorup/notes/sympol.pdf} .

\bibitem[Lalonde]{Lalonde}Pierre Lalonde, \textit{A non-commutative version of
Jacobi's equality on the cofactors of a matrix}, Discrete Mathematics 158
(1996), pp. 161--172.\newline%
\url{http://www.sciencedirect.com/science/article/pii/0012365X95000404} .

\bibitem[Lampe]{Lampe}Philipp Lampe, \textit{Cluster algebras},\newline%
\url{http://www.math.uni-bielefeld.de/~lampe/teaching/cluster/cluster.pdf} .

\begin{verlong}
A version with my corrections:\newline%
\url{https://www.dropbox.com/s/aush67ecrx6twlf/cluster - version 4 dec 2013 EV.pdf?dl=0}
.
\end{verlong}

\bibitem[Lang02]{Lang02}%
\href{http://link.springer.com/book/10.1007/978-1-4613-0041-0}{Serge Lang,
\textit{Algebra}, Revised Third Edition, Graduate Texts in Mathematics \#211,
Springer 2002.}

\bibitem[Laue]{Laue-det}Hartmut Laue, \textit{Determinants}, version 17 May
2015,\newline%
\url{http://www.uni-kiel.de/math/algebra/laue/homepagetexte/det.pdf} .

\bibitem[LeeSchi]{LS2}Kyungyong Lee, Ralf Schiffler, \textit{A Combinatorial
Formula for Rank }$2$\textit{ Cluster Variables},\newline%
\url{http://arxiv.org/abs/1106.0952v3} .

\bibitem[LeeSch2]{LS}Kyungyong Lee, Ralf Schiffler, \textit{Positivity for
cluster algebras},\newline\url{http://arxiv.org/abs/1306.2415v3} .

\bibitem[Leeuwen]{Leeuwen-aS}%
\href{http://wwwmathlabo.univ-poitiers.fr/~maavl/pdf/alt-Schur.pdf}{Marc A. A.
van Leeuwen, \textit{Schur functions and alternating sums}}, Electronic
Journal of Combinatorics Vol 11(2) A5 (2006), also available as
\href{http://arxiv.org/abs/math.CO/0602357}{arXiv:math.CO/0602357}.

\bibitem[LeLeMe16]{LeLeMe16}Eric Lehman, F. Thomson Leighton, Albert R. Meyer,
\textit{Mathematics for Computer Science}, revised Tuesday 17th May 2017,
\newline\url{https://courses.csail.mit.edu/6.042/spring17/mcs.pdf} .

\bibitem[Loehr11]{Loehr-BC}%
\href{http://www.math.vt.edu/people/nloehr/bijbook.html}{Nicholas A. Loehr,
\textit{Bijective Combinatorics}, Chapman \& Hall/CRC 2011.}

\bibitem[Muir]{Muir}%
\href{http://phalanstere.univ-mlv.fr/~al/#Classiques}{Thomas Muir, \textit{The
theory of determinants in the historical order of development}, 5 volumes
(1906--1930), later reprinted by Dover.}

\bibitem[NouYam]{NoumiYamada}%
\href{http://arxiv.org/abs/math-ph/0203030v2}{Masatoshi Noumi, Yasuhiko
Yamada, \textit{Tropical Robinson-Schensted-Knuth correspondence and
birational Weyl group actions}, arXiv:math-ph/0203030v2.}

\bibitem[OlvSha]{OlvSha}Peter J. Olver, Chehrzad Shakiban, \textit{Applied
Linear Algebra}, Prentice Hall, 2006.\newline See also
\url{http://www.math.umn.edu/~olver/ala.html} for corrections.

\bibitem[OruPhi]{OruPhi}%
\href{http://www.sciencedirect.com/science/article/pii/S0024379500001245}{Halil
Oru\c{c}, George M. Phillips, \textit{Explicit factorization of the
Vandermonde matrix}, Linear Algebra and its Applications 315 (2000), pp.
113--123.}

\bibitem[Prasolov]{Prasolov}Viktor V. Prasolov,
\textit{\href{http://www2.math.su.se/~mleites/books/prasolov-1994-problems.pdf}{\textit{Problems
and Theorems in Linear Algebra}}}, Translations of Mathematical Monographs,
vol. \#134, AMS 1994.

\bibitem[Pretzel]{Pretzel}Oliver Pretzel, \textit{On reorienting graphs by
pushing down maximal vertices, }Order, 1986, Volume 3, Issue 2, pp. 135--153.

\bibitem[Richma]{Richman}%
\href{http://www.ams.org/journals/proc/1988-103-04/S0002-9939-1988-0954974-5/}{Fred
Richman, \textit{Nontrivial uses of trivial rings}, Proceedings of the
American Mathematical Society, Volume 103, Number 4, August 1988, pp.
1012--1014.}

\bibitem[Rote]{Rote}G\"{u}nter Rote, \textit{Division-Free Algorithms for the
Determinant and the Pfaffian: Algebraic and Combinatorial Approaches},
Computational Discrete Mathematics, Lecture Notes in Computer Science, Volume
2122, 2001, pp. 119--135.\newline\url{http://www.inf.fu-berlin.de/groups/ag-ti/people/rote/Papers/pdf/Division-free+algorithms.pdf}

\bibitem[Rotman15]{Rotman15}Joseph J. Rotman, \textit{Advanced Modern Algebra:
Part 1}, Graduate Studies in Mathematics \#165, 3rd edition, AMS 2015.

\bibitem[Silvest]{Silvest}%
\href{https://web.archive.org/web/20140505161153/http://www.mth.kcl.ac.uk/~jrs/gazette/blocks.pdf}{John
R. Silvester, \textit{Determinants of Block Matrices}, The Mathematical
Gazette, Vol. 84, No. 501 (Nov., 2000), pp. 460--467.}

\bibitem[Stan11]{Stanley-EC1}Richard Stanley, \textit{Enumerative
Combinatorics, volume 1}, Second edition, version of 15 July 2011. Available
at \url{http://math.mit.edu/~rstan/ec/} .

\bibitem[Stan01]{Stanley-EC2}Richard Stanley, \textit{Enumerative
Combinatorics, volume 2}, First edition 2001.

\bibitem[Stembr]{Stembridge}%
\href{http://www.combinatorics.org/ojs/index.php/eljc/article/view/v9i1n5}{John
R. Stembridge, \textit{A Concise Proof of the Littlewood-Richardson Rule}, The
Electronic Journal of Combinatorics, Volume 9 (2002), Note \#N5.}

\bibitem[Tenner04]{Tenner-NMU}Bridget Eileen Tenner, \textit{A Non-Messing-Up
Phenomenon for Posets}, Annals of Combinatorics 11 (2007), pp.
101--114.\newline A preprint is available as
\href{http://arxiv.org/abs/math/0404396}{arXiv:math/0404396}.

\bibitem[Walker87]{Walker87}%
\href{https://www.math.nmsu.edu/~elbert/AbsAlgeb.pdf}{Elbert A. Walker,
\textit{Introduction to Abstract Algebra}, Random House/Birkhauser, New York,
1987.}

\bibitem[William03]{William03}%
\href{http://people.mpim-bonn.mpg.de/geordie/Hecke.pdf}{Geordie Williamson,
\textit{Mind your }$P$\textit{ and }$Q$\textit{-symbols: Why the
Kazhdan-Lusztig basis of the Hecke algebra of type A is cellular}, Honours
thesis at the University of Sydney, October 2003.}

\bibitem[Zeilbe85]{Zeilbe}%
\href{http://www.math.rutgers.edu/~zeilberg/mamarimY/DM85.pdf}{Doron
Zeilberger, \textit{A combinatorial approach to matrix algebra}, Discrete
Mathematics 56 (1985), pp. 61--72.}

\bibitem[Zeilbe98]{zeilberger-twotime}Doron Zeilberger, \textit{Dodgson's
Determinant-Evaluation Rule proved by Two-Timing Men and Women}, The
Electronic Journal of Combinatorics, vol. 4, issue 2 (1997) (The Wilf
Festschrift volume), R22.\newline%
\url{http://www.combinatorics.org/ojs/index.php/eljc/article/view/v4i2r22}
\newline Also available as arXiv:math/9808079v1.\newline\url{http://arxiv.org/abs/math/9808079v1}
\end{thebibliography}


\end{document}